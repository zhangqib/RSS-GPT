<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices</title>
<link>https://arxiv.org/abs/2511.10680</link>
<guid>https://arxiv.org/abs/2511.10680</guid>
<content:encoded><![CDATA[
<div> arXiv, energy forecasting, edge devices, Temporal Convolutional Network, real-time inference<br><br>Summary:<br><br>1. This paper introduces LAD-BNet (Lag-Aware Dual-Branch Network), a novel neural network architecture designed specifically for real-time energy forecasting on edge devices, focusing on smart grid optimization and intelligent building management.<br><br>2. LAD-BNet features a hybrid design combining a branch that explicitly leverages temporal lags with a Temporal Convolutional Network (TCN) that utilizes dilated convolutions, allowing the model to capture both short- and long-term temporal dependencies effectively.<br><br>3. The model was evaluated on real energy consumption datasets with a temporal resolution of 10 minutes, achieving a mean absolute percentage error (MAPE) of 14.49% for 1-hour ahead forecasts.<br><br>4. It runs with only 18 milliseconds inference time on a Google Coral Edge TPU, delivering an 8 to 12 times speedup compared to CPU-based inference, making it well-suited for resource-constrained embedded devices.<br><br>5. LAD-BNet demonstrates a 2.39% improvement over LSTM baselines and a 3.04% increase over pure TCN models, while maintaining a moderate memory footprint of 180MB, enabling predictions up to 12 hours with manageable accuracy degradation—highlighting its industrial potential for real-time energy optimization and demand management applications. <div>
arXiv:2511.10680v1 Announce Type: new 
Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups</title>
<link>https://arxiv.org/abs/2511.10683</link>
<guid>https://arxiv.org/abs/2511.10683</guid>
<content:encoded><![CDATA[
<div> Keywords: long-tailed distributions, parameter-efficient fine-tuning, head-tail ratio, LT-Soups, model soups<br><br>Summary:  
This paper addresses the challenge of learning from real-world long-tailed (LT) datasets, where head classes dominate and tail classes are underrepresented. It highlights that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer, while effective at preserving tail-class performance in foundation models such as CLIP, tend to reduce accuracy on the head classes. The authors identify the head-tail ratio—the proportion between head and tail classes—as a key factor influencing this trade-off, which has been overlooked in prior work. Through controlled experiments on CIFAR100 varying both imbalance ratio (ρ) and head-tail ratio (η), they demonstrate that PEFT methods excel in tail-heavy scenarios but perform worse in balanced or head-heavy distributions. To address these shortcomings, they propose LT-Soups, a two-stage model soups framework. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to mitigate head-class bias. In the second stage, it fine-tunes only the classifier on the entire dataset to recover head-class accuracy. Experimental evaluations on six benchmark datasets show that LT-Soups consistently outperforms both PEFT and traditional model soups, achieving a better balance across various LT regimes and improving generalization over diverse long-tailed data distributions. <div>
arXiv:2511.10683v1 Announce Type: new 
Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($\rho$) and head-tail ratio ($\eta$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable Sparse Identification of Lagrangian Dynamics</title>
<link>https://arxiv.org/abs/2511.10706</link>
<guid>https://arxiv.org/abs/2511.10706</guid>
<content:encoded><![CDATA[
<div> Keywords: Lagrangian system identification, cubic B-Spline, sparse regression, noise robustness, nonlinear dynamics<br><br>Summary:<br><br>This paper addresses the challenge of discovering governing equations from data in nonlinear dynamical systems, focusing on overcoming the limitations of sparse regression methods that struggle with rational functions and noise sensitivity. First, it introduces the integration of cubic B-Spline approximation into Lagrangian system identification, allowing for accurate modeling of complex nonlinearities within mechanical systems. Second, it proposes a robust equation discovery mechanism that leverages measurement data efficiently while embedding known physical constraints to improve identification accuracy. Third, the work develops a recursive derivative computation scheme based on B-spline basis functions, which constrains higher-order derivatives and significantly reduces sensitivity to measurement noise, particularly in second-order dynamical systems. The combination of these contributions results in a differentiable sparse identification framework that outperforms existing baseline methods, delivering more accurate and reliable extraction of physical laws from noisy data. This approach is particularly well-suited for complex mechanical systems where data is often limited and corrupted by noise, making it a promising advancement in data-driven nonlinear system identification. <div>
arXiv:2511.10706v1 Announce Type: new 
Abstract: Data-driven discovery of governing equations from data remains a fundamental challenge in nonlinear dynamics. Although sparse regression techniques have advanced system identification, they struggle with rational functions and noise sensitivity in complex mechanical systems. The Lagrangian formalism offers a promising alternative, as it typically avoids rational expressions and provides a more concise representation of system dynamics. However, existing Lagrangian identification methods are significantly affected by measurement noise and limited data availability. This paper presents a novel differentiable sparse identification framework that addresses these limitations through three key contributions: (1) the first integration of cubic B-Spline approximation into Lagrangian system identification, enabling accurate representation of complex nonlinearities, (2) a robust equation discovery mechanism that effectively utilizes measurements while incorporating known physical constraints, (3) a recursive derivative computation scheme based on B-spline basis functions, effectively constraining higher-order derivatives and reducing noise sensitivity on second-order dynamical systems. The proposed method demonstrates superior performance and enables more accurate and reliable extraction of physical laws from noisy data, particularly in complex mechanical systems compared to baseline methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2511.10707</link>
<guid>https://arxiv.org/abs/2511.10707</guid>
<content:encoded><![CDATA[
<div> Parameter-Efficient Finetuning, Representation Finetuning, Mathematical Reasoning, Prefix Generation, Error Accumulation  

<br><br>Summary:  
This paper addresses the challenge of improving mathematical reasoning in large language models through finetuning techniques. It identifies that while Representation Finetuning (ReFT) is more parameter-efficient than standard Parameter-Efficient Finetuning (PEFT) and shows superior performance on several tasks, it underperforms significantly on mathematical reasoning tasks. The primary cause is ReFT’s difficulty in generating effective reasoning prefixes during the early stages of inference, which leads to disruption of numerical encoding and error accumulation during chain-of-thought (CoT) reasoning. To overcome these limitations, the authors propose Bias-REstrained Prefix Representation FineTuning (BREP ReFT). BREP ReFT enhances performance by truncating training data to better optimize the generation of initial reasoning prefixes, intervening during the early inference phase to prevent error buildup, and constraining the magnitude of intervention vectors to avoid disturbing numerical encoding. Extensive experiments conducted across various model architectures demonstrate that BREP outperforms both standard ReFT and traditional weight-based PEFT methods on mathematical reasoning tasks. This method achieves superior effectiveness, efficiency, and robust generalization. The paper also provides the source code publicly at https://github.com/LiangThree/BREP for further research and application. <div>
arXiv:2511.10707v1 Announce Type: new 
Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Uncertainty Quantification in Generative Model Learning</title>
<link>https://arxiv.org/abs/2511.10710</link>
<guid>https://arxiv.org/abs/2511.10710</guid>
<content:encoded><![CDATA[
<div> Uncertainty quantification, generative models, precision-recall curves, distribution approximation, ensemble methods<br><br>Summary: This paper addresses a critical yet often overlooked issue in generative model research: the quantification of uncertainty in how well these models approximate target distributions. While typical evaluations measure the closeness between learned and true distributions, they fail to account for the inherent uncertainty in these estimates. The authors formalize the concept of uncertainty quantification in the context of generative model learning and propose new directions for research to fill this gap. One such direction is the use of ensemble-based precision-recall curves, which aggregate multiple model outputs to better capture uncertainty. Preliminary experiments conducted on synthetic datasets provide evidence that aggregated precision-recall curves effectively reflect model approximation uncertainty. This approach not only offers a more nuanced evaluation metric but also facilitates systematic comparisons between different generative model architectures based on their uncertainty profiles. Overall, this position paper highlights the importance of incorporating uncertainty quantification into the evaluation framework of generative models, thus promoting more robust assessments and guiding future methodological advancements in the field. <div>
arXiv:2511.10710v1 Announce Type: new 
Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Movement-Specific Analysis for FIM Score Classification Using Spatio-Temporal Deep Learning</title>
<link>https://arxiv.org/abs/2511.10713</link>
<guid>https://arxiv.org/abs/2511.10713</guid>
<content:encoded><![CDATA[
<div> Functional Independence Measure, Deep Neural Network, ST-GCN, BiLSTM, Attention Mechanism<br><br>Summary:<br><br>The article addresses the challenge of assessing patients' physical independence in daily activities through the Functional Independence Measure (FIM), which traditionally requires significant effort from both patients and healthcare professionals. To alleviate this burden, the authors propose an automated method for estimating FIM scores using simple exercises that differ from the standard FIM assessment tasks. Their approach integrates a deep neural network architecture combining a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to capture temporal dependencies and highlight key body-joint contributions. The model was evaluated on data from 277 rehabilitation patients, focusing specifically on FIM transfer and locomotion items. Results demonstrate the model's capability to differentiate between fully independent patients and those needing assistance, achieving balanced accuracy scores between 70.09% and 78.79% across various FIM items. Furthermore, the study identifies distinct movement patterns serving as reliable predictors for certain FIM evaluation components, suggesting potential for automated, less burdensome assessment in clinical rehabilitation contexts. <div>
arXiv:2511.10713v1 Announce Type: new 
Abstract: The functional independence measure (FIM) is widely used to evaluate patients' physical independence in activities of daily living. However, traditional FIM assessment imposes a significant burden on both patients and healthcare professionals. To address this challenge, we propose an automated FIM score estimation method that utilizes simple exercises different from the designated FIM assessment actions. Our approach employs a deep neural network architecture integrating a spatial-temporal graph convolutional network (ST-GCN), bidirectional long short-term memory (BiLSTM), and an attention mechanism to estimate FIM motor item scores. The model effectively captures long-term temporal dependencies and identifies key body-joint contributions through learned attention weights. We evaluated our method in a study of 277 rehabilitation patients, focusing on FIM transfer and locomotion items. Our approach successfully distinguishes between completely independent patients and those requiring assistance, achieving balanced accuracies of 70.09-78.79 % across different FIM items. Additionally, our analysis reveals specific movement patterns that serve as reliable predictors for particular FIM evaluation items.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation</title>
<link>https://arxiv.org/abs/2511.10796</link>
<guid>https://arxiv.org/abs/2511.10796</guid>
<content:encoded><![CDATA[
<div> Neural Tangent Kernel, trace estimation, Hutch++, automatic differentiation, matrix-free methods<br><br>Summary:<br><br>This paper addresses the computational challenges associated with analyzing the Neural Tangent Kernel (NTK), particularly focusing on recurrent architectures where full NTK matrix calculation is often impractical. The authors propose a matrix-free approach leveraging trace estimation techniques, enabling rapid analysis of finite-width empirical NTKs. They introduce numerical methods based on the Hutch++ trace estimator, which come with provable fast convergence guarantees. A key insight is that the NTK's structure allows the trace to be computed using only one mode of automatic differentiation (either forward or reverse), eliminating the need for both modes and simplifying computation. These one-sided estimators demonstrate superior performance compared to Hutch++ in scenarios with low sample sizes and when there is a significant difference between the model’s state size and parameter count. Overall, the study shows that randomized matrix-free methods can accelerate NTK analysis by several orders of magnitude, facilitating faster experimentation and potential applications in machine learning theory and practice. <div>
arXiv:2511.10796v1 Announce Type: new 
Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions</title>
<link>https://arxiv.org/abs/2511.10809</link>
<guid>https://arxiv.org/abs/2511.10809</guid>
<content:encoded><![CDATA[
<div> Linear Predictive Clustering, Mixed-Integer Programming, Quadratic Pseudo-Boolean Optimization, Global Optimization, Scalability  

<br><br>Summary:  
This paper addresses the problem of Linear Predictive Clustering (LPC), which clusters samples based on linear relationships between features and target variables. Traditional greedy optimization methods used for LPC alternate between clustering and regression but fail to guarantee global optimality and struggle with overlapping (non-separable) clusters. To overcome these limitations, the authors build on a constrained optimization approach initially proposed by Bertsimas and Shioda (2007), who formulated LPC as a Mixed-Integer Program (MIP) that ensures global optimality but is computationally expensive and poorly scalable. The paper proposes two novel improvements to enhance efficiency and scalability of this global optimization paradigm. First, by exploiting theoretical properties of separability, the authors develop near-optimal approximations with provable error bounds, reducing the complexity of the MIP formulation. Second, they approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, offering further computational gains in certain scenarios. Experimental evaluations on synthetic and real-world datasets reveal that the new methods consistently produce near-optimal solutions with significantly lower regression errors than greedy methods, while also being more scalable than existing MIP solutions. These advances enable practical LPC applications in diverse domains such as marketing, medicine, and education. <div>
arXiv:2511.10809v1 Announce Type: new 
Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers know more than they can tell -- Learning the Collatz sequence</title>
<link>https://arxiv.org/abs/2511.10811</link>
<guid>https://arxiv.org/abs/2511.10811</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer models, Collatz sequence, arithmetic functions, loop length prediction, model accuracy<br><br>Summary:<br><br>This article studies the ability of transformer models to predict long steps in the Collatz sequence, a complex arithmetic function defined by specific rules for even and odd integers. The accuracy of the models is dependent on the numerical base used for encoding inputs and outputs, with the highest accuracy (up to 99.7%) achieved using bases 24 and 32, and much lower accuracy (around 25–37%) for bases 3 and 11. Regardless of the base, all models exhibit a consistent learning behavior where they identify classes of inputs sharing the same residue modulo powers of two, achieving near-perfect accuracy on these classes and very low accuracy outside them. This pattern correlates with a known mathematical property: the length of the loops in the Collatz computation can be derived from the binary representation of inputs. The models effectively learn to predict inputs corresponding to increasing loop lengths, which represents the core control structure of this arithmetic problem. Analysis of incorrect predictions shows that errors are mostly systematic, involving misestimation of loop lengths rather than hallucinations or random mistakes. These insights not only clarify the exact algorithms the models have internalized but also suggest that understanding such control structures is key when modeling complex arithmetic functions. The authors propose that their methodology can improve understanding and development of language models more broadly. <div>
arXiv:2511.10811v1 Announce Type: new 
Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Universal Neural Operators through Multiphysics Pretraining</title>
<link>https://arxiv.org/abs/2511.10829</link>
<guid>https://arxiv.org/abs/2511.10829</guid>
<content:encoded><![CDATA[
<div> neural operators, transformer, PDE, transfer learning, fine-tuning<br><br>Summary:<br><br>This paper addresses the high computational cost of training neural operators used in data-driven physical simulations. The authors explore the use of transformer-based neural operators within a general transfer learning framework, expanding their application beyond previously studied specific problems. The study investigates the ability of these models to transfer knowledge by pretraining on simpler PDE problems and fine-tuning on more complex ones. Performance is evaluated across a variety of partial differential equation (PDE) tasks, including challenges such as extrapolating to unseen parameter regimes, integrating additional variables not present during pretraining, and transferring knowledge from datasets involving multiple equations. Results show that transformer-based neural operators exhibit strong transfer capabilities, effectively adapting learned representations to new PDE scenarios and tasks. This suggests that advanced neural operator architectures can facilitate more efficient training workflows and broader applicability in computational physics by leveraging downstream learning techniques. <div>
arXiv:2511.10829v1 Announce Type: new 
Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Quantum Kernels Across Diverse and Complex Data</title>
<link>https://arxiv.org/abs/2511.10831</link>
<guid>https://arxiv.org/abs/2511.10831</guid>
<content:encoded><![CDATA[
<div> Quantum kernel methods, variational quantum kernel, parameter scaling, high-dimensional datasets, real-world machine learning<br><br>Summary:<br><br>This paper addresses the gap in evaluating quantum kernel methods on complex, high-dimensional, real-world datasets, moving beyond prior work focused mainly on low-dimensional or synthetic data. The authors developed a variational quantum kernel framework that employs resource-efficient ansätze tailored for complex classification tasks. They introduced a novel parameter scaling technique aimed at accelerating the convergence of the quantum kernel training process. The framework was benchmarked extensively using eight challenging datasets spanning various data types, including tabular data, images, time series, and graphs. Results were obtained through classical simulation, where the proposed quantum kernel consistently outperformed traditional classical kernels such as the radial basis function (RBF) kernel. These findings suggest that carefully designed quantum kernels can serve as versatile and high-performance tools in machine learning applications. Despite promising results, the paper notes that further research is necessary to conclusively demonstrate practical quantum advantage in real-world scenarios. Overall, the work lays a foundational platform for quantum-enhanced machine learning applications by validating quantum kernel potential across diverse, realistic datasets. <div>
arXiv:2511.10831v1 Announce Type: new 
Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ans\"atze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?</title>
<link>https://arxiv.org/abs/2511.10833</link>
<guid>https://arxiv.org/abs/2511.10833</guid>
<content:encoded><![CDATA[
<div> Equation discovery, symbolic regression, surface benchmarking, large language models, geometry-aware metrics  

<br><br>Summary:  
This paper introduces SurfaceBench, the first comprehensive benchmark designed for symbolic surface discovery, addressing a key challenge in machine learning for science: recovering concise symbolic equations governing complex physical and geometric phenomena. Unlike existing benchmarks that focus largely on scalar functions and use brittle string-matching metrics, SurfaceBench offers 183 tasks covering 15 symbolic complexity categories with explicit, implicit, and parametric equation forms. Each task contains ground-truth formulas, variable semantics, and synthetically generated 3D data, emphasizing surface-level structure and resisting memorization by large language models through novel symbolic compositions. The benchmark draws from real scientific domains including fluid dynamics, robotics, electromagnetics, and geometry, ensuring domain grounding. To evaluate performance, SurfaceBench combines symbolic equation checks with geometry-aware metrics like Chamfer and Hausdorff distances, capturing both algebraic correctness and spatial reconstruction accuracy. Experimental results reveal that state-of-the-art methods struggle to generalize across varied representation types and complexities, performing well only on specific formula families. SurfaceBench thus presents a challenging and diagnostic testbed that facilitates principled benchmarking of advancements in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. The authors provide open-source code for further research and development. <div>
arXiv:2511.10833v1 Announce Type: new 
Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EarthSight: A Distributed Framework for Low-Latency Satellite Intelligence</title>
<link>https://arxiv.org/abs/2511.10834</link>
<guid>https://arxiv.org/abs/2511.10834</guid>
<content:encoded><![CDATA[
<div> Keywords: satellite imagery, low-latency delivery, distributed runtime, multi-task inference, dynamic filter ordering<br><br>Summary:<br><br>1. The paper addresses the challenge of reducing latency in delivering satellite imagery for time-critical applications such as disaster response, intelligence, and infrastructure monitoring, which traditionally experience delays due to bandwidth limitations when downlinking all images before analysis.<br><br>2. To overcome these limitations, the authors propose EarthSight, a distributed runtime framework that redefines satellite image intelligence as a joint decision problem between satellites in orbit and ground stations.<br><br>3. EarthSight introduces three main innovations: (a) multi-task inference on satellites using shared neural network backbones to efficiently handle multiple vision tasks simultaneously, reducing redundant computation; (b) a ground-station query scheduler that aggregates user requests, predicts image priorities, and allocates compute resources accordingly; and (c) dynamic filter ordering, which balances model selectivity, accuracy, and computational cost to discard low-value images early and conserve onboard resources.<br><br>4. This framework leverages global context from ground stations and adaptive, resource-aware decisions onboard satellites, allowing constellation-wide scalable and low-latency image analysis within strict bandwidth and power constraints.<br><br>5. Evaluations using a satellite simulator demonstrate EarthSight reduces average compute time per image by 1.9x and cuts the 90th percentile latency from first contact to image delivery from 51 minutes to 21 minutes compared to existing baselines, significantly enhancing responsiveness and mission scope. <div>
arXiv:2511.10834v1 Announce Type: new 
Abstract: Low-latency delivery of satellite imagery is essential for time-critical applications such as disaster response, intelligence, and infrastructure monitoring. However, traditional pipelines rely on downlinking all captured images before analysis, introducing delays of hours to days due to restricted communication bandwidth. To address these bottlenecks, emerging systems perform onboard machine learning to prioritize which images to transmit. However, these solutions typically treat each satellite as an isolated compute node, limiting scalability and efficiency. Redundant inference across satellites and tasks further strains onboard power and compute costs, constraining mission scope and responsiveness. We present EarthSight, a distributed runtime framework that redefines satellite image intelligence as a distributed decision problem between orbit and ground. EarthSight introduces three core innovations: (1) multi-task inference on satellites using shared backbones to amortize computation across multiple vision tasks; (2) a ground-station query scheduler that aggregates user requests, predicts priorities, and assigns compute budgets to incoming imagery; and (3) dynamic filter ordering, which integrates model selectivity, accuracy, and execution cost to reject low-value images early and conserve resources. EarthSight leverages global context from ground stations and resource-aware adaptive decisions in orbit to enable constellations to perform scalable, low-latency image analysis within strict downlink bandwidth and onboard power budgets. Evaluations using a prior established satellite simulator show that EarthSight reduces average compute time per image by 1.9x and lowers 90th percentile end-to-end latency from first contact to delivery from 51 to 21 minutes compared to the state-of-the-art baseline.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns</title>
<link>https://arxiv.org/abs/2511.10837</link>
<guid>https://arxiv.org/abs/2511.10837</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, hallucination detection, attention-based uncertainty, extrinsic hallucinations, intrinsic hallucinations<br><br>Summary: This paper addresses the challenge of hallucinations in Large Language Models (LLMs), especially in safety-critical applications. It identifies two distinct types of hallucinations: extrinsic, where the model generates information not supported by the input, and intrinsic, where the model distorts or fabricates details within the given input. The authors introduce a principled evaluation framework that clearly differentiates these hallucination types and tests detection methods on targeted benchmark datasets. They investigate existing confidence representation techniques, noting that sampling-based methods such as Semantic Entropy perform well for identifying extrinsic hallucinations but are ineffective for intrinsic ones. To overcome these limitations, the paper proposes novel attention aggregation strategies based on a recent attention-based uncertainty quantification algorithm. These strategies not only improve hallucination detection performance, especially for intrinsic hallucinations, but also enhance interpretability by leveraging attention signals as a rich source of uncertainty information. The findings emphasize that aligning detection methods to the specific nature of hallucination improves effectiveness, suggesting attention mechanisms are valuable for uncertainty estimation in LLM outputs. This work offers new directions for creating more reliable and explainable hallucination detection tools for critical LLM deployments. <div>
arXiv:2511.10837v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification</title>
<link>https://arxiv.org/abs/2511.10841</link>
<guid>https://arxiv.org/abs/2511.10841</guid>
<content:encoded><![CDATA[
<div> Keywords: continuous-time dynamics, neural controlled differential equations, invertible neural flow, irregular time series, data-adaptive manifold<br><br>Summary:<br><br>1. The paper addresses the challenge of modeling continuous-time dynamics from sparse and irregularly sampled time series data, a problem relevant in many real-world applications.  
2. Neural controlled differential equations (Neural CDEs) provide a suitable framework, but their effectiveness depends heavily on how the control path is constructed from discrete observations.  
3. Traditional approaches use fixed interpolation methods to construct the control path, which often fail to accurately capture the underlying data geometry, especially when data is highly missing.  
4. The authors propose FlowPath, a novel method that learns the geometry of the control path through an invertible neural flow, creating a continuous, data-adaptive manifold for the control path instead of simple interpolations.  
5. The invertibility constraint of FlowPath ensures information preservation and well-behaved transformations, differentiating it from prior learnable path models that lack such constraints.  
6. Experimental results on 18 benchmark datasets and one real-world case study show that FlowPath consistently improves classification accuracy compared to fixed interpolation baselines and non-invertible models.  
7. This work emphasizes the significance of modeling the geometry of the control path, alongside the dynamics evolving on it, for robust and generalizable learning from irregular time series data. <div>
arXiv:2511.10841v1 Announce Type: new 
Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10843</link>
<guid>https://arxiv.org/abs/2511.10843</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, off-policy evaluation, behaviour policy, variance reduction, policy-gradient methods<br><br>Summary:<br>1. This paper addresses the issue of high variance in return estimates, which leads to poor sample efficiency and instability in many reinforcement learning (RL) algorithms that rely on these estimates for policy improvement.  <br>2. It builds on recent findings in off-policy evaluation demonstrating that well-designed behaviour policies, used to collect off-policy data, can produce provably lower variance return estimates compared to on-policy data collection, a surprising and counterintuitive result. <br>3. The key insight is extended to an online RL setting, where policy evaluation and policy improvement occur simultaneously to learn optimal policies. <br>4. While traditional off-policy RL approaches focus on reconciling data collected asynchronously from multiple workers (like in IMPALA), here the authors consider a single worker setup, emphasizing the role of the behaviour policy in data collection for variance reduction. <br>5. Empirical results show that integrating this variance reduction regime into two policy-gradient methods leads to improved sample efficiency and overall performance across diverse environments, validating the theoretical claims. <div>
arXiv:2511.10843v1 Announce Type: new 
Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAMP: Spatial-Temporal Adapter with Multi-Head Pooling</title>
<link>https://arxiv.org/abs/2511.10848</link>
<guid>https://arxiv.org/abs/2511.10848</guid>
<content:encoded><![CDATA[
<div> Time series, foundation models, EEG, spatial-temporal adapter, multi-head pooling<br><br>Summary:<br><br>This paper addresses the performance comparison between EEG-specific foundation models (EEGFMs) and general time series foundation models (TSFMs) on EEG-related tasks. It introduces a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), designed to enhance general TSFMs by explicitly modeling the spatial-temporal characteristics of EEG data. STAMP leverages univariate embeddings generated by general TSFMs, adapting these representations to better capture EEG-specific features. The proposed adapter achieves performance comparable to current state-of-the-art EEGFMs. A thorough evaluation is conducted on eight benchmark datasets focused on clinical EEG classification tasks, providing robust experimental validation. Ablation studies additionally highlight the contributions of different components of the STAMP design. Importantly, the adapter architecture is lightweight with fewer trainable parameters, making it computationally efficient. It also offers flexibility by supporting various input configurations, facilitating easy integration of EEG data into general TSFMs without requiring full retraining of specialized EEG models. Overall, the work presents a promising approach to leverage general foundation models for EEG-specific tasks by applying targeted spatial-temporal adaptations, expanding the potential use of TSFMs in neuroscience and clinical applications. <div>
arXiv:2511.10848v1 Announce Type: new 
Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries</title>
<link>https://arxiv.org/abs/2511.10855</link>
<guid>https://arxiv.org/abs/2511.10855</guid>
<content:encoded><![CDATA[
<div> code generation, code selection, large language models, pairwise queries, ExPairT-LLM<br><br>Summary: Despite significant progress in large language models (LLMs), generating correct code remains challenging. To address this, code selection algorithms pick the best program from multiple LLM-generated candidates but often struggle due to incorrectly distinguishing nonequivalent programs or over-relying on the LLM's ability to perfectly predict outputs. The paper introduces ExPairT-LLM, an exact learning algorithm that improves code selection by using two novel types of queries posed to an LLM oracle: pairwise membership and pairwise equivalence queries. These queries are simpler for LLMs to handle, allowing ExPairT-LLM to conduct a tournament-style selection that is robust even when the LLM makes occasional mistakes. Experiments conducted on four popular code datasets demonstrate that ExPairT-LLM outperforms existing state-of-the-art code selection algorithms, improving the top-1 pass rate (pass@1) by an average of 13.0% and up to 27.1%. Furthermore, it substantially enhances the pass@1 rate for LLMs engaged in complex reasoning tasks, yielding an improvement of 24.0%. This approach represents a significant advancement in leveraging LLMs for reliable and accurate code generation. <div>
arXiv:2511.10855v1 Announce Type: new 
Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Zeroth-Order Optimization with Public Data</title>
<link>https://arxiv.org/abs/2511.10859</link>
<guid>https://arxiv.org/abs/2511.10859</guid>
<content:encoded><![CDATA[
<div> Differential Privacy, Zeroth-Order Optimization, Public Data, Gradient Approximation, Privacy-Utility Tradeoff

<br><br>Summary: This paper addresses the computational and memory inefficiencies of first-order differentially private (DP) machine learning methods like DP-SGD, which hinder their practical deployment despite existing optimizations. It highlights the potential of zeroth-order methods, which approximate gradients using function evaluations and are easier to privatize, as a solution to reduce these overheads. However, prior zeroth-order approaches have shown lower utility than DP-SGD and have been tested in limited domains. To improve upon this, the authors propose a framework called Public-Data-Assisted Zeroth-Order optimizers (PAZO), which leverages publicly available data to guide and enhance gradient approximations in private zeroth-order algorithms while incurring minimal additional cost. They provide theoretical analysis assuming similarity between public and private datasets to justify the approach. Empirically, PAZO is shown to achieve better privacy-utility trade-offs on vision and text tasks, in both pre-training and fine-tuning scenarios. It outperforms the best first-order methods that also use public data, especially when privacy requirements are stringent, while offering up to 16 times faster runtime. This work suggests that combining public data with zeroth-order techniques significantly advances the efficiency and effectiveness of DP machine learning. <div>
arXiv:2511.10859v1 Announce Type: new 
Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go</title>
<link>https://arxiv.org/abs/2511.10868</link>
<guid>https://arxiv.org/abs/2511.10868</guid>
<content:encoded><![CDATA[
<div> Keywords: code LLMs, data imbalance, Golang, unit test generation, GO UT Bench

<br><br>Summary:  
Training data imbalance is a significant challenge for code large language models (LLMs), with existing datasets predominantly composed of raw open-source code, leading to poor performance on broader software engineering tasks. Low-resource languages such as Golang are especially underrepresented, causing models to excel at tasks like code autocompletion but struggle with practical developer workflows like unit test generation. To address this, the authors introduce GO UT Bench, a benchmark dataset consisting of 5,264 pairs of Golang code and corresponding unit tests, collected from 10 permissively licensed repositories covering diverse domains. The dataset aims to bridge the gap in training resources for realistic software engineering tasks. The authors evaluate GO UT Bench by fine-tuning two families of LLM architectures—mixture of experts and dense decoders—to assess its effectiveness. Experimental results demonstrate that fine-tuned models consistently outperform their base versions on more than 75% of the benchmark tasks, confirming the dataset's utility in enhancing code LLM performance on unit test generation for Golang. This work highlights the importance of balanced training data that includes real-world developer tasks to improve code model applicability beyond code completion. <div>
arXiv:2511.10868v1 Announce Type: new 
Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Spatial Information into Goal-Conditioned Hierarchical Reinforcement Learning via Graph Representations</title>
<link>https://arxiv.org/abs/2511.10872</link>
<guid>https://arxiv.org/abs/2511.10872</guid>
<content:encoded><![CDATA[
<div> arXiv, Hierarchical Reinforcement Learning, Graph Encoder-Decoder, Subgoal Representation, Intrinsic Rewards  

<br><br>Summary:  
This paper addresses the integration of graphs into Goal-conditioned Hierarchical Reinforcement Learning (GCHRL), highlighting the limitation of existing methods which depend heavily on domain-specific knowledge to construct graphs, thus restricting their adaptability to new tasks. It also points out the challenges faced by dynamic graph construction approaches during exploration, particularly in effectively transmitting graph information to newly encountered states. The authors propose a novel method called Graph-Guided sub-Goal representation Generation RL (G4RL), which leverages a graph encoder-decoder trained on the state graph generated during exploration to evaluate unseen states. G4RL is designed to integrate easily with existing GCHRL frameworks, especially in environments characterized by symmetric and reversible transitions. By incorporating G4RL, the approach enhances subgoal representation and improves sample efficiency. Experimental results demonstrate that utilizing both high-level and low-level intrinsic rewards derived from the graph encoder-decoder leads to significant performance gains in both dense and sparse reward environments. Importantly, these improvements come with minimal additional computational cost, making G4RL a practical enhancement for state-of-the-art GCHRL methods. <div>
arXiv:2511.10872v1 Announce Type: new 
Abstract: The integration of graphs with Goal-conditioned Hierarchical Reinforcement Learning (GCHRL) has recently gained attention, as intermediate goals (subgoals) can be effectively sampled from graphs that naturally represent the overall task structure in most RL tasks. However, existing approaches typically rely on domain-specific knowledge to construct these graphs, limiting their applicability to new tasks. Other graph-based approaches create graphs dynamically during exploration but struggle to fully utilize them, because they have problems passing the information in the graphs to newly visited states. Additionally, current GCHRL methods face challenges such as sample inefficiency and poor subgoal representation. This paper proposes a solution to these issues by developing a graph encoder-decoder to evaluate unseen states. Our proposed method, Graph-Guided sub-Goal representation Generation RL (G4RL), can be incorporated into any existing GCHRL method when operating in environments with primarily symmetric and reversible transitions to enhance performance across this class of problems. We show that the graph encoder-decoder can be effectively implemented using a network trained on the state graph generated during exploration. Empirical results indicate that leveraging high and low-level intrinsic rewards from the graph encoder-decoder significantly enhances the performance of state-of-the-art GCHRL approaches with an extra small computational cost in dense and sparse reward environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics</title>
<link>https://arxiv.org/abs/2511.10878</link>
<guid>https://arxiv.org/abs/2511.10878</guid>
<content:encoded><![CDATA[
<div> Keywords: muscle activations, multi-joint systems, physics-informed deep learning, cross-attention, BiGRU  

<br><br>Summary:  
This paper addresses the challenge of time-efficient estimation of muscle activations and forces in multi-joint systems, which is vital for clinical assessment and assistive device control but is often hindered by computational expense and lack of high-quality labeled datasets. The authors propose a novel physics-informed deep learning framework, the PI-MJCA-BiGRU, that estimates muscle activations and forces directly from kinematic data without relying on ground-truth labels. Central to this framework is the Multi-Joint Cross-Attention (MJCA) module combined with Bidirectional Gated Recurrent Unit (BiGRU) layers, which effectively capture inter-joint coordination by allowing each joint to adaptively integrate motion information from others. The framework incorporates multi-joint dynamics, inter-joint coupling, and external force interactions directly into the loss function to ensure physiologically consistent predictions. Experimental validation on two datasets demonstrates that the PI-MJCA-BiGRU framework achieves performance comparable to conventional supervised methods, despite operating without labeled data. Additionally, the MJCA module significantly improves the modeling of inter-joint coordination compared to standard baseline architectures. Overall, this approach offers a time-efficient, label-free solution for accurately estimating muscle activations and forces in complex multi-joint systems. <div>
arXiv:2511.10878v1 Announce Type: new 
Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-View Polymer Representations for the Open Polymer Prediction</title>
<link>https://arxiv.org/abs/2511.10893</link>
<guid>https://arxiv.org/abs/2511.10893</guid>
<content:encoded><![CDATA[
<div> polymer property prediction, multi-view design, graph neural networks, SMILES language models, Open Polymer Prediction Challenge<br><br>Summary:<br><br>This paper addresses the challenge of predicting polymer properties using a multi-view approach that leverages complementary molecular representations. The system integrates four distinct families of models: (i) tabular RDKit/Morgan descriptors that capture traditional chemical descriptors, (ii) graph neural networks that model molecular connectivity, (iii) 3D-informed representations encoding spatial conformation, and (iv) pretrained SMILES language models that learn from sequential molecular data. By training these models using 10-fold cross-validation splits, the approach ensures robust learning and generalization. The ensemble combines per-property predictions through a uniform averaging method, enhancing performance by aggregating diverse insights from the four model families. Additionally, test-time augmentation is applied by generating multiple SMILES strings per polymer to improve prediction stability. The resulting method was evaluated in the Open Polymer Prediction Challenge at NeurIPS 2025, where it ranked 9th out of 2,241 participating teams. The ensemble achieved a mean absolute error (MAE) of 0.057 on the public leaderboard and 0.082 on the private leaderboard, demonstrating strong predictive accuracy and competitive performance against a large pool of competitors. This work highlights the effectiveness of combining multiple molecular representation techniques within an ensemble framework for polymer property prediction tasks. <div>
arXiv:2511.10893v1 Announce Type: new 
Abstract: We address polymer property prediction with a multi-view design that exploits complementary representations. Our system integrates four families: (i) tabular RDKit/Morgan descriptors, (ii) graph neural networks, (iii) 3D-informed representations, and (iv) pretrained SMILES language models, and averages per-property predictions via a uniform ensemble. Models are trained with 10-fold splits and evaluated with SMILES test-time augmentation. The approach ranks 9th of 2241 teams in the Open Polymer Prediction Challenge at NeurIPS 2025. The submitted ensemble achieves a public MAE of 0.057 and a private MAE of 0.082.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Attention Network for Predicting Duration of Large-Scale Power Outages Induced by Natural Disasters</title>
<link>https://arxiv.org/abs/2511.10898</link>
<guid>https://arxiv.org/abs/2511.10898</guid>
<content:encoded><![CDATA[
<div> Keywords: power outages, Graph Attention Networks, spatial heterogeneity, weather-induced, machine learning<br><br>Summary:<br><br>1. The study addresses the significant issue of large-scale power outages in the U.S. caused by natural disasters such as hurricanes, wildfires, and winter storms, highlighting their severe economic and societal impacts.  
2. Accurately predicting power outage recovery times is essential to enhancing the resilience of the power grid.  
3. While machine learning methods have been previously applied to estimate outage durations using geospatial and weather data, three key challenges remain: spatial dependency between data points, spatial heterogeneity of outage impacts, and moderate availability of event data.  
4. The authors propose a novel solution based on Graph Attention Networks (GAT), which leverages a simple network structure enhanced by unsupervised pre-training followed by semi-supervised learning to effectively capture spatial information.  
5. The model was trained and tested on field data from four major hurricanes impacting 501 counties across eight Southeastern U.S. states. It demonstrated excellent performance with over 93% accuracy, outperforming established methods such as XGBoost, Random Forest, Graph Convolutional Networks (GCN), and simpler GAT variants by 2% to 15% in both overall and class-wise accuracy metrics. <div>
arXiv:2511.10898v1 Announce Type: new 
Abstract: Natural disasters such as hurricanes, wildfires, and winter storms have induced large-scale power outages in the U.S., resulting in tremendous economic and societal impacts. Accurately predicting power outage recovery and impact is key to resilience of power grid. Recent advances in machine learning offer viable frameworks for estimating power outage duration from geospatial and weather data. However, three major challenges are inherent to the task in a real world setting: spatial dependency of the data, spatial heterogeneity of the impact, and moderate event data. We propose a novel approach to estimate the duration of severe weather-induced power outages through Graph Attention Networks (GAT). Our network uses a simple structure from unsupervised pre-training, followed by semi-supervised learning. We use field data from four major hurricanes affecting $501$ counties in eight Southeastern U.S. states. The model exhibits an excellent performance ($>93\%$ accuracy) and outperforms the existing methods XGBoost, Random Forest, GCN and simple GAT by $2\% - 15\%$ in both the overall performance and class-wise accuracy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework</title>
<link>https://arxiv.org/abs/2511.10915</link>
<guid>https://arxiv.org/abs/2511.10915</guid>
<content:encoded><![CDATA[
<div> Federated clustering, privacy-preserving, structural graphs, knowledge sharing, graph alignment  

<br><br>Summary:  
The paper addresses challenges in federated clustering, specifically balancing data privacy with model performance, noting current methods either risk sensitive data leakage by transmitting embeddings or suffer accuracy loss by sharing only abstract cluster prototypes. To overcome this, the authors propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), which uses local structural graphs as a privacy-friendly medium for knowledge sharing, moving beyond conventional embedding or prototype exchange. The framework follows a client-server model where clients build private structural graphs that capture intrinsic data relationships; these graphs are securely aggregated and aligned by the server to form a global graph for unified clustering. SPP-FGC supports two modes: a one-shot method for rapid single-round communication and an iterative version, SPP-FGC+, suited for complex data like images, where feature representations are collaboratively refined to improve performance. Extensive experiments demonstrate that SPP-FGC improves clustering accuracy by up to 10% in normalized mutual information (NMI) compared to federated baselines while ensuring provable privacy guarantees. This approach effectively enhances federated clustering by safeguarding user privacy without sacrificing model quality, making it suitable for decentralized, unlabeled data analysis. <div>
arXiv:2511.10915v1 Announce Type: new 
Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphToxin: Reconstructing Full Unlearned Graphs from Graph Unlearning</title>
<link>https://arxiv.org/abs/2511.10936</link>
<guid>https://arxiv.org/abs/2511.10936</guid>
<content:encoded><![CDATA[
<div> graph unlearning, privacy, graph neural networks, graph reconstruction attack, defense mechanisms<br><br>Summary:<br><br>1. The paper addresses graph unlearning as a method to comply with "the right to be forgotten" by enabling the removal of sensitive data from graph neural networks, but highlights that it is vulnerable to attacks.<br><br>2. The authors introduce GraphToxin, the first graph reconstruction attack designed specifically to recover deleted data from supposedly unlearned graph neural networks by using a curvature matching module for detailed recovery.<br><br>3. GraphToxin can retrieve not only the erased individual’s information and personal connections but also sensitive content from their associated nodes, posing significant privacy risks beyond initial expectations.<br><br>4. The attack is extended to scenarios involving multiple node removals and is evaluated under both white-box and black-box settings, with a novel evaluation framework that assesses performance in random and worst-case removal scenarios.<br><br>5. Experimental results show GraphToxin’s high effectiveness and adaptability, while current defense methods fail to stop the attack and sometimes even enhance its success, emphasizing the urgent need for stronger and more reliable defenses against such graph reconstruction threats. <div>
arXiv:2511.10936v1 Announce Type: new 
Abstract: Graph unlearning has emerged as a promising solution for complying with "the right to be forgotten" regulations by enabling the removal of sensitive information upon request. However, this solution is not foolproof. The involvement of multiple parties creates new attack surfaces, and residual traces of deleted data can still remain in the unlearned graph neural networks. These vulnerabilities can be exploited by attackers to recover the supposedly erased samples, thereby undermining the inherent functionality of graph unlearning. In this work, we propose GraphToxin, the first graph reconstruction attack against graph unlearning. Specifically, we introduce a novel curvature matching module to provide a fine-grained guidance for full unlearned graph recovery. We demonstrate that GraphToxin can successfully subvert the regulatory guarantees expected from graph unlearning - it can recover not only a deleted individual's information and personal links but also sensitive content from their connections, thereby posing substantially more detrimental threats. Furthermore, we extend GraphToxin to multiple node removals under both white-box and black-box setting. We highlight the necessity of a worst-case analysis and propose a comprehensive evaluation framework to systematically assess the attack performance under both random and worst-case node removals. This provides a more robust and realistic measure of the vulnerability of graph unlearning methods to graph reconstruction attacks. Our extensive experiments demonstrate the effectiveness and flexibility of GraphToxin. Notably, we show that existing defense mechanisms are largely ineffective against this attack and, in some cases, can even amplify its performance. Given the severe privacy risks posed by GraphToxin, our work underscores the urgent need for the development of more effective and robust defense strategies against this attack.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cascading Bandits With Feedback</title>
<link>https://arxiv.org/abs/2511.10938</link>
<guid>https://arxiv.org/abs/2511.10938</guid>
<content:encoded><![CDATA[
<div> cascade bandit, edge inference, regret analysis, decision-making policies, adaptivity<br><br>Summary:<br><br>This paper studies a variant of the cascade bandit model tailored for edge inference scenarios where each arm represents an inference model characterized by its accuracy and error probability. Four decision-making policies—Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling—are analyzed to evaluate performance in terms of regret. It is shown that the traditional policies Explore-then-Commit and Action Elimination perform suboptimally because they fix their action ordering after an initial exploration phase, reducing their ability to adapt to new information. In contrast, adaptive algorithms such as LCB and Thompson Sampling continually update their action choices based on ongoing feedback from the environment, enabling them to achieve a constant O(1) regret bound. This demonstrates that continuous adaptivity is essential for efficient decision-making in uncertain edge inference settings. The theoretical analysis is supported by simulation results which confirm the superiority of adaptive policies over non-adaptive ones in minimizing regret under uncertainty. The findings highlight the importance of policy design that dynamically leverages feedback to optimize inference model selection at the edge. <div>
arXiv:2511.10938v1 Announce Type: new 
Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow matching-based generative models for MIMO channel estimation</title>
<link>https://arxiv.org/abs/2511.10941</link>
<guid>https://arxiv.org/abs/2511.10941</guid>
<content:encoded><![CDATA[
<div> Diffusion model, flow matching, MIMO, channel estimation, velocity field<br><br>Summary:<br><br>This paper addresses the challenge of slow sampling speed in diffusion model (DM)-based channel estimation for MIMO systems, which is used to acquire high-precision channel state information (CSI) via stepwise posterior sampling and denoising. To overcome this limitation, the authors propose a novel generative model based on flow matching (FM) techniques. They first reformulate the channel estimation problem under the FM framework by defining a conditional probability path that smoothly transitions from noisy channel data to the true channel distribution along a straight-line trajectory at constant speed. Using this formulation, they derive a velocity field dependent only on noise statistics, which guides the training of the generative model. During sampling, this trained velocity field acts as prior knowledge, enabling rapid and reliable noise channel enhancement through an ordinary differential equation (ODE) Euler solver. Numerical experiments demonstrate that the FM-based channel estimation significantly reduces sampling overhead compared to existing score matching (SM)-based DM methods. Additionally, the proposed method achieves superior channel estimation accuracy under various channel conditions, indicating its potential for efficient and precise CSI acquisition in practical MIMO communication systems. <div>
arXiv:2511.10941v1 Announce Type: new 
Abstract: Diffusion model (DM)-based channel estimation, which generates channel samples via a posteriori sampling stepwise with denoising process, has shown potential in high-precision channel state information (CSI) acquisition. However, slow sampling speed is an essential challenge for recent developed DM-based schemes. To alleviate this problem, we propose a novel flow matching (FM)-based generative model for multiple-input multiple-output (MIMO) channel estimation. We first formulate the channel estimation problem within FM framework, where the conditional probability path is constructed from the noisy channel distribution to the true channel distribution. In this case, the path evolves along the straight-line trajectory at a constant speed. Then, guided by this, we derive the velocity field that depends solely on the noise statistics to guide generative models training. Furthermore, during the sampling phase, we utilize the trained velocity field as prior information for channel estimation, which allows for quick and reliable noise channel enhancement via ordinary differential equation (ODE) Euler solver. Finally, numerical results demonstrate that the proposed FM-based channel estimation scheme can significantly reduce the sampling overhead compared to other popular DM-based schemes, such as the score matching (SM)-based scheme. Meanwhile, it achieves superior channel estimation accuracy under different channel conditions.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging</title>
<link>https://arxiv.org/abs/2511.10943</link>
<guid>https://arxiv.org/abs/2511.10943</guid>
<content:encoded><![CDATA[
<div> Keywords: model merging, multitask performance, parameter interference, linear transformation, Pareto-optimal  

<br><br>Summary:  
1. The paper addresses challenges in model merging, where combining expert models for multitask performance is hindered by parameter interference.  
2. Current controllable model merging techniques rely on a compile-then-query paradigm involving costly offline multi-objective optimization, which becomes exponentially complex as the number of tasks increases.  
3. The authors propose a novel approach that bypasses parameter-space optimization by directly correcting the model’s final representation through an optimal linear transformation.  
4. This method provides a closed-form, architecture-agnostic solution that eliminates the need for iterative search or additional training, reducing the complexity to scale linearly with the number of tasks.  
5. Experimental results demonstrate that the approach produces a superior Pareto front, offering more accurate alignment with user preferences and a significant reduction in computational cost, enabling on-the-fly generation of Pareto-optimal models. <div>
arXiv:2511.10943v1 Announce Type: new 
Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Data Quality Affects Machine Learning Models for Credit Risk Assessment</title>
<link>https://arxiv.org/abs/2511.10964</link>
<guid>https://arxiv.org/abs/2511.10964</guid>
<content:encoded><![CDATA[
<div> Keywords: Machine Learning, credit risk, data quality, robustness, data corruption  

<br><br>Summary:  
This paper examines the influence of data quality issues on the performance of machine learning models used for credit risk evaluation. It focuses on common data problems such as missing values, noisy features, outliers, and incorrect labels, which can degrade predictive accuracy. Using an open-source credit risk dataset, the authors apply controlled data corruptions via the Pucktrick library to simulate these quality issues systematically. They evaluate the robustness of ten widely used models, including Random Forest, Support Vector Machines (SVM), and Logistic Regression, under varying levels and types of data contamination. Experimental results reveal notable differences in model resilience depending on both the severity and nature of data degradation, highlighting that some models maintain higher predictive accuracy despite corrupted input data. The study offers a practical methodology and associated tools that help practitioners strengthen data processing pipelines against quality-related breakdowns. Additionally, the framework provides a flexible experimental environment for researchers aiming to explore data-centric AI challenges and advance robust machine learning applications within credit risk assessment and beyond. <div>
arXiv:2511.10964v1 Announce Type: new 
Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm</title>
<link>https://arxiv.org/abs/2511.11009</link>
<guid>https://arxiv.org/abs/2511.11009</guid>
<content:encoded><![CDATA[
<div> Unsupervised Domain Adaptation, Adversarial Robustness, VAT, URDA, DART<br><br>Summary:  
This paper addresses the challenge of improving adversarial robustness in unsupervised domain adaptation (UDA), where traditional UDA methods focus primarily on transferability but neglect defense against adversarial attacks. It investigates why vanilla adversarial training (VAT), despite its success in standard deep learning, fails to enhance robustness in the UDA setting. The authors develop a novel theoretical framework by deriving a new generalization bound that integrates adversarial noise resistance with domain shift adaptation, extending classical UDA theory. To overcome inherent difficulties in merging VAT with UDA, they propose a new paradigm named Unsupervised Robust Domain Adaptation (URDA). Building upon this, the paper introduces the Disentangled Adversarial Robustness Training (DART) algorithm, a simple yet effective two-step training procedure. DART first pre-trains any UDA model and subsequently applies a robustification post-training step using disentangled distillation to ensure both transferability and adversarial robustness. Experimental results on four benchmark datasets, both with and without adversarial attacks, demonstrate that DART significantly improves robustness while preserving domain adaptation performance. This work is the first to establish the URDA paradigm and provide accompanying theoretical guarantees, contributing a practical and theoretically grounded approach to robust domain adaptation. <div>
arXiv:2511.11009v1 Announce Type: new 
Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Graph Representations with Neighborhood-Contextualized Message-Passing</title>
<link>https://arxiv.org/abs/2511.11046</link>
<guid>https://arxiv.org/abs/2511.11046</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Message-Passing, Neighborhood-Contextualization, Attention Mechanism, SINC-GCN<br><br>Summary:<br><br>1. The paper addresses a limitation in classical message-passing Graph Neural Networks (GNNs), which typically focus on pairwise interactions between a central node and its neighbors, neglecting rich contextual information from the broader local neighborhood.<br>2. It formalizes the concept of neighborhood-contextualization, inspired by the attentional GNN variant, aiming to incorporate wider neighborhood context into message-passing.<br>3. Building on this concept, the authors propose the neighborhood-contextualized message-passing (NCMP) framework, which generalizes existing message-passing GNN architectures.<br>4. To operationalize NCMP, a simple, practical, and efficient parametrization method is introduced, resulting in the Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN).<br>5. Preliminary experiments on a synthetic binary node classification task demonstrate that SINC-GCN improves both expressivity and efficiency, highlighting the potential of NCMP to enhance the representational power of classical GNNs and suggesting a promising direction for future graph learning research. <div>
arXiv:2511.11046v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) have become an indispensable tool for analyzing relational data. In the literature, classical GNNs may be classified into three variants: convolutional, attentional, and message-passing. While the standard message-passing variant is highly expressive, its typical pair-wise messages nevertheless only consider the features of the center node and each neighboring node individually. This design fails to incorporate the rich contextual information contained within the broader local neighborhood, potentially hindering its ability to learn complex relationships within the entire set of neighboring nodes. To address this limitation, this work first formalizes the concept of neighborhood-contextualization, rooted in a key property of the attentional variant. This then serves as the foundation for generalizing the message-passing variant to the proposed neighborhood-contextualized message-passing (NCMP) framework. To demonstrate its utility, a simple, practical, and efficient method to parametrize and operationalize NCMP is presented, leading to the development of the proposed Soft-Isomorphic Neighborhood-Contextualized Graph Convolution Network (SINC-GCN). A preliminary analysis on a synthetic binary node classification problem then underscores both the expressivity and efficiency of the proposed GNN architecture. Overall, the paper lays the foundation for the novel NCMP framework as a practical path toward further enhancing the graph representational power of classical GNNs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Echoless Label-Based Pre-computation for Memory-Efficient Heterogeneous Graph Learning</title>
<link>https://arxiv.org/abs/2511.11081</link>
<guid>https://arxiv.org/abs/2511.11081</guid>
<content:encoded><![CDATA[
<div> Heterogeneous Graph Neural Networks, Pre-computation, Label Leakage, Echoless Propagation, Partitioning Scheme<br><br>Summary:<br><br>Heterogeneous Graph Neural Networks (HGNNs) are effective for learning on heterogeneous graphs but face efficiency challenges due to repetitive message passing during training on large-scale graphs. Pre-computation-based HGNNs improve efficiency by performing message passing once during preprocessing, enabling mini-batch training using regular-shaped tensors. However, label-based pre-computation approaches suffer from training label leakage caused by the echo effect, where a node's own label information loops back during multi-hop message passing. Existing solutions either require high memory or are incompatible with advanced message passing techniques. To address these issues, the paper proposes Echoless Label-based Pre-computation (Echoless-LP), featuring Partition-Focused Echoless Propagation (PFEP), which partitions target nodes and ensures that nodes only collect label information from neighbors in other partitions. This eliminates label echo while retaining memory efficiency and compatibility with any message passing method. Additionally, an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism are introduced to mitigate information loss from partitioning and manage distributional shifts among partitions. Experiments on public datasets demonstrate that Echoless-LP outperforms existing baselines in accuracy while maintaining memory efficiency, making it a practical solution for training HGNNs on large heterogeneous graphs. <div>
arXiv:2511.11081v1 Announce Type: new 
Abstract: Heterogeneous Graph Neural Networks (HGNNs) are widely used for deep learning on heterogeneous graphs. Typical end-to-end HGNNs require repetitive message passing during training, limiting efficiency for large-scale real-world graphs. Pre-computation-based HGNNs address this by performing message passing only once during preprocessing, collecting neighbor information into regular-shaped tensors, which enables efficient mini-batch training. Label-based pre-computation methods collect neighbors' label information but suffer from training label leakage, where a node's own label information propagates back to itself during multi-hop message passing - the echo effect. Existing mitigation strategies are memory-inefficient on large graphs or suffer from compatibility issues with advanced message passing methods. We propose Echoless Label-based Pre-computation (Echoless-LP), which eliminates training label leakage with Partition-Focused Echoless Propagation (PFEP). PFEP partitions target nodes and performs echoless propagation, where nodes in each partition collect label information only from neighbors in other partitions, avoiding echo while remaining memory-efficient and compatible with any message passing method. We also introduce an Asymmetric Partitioning Scheme (APS) and a PostAdjust mechanism to address information loss from partitioning and distributional shifts across partitions. Experiments on public datasets demonstrate that Echoless-LP achieves superior performance and maintains memory efficiency compared to baselines.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Population Training for Zero-Shot Coordination</title>
<link>https://arxiv.org/abs/2511.11083</link>
<guid>https://arxiv.org/abs/2511.11083</guid>
<content:encoded><![CDATA[
<div> Zero-shot coordination, population-based training, Scalable Population Training, meta-agent, mutual information regularizer  

<br><br>Summary:  
Zero-shot coordination (ZSC) is a significant topic in reinforcement learning, emphasizing agents' ability to generalize and effectively coordinate with unseen collaborators without further fine-tuning. Traditional population-based training methods have demonstrated promising results for ZSC but are constrained by computational resources. These methods typically optimize diversity within small populations but fail to leverage potential performance benefits that arise from increasing population size. To overcome these limitations, the paper introduces Scalable Population Training (ScaPT), a novel and efficient framework designed to scale population-based training effectively. ScaPT incorporates two main components: a meta-agent mechanism that efficiently represents a large population by selectively sharing parameters among agents, reducing computational overhead, and a mutual information regularizer that ensures and maintains diversity within the population. The combined approach promotes both scalability and diversity, critical factors for improved zero-shot coordination outcomes. The paper empirically validates ScaPT’s effectiveness in the challenging environment of Hanabi, a cooperative card game frequently used for evaluating coordination in multi-agent systems. Results demonstrate that ScaPT outperforms existing representational frameworks and training methods, confirming its capacity to scale populations while achieving superior zero-shot coordination performance. <div>
arXiv:2511.11083v1 Announce Type: new 
Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sheaf Cohomology of Linear Predictive Coding Networks</title>
<link>https://arxiv.org/abs/2511.11092</link>
<guid>https://arxiv.org/abs/2511.11092</guid>
<content:encoded><![CDATA[
<div> Predictive coding, cellular sheaves, sheaf cohomology, recurrent networks, Hodge decomposition<br><br>Summary: This paper presents a novel perspective on predictive coding (PC) networks by formulating linear PC networks as cellular sheaves, where the sheaf coboundary maps activations to edge-wise prediction errors. It demonstrates that PC inference operates as a diffusion process governed by the sheaf Laplacian. The authors use sheaf cohomology to characterize irreducible error patterns that cannot be resolved through inference, providing insight into the limitations of error correction in these networks. The study focuses on recurrent network topologies, which often create feedback loops that introduce internal contradictions, leading to prediction errors independent of external supervision. By applying a Hodge decomposition, the paper identifies conditions under which these contradictions halt learning, effectively diagnosing when and why learning stalls. The sheaf-theoretic framework not only elucidates these critical issues but also serves as a practical tool for diagnosing problematic network configurations. Furthermore, the approach offers design principles aimed at effective weight initialization strategies to improve learning outcomes in recurrent predictive coding networks. This work bridges abstract mathematical concepts with neural network optimization, fostering new methods for network analysis and design in predictive coding frameworks. <div>
arXiv:2511.11092v1 Announce Type: new 
Abstract: Predictive coding (PC) replaces global backpropagation with local optimization over weights and activations. We show that linear PC networks admit a natural formulation as cellular sheaves: the sheaf coboundary maps activations to edge-wise prediction errors, and PC inference is diffusion under the sheaf Laplacian. Sheaf cohomology then characterizes irreducible error patterns that inference cannot remove. We analyze recurrent topologies where feedback loops create internal contradictions, introducing prediction errors unrelated to supervision. Using a Hodge decomposition, we determine when these contradictions cause learning to stall. The sheaf formalism provides both diagnostic tools for identifying problematic network configurations and design principles for effective weight initialization for recurrent PC networks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMART: A Surrogate Model for Predicting Application Runtime in Dragonfly Systems</title>
<link>https://arxiv.org/abs/2511.11111</link>
<guid>https://arxiv.org/abs/2511.11111</guid>
<content:encoded><![CDATA[
<div> Keywords: Dragonfly network, workload interference, parallel discrete event simulation, graph neural networks, large language models  

<br><br>Summary: The Dragonfly network is recognized for its high-radix and low-diameter topology, making it a top choice for interconnects in high-performance computing environments. A significant challenge within this network structure is managing workload interference on shared network links, which can degrade overall system performance. Parallel discrete event simulation (PDES) serves as a standard technique to analyze these interference patterns; however, high-fidelity PDES demands substantial computational resources, thus limiting its practicality for large-scale and real-time applications. To address this limitation, hybrid simulation approaches incorporating data-driven surrogate models have emerged as viable alternatives. These models are particularly effective for predicting application runtime amidst the dynamic and complex behavior of network traffic. The paper introduces \ourmodel, a novel surrogate model that integrates graph neural networks (GNNs) and large language models (LLMs) to effectively capture both spatial and temporal characteristics from port-level router data. Experimental results demonstrate that \ourmodel surpasses traditional statistical methods and existing machine learning baselines in accuracy. Consequently, \ourmodel facilitates precise runtime prediction, which enhances the efficiency of hybrid simulations tailored for Dragonfly networks, ultimately contributing to better workload management and network performance optimization. <div>
arXiv:2511.11111v1 Announce Type: new 
Abstract: The Dragonfly network, with its high-radix and low-diameter structure, is a leading interconnect in high-performance computing. A major challenge is workload interference on shared network links. Parallel discrete event simulation (PDES) is commonly used to analyze workload interference. However, high-fidelity PDES is computationally expensive, making it impractical for large-scale or real-time scenarios. Hybrid simulation that incorporates data-driven surrogate models offers a promising alternative, especially for forecasting application runtime, a task complicated by the dynamic behavior of network traffic. We present \ourmodel, a surrogate model that combines graph neural networks (GNNs) and large language models (LLMs) to capture both spatial and temporal patterns from port level router data. \ourmodel outperforms existing statistical and machine learning baselines, enabling accurate runtime prediction and supporting efficient hybrid simulation of Dragonfly networks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization</title>
<link>https://arxiv.org/abs/2511.11118</link>
<guid>https://arxiv.org/abs/2511.11118</guid>
<content:encoded><![CDATA[
<div> Knowledge Graph Embeddings, Continual Learning, Embedding Initialization, Catastrophic Forgetting, Knowledge Retention<br><br>Summary:<br><br>This paper addresses the challenge of updating Knowledge Graph Embeddings (KGEs) when Knowledge Graphs (KGs) are frequently modified, requiring the embeddings to adapt continuously. The authors focus on the critical initial step of embedding initialization for new entities within continual learning frameworks, emphasizing its influence on final embedding accuracy and training duration, especially during small, incremental updates. They propose a novel informed embedding initialization strategy that leverages the KG schema and previously learned embeddings, utilizing the classes to which new entities belong to derive their initial representations. This approach integrates seamlessly with existing continual learning methods, improving the acquisition of new knowledge and mitigating catastrophic forgetting of prior information. Extensive experiments demonstrate that this initialization strategy enhances the predictive performance of KGEs and strengthens knowledge retention. Additionally, the method reduces the number of training epochs required, accelerating the incremental learning process and decreasing computational time. Lastly, the strategy’s effectiveness is validated across various types of KGE learning models, underscoring its broad applicability and benefits for continual updating scenarios in Knowledge Graph embeddings. <div>
arXiv:2511.11118v1 Announce Type: new 
Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods</title>
<link>https://arxiv.org/abs/2511.11143</link>
<guid>https://arxiv.org/abs/2511.11143</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, robust statistics, high-dimensional data, bank account balances, computational efficiency<br><br>Summary:<br><br>1. The paper focuses on detecting point anomalies in bank account balances, which is critical for financial institutions to uncover fraud, operational problems, or irregularities.<br><br>2. Robust statistical methods are employed to identify outliers and estimate distribution parameters that remain reliable despite contaminated observations.<br><br>3. Traditional robust approaches face challenges in high-dimensional settings, often resulting in computational inefficiency and decreased effectiveness.<br><br>4. The authors propose multiple robust techniques designed to be computationally efficient while maintaining high breakdown points, suitable for medium and high-dimensional datasets.<br><br>5. The methods are empirically evaluated using a large-scale dataset containing approximately 2.6 million daily records of anonymous users' bank account balances, demonstrating their practicality and performance for anomaly detection in realistic financial data scenarios. <div>
arXiv:2511.11143v1 Announce Type: new 
Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI</title>
<link>https://arxiv.org/abs/2511.11152</link>
<guid>https://arxiv.org/abs/2511.11152</guid>
<content:encoded><![CDATA[
<div> precipitation forecasting, deep learning, interpretability, ConvLSTM, Indian cities

<br><br>Summary:  
This study presents an interpretable deep learning framework tailored for short-term precipitation forecasting in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, which represent diverse climatic zones. The framework employs a hybrid Time-Distributed CNN-ConvLSTM architecture trained on multi-decadal ERA5 reanalysis data, allowing the model to capture spatiotemporal precipitation patterns effectively. Model optimization was city-specific, varying the number of convolutional filters to 32 for Bengaluru, 64 for Mumbai and Delhi, and 128 for Kolkata, thereby enhancing prediction accuracy. The models achieved respective RMSE values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata), indicating good performance across different urban environments. To promote transparency and facilitate real-world application, various explainability techniques were employed, including permutation importance, Grad-CAM, temporal occlusion, and counterfactual perturbation. These analyses revealed that the models leveraged distinct, city-specific variable patterns and that effective prediction horizons varied by location, from one day in Bengaluru to up to five days in Kolkata. Overall, this research demonstrates the potential of combining explainable AI methods with deep learning to deliver both accurate and interpretable precipitation forecasts customized for diverse urban climates. <div>
arXiv:2511.11152v1 Announce Type: new 
Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Symmetrization of the KL Divergence</title>
<link>https://arxiv.org/abs/2511.11159</link>
<guid>https://arxiv.org/abs/2511.11159</guid>
<content:encoded><![CDATA[
<div> Keywords: Jeffreys divergence, normalizing flows, energy-based models, symmetric divergence, constrained optimization<br><br>Summary:<br><br>This paper addresses the challenge of learning probability distributions from finite samples by focusing on minimizing the symmetric Jeffreys divergence, which combines forward and reverse KL divergences but is difficult to compute directly from samples. The authors propose a novel approach involving a proxy model designed not only to fit the data but also to assist in optimizing the Jeffreys divergence of the main model. This joint training is formulated as a constrained optimization problem, enabling the adaptive balancing of priorities between the main and proxy models during training. By doing so, the method avoids brittle min-max adversarial formulations and leverages the strengths of both normalizing flows (NFs) and energy-based models (EBMs). The approach is demonstrated in practical applications including density estimation, image generation, and simulation-based inference, showing how it can combine the tractability of forward KL methods and the more symmetric properties of the Jeffreys divergence to better capture target distribution characteristics. This work contributes a practical algorithmic framework for minimizing a challenging symmetric divergence measure and opens avenues for improved model fitting in various machine learning tasks. <div>
arXiv:2511.11159v1 Announce Type: new 
Abstract: Many tasks in machine learning can be described as or reduced to learning a probability distribution given a finite set of samples. A common approach is to minimize a statistical divergence between the (empirical) data distribution and a parameterized distribution, e.g., a normalizing flow (NF) or an energy-based model (EBM). In this context, the forward KL divergence is a ubiquitous due to its tractability, though its asymmetry may prevent capturing some properties of the target distribution. Symmetric alternatives involve brittle min-max formulations and adversarial training (e.g., generative adversarial networks) or evaluating the reverse KL divergence, as is the case for the symmetric Jeffreys divergence, which is challenging to compute from samples. This work sets out to develop a new approach to minimize the Jeffreys divergence. To do so, it uses a proxy model whose goal is not only to fit the data, but also to assist in optimizing the Jeffreys divergence of the main model. This joint training task is formulated as a constrained optimization problem to obtain a practical algorithm that adapts the models priorities throughout training. We illustrate how this framework can be used to combine the advantages of NFs and EBMs in tasks such as density estimation, image generation, and simulation-based inference.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Neural Networks at Any Scale</title>
<link>https://arxiv.org/abs/2511.11163</link>
<guid>https://arxiv.org/abs/2511.11163</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, optimization methods, efficiency, scale adaptation, algorithmic template<br><br>Summary: This article provides a comprehensive overview of modern optimization techniques tailored for training neural networks, focusing primarily on enhancing efficiency and scalability. It introduces a unified algorithmic framework that encapsulates state-of-the-art optimization algorithms, underlining the critical role of adapting optimization strategies to the specific structural characteristics of neural network problems. The review emphasizes how these methods can be designed to be invariant to the scale of the problem, ensuring their applicability across a wide range of network sizes and complexities. Additionally, the article aims to bridge the gap between theory and practice by presenting concepts accessible to both researchers seeking to contribute to methodological advancements and practitioners aiming to implement these algorithms effectively. Through this exposition, readers gain insight into the current landscape of optimization in deep learning, preparing them to engage with cutting-edge developments and apply these techniques to real-world large-scale neural network training challenges. <div>
arXiv:2511.11163v1 Announce Type: new 
Abstract: This article reviews modern optimization methods for training neural networks with an emphasis on efficiency and scale. We present state-of-the-art optimization algorithms under a unified algorithmic template that highlights the importance of adapting to the structures in the problem. We then cover how to make these algorithms agnostic to the scale of the problem. Our exposition is intended as an introduction for both practitioners and researchers who wish to be involved in these exciting new developments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Power Ensemble Aggregation for Improved Extreme Event AI Prediction</title>
<link>https://arxiv.org/abs/2511.11170</link>
<guid>https://arxiv.org/abs/2511.11170</guid>
<content:encoded><![CDATA[
<div> Keywords: climate extremes, heat waves, machine learning, power mean aggregation, classification  

<br><br>Summary:  
This paper tackles the critical problem of improving predictions for climate extreme events, with a focus on heat waves, by leveraging machine learning techniques. The authors formulate the task as a classification problem, aiming to predict if surface air temperature will surpass a specified local quantile threshold within a given timeframe. A major contribution is the introduction of a power mean aggregation strategy to combine ensemble predictions, which markedly improves prediction accuracy over the conventional approach of averaging (mean aggregation). By transforming a machine learning weather forecasting model into a generative one and applying this non-linear aggregation, the method achieves superior performance in identifying extreme heat occurrences. Importantly, the power mean aggregation’s optimal parameters depend on the chosen quantile threshold, suggesting its adaptability and enhanced effectiveness for predicting rarer, more extreme temperature events. This adaptable approach holds promise for better anticipating climate extremes, which is crucial for disaster preparedness and climate resilience planning. <div>
arXiv:2511.11170v1 Announce Type: new 
Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-line learning of dynamic systems: sparse regression meets Kalman filtering</title>
<link>https://arxiv.org/abs/2511.11178</link>
<guid>https://arxiv.org/abs/2511.11178</guid>
<content:encoded><![CDATA[
<div> SINDy, Kalman filter, real-time learning, nonlinear dynamics, sparse modeling<br><br>Summary:<br><br>1. The paper addresses the challenge of learning governing equations from data in real-time, which is crucial for understanding physical systems in physics, biology, and engineering.<br><br>2. It builds upon the Sparse Identification of Nonlinear Dynamical systems (SINDy) algorithm, known for leveraging sparsity to discover concise models of nonlinear systems.<br><br>3. The authors propose a novel approach called the SINDy Kalman Filter (SKF), which integrates SINDy with the Kalman filter from control theory by treating unknown system parameters as part of the state vector.<br><br>4. SKF enables real-time inference of complex, time-varying nonlinear models that neither SINDy nor Kalman filters can achieve individually.<br><br>5. The method also improves parameter identification in Kalman filter frameworks through a look-ahead error strategy, simplifying the estimation of sparsity levels, noise variance, and detecting switching instants in the system.<br><br>6. Validation is performed on challenging cases including a chaotic Lorenz system with drifting or switching parameters.<br><br>7. The approach is demonstrated effectively on a real-world nonlinear aircraft model, successfully identifying sparse dynamics from real flight data in real time. <div>
arXiv:2511.11178v1 Announce Type: new 
Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss</title>
<link>https://arxiv.org/abs/2511.11181</link>
<guid>https://arxiv.org/abs/2511.11181</guid>
<content:encoded><![CDATA[
<div> Keywords: Incomplete Multi-View Clustering, Graph Neural Networks, Dynamic Graph Learning, Masked Graph Reconstruction Loss, Contrastive Learning  

<br><br>Summary:  
This paper addresses challenges in incomplete multi-view clustering (IMVC), particularly the limitations of existing Graph Neural Network (GNN)-based methods. First, the authors observe that current IMVC approaches often use static graphs constructed by the K-Nearest Neighbors (KNN) algorithm, which introduces noise and reduces graph topology robustness. Second, standard use of Mean Squared Error (MSE) loss on graph reconstruction leads to high gradient noise during optimization. To overcome these issues, the paper proposes DGIMVCM, a novel framework that dynamically learns graph structures and employs a masked graph reconstruction loss strategy. The method begins by constructing a missing-robust global graph from raw multi-view data, which supports imputation of missing views through graph convolutional embedding layers extracting view-specific dynamic graphs. It integrates graph structure contrastive learning to enforce consistency across views. Next, high-level representations are extracted via a graph self-attention encoder, optimized using masked graph reconstruction loss to reduce gradient noise. Finally, a clustering module with pseudo-label self-supervised training refines the clustering results. Extensive experiments on various datasets demonstrate DGIMVCM’s effectiveness and superiority over existing approaches, highlighting its robustness and improved clustering performance in incomplete multi-view scenarios. <div>
arXiv:2511.11181v1 Announce Type: new 
Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag</title>
<link>https://arxiv.org/abs/2511.11190</link>
<guid>https://arxiv.org/abs/2511.11190</guid>
<content:encoded><![CDATA[
<div> LoRa, Reinforcement Learning, RSSI, Localization, Exploration  

<br><br>Summary:  
This paper addresses the challenge of locating LoRa tags worn by mentally incapacitated persons or others at risk of going missing using mobile sensors. It focuses on optimizing the sequential decision-making process to minimize the number of moves needed to find a periodically broadcasting LoRa tag in unknown environments, relying on the received signal strength indicator (RSSI). Existing reinforcement learning approaches suffer from vulnerabilities to domain shifts and RSSI fluctuations, which cause cascading errors and reduce localization accuracy. To overcome these issues, the authors propose LoRaCompass, a novel reinforcement learning model. LoRaCompass enhances robustness by learning a spatial representation from RSSI data through a spatially-aware feature extractor combined with a policy distillation loss to ensure the sensor moves closer to the tag. Additionally, it incorporates an exploration mechanism inspired by the upper confidence bound (UCB) strategy, which steadily increases the confidence in moves toward the tag. The system was tested in diverse, unseen ground-based and drone-assisted environments across an area exceeding 80 km². The results show LoRaCompass achieves over 90% success in locating the tag within 100 meters, marking a 40% improvement compared to prior methods, and demonstrates efficient search with a path length scaling linearly with the initial distance to the tag. <div>
arXiv:2511.11190v1 Announce Type: new 
Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping</title>
<link>https://arxiv.org/abs/2511.11208</link>
<guid>https://arxiv.org/abs/2511.11208</guid>
<content:encoded><![CDATA[
<div> Federated Learning, early stopping, synthetic validation, generative AI, chest X-ray classification<br><br>Summary:  
This paper addresses inefficiencies in Federated Learning (FL), where training typically runs for a fixed number of global rounds, often resulting in unnecessary computation or suboptimal performance if stopped too late or too early. To mitigate this, the authors propose a zero-shot synthetic validation framework that utilizes generative AI to monitor model performance during training. This framework enables adaptive early stopping by identifying the optimal training round without relying on additional labeled validation data. The approach conserves computational resources by reducing redundant training rounds and facilitates faster hyperparameter tuning. Experimental evaluation on multi-label chest X-ray classification tasks demonstrates the effectiveness of this method, achieving up to a 74% reduction in training rounds while maintaining model accuracy within 1% of the optimal performance. Overall, the work provides a practical solution for improving the efficiency and responsiveness of FL systems, especially in privacy-sensitive medical applications where data sharing is limited. <div>
arXiv:2511.11208v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates</title>
<link>https://arxiv.org/abs/2511.11211</link>
<guid>https://arxiv.org/abs/2511.11211</guid>
<content:encoded><![CDATA[
<div> Tsallis-INF, multi-armed bandit, online convex optimization, best-of-both-world, regret bounds<br><br>Summary:<br><br>1. This paper provides a simplified derivation of the best-of-both-world guarantee for the Tsallis-INF algorithm, which is used in multi-armed bandit problems.<br>2. The Tsallis-INF algorithm was originally proposed by J. Zimmert and Y. Seldin and is known for its optimal performance in both stochastic and adversarial bandit settings.<br>3. Unlike previous proofs, this derivation employs modern techniques from online convex optimization, avoiding reliance on conjugate functions.<br>4. The main goal is to present a clearer and more concise proof rather than optimize constants in the regret bounds.<br>5. This note contributes by making the best-of-both-worlds result more accessible and easier to understand for researchers working on bandit algorithms and online learning.<br><br> <div>
arXiv:2511.11211v1 Announce Type: new 
Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Methods for Vector Embeddings of TPC Data</title>
<link>https://arxiv.org/abs/2511.11221</link>
<guid>https://arxiv.org/abs/2511.11221</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Projection Chambers, sparse convolutional networks, ResNet, representation learning, nuclear physics detectors<br><br>Summary:<br><br>1. Time Projection Chambers (TPCs) are detectors that track charged particles in an ionizing medium, widely used in nuclear physics experiments for precise measurements.<br><br>2. The study investigates the use of sparse convolutional neural networks, specifically a sparse ResNet architecture, for learning representations of event data from TPCs.<br><br>3. Remarkably, even without training (i.e., with randomly initialized weights), the sparse ResNet produces meaningful structured embeddings of event data, indicating intrinsic feature extraction capability.<br><br>4. Further enhancement of the embeddings is achieved by pre-training the network on a simple physics-inspired binary classification task.<br><br>5. The experimental dataset comes from the GADGET II TPC, designed for low-energy β-delayed particle decay measurements, where raw pad-level signals are represented as sparse tensors for training with the Minkowski Engine.<br><br>6. Event embeddings generated reveal rich structural information about particle interactions.<br><br>7. To test cross-detector applicability, data from the Active-Target TPC (AT-TPC) was encoded using the same trained ResNet model, demonstrating useful embeddings even when the model was untrained.<br><br>8. Performance on AT-TPC embeddings improved after training on GADGET data, implying transfer learning benefits.<br><br>9. These findings suggest that sparse convolutional techniques are promising general tools for representation learning across various TPC nuclear physics experiments. <div>
arXiv:2511.11221v1 Announce Type: new 
Abstract: Time Projection Chambers (TPCs) are versatile detectors that reconstruct charged-particle tracks in an ionizing medium, enabling sensitive measurements across a wide range of nuclear physics experiments. We explore sparse convolutional networks for representation learning on TPC data, finding that a sparse ResNet architecture, even with randomly set weights, provides useful structured vector embeddings of events. Pre-training this architecture on a simple physics-motivated binary classification task further improves the embedding quality. Using data from the GAseous Detector with GErmanium Tagging (GADGET) II TPC, a detector optimized for measuring low-energy $\beta$-delayed particle decays, we represent raw pad-level signals as sparse tensors, train Minkowski Engine ResNet models, and probe the resulting event-level embeddings which reveal rich event structure. As a cross-detector test, we embed data from the Active-Target TPC (AT-TPC) -- a detector designed for nuclear reaction studies in inverse kinematics -- using the same encoder. We find that even an untrained sparse ResNet model provides useful embeddings of AT-TPC data, and we observe improvements when the model is trained on GADGET data. Together, these results highlight the potential of sparse convolutional techniques as a general tool for representation learning in diverse TPC experiments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Network-Powered Finger-Drawn Biometric Authentication</title>
<link>https://arxiv.org/abs/2511.11235</link>
<guid>https://arxiv.org/abs/2511.11235</guid>
<content:encoded><![CDATA[
<div> Neural networks, biometric authentication, finger-drawn digits, CNN, autoencoder<br><br>Summary:<br><br>This paper explores the use of neural network models for biometric authentication based on finger-drawn digits on touchscreen devices. The study involved twenty participants who each provided 2,000 digit samples drawn with their fingers on personal touchscreen devices. Two convolutional neural network (CNN) architectures were evaluated: a modified Inception-V1 network and a lightweight shallow CNN optimized for mobile environments. Both CNNs achieved approximately 89% authentication accuracy, with the shallow CNN being more parameter-efficient and suitable for mobile deployment. In addition to CNNs, convolutional and fully connected autoencoder models were tested for anomaly detection, achieving around 75% accuracy. The research highlights that simple finger-drawn digit patterns (0-9) can serve as effective biometric identifiers. The findings suggest that finger-drawn symbol authentication is a viable, secure, and user-friendly method for touchscreen device security. Furthermore, this biometric approach can be combined with existing pattern-based authentication systems to enhance security through multi-layered mechanisms, particularly for mobile applications. Overall, the paper provides evidence supporting neural network-based finger-drawn digit authentication as a promising biometric technology for future secure mobile user verification. <div>
arXiv:2511.11235v1 Announce Type: new 
Abstract: This paper investigates neural network-based biometric authentication using finger-drawn digits on touchscreen devices. We evaluated CNN and autoencoder architectures for user authentication through simple digit patterns (0-9) traced with finger input. Twenty participants contributed 2,000 finger-drawn digits each on personal touchscreen devices. We compared two CNN architectures: a modified Inception-V1 network and a lightweight shallow CNN for mobile environments. Additionally, we examined Convolutional and Fully Connected autoencoders for anomaly detection. Both CNN architectures achieved ~89% authentication accuracy, with the shallow CNN requiring fewer parameters. Autoencoder approaches achieved ~75% accuracy. The results demonstrate that finger-drawn symbol authentication provides a viable, secure, and user-friendly biometric solution for touchscreen devices. This approach can be integrated with existing pattern-based authentication methods to create multi-layered security systems for mobile applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Virtual Width Networks</title>
<link>https://arxiv.org/abs/2511.11238</link>
<guid>https://arxiv.org/abs/2511.11238</guid>
<content:encoded><![CDATA[
<div> Virtual Width Networks, representational width, convergence speedup, token prediction, scaling efficiency  
<br><br>Summary:  
This paper introduces Virtual Width Networks (VWN), a novel framework designed to enhance model representations by increasing width without the usual quadratic increase in computational cost tied to hidden size expansion. VWN disentangles representational width from the backbone network's width, allowing the embedding space to grow substantially while keeping core compute nearly unchanged. Large-scale experiments demonstrate that an 8-fold increase in virtual width accelerates optimization by more than 2 times in next-token prediction tasks and over 3 times in next-2-token prediction tasks. The benefits of VWN become more pronounced as training progresses, with both the performance gap and speedup ratio expanding, indicating greater token efficiency and heightened effectiveness at scale. Additionally, the authors uncover an approximately log-linear relationship between virtual width and loss reduction, which provides a new empirical foundation to consider virtual-width scaling as an independent and promising direction for improving large model efficiency. This approach offers a scalable and computationally efficient avenue to boost model performance without proportionally increasing computational demands. <div>
arXiv:2511.11238v1 Announce Type: new 
Abstract: We introduce Virtual Width Networks (VWN), a framework that delivers the benefits of wider representations without incurring the quadratic cost of increasing the hidden size. VWN decouples representational width from backbone width, expanding the embedding space while keeping backbone compute nearly constant. In our large-scale experiment, an 8-times expansion accelerates optimization by over 2 times for next-token and 3 times for next-2-token prediction. The advantage amplifies over training as both the loss gap grows and the convergence-speedup ratio increases, showing that VWN is not only token-efficient but also increasingly effective with scale. Moreover, we identify an approximately log-linear scaling relation between virtual width and loss reduction, offering an initial empirical basis and motivation for exploring virtual-width scaling as a new dimension of large-model efficiency.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning</title>
<link>https://arxiv.org/abs/2511.11240</link>
<guid>https://arxiv.org/abs/2511.11240</guid>
<content:encoded><![CDATA[

arXiv:2511.11240v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Attributed Graph Learning via Neighborhood-Aware Star Kernels</title>
<link>https://arxiv.org/abs/2511.11245</link>
<guid>https://arxiv.org/abs/2511.11245</guid>
<content:encoded><![CDATA[

arXiv:2511.11245v1 Announce Type: new 
Abstract: Attributed graphs, typically characterized by irregular topologies and a mix of numerical and categorical attributes, are ubiquitous in diverse domains such as social networks, bioinformatics, and cheminformatics. While graph kernels provide a principled framework for measuring graph similarity, existing kernel methods often struggle to simultaneously capture heterogeneous attribute semantics and neighborhood information in attributed graphs. In this work, we propose the Neighborhood-Aware Star Kernel (NASK), a novel graph kernel designed for attributed graph learning. NASK leverages an exponential transformation of the Gower similarity coefficient to jointly model numerical and categorical features efficiently, and employs star substructures enhanced by Weisfeiler-Lehman iterations to integrate multi-scale neighborhood structural information. We theoretically prove that NASK is positive definite, ensuring compatibility with kernel-based learning frameworks such as SVMs. Extensive experiments are conducted on eleven attributed and four large-scale real-world graph benchmarks. The results demonstrate that NASK consistently achieves superior performance over sixteen state-of-the-art baselines, including nine graph kernels and seven Graph Neural Networks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria</title>
<link>https://arxiv.org/abs/2511.11293</link>
<guid>https://arxiv.org/abs/2511.11293</guid>
<content:encoded><![CDATA[

arXiv:2511.11293v1 Announce Type: new 
Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Expressive Multi-Token Prediction with Probabilistic Circuits</title>
<link>https://arxiv.org/abs/2511.11346</link>
<guid>https://arxiv.org/abs/2511.11346</guid>
<content:encoded><![CDATA[

arXiv:2511.11346v1 Announce Type: new 
Abstract: Multi-token prediction (MTP) is a prominent strategy to significantly speed up generation in large language models (LLMs), including byte-level LLMs, which are tokeniser-free but prohibitively slow. However, existing MTP methods often sacrifice expressiveness by assuming independence between future tokens. In this work, we investigate the trade-off between expressiveness and latency in MTP within the framework of probabilistic circuits (PCs). Our framework, named MTPC, allows one to explore different ways to encode the joint distributions over future tokens by selecting different circuit architectures, generalising classical models such as (hierarchical) mixture models, hidden Markov models and tensor networks. We show the efficacy of MTPC by retrofitting existing byte-level LLMs, such as EvaByte. Our experiments show that, when combined with speculative decoding, MTPC significantly speeds up generation compared to MTP with independence assumptions, while guaranteeing to retain the performance of the original verifier LLM. We also rigorously study the optimal trade-off between expressiveness and latency when exploring the possible parameterisations of MTPC, such as PC architectures and partial layer sharing between the verifier and draft LLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials</title>
<link>https://arxiv.org/abs/2511.11361</link>
<guid>https://arxiv.org/abs/2511.11361</guid>
<content:encoded><![CDATA[

arXiv:2511.11361v1 Announce Type: new 
Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2511.11362</link>
<guid>https://arxiv.org/abs/2511.11362</guid>
<content:encoded><![CDATA[

arXiv:2511.11362v1 Announce Type: new 
Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Genes Speak: A Semantic-Guided Framework for Spatially Resolved Transcriptomics Data Clustering</title>
<link>https://arxiv.org/abs/2511.11380</link>
<guid>https://arxiv.org/abs/2511.11380</guid>
<content:encoded><![CDATA[

arXiv:2511.11380v1 Announce Type: new 
Abstract: Spatial transcriptomics enables gene expression profiling with spatial context, offering unprecedented insights into the tissue microenvironment. However, most computational models treat genes as isolated numerical features, ignoring the rich biological semantics encoded in their symbols. This prevents a truly deep understanding of critical biological characteristics. To overcome this limitation, we present SemST, a semantic-guided deep learning framework for spatial transcriptomics data clustering. SemST leverages Large Language Models (LLMs) to enable genes to "speak" through their symbolic meanings, transforming gene sets within each tissue spot into biologically informed embeddings. These embeddings are then fused with the spatial neighborhood relationships captured by Graph Neural Networks (GNNs), achieving a coherent integration of biological function and spatial structure. We further introduce the Fine-grained Semantic Modulation (FSM) module to optimally exploit these biological priors. The FSM module learns spot-specific affine transformations that empower the semantic embeddings to perform an element-wise calibration of the spatial features, thus dynamically injecting high-order biological knowledge into the spatial context. Extensive experiments on public spatial transcriptomics datasets show that SemST achieves state-of-the-art clustering performance. Crucially, the FSM module exhibits plug-and-play versatility, consistently improving the performance when integrated into other baseline methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust inverse material design with physical guarantees using the Voigt-Reuss Net</title>
<link>https://arxiv.org/abs/2511.11388</link>
<guid>https://arxiv.org/abs/2511.11388</guid>
<content:encoded><![CDATA[

arXiv:2511.11388v1 Announce Type: new 
Abstract: We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the L\"owner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $>\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2>0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming</title>
<link>https://arxiv.org/abs/2511.11391</link>
<guid>https://arxiv.org/abs/2511.11391</guid>
<content:encoded><![CDATA[

arXiv:2511.11391v1 Announce Type: new 
Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.11402</link>
<guid>https://arxiv.org/abs/2511.11402</guid>
<content:encoded><![CDATA[

arXiv:2511.11402v1 Announce Type: new 
Abstract: Autonomous spacecraft control for mission phases such as launch, ascent, stage separation, and orbit insertion remains a critical challenge due to the need for adaptive policies that generalize across dynamically distinct regimes. While reinforcement learning (RL) has shown promise in individual astrodynamics tasks, existing approaches often require separate policies for distinct mission phases, limiting adaptability and increasing operational complexity. This work introduces a transformer-based RL framework that unifies multi-phase trajectory optimization through a single policy architecture, leveraging the transformer's inherent capacity to model extended temporal contexts. Building on proximal policy optimization (PPO), our framework replaces conventional recurrent networks with a transformer encoder-decoder structure, enabling the agent to maintain coherent memory across mission phases spanning seconds to minutes during critical operations. By integrating a Gated Transformer-XL (GTrXL) architecture, the framework eliminates manual phase transitions while maintaining stability in control decisions. We validate our approach progressively: first demonstrating near-optimal performance on single-phase benchmarks (double integrator and Van der Pol oscillator), then extending to multiphase waypoint navigation variants, and finally tackling a complex multiphase rocket ascent problem that includes atmospheric flight, stage separation, and vacuum operations. Results demonstrate that the transformer-based framework not only matches analytical solutions in simple cases but also effectively learns coherent control policies across dynamically distinct regimes, establishing a foundation for scalable autonomous mission planning that reduces reliance on phase-specific controllers while maintaining compatibility with safety-critical verification protocols.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multicalibration yields better matchings</title>
<link>https://arxiv.org/abs/2511.11413</link>
<guid>https://arxiv.org/abs/2511.11413</guid>
<content:encoded><![CDATA[

arXiv:2511.11413v1 Announce Type: new 
Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $\gamma$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat \gamma$, with the following property. Picking the best matching based on the output of $\hat \gamma$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $\gamma$. We complement this result by providing sample complexity bounds.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization</title>
<link>https://arxiv.org/abs/2511.11415</link>
<guid>https://arxiv.org/abs/2511.11415</guid>
<content:encoded><![CDATA[

arXiv:2511.11415v1 Announce Type: new 
Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching</title>
<link>https://arxiv.org/abs/2511.11418</link>
<guid>https://arxiv.org/abs/2511.11418</guid>
<content:encoded><![CDATA[

arXiv:2511.11418v1 Announce Type: new 
Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrofit: Continual Learning with Bounded Forgetting for Security Applications</title>
<link>https://arxiv.org/abs/2511.11439</link>
<guid>https://arxiv.org/abs/2511.11439</guid>
<content:encoded><![CDATA[

arXiv:2511.11439v1 Announce Type: new 
Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference</title>
<link>https://arxiv.org/abs/2511.11446</link>
<guid>https://arxiv.org/abs/2511.11446</guid>
<content:encoded><![CDATA[

arXiv:2511.11446v1 Announce Type: new 
Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression</title>
<link>https://arxiv.org/abs/2511.11459</link>
<guid>https://arxiv.org/abs/2511.11459</guid>
<content:encoded><![CDATA[

arXiv:2511.11459v1 Announce Type: new 
Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies</title>
<link>https://arxiv.org/abs/2511.11461</link>
<guid>https://arxiv.org/abs/2511.11461</guid>
<content:encoded><![CDATA[

arXiv:2511.11461v1 Announce Type: new 
Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture</title>
<link>https://arxiv.org/abs/2511.11462</link>
<guid>https://arxiv.org/abs/2511.11462</guid>
<content:encoded><![CDATA[

arXiv:2511.11462v1 Announce Type: new 
Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations</title>
<link>https://arxiv.org/abs/2511.11472</link>
<guid>https://arxiv.org/abs/2511.11472</guid>
<content:encoded><![CDATA[

arXiv:2511.11472v1 Announce Type: new 
Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-efficient U-Net for Segmentation of Carbide Microstructures in SEM Images of Steel Alloys</title>
<link>https://arxiv.org/abs/2511.11485</link>
<guid>https://arxiv.org/abs/2511.11485</guid>
<content:encoded><![CDATA[

arXiv:2511.11485v1 Announce Type: new 
Abstract: Understanding reactor-pressure-vessel steel microstructure is crucial for predicting mechanical properties, as carbide precipitates both strengthen the alloy and can initiate cracks. In scanning electron microscopy images, gray-value overlap between carbides and matrix makes simple thresholding ineffective. We present a data-efficient segmentation pipeline using a lightweight U-Net (30.7~M parameters) trained on just \textbf{10 annotated scanning electron microscopy images}. Despite limited data, our model achieves a \textbf{Dice-S{\o}rensen coefficient of 0.98}, significantly outperforming the state-of-the-art in the field of metallurgy (classical image analysis: 0.85), while reducing annotation effort by one order of magnitude compared to the state-of-the-art data efficient segmentation model. This approach enables rapid, automated carbide quantification for alloy design and generalizes to other steel types, demonstrating the potential of data-efficient deep learning in reactor-pressure-vessel steel analysis.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Dimension Estimation for Radio Galaxy Zoo using Diffusion Models</title>
<link>https://arxiv.org/abs/2511.11490</link>
<guid>https://arxiv.org/abs/2511.11490</guid>
<content:encoded><![CDATA[

arXiv:2511.11490v1 Announce Type: new 
Abstract: In this work, we estimate the intrinsic dimension (iD) of the Radio Galaxy Zoo (RGZ) dataset using a score-based diffusion model. We examine how the iD estimates vary as a function of Bayesian neural network (BNN) energy scores, which measure how similar the radio sources are to the MiraBest subset of the RGZ dataset. We find that out-of-distribution sources exhibit higher iD values, and that the overall iD for RGZ exceeds those typically reported for natural image datasets. Furthermore, we analyse how iD varies across Fanaroff-Riley (FR) morphological classes and as a function of the signal-to-noise ratio (SNR). While no relationship is found between FR I and FR II classes, a weak trend toward higher SNR at lower iD. Future work using the RGZ dataset could make use of the relationship between iD and energy scores to quantitatively study and improve the representations learned by various self-supervised learning algorithms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation</title>
<link>https://arxiv.org/abs/2511.11500</link>
<guid>https://arxiv.org/abs/2511.11500</guid>
<content:encoded><![CDATA[

arXiv:2511.11500v1 Announce Type: new 
Abstract: Modern language models fail a fundamental requirement of trustworthy intelligence: knowing when not to answer. Despite achieving impressive accuracy on benchmarks, these models produce confident hallucinations, even when wrong answers carry catastrophic consequences. Our evaluations on GSM8K, MedQA and GPQA show frontier models almost never abstain despite explicit warnings of severe penalties, suggesting that prompts cannot override training that rewards any answer over no answer. As a remedy, we propose Reinforced Hesitation (RH): a modification to Reinforcement Learning from Verifiable Rewards (RLVR) to use ternary rewards (+1 correct, 0 abstention, -$\lambda$ error) instead of binary. Controlled experiments on logic puzzles reveal that varying $\lambda$ produces distinct models along a Pareto frontier, where each training penalty yields the optimal model for its corresponding risk regime: low penalties produce aggressive answerers, high penalties conservative abstainers. We then introduce two inference strategies that exploit trained abstention as a coordination signal: cascading routes queries through models with decreasing risk tolerance, while self-cascading re-queries the same model on abstention. Both outperform majority voting with lower computational cost. These results establish abstention as a first-class training objective that transforms ``I don't know'' from failure into a coordination signal, enabling models to earn trust through calibrated honesty about their limits.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models</title>
<link>https://arxiv.org/abs/2511.11505</link>
<guid>https://arxiv.org/abs/2511.11505</guid>
<content:encoded><![CDATA[

arXiv:2511.11505v1 Announce Type: new 
Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications</title>
<link>https://arxiv.org/abs/2511.11539</link>
<guid>https://arxiv.org/abs/2511.11539</guid>
<content:encoded><![CDATA[

arXiv:2511.11539v1 Announce Type: new 
Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multistability of Self-Attention Dynamics in Transformers</title>
<link>https://arxiv.org/abs/2511.11553</link>
<guid>https://arxiv.org/abs/2511.11553</guid>
<content:encoded><![CDATA[

arXiv:2511.11553v1 Announce Type: new 
Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</title>
<link>https://arxiv.org/abs/2511.11560</link>
<guid>https://arxiv.org/abs/2511.11560</guid>
<content:encoded><![CDATA[

arXiv:2511.11560v1 Announce Type: new 
Abstract: In semi-decentralized federated learning, devices primarily rely on device-to-device communication but occasionally interact with a central server. Periodically, a sampled subset of devices uploads their local models to the server, which computes an aggregate model. The server can then either (i) share this aggregate model only with the sampled clients (sampled-to-sampled, S2S) or (ii) broadcast it to all clients (sampled-to-all, S2A). Despite their practical significance, a rigorous theoretical and empirical comparison of these two strategies remains absent. We address this gap by analyzing S2S and S2A within a unified convergence framework that accounts for key system parameters: sampling rate, server aggregation frequency, and network connectivity. Our results, both analytical and experimental, reveal distinct regimes where one strategy outperforms the other, depending primarily on the degree of data heterogeneity across devices. These insights lead to concrete design guidelines for practical semi-decentralized FL deployments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Mixture of Block Attention</title>
<link>https://arxiv.org/abs/2511.11571</link>
<guid>https://arxiv.org/abs/2511.11571</guid>
<content:encoded><![CDATA[

arXiv:2511.11571v1 Announce Type: new 
Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cognitively-Inspired Episodic Memory Architectures for Accurate and Efficient Character AI</title>
<link>https://arxiv.org/abs/2511.10652</link>
<guid>https://arxiv.org/abs/2511.10652</guid>
<content:encoded><![CDATA[

arXiv:2511.10652v1 Announce Type: cross 
Abstract: Large language models show promise for embodying historical characters in dialogue systems, but existing approaches face a critical trade-off: simple retrieval-augmented generation produces shallow responses, while multi-stage reflection achieves depth at prohibitive latency. We present an architecture that resolves this tension through offline data augmentation and efficient parallel retrieval from structured episodic memory. Our system transforms biographical data into 1,774 enriched first-person memories with affective-semantic metadata, then employs two-stage retrieval achieving 0.52s prompt generation. Evaluation using LLM-as-judge and RAGAs metrics shows our approach achieves parity with traditional RAG on GPT-4 while significantly outperforming it on smaller models (GPT-3.5, GPT-3), suggesting particular value for resource-constrained deployments. Beyond dialogue, the structured memory enables novel visualization tools: spatiotemporal heatmaps, emotional trajectory analysis, and interactive path tracking, positioning the system as both a dialogue interface and research tool for biographical analysis. We use Van Gogh as a test case, but the architecture is generalizable to any historical figure with substantial textual records, offering a practical framework for educational, museum, and research applications requiring both accuracy and efficiency
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patent Representation Learning via Self-supervision</title>
<link>https://arxiv.org/abs/2511.10657</link>
<guid>https://arxiv.org/abs/2511.10657</guid>
<content:encoded><![CDATA[

arXiv:2511.10657v1 Announce Type: cross 
Abstract: This paper presents a simple yet effective contrastive learning framework for learning patent embeddings by leveraging multiple views from within the same document. We first identify a patent-specific failure mode of SimCSE style dropout augmentation: it produces overly uniform embeddings that lose semantic cohesion. To remedy this, we propose section-based augmentation, where different sections of a patent (e.g., abstract, claims, background) serve as complementary views. This design introduces natural semantic and structural diversity, mitigating over-dispersion and yielding embeddings that better preserve both global structure and local continuity. On large-scale benchmarks, our fully self-supervised method matches or surpasses citation-and IPC-supervised baselines in prior-art retrieval and classification, while avoiding reliance on brittle or incomplete annotations. Our analysis further shows that different sections specialize for different tasks-claims and summaries benefit retrieval, while background sections aid classification-highlighting the value of patents' inherent discourse structure for representation learning. These results highlight the value of exploiting intra-document views for scalable and generalizable patent understanding.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Evaluation of Large Language Model Behavior</title>
<link>https://arxiv.org/abs/2511.10661</link>
<guid>https://arxiv.org/abs/2511.10661</guid>
<content:encoded><![CDATA[

arXiv:2511.10661v1 Announce Type: cross 
Abstract: It is increasingly important to evaluate how text generation systems based on large language models (LLMs) behave, such as their tendency to produce harmful output or their sensitivity to adversarial inputs. Such evaluations often rely on a curated benchmark set of input prompts provided to the LLM, where the output for each prompt may be assessed in a binary fashion (e.g., harmful/non-harmful or does not leak/leaks sensitive information), and the aggregation of binary scores is used to evaluate the LLM. However, existing approaches to evaluation often neglect statistical uncertainty quantification. With an applied statistics audience in mind, we provide background on LLM text generation and evaluation, and then describe a Bayesian approach for quantifying uncertainty in binary evaluation metrics. We focus in particular on uncertainty that is induced by the probabilistic text generation strategies typically deployed in LLM-based systems. We present two case studies applying this approach: 1) evaluating refusal rates on a benchmark of adversarial inputs designed to elicit harmful responses, and 2) evaluating pairwise preferences of one LLM over another on a benchmark of open-ended interactive dialogue examples. We demonstrate how the Bayesian approach can provide useful uncertainty quantification about the behavior of LLM-based systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guarding the Meaning: Self-Supervised Training for Semantic Robustness in Guard Models</title>
<link>https://arxiv.org/abs/2511.10665</link>
<guid>https://arxiv.org/abs/2511.10665</guid>
<content:encoded><![CDATA[

arXiv:2511.10665v1 Announce Type: cross 
Abstract: Guard models are a critical component of LLM safety, but their sensitivity to superficial linguistic variations remains a key vulnerability. We show that even meaning-preserving paraphrases can cause large fluctuations in safety scores, revealing a lack of semantic grounding. To address this, we introduce a practical, self-supervised framework for improving the semantic robustness of guard models. Our method leverages paraphrase sets to enforce prediction consistency using a novel, skew-aware aggregation strategy for robust target computation. Notably, we find that standard aggregation methods like mean and median can degrade safety, underscoring the need for skew-aware alternatives. We analyze six open-source guard models and show that our approach reduces semantic variability across paraphrases by ~58%, improves benchmark accuracy by ~2.5% on average, and generalizes to unseen stylistic variations. Intriguingly, we discover a bidirectional relationship between model calibration and consistency: our robustness training improves calibration by up to 40%, revealing a fundamental connection between these properties. These results highlight the value of treating semantic consistency as a first-class training objective and provide a scalable recipe for building more reliable guard models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating LLM Understanding via Structured Tabular Decision Simulations</title>
<link>https://arxiv.org/abs/2511.10667</link>
<guid>https://arxiv.org/abs/2511.10667</guid>
<content:encoded><![CDATA[

arXiv:2511.10667v1 Announce Type: cross 
Abstract: Large language models (LLMs) often achieve impressive predictive accuracy, yet correctness alone does not imply genuine understanding. True LLM understanding, analogous to human expertise, requires making consistent, well-founded decisions across multiple instances and diverse domains, relying on relevant and domain-grounded decision factors. We introduce Structured Tabular Decision Simulations (STaDS), a suite of expert-like decision settings that evaluate LLMs as if they were professionals undertaking structured decision ``exams''. In this context, understanding is defined as the ability to identify and rely on the correct decision factors, features that determine outcomes within a domain. STaDS jointly assesses understanding through: (i) question and instruction comprehension, (ii) knowledge-based prediction, and (iii) reliance on relevant decision factors. By analyzing 9 frontier LLMs across 15 diverse decision settings, we find that (a) most models struggle to achieve consistently strong accuracy across diverse domains; (b) models can be accurate yet globally unfaithful, and there are frequent mismatches between stated rationales and factors driving predictions. Our findings highlight the need for global-level understanding evaluation protocols and advocate for novel frameworks that go beyond accuracy to enhance LLMs' understanding ability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Spoken Language Development in Children with Cochlear Implants Using Preimplantation MRI</title>
<link>https://arxiv.org/abs/2511.10669</link>
<guid>https://arxiv.org/abs/2511.10669</guid>
<content:encoded><![CDATA[

arXiv:2511.10669v1 Announce Type: cross 
Abstract: Cochlear implants (CI) significantly improve spoken language in children with severe-to-profound sensorineural hearing loss (SNHL), yet outcomes remain more variable than in children with normal hearing. This variability cannot be reliably predicted for individual children using age at implantation or residual hearing. This study aims to compare the accuracy of traditional machine learning (ML) to deep transfer learning (DTL) algorithms to predict post-CI spoken language development of children with bilateral SNHL using a binary classification model of high versus low language improvers. A total of 278 implanted children enrolled from three centers. The accuracy, sensitivity and specificity of prediction models based upon brain neuroanatomic features using traditional ML and DTL learning. DTL prediction models using bilinear attention-based fusion strategy achieved: accuracy of 92.39% (95% CI, 90.70%-94.07%), sensitivity of 91.22% (95% CI, 89.98%-92.47%), specificity of 93.56% (95% CI, 90.91%-96.21%), and area under the curve (AUC) of 0.977 (95% CI, 0.969-0.986). DTL outperformed traditional ML models in all outcome measures. DTL was significantly improved by direct capture of discriminative and task-specific information that are advantages of representation learning enabled by this approach over ML. The results support the feasibility of a single DTL prediction model for language prediction of children served by CI programs worldwide.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PISanitizer: Preventing Prompt Injection to Long-Context LLMs via Prompt Sanitization</title>
<link>https://arxiv.org/abs/2511.10720</link>
<guid>https://arxiv.org/abs/2511.10720</guid>
<content:encoded><![CDATA[

arXiv:2511.10720v1 Announce Type: cross 
Abstract: Long context LLMs are vulnerable to prompt injection, where an attacker can inject an instruction in a long context to induce an LLM to generate an attacker-desired output. Existing prompt injection defenses are designed for short contexts. When extended to long-context scenarios, they have limited effectiveness. The reason is that an injected instruction constitutes only a very small portion of a long context, making the defense very challenging. In this work, we propose PISanitizer, which first pinpoints and sanitizes potential injected tokens (if any) in a context before letting a backend LLM generate a response, thereby eliminating the influence of the injected instruction. To sanitize injected tokens, PISanitizer builds on two observations: (1) prompt injection attacks essentially craft an instruction that compels an LLM to follow it, and (2) LLMs intrinsically leverage the attention mechanism to focus on crucial input tokens for output generation. Guided by these two observations, we first intentionally let an LLM follow arbitrary instructions in a context and then sanitize tokens receiving high attention that drive the instruction-following behavior of the LLM. By design, PISanitizer presents a dilemma for an attacker: the more effectively an injected instruction compels an LLM to follow it, the more likely it is to be sanitized by PISanitizer. Our extensive evaluation shows that PISanitizer can successfully prevent prompt injection, maintain utility, outperform existing defenses, is efficient, and is robust to optimization-based and strong adaptive attacks. The code is available at https://github.com/sleeepeer/PISanitizer.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Data Attribution for Text-to-Image Models</title>
<link>https://arxiv.org/abs/2511.10721</link>
<guid>https://arxiv.org/abs/2511.10721</guid>
<content:encoded><![CDATA[

arXiv:2511.10721v1 Announce Type: cross 
Abstract: Data attribution for text-to-image models aims to identify the training images that most significantly influenced a generated output. Existing attribution methods involve considerable computational resources for each query, making them impractical for real-world applications. We propose a novel approach for scalable and efficient data attribution. Our key idea is to distill a slow, unlearning-based attribution method to a feature embedding space for efficient retrieval of highly influential training images. During deployment, combined with efficient indexing and search methods, our method successfully finds highly influential images without running expensive attribution algorithms. We show extensive results on both medium-scale models trained on MSCOCO and large-scale Stable Diffusion models trained on LAION, demonstrating that our method can achieve better or competitive performance in a few seconds, faster than existing methods by 2,500x - 400,000x. Our work represents a meaningful step towards the large-scale application of data attribution methods on real-world models such as Stable Diffusion.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate-Based Differentiable Pipeline for Shape Optimization</title>
<link>https://arxiv.org/abs/2511.10761</link>
<guid>https://arxiv.org/abs/2511.10761</guid>
<content:encoded><![CDATA[

arXiv:2511.10761v1 Announce Type: cross 
Abstract: Gradient-based optimization of engineering designs is limited by non-differentiable components in the typical computer-aided engineering (CAE) workflow, which calculates performance metrics from design parameters. While gradient-based methods could provide noticeable speed-ups in high-dimensional design spaces, codes for meshing, physical simulations, and other common components are not differentiable even if the math or physics underneath them is. We propose replacing non-differentiable pipeline components with surrogate models which are inherently differentiable. Using a toy example of aerodynamic shape optimization, we demonstrate an end-to-end differentiable pipeline where a 3D U-Net full-field surrogate replaces both meshing and simulation steps by training it on the mapping between the signed distance field (SDF) of the shape and the fields of interest. This approach enables gradient-based shape optimization without the need for differentiable solvers, which can be useful in situations where adjoint methods are unavailable and/or hard to implement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Local Wasserstein Regression</title>
<link>https://arxiv.org/abs/2511.10824</link>
<guid>https://arxiv.org/abs/2511.10824</guid>
<content:encoded><![CDATA[

arXiv:2511.10824v1 Announce Type: cross 
Abstract: We study the estimation problem of distribution-on-distribution regression, where both predictors and responses are probability measures. Existing approaches typically rely on a global optimal transport map or tangent-space linearization, which can be restrictive in approximation capacity and distort geometry in multivariate underlying domains. In this paper, we propose the \emph{Neural Local Wasserstein Regression}, a flexible nonparametric framework that models regression through locally defined transport maps in Wasserstein space. Our method builds on the analogy with classical kernel regression: kernel weights based on the 2-Wasserstein distance localize estimators around reference measures, while neural networks parameterize transport operators that adapt flexibly to complex data geometries. This localized perspective broadens the class of admissible transformations and avoids the limitations of global map assumptions and linearization structures. We develop a practical training procedure using DeepSets-style architectures and Sinkhorn-approximated losses, combined with a greedy reference selection strategy for scalability. Through synthetic experiments on Gaussian and mixture models, as well as distributional prediction tasks on MNIST, we demonstrate that our approach effectively captures nonlinear and high-dimensional distributional relationships that elude existing methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings</title>
<link>https://arxiv.org/abs/2511.10842</link>
<guid>https://arxiv.org/abs/2511.10842</guid>
<content:encoded><![CDATA[

arXiv:2511.10842v1 Announce Type: cross 
Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs</title>
<link>https://arxiv.org/abs/2511.10850</link>
<guid>https://arxiv.org/abs/2511.10850</guid>
<content:encoded><![CDATA[

arXiv:2511.10850v1 Announce Type: cross 
Abstract: Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accuracy-Preserving CNN Pruning Method under Limited Data Availability</title>
<link>https://arxiv.org/abs/2511.10861</link>
<guid>https://arxiv.org/abs/2511.10861</guid>
<content:encoded><![CDATA[

arXiv:2511.10861v1 Announce Type: cross 
Abstract: Convolutional Neural Networks (CNNs) are widely used in image recognition and have succeeded in various domains. CNN models have become larger-scale to improve accuracy and generalization performance. Research has been conducted on compressing pre-trained models for specific target applications in environments with limited computing resources. Among model compression techniques, methods using Layer-wise Relevance Propagation (LRP), an explainable AI technique, have shown promise by achieving high pruning rates while preserving accuracy, even without fine-tuning. Because these methods do not require fine-tuning, they are suited to scenarios with limited data. However, existing LRP-based pruning approaches still suffer from significant accuracy degradation, limiting their practical usability. This study proposes a pruning method that achieves a higher pruning rate while preserving better model accuracy. Our approach to pruning with a small amount of data has achieved pruning that preserves accuracy better than existing methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecting software monitors for control-flow anomaly detection through large language models and conformance checking</title>
<link>https://arxiv.org/abs/2511.10876</link>
<guid>https://arxiv.org/abs/2511.10876</guid>
<content:encoded><![CDATA[

arXiv:2511.10876v1 Announce Type: cross 
Abstract: Context: Ensuring high levels of dependability in modern computer-based systems has become increasingly challenging due to their complexity. Although systems are validated at design time, their behavior can be different at run-time, possibly showing control-flow anomalies due to "unknown unknowns".
  Objective: We aim to detect control-flow anomalies through software monitoring, which verifies run-time behavior by logging software execution and detecting deviations from expected control flow.
  Methods: We propose a methodology to develop software monitors for control-flow anomaly detection through Large Language Models (LLMs) and conformance checking. The methodology builds on existing software development practices to maintain traditional V&amp;V while providing an additional level of robustness and trustworthiness. It leverages LLMs to link design-time models and implementation code, automating source-code instrumentation. The resulting event logs are analyzed via conformance checking, an explainable and effective technique for control-flow anomaly detection.
  Results: We test the methodology on a case-study scenario from the European Railway Traffic Management System / European Train Control System (ERTMS/ETCS), which is a railway standard for modern interoperable railways. The results obtained from the ERTMS/ETCS case study demonstrate that LLM-based source-code instrumentation can achieve up to 84.775% control-flow coverage of the reference design-time process model, while the subsequent conformance checking-based anomaly detection reaches a peak performance of 96.610% F1-score and 93.515% AUC.
  Conclusion: Incorporating domain-specific knowledge to guide LLMs in source-code instrumentation significantly allowed obtaining reliable and quality software logs and enabled effective control-flow anomaly detection through conformance checking.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICX360: In-Context eXplainability 360 Toolkit</title>
<link>https://arxiv.org/abs/2511.10879</link>
<guid>https://arxiv.org/abs/2511.10879</guid>
<content:encoded><![CDATA[

arXiv:2511.10879v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MMA-Sim: Bit-Accurate Reference Model of Tensor Cores and Matrix Cores</title>
<link>https://arxiv.org/abs/2511.10909</link>
<guid>https://arxiv.org/abs/2511.10909</guid>
<content:encoded><![CDATA[

arXiv:2511.10909v1 Announce Type: cross 
Abstract: The rapidly growing computation demands of deep neural networks (DNNs) have driven hardware vendors to integrate matrix multiplication accelerators (MMAs), such as NVIDIA Tensor Cores and AMD Matrix Cores, into modern GPUs. However, due to distinct and undocumented arithmetic specifications for floating-point matrix multiplication, some MMAs can lead to numerical imprecision and inconsistency that can compromise the stability and reproducibility of DNN training and inference.
  This paper presents MMA-Sim, the first bit-accurate reference model that reveals the detailed arithmetic behaviors of the MMAs from ten GPU architectures (eight from NVIDIA and two from AMD). By dissecting the MMAs using a combination of targeted and randomized tests, our methodology derives nine arithmetic algorithms to simulate the floating-point matrix multiplication of the MMAs. Large-scale validation confirms bitwise equivalence between MMA-Sim and the real hardware. Using MMA-Sim, we investigate arithmetic behaviors that affect DNN training stability, and identify undocumented behaviors that could lead to significant errors.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Multisource Transfer Learning via Model Averaging for Positive-Unlabeled Data</title>
<link>https://arxiv.org/abs/2511.10919</link>
<guid>https://arxiv.org/abs/2511.10919</guid>
<content:encoded><![CDATA[

arXiv:2511.10919v1 Announce Type: cross 
Abstract: Positive-Unlabeled (PU) learning presents unique challenges due to the lack of explicitly labeled negative samples, particularly in high-stakes domains such as fraud detection and medical diagnosis. To address data scarcity and privacy constraints, we propose a novel transfer learning with model averaging framework that integrates information from heterogeneous data sources - including fully binary labeled, semi-supervised, and PU data sets - without direct data sharing. For each source domain type, a tailored logistic regression model is conducted, and knowledge is transferred to the PU target domain through model averaging. Optimal weights for combining source models are determined via a cross-validation criterion that minimizes the Kullback-Leibler divergence. We establish theoretical guarantees for weight optimality and convergence, covering both misspecified and correctly specified target models, with further extensions to high-dimensional settings using sparsity-penalized estimators. Extensive simulations and real-world credit risk data analyses demonstrate that our method outperforms other comparative methods in terms of predictive accuracy and robustness, especially under limited labeled data and heterogeneous environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology</title>
<link>https://arxiv.org/abs/2511.10930</link>
<guid>https://arxiv.org/abs/2511.10930</guid>
<content:encoded><![CDATA[

arXiv:2511.10930v1 Announce Type: cross 
Abstract: Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAT-Net: A Cross-Attention Tone Network for Cross-Subject EEG-EMG Fusion Tone Decoding</title>
<link>https://arxiv.org/abs/2511.10935</link>
<guid>https://arxiv.org/abs/2511.10935</guid>
<content:encoded><![CDATA[

arXiv:2511.10935v1 Announce Type: cross 
Abstract: Brain-computer interface (BCI) speech decoding has emerged as a promising tool for assisting individuals with speech impairments. In this context, the integration of electroencephalography (EEG) and electromyography (EMG) signals offers strong potential for enhancing decoding performance. Mandarin tone classification presents particular challenges, as tonal variations convey distinct meanings even when phonemes remain identical. In this study, we propose a novel cross-subject multimodal BCI decoding framework that fuses EEG and EMG signals to classify four Mandarin tones under both audible and silent speech conditions. Inspired by the cooperative mechanisms of neural and muscular systems in speech production, our neural decoding architecture combines spatial-temporal feature extraction branches with a cross-attention fusion mechanism, enabling informative interaction between modalities. We further incorporate domain-adversarial training to improve cross-subject generalization. We collected 4,800 EEG trials and 4,800 EMG trials from 10 participants using only twenty EEG and five EMG channels, demonstrating the feasibility of minimal-channel decoding. Despite employing lightweight modules, our model outperforms state-of-the-art baselines across all conditions, achieving average classification accuracies of 87.83% for audible speech and 88.08% for silent speech. In cross-subject evaluations, it still maintains strong performance with accuracies of 83.27% and 85.10% for audible and silent speech, respectively. We further conduct ablation studies to validate the effectiveness of each component. Our findings suggest that tone-level decoding with minimal EEG-EMG channels is feasible and potentially generalizable across subjects, contributing to the development of practical BCI applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities</title>
<link>https://arxiv.org/abs/2511.10997</link>
<guid>https://arxiv.org/abs/2511.10997</guid>
<content:encoded><![CDATA[

arXiv:2511.10997v1 Announce Type: cross 
Abstract: Multimodal models integrating natural language and visual information have substantially improved generalization of representation models. However, their effectiveness significantly declines in real-world situations where certain modalities are missing or unavailable. This degradation primarily stems from inconsistent representation learning between complete multimodal data and incomplete modality scenarios. Existing approaches typically address missing modalities through relatively simplistic generation methods, yet these approaches fail to adequately preserve cross-modal consistency, leading to suboptimal performance. To overcome this limitation, we propose a novel multimodal framework named PROMISE, a PROMpting-Attentive HIerarchical ContraStive LEarning approach designed explicitly for robust cross-modal representation under conditions of missing modalities. Specifically, PROMISE innovatively incorporates multimodal prompt learning into a hierarchical contrastive learning framework, equipped with a specially designed prompt-attention mechanism. This mechanism dynamically generates robust and consistent representations for scenarios where particular modalities are absent, thereby effectively bridging the representational gap between complete and incomplete data. Extensive experiments conducted on benchmark datasets, along with comprehensive ablation studies, clearly demonstrate the superior performance of PROMISE compared to current state-of-the-art multimodal methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisMem: Latent Vision Memory Unlocks Potential of Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.11007</link>
<guid>https://arxiv.org/abs/2511.11007</guid>
<content:encoded><![CDATA[

arXiv:2511.11007v1 Announce Type: cross 
Abstract: Despite the remarkable success of Vision-Language Models (VLMs), their performance on a range of complex visual tasks is often hindered by a "visual processing bottleneck": a propensity to lose grounding in visual evidence and exhibit a deficit in contextualized visual experience during prolonged generation. Drawing inspiration from human cognitive memory theory, which distinguishes short-term visually-dominant memory and long-term semantically-dominant memory, we propose VisMem, a cognitively-aligned framework that equips VLMs with dynamic latent vision memories, a short-term module for fine-grained perceptual retention and a long-term module for abstract semantic consolidation. These memories are seamlessly invoked during inference, allowing VLMs to maintain both perceptual fidelity and semantic consistency across thinking and generation. Extensive experiments across diverse visual benchmarks for understanding, reasoning, and generation reveal that VisMem delivers a significant average performance boost of 11.8% relative to the vanilla model and outperforms all counterparts, establishing a new paradigm for latent-space memory enhancement. The code will be available: https://github.com/YU-deep/VisMem.git.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automata-Based Steering of Large Language Models for Diverse Structured Generation</title>
<link>https://arxiv.org/abs/2511.11018</link>
<guid>https://arxiv.org/abs/2511.11018</guid>
<content:encoded><![CDATA[

arXiv:2511.11018v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly tasked with generating structured outputs. While structured generation methods ensure validity, they often lack output diversity, a critical limitation that we confirm in our preliminary study. We propose a novel method to enhance diversity in automaton-based structured generation. Our approach utilizes automata traversal history to steer LLMs towards novel structural patterns. Evaluations show our method significantly improves structural and content diversity while maintaining comparable generation efficiency. Furthermore, we conduct a case study showcasing the effectiveness of our method in generating diverse test cases for testing open-source libraries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB</title>
<link>https://arxiv.org/abs/2511.11041</link>
<guid>https://arxiv.org/abs/2511.11041</guid>
<content:encoded><![CDATA[

arXiv:2511.11041v1 Announce Type: cross 
Abstract: We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\tilde{e} + \mu$, where $\mu$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $\sigma$ on retrieval tasks, 3.1 $\sigma$ on classification tasks, and 0.8 $\sigma$ on other types of tasks. Renormalization has two variants: directly subtracting $\mu$ from $e$, or subtracting the projection of $e$ onto $\mu$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PINGS-X: Physics-Informed Normalized Gaussian Splatting with Axes Alignment for Efficient Super-Resolution of 4D Flow MRI</title>
<link>https://arxiv.org/abs/2511.11048</link>
<guid>https://arxiv.org/abs/2511.11048</guid>
<content:encoded><![CDATA[

arXiv:2511.11048v1 Announce Type: cross 
Abstract: 4D flow magnetic resonance imaging (MRI) is a reliable, non-invasive approach for estimating blood flow velocities, vital for cardiovascular diagnostics. Unlike conventional MRI focused on anatomical structures, 4D flow MRI requires high spatiotemporal resolution for early detection of critical conditions such as stenosis or aneurysms. However, achieving such resolution typically results in prolonged scan times, creating a trade-off between acquisition speed and prediction accuracy. Recent studies have leveraged physics-informed neural networks (PINNs) for super-resolution of MRI data, but their practical applicability is limited as the prohibitively slow training process must be performed for each patient. To overcome this limitation, we propose PINGS-X, a novel framework modeling high-resolution flow velocities using axes-aligned spatiotemporal Gaussian representations. Inspired by the effectiveness of 3D Gaussian splatting (3DGS) in novel view synthesis, PINGS-X extends this concept through several non-trivial novel innovations: (i) normalized Gaussian splatting with a formal convergence guarantee, (ii) axes-aligned Gaussians that simplify training for high-dimensional data while preserving accuracy and the convergence guarantee, and (iii) a Gaussian merging procedure to prevent degenerate solutions and boost computational efficiency. Experimental results on computational fluid dynamics (CFD) and real 4D flow MRI datasets demonstrate that PINGS-X substantially reduces training time while achieving superior super-resolution accuracy. Our code and datasets are available at https://github.com/SpatialAILab/PINGS-X.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VIDEOP2R: Video Understanding from Perception to Reasoning</title>
<link>https://arxiv.org/abs/2511.11113</link>
<guid>https://arxiv.org/abs/2511.11113</guid>
<content:encoded><![CDATA[

arXiv:2511.11113v1 Announce Type: cross 
Abstract: Reinforcement fine-tuning (RFT), a two-stage framework consisting of supervised fine-tuning (SFT) and reinforcement learning (RL) has shown promising results on improving reasoning ability of large language models (LLMs). Yet extending RFT to large video language models (LVLMs) remains challenging. We propose VideoP2R, a novel process-aware video RFT framework that enhances video reasoning by modeling perception and reasoning as distinct processes. In the SFT stage, we develop a three-step pipeline to generate VideoP2R-CoT-162K, a high-quality, process-aware chain-of-thought (CoT) dataset for perception and reasoning. In the RL stage, we introduce a novel process-aware group relative policy optimization (PA-GRPO) algorithm that supplies separate rewards for perception and reasoning. Extensive experiments show that VideoP2R achieves state-of-the-art (SotA) performance on six out of seven video reasoning and understanding benchmarks. Ablation studies further confirm the effectiveness of our process-aware modeling and PA-GRPO and demonstrate that model's perception output is information-sufficient for downstream reasoning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Shot Transfer Learning for Nonlinear PDEs with Perturbative PINNs</title>
<link>https://arxiv.org/abs/2511.11137</link>
<guid>https://arxiv.org/abs/2511.11137</guid>
<content:encoded><![CDATA[

arXiv:2511.11137v1 Announce Type: cross 
Abstract: We propose a framework for solving nonlinear partial differential equations (PDEs) by combining perturbation theory with one-shot transfer learning in Physics-Informed Neural Networks (PINNs). Nonlinear PDEs with polynomial terms are decomposed into a sequence of linear subproblems, which are efficiently solved using a Multi-Head PINN. Once the latent representation of the linear operator is learned, solutions to new PDE instances with varying perturbations, forcing terms, or boundary/initial conditions can be obtained in closed form without retraining.
  We validate the method on KPP-Fisher and wave equations, achieving errors on the order of 1e-3 while adapting to new problem instances in under 0.2 seconds; comparable accuracy to classical solvers but with faster transfer. Sensitivity analyses show predictable error growth with epsilon and polynomial degree, clarifying the method's effective regime.
  Our contributions are: (i) extending one-shot transfer learning from nonlinear ODEs to PDEs, (ii) deriving a closed-form solution for adapting to new PDE instances, and (iii) demonstrating accuracy and efficiency on canonical nonlinear PDEs. We conclude by outlining extensions to derivative-dependent nonlinearities and higher-dimensional PDEs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</title>
<link>https://arxiv.org/abs/2511.11141</link>
<guid>https://arxiv.org/abs/2511.11141</guid>
<content:encoded><![CDATA[

arXiv:2511.11141v1 Announce Type: cross 
Abstract: Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drift Estimation for Diffusion Processes Using Neural Networks Based on Discretely Observed Independent Paths</title>
<link>https://arxiv.org/abs/2511.11161</link>
<guid>https://arxiv.org/abs/2511.11161</guid>
<content:encoded><![CDATA[

arXiv:2511.11161v1 Announce Type: cross 
Abstract: This paper addresses the nonparametric estimation of the drift function over a compact domain for a time-homogeneous diffusion process, based on high-frequency discrete observations from $N$ independent trajectories. We propose a neural network-based estimator and derive a non-asymptotic convergence rate, decomposed into a training error, an approximation error, and a diffusion-related term scaling as ${\log N}/{N}$. For compositional drift functions, we establish an explicit rate. In the numerical experiments, we consider a drift function with local fluctuations generated by a double-layer compositional structure featuring local oscillations, and show that the empirical convergence rate becomes independent of the input dimension $d$. Compared to the $B$-spline method, the neural network estimator achieves better convergence rates and more effectively captures local features, particularly in higher-dimensional settings.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</title>
<link>https://arxiv.org/abs/2511.11169</link>
<guid>https://arxiv.org/abs/2511.11169</guid>
<content:encoded><![CDATA[

arXiv:2511.11169v1 Announce Type: cross 
Abstract: In the context of Visual Question Answering (VQA) and Agentic AI, calibration refers to how closely an AI system's confidence in its answers reflects their actual correctness. This aspect becomes especially important when such systems operate autonomously and must make decisions under visual uncertainty. While modern VQA systems, powered by advanced vision-language models (VLMs), are increasingly used in high-stakes domains like medical diagnostics and autonomous navigation due to their improved accuracy, the reliability of their confidence estimates remains under-examined. Particularly, these systems often produce overconfident responses. To address this, we introduce AlignVQA, a debate-based multi-agent framework, in which diverse specialized VLM -- each following distinct prompting strategies -- generate candidate answers and then engage in two-stage interaction: generalist agents critique, refine and aggregate these proposals. This debate process yields confidence estimates that more accurately reflect the model's true predictive performance. We find that more calibrated specialized agents produce better aligned confidences. Furthermore, we introduce a novel differentiable calibration-aware loss function called aligncal designed to fine-tune the specialized agents by minimizing an upper bound on the calibration error. This objective explicitly improves the fidelity of each agent's confidence estimates. Empirical results across multiple benchmark VQA datasets substantiate the efficacy of our approach, demonstrating substantial reductions in calibration discrepancies. Furthermore, we propose a novel differentiable calibration-aware loss to fine-tune the specialized agents and improve the quality of their individual confidence estimates based on minimising upper bound calibration error.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Questioning the Stability of Visual Question Answering</title>
<link>https://arxiv.org/abs/2511.11206</link>
<guid>https://arxiv.org/abs/2511.11206</guid>
<content:encoded><![CDATA[

arXiv:2511.11206v1 Announce Type: cross 
Abstract: Visual Language Models (VLMs) have achieved remarkable progress, yet their reliability under small, meaning-preserving input changes remains poorly understood. We present the first large-scale, systematic study of VLM robustness to benign visual and textual perturbations: pixel-level shifts, light geometric transformations, padded rescaling, paraphrasing, and multilingual rewrites that do not alter the underlying semantics of an image-question pair. Across a broad set of models and datasets, we find that modern VLMs are highly sensitive to such minor perturbations: a substantial fraction of samples change their predicted answer under at least one visual or textual modification. We characterize how this instability varies across perturbation types, question categories, and models, revealing that even state-of-the-art systems (e.g., GPT-4o, Gemini 2.0 Flash) frequently fail under shifts as small as a few pixels or harmless rephrasings. We further show that sample-level stability serves as a strong indicator of correctness: stable samples are consistently far more likely to be answered correctly. Leveraging this, we demonstrate that the stability patterns of small, accessible open-source models can be used to predict the correctness of much larger closed-source models with high precision. Our findings expose a fundamental fragility in current VLMs and highlight the need for robustness evaluations that go beyond adversarial perturbations, focusing instead on invariances that models should reliably uphold.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AIonopedia: an LLM agent orchestrating multimodal learning for ionic liquid discovery</title>
<link>https://arxiv.org/abs/2511.11257</link>
<guid>https://arxiv.org/abs/2511.11257</guid>
<content:encoded><![CDATA[

arXiv:2511.11257v1 Announce Type: cross 
Abstract: The discovery of novel Ionic Liquids (ILs) is hindered by critical challenges in property prediction, including limited data, poor model accuracy, and fragmented workflows. Leveraging the power of Large Language Models (LLMs), we introduce AIonopedia, to the best of our knowledge, the first LLM agent for IL discovery. Powered by an LLM-augmented multimodal domain foundation model for ILs, AIonopedia enables accurate property predictions and incorporates a hierarchical search architecture for molecular screening and design. Trained and evaluated on a newly curated and comprehensive IL dataset, our model delivers superior performance. Complementing these results, evaluations on literature-reported systems indicate that the agent can perform effective IL modification. Moving beyond offline tests, the practical efficacy was further confirmed through real-world wet-lab validation, in which the agent demonstrated exceptional generalization capabilities on challenging out-of-distribution tasks, underscoring its ability to accelerate real-world IL discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint</title>
<link>https://arxiv.org/abs/2511.11294</link>
<guid>https://arxiv.org/abs/2511.11294</guid>
<content:encoded><![CDATA[

arXiv:2511.11294v1 Announce Type: cross 
Abstract: Linear models are widely used in high-stakes decision-making due to their simplicity and interpretability. Yet when fairness constraints such as demographic parity are introduced, their effects on model coefficients, and thus on how predictive bias is distributed across features, remain opaque. Existing approaches on linear models often rely on strong and unrealistic assumptions, or overlook the explicit role of the sensitive attribute, limiting their practical utility for fairness assessment. We extend the work of (Chzhen and Schreuder, 2022) and (Fukuchi and Sakuma, 2023) by proposing a post-processing framework that can be applied on top of any linear model to decompose the resulting bias into direct (sensitive-attribute) and indirect (correlated-features) components. Our method analytically characterizes how demographic parity reshapes each model coefficient, including those of both sensitive and non-sensitive features. This enables a transparent, feature-level interpretation of fairness interventions and reveals how bias may persist or shift through correlated variables. Our framework requires no retraining and provides actionable insights for model auditing and mitigation. Experiments on both synthetic and real-world datasets demonstrate that our method captures fairness dynamics missed by prior work, offering a practical and interpretable tool for responsible deployment of linear models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising</title>
<link>https://arxiv.org/abs/2511.11305</link>
<guid>https://arxiv.org/abs/2511.11305</guid>
<content:encoded><![CDATA[

arXiv:2511.11305v1 Announce Type: cross 
Abstract: We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of "Pretraining, Post-training, and Application", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large-scale modality-invariant foundation models for brain MRI analysis: Application to lesion segmentation</title>
<link>https://arxiv.org/abs/2511.11311</link>
<guid>https://arxiv.org/abs/2511.11311</guid>
<content:encoded><![CDATA[

arXiv:2511.11311v1 Announce Type: cross 
Abstract: The field of computer vision is undergoing a paradigm shift toward large-scale foundation model pre-training via self-supervised learning (SSL). Leveraging large volumes of unlabeled brain MRI data, such models can learn anatomical priors that improve few-shot performance in diverse neuroimaging tasks. However, most SSL frameworks are tailored to natural images, and their adaptation to capture multi-modal MRI information remains underexplored. This work proposes a modality-invariant representation learning setup and evaluates its effectiveness in stroke and epilepsy lesion segmentation, following large-scale pre-training. Experimental results suggest that despite successful cross-modality alignment, lesion segmentation primarily benefits from preserving fine-grained modality-specific features. Model checkpoints and code are made publicly available.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2511.11320</link>
<guid>https://arxiv.org/abs/2511.11320</guid>
<content:encoded><![CDATA[

arXiv:2511.11320v1 Announce Type: cross 
Abstract: Spiking Neural Networks (SNNs) promise energy-efficient, sparse, biologically inspired computation. Training them with Backpropagation Through Time (BPTT) and surrogate gradients achieves strong performance but remains biologically implausible. Equilibrium Propagation (EP) provides a more local and biologically grounded alternative. However, existing EP frameworks, primarily based on deterministic neurons, either require complex mechanisms to handle discontinuities in spiking dynamics or fail to scale beyond simple visual tasks. Inspired by the stochastic nature of biological spiking mechanism and recent hardware trends, we propose a stochastic EP framework that integrates probabilistic spiking neurons into the EP paradigm. This formulation smoothens the optimization landscape, stabilizes training, and enables scalable learning in deep convolutional spiking convergent recurrent neural networks (CRNNs). We provide theoretical guarantees showing that the proposed stochastic EP dynamics approximate deterministic EP under mean-field theory, thereby inheriting its underlying theoretical guarantees. The proposed framework narrows the gap to both BPTT-trained SNNs and EP-trained non-spiking CRNNs in vision benchmarks while preserving locality, highlighting stochastic EP as a promising direction for neuromorphic and on-chip learning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Security Evaluation of Wi-Fi CSI Biometrics: Attacks, Metrics, and Systemic Weaknesses</title>
<link>https://arxiv.org/abs/2511.11381</link>
<guid>https://arxiv.org/abs/2511.11381</guid>
<content:encoded><![CDATA[

arXiv:2511.11381v1 Announce Type: cross 
Abstract: Wi-Fi Channel State Information (CSI) has been repeatedly proposed as a biometric modality, often with reports of high accuracy and operational feasibility. However, the field lacks a consolidated understanding of its security properties, adversarial resilience, and methodological consistency. This Systematization of Knowledge (SoK) examines CSI-based biometric authentication through a security perspective, analyzing how existing work differs across sensing infrastructure, signal representations, feature pipelines, learning models, and evaluation methodologies. Our synthesis reveals systemic inconsistencies: reliance on aggregate accuracy metrics, limited reporting of FAR/FRR/EER, absence of per-user risk analysis, and scarce consideration of threat models or adversarial feasibility. We construct a unified evaluation framework to empirically expose these issues and demonstrate how security-relevant metrics, such as per-class EER, FCS, and the Gini Coefficient, uncover risk concentration that remains hidden under traditional reporting practices. Our analysis highlights concrete attack surfaces and shows how methodological choices materially influence vulnerability profiles, which include replay, geometric mimicry, and environmental perturbation. Based on these findings, we articulate the security boundaries of current CSI biometrics and provide guidelines for rigorous evaluation, reproducible experimentation, and future research directions. This SoK offers the security community a structured, evidence-driven reassessment of Wi-Fi CSI biometrics and their suitability as an authentication primitive.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BOFA: Bridge-Layer Orthogonal Low-Rank Fusion for CLIP-Based Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2511.11421</link>
<guid>https://arxiv.org/abs/2511.11421</guid>
<content:encoded><![CDATA[

arXiv:2511.11421v1 Announce Type: cross 
Abstract: Class-Incremental Learning (CIL) aims to continually learn new categories without forgetting previously acquired knowledge. Vision-language models such as CLIP offer strong transferable representations via multi-modal supervision, making them promising for CIL. However, applying CLIP to CIL poses two major challenges: (1) adapting to downstream tasks often requires additional learnable modules, increasing model complexity and susceptibility to forgetting; and (2) while multi-modal representations offer complementary strengths, existing methods have yet to fully realize their potential in effectively integrating visual and textual modalities. To address these issues, we propose BOFA (Bridge-layer Orthogonal Fusion for Adaptation), a novel framework for CIL. BOFA confines all model adaptation exclusively to CLIP's existing cross-modal bridge-layer, thereby adding no extra parameters or inference cost. To prevent forgetting within this layer, it leverages Orthogonal Low-Rank Fusion, a mechanism that constrains parameter updates to a low-rank ``safe subspace" mathematically constructed to be orthogonal to past task features. This ensures stable knowledge accumulation without data replay. Furthermore, BOFA employs a cross-modal hybrid prototype that synergizes stable textual prototypes with visual counterparts derived from our stably adapted bridge-layer, enhancing classification performance. Extensive experiments on standard benchmarks show that BOFA achieves superior accuracy and efficiency compared to existing methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2511.11450</link>
<guid>https://arxiv.org/abs/2511.11450</guid>
<content:encoded><![CDATA[

arXiv:2511.11450v1 Announce Type: cross 
Abstract: We introduce VoxTell, a vision-language model for text-prompted volumetric medical image segmentation. It maps free-form descriptions, from single words to full clinical sentences, to 3D masks. Trained on 62K+ CT, MRI, and PET volumes spanning over 1K anatomical and pathological classes, VoxTell uses multi-stage vision-language fusion across decoder layers to align textual and visual features at multiple scales. It achieves state-of-the-art zero-shot performance across modalities on unseen datasets, excelling on familiar concepts while generalizing to related unseen classes. Extensive experiments further demonstrate strong cross-modality transfer, robustness to linguistic variations and clinical language, as well as accurate instance-specific segmentation from real-world text. Code is available at: https://www.github.com/MIC-DKFZ/VoxTell
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer</title>
<link>https://arxiv.org/abs/2511.11452</link>
<guid>https://arxiv.org/abs/2511.11452</guid>
<content:encoded><![CDATA[

arXiv:2511.11452v1 Announce Type: cross 
Abstract: Multimodal deep learning (MDL) has emerged as a transformative approach in computational pathology. By integrating complementary information from multiple data sources, MDL models have demonstrated superior predictive performance across diverse clinical tasks compared to unimodal models. However, the assumption that combining modalities inherently improves performance remains largely unexamined. We hypothesise that multimodal gains depend critically on the predictive quality of individual modalities, and that integrating weak modalities may introduce noise rather than complementary information. We test this hypothesis on a prostate cancer dataset with histopathology, radiology, and clinical data to predict time-to-biochemical recurrence. Our results confirm that combining high-performing modalities yield superior performance compared to unimodal approaches. However, integrating a poor-performing modality with other higher-performing modalities degrades predictive accuracy. These findings demonstrate that multimodal benefit requires selective, performance-guided integration rather than indiscriminate modality combination, with implications for MDL design across computational pathology and medical imaging.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Intrusion Detection for Evolving RPL IoT Attacks Using Incremental Learning</title>
<link>https://arxiv.org/abs/2511.11464</link>
<guid>https://arxiv.org/abs/2511.11464</guid>
<content:encoded><![CDATA[

arXiv:2511.11464v1 Announce Type: cross 
Abstract: The routing protocol for low-power and lossy networks (RPL) has become the de facto routing standard for resource-constrained IoT systems, but its lightweight design exposes critical vulnerabilities to a wide range of routing-layer attacks such as hello flood, decreased rank, and version number manipulation. Traditional countermeasures, including protocol-level modifications and machine learning classifiers, can achieve high accuracy against known threats, yet they fail when confronted with novel or zero-day attacks unless fully retrained, an approach that is impractical for dynamic IoT environments. In this paper, we investigate incremental learning as a practical and adaptive strategy for intrusion detection in RPL-based networks. We systematically evaluate five model families, including ensemble models and deep learning models. Our analysis highlights that incremental learning not only restores detection performance on new attack classes but also mitigates catastrophic forgetting of previously learned threats, all while reducing training time compared to full retraining. By combining five diverse models with attack-specific analysis, forgetting behavior, and time efficiency, this study provides systematic evidence that incremental learning offers a scalable pathway to maintain resilient intrusion detection in evolving RPL-based IoT networks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Euclidean SGD for Structured Optimization: Unified Analysis and Improved Rates</title>
<link>https://arxiv.org/abs/2511.11466</link>
<guid>https://arxiv.org/abs/2511.11466</guid>
<content:encoded><![CDATA[

arXiv:2511.11466v1 Announce Type: cross 
Abstract: Recently, several instances of non-Euclidean SGD, including SignSGD, Lion, and Muon, have attracted significant interest from the optimization community due to their practical success in training deep neural networks. Consequently, a number of works have attempted to explain this success by developing theoretical convergence analyses. Unfortunately, these results cannot properly justify the superior performance of these methods, as they could not beat the convergence rate of vanilla Euclidean SGD. We resolve this important open problem by developing a new unified convergence analysis under the structured smoothness and gradient noise assumption. In particular, our results indicate that non-Euclidean SGD (i) can exploit the sparsity or low-rank structure of the upper bounds on the Hessian and gradient noise, (ii) can provably benefit from popular algorithmic tools such as extrapolation or momentum variance reduction, and (iii) can match the state-of-the-art convergence rates of adaptive and more complex optimization algorithms such as AdaGrad and Shampoo.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring response times of perceptual decisions with Poisson variational autoencoders</title>
<link>https://arxiv.org/abs/2511.11480</link>
<guid>https://arxiv.org/abs/2511.11480</guid>
<content:encoded><![CDATA[

arXiv:2511.11480v1 Announce Type: cross 
Abstract: Many properties of perceptual decision making are well-modeled by deep neural networks. However, such architectures typically treat decisions as instantaneous readouts, overlooking the temporal dynamics of the decision process. We present an image-computable model of perceptual decision making in which choices and response times arise from efficient sensory encoding and Bayesian decoding of neural spiking activity. We use a Poisson variational autoencoder to learn unsupervised representations of visual stimuli in a population of rate-coded neurons, modeled as independent homogeneous Poisson processes. A task-optimized decoder then continually infers an approximate posterior over actions conditioned on incoming spiking activity. Combining these components with an entropy-based stopping rule yields a principled and image-computable model of perceptual decisions capable of generating trial-by-trial patterns of choices and response times. Applied to MNIST digit classification, the model reproduces key empirical signatures of perceptual decision making, including stochastic variability, right-skewed response time distributions, logarithmic scaling of response times with the number of alternatives (Hick's law), and speed-accuracy trade-offs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning and Testing Convex Functions</title>
<link>https://arxiv.org/abs/2511.11498</link>
<guid>https://arxiv.org/abs/2511.11498</guid>
<content:encoded><![CDATA[

arXiv:2511.11498v1 Announce Type: cross 
Abstract: We consider the problems of \emph{learning} and \emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.
  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:
  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\varepsilon$ using $n^{O(1/\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\mathrm{poly}(1/\varepsilon)}$ samples in the \emph{correlational statistical query (CSQ)} model.
  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\sqrt{n}/\varepsilon)^n$ samples.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experience-Guided Adaptation of Inference-Time Reasoning Strategies</title>
<link>https://arxiv.org/abs/2511.11519</link>
<guid>https://arxiv.org/abs/2511.11519</guid>
<content:encoded><![CDATA[

arXiv:2511.11519v1 Announce Type: cross 
Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CVChess: A Deep Learning Framework for Converting Chessboard Images to Forsyth-Edwards Notation</title>
<link>https://arxiv.org/abs/2511.11522</link>
<guid>https://arxiv.org/abs/2511.11522</guid>
<content:encoded><![CDATA[

arXiv:2511.11522v1 Announce Type: cross 
Abstract: Chess has experienced a large increase in viewership since the pandemic, driven largely by the accessibility of online learning platforms. However, no equivalent assistance exists for physical chess games, creating a divide between analog and digital chess experiences. This paper presents CVChess, a deep learning framework for converting chessboard images to Forsyth-Edwards Notation (FEN), which is later input into online chess engines to provide you with the best next move. Our approach employs a convolutional neural network (CNN) with residual layers to perform piece recognition from smartphone camera images. The system processes RGB images of a physical chess board through a multistep process: image preprocessing using the Hough Line Transform for edge detection, projective transform to achieve a top-down board alignment, segmentation into 64 individual squares, and piece classification into 13 classes (6 unique white pieces, 6 unique black pieces and an empty square) using the residual CNN. Residual connections help retain low-level visual features while enabling deeper feature extraction, improving accuracy and stability during training. We train and evaluate our model using the Chess Recognition Dataset (ChessReD), containing 10,800 annotated smartphone images captured under diverse lighting conditions and angles. The resulting classifications are encoded as an FEN string, which can be fed into a chess engine to generate the most optimal move
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Total Effects in Bipartite Experiments with Spillovers and Partial Eligibility</title>
<link>https://arxiv.org/abs/2511.11564</link>
<guid>https://arxiv.org/abs/2511.11564</guid>
<content:encoded><![CDATA[

arXiv:2511.11564v1 Announce Type: cross 
Abstract: We study randomized experiments in bipartite systems where only a subset of treatment-side units are eligible for assignment while all units continue to interact, generating interference. We formalize eligibility-constrained bipartite experiments and define estimands aligned with full deployment: the Primary Total Treatment Effect (PTTE) on eligible units and the Secondary Total Treatment Effect (STTE) on ineligible units. Under randomization within the eligible set, we give identification conditions and develop interference-aware ensemble estimators that combine exposure mappings, generalized propensity scores, and flexible machine learning. We further introduce a projection that links treatment- and outcome-level estimands; this mapping is exact under a Linear Additive Edges condition and enables estimation on the (typically much smaller) treatment side with deterministic aggregation to outcomes. In simulations with known ground truth across realistic exposure regimes, the proposed estimators recover PTTE and STTE with low bias and variance and reduce the bias that could arise when interference is ignored. Two field experiments illustrate practical relevance: our method corrects the direction of expected interference bias for a pre-specified metric in both studies and reverses the sign and significance of the primary decision metric in one case.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On bounds for norms of reparameterized ReLU artificial neural network parameters: sums of fractional powers of the Lipschitz norm control the network parameter vector</title>
<link>https://arxiv.org/abs/2206.13646</link>
<guid>https://arxiv.org/abs/2206.13646</guid>
<content:encoded><![CDATA[

arXiv:2206.13646v2 Announce Type: replace 
Abstract: It is an elementary fact in the scientific literature that the Lipschitz norm of the realization function of a feedforward fully-connected rectified linear unit (ReLU) artificial neural network (ANN) can, up to a multiplicative constant, be bounded from above by sums of powers of the norm of the ANN parameter vector. Roughly speaking, in this work we reveal in the case of shallow ANNs that the converse inequality is also true. More formally, we prove that the norm of the equivalence class of ANN parameter vectors with the same realization function is, up to a multiplicative constant, bounded from above by the sum of powers of the Lipschitz norm of the ANN realization function (with the exponents $ 1/2 $ and $ 1 $). Moreover, we prove that this upper bound only holds when employing the Lipschitz norm but does neither hold for H\"older norms nor for Sobolev-Slobodeckij norms. Furthermore, we prove that this upper bound only holds for sums of powers of the Lipschitz norm with the exponents $ 1/2 $ and $ 1 $ but does not hold for the Lipschitz norm alone.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Relationship Between Adversarial Robustness and Decision Region in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2207.03400</link>
<guid>https://arxiv.org/abs/2207.03400</guid>
<content:encoded><![CDATA[

arXiv:2207.03400v3 Announce Type: replace 
Abstract: In general, Deep Neural Networks (DNNs) are evaluated by the generalization performance measured on unseen data excluded from the training phase. Along with the development of DNNs, the generalization performance converges to the state-of-the-art and it becomes difficult to evaluate DNNs solely based on this metric. The robustness against adversarial attack has been used as an additional metric to evaluate DNNs by measuring their vulnerability. However, few studies have been performed to analyze the adversarial robustness in terms of the geometry in DNNs. In this work, we perform an empirical study to analyze the internal properties of DNNs that affect model robustness under adversarial attacks. In particular, we propose the novel concept of the Populated Region Set (PRS), where training samples are populated more frequently, to represent the internal properties of DNNs in a practical setting. From systematic experiments with the proposed concept, we provide empirical evidence to validate that a low PRS ratio has a strong relationship with the adversarial robustness of DNNs. We also devise PRS regularizer leveraging the characteristics of PRS to improve the adversarial robustness without adversarial training.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Higher-order Neural Additive Models: An Interpretable Machine Learning Model with Feature Interactions</title>
<link>https://arxiv.org/abs/2209.15409</link>
<guid>https://arxiv.org/abs/2209.15409</guid>
<content:encoded><![CDATA[

arXiv:2209.15409v2 Announce Type: replace 
Abstract: Neural Additive Models (NAMs) have recently demonstrated promising predictive performance while maintaining interpretability. However, their capacity is limited to capturing only first-order feature interactions, which restricts their effectiveness on real-world datasets. To address this limitation, we propose Higher-order Neural Additive Models (HONAMs), an interpretable machine learning model that effectively and efficiently captures feature interactions of arbitrary orders. HONAMs improve predictive accuracy without compromising interpretability, an essential requirement in high-stakes applications. This advantage of HONAM can help analyze and extract high-order interactions present in datasets. The source code for HONAM is publicly available at https://github.com/gim4855744/HONAM/.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination</title>
<link>https://arxiv.org/abs/2311.02960</link>
<guid>https://arxiv.org/abs/2311.02960</guid>
<content:encoded><![CDATA[

arXiv:2311.02960v4 Announce Type: replace 
Abstract: Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is nearly orthogonal and the network weights are minimum-norm, balanced, and approximate low-rank: Each layer of the linear network progressively compresses within-class features at a geometric rate and discriminates between-class features at a linear rate with respect to the number of layers that data have passed through. To the best of our knowledge, this is the first quantitative characterization of feature evolution in hierarchical representations of deep linear networks. Empirically, our extensive experiments not only validate our theoretical results numerically but also reveal a similar pattern in deep nonlinear networks which aligns well with recent empirical studies. Moreover, we demonstrate the practical implications of our results in transfer learning. Our code is available at https://github.com/Heimine/PNC_DLN.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoPE: Mixture of Prompt Experts for Parameter-Efficient and Scalable Multimodal Fusion</title>
<link>https://arxiv.org/abs/2403.10568</link>
<guid>https://arxiv.org/abs/2403.10568</guid>
<content:encoded><![CDATA[

arXiv:2403.10568v4 Announce Type: replace 
Abstract: Despite the demonstrated parameter efficiency of prompt-based fusion, its limited adaptivity and expressiveness hinder its effectiveness for multimodal applications at scale. In this paper, we present the first comprehensive study addressing these limitations. Our key motivation is to ``divide and conquer'' the vanilla prompt, traditionally shared across all instances, by generating instance-specific prompts. Specifically, we propose the Mixture of Prompt Experts (MoPE), a framework that significantly enhances prompt adaptivity and expressiveness by dynamically generating instance-specific prompts. MoPE leverages multimodal pairings as additional evidence, allowing the model to adaptively select optimal prompts tailored to each individual instance. Unlike traditional prompt-fusion methods, which encounter scalability bottlenecks when optimizing long unified prompts, MoPE maintains fixed prompt length while effectively scaling the number of specialized experts. Moreover, we investigate regularization terms to encourage expert specialization, resulting in highly adaptive and interpretable prompting. MoPE fundamentally changes the scaling dynamic, unlocking greater expressiveness and adaptability to complex multimodal relationships, enabling the model to selectively attend to task-relevant sub-sequences based on instance-specific multimodal input. Extensive experiments across six multimodal datasets spanning four modalities demonstrate state-of-the-art performance for multimodal fusion, matching or surpassing the performance of fine-tuning while requiring only 0.8% of the trainable parameters. Code is available: https://github.com/songrise/MoPE.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial Information Decomposition for Data Interpretability and Feature Selection</title>
<link>https://arxiv.org/abs/2405.19212</link>
<guid>https://arxiv.org/abs/2405.19212</guid>
<content:encoded><![CDATA[

arXiv:2405.19212v4 Announce Type: replace 
Abstract: In this paper, we introduce Partial Information Decomposition of Features (PIDF), a new paradigm for simultaneous data interpretability and feature selection. Contrary to traditional methods that assign a single importance value, our approach is based on three metrics per feature: the mutual information shared with the target variable, the feature's contribution to synergistic information, and the amount of this information that is redundant. In particular, we develop a novel procedure based on these three metrics, which reveals not only how features are correlated with the target but also the additional and overlapping information provided by considering them in combination with other features. We extensively evaluate PIDF using both synthetic and real-world data, demonstrating its potential applications and effectiveness, by considering case studies from genetics and neuroscience.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Posterior Label Smoothing for Node Classification</title>
<link>https://arxiv.org/abs/2406.00410</link>
<guid>https://arxiv.org/abs/2406.00410</guid>
<content:encoded><![CDATA[

arXiv:2406.00410v2 Announce Type: replace 
Abstract: Label smoothing is a widely studied regularization technique in machine learning. However, its potential for node classification in graph-structured data, spanning homophilic to heterophilic graphs, remains largely unexplored. We introduce posterior label smoothing, a novel method for transductive node classification that derives soft labels from a posterior distribution conditioned on neighborhood labels. The likelihood and prior distributions are estimated from the global statistics of the graph structure, allowing our approach to adapt naturally to various graph properties. We evaluate our method on 10 benchmark datasets using eight baseline models, demonstrating consistent improvements in classification accuracy. The following analysis demonstrates that soft labels mitigate overfitting during training, leading to better generalization performance, and that pseudo-labeling effectively refines the global label statistics of the graph. Our code is available at https://github.com/ml-postech/PosteL.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Global Geometric Analysis of Maximal Coding Rate Reduction</title>
<link>https://arxiv.org/abs/2406.01909</link>
<guid>https://arxiv.org/abs/2406.01909</guid>
<content:encoded><![CDATA[

arXiv:2406.01909v2 Announce Type: replace 
Abstract: The maximal coding rate reduction (MCR$^2$) objective for learning structured and compact deep representations is drawing increasing attention, especially after its recent usage in the derivation of fully explainable and highly effective deep network architectures. However, it lacks a complete theoretical justification: only the properties of its global optima are known, and its global landscape has not been studied. In this work, we give a complete characterization of the properties of all its local and global optima, as well as other types of critical points. Specifically, we show that each (local or global) maximizer of the MCR$^2$ problem corresponds to a low-dimensional, discriminative, and diverse representation, and furthermore, each critical point of the objective is either a local maximizer or a strict saddle point. Such a favorable landscape makes MCR$^2$ a natural choice of objective for learning diverse and discriminative representations via first-order optimization methods. To validate our theoretical findings, we conduct extensive experiments on both synthetic and real data sets.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Formalizing Spuriousness of Biased Datasets Using Partial Information Decomposition</title>
<link>https://arxiv.org/abs/2407.00482</link>
<guid>https://arxiv.org/abs/2407.00482</guid>
<content:encoded><![CDATA[

arXiv:2407.00482v2 Announce Type: replace 
Abstract: Spuriousness arises when there is an association between two or more variables in a dataset that are not causally related. In this work, we propose an explainability framework to preemptively disentangle the nature of such spurious associations in a dataset before model training. We leverage a body of work in information theory called Partial Information Decomposition (PID) to decompose the total information about the target into four non-negative quantities, namely unique information (in core and spurious features, respectively), redundant information, and synergistic information. Our framework helps anticipate when the core or spurious feature is indispensable, when either suffices, and when both are jointly needed for an optimal classifier trained on the dataset. Next, we leverage this decomposition to propose a novel measure of the spuriousness of a dataset. We arrive at this measure systematically by examining several candidate measures, and demonstrating what they capture and miss through intuitive canonical examples and counterexamples. Our framework Spurious Disentangler consists of segmentation, dimensionality reduction, and estimation modules, with capabilities to specifically handle high-dimensional image data efficiently. Finally, we also perform empirical evaluation to demonstrate the trends of unique, redundant, and synergistic information, as well as our proposed spuriousness measure across $6$ benchmark datasets under various experimental settings. We observe an agreement between our preemptive measure of dataset spuriousness and post-training model generalization metrics such as worst-group accuracy, further supporting our proposition. The code is available at https://github.com/Barproda/spuriousness-disentangler.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Domain Adaptation for Offline Reinforcement Learning with Limited Samples</title>
<link>https://arxiv.org/abs/2408.12136</link>
<guid>https://arxiv.org/abs/2408.12136</guid>
<content:encoded><![CDATA[

arXiv:2408.12136v4 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) learns effective policies from a static target dataset. The performance of state-of-the-art offline RL algorithms notwithstanding, it relies on the size of the target dataset, and it degrades if limited samples in the target dataset are available, which is often the case in real-world applications. To address this issue, domain adaptation that leverages auxiliary samples from related source datasets (such as simulators) can be beneficial. However, establishing the optimal way to trade off the limited target dataset and the large-but-biased source dataset while ensuring provably theoretical guarantees remains an open challenge. To the best of our knowledge, this paper proposes the first framework that theoretically explores the impact of the weights assigned to each dataset on the performance of offline RL. In particular, we establish performance bounds and the existence of the optimal weight, which can be computed in closed form under simplifying assumptions. We also provide algorithmic guarantees in terms of convergence to a neighborhood of the optimum. Notably, these results depend on the quality of the source dataset and the number of samples in the target dataset. Our empirical results on the well-known offline Procgen benchmark substantiate the theoretical contributions in this work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study on Improving SimCLR's Nonlinear Projection Head using Pretrained Autoencoder Embeddings</title>
<link>https://arxiv.org/abs/2408.14514</link>
<guid>https://arxiv.org/abs/2408.14514</guid>
<content:encoded><![CDATA[

arXiv:2408.14514v2 Announce Type: replace 
Abstract: This paper focuses on improving the effectiveness of the standard 2-layer MLP projection head featured in the SimCLR framework through the use of pretrained autoencoder embeddings. Given a contrastive learning task with a largely unlabeled image classification dataset, we first train a shallow autoencoder architecture and extract its compressed representations contained in the encoder's embedding layer. After freezing the weights within this pretrained layer, we use it as a drop-in replacement for the input layer of SimCLR's default projector. Additionally, we also apply further architectural changes to the projector by decreasing its width and changing its activation function. The different projection heads are then used to contrastively train and evaluate a feature extractor following the SimCLR protocol. Our experiments indicate that using a pretrained autoencoder embedding in the projector can not only increase classification accuracy by up to 2.9% or 1.7% on average, but can also significantly decrease the dimensionality of the projection space. Our results also suggest, that using the sigmoid and tanh activation functions within the projector can outperform ReLU in terms of peak and average classification accuracy. All experiments involving our pretrained projectors are conducted with frozen embeddings, since our test results indicate an advantage compared to using their non-frozen counterparts.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Iterative Solvers for Constrained Optimization</title>
<link>https://arxiv.org/abs/2409.08066</link>
<guid>https://arxiv.org/abs/2409.08066</guid>
<content:encoded><![CDATA[

arXiv:2409.08066v2 Announce Type: replace 
Abstract: The real-time solution of parametric optimization problems is critical for applications that demand high accuracy under tight real-time constraints, such as model predictive control. To this end, this work presents a learning-based iterative solver for constrained optimization, comprising a neural network predictor that generates initial primal-dual solution estimates, followed by a learned iterative solver that refines these estimates to reach high accuracy. We introduce a novel loss function based on Karush-Kuhn-Tucker (KKT) optimality conditions, enabling fully self-supervised training without pre-sampled optimizer solutions. Theoretical guarantees ensure that the training loss function attains minima exclusively at KKT points. A convexification procedure enables application to nonconvex problems while preserving these guarantees. Experiments on two nonconvex case studies demonstrate speedups of up to one order of magnitude compared to state-of-the-art solvers such as IPOPT, while achieving orders of magnitude higher accuracy than competing learning-based approaches.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Control and Regret Analysis of Non-Stationary MDP with Look-ahead Information</title>
<link>https://arxiv.org/abs/2409.08434</link>
<guid>https://arxiv.org/abs/2409.08434</guid>
<content:encoded><![CDATA[

arXiv:2409.08434v2 Announce Type: replace 
Abstract: Policy design in non-stationary Markov Decision Processes (MDPs) is inherently challenging due to the complexities introduced by time-varying system transition and reward, which make it difficult for learners to determine the optimal actions for maximizing cumulative future rewards. Fortunately, in many practical applications, such as energy systems, look-ahead predictions are available, including forecasts for renewable energy generation and demand. In this paper, we leverage these look-ahead predictions and propose an algorithm designed to achieve low regret in non-stationary MDPs by incorporating such predictions. Our theoretical analysis demonstrates that, under certain assumptions, the regret decreases exponentially as the look-ahead window expands. When the system prediction is subject to error, the regret does not explode even if the prediction error grows sub-exponentially as a function of the prediction horizon. We validate our approach through simulations, confirming the efficacy of our algorithm in non-stationary environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Retrofitting</title>
<link>https://arxiv.org/abs/2410.11330</link>
<guid>https://arxiv.org/abs/2410.11330</guid>
<content:encoded><![CDATA[

arXiv:2410.11330v2 Announce Type: replace 
Abstract: AfterLearnER (After Learning Evolutionary Retrofitting) consists in applying evolutionary optimization to refine fully trained machine learning models by optimizing a set of carefully chosen parameters or hyperparameters of the model, with respect to some actual, exact, and hence possibly non-differentiable error signal, performed on a subset of the standard validation set. The efficiency of AfterLearnER is demonstrated by tackling non-differentiable signals such as threshold-based criteria in depth sensing, the word error rate in speech re-synthesis, the number of kills per life at Doom, computational accuracy or BLEU in code translation, image quality in 3D generative adversarial networks (GANs), and user feedback in image generation via Latent Diffusion Models (LDM). This retrofitting can be done after training, or dynamically at inference time by taking into account the user feedback. The advantages of AfterLearnER are its versatility, the possibility to use non-differentiable feedback, including human evaluations (i.e., no gradient is needed), the limited overfitting supported by a theoretical study, and its anytime behavior. Last but not least, AfterLearnER requires only a small amount of feedback, i.e., a few dozen to a few hundred scalars, compared to the tens of thousands needed in most related published works.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SGLP: A Similarity Guided Fast Layer Partition Pruning for Compressing Large Deep Models</title>
<link>https://arxiv.org/abs/2410.14720</link>
<guid>https://arxiv.org/abs/2410.14720</guid>
<content:encoded><![CDATA[

arXiv:2410.14720v2 Announce Type: replace 
Abstract: Layer pruning has emerged as a potent approach to remove redundant layers in the pre-trained network on the purpose of reducing network size and improve computational efficiency. However, existing layer pruning methods mostly overlook the intrinsic connections and inter-dependencies between different layers within complicated deep neural networks. This oversight can result in pruned models that do not preserve the essential characteristics of the pre-trained network as effectively as desired. To address these limitations, we propose a Similarity-Guided Layer Partition (SGLP) Pruning, a novel pruning framework that exploits representation similarity to guide efficient and informed layer removal for compressing large deep models. Our method begins by employing Centered Kernel Alignment (CKA) to quantify representational similarity between layers, uncovering structural patterns within the network. We then apply Fisher Optimal Segmentation on the similarity matrix to partition the network into semantically coherent layer segments. This segmentation allows pruning decisions to respect layer interdependencies and preserve essential knowledge. Within each segment, we introduce a fine-tuning-free importance evaluation using GradNorm, identifying and removing redundant layers in a targeted, segment-wise manner. Experimental results on both image classification tasks and large language models (LLMs) demonstrate that our proposed SGLP outperforms the state-of-the-art methods in accuracy and efficiency. Our approach achieves significant model compression with minimal performance degradation, making it well-suited for deployment in resource-limited environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strada-LLM: Graph LLM for traffic prediction</title>
<link>https://arxiv.org/abs/2410.20856</link>
<guid>https://arxiv.org/abs/2410.20856</guid>
<content:encoded><![CDATA[

arXiv:2410.20856v3 Announce Type: replace 
Abstract: Traffic forecasting is pivotal for intelligent transportation systems, where accurate and interpretable predictions can significantly enhance operational efficiency and safety. A key challenge stems from the heterogeneity of traffic conditions across diverse locations, leading to highly varied traffic data distributions. Large language models (LLMs) show exceptional promise for few-shot learning in such dynamic and data-sparse scenarios. However, existing LLM-based solutions often rely on prompt-tuning, which can struggle to fully capture complex graph relationships and spatiotemporal dependencies-thereby limiting adaptability and interpretability in real-world traffic networks. We address these gaps by introducing Strada-LLM, a novel multivariate probabilistic forecasting LLM that explicitly models both temporal and spatial traffic patterns. By incorporating proximal traffic information as covariates, Strada-LLM more effectively captures local variations and outperforms prompt-based existing LLMs. To further enhance adaptability, we propose a lightweight distribution-derived strategy for domain adaptation, enabling parameter-efficient model updates when encountering new data distributions or altered network topologies-even under few-shot constraints. Empirical evaluations on spatio-temporal transportation datasets demonstrate that Strada-LLM consistently surpasses state-of-the-art LLM-driven and traditional GNN-based predictors. Specifically, it improves long-term forecasting by 17% in RMSE error and 16% more efficiency. Moreover, it maintains robust performance across different LLM backbones with minimal degradation, making it a versatile and powerful solution for real-world traffic prediction tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Linear Bandits under Stochastic Latent Heterogeneity</title>
<link>https://arxiv.org/abs/2502.00423</link>
<guid>https://arxiv.org/abs/2502.00423</guid>
<content:encoded><![CDATA[

arXiv:2502.00423v2 Announce Type: replace 
Abstract: This paper addresses the critical challenge of stochastic latent heterogeneity in online decision-making, where individuals' responses to actions vary not only with observable contexts but also with unobserved, randomly realized subgroups. Existing data-driven approaches largely capture observable heterogeneity through contextual features but fail when the sources of variation are latent and stochastic. We propose a latent heterogeneous bandit framework that explicitly models probabilistic subgroup membership and group-specific reward functions, using promotion targeting as a motivating example. Our phased EM-greedy algorithm jointly learns latent group probabilities and reward parameters in high dimensions, achieving optimal estimation and classification guarantees. Our analysis reveals a new phenomenon unique to decision-making with stochastic latent subgroups: randomness in group realizations creates irreducible classification uncertainty, making sub-linear regret against a fully informed strong oracle fundamentally impossible. We establish matching upper and minimax lower bounds for both the strong and regular regrets, corresponding, respectively, to oracles with and without access to realized group memberships. The strong regret necessarily grows linearly, while the regular regret achieves a minimax-optimal sublinear rate. These findings uncover a fundamental stochastic barrier in online decision-making and point to potential remedies through simple strategic interventions and mechanism-design-based elicitation of latent information.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training speedups via batching for geometric learning: an analysis of static and dynamic algorithms</title>
<link>https://arxiv.org/abs/2502.00944</link>
<guid>https://arxiv.org/abs/2502.00944</guid>
<content:encoded><![CDATA[

arXiv:2502.00944v3 Announce Type: replace 
Abstract: Graph neural networks (GNN) have shown promising results for several domains such as materials science, chemistry, and the social sciences. GNN models often contain millions of parameters, and like other neural network (NN) models, are often fed only a fraction of the graphs that make up the training dataset in batches to update model parameters. The effect of batching algorithms on training time and model performance has been thoroughly explored for NNs but not yet for GNNs. We analyze two different batching algorithms for graph based models, namely static and dynamic batching for two datasets, the QM9 dataset of small molecules and the AFLOW materials database. Our experiments show that changing the batching algorithm can provide up to a 2.7x speedup, but the fastest algorithm depends on the data, model, batch size, hardware, and number of training steps run. Experiments show that for a select number of combinations of batch size, dataset, and model, significant differences in model learning metrics are observed between static and dynamic batching algorithms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapt-Pruner: Adaptive Structural Pruning for Efficient Small Language Model Training</title>
<link>https://arxiv.org/abs/2502.03460</link>
<guid>https://arxiv.org/abs/2502.03460</guid>
<content:encoded><![CDATA[

arXiv:2502.03460v3 Announce Type: replace 
Abstract: Small language models (SLMs) have attracted considerable attention from both academia and industry due to their broad range of applications in edge devices. To obtain SLMs with strong performance, conventional approaches either pre-train the models from scratch, which incurs substantial computational costs, or compress/prune existing large language models (LLMs), which results in performance drops and falls short in comparison to pre-training. In this paper, we investigate the family of acceleration methods that involve both structured pruning and model training. We found 1) layer-wise adaptive pruning (Adapt-Pruner) is extremely effective in LLMs and yields significant improvements over existing pruning techniques, 2) adaptive pruning equipped with further training leads to models comparable to those pre-training from scratch, 3) incremental pruning brings non-trivial performance gain by interleaving pruning with training and only removing a small portion of neurons ($\sim$5%) at a time. Experimental results on LLaMA-3.1-8B demonstrate that Adapt-Pruner outperforms conventional pruning methods, such as LLM-Pruner, FLAP, and SliceGPT, by an average of 1%-7% in accuracy on commonsense benchmarks. Additionally, Adapt-Pruner restores the performance of MobileLLM-125M to 600M on the MMLU benchmark with 200$\times$ fewer tokens via pruning from its larger counterparts, and discovers a new 1B model that surpasses LLaMA-3.2-1B in multiple benchmarks. The official code is released at https://github.com/research4pan/AdaptPruner.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAG-Enhanced Collaborative LLM Agents for Drug Discovery</title>
<link>https://arxiv.org/abs/2502.17506</link>
<guid>https://arxiv.org/abs/2502.17506</guid>
<content:encoded><![CDATA[

arXiv:2502.17506v3 Announce Type: replace 
Abstract: Recent advances in large language models (LLMs) have shown great potential to accelerate drug discovery. However, the specialized nature of biochemical data often necessitates costly domain-specific fine-tuning, posing major challenges. First, it hinders the application of more flexible general-purpose LLMs for cutting-edge drug discovery tasks. More importantly, it limits the rapid integration of the vast amounts of scientific data continuously generated through experiments and research. Compounding these challenges is the fact that real-world scientific questions are typically complex and open-ended, requiring reasoning beyond pattern matching or static knowledge retrieval.To address these challenges, we propose CLADD, a retrieval-augmented generation (RAG)-empowered agentic system tailored to drug discovery tasks. Through the collaboration of multiple LLM agents, CLADD dynamically retrieves information from biomedical knowledge bases, contextualizes query molecules, and integrates relevant evidence to generate responses - all without the need for domain-specific fine-tuning. Crucially, we tackle key obstacles in applying RAG workflows to biochemical data, including data heterogeneity, ambiguity, and multi-source integration. We demonstrate the flexibility and effectiveness of this framework across a variety of drug discovery tasks, showing that it outperforms general-purpose and domain-specific LLMs as well as traditional deep learning approaches. Our code is publicly available at https://github.com/Genentech/CLADD.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMUN: Adversarial Machine UNlearning</title>
<link>https://arxiv.org/abs/2503.00917</link>
<guid>https://arxiv.org/abs/2503.00917</guid>
<content:encoded><![CDATA[

arXiv:2503.00917v3 Announce Type: replace 
Abstract: Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on ``exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, ``approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random $10\%$ of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedALT: Federated Fine-Tuning through Adaptive Local Training with Rest-of-World LoRA</title>
<link>https://arxiv.org/abs/2503.11880</link>
<guid>https://arxiv.org/abs/2503.11880</guid>
<content:encoded><![CDATA[

arXiv:2503.11880v3 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) in federated settings enables privacy-preserving adaptation but suffers from cross-client interference due to model aggregation. Existing federated LoRA fine-tuning methods, primarily based on FedAvg, struggle with data heterogeneity, leading to harmful cross-client interference and suboptimal personalization. In this work, we propose \textbf{FedALT}, a novel personalized federated LoRA fine-tuning algorithm that fundamentally departs from FedAvg. Instead of using an aggregated model to initialize local training, each client continues training its individual LoRA while incorporating shared knowledge through a separate Rest-of-World (RoW) LoRA component. To effectively balance local adaptation and global information, FedALT introduces an adaptive mixer that dynamically learns input-specific weightings between the individual and RoW LoRA components, drawing conceptual foundations from the Mixture-of-Experts (MoE) paradigm. Through extensive experiments on NLP benchmarks, we demonstrate that FedALT significantly outperforms state-of-the-art personalized federated LoRA fine-tuning methods, achieving superior local adaptation without sacrificing computational efficiency.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mining--Gym: A Configurable RL Benchmarking Environment for Truck Dispatch Scheduling</title>
<link>https://arxiv.org/abs/2503.19195</link>
<guid>https://arxiv.org/abs/2503.19195</guid>
<content:encoded><![CDATA[

arXiv:2503.19195v2 Announce Type: replace 
Abstract: Optimizing the mining process -- particularly truck dispatch scheduling -- is a key driver of efficiency in open-pit operations. However, the dynamic and stochastic nature of these environments, with uncertainties such as equipment failures, truck maintenance, and variable haul cycle times, challenges traditional optimization. While Reinforcement Learning (RL) shows strong potential for adaptive decision-making in mining logistics, practical deployment requires evaluation in realistic, customizable simulation environments. The lack of standardized benchmarking hampers fair algorithm comparison, reproducibility, and real-world applicability of RL solutions.
  To address this, we present Mining-Gym -- a configurable, open-source benchmarking environment for training, testing, and evaluating RL algorithms in mining process optimization. Built on Salabim-based Discrete Event Simulation (DES) and integrated with Gymnasium, Mining-Gym captures mining-specific uncertainties through an event-driven decision-point architecture. It offers a GUI for parameter configuration, data logging, and real-time visualization, supporting reproducible evaluation of RL strategies and heuristic baselines.
  We validate Mining-Gym by comparing classical heuristics with RL-based scheduling across six scenarios from normal operation to severe equipment failures. Results show it is an effective, reproducible testbed, enabling fair evaluation of adaptive decision-making and demonstrating the strong performance potential of RL-trained schedulers.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Process Tilted Nonparametric Density Estimation using Fisher Divergence Score Matching</title>
<link>https://arxiv.org/abs/2504.03485</link>
<guid>https://arxiv.org/abs/2504.03485</guid>
<content:encoded><![CDATA[

arXiv:2504.03485v2 Announce Type: replace 
Abstract: We propose a nonparametric density estimator based on the Gaussian process (GP) and derive three novel closed form learning algorithms based on Fisher divergence (FD) score matching. The density estimator is formed by multiplying a base multivariate normal distribution with an exponentiated GP refinement, and so we refer to it as a GP-tilted nonparametric density. By representing the GP part of the score as a linear function using the random Fourier feature (RFF) approximation, we show that optimization can be solved in closed form for the three FD-based objectives considered. This includes the basic and noise conditional versions of the Fisher divergence, as well as an alternative to noise conditional FD models based on variational inference (VI) that we propose in this paper. For this novel learning approach, we propose an ELBO-like optimization to approximate the posterior distribution, with which we then derive a Fisher variational predictive distribution. The RFF representation of the GP, which is functionally equivalent to a single layer neural network score model with cosine activation, provides a useful linear representation of the GP for which all expectations can be solved. The Gaussian base distribution also helps with tractability of the VI approximation and ensures that our proposed density is well-defined. We demonstrate our three learning algorithms, as well as a MAP baseline algorithm, on several low dimensional density estimation problems. The closed form nature of the learning problem removes the reliance on iterative learning algorithms, making this technique particularly well-suited to big data sets, since only sufficient statistics collected from a single pass through the data is needed.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Variational Inference with Tuneable Stochastic Annealing</title>
<link>https://arxiv.org/abs/2504.03902</link>
<guid>https://arxiv.org/abs/2504.03902</guid>
<content:encoded><![CDATA[

arXiv:2504.03902v2 Announce Type: replace 
Abstract: We exploit the observation that stochastic variational inference (SVI) is a form of annealing and present a modified SVI approach -- applicable to both large and small datasets -- that allows the amount of annealing done by SVI to be tuned. We are motivated by the fact that, in SVI, the larger the batch size the more approximately Gaussian is the noise of the gradient, but the smaller its variance, which reduces the amount of annealing done to escape bad local optimal solutions. We propose a simple method for achieving both goals of having larger variance noise to escape bad local optimal solutions and more data information to obtain more accurate gradient directions. The idea is to set an actual batch size, which may be the size of the data set, and an effective batch size that matches the increased variance of a smaller batch size. The result is an approximation to the maximum entropy stochastic gradient at a desired variance level. We theoretically motivate our ``SVI+'' approach for conjugate exponential family model framework and illustrate its empirical performance for learning the probabilistic matrix factorization collaborative filter (PMF), the Latent Dirichlet Allocation topic model (LDA), and the Gaussian mixture model (GMM).
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond $\tilde{O}(\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints</title>
<link>https://arxiv.org/abs/2505.06709</link>
<guid>https://arxiv.org/abs/2505.06709</guid>
<content:encoded><![CDATA[

arXiv:2505.06709v2 Announce Type: replace 
Abstract: We study Online Convex Optimization with adversarial constraints (COCO). At each round a learner selects an action from a convex decision set and then an adversary reveals a convex cost and a convex constraint function. The goal of the learner is to select a sequence of actions to minimize both regret and the cumulative constraint violation (CCV) over a horizon of length $T$. The best-known policy for this problem achieves $O(\sqrt{T})$ regret and $\tilde{O}(\sqrt{T})$ CCV. In this paper, we improve this by trading off regret to achieve substantially smaller CCV. This trade-off is especially important in safety-critical applications, where satisfying the safety constraints is non-negotiable. Specifically, for any bounded convex cost and constraint functions, we propose an online policy that achieves $\tilde{O}(\sqrt{dT}+ T^\beta)$ regret and $\tilde{O}(dT^{1-\beta})$ CCV, where $d$ is the dimension of the decision set and $\beta \in [0,1]$ is a tunable parameter. We begin with a special case, called the $\textsf{Constrained Expert}$ problem, where the decision set is a probability simplex and the cost and constraint functions are linear. Leveraging a new adaptive small-loss regret bound, we propose a computationally efficient policy for the $\textsf{Constrained Expert}$ problem, that attains $O(\sqrt{T\ln N}+T^{\beta})$ regret and $\tilde{O}(T^{1-\beta} \ln N)$ CCV for $N$ number of experts. The original problem is then reduced to the $\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an additional $M$-smoothness assumption, we propose a computationally efficient first-order policy attaining $O(\sqrt{MT}+T^{\beta})$ regret and $\tilde{O}(MT^{1-\beta})$ CCV.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Effective Federated Graph Foundation Model via Mitigating Knowledge Entanglement</title>
<link>https://arxiv.org/abs/2505.12684</link>
<guid>https://arxiv.org/abs/2505.12684</guid>
<content:encoded><![CDATA[

arXiv:2505.12684v2 Announce Type: replace 
Abstract: Recent advances in graph machine learning have shifted to data-centric paradigms, driven by two emerging fields: (1) Federated graph learning (FGL) enables multi-client collaboration but faces challenges from data and task heterogeneity, limiting its practicality; (2) Graph foundation models (GFM) offer strong domain generalization but are usually trained on single machines, missing out on cross-silo data and resources.
  These paradigms are complementary, and their integration brings notable benefits. Motivated by this, we propose FedGFM, a novel decentralized GFM training paradigm. However, a key challenge is knowledge entanglement, where multi-domain knowledge merges into indistinguishable representations, hindering downstream adaptation.
  To address this, we present FedGFM+, an enhanced framework with two core modules to reduce knowledge entanglement: (1) AncDAI: A global anchor-based domain-aware initialization strategy. Before pre-training, each client encodes its local graph into domain-specific prototypes that serve as semantic anchors. Synthetic embeddings around these anchors initialize the global model. We theoretically prove these prototypes are distinguishable across domains, providing a strong inductive bias to disentangle domain-specific knowledge. (2) AdaDPP: A local adaptive domain-sensitive prompt pool. Each client learns a lightweight graph prompt capturing domain semantics during pre-training. During fine-tuning, prompts from all clients form a pool from which the GFM selects relevant prompts to augment target graph attributes, improving downstream adaptation.
  FedGFM+ is evaluated on 8 diverse benchmarks across multiple domains and tasks, outperforming 20 baselines from supervised learning, FGL, and federated GFM variants.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Long-term Earth System Forecasting</title>
<link>https://arxiv.org/abs/2505.19432</link>
<guid>https://arxiv.org/abs/2505.19432</guid>
<content:encoded><![CDATA[

arXiv:2505.19432v2 Announce Type: replace 
Abstract: Reliable long-term forecasting of Earth system dynamics is fundamentally limited by instabilities in current artificial intelligence (AI) models during extended autoregressive simulations. These failures often originate from inherent spectral bias, leading to inadequate representation of critical high-frequency, small-scale processes and subsequent uncontrolled error amplification. Inspired by the nested grids in numerical models used to resolve small scales, we present TritonCast. At the core of its design is a dedicated latent dynamical core, which ensures the long-term stability of the macro-evolution at a coarse scale. An outer structure then fuses this stable trend with fine-grained local details. This design effectively mitigates the spectral bias caused by cross-scale interactions. In atmospheric science, it achieves state-of-the-art accuracy on the WeatherBench 2 benchmark while demonstrating exceptional long-term stability: executing year-long autoregressive global forecasts and completing multi-year climate simulations that span the entire available $2500$-day test period without drift. In oceanography, it extends skillful eddy forecast to $120$ days and exhibits unprecedented zero-shot cross-resolution generalization. Ablation studies reveal that this performance stems from the synergistic interplay of the architecture's core components. TritonCast thus offers a promising pathway towards a new generation of trustworthy, AI-driven simulations. This significant advance has the potential to accelerate discovery in climate and Earth system science, enabling more reliable long-term forecasting and deeper insights into complex geophysical dynamics.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Tuning Enhances Plasticity in PTM-based Continual Learning</title>
<link>https://arxiv.org/abs/2505.19943</link>
<guid>https://arxiv.org/abs/2505.19943</guid>
<content:encoded><![CDATA[

arXiv:2505.19943v2 Announce Type: replace 
Abstract: Continual Learning with Pre-trained Models holds great promise for efficient adaptation across sequential tasks. However, most existing approaches freeze PTMs and rely on auxiliary modules like prompts or adapters, limiting model plasticity and leading to suboptimal generalization when facing significant distribution shifts. While full fine-tuning can improve adaptability, it risks disrupting crucial pre-trained knowledge. In this paper, we propose Mutual Information-guided Sparse Tuning (MIST), a plug-and-play method that selectively updates a small subset of PTM parameters, less than 5%, based on sensitivity to mutual information objectives. MIST enables effective task-specific adaptation while preserving generalization. To further reduce interference, we introduce strong sparsity regularization by randomly dropping gradients during tuning, resulting in fewer than 0.5% of parameters being updated per step. Applied before standard freeze-based methods, MIST consistently boosts performance across diverse continual learning benchmarks. Experiments show that integrating our method into multiple baselines yields significant performance gains. Our code is available at https://github.com/zhwhu/MIST.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.22573</link>
<guid>https://arxiv.org/abs/2505.22573</guid>
<content:encoded><![CDATA[

arXiv:2505.22573v2 Announce Type: replace 
Abstract: Simulation-based inference (SBI) is an established approach for performing Bayesian inference on scientific simulators. SBI so far works best on low-dimensional parametric models. However, it is difficult to infer function-valued parameters, which frequently occur in disciplines that model spatiotemporal processes such as the climate and earth sciences. Here, we introduce an approach for efficient posterior estimation, using a Fourier Neural Operator (FNO) architecture with a flow matching objective. We show that our approach, FNOPE, can perform inference of function-valued parameters at a fraction of the simulation budget of state of the art methods. In addition, FNOPE supports posterior evaluation at arbitrary discretizations of the domain, as well as simultaneous estimation of vector-valued parameters. We demonstrate the effectiveness of our approach on several benchmark tasks and a challenging spatial inference task from glaciology. FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cautious Optimism: A Meta-Algorithm for Near-Constant Regret in General Games</title>
<link>https://arxiv.org/abs/2506.05005</link>
<guid>https://arxiv.org/abs/2506.05005</guid>
<content:encoded><![CDATA[

arXiv:2506.05005v2 Announce Type: replace 
Abstract: We introduce Cautious Optimism, a framework for substantially faster regularized learning in general games. Cautious Optimism, as a variant of Optimism, adaptively controls the learning pace in a dynamic, non-monotone manner to accelerate no-regret learning dynamics. Cautious Optimism takes as input any instance of Follow-the-Regularized-Leader (FTRL) and outputs an accelerated no-regret learning algorithm (COFTRL) by pacing the underlying FTRL with minimal computational overhead. Importantly, it retains uncoupledness, that is, learners do not need to know other players' utilities. Cautious Optimistic FTRL (COFTRL) achieves near-optimal $O_T(\log T)$ regret in diverse self-play (mixing and matching regularizers) while preserving the optimal $O_T(\sqrt{T})$ regret in adversarial scenarios. In contrast to prior works (e.g., Syrgkanis et al. [2015], Daskalakis et al. [2021]), our analysis does not rely on monotonic step sizes, showcasing a novel route for fast learning in general games. Moreover, instances of COFTRL achieve new state-of-the-art regret minimization guarantees in general convex games, exponentially improving the dependence on the dimension of the action space $d$ over previous works [Farina et al., 2022a].
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow-Attentional Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.06127</link>
<guid>https://arxiv.org/abs/2506.06127</guid>
<content:encoded><![CDATA[

arXiv:2506.06127v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have become essential for learning from graph-structured data. However, existing GNNs do not consider the conservation law inherent in graphs associated with a flow of physical resources, such as electrical current in power grids or traffic in transportation networks, which can lead to reduced model performance. To address this, we propose flow attention, which adapts existing graph attention mechanisms to satisfy Kirchhoff$\text{'}$s first law. Furthermore, we discuss how this modification influences the expressivity and identify sets of non-isomorphic graphs that can be discriminated by flow attention but not by standard attention. Through extensive experiments on two flow graph datasets (electronic circuits and power grids) we demonstrate that flow attention enhances the performance of attention-based GNNs on both graph-level classification and regression tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Urban Incident Prediction with Graph Neural Networks: Integrating Government Ratings and Crowdsourced Reports</title>
<link>https://arxiv.org/abs/2506.08740</link>
<guid>https://arxiv.org/abs/2506.08740</guid>
<content:encoded><![CDATA[

arXiv:2506.08740v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) are widely used in urban spatiotemporal forecasting, such as predicting infrastructure problems. In this setting, government officials wish to know in which neighborhoods incidents like potholes or rodent issues occur. The true state of incidents (e.g., street conditions) for each neighborhood is observed via government inspection ratings. However, these ratings are only conducted for a sparse set of neighborhoods and incident types. We also observe the state of incidents via crowdsourced reports, which are more densely observed but may be biased due to heterogeneous reporting behavior. First, for such settings, we propose a multiview, multioutput GNN-based model that uses both unbiased rating data and biased reporting data to predict the true latent state of incidents. Second, we investigate a case study of New York City urban incidents and collect, standardize, and make publicly available a dataset of 9,615,863 crowdsourced reports and 1,041,415 government inspection ratings over 3 years and across 139 types of incidents. Finally, we show on both real and semi-synthetic data that our model can better predict the latent state compared to models that use only reporting data or models that use only rating data, especially when rating data is sparse and reports are predictive of ratings. We also quantify demographic biases in crowdsourced reporting, e.g., higher-income neighborhoods report problems at higher rates. Our analysis showcases a widely applicable approach for latent state prediction using heterogeneous, sparse, and biased data.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preserving Task-Relevant Information Under Linear Concept Removal</title>
<link>https://arxiv.org/abs/2506.10703</link>
<guid>https://arxiv.org/abs/2506.10703</guid>
<content:encoded><![CDATA[

arXiv:2506.10703v2 Announce Type: replace 
Abstract: Modern neural networks often encode unwanted concepts alongside task-relevant information, leading to fairness and interpretability concerns. Existing post-hoc approaches can remove undesired concepts but often degrade useful signals. We introduce SPLINCE-Simultaneous Projection for LINear concept removal and Covariance prEservation - which eliminates sensitive concepts from representations while exactly preserving their covariance with a target label. SPLINCE achieves this via an oblique projection that 'splices out' the unwanted direction yet protects important label correlations. Theoretically, it is the unique solution that removes linear concept predictability and maintains target covariance with minimal embedding distortion. Empirically, SPLINCE outperforms baselines on benchmarks such as Bias in Bios and Winobias, removing protected attributes while minimally damaging main-task information.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks</title>
<link>https://arxiv.org/abs/2506.18588</link>
<guid>https://arxiv.org/abs/2506.18588</guid>
<content:encoded><![CDATA[

arXiv:2506.18588v2 Announce Type: replace 
Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural networks to small input perturbations; yet its dynamics (i.e. temporal evolution) during training remains under-explored. We present a rigorous mathematical framework to model the temporal evolution of Lipschitz continuity during training with stochastic gradient descent (SGD). This framework leverages a system of stochastic differential equations (SDEs) to capture both deterministic and stochastic forces. Our theoretical analysis identifies three principal factors driving the evolution: (i) the projection of gradient flows, induced by the optimization dynamics, onto the operator-norm Jacobian of parameter matrices; (ii) the projection of gradient noise, arising from the randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii) the projection of the gradient noise onto the operator-norm Hessian of parameter matrices. Furthermore, our theoretical framework sheds light on such as how noisy supervision, parameter initialization, batch size, and mini-batch sampling trajectories, among other factors, shape the evolution of the Lipschitz continuity of neural networks. Our experimental results demonstrate strong agreement between the theoretical implications and the observed behaviors.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orthogonal Soft Pruning for Efficient Class Unlearning</title>
<link>https://arxiv.org/abs/2506.19891</link>
<guid>https://arxiv.org/abs/2506.19891</guid>
<content:encoded><![CDATA[

arXiv:2506.19891v2 Announce Type: replace 
Abstract: Efficient and controllable data unlearning in federated learning remains challenging, due to the trade-off between forgetting and retention performance. Especially under non-independent and identically distributed (non-IID) settings, where deep feature entanglement exacerbates this dilemma. To address this challenge, we propose FedOrtho, a federated unlearning framework that combines orthogonalized deep convolutional kernels with an activation-driven controllable one-shot soft pruning (OSP) mechanism. FedOrtho enforces kernel orthogonality and local-global alignment to decouple feature representations and mitigate client drift. This structural independence enables precise one-shot pruning of forgetting-related kernels while preserving retained knowledge. FedOrtho achieves SOTA performance on CIFAR-10, CIFAR100 and TinyImageNet with ResNet and VGG frameworks, verifying that FedOrtho supports class-, client-, and sample-level unlearning with over 98% forgetting quality. It reduces computational and communication costs by 2-3 orders of magnitude in federated settings and achieves subsecond-level erasure in centralized scenarios while maintaining over 97% retention accuracy and mitigating membership inference risks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Necessity of Output Distribution Reweighting for Effective Class Unlearning</title>
<link>https://arxiv.org/abs/2506.20893</link>
<guid>https://arxiv.org/abs/2506.20893</guid>
<content:encoded><![CDATA[

arXiv:2506.20893v4 Announce Type: replace 
Abstract: In this paper, we reveal a significant shortcoming in class unlearning evaluations: overlooking the underlying class geometry can cause privacy leakage. We further propose a simple yet effective solution to mitigate this issue. We introduce a membership-inference attack via nearest neighbors (MIA-NN) that uses the probabilities the model assigns to neighboring classes to detect unlearned samples. Our experiments show that existing unlearning methods are vulnerable to MIA-NN across multiple datasets. We then propose a new fine-tuning objective that mitigates this privacy leakage by approximating, for forget-class inputs, the distribution over the remaining classes that a retrained-from-scratch model would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting (TRW) distribution serves as the desired distribution during fine-tuning. We also show that across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior unlearning metrics. More specifically, on CIFAR-10, it reduces the gap with retrained models by 19% and 46% for U-LiRA and MIA-NN scores, accordingly, compared to the SOTA method for each category.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence Bound and Critical Batch Size of Muon Optimizer</title>
<link>https://arxiv.org/abs/2507.01598</link>
<guid>https://arxiv.org/abs/2507.01598</guid>
<content:encoded><![CDATA[

arXiv:2507.01598v3 Announce Type: replace 
Abstract: Muon, a recently proposed optimizer that leverages the inherent matrix structure of neural network parameters, has demonstrated strong empirical performance, indicating its potential as a successor to standard optimizers such as AdamW. This paper presents theoretical analysis to support its practical success. We provide convergence proofs for Muon across four practical settings, systematically examining its behavior with and without the inclusion of Nesterov momentum and weight decay. Our analysis covers the standard configuration using both, thereby elucidating its real-world performance. We then demonstrate that the addition of weight decay yields strictly tighter theoretical bounds and clarify the interplay between the weight decay coefficient and the learning rate. Finally, we derive the critical batch size for Muon that minimizes the computational cost of training. Our analysis identifies the hyperparameters governing this value, and our experiments validate the corresponding theoretical findings across workloads including image classification and language modeling task.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RetrySQL: text-to-SQL training with retry data for self-correcting query generation</title>
<link>https://arxiv.org/abs/2507.02529</link>
<guid>https://arxiv.org/abs/2507.02529</guid>
<content:encoded><![CDATA[

arXiv:2507.02529v2 Announce Type: replace 
Abstract: The text-to-SQL task is an active challenge in Natural Language Processing. Many existing solutions focus on using black-box language models extended with specialized components within customized end-to-end text-to-SQL pipelines. While these solutions use both closed-source proprietary language models and coding-oriented open-source models, there is a lack of research regarding SQL-specific generative models. At the same time, recent advancements in self-correcting generation strategies show promise for improving the capabilities of existing architectures. The application of these concepts to the text-to-SQL task remains unexplored. In this paper, we introduce RetrySQL, a new approach to training text-to-SQL generation models. We prepare reasoning steps for reference SQL queries and then corrupt them to create retry data that contains both incorrect and corrected steps, divided with a special token. We continuously pre-train an open-source coding model with this data and demonstrate that retry steps yield an improvement of up to 4 percentage points in both overall and challenging execution accuracy metrics, compared to pre-training without retry data. Additionally, we confirm that supervised fine-tuning with LoRA is ineffective for learning from retry data and that full-parameter pre-training is a necessary requirement for that task. We showcase that the self-correcting behavior is learned by the model and the increase in downstream accuracy metrics is a result of this additional skill. Finally, we incorporate RetrySQL-trained models into the full text-to-SQL pipeline and showcase that they are competitive in terms of execution accuracy with proprietary models that contain orders of magnitude more parameters. RetrySQL demonstrates that self-correction can be learned in the text-to-SQL task and provides a novel way of improving generation accuracy for SQL-oriented language models.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NTSFormer: A Self-Teaching Graph Transformer for Multimodal Isolated Cold-Start Node Classification</title>
<link>https://arxiv.org/abs/2507.04870</link>
<guid>https://arxiv.org/abs/2507.04870</guid>
<content:encoded><![CDATA[

arXiv:2507.04870v2 Announce Type: replace 
Abstract: Isolated cold-start node classification on multimodal graphs is challenging because such nodes have no edges and often have missing modalities (e.g., absent text or image features). Existing methods address structural isolation by degrading graph learning models to multilayer perceptrons (MLPs) for isolated cold-start inference, using a teacher model (with graph access) to guide the MLP. However, this results in limited model capacity in the student, which is further challenged when modalities are missing. In this paper, we propose Neighbor-to-Self Graph Transformer (NTSFormer), a unified Graph Transformer framework that jointly tackles the isolation and missing-modality issues via a self-teaching paradigm. Specifically, NTSFormer uses a cold-start attention mask to simultaneously make two predictions for each node: a "student" prediction based only on self information (i.e., the node's own features), and a "teacher" prediction incorporating both self and neighbor information. This enables the model to supervise itself without degrading to an MLP, thereby fully leveraging the Transformer's capacity to handle missing modalities. To handle diverse graph information and missing modalities, NTSFormer performs a one-time multimodal graph pre-computation that converts structural and feature data into token sequences, which are then processed by Mixture-of-Experts (MoE) Input Projection and Transformer layers for effective fusion. Experiments on public datasets show that NTSFormer achieves superior performance for multimodal isolated cold-start node classification.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First-Order Error Matters: Accurate Compensation for Quantized Large Language Models</title>
<link>https://arxiv.org/abs/2507.11017</link>
<guid>https://arxiv.org/abs/2507.11017</guid>
<content:encoded><![CDATA[

arXiv:2507.11017v2 Announce Type: replace 
Abstract: Post-training quantization (PTQ) offers an efficient approach to compressing large language models (LLMs), significantly reducing memory access and computational costs. Existing compensation-based weight calibration methods often rely on a second-order Taylor expansion to model quantization error, under the assumption that the first-order term is negligible in well-trained full-precision models. However, we reveal that the progressive compensation process introduces accumulated first-order deviations between latent weights and their full-precision counterparts, making this assumption fundamentally flawed. To address this, we propose FOEM, a novel PTQ method that explicitly incorporates first-order gradient terms to improve quantization error compensation. FOEM approximates gradients by performing a first-order Taylor expansion around the pre-quantization weights. This yields an approximation based on the difference between latent and full-precision weights as well as the Hessian matrix. When substituted into the theoretical solution, the formulation eliminates the need to explicitly compute the Hessian, thereby avoiding the high computational cost and limited generalization of backpropagation-based gradient methods. This design introduces only minimal additional computational overhead. Extensive experiments across a wide range of models and benchmarks demonstrate that FOEM consistently outperforms the classical GPTQ method. In 3-bit weight-only quantization, FOEM reduces the perplexity of Llama3-8B by 17.3% and increases the 5-shot MMLU accuracy from 53.8% achieved by GPTAQ to 56.1%. Moreover, FOEM can be seamlessly combined with advanced techniques such as SpinQuant, delivering additional gains under the challenging W4A4KV4 setting and further narrowing the performance gap with full-precision baselines, surpassing existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OccamVTS: Distilling Vision Models to 1% Parameters for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2508.01727</link>
<guid>https://arxiv.org/abs/2508.01727</guid>
<content:encoded><![CDATA[

arXiv:2508.01727v2 Announce Type: replace 
Abstract: Time series forecasting is fundamental to diverse applications, with recent approaches leverage large vision models (LVMs) to capture temporal patterns through visual representations. We reveal that while vision models enhance forecasting performance, 99% of their parameters are unnecessary for time series tasks. Through cross-modal analysis, we find that time series align with low-level textural features but not high-level semantics, which can impair forecasting accuracy. We propose OccamVTS, a knowledge distillation framework that extracts only the essential 1% of predictive information from LVMs into lightweight networks. Using pre-trained LVMs as privileged teachers, OccamVTS employs pyramid-style feature alignment combined with correlation and feature distillation to transfer beneficial patterns while filtering out semantic noise. Counterintuitively, this aggressive parameter reduction improves accuracy by eliminating overfitting to irrelevant visual features while preserving essential temporal patterns. Extensive experiments across multiple benchmark datasets demonstrate that OccamVTS consistently achieves state-of-the-art performance with only 1% of the original parameters, particularly excelling in few-shot and zero-shot scenarios.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VITA: Variational Pretraining of Transformers for Climate-Robust Crop Yield Forecasting</title>
<link>https://arxiv.org/abs/2508.03589</link>
<guid>https://arxiv.org/abs/2508.03589</guid>
<content:encoded><![CDATA[

arXiv:2508.03589v3 Announce Type: replace 
Abstract: Accurate crop yield forecasting is essential for global food security. However, current AI models systematically underperform when yields deviate from historical trends. We attribute this to the lack of rich, physically grounded datasets directly linking atmospheric states to yields. To address this, we introduce VITA (Variational Inference Transformer for Asymmetric Data), a variational pretraining framework that learns representations from large satellite-based weather datasets and transfers to the ground-based limited measurements available for yield prediction. VITA is trained using detailed meteorological variables as proxy targets during pretraining and learns to predict latent atmospheric states under a seasonality-aware sinusoidal prior. This allows the model to be fine-tuned using limited weather statistics during deployment. Applied to 763 counties in the US Corn Belt, VITA achieves state-of-the-art performance in predicting corn and soybean yields across all evaluation scenarios, particularly during extreme years, with statistically significant improvements (paired t-test, p < 0.0001). Importantly, VITA outperforms prior frameworks like GNN-RNN without soil data, and larger foundational models (e.g., Chronos-Bolt) with less compute, making it practical for real-world use, especially in data-scarce regions. This work highlights how domain-aware AI design can overcome data limitations and support resilient agricultural forecasting in a changing climate.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BubbleOKAN: A Physics-Informed Interpretable Neural Operator for High-Frequency Bubble Dynamics</title>
<link>https://arxiv.org/abs/2508.03965</link>
<guid>https://arxiv.org/abs/2508.03965</guid>
<content:encoded><![CDATA[

arXiv:2508.03965v2 Announce Type: replace 
Abstract: In this work, we employ physics-informed neural operators to map pressure profiles from an input function space to the corresponding bubble radius responses. Our approach employs a two-step DeepONet architecture. To address the intrinsic spectral bias of deep learning models, our model incorporates the Rowdy adaptive activation function, enhancing the representation of high-frequency features. Moreover, we introduce the Kolmogorov-Arnold network (KAN) based two-step DeepOKAN model, which enhances interpretability (often lacking in conventional multilayer perceptron architectures) while efficiently capturing high-frequency bubble dynamics without explicit utilization of activation functions in any form. We particularly investigate the use of spline basis functions in combination with radial basis functions (RBF) within our architecture, as they demonstrate superior performance in constructing a universal basis for approximating high-frequency bubble dynamics compared to alternative formulations. Furthermore, we emphasize on the performance bottleneck of RBF while learning the high frequency bubble dynamics and showcase the advantage of using spline basis function for the trunk network in overcoming this inherent spectral bias. The model is systematically evaluated across three representative scenarios: (1) bubble dynamics governed by the Rayleigh-Plesset equation with a single initial radius, (2) bubble dynamics governed by the Keller-Miksis equation with a single initial radius, and (3) Keller-Miksis dynamics with multiple initial radii. We also compare our results with state-of-the-art neural operators, including Fourier Neural Operators, Wavelet Neural Operators, OFormer, and Convolutional Neural Operators. Our findings demonstrate that the two-step DeepOKAN accurately captures both low- and high-frequency behaviors, and offers a promising alternative to conventional numerical solvers.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivDFS: Private Inference via Distributed Feature Sharing against Data Reconstruction Attacks</title>
<link>https://arxiv.org/abs/2508.04346</link>
<guid>https://arxiv.org/abs/2508.04346</guid>
<content:encoded><![CDATA[

arXiv:2508.04346v2 Announce Type: replace 
Abstract: In this paper, we introduce PrivDFS, a distributed feature-sharing framework for input-private inference in image classification. A single holistic intermediate representation in split inference gives diffusion-based Data Reconstruction Attacks (DRAs) sufficient signal to reconstruct the input with high fidelity. PrivDFS restructures this vulnerability by fragmenting the representation and processing the fragments independently across a majority-honest set of servers. As a result, each branch observes only an incomplete and reconstruction-insufficient view of the input. To realize this, PrivDFS employs learnable binary masks that partition the intermediate representation into sparse and largely non-overlapping feature shares, each processed by a separate server, while a lightweight fusion module aggregates their predictions on the client. This design preserves full task accuracy when all branches are combined, yet sharply limits the reconstructive power available to any individual server. PrivDFS applies seamlessly to both ResNet-based CNNs and Vision Transformers. Across CIFAR-10/100, CelebA, and ImageNet-1K, PrivDFS induces a pronounced collapse in DRA performance, e.g., on CIFAR-10, PSNR drops from 23.25 -> 12.72 and SSIM from 0.963 -> 0.260, while maintaining accuracy within 1% of non-private split inference. These results establish structural feature partitioning as a practical and architecture-agnostic approach to reducing reconstructive leakage in cloud-based vision inference.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergraph Neural Network with State Space Models for Node Classification</title>
<link>https://arxiv.org/abs/2508.06587</link>
<guid>https://arxiv.org/abs/2508.06587</guid>
<content:encoded><![CDATA[

arXiv:2508.06587v2 Announce Type: replace 
Abstract: In recent years, graph neural networks (GNNs) have gained significant attention for node classification tasks on graph-structured data. However, traditional GNNs primarily focus on adjacency relationships between nodes, often overlooking the role-based characteristics that can provide complementary insights for learning expressive node representations. Existing frameworks for extracting role-based features are largely unsupervised and often fail to translate effectively into downstream predictive tasks. To address these limitations, we propose a hypergraph neural network with a state space model (HGMN). The model integrates role-aware representations into GNNs by combining hypergraph construction with state-space modeling in a principled manner. HGMN employs hypergraph construction techniques to capture higher-order relationships and leverages a learnable mamba transformer mechanism to fuse role-based and adjacency-based embeddings. By exploring two distinct hypergraph construction strategies, degree-based and neighborhood-based, the framework reinforces connectivity among nodes with structural similarity, thereby enriching the learned representations. Furthermore, the inclusion of hypergraph convolution layers enables the model to account for complex dependencies within hypergraph structures. To alleviate the over-smoothing problem encountered in deeper networks, we incorporate residual connections, which improve stability and promote effective feature propagation across layers. Comprehensive experiments on benchmark datasets including OGB, ACM, DBLP, IIP TerroristRel, Cora, Citeseer, and Pubmed demonstrate that HGMN consistently outperforms strong baselines in node classification tasks. These results support the claim that explicitly incorporating role-based features within a hypergraph framework offers tangible benefits for node classification tasks.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines</title>
<link>https://arxiv.org/abs/2508.14482</link>
<guid>https://arxiv.org/abs/2508.14482</guid>
<content:encoded><![CDATA[

arXiv:2508.14482v2 Announce Type: replace 
Abstract: The explainability of deep learning models remains a significant challenge, particularly in the medical domain where interpretable outputs are critical for clinical trust and transparency. Path attribution methods such as Integrated Gradients rely on a baseline representing the absence of relevant features ("missingness"). Commonly used baselines, such as all-zero inputs, are often semantically meaningless, especially in medical contexts. While alternative baseline choices have been explored, existing methods lack a principled approach to dynamically select baselines tailored to each input. In this work, we examine the notion of missingness in the medical context, analyze its implications for baseline selection, and introduce a counterfactual-guided approach to address the limitations of conventional baselines. We argue that a generated counterfactual (i.e. clinically "normal" variation of the pathological input) represents a more accurate representation of a meaningful absence of features. We use a Variational Autoencoder in our implementation, though our concept is model-agnostic and can be applied with any suitable counterfactual method. We evaluate our concept on three distinct medical data sets and empirically demonstrate that counterfactual baselines yield more faithful and medically relevant attributions, outperforming standard baseline choices as well as other related methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness for the People, by the People: Minority Collective Action</title>
<link>https://arxiv.org/abs/2508.15374</link>
<guid>https://arxiv.org/abs/2508.15374</guid>
<content:encoded><![CDATA[

arXiv:2508.15374v2 Announce Type: replace 
Abstract: Machine learning models often preserve biases present in training data, leading to unfair treatment of certain minority groups. Despite an array of existing firm-side bias mitigation techniques, they typically incur utility costs and require organizational buy-in. Recognizing that many models rely on user-contributed data, end-users can induce fairness through the framework of Algorithmic Collective Action, where a coordinated minority group strategically relabels its own data to enhance fairness, without altering the firm's training process. We propose three practical, model-agnostic methods to approximate ideal relabeling and validate them on real-world datasets. Our findings show that a subgroup of the minority can substantially reduce unfairness with a small impact on the overall prediction error.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRMD: Deep Reinforcement Learning for Malware Detection under Concept Drift</title>
<link>https://arxiv.org/abs/2508.18839</link>
<guid>https://arxiv.org/abs/2508.18839</guid>
<content:encoded><![CDATA[

arXiv:2508.18839v2 Announce Type: replace 
Abstract: Malware detection in real-world settings must deal with evolving threats, limited labeling budgets, and uncertain predictions. Traditional classifiers, without additional mechanisms, struggle to maintain performance under concept drift in malware domains, as their supervised learning formulation cannot optimize when to defer decisions to manual labeling and adaptation. Modern malware detection pipelines combine classifiers with monthly active learning (AL) and rejection mechanisms to mitigate the impact of concept drift. In this work, we develop a novel formulation of malware detection as a one-step Markov Decision Process and train a deep reinforcement learning (DRL) agent, simultaneously optimizing sample classification performance and rejecting high-risk samples for manual labeling. We evaluated the joint detection and drift mitigation policy learned by the DRL-based Malware Detection (DRMD) agent through time-aware evaluations on Android malware datasets subject to realistic drift requiring multi-year performance stability. The policies learned under these conditions achieve a higher Area Under Time (AUT) performance compared to standard classification approaches used in the domain, showing improved resilience to concept drift. Specifically, the DRMD agent achieved an average AUT improvement of 8.66 and 10.90 for the classification-only and classification-rejection policies, respectively. Our results demonstrate for the first time that DRL can facilitate effective malware detection and improved resiliency to concept drift in the dynamic setting of Android malware detection.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Torrential Loss Function for Precipitation Forecasting</title>
<link>https://arxiv.org/abs/2509.01348</link>
<guid>https://arxiv.org/abs/2509.01348</guid>
<content:encoded><![CDATA[

arXiv:2509.01348v2 Announce Type: replace 
Abstract: Accurate precipitation forecasting is becoming increasingly important in the context of climate change. In response, machine learning-based approaches have recently gained attention as an emerging alternative to traditional methods such as numerical weather prediction and climate models. Nonetheless, many recent approaches still rely on off-the-shelf loss functions, and even the more advanced ones merely involve optimization processes based on the critical success index (CSI). The problem, however, is that CSI may become ineffective during extended dry periods when precipitation remains below the threshold, rendering it less than ideal as a criterion for optimization. To address this limitation, we introduce a simple penalty expression and reinterpret it as a quadratic unconstrained binary optimization (QUBO) formulation. Ultimately, the resulting QUBO formulation is relaxed into a differentiable advanced torrential (AT) loss function through an approximation process. The proposed AT loss demonstrates its superiority through the Lipschitz constant, forecast performance evaluations, consistency experiments, and ablation studies with the operational model.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuro-Spectral Architectures for Causal Physics-Informed Networks</title>
<link>https://arxiv.org/abs/2509.04966</link>
<guid>https://arxiv.org/abs/2509.04966</guid>
<content:encoded><![CDATA[

arXiv:2509.04966v2 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful framework for solving partial differential equations (PDEs). However, standard MLP-based PINNs often fail to converge when dealing with complex initial value problems, leading to solutions that violate causality and suffer from a spectral bias towards low-frequency components. To address these issues, we introduce NeuSA (Neuro-Spectral Architectures), a novel class of PINNs inspired by classical spectral methods, designed to solve linear and nonlinear PDEs with variable coefficients. NeuSA learns a projection of the underlying PDE onto a spectral basis, leading to a finite-dimensional representation of the dynamics which is then integrated with an adapted Neural ODE (NODE). This allows us to overcome spectral bias, by leveraging the high-frequency components enabled by the spectral representation; to enforce causality, by inheriting the causal structure of NODEs, and to start training near the target solution, by means of an initialization scheme based on classical methods. We validate NeuSA on canonical benchmarks for linear and nonlinear wave equations, demonstrating strong performance as compared to other architectures, with faster convergence, improved temporal consistency and superior predictive accuracy. Code and pretrained models are available in https://github.com/arthur-bizzi/neusa.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Active Learning and Explainable AI for Multi-Objective Optimization of Spin Coated Polymers</title>
<link>https://arxiv.org/abs/2509.08988</link>
<guid>https://arxiv.org/abs/2509.08988</guid>
<content:encoded><![CDATA[

arXiv:2509.08988v2 Announce Type: replace 
Abstract: Spin coating polymer thin films to achieve specific mechanical properties is inherently a multi-objective optimization problem. We present a framework that integrates an active Pareto front learning algorithm (PyePAL) with visualization and explainable AI techniques to optimize processing parameters. PyePAL uses Gaussian process models to predict objective values (hardness and elasticity) from the design variables (spin speed, dilution, and polymer mixture), guiding the adaptive selection of samples toward promising regions of the design space. To enable interpretable insights into the high-dimensional design space, we utilize UMAP (Uniform Manifold Approximation and Projection) for two-dimensional visualization of the Pareto front exploration. Additionally, we incorporate fuzzy linguistic summaries, which translate the learned relationships between process parameters and performance objectives into linguistic statements, thus enhancing the explainability and understanding of the optimization results. Experimental results demonstrate that our method efficiently identifies promising polymer designs, while the visual and linguistic explanations facilitate expert-driven analysis and knowledge discovery.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Pareto-Optimal Token Merging for Edge Transformer Models in Semantic Communication</title>
<link>https://arxiv.org/abs/2509.09168</link>
<guid>https://arxiv.org/abs/2509.09168</guid>
<content:encoded><![CDATA[

arXiv:2509.09168v2 Announce Type: replace 
Abstract: Large-scale transformer models have emerged as a powerful tool for semantic communication systems, enabling edge devices to extract rich representations for robust inference across noisy wireless channels. However, their substantial computational demands remain a major barrier to practical deployment in resource-constrained 6G networks. In this paper, we present a training-free framework for adaptive token merging in pretrained vision transformers to jointly reduce inference time and transmission resource usage. We formulate the selection of per-layer merging proportions as a multi-objective optimization problem to balance accuracy and computational cost. We employ Gaussian process-based Bayesian optimization to construct a Pareto frontier of optimal configurations, enabling flexible runtime adaptation to dynamic application requirements and channel conditions. Extensive experiments demonstrate that our method consistently outperforms other baselines and achieves significant reductions in floating-point operations while maintaining competitive accuracy across a wide range of signal-to-noise ratio (SNR) conditions. Additional results highlight the effectiveness of adaptive policies that adjust merging aggressiveness in response to channel quality, providing a practical mechanism to trade off latency and semantic fidelity on demand. These findings establish a scalable and efficient approach for deploying transformer-based semantic communication in future edge intelligence systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttentiveGRUAE: An Attention-Based GRU Autoencoder for Temporal Clustering and Behavioral Characterization of Depression from Wearable Data</title>
<link>https://arxiv.org/abs/2510.02558</link>
<guid>https://arxiv.org/abs/2510.02558</guid>
<content:encoded><![CDATA[

arXiv:2510.02558v2 Announce Type: replace 
Abstract: In this study, we present AttentiveGRUAE, a novel attention-based gated recurrent unit (GRU) autoencoder designed for temporal clustering and prediction of outcome from longitudinal wearable data. Our model jointly optimizes three objectives: (1) learning a compact latent representation of daily behavioral features via sequence reconstruction, (2) predicting end-of-period depression rate through a binary classification head, and (3) identifying behavioral subtypes through Gaussian Mixture Model (GMM) based soft clustering of learned embeddings. We evaluate AttentiveGRUAE on longitudinal sleep data from 372 participants (GLOBEM 2018-2019), and it demonstrates superior performance over baseline clustering, domain-aligned self-supervised, and ablated models in both clustering quality (silhouette score = 0.70 vs 0.32-0.70) and depression classification (AUC = 0.74 vs 0.50-0.67). Additionally, external validation on cross-year cohorts from 332 participants (GLOBEM 2020-2021) confirms cluster reproducibility (silhouette score = 0.63, AUC = 0.61) and stability. We further perform subtype analysis and visualize temporal attention, which highlights sleep-related differences between clusters and identifies salient time windows that align with changes in sleep regularity, yielding clinically interpretable explanations of risk.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICL-Router: In-Context Learned Model Representations for LLM Routing</title>
<link>https://arxiv.org/abs/2510.09719</link>
<guid>https://arxiv.org/abs/2510.09719</guid>
<content:encoded><![CDATA[

arXiv:2510.09719v3 Announce Type: replace 
Abstract: Large language models (LLMs) often exhibit complementary strengths. Model routing harnesses these strengths by dynamically directing each query to the most suitable model, given a candidate model pool. However, routing performance relies on accurate model representations, and adding new models typically requires retraining, limiting scalability. To address these challenges, we propose a novel routing method using in-context vectors to represent model capabilities. The method proceeds in two stages. First, queries are embedded and projected into vectors, with a projector and LLM-based router trained to reconstruct the original queries, aligning vector representations with the router's semantic space. Second, each candidate model is profiled on a query set, and the router learns -- based on in-context vectors of query and model performance -- to predict whether each model can correctly answer new queries. Extensive experiments demonstrate that our method achieves state-of-the-art routing performance in both in-distribution and out-of-distribution tasks. Moreover, our method allows for seamless integration of new models without retraining the router. The code is available at https://github.com/lalalamdbf/ICL-Router.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Personalized Treatment Plan: Geometrical Model-Agnostic Approach to Counterfactual Explanations</title>
<link>https://arxiv.org/abs/2510.22911</link>
<guid>https://arxiv.org/abs/2510.22911</guid>
<content:encoded><![CDATA[

arXiv:2510.22911v4 Announce Type: replace 
Abstract: In our article, we describe a method for generating counterfactual explanations in high-dimensional spaces using four steps that involve fitting our dataset to a model, finding the decision boundary, determining constraints on the problem, and computing the closest point (counterfactual explanation) from that boundary. We propose a discretized approach where we find many discrete points on the boundary and then identify the closest feasible counterfactual explanation. This method, which we later call $\textit{Segmented Sampling for Boundary Approximation}$ (SSBA), applies binary search to find decision boundary points and then searches for the closest boundary point. Across four datasets of varying dimensionality, we show that our method can outperform current methods for counterfactual generation with reductions in distance between $5\%$ to $50\%$ in terms of the $L_2$ norm. Our method can also handle real-world constraints by restricting changes to immutable and categorical features, such as age, gender, sex, height, and other related characteristics such as the case for a health-based dataset. In terms of runtime, the SSBA algorithm generates decision boundary points on multiple orders of magnitude in the same given time when we compare to a grid-based approach. In general, our method provides a simple and effective model-agnostic method that can compute nearest feasible (i.e. realistic with constraints) counterfactual explanations. All of our results and code are available at: https://github.com/dsin85691/SSBA_For_Counterfactuals
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advantage Shaping as Surrogate Reward Maximization: Unifying Pass@K Policy Gradients</title>
<link>https://arxiv.org/abs/2510.23049</link>
<guid>https://arxiv.org/abs/2510.23049</guid>
<content:encoded><![CDATA[

arXiv:2510.23049v2 Announce Type: replace 
Abstract: This note reconciles two seemingly distinct approaches to policy gradient optimization for the Pass@K objective in reinforcement learning with verifiable rewards: (1) direct REINFORCE-style methods, and (2) advantage-shaping techniques that directly modify GRPO. We show that these are two sides of the same coin. By reverse-engineering existing advantage-shaping algorithms, we reveal that they implicitly optimize surrogate rewards. We specifically interpret practical "hard-example up-weighting" modifications to GRPO as reward-level regularization. Conversely, starting from surrogate reward objectives, we provide a simple recipe for deriving both existing and new advantage-shaping methods. This perspective provides a lens for RLVR policy gradient optimization beyond our original motivation of Pass@K.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pelican-VL 1.0: A Foundation Brain Model for Embodied Intelligence</title>
<link>https://arxiv.org/abs/2511.00108</link>
<guid>https://arxiv.org/abs/2511.00108</guid>
<content:encoded><![CDATA[

arXiv:2511.00108v2 Announce Type: replace 
Abstract: This report presents Pelican-VL 1.0, a new family of open-source embodied brain models with parameter scales ranging from 7 billion to 72 billion. Our explicit mission is clearly stated as: To embed powerful intelligence into various embodiments. Pelican-VL 1.0 is currently the largest-scale open-source embodied multimodal brain model. Its core advantage lies in the in-depth integration of data power and intelligent adaptive learning mechanisms. Specifically, metaloop distilled a high-quality dataset from a raw dataset containing 4+ billion tokens. Pelican-VL 1.0 is trained on a large-scale cluster of 1000+ A800 GPUs, consuming over 50k+ A800 GPU-hours per checkpoint. This translates to a 20.3% performance uplift from its base model and outperforms 100B-level open-source counterparts by 10.6%, placing it on par with leading proprietary systems on well-known embodied benchmarks. We establish a novel framework, DPPO (Deliberate Practice Policy Optimization), inspired by human metacognition to train Pelican-VL 1.0. We operationalize this as a metaloop that teaches the AI to practice deliberately, which is a RL-Refine-Diagnose-SFT loop.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NervePool: A Simplicial Pooling Layer</title>
<link>https://arxiv.org/abs/2305.06315</link>
<guid>https://arxiv.org/abs/2305.06315</guid>
<content:encoded><![CDATA[

arXiv:2305.06315v2 Announce Type: replace-cross 
Abstract: For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, nervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CHNNet: An Artificial Neural Network With Connected Hidden Neurons</title>
<link>https://arxiv.org/abs/2305.10468</link>
<guid>https://arxiv.org/abs/2305.10468</guid>
<content:encoded><![CDATA[

arXiv:2305.10468v3 Announce Type: replace-cross 
Abstract: In contrast to biological neural circuits, conventional artificial neural networks are commonly organized as strictly hierarchical architectures that exclude direct connections among neurons within the same layer. Consequently, information flow is primarily confined to feedforward and feedback pathways across layers, which limits lateral interactions and constrains the potential for intra-layer information integration. We introduce an artificial neural network featuring intra-layer connections among hidden neurons to overcome this limitation. Owing to the proposed method for facilitating intra-layer connections, the model is theoretically anticipated to achieve faster convergence compared to conventional feedforward neural networks. The experimental findings provide further validation of the theoretical analysis.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiAReL: Reinforcement Learning with Disturbance Awareness for Robust Sim2Real Policy Transfer in Robot Control</title>
<link>https://arxiv.org/abs/2306.09010</link>
<guid>https://arxiv.org/abs/2306.09010</guid>
<content:encoded><![CDATA[

arXiv:2306.09010v2 Announce Type: replace-cross 
Abstract: Delayed Markov decision processes (DMDPs) fulfill the Markov property by augmenting the state space of agents with a finite time window of recently committed actions. In reliance on these state augmentations, delay-resolved reinforcement learning algorithms train policies to learn optimal interactions with environments featuring observation or action delays. Although such methods can be directly trained on the real robots, due to sample inefficiency, limited resources, or safety constraints, a common approach is to transfer models trained in simulation to the physical robot. However, robotic simulations rely on approximated models of the physical systems, which hinders the sim2real transfer. In this work, we consider various uncertainties in modeling the robot or environment dynamics as unknown intrinsic disturbances applied to the system input. We introduce the disturbance-augmented Markov decision process (DAMDP) in delayed settings as a novel representation to incorporate disturbance estimation in training on-policy reinforcement learning algorithms. The proposed method is validated across several metrics on learning robotic reaching and pushing tasks and compared with disturbance-unaware baselines. The results show that the disturbance-augmented models can achieve higher stabilization and robustness in the control response, which in turn improves the prospects of successful sim2real transfer.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems</title>
<link>https://arxiv.org/abs/2403.02912</link>
<guid>https://arxiv.org/abs/2403.02912</guid>
<content:encoded><![CDATA[

arXiv:2403.02912v2 Announce Type: replace-cross 
Abstract: We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the $\ell_1$ setting. We propose $(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we show that the duality gap is bounded by $\sqrt{\log(d)/n} + \log(d)/\sqrt{n\varepsilon}$ with high probability, by using bias-reduced gradient estimators. This rate provides evidence of the near-optimality of our approach, since a lower bound of $\sqrt{\log(d)/n} + \log(d)^{3/4}/\sqrt{n\varepsilon}$ exists. Finally, we show that combining our methods with acceleration techniques from online learning leads to the first algorithm for DP Stochastic Convex Optimization in the $\ell_1$ setting that is not based on Frank-Wolfe methods. For convex and first-order-smooth stochastic objectives, our algorithms attain an excess risk of $\sqrt{\log(d)/n} + \log(d)^{7/10}/[n\varepsilon]^{2/5}$, and when additionally assuming second-order-smoothness, we improve the rate to $\sqrt{\log(d)/n} + \log(d)/\sqrt{n\varepsilon}$. Instrumental to all of these results are various extensions of the classical Maurey Sparsification Lemma \cite{Pisier:1980}, which may be of independent interest.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian ICA with super-Gaussian Source Priors</title>
<link>https://arxiv.org/abs/2406.17058</link>
<guid>https://arxiv.org/abs/2406.17058</guid>
<content:encoded><![CDATA[

arXiv:2406.17058v3 Announce Type: replace-cross 
Abstract: Independent Component Analysis (ICA) plays a central role in modern machine learning as a flexible framework for feature extraction. We introduce a horseshoe-type prior with a latent Polya-Gamma scale mixture representation, yielding scalable algorithms for both point estimation via expectation-maximization (EM) and full posterior inference via Markov chain Monte Carlo (MCMC). This hierarchical formulation unifies several previously disparate estimation strategies within a single Bayesian framework. We also establish the first theoretical guarantees for hierarchical Bayesian ICA, including posterior contraction and local asymptotic normality results for the unmixing matrix. Comprehensive simulation studies demonstrate that our methods perform competitively with widely used ICA tools. We further discuss implementation of conditional posteriors, envelope-based optimization, and possible extensions to flow-based architectures for nonlinear feature extraction and deep learning. Finally, we outline several promising directions for future work.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Parametric Activation: Unifying and Generalising Activation Functions Across Tasks</title>
<link>https://arxiv.org/abs/2407.08567</link>
<guid>https://arxiv.org/abs/2407.08567</guid>
<content:encoded><![CDATA[

arXiv:2407.08567v3 Announce Type: replace-cross 
Abstract: The activation function plays a crucial role in model optimisation, yet the optimal choice remains unclear. For example, the Sigmoid activation is the de-facto activation in balanced classification tasks, however, in imbalanced classification, it proves inappropriate due to bias towards frequent classes. In this work, we delve deeper in this phenomenon by performing a comprehensive statistical analysis in the classification and intermediate layers of both balanced and imbalanced networks and we empirically show that aligning the activation function with the data distribution, enhances the performance in both balanced and imbalanced tasks. To this end, we propose the Adaptive Parametric Activation (APA) function, a novel and versatile activation function that unifies most common activation functions under a single formula. APA can be applied in both intermediate layers and attention layers, significantly outperforming the state-of-the-art on several imbalanced benchmarks such as ImageNet-LT, iNaturalist2018, Places-LT, CIFAR100-LT and LVIS. Also, we extend APA to a plethora of other tasks such as classification, detection, visual instruction following tasks, image generation and next-text-token prediction benchmarks. APA increases the performance in multiple benchmarks across various model architectures. The code is available at https://github.com/kostas1515/AGLU.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing importance weighting in the presence of sub-population shifts</title>
<link>https://arxiv.org/abs/2410.14315</link>
<guid>https://arxiv.org/abs/2410.14315</guid>
<content:encoded><![CDATA[

arXiv:2410.14315v2 Announce Type: replace-cross 
Abstract: A distribution shift between the training and test data can severely harm performance of machine learning models. Importance weighting addresses this issue by assigning different weights to data points during training. We argue that existing heuristics for determining the weights are suboptimal, as they neglect the increase of the variance of the estimated model due to the finite sample size of the training data. We interpret the optimal weights in terms of a bias-variance trade-off, and propose a bi-level optimization procedure in which the weights and model parameters are optimized simultaneously. We apply this optimization to existing importance weighting techniques for last-layer retraining of deep neural networks in the presence of sub-population shifts and show empirically that optimizing weights significantly improves generalization performance.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Probabilistic Conformal Prediction for Distributed Energy Resources Adoption</title>
<link>https://arxiv.org/abs/2411.12193</link>
<guid>https://arxiv.org/abs/2411.12193</guid>
<content:encoded><![CDATA[

arXiv:2411.12193v3 Announce Type: replace-cross 
Abstract: The rapid growth of distributed energy resources (DERs) presents both opportunities and operational challenges for electric grid management. Accurately predicting DER adoption is critical for proactive infrastructure planning, but the inherent uncertainty and spatial disparity of DER growth complicate traditional forecasting approaches. Moreover, the hierarchical structure of distribution grids demands that predictions satisfy statistical guarantees at both the circuit and substation levels, a non-trivial requirement for reliable decision-making. In this paper, we propose a novel uncertainty quantification framework for DER adoption predictions that ensures validity across hierarchical grid structures. Leveraging a multivariate Hawkes process to model DER adoption dynamics and a tailored split conformal prediction algorithm, we introduce a new nonconformity score that preserves statistical guarantees under aggregation while maintaining prediction efficiency. We establish theoretical validity under mild conditions and demonstrate through empirical evaluation on customer-level solar panel installation data from Indianapolis, Indiana that our method consistently outperforms existing baselines in both predictive accuracy and uncertainty calibration.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying the Limits of Segmentation Foundation Models: Modeling Challenges in Segmenting Tree-Like and Low-Contrast Objects</title>
<link>https://arxiv.org/abs/2412.04243</link>
<guid>https://arxiv.org/abs/2412.04243</guid>
<content:encoded><![CDATA[

arXiv:2412.04243v3 Announce Type: replace-cross 
Abstract: Image segmentation foundation models (SFMs) like Segment Anything Model (SAM) have achieved impressive zero-shot and interactive segmentation across diverse domains. However, they struggle to segment objects with certain structures, particularly those with dense, tree-like morphology and low textural contrast from their surroundings. These failure modes are crucial for understanding the limitations of SFMs in real-world applications. To systematically study this issue, we introduce interpretable metrics quantifying object tree-likeness and textural separability. On carefully controlled synthetic experiments and real-world datasets, we show that SFM performance (\eg, SAM, SAM 2, HQ-SAM) noticeably correlates with these factors. We attribute these failures to SFMs misinterpreting local structure as global texture, resulting in over-segmentation or difficulty distinguishing objects from similar backgrounds. Notably, targeted fine-tuning fails to resolve this issue, indicating a fundamental limitation. Our study provides the first quantitative framework for modeling the behavior of SFMs on challenging structures, offering interpretable insights into their segmentation capabilities.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models</title>
<link>https://arxiv.org/abs/2412.18890</link>
<guid>https://arxiv.org/abs/2412.18890</guid>
<content:encoded><![CDATA[

arXiv:2412.18890v2 Announce Type: replace-cross 
Abstract: The discovery of symbolic solutions -- mathematical expressions, logical rules, and algorithmic structures -- is fundamental to advancing scientific and engineering progress.
  However, traditional methods often struggle with search efficiency and fail to integrate knowledge effectively.
  While recent large language model-based (LLM-based) approaches have demonstrated improvements in search efficiency, they lack the ability to continually refine and expand upon discovered solutions and their underlying knowledge, limiting their potential for open-ended innovation.
  To address these limitations, we introduce CoEvo, a novel framework that leverages large language models within an evolutionary search methodology to continually generate and refine symbolic solutions. CoEvo integrates a dynamic knowledge library, enabling open-ended innovation of solutions through effective knowledge management. Additionally, CoEvo leverages multiple representations of solutions -- including natural language, mathematical expressions, and code -- to further enhance search efficiency.
  By combining the reasoning capabilities of LLMs with the exploratory power of evolutionary algorithms, CoEvo significantly improves the efficiency and scope of symbolic discovery.
  Our experimental results demonstrate that this method not only enhances the efficiency of searching for symbolic solutions but also supports the ongoing discovery process, akin to human scientific endeavors. This study represents a first effort in conceptualizing the search for symbolic solutions as a lifelong, iterative
  process, marking a significant step towards harnessing LLMs in the perpetual pursuit of scientific and engineering breakthroughs.
  Our code is available at https://github.com/pgg3/CoEvo.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Temporal Trap: Entanglement in Pre-Trained Visual Representations for Visuomotor Policy Learning</title>
<link>https://arxiv.org/abs/2502.03270</link>
<guid>https://arxiv.org/abs/2502.03270</guid>
<content:encoded><![CDATA[

arXiv:2502.03270v3 Announce Type: replace-cross 
Abstract: The integration of pre-trained visual representations (PVRs) has significantly advanced visuomotor policy learning. However, effectively leveraging these models remains a challenge. We identify temporal entanglement as a critical, inherent issue when using these time-invariant models in sequential decision-making tasks. This entanglement arises because PVRs, optimised for static image understanding, struggle to represent the temporal dependencies crucial for visuomotor control. In this work, we quantify the impact of temporal entanglement, demonstrating a strong correlation between a policy's success rate and the ability of its latent space to capture task-progression cues. Based on these insights, we propose a simple, yet effective disentanglement baseline designed to mitigate temporal entanglement. Our empirical results show that traditional methods aimed at enriching features with temporal components are insufficient on their own, highlighting the necessity of explicitly addressing temporal disentanglement for robust visuomotor policy learning.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Computational Advantage of Depth: Learning High-Dimensional Hierarchical Functions with Gradient Descent</title>
<link>https://arxiv.org/abs/2502.13961</link>
<guid>https://arxiv.org/abs/2502.13961</guid>
<content:encoded><![CDATA[

arXiv:2502.13961v4 Announce Type: replace-cross 
Abstract: Understanding the advantages of deep neural networks trained by gradient descent (GD) compared to shallow models remains an open theoretical challenge. In this paper, we introduce a class of target functions (single and multi-index Gaussian hierarchical targets) that incorporate a hierarchy of latent subspace dimensionalities. This framework enables us to analytically study the learning dynamics and generalization performance of deep networks compared to shallow ones in the high-dimensional limit. Specifically, our main theorem shows that feature learning with GD successively reduces the effective dimensionality, transforming a high-dimensional problem into a sequence of lower-dimensional ones. This enables learning the target function with drastically less samples than with shallow networks. While the results are proven in a controlled training setting, we also discuss more common training procedures and argue that they learn through the same mechanisms.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walk Before You Dance: High-fidelity and Editable Dance Synthesis via Generative Masked Motion Prior</title>
<link>https://arxiv.org/abs/2504.04634</link>
<guid>https://arxiv.org/abs/2504.04634</guid>
<content:encoded><![CDATA[

arXiv:2504.04634v2 Announce Type: replace-cross 
Abstract: Recent advances in dance generation have enabled the automatic synthesis of 3D dance motions. However, existing methods still face significant challenges in simultaneously achieving high realism, precise dance-music synchronization, diverse motion expression, and physical plausibility. To address these limitations, we propose a novel approach that leverages a generative masked text-to-motion model as a distribution prior to learn a probabilistic mapping from diverse guidance signals, including music, genre, and pose, into high-quality dance motion sequences. Our framework also supports semantic motion editing, such as motion inpainting and body part modification. Specifically, we introduce a multi-tower masked motion model that integrates a text-conditioned masked motion backbone with two parallel, modality-specific branches: a music-guidance tower and a pose-guidance tower. The model is trained using synchronized and progressive masked training, which allows effective infusion of the pretrained text-to-motion prior into the dance synthesis process while enabling each guidance branch to optimize independently through its own loss function, mitigating gradient interference. During inference, we introduce classifier-free logits guidance and pose-guided token optimization to strengthen the influence of music, genre, and pose signals. Extensive experiments demonstrate that our method sets a new state of the art in dance generation, significantly advancing both the quality and editability over existing approaches. Project Page available at https://foram-s1.github.io/DanceMosaic/
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical Echo State Network Reservoir Computing</title>
<link>https://arxiv.org/abs/2504.08224</link>
<guid>https://arxiv.org/abs/2504.08224</guid>
<content:encoded><![CDATA[

arXiv:2504.08224v3 Announce Type: replace-cross 
Abstract: We propose an innovative design for an optical Echo State Network (ESN), an advanced type of reservoir computer known for its universal computational capabilities. Our design enables an optical implementation of arbitrary ESNs, featuring flexibility in optical matrix multiplication and nonlinear activation. Leveraging the nonlinear characteristics of stimulated Brillouin scattering (SBS), the architecture efficiently realizes measurement-free nonlinear activation. The approach significantly reduces computational overhead and energy consumption compared to traditional software-based methods. Comprehensive simulations validate the system's memory capacity, nonlinear processing strength, and polynomial algebra capabilities, showcasing performance comparable to software ESNs across key benchmark tasks. Our design establishes a feasible, scalable, and universally applicable framework for optical reservoir computing, suitable for diverse machine learning applications.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpolation Conditions for Data Consistency and Prediction in Noisy Linear Systems</title>
<link>https://arxiv.org/abs/2504.08484</link>
<guid>https://arxiv.org/abs/2504.08484</guid>
<content:encoded><![CDATA[

arXiv:2504.08484v2 Announce Type: replace-cross 
Abstract: We develop an interpolation-based framework for noisy linear systems with unknown system matrix with bounded norm (implying bounded growth or non-increasing energy), and bounded process noise energy. The proposed approach characterizes all trajectories consistent with the measured data and these prior bounds in a purely data-driven manner. This characterization enables data-consistency verification, inference, and one-step ahead prediction, which can be leveraged for safety verification and cost minimization. Ultimately, this work represents a preliminary step toward exploiting interpolation conditions in data-driven control, offering a systematic way to characterize trajectories consistent with a dynamical system within a given class and enabling their use in control design.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Federated Learning Meets Quantum Computing: Survey and Research Opportunities</title>
<link>https://arxiv.org/abs/2504.08814</link>
<guid>https://arxiv.org/abs/2504.08814</guid>
<content:encoded><![CDATA[

arXiv:2504.08814v4 Announce Type: replace-cross 
Abstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. With the introduction of two novel metrics, qubit utilization efficiency and quantum model training strategy, we present a thorough analysis of the current status of the QFL research. This work explores key developments and integration strategies, along with the impact of QC on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shifting Work Patterns with Generative AI</title>
<link>https://arxiv.org/abs/2504.11436</link>
<guid>https://arxiv.org/abs/2504.11436</guid>
<content:encoded><![CDATA[

arXiv:2504.11436v4 Announce Type: replace-cross 
Abstract: We present evidence from a field experiment across 66 firms and 7,137 knowledge workers. Workers were randomly selected to access a generative AI tool integrated into applications they already used at work for email, meetings, and writing. In the second half of the 6-month experiment, the 80% of treated workers who used this tool spent two fewer hours on email each week and reduced their time working outside of regular hours. Apart from these individual time savings, we do not detect shifts in the quantity or composition of workers' tasks resulting from individual-level AI provision.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\textit{New News}$: System-2 Fine-tuning for Robust Integration of New Knowledge</title>
<link>https://arxiv.org/abs/2505.01812</link>
<guid>https://arxiv.org/abs/2505.01812</guid>
<content:encoded><![CDATA[

arXiv:2505.01812v3 Announce Type: replace-cross 
Abstract: Humans and intelligent animals can internalize new information and accurately internalize their implications to perform downstream tasks. While large language models (LLMs) can achieve this through in-context learning (ICL) when the information (news) is explicitly given as context, adequately integrating the information into model weights via fine-tuning remains challenging. In this paper, we introduce New News, a dataset composed of hypothetical yet plausible news spanning multiple domains (mathematics, coding, discoveries, leaderboards, events), accompanied by downstream evaluation questions whose correct answers critically depend on understanding and internalizing the news. First, we demonstrate a substantial gap between naive fine-tuning and in-context learning (FT-ICL gap) on our dataset. To address this gap, we explore a suite of self-play data generation protocols -- paraphrases, implications, and Self-QA -- designed to distill the knowledge processed by the model with context into the weights of the model, which we term System-2 Fine-tuning (Sys2-FT). We systematically evaluate ICL and Sys2-FT performance across data domains and model scales with the Qwen 2.5 family of models. Our results demonstrate that the Self-QA protocol of Sys2-FT significantly improves models' in-weight learning of the news while preserving general capabilities. Furthermore, we discover the contextual shadowing effect, where training with the news in context followed by its rephrases or QAs catastrophically degrades learning of the news. Finally, we show preliminary evidence of an emerging scaling law of Sys2-FT.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonlinear Laplacians: Tunable principal component analysis under directional prior information</title>
<link>https://arxiv.org/abs/2505.12528</link>
<guid>https://arxiv.org/abs/2505.12528</guid>
<content:encoded><![CDATA[

arXiv:2505.12528v2 Announce Type: replace-cross 
Abstract: We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation $\mathbf{Y}$, our algorithms construct a nonlinear Laplacian, another matrix of the form $\mathbf{Y}+\mathrm{diag}(\sigma(\mathbf{Y1}))$ for a nonlinear $\sigma:\mathbb{R}\to\mathbb{R}$, and examine the top eigenvalue and eigenvector of this matrix. When $\mathbf{Y}$ is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph "deformed" by the degree profile $\mathbf{Y1}$. We study the performance of such algorithms compared to direct spectral algorithms (the case $\sigma=0$) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of $\sigma$, required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the $\sigma$ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design $\sigma$ numerically: exhaustively searching over simple classes of $\sigma$, learning $\sigma$ from datasets of problem instances, and tuning $\sigma$ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if $\sigma$ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Copilot: Learning from The Mistake Log in LLM Fine-tuning</title>
<link>https://arxiv.org/abs/2505.16270</link>
<guid>https://arxiv.org/abs/2505.16270</guid>
<content:encoded><![CDATA[

arXiv:2505.16270v2 Announce Type: replace-cross 
Abstract: Large language models are typically adapted to downstream tasks through supervised fine-tuning on domain-specific data. While standard fine-tuning focuses on minimizing generation loss to optimize model parameters, we take a deeper step by retaining and leveraging the model's own learning signals, analogous to how human learners reflect on past mistakes to improve future performance. We first introduce the concept of Mistake Log to systematically track the model's learning behavior and recurring errors throughout fine-tuning. Treating the original transformer-based model as the Pilot, we correspondingly design a Copilot model to refine the Pilot's inference performance via logits rectification. We name the overall Pilot-Copilot framework the Transformer Copilot, which introduces (i) a novel Copilot model design, (ii) a joint training paradigm where the Copilot continuously learns from the evolving Mistake Log alongside the Pilot, and (iii) a fused inference paradigm where the Copilot rectifies the Pilot's logits for enhanced generation. We provide both theoretical and empirical analyses on our new learning framework. Experiments on 12 benchmarks spanning commonsense, arithmetic, and recommendation tasks demonstrate that Transformer Copilot consistently improves performance by up to 34.5%, while introducing marginal computational overhead to Pilot models and exhibiting strong scalability and transferability. Our code is released at https://github.com/jiaruzouu/TransformerCopilot.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Principle Discovery for Language Model Self-Improvement</title>
<link>https://arxiv.org/abs/2505.16927</link>
<guid>https://arxiv.org/abs/2505.16927</guid>
<content:encoded><![CDATA[

arXiv:2505.16927v2 Announce Type: replace-cross 
Abstract: When language model (LM) users aim to improve the quality of its generations, it is crucial to specify concrete behavioral attributes that the model should strive to reflect. However, curating such principles across many domains, even non-exhaustively, requires a labor-intensive annotation process. To automate this process, we propose eliciting these latent attributes that guide model reasoning toward human-preferred responses by explicitly modeling them in a self-correction setting. Our approach mines new principles from the LM itself and compresses the discovered elements to an interpretable set via clustering. Specifically, we employ a form of posterior-regularized Monte Carlo Expectation-Maximization to both identify a condensed set of the most effective latent principles and teach the LM to strategically invoke them in order to intrinsically refine its responses. We demonstrate that bootstrapping our algorithm over multiple iterations enables smaller language models (7-8B parameters) to self-improve, achieving +8-10% in AlpacaEval win-rate, an average of +0.3 on MT-Bench, and +19-23% in principle-following win-rate on IFEval. We also show that clustering the principles yields interpretable and diverse model-generated constitutions while retaining model performance. The gains that our method achieves highlight the potential of automated, principle-driven post-training recipes toward continual self-improvement.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensory-Motor Control with Large Language Models via Iterative Policy Refinement</title>
<link>https://arxiv.org/abs/2506.04867</link>
<guid>https://arxiv.org/abs/2506.04867</guid>
<content:encoded><![CDATA[

arXiv:2506.04867v3 Announce Type: replace-cross 
Abstract: We propose a method that enables large language models (LLMs) to control embodied agents through the generation of control policies that directly map continuous observation vectors to continuous action vectors. At the outset, the LLMs generate a control strategy based on a textual description of the agent, its environment, and the intended goal. This strategy is then iteratively refined through a learning process in which the LLMs are repeatedly prompted to improve the current strategy, using performance feedback and sensory-motor data collected during its evaluation. The method is validated on classic control tasks from the Gymnasium library and the inverted pendulum task from the MuJoCo library. The approach proves effective with relatively compact models such as GPT-oss:120b and Qwen2.5:72b. In most cases, it successfully identifies optimal or near-optimal solutions by integrating symbolic knowledge derived through reasoning with sub-symbolic sensory-motor data gathered as the agent interacts with its environment.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-supervised Learning of Echocardiographic Video Representations via Online Cluster Distillation</title>
<link>https://arxiv.org/abs/2506.11777</link>
<guid>https://arxiv.org/abs/2506.11777</guid>
<content:encoded><![CDATA[

arXiv:2506.11777v2 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has achieved major advances in natural images and video understanding, but challenges remain in domains like echocardiography (heart ultrasound) due to subtle anatomical structures, complex temporal dynamics, and the current lack of domain-specific pre-trained models. Existing SSL approaches such as contrastive, masked modeling, and clustering-based methods struggle with high intersample similarity, sensitivity to low PSNR inputs common in ultrasound, or aggressive augmentations that distort clinically relevant features. We present DISCOVR (Distilled Image Supervision for Cross Modal Video Representation), a self-supervised dual branch framework for cardiac ultrasound video representation learning. DISCOVR combines a clustering-based video encoder that models temporal dynamics with an online image encoder that extracts fine-grained spatial semantics. These branches are connected through a semantic cluster distillation loss that transfers anatomical knowledge from the evolving image encoder to the video encoder, enabling temporally coherent representations enriched with fine-grained semantic understanding.Evaluated on six echocardiography datasets spanning fetal, pediatric, and adult populations, DISCOVR outperforms both specialized video anomaly detection methods and state-of-the-art video-SSL baselines in zero-shot and linear probing setups,achieving superior segmentation transfer and strong downstream performance on clinically relevant tasks such as LVEF prediction. Code available at: https://github.com/mdivyanshu97/DISCOVR
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC</title>
<link>https://arxiv.org/abs/2506.14987</link>
<guid>https://arxiv.org/abs/2506.14987</guid>
<content:encoded><![CDATA[

arXiv:2506.14987v2 Announce Type: replace-cross 
Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a CNN-based dynamic priority prediction mechanism for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach uses a Convolutional Neural Network and graph coloring to adaptively assign link priorities based on real-time traffic, transmission opportunities, and network conditions. Assuming that first training phase is performed offline, our approach introduced minimal overhead, while enabling more efficient resource allocation, boosting network capacity, SINR, and schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and 49\% over LDP across three network configurations, highlighting its effectiveness for complex URLLC scenarios.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StreamDiT: Real-Time Streaming Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2507.03745</link>
<guid>https://arxiv.org/abs/2507.03745</guid>
<content:encoded><![CDATA[

arXiv:2507.03745v3 Announce Type: replace-cross 
Abstract: Recently, great progress has been achieved in text-to-video (T2V) generation by scaling transformer-based diffusion models to billions of parameters, which can generate high-quality videos. However, existing models typically produce only short clips offline, restricting their use cases in interactive and real-time applications. This paper addresses these challenges by proposing StreamDiT, a streaming video generation model. StreamDiT training is based on flow matching by adding a moving buffer. We design mixed training with different partitioning schemes of buffered frames to boost both content consistency and visual quality. StreamDiT modeling is based on adaLN DiT with varying time embedding and window attention. To practice the proposed method, we train a StreamDiT model with 4B parameters. In addition, we propose a multistep distillation method tailored for StreamDiT. Sampling distillation is performed in each segment of a chosen partitioning scheme. After distillation, the total number of function evaluations (NFEs) is reduced to the number of chunks in a buffer. Finally, our distilled model reaches real-time performance at 16 FPS on one GPU, which can generate video streams at 512p resolution. We evaluate our method through both quantitative metrics and human evaluation. Our model enables real-time applications, e.g. streaming generation, interactive generation, and video-to-video. We provide video results and more examples in our project website: https://cumulo-autumn.github.io/StreamDiT/
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comprehension Without Competence: Architectural Limits of LLMs in Symbolic Computation and Reasoning</title>
<link>https://arxiv.org/abs/2507.10624</link>
<guid>https://arxiv.org/abs/2507.10624</guid>
<content:encoded><![CDATA[

arXiv:2507.10624v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) display striking surface fluency yet systematically fail at tasks requiring symbolic reasoning, arithmetic accuracy, and logical consistency. This paper offers a structural diagnosis of such failures, revealing a persistent gap between \textit{comprehension} and \textit{competence}. Through controlled experiments and architectural analysis, we demonstrate that LLMs often articulate correct principles without reliably applying them--a failure rooted not in knowledge access, but in computational execution. We term this phenomenon the computational \textit{split-brain syndrome}, where instruction and action pathways are geometrically and functionally dissociated. This core limitation recurs across domains, from mathematical operations to relational inferences, and explains why model behavior remains brittle even under idealized prompting. We argue that LLMs function as powerful pattern completion engines, but lack the architectural scaffolding for principled, compositional reasoning. Our findings delineate the boundary of current LLM capabilities and motivate future models with metacognitive control, principle lifting, and structurally grounded execution. This diagnosis also clarifies why mechanistic interpretability findings may reflect training-specific pattern coordination rather than universal computational principles, and why the geometric separation between instruction and execution pathways suggests limitations in neural introspection and mechanistic analysis.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMA: Enhancing Mathematical Reasoning in Large Language Models with Causal Knowledge</title>
<link>https://arxiv.org/abs/2508.02583</link>
<guid>https://arxiv.org/abs/2508.02583</guid>
<content:encoded><![CDATA[

arXiv:2508.02583v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated strong performance across a wide range of tasks, yet they still struggle with complex mathematical reasoning, a challenge fundamentally rooted in deep structural dependencies. To address this challenge, we propose \textbf{CA}usal \textbf{MA}thematician (\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit, reusable mathematical structure. In the learning stage, CAMA first constructs the \textbf{M}athematical \textbf{C}ausal \textbf{G}raph (\textbf{MCG}), a high-level representation of solution strategies, by combining LLM priors with causal discovery algorithms applied to a corpus of question-solution pairs. The resulting MCG encodes essential knowledge points and their causal dependencies. To better align the graph with downstream reasoning tasks, CAMA further refines the MCG through iterative feedback derived from a selected subset of the question-solution pairs. In the reasoning stage, given a new question, CAMA dynamically extracts a task-relevant subgraph from the MCG, conditioned on both the question content and the LLM's intermediate reasoning trace. This subgraph, which encodes the most pertinent knowledge points and their causal dependencies, is then injected back into the LLM to guide its reasoning process. Empirical results on real-world datasets show that CAMA significantly improves LLM performance on challenging mathematical problems. Furthermore, our experiments demonstrate that structured guidance consistently outperforms unstructured alternatives, and that incorporating asymmetric causal relationships yields greater improvements than using symmetric associations alone.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Format as a Prior: Quantifying and Analyzing Bias in LLMs for Heterogeneous Data</title>
<link>https://arxiv.org/abs/2508.15793</link>
<guid>https://arxiv.org/abs/2508.15793</guid>
<content:encoded><![CDATA[

arXiv:2508.15793v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly employed in applications that require processing information from heterogeneous formats, including texts, tables, infoboxes, and knowledge graphs. However, systematic biases toward particular formats may undermine LLMs' ability to integrate heterogeneous data impartially, potentially resulting in reasoning errors and increased risks in downstream tasks. Yet it remains unclear whether such biases are systematic, which data-level factors drive them, and what internal mechanisms underlie their emergence.
  In this paper, we present the first comprehensive study of format bias in LLMs through a three-stage empirical analysis. The first stage explores the presence and direction of bias across a diverse range of LLMs. The second stage examines how key data-level factors influence these biases. The third stage analyzes how format bias emerges within LLMs' attention patterns and evaluates a lightweight intervention to test its effectiveness. Our results show that format bias is consistent across model families, driven by information richness, structure quality, and representation type, and is closely associated with attention imbalance within the LLMs. Based on these investigations, we identify three future research directions to reduce format bias: enhancing data pre-processing through format repair and normalization, introducing inference-time interventions such as attention re-weighting, and developing format-balanced training corpora. These directions will support the design of more robust and fair heterogeneous data processing systems.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2508.19257</link>
<guid>https://arxiv.org/abs/2508.19257</guid>
<content:encoded><![CDATA[

arXiv:2508.19257v3 Announce Type: replace-cross 
Abstract: Vision-Language-Action (VLA) models process visual inputs independently at each timestep, discarding valuable temporal information inherent in robotic manipulation tasks. This frame-by-frame processing makes models vulnerable to visual noise while ignoring the substantial coherence between consecutive frames in manipulation sequences. We propose Temporal Token Fusion (TTF), a training-free approach that intelligently integrates historical and current visual representations to enhance VLA inference quality. Our method employs dual-dimension detection combining efficient grayscale pixel difference analysis with attention-based semantic relevance assessment, enabling selective temporal token fusion through hard fusion strategies and keyframe anchoring to prevent error accumulation. Comprehensive experiments across LIBERO, SimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0 percentage points average on LIBERO (72.4\% vs 68.4\% baseline), cross-environment validation on SimplerEnv (4.8\% relative improvement), and 8.7\% relative improvement on real robot tasks. Our approach proves model-agnostic, working across OpenVLA and VLA-Cache architectures. Notably, TTF reveals that selective Query matrix reuse in attention mechanisms enhances rather than compromises performance, suggesting promising directions for direct KQV matrix reuse strategies that achieve computational acceleration while improving task success rates.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequentially Auditing Differential Privacy</title>
<link>https://arxiv.org/abs/2509.07055</link>
<guid>https://arxiv.org/abs/2509.07055</guid>
<content:encoded><![CDATA[

arXiv:2509.07055v2 Announce Type: replace-cross 
Abstract: We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \textit{under} one training run, unlike prior methods needing full model training.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2509.15695</link>
<guid>https://arxiv.org/abs/2509.15695</guid>
<content:encoded><![CDATA[

arXiv:2509.15695v2 Announce Type: replace-cross 
Abstract: Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs. The code is available at https://github.com/ZhaoyangLi-1/ORIC.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging NTPs for Efficient Hallucination Detection in VLMs</title>
<link>https://arxiv.org/abs/2509.20379</link>
<guid>https://arxiv.org/abs/2509.20379</guid>
<content:encoded><![CDATA[

arXiv:2509.20379v2 Announce Type: replace-cross 
Abstract: Hallucinations of vision-language models (VLMs), which are misalignments between visual content and generated text, undermine the reliability of VLMs. One common approach for detecting them employs the same VLM, or a different one, to assess generated outputs. This process is computationally intensive and increases model latency. In this paper, we explore an efficient on-the-fly method for hallucination detection by training traditional ML models over signals based on the VLM's next-token probabilities (NTPs). NTPs provide a direct quantification of model uncertainty. We hypothesize that high uncertainty (i.e., a low NTP value) is strongly associated with hallucinations. To test this, we introduce a dataset of 1,400 human-annotated statements derived from VLM-generated content, each labeled as hallucinated or not, and use it to test our NTP-based lightweight method. Our results demonstrate that NTP-based features are valuable predictors of hallucinations, enabling fast and simple ML models to achieve performance comparable to that of strong VLMs. Furthermore, augmenting these NTPs with linguistic NTPs, computed by feeding only the generated text back into the VLM, enhances hallucination detection performance. Finally, integrating hallucination prediction scores from VLMs into the NTP-based models led to better performance than using either VLMs or NTPs alone. We hope this study paves the way for simple, lightweight solutions that enhance the reliability of VLMs.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Verified Code Reasoning by LLMs</title>
<link>https://arxiv.org/abs/2509.26546</link>
<guid>https://arxiv.org/abs/2509.26546</guid>
<content:encoded><![CDATA[

arXiv:2509.26546v2 Announce Type: replace-cross 
Abstract: While LLM-based agents are able to tackle a wide variety of code reasoning questions, the answers are not always correct. This prevents the agent from being useful in situations where high precision is desired: (1) helping a software engineer understand a new code base, (2) helping a software engineer during code review sessions, and (3) ensuring that the code generated by an automated code generation system meets certain requirements (e.g. fixes a bug, improves readability, implements a feature).
  As a result of this lack of trustworthiness, the agent's answers need to be manually verified before they can be trusted. Manually confirming responses from a code reasoning agent requires human effort and can result in slower developer productivity, which weakens the assistance benefits of the agent. In this paper, we describe a method to automatically validate the answers provided by a code reasoning agent by verifying its reasoning steps. At a very high level, the method consists of extracting a formal representation of the agent's response and, subsequently, using formal verification and program analysis tools to verify the agent's reasoning steps.
  We applied this approach to a benchmark set of 20 uninitialized variable errors detected by sanitizers and 20 program equivalence queries. For the uninitialized variable errors, the formal verification step was able to validate the agent's reasoning on 13/20 examples, and for the program equivalence queries, the formal verification step successfully caught 6/8 incorrect judgments made by the agent.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects</title>
<link>https://arxiv.org/abs/2510.06188</link>
<guid>https://arxiv.org/abs/2510.06188</guid>
<content:encoded><![CDATA[

arXiv:2510.06188v2 Announce Type: replace-cross 
Abstract: Real-time speech assistants are becoming increasingly popular for ensuring improved accessibility to information. Bengali, being a low-resource language with a high regional dialectal diversity, has seen limited progress in developing such systems. Existing systems are not optimized for real-time use and focus only on standard Bengali. In this work, we present BanglaTalk, the first real-time speech assistance system for Bengali regional dialects. BanglaTalk follows the client-server architecture and uses the Real-time Transport Protocol (RTP) to ensure low-latency communication. To address dialectal variation, we introduce a dialect-aware ASR system, BRDialect, developed by fine-tuning the IndicWav2Vec model in ten Bengali regional dialects. It outperforms the baseline ASR models by 12.41-33.98% on the RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of 24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low bandwidth usage and minimal end-to-end delay make the system both cost-effective and interactive for real-time use cases, enabling inclusive and accessible speech technology for the diverse community of Bengali speakers. Code is available in https://github.com/Jak57/BanglaTalk
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Augmented data and neural networks for robust epidemic forecasting: application to COVID-19 in Italy</title>
<link>https://arxiv.org/abs/2510.09192</link>
<guid>https://arxiv.org/abs/2510.09192</guid>
<content:encoded><![CDATA[

arXiv:2510.09192v2 Announce Type: replace-cross 
Abstract: In this work, we propose a data augmentation strategy aimed at improving the training phase of neural networks and, consequently, the accuracy of their predictions. Our approach relies on generating synthetic data through a suitable compartmental model combined with the incorporation of uncertainty. The available data are then used to calibrate the model, which is further integrated with deep learning techniques to produce additional synthetic data for training. The results show that neural networks trained on these augmented datasets exhibit significantly improved predictive performance. We focus in particular on two different neural network architectures: Physics-Informed Neural Networks (PINNs) and Nonlinear Autoregressive (NAR) models. The NAR approach proves especially effective for short-term forecasting, providing accurate quantitative estimates by directly learning the dynamics from data and avoiding the additional computational cost of embedding physical constraints into the training. In contrast, PINNs yield less accurate quantitative predictions but capture the qualitative long-term behavior of the system, making them more suitable for exploring broader dynamical trends. Numerical simulations of the second phase of the COVID-19 pandemic in the Lombardy region (Italy) validate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Speech Emotion Recognition with Mutual Information Regularized Generative Model</title>
<link>https://arxiv.org/abs/2510.10078</link>
<guid>https://arxiv.org/abs/2510.10078</guid>
<content:encoded><![CDATA[

arXiv:2510.10078v2 Announce Type: replace-cross 
Abstract: Although speech emotion recognition (SER) research has been advanced, thanks to deep learning methods, it still suffers from obtaining inputs from large quality-labelled training data. Data augmentation methods have been attempted to mitigate this issue, generative models have shown success among them recently. We propose a data augmentation framework that is aided by cross-modal information transfer and mutual information regularization. Mutual information based metric can serve as an indicator for the quality. Furthermore, we expand this data augmentation scope to multimodal inputs, thanks to mutual information ensureing dependency between modalities. Our framework was tested on three benchmark datasets: IEMOCAP, MSP-IMPROV and MSP-Podcast. The implementation was designed to generate input features that are fed into last layer for emotion classification. Our framework improved the performance of emotion prediction against existing works. Also, we discovered that our framework is able to generate new inputs without any cross-modal information.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A filtering scheme for confocal laser endomicroscopy (CLE)-video sequences for self-supervised learning</title>
<link>https://arxiv.org/abs/2511.00098</link>
<guid>https://arxiv.org/abs/2511.00098</guid>
<content:encoded><![CDATA[

arXiv:2511.00098v2 Announce Type: replace-cross 
Abstract: Confocal laser endomicroscopy (CLE) is a non-invasive, real-time imaging modality that can be used for in-situ, in-vivo imaging and the microstructural analysis of mucous structures. The diagnosis using CLE is, however, complicated by images being hard to interpret for non-experienced physicians. Utilizing machine learning as an augmentative tool would hence be beneficial, but is complicated by the shortage of histopathology-correlated CLE imaging sequences with respect to the plurality of patterns in this domain, leading to overfitting of machine learning models. To overcome this, self-supervised learning (SSL) can be employed on larger unlabeled datasets. CLE is a video-based modality with high inter-frame correlation, leading to a non-stratified data distribution for SSL training. In this work, we propose a filter functionality on CLE video sequences to reduce the dataset redundancy in SSL training and improve SSL training convergence and training efficiency. We use four state-of-the-art baseline networks and a SSL teacher-student network with a vision transformer small backbone for the evaluation. These networks were evaluated on downstream tasks for a sinonasal tumor dataset and a squamous cell carcinoma of the skin dataset. On both datasets, we found the highest test accuracy on the filtered SSL-pretrained model, with 67.48% and 73.52%, both considerably outperforming their non-SSL baselines. Our results show that SSL is an effective method for CLE pretraining. Further, we show that our proposed CLE video filter can be utilized to improve training efficiency in self-supervised scenarios, resulting in a reduction of 67% in training time.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Detection of Software Aging under Workload Shift</title>
<link>https://arxiv.org/abs/2511.03103</link>
<guid>https://arxiv.org/abs/2511.03103</guid>
<content:encoded><![CDATA[

arXiv:2511.03103v2 Announce Type: replace-cross 
Abstract: Software aging is a phenomenon that affects long-running systems, leading to progressive performance degradation and increasing the risk of failures. To mitigate this problem, this work proposes an adaptive approach based on machine learning for software aging detection in environments subject to dynamic workload conditions. We evaluate and compare a static model with adaptive models that incorporate adaptive detectors, specifically the Drift Detection Method (DDM) and Adaptive Windowing (ADWIN), originally developed for concept drift scenarios and applied in this work to handle workload shifts. Experiments with simulated sudden, gradual, and recurring workload transitions show that static models suffer a notable performance drop when applied to unseen workload profiles, whereas the adaptive model with ADWIN maintains high accuracy, achieving an F1-Score above 0.93 in all analyzed scenarios.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>YOLO-SAT: A Data-based and Model-based Enhanced YOLOv12 Model for Desert Waste Detection and Classification</title>
<link>https://arxiv.org/abs/2511.03888</link>
<guid>https://arxiv.org/abs/2511.03888</guid>
<content:encoded><![CDATA[

arXiv:2511.03888v2 Announce Type: replace-cross 
Abstract: The global waste crisis is escalating, with solid waste generation expected to increase tremendously in the coming years. Traditional waste collection methods, particularly in remote or harsh environments like deserts, are labor-intensive, inefficient, and often hazardous. Recent advances in computer vision and deep learning have opened the door to automated waste detection systems, yet most research focuses on urban environments and recyclable materials, overlooking organic and hazardous waste and underexplored terrains such as deserts. In this work, we propose YOLO-SAT, an enhanced real-time object detection framework based on a pruned, lightweight version of YOLOv12 integrated with Self-Adversarial Training (SAT) and specialized data augmentation strategies. Using the DroneTrashNet dataset, we demonstrate significant improvements in precision, recall, and mean average precision (mAP), while achieving low latency and compact model size suitable for deployment on resource-constrained aerial drones. Benchmarking YOLO-SAT against state-of-the-art lightweight YOLO variants further highlights its optimal balance of accuracy and efficiency. Our results validate the effectiveness of combining data-centric and model-centric enhancements for robust, real-time waste detection in desert environments.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSGaze: Context-aware Social Gaze Prediction</title>
<link>https://arxiv.org/abs/2511.05955</link>
<guid>https://arxiv.org/abs/2511.05955</guid>
<content:encoded><![CDATA[

arXiv:2511.05955v2 Announce Type: replace-cross 
Abstract: A person's gaze offers valuable insights into their focus of attention, level of social engagement, and confidence. In this work, we investigate how contextual cues combined with visual scene and facial information can be effectively utilized to predict and interpret social gaze patterns during conversational interactions. We introduce CSGaze, a context aware multimodal approach that leverages facial, scene information as complementary inputs to enhance social gaze pattern prediction from multi-person images. The model also incorporates a fine-grained attention mechanism centered on the principal speaker, which helps in better modeling social gaze dynamics. Experimental results show that CSGaze performs competitively with state-of-the-art methods on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of contextual cues in improving social gaze prediction. Additionally, we provide initial explainability through generated attention scores, offering insights into the model's decision-making process. We also demonstrate our model's generalizability by testing our model on open set datasets that demonstrating its robustness across diverse scenarios.
]]></content:encoded>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
<pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>The Markovian Thinker: Architecture-Agnostic Linear Scaling of Reasoning</title>
<link>https://arxiv.org/abs/2510.06557</link>
<guid>https://arxiv.org/abs/2510.06557</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Markovian Thinking, Delethink, Long Chains of Thought, Efficient Reasoning<br /><br />Summary: This work addresses the computational inefficiency of reinforcement learning (RL) approaches for training large language models (LLMs) to perform long chains of thought (LongCoT) reasoning. Traditional RL environments treat the state as the prompt plus all prior tokens, leading to unbounded state size and quadratic compute costs with increasing reasoning length. The authors introduce Markovian Thinking, a new paradigm where the policy conditions on a fixed-size state, decoupling reasoning length from context size and enabling linear compute with constant memory usage. They implement this concept in Delethink, an RL environment that segments reasoning into fixed-size chunks, resetting context boundaries and carrying over a concise textual state to allow seamless continuation of thought. A 1.5B parameter R1-Distill model trained with Delethink can reason over 8K-token chunks while effectively thinking up to 24K tokens, matching or outperforming existing LongCoT-RL models with larger budget requirements. At scale, Delethink continues to improve beyond lengths where LongCoT plateaus. The linear compute advantage significantly reduces training costs, exemplified by a comparison showing Delethink requires 7 H100-months versus 27 for LongCoT-RL at an estimated 96K token reasoning length. Initial analysis shows many off-the-shelf reasoning models sample Markovian traces zero-shot, aiding effective RL training. Overall, redesigning the reasoning environment enables much longer, more efficient, and scalable reasoning in LLMs. <div>
arXiv:2510.06557v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has recently become a strong recipe for training reasoning LLMs that produce long chains of thought (LongCoT). Yet the standard RL "thinking environment", where the state is the prompt plus all prior reasoning tokens, makes the state unbounded and forces attention-based policies to pay quadratic compute as thoughts lengthen. We revisit the environment itself. We propose Markovian Thinking, a paradigm in which the policy advances reasoning while conditioning on a constant-size state, decoupling thinking length from context size. As an immediate consequence this yields linear compute with constant memory. We instantiate this idea with Delethink, an RL environment that structures reasoning into fixed-size chunks. Within each chunk, the model thinks as usual; at the boundary, the environment resets the context and reinitializes the prompt with a short carryover. Through RL, the policy learns to write a textual state near the end of each chunk sufficient for seamless continuation of reasoning after reset. Trained in this environment, an R1-Distill 1.5B model reasons in 8K-token chunks yet thinks up to 24K tokens, matching or surpassing LongCoT-RL trained with a 24K budget. With test-time scaling, Delethink continues to improve where LongCoT plateaus. The effect of linear compute is substantial: we empirically estimate at 96K average thinking length LongCoT-RL costs 27 H100-months vs. 7 for Delethink. Analysis at RL initialization shows off-the-shelf reasoning models (1.5B-120B) often sample Markovian traces zero-shot across diverse benchmarks, providing positive samples that make RL effective at scale. Our results show that redesigning the thinking environment is a powerful lever: it enables very long reasoning without quadratic overhead and opens a path toward efficient, scalable reasoning LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model</title>
<link>https://arxiv.org/abs/2511.06040</link>
<guid>https://arxiv.org/abs/2511.06040</guid>
<content:encoded><![CDATA[
<div> Keywords: spiked Wigner matrices, correlated signals, detection algorithm, computational threshold, low-degree polynomial method<br /><br />Summary:<br /><br />1. This paper investigates the problem of detecting and estimating correlated signals embedded in two spiked Wigner matrices, a fundamental model in high-dimensional statistics.<br /><br />2. The observations follow the model \( X = \frac{\lambda}{\sqrt{n}} xx^{\top} + W \) and \( Y = \frac{\mu}{\sqrt{n}} yy^{\top} + Z \), where \(x,y \in \mathbb{R}^n\) are signal vectors exhibiting correlation \( \rho \), and \( W, Z \) are independent Gaussian noise matrices.<br /><br />3. The authors introduce an efficient algorithm that successfully detects and estimates the correlated signals whenever the function \( F(\lambda, \mu, \rho) > 1 \), where \( F \) measures a combination of signal strengths and correlation.<br /><br />4. A key insight is that leveraging the correlation \( \rho \) enables signal recovery in parameter regimes where recovery from each matrix individually is computationally infeasible.<br /><br />5. Complementing their algorithmic upper bound, the authors provide evidence of a matching computational lower bound by showing that all low-degree polynomial algorithms fail when \( F(\lambda, \mu, \rho) < 1 \), suggesting that the threshold \( F=1 \) marks a sharp computational phase transition.<br /><br />Overall, the work characterizes the precise limits of efficient detection and estimation of correlated signals in spiked Wigner matrices and establishes a strong connection between algorithmic performance and computational hardness indicators. <div>
arXiv:2511.06040v2 Announce Type: replace-cross 
Abstract: We study the computational task of detecting and estimating correlated signals in a pair of spiked Wigner matrices. Our model consists of observations
  $$
  X = \tfrac{\lambda}{\sqrt{n}} xx^{\top} + W \,, \quad Y = \tfrac{\mu}{\sqrt{n}} yy^{\top} + Z \,.
  $$
  where $x,y \in \mathbb R^n$ are signal vectors with norm $\|x\|,\|y\| \approx\sqrt{n}$ and correlation $\langle x,y \rangle \approx \rho\|x\|\|y\|$, while $W,Z$ are independent Gaussian Wigner matrices. We propose an efficient algorithm that succeeds whenever $F(\lambda,\mu,\rho)>1$, where
  $$
  F(\lambda,\mu,\rho)=\max\Big\{ \lambda,\mu, \frac{ \lambda^2 \rho^2 }{ 1-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ 1-\mu^2+\mu^2 \rho^2 } \Big\} \,.
  $$
  Our result shows that an algorithm can leverage the correlation between the spikes to detect and estimate the signals even in regimes where efficiently recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be computationally infeasible.
  We complement our algorithmic result with evidence for a matching computational lower bound. In particular, we prove that when $F(\lambda,\mu,\rho)<1$, all algorithms based on {\em low-degree polynomials} fails to distinguish $(X,Y)$ with two independent Wigner matrices. This low-degree analysis strongly suggests that $F(\lambda,\mu,\rho)=1$ is the precise computation threshold for this problem.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Cross-Disease Reasoning for Cardiovascular Risk Assessment from LDCT</title>
<link>https://arxiv.org/abs/2511.06625</link>
<guid>https://arxiv.org/abs/2511.06625</guid>
<content:encoded><![CDATA[
<div> Low-dose chest CT, cardiopulmonary risk, explainable AI, cross-disease reasoning, cardiovascular prediction<br /><br />Summary: This article presents an Explainable Cross-Disease Reasoning Framework designed to assess cardiopulmonary risk from a single low-dose chest CT (LDCT) scan. Unlike previous methods that treat pulmonary and cardiac analyses separately, this framework mimics clinical diagnostic reasoning by first detecting lung abnormalities, then applying medical knowledge to infer their cardiovascular implications, and finally encoding cardiac structural biomarkers. The system consists of three interconnected modules: a pulmonary perception module that identifies lung findings, a knowledge-guided reasoning module that links these findings to cardiovascular risk, and a cardiac representation module that encodes heart-related markers. Their combined output results in an accurate and physiologically interpretable cardiovascular risk prediction. Evaluated on the NLST cohort, the framework outperforms traditional single-disease and image-based approaches in cardiovascular disease screening and mortality prediction. Importantly, it provides explanations consistent with cardiological understanding, illustrating meaningful connections between pulmonary abnormalities and cardiac stress mechanisms. This unified and interpretable approach bridges the gap between image-driven prediction and mechanism-based medical interpretation, offering a novel pathway for integrated cardiopulmonary health assessment from routine LDCT scans. <div>
arXiv:2511.06625v2 Announce Type: replace-cross 
Abstract: Low-dose chest computed tomography (LDCT) inherently captures both pulmonary and cardiac structures, offering a unique opportunity for joint assessment of lung and cardiovascular health. However, most existing approaches treat these domains as independent tasks, overlooking their physiological interplay and shared imaging biomarkers. We propose an Explainable Cross-Disease Reasoning Framework that enables interpretable cardiopulmonary risk assessment from a single LDCT scan. The framework introduces an agentic reasoning process that emulates clinical diagnostic thinking-first perceiving pulmonary findings, then reasoning through established medical knowledge, and finally deriving a cardiovascular judgment with explanatory rationale. It integrates three synergistic components: a pulmonary perception module that summarizes lung abnormalities, a knowledge-guided reasoning module that infers their cardiovascular implications, and a cardiac representation module that encodes structural biomarkers. Their outputs are fused to produce a holistic cardiovascular risk prediction that is both accurate and physiologically grounded. Experiments on the NLST cohort demonstrate that the proposed framework achieves state-of-the-art performance for CVD screening and mortality prediction, outperforming single-disease and purely image-based baselines. Beyond quantitative gains, the framework provides human-verifiable reasoning that aligns with cardiological understanding, revealing coherent links between pulmonary abnormalities and cardiac stress mechanisms. Overall, this work establishes a unified and explainable paradigm for cardiovascular analysis from LDCT, bridging the gap between image-based prediction and mechanism-based medical interpretation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probability-Biased Attention over Directed Bipartite Graphs for Long-Tail ICD Coding</title>
<link>https://arxiv.org/abs/2511.09559</link>
<guid>https://arxiv.org/abs/2511.09559</guid>
<content:encoded><![CDATA[
<div> Keywords: Automated ICD coding, long-tail distribution, co-occurrence encoding, directed bipartite graph encoder, large language model  

<br /><br />Summary:  
This paper addresses the challenge of automated International Classification of Diseases (ICD) coding, a multi-label classification task with a very large label set and significant long-tail distribution issues. To improve predictions for rare codes, the authors propose a novel Directed Bipartite Graph Encoder that separates common and rare code nodes, allowing one-way directed edges from common to rare codes. The edges incorporate a probability-based bias derived from the conditional probability of common code co-occurrence given rare codes, which is integrated into the attention module as Co-occurrence Encoding. This method enriches rare code embeddings by aggregating latent comorbidity information from common codes. Moreover, initial code embeddings are enhanced using comprehensive descriptions generated by a large language model (LLM), which provides clinical context and external knowledge about comorbidity relationships. Experimental results on three benchmark automated ICD coding datasets demonstrate that this approach achieves state-of-the-art performance, particularly improving Macro-F1 scores, highlighting its effectiveness in handling the long-tail distribution problem in ICD coding. <div>
arXiv:2511.09559v1 Announce Type: new 
Abstract: Automated International Classification of Diseases (ICD) coding aims to assign multiple disease codes to clinical documents, constituting a crucial multi-label text classification task in healthcare informatics. However, the task is challenging due to its large label space (10,000 to 20,000 codes) and long-tail distribution, where a few codes dominate while many rare codes lack sufficient training data. To address this, we propose a learning method that models fine-grained co-occurrence relationships among codes. Specifically, we construct a Directed Bipartite Graph Encoder with disjoint sets of common and rare code nodes. To facilitate a one-way information flow, edges are directed exclusively from common to rare codes. The nature of these connections is defined by a probability-based bias, which is derived from the conditional probability of a common code co-occurring given the presence of a rare code. This bias is then injected into the encoder's attention module, a process we term Co-occurrence Encoding. This structure empowers the graph encoder to enrich rare code representations by aggregating latent comorbidity information reflected in the statistical co-occurrence of their common counterparts. To ensure high-quality input to the graph, we utilize a large language model (LLM) to generate comprehensive descriptions for codes, enriching initial embeddings with clinical context and comorbidity information, serving as external knowledge for the statistical co-occurrence relationships in the code system. Experiments on three automated ICD coding benchmark datasets demonstrate that our method achieves state-of-the-art performance with particularly notable improvements in Macro-F1, which is the key metric for long-tail classification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Let the Experts Speak: Improving Survival Prediction &amp; Calibration via Mixture-of-Experts Heads</title>
<link>https://arxiv.org/abs/2511.09567</link>
<guid>https://arxiv.org/abs/2511.09567</guid>
<content:encoded><![CDATA[
<div> mixture-of-experts, survival analysis, patient clustering, calibration, predictive accuracy<br /><br />Summary:<br /><br />This paper addresses the challenge of balancing patient clustering with calibration and predictive accuracy in survival analysis using deep mixture-of-experts (MoE) models. Traditional MoE approaches cluster patients but often sacrifice calibration error and prediction precision because predictions for individuals are forced to align closely with their assigned group. The authors propose several discrete-time deep MoE architectures that aim to simultaneously achieve patient clustering, improved calibration, and better predictive accuracy. A key finding is that the expressiveness of the individual experts within the MoE framework significantly influences performance. Specifically, experts that make tailored predictions for each patient outperform those relying on fixed group prototypes. This suggests that allowing experts more flexibility to adapt to individual patient data rather than strictly enforcing group-level predictions leads to better overall outcomes. The study demonstrates the viability of discovering meaningful patient group structures without compromising on key performance metrics, advancing methods for personalized survival prognosis. <div>
arXiv:2511.09567v1 Announce Type: new 
Abstract: Deep mixture-of-experts models have attracted a lot of attention for survival analysis problems, particularly for their ability to cluster similar patients together. In practice, grouping often comes at the expense of key metrics such calibration error and predictive accuracy. This is due to the restrictive inductive bias that mixture-of-experts imposes, that predictions for individual patients must look like predictions for the group they're assigned to. Might we be able to discover patient group structure, where it exists, while improving calibration and predictive accuracy? In this work, we introduce several discrete-time deep mixture-of-experts (MoE) based architectures for survival analysis problems, one of which achieves all desiderata: clustering, calibration, and predictive accuracy. We show that a key differentiator between this array of MoEs is how expressive their experts are. We find that more expressive experts that tailor predictions per patient outperform experts that rely on fixed group prototypes.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filtering Jump Markov Systems with Partially Known Dynamics: A Model-Based Deep Learning Approach</title>
<link>https://arxiv.org/abs/2511.09569</link>
<guid>https://arxiv.org/abs/2511.09569</guid>
<content:encoded><![CDATA[
<div> Jump Markov Filtering, Deep Learning, Recurrent Neural Networks, KalmanNet, State Estimation<br /><br />Summary:<br /><br />This paper introduces the Jump Markov Filtering Network (JMFNet), a novel model-based deep learning framework designed for real-time state estimation in jump Markov systems with unknown noise statistics and mode transition dynamics. The proposed architecture integrates two recurrent neural networks (RNNs): one dedicated to mode prediction and another for filtering, which builds upon a mode-augmented version of the KalmanNet framework. These RNNs are trained jointly using an alternating least squares strategy, enabling them to adapt mutually without requiring supervision of the latent modes. Extensive numerical experiments demonstrate JMFNet’s superior performance over classical model-based filters, such as interacting multiple models and particle filters, as well as model-free deep learning methods, especially in scenarios characterized by non-stationarity and high noise levels. The framework is tested on both linear and nonlinear systems, including applications like target tracking, pendulum angle tracking, Lorenz attractor dynamics, and real-life datasets. JMFNet also produces small yet meaningful improvements over KalmanNet, which become more significant in complex systems or long trajectories. Finally, empirical evaluations confirm that JMFNet is consistent and reliable, showing robustness against variations in initial conditions, hyperparameter choices, and inaccuracies in model knowledge. <div>
arXiv:2511.09569v1 Announce Type: new 
Abstract: This paper presents the Jump Markov Filtering Network (JMFNet), a novel model-based deep learning framework for real-time state-state estimation in jump Markov systems with unknown noise statistics and mode transition dynamics. A hybrid architecture comprising two Recurrent Neural Networks (RNNs) is proposed: one for mode prediction and another for filtering that is based on a mode-augmented version of the recently presented KalmanNet architecture. The proposed RNNs are trained jointly using an alternating least squares strategy that enables mutual adaptation without supervision of the latent modes. Extensive numerical experiments on linear and nonlinear systems, including target tracking, pendulum angle tracking, Lorenz attractor dynamics, and a real-life dataset demonstrate that the proposed JMFNet framework outperforms classical model-based filters (e.g., interacting multiple models and particle filters) as well as model-free deep learning baselines, particularly in non-stationary and high-noise regimes. It is also showcased that JMFNet achieves a small yet meaningful improvement over the KalmanNet framework, which becomes much more pronounced in complicated systems or long trajectories. Finally, the method's performance is empirically validated to be consistent and reliable, exhibiting low sensitivity to initial conditions, hyperparameter selection, as well as to incorrect model knowledge
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Averaging for Physics Applications: Accuracy Improvements at Zero Training Cost</title>
<link>https://arxiv.org/abs/2511.09573</link>
<guid>https://arxiv.org/abs/2511.09573</guid>
<content:encoded><![CDATA[
<div> Keywords: group averaging, equivariance, machine learning, symmetries, differential equations<br /><br />Summary:<br /><br />1. Many machine learning tasks in natural sciences require models that are exactly equivariant to certain symmetries, but equivariant methods are underused due to training challenges, expectations that symmetries will be learned automatically, or implementation difficulties.<br /><br />2. Group averaging is a method applied at test time that can enforce exact equivariance on any pre-trained model without changing its structure or training process, by averaging predictions over a symmetry group.<br /><br />3. This technique incurs a computational cost proportional to the size of the symmetry group, which is often small, making it an inexpensive way to improve model predictions.<br /><br />4. Under mild assumptions, models averaged over symmetry groups provably achieve better prediction accuracy than their original counterparts.<br /><br />5. The paper demonstrates empirically, using benchmark tasks involving differential equations with known symmetries, that applying group averaging at evaluation time consistently reduces the evaluation loss by up to 37% (measured via VRMSE), and produces visually improved dynamic predictions.<br /><br />6. The authors conclude that imposing exact symmetries via group averaging is beneficial, free of drawbacks under common conditions, and recommend the ML for Physical Sciences community adopt this simple, cost-effective method to boost model accuracy. <div>
arXiv:2511.09573v1 Announce Type: new 
Abstract: Many machine learning tasks in the natural sciences are precisely equivariant to particular symmetries. Nonetheless, equivariant methods are often not employed, perhaps because training is perceived to be challenging, or the symmetry is expected to be learned, or equivariant implementations are seen as hard to build. Group averaging is an available technique for these situations. It happens at test time; it can make any trained model precisely equivariant at a (often small) cost proportional to the size of the group; it places no requirements on model structure or training. It is known that, under mild conditions, the group-averaged model will have a provably better prediction accuracy than the original model. Here we show that an inexpensive group averaging can improve accuracy in practice. We take well-established benchmark machine learning models of differential equations in which certain symmetries ought to be obeyed. At evaluation time, we average the models over a small group of symmetries. Our experiments show that this procedure always decreases the average evaluation loss, with improvements of up to 37\% in terms of the VRMSE. The averaging produces visually better predictions for continuous dynamics. This short paper shows that, under certain common circumstances, there are no disadvantages to imposing exact symmetries; the ML4PS community should consider group averaging as a cheap and simple way to improve model accuracy.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeatGen: A Guided Diffusion Framework for Multiphysics Heat Sink Design Optimization</title>
<link>https://arxiv.org/abs/2511.09578</link>
<guid>https://arxiv.org/abs/2511.09578</guid>
<content:encoded><![CDATA[
<div> Keywords: denoising diffusion probabilistic model, heat sink design, surrogate gradients, multi-fidelity training, pressure drop optimization

<br /><br />Summary:  
This study introduces a generative optimization framework utilizing a guided denoising diffusion probabilistic model (DDPM) to design heat sinks that minimize pressure drop while keeping surface temperatures below a set threshold. It represents geometries through boundary representations of multiple fins and employs a multi-fidelity approach to assemble training data, enabling efficient learning across different simulation accuracies. Two residual neural networks are trained to predict pressure drop and surface temperature for each geometry, and gradients from these surrogate models guide the generative process toward designs satisfying thermal and fluid flow constraints. Unlike traditional black-box optimization methods such as CMA-ES, this approach uses surrogate gradients for inference-time guidance, enhancing scalability as more training data is provided. Additionally, the method contrasts with traditional topology optimization, offering low computational cost during inference without retraining when new constraints arise. The results show that generated heat sink designs achieve up to 10% lower pressure drops than those found by conventional optimization techniques, while reliably maintaining temperature limits. This work advances the field by providing a scalable, generative approach capable of producing efficient electronics cooling designs, marking progress toward foundational models for complex heat exchanger systems. <div>
arXiv:2511.09578v1 Announce Type: new 
Abstract: This study presents a generative optimization framework based on a guided denoising diffusion probabilistic model (DDPM) that leverages surrogate gradients to generate heat sink designs minimizing pressure drop while maintaining surface temperatures below a specified threshold. Geometries are represented using boundary representations of multiple fins, and a multi-fidelity approach is employed to generate training data. Using this dataset, along with vectors representing the boundary representation geometries, we train a denoising diffusion probabilistic model to generate heat sinks with characteristics consistent with those observed in the data. We train two different residual neural networks to predict the pressure drop and surface temperature for each geometry. We use the gradients of these surrogate models with respect to the design variables to guide the geometry generation process toward satisfying the low-pressure and surface temperature constraints. This inference-time guidance directs the generative process toward heat sink designs that not only prevent overheating but also achieve lower pressure drops compared to traditional optimization methods such as CMA-ES. In contrast to traditional black-box optimization approaches, our method is scalable, provided sufficient training data is available. Unlike traditional topology optimization methods, once the model is trained and the heat sink world model is saved, inference under new constraints (e.g., temperature) is computationally inexpensive and does not require retraining. Samples generated using the guided diffusion model achieve pressure drops up to 10 percent lower than the limits obtained by traditional black-box optimization methods. This work represents a step toward building a foundational generative model for electronics cooling.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Environments for LLM Agents in the Era of Learning from Interaction: A Survey</title>
<link>https://arxiv.org/abs/2511.09586</link>
<guid>https://arxiv.org/abs/2511.09586</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-based agents, reinforcement learning, Generation-Execution-Feedback loop, environment scaling, experiential data<br /><br />Summary:<br /><br />This paper addresses the limitations of training large language model (LLM)-based agents solely on static, human-generated datasets for complex tasks. It highlights that such datasets are expensive to create and lack real-time adaptability and realism needed for advanced capabilities like adaptive behavior and long-term decision-making. To overcome these challenges, the authors propose a reinforcement learning framework where agents interact dynamically with environments and learn from experience. They introduce the Generation-Execution-Feedback (GEF) loop, a formalization describing an iterative process where environments generate tasks, observe agent actions during execution, and provide evaluative feedback based on rollouts to drive learning. Emphasizing the critical role environments play as producers of experiential data, the paper surveys state-of-the-art methods for environment scaling, focusing on increasing complexity, realism, and interactivity. The survey organizes these methods along the three stages of the GEF loop: task generation, task execution, and feedback, providing a structured perspective on environment-centric approaches. Additionally, the authors analyze benchmarks, implementation techniques, and practical applications, consolidating fragmented research efforts and outlining future directions to advance autonomous agent intelligence through richer environment interaction. <div>
arXiv:2511.09586v1 Announce Type: new 
Abstract: LLM-based agents can autonomously accomplish complex tasks across various domains. However, to further cultivate capabilities such as adaptive behavior and long-term decision-making, training on static datasets built from human-level knowledge is insufficient. These datasets are costly to construct and lack both dynamism and realism. A growing consensus is that agents should instead interact directly with environments and learn from experience through reinforcement learning. We formalize this iterative process as the Generation-Execution-Feedback (GEF) loop, where environments generate tasks to challenge agents, return observations in response to agents' actions during task execution, and provide evaluative feedback on rollouts for subsequent learning. Under this paradigm, environments function as indispensable producers of experiential data, highlighting the need to scale them toward greater complexity, realism, and interactivity. In this survey, we systematically review representative methods for environment scaling from a pioneering environment-centric perspective and organize them along the stages of the GEF loop, namely task generation, task execution, and feedback. We further analyze benchmarks, implementation strategies, and applications, consolidating fragmented advances and outlining future research directions for agent intelligence.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynamicRTL: RTL Representation Learning for Dynamic Circuit Behavior</title>
<link>https://arxiv.org/abs/2511.09593</link>
<guid>https://arxiv.org/abs/2511.09593</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, RTL circuits, dynamic behavior, Control Data Flow Graph, circuit representation<br /><br />Summary:<br /><br />This paper introduces DR-GNN (DynamicRTL-GNN), a novel Graph Neural Network model designed to capture both static and dynamic aspects of Register Transfer Level (RTL) circuit representations. Unlike prior GNN models that focus only on static structural characteristics, DR-GNN incorporates multi-cycle execution behaviors by leveraging an operator-level Control Data Flow Graph (CDFG). This approach enables the model to effectively represent dynamic dependencies and runtime execution flows of circuits, which are critical for tasks like circuit verification and optimization. To facilitate model training and evaluation, the authors have created the first large-scale dynamic circuit dataset containing over 6,300 Verilog designs and 63,000 simulation traces. Experimental results reveal that DR-GNN significantly outperforms existing models on tasks such as branch hit prediction and toggle rate prediction. Additionally, the learned representations demonstrate strong transfer learning capabilities, showing improved performance on related dynamic circuit analysis tasks including power estimation and assertion prediction. Overall, this work marks an important step in bridging the gap between static and dynamic circuit analysis by integrating runtime information into graph-based circuit representation learning. <div>
arXiv:2511.09593v1 Announce Type: new 
Abstract: There is a growing body of work on using Graph Neural Networks (GNNs) to learn representations of circuits, focusing primarily on their static characteristics. However, these models fail to capture circuit runtime behavior, which is crucial for tasks like circuit verification and optimization. To address this limitation, we introduce DR-GNN (DynamicRTL-GNN), a novel approach that learns RTL circuit representations by incorporating both static structures and multi-cycle execution behaviors. DR-GNN leverages an operator-level Control Data Flow Graph (CDFG) to represent Register Transfer Level (RTL) circuits, enabling the model to capture dynamic dependencies and runtime execution. To train and evaluate DR-GNN, we build the first comprehensive dynamic circuit dataset, comprising over 6,300 Verilog designs and 63,000 simulation traces. Our results demonstrate that DR-GNN outperforms existing models in branch hit prediction and toggle rate prediction. Furthermore, its learned representations transfer effectively to related dynamic circuit tasks, achieving strong performance in power estimation and assertion prediction.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Making Every Head Count: Sparse Attention Without the Speed-Performance Trade-off</title>
<link>https://arxiv.org/abs/2511.09596</link>
<guid>https://arxiv.org/abs/2511.09596</guid>
<content:encoded><![CDATA[
<div> Efficiency, Large Language Models, Multi-head attention, Structural sparsity, Sparse attention methods<br /><br />Summary: The article addresses the computational inefficiency in Large Language Models (LLMs) stemming from the multi-head attention mechanism, whose complexity scales as \(O(H \cdot N^2)\), where \(H\) is the number of heads and \(N\) the context size. The standard approach results in redundant computations as each head processes the entire sequence independently. Existing sparse attention methods improve efficiency but often at the cost of losing performance integrity. To resolve this, the authors introduce SPAttention, which implements a novel concept called Principled Structural Sparsity. Instead of dropping connections arbitrarily, SPAttention partitions the attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment to focus on. This reorganization transforms \(H\) independent \(O(N^2)\) computations into a single collaborative \(O(N^2)\) computation, effectively reducing complexity by a factor of \(H\). This structured inductive bias encourages functional specialization among heads, improving computational resource allocation and modeling distinct dependencies across the sequence. Empirical evaluations on various OLMoE models demonstrate that SPAttention nearly doubles training throughput while matching or exceeding the performance of dense attention. It also consistently outperforms leading sparse methods such as Longformer, Reformer, and BigBird across all tested metrics. <div>
arXiv:2511.09596v1 Announce Type: new 
Abstract: The design of Large Language Models (LLMs) has long been hampered by a fundamental conflict within their core attention mechanism: its remarkable expressivity is built upon a computational complexity of $O(H \cdot N^2)$ that grows quadratically with the context size ($N$) and linearly with the number of heads ($H$). This standard implementation harbors significant computational redundancy, as all heads independently compute attention over the same sequence space. Existing sparse methods, meanwhile, often trade information integrity for computational efficiency. To resolve this efficiency-performance trade-off, we propose SPAttention, whose core contribution is the introduction of a new paradigm we term Principled Structural Sparsity. SPAttention does not merely drop connections but instead reorganizes the computational task by partitioning the total attention workload into balanced, non-overlapping distance bands, assigning each head a unique segment. This approach transforms the multi-head attention mechanism from $H$ independent $O(N^2)$ computations into a single, collaborative $O(N^2)$ computation, fundamentally reducing complexity by a factor of $H$. The structured inductive bias compels functional specialization among heads, enabling a more efficient allocation of computational resources from redundant modeling to distinct dependencies across the entire sequence span. Extensive empirical validation on the OLMoE-1B-7B and 0.25B-1.75B model series demonstrates that while delivering an approximately two-fold increase in training throughput, its performance is on par with standard dense attention, even surpassing it on select key metrics, while consistently outperforming representative sparse attention methods including Longformer, Reformer, and BigBird across all evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parametric Expensive Multi-Objective Optimization via Generative Solution Modeling</title>
<link>https://arxiv.org/abs/2511.09598</link>
<guid>https://arxiv.org/abs/2511.09598</guid>
<content:encoded><![CDATA[
<div> Keywords: parametric multi-objective optimization, Bayesian optimization, inverse model, task-aware Gaussian processes, conditional generative models  

<br /><br />Summary: This paper addresses the challenge of solving parametric expensive multi-objective optimization problems (P-EMOPs), where a continuous set of task parameters leads to infinitely many distinct optimization problems, each typically requiring costly evaluations. To overcome this, the authors propose the first parametric multi-objective Bayesian optimizer designed to learn an inverse model that can predict optimized solutions directly for any given task-preference query, eliminating the need for additional expensive re-evaluations. The proposed method alternates between two key processes: (1) acquisition-driven search that leverages synergies among related tasks by using task-aware Gaussian processes, enabling faster convergence and efficient exploration across tasks, and (2) generative solution sampling powered by conditional generative models, which provides diverse candidate solutions representative of the task space. This alternating framework exploits inter-task relationships to improve optimization effectiveness and efficiency. Theoretical analysis is provided to justify the accelerated convergence achieved by incorporating task awareness. Extensive empirical experiments on both synthetic benchmarks and real-world problems demonstrate the superior performance of the proposed approach, validating its ability to efficiently optimize across related tasks and predict solutions for unseen parameterized optimization problems without new expensive function evaluations. <div>
arXiv:2511.09598v1 Announce Type: new 
Abstract: Many real-world applications require solving families of expensive multi-objective optimization problems~(EMOPs) under varying operational conditions. This gives rise to parametric expensive multi-objective optimization problems (P-EMOPs) where each task parameter defines a distinct optimization instance. Current multi-objective Bayesian optimization methods have been widely used for finding finite sets of Pareto optimal solutions for individual tasks. However, P-EMOPs present a fundamental challenge: the continuous task parameter space can contain infinite distinct problems, each requiring separate expensive evaluations. This demands learning an inverse model that can directly predict optimized solutions for any task-preference query without expensive re-evaluation. This paper introduces the first parametric multi-objective Bayesian optimizer that learns this inverse model by alternating between (1) acquisition-driven search leveraging inter-task synergies and (2) generative solution sampling via conditional generative models. This approach enables efficient optimization across related tasks and finally achieves direct solution prediction for unseen parameterized EMOPs without additional expensive evaluations. We theoretically justify the faster convergence by leveraging inter-task synergies through task-aware Gaussian processes. Meanwhile, empirical studies in synthetic and real-world benchmarks further verify the effectiveness of our alternating framework.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimistic Reinforcement Learning with Quantile Objectives</title>
<link>https://arxiv.org/abs/2511.09652</link>
<guid>https://arxiv.org/abs/2511.09652</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Quantile Optimization, UCB-QRL, Finite-Horizon MDPs, Regret Bound  

<br /><br />Summary:  
This paper addresses the limitation in classical Reinforcement Learning (RL) which traditionally optimizes expected rewards but neglects risk sensitivity, a crucial factor in domains like healthcare and finance. To incorporate risk sensitivity, the authors focus on optimizing a specific quantile (the $\tau$-quantile) of the cumulative reward distribution rather than the mean. They propose a novel algorithm called UCB-QRL, designed for finite-horizon Markov Decision Processes (MDPs), which operates optimistically by maintaining confidence intervals around the estimated transition probabilities. At each iteration, UCB-QRL updates its estimate of the transition dynamics and then optimizes the quantile value function over a confidence set surrounding this estimate. The paper provides theoretical guarantees by deriving a high-probability regret bound for UCB-QRL in the episodic RL setting with $S$ states, $A$ actions, $T$ episodes, and horizon length $H$. The regret bound scales as $\mathcal O\left((2/\kappa)^{H+1}H\sqrt{SATH\log(2SATH/\delta)}\right)$, where $\kappa$ is a positive constant representing the sensitivity of the quantile value function of the MDP. This work thus advances RL by rigorously integrating risk-sensitive objectives, providing both methodology and theoretical analysis. <div>
arXiv:2511.09652v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has achieved tremendous success in recent years. However, the classical foundations of RL do not account for the risk sensitivity of the objective function, which is critical in various fields, including healthcare and finance. A popular approach to incorporate risk sensitivity is to optimize a specific quantile of the cumulative reward distribution. In this paper, we develop UCB-QRL, an optimistic learning algorithm for the $\tau$-quantile objective in finite-horizon Markov decision processes (MDPs). UCB-QRL is an iterative algorithm in which, at each iteration, we first estimate the underlying transition probability and then optimize the quantile value function over a confidence ball around this estimate. We show that UCB-QRL yields a high-probability regret bound $\mathcal O\left((2/\kappa)^{H+1}H\sqrt{SATH\log(2SATH/\delta)}\right)$ in the episodic setting with $S$ states, $A$ actions, $T$ episodes, and $H$ horizons. Here, $\kappa>0$ is a problem-dependent constant that captures the sensitivity of the underlying MDP's quantile value.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Can Emerge in Tabular Foundation Models From a Single Table</title>
<link>https://arxiv.org/abs/2511.09665</link>
<guid>https://arxiv.org/abs/2511.09665</guid>
<content:encoded><![CDATA[
<div> Keywords: deep tabular modelling, in-context learning, self-supervised pre-training, Tabular Foundation Model, transfer learning<br /><br />Summary:  
1. The article investigates deep tabular modelling using in-context learning, where a model predicts labels from given $(x,y)$ pairs without weight updates during inference.  
2. It challenges the common belief that successful generalization requires extensive pre-training on large synthetic datasets (like TabPFN) or vast real datasets (like TabDPT).  
3. The authors find that effective generalization can be achieved with surprisingly little data, specifically through self-supervised pre-training on just a single real table.  
4. By systematically pre-training and evaluating across various diverse datasets, they analyze which data characteristics are crucial for building a robust Tabular Foundation Model (TFM) capable of transfer across different domains.  
5. They reveal that the key factor influencing downstream performance is the number and quality of distinct tasks that can be constructed from the dataset, highlighting the importance of task diversity in the pre-training phase shared by most TFMs. <div>
arXiv:2511.09665v1 Announce Type: new 
Abstract: Deep tabular modelling increasingly relies on in-context learning where, during inference, a model receives a set of $(x,y)$ pairs as context and predicts labels for new inputs without weight updates. We challenge the prevailing view that broad generalization here requires pre-training on large synthetic corpora (e.g., TabPFN priors) or a large collection of real data (e.g., TabDPT training datasets), discovering that a relatively small amount of data suffices for generalization. We find that simple self-supervised pre-training on just a \emph{single} real table can produce surprisingly strong transfer across heterogeneous benchmarks. By systematically pre-training and evaluating on many diverse datasets, we analyze what aspects of the data are most important for building a Tabular Foundation Model (TFM) generalizing across domains. We then connect this to the pre-training procedure shared by most TFMs and show that the number and quality of \emph{tasks} one can construct from a dataset is key to downstream performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GEM+: Scalable State-of-the-Art Private Synthetic Data with Generator Networks</title>
<link>https://arxiv.org/abs/2511.09672</link>
<guid>https://arxiv.org/abs/2511.09672</guid>
<content:encoded><![CDATA[
<div> Differential Privacy, Synthetic Data, Graphical Models, Neural Networks, Scalability  

<br /><br />Summary:  
This paper addresses the challenges in generating differentially private synthetic tabular data by improving existing frameworks. It highlights that current state-of-the-art methods, such as AIM, utilize an adaptive 'select-measure-generate' approach that iteratively measures low-order noisy marginals and fits graphical models, but these models become inefficient for high-dimensional data due to memory constraints and retraining overhead. The authors note that newer methods like GEM replace graphical models with generator neural networks to enhance scalability, though prior evaluations have been limited to small datasets. To overcome these limitations, the paper introduces GEM+, which combines AIM’s adaptive measurement strategy with GEM’s scalable neural network generator. Experimental results demonstrate that GEM+ surpasses AIM in both data utility and computational scalability. Importantly, GEM+ is able to handle datasets with over a hundred columns effectively, where AIM struggles with memory usage and computational demands. The study thus advances the practical applicability of differentially private synthetic data generation to larger, more complex real-world datasets by integrating adaptive measurement with scalable neural network synthesis. <div>
arXiv:2511.09672v1 Announce Type: new 
Abstract: State-of-the-art differentially private synthetic tabular data has been defined by adaptive 'select-measure-generate' frameworks, exemplified by methods like AIM. These approaches iteratively measure low-order noisy marginals and fit graphical models to produce synthetic data, enabling systematic optimisation of data quality under privacy constraints. Graphical models, however, are inefficient for high-dimensional data because they require substantial memory and must be retrained from scratch whenever the graph structure changes, leading to significant computational overhead. Recent methods, like GEM, overcome these limitations by using generator neural networks for improved scalability. However, empirical comparisons have mostly focused on small datasets, limiting real-world applicability. In this work, we introduce GEM+, which integrates AIM's adaptive measurement framework with GEM's scalable generator network. Our experiments show that GEM+ outperforms AIM in both utility and scalability, delivering state-of-the-art results while efficiently handling datasets with over a hundred columns, where AIM fails due to memory and computational overheads.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosted GFlowNets: Improving Exploration via Sequential Learning</title>
<link>https://arxiv.org/abs/2511.09677</link>
<guid>https://arxiv.org/abs/2511.09677</guid>
<content:encoded><![CDATA[
<div> Generative Flow Networks, Boosted GFlowNets, exploration, sample diversity, trajectory-balance<br /><br />Summary:<br /><br />Generative Flow Networks (GFlowNets) are effective samplers designed to generate compositional objects proportionally to a given non-negative reward. However, standard GFlowNets often struggle with uneven exploration of the reward landscape, where trajectories leading to easily reachable regions dominate training, causing poor coverage of high-reward but hard-to-reach modes. To address this issue, the paper introduces Boosted GFlowNets, a novel method that sequentially trains an ensemble of GFlowNets. Each subsequent model in the ensemble optimizes a residual reward that accounts for the reward mass already captured by previous models, thereby reactivating learning signals in underexplored areas. This residual approach ensures a monotone non-degradation property under mild assumptions, meaning that adding booster models cannot worsen the learned distribution and generally improves it. Empirical results demonstrate that Boosted GFlowNets achieve significantly better exploration and greater sample diversity on challenging multimodal synthetic benchmarks and peptide design tasks. Importantly, this method maintains the stability and simplicity characteristic of standard trajectory-balance training, making it an efficient and effective enhancement for GFlowNets in sampling complex distributions. <div>
arXiv:2511.09677v1 Announce Type: new 
Abstract: Generative Flow Networks (GFlowNets) are powerful samplers for compositional objects that, by design, sample proportionally to a given non-negative reward. Nonetheless, in practice, they often struggle to explore the reward landscape evenly: trajectories toward easy-to-reach regions dominate training, while hard-to-reach modes receive vanishing or uninformative gradients, leading to poor coverage of high-reward areas. We address this imbalance with Boosted GFlowNets, a method that sequentially trains an ensemble of GFlowNets, each optimizing a residual reward that compensates for the mass already captured by previous models. This residual principle reactivates learning signals in underexplored regions and, under mild assumptions, ensures a monotone non-degradation property: adding boosters cannot worsen the learned distribution and typically improves it. Empirically, Boosted GFlowNets achieve substantially better exploration and sample diversity on multimodal synthetic benchmarks and peptide design tasks, while preserving the stability and simplicity of standard trajectory-balance training.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEBA: Sample-Efficient Black-Box Attacks on Visual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09681</link>
<guid>https://arxiv.org/abs/2511.09681</guid>
<content:encoded><![CDATA[
<div> Sample-efficient, Black-box attack, Visual reinforcement learning, Shadow Q model, Generative adversarial network<br /><br />Summary: Visual reinforcement learning (RL) has made significant advancements in visual control and robotics, yet its susceptibility to adversarial perturbations is not well studied. Existing black-box attacks primarily target vector-based or discrete-action RL, and they struggle with image-based continuous control due to large action spaces and high environment query demands. SEBA is introduced as a novel sample-efficient framework designed for black-box adversarial attacks on visual RL agents. SEBA incorporates a shadow Q model to estimate cumulative rewards under adversarial scenarios, a generative adversarial network (GAN) to produce visually imperceptible perturbations, and a world model that simulates environment dynamics to minimize real environment queries. The framework employs a two-stage iterative training procedure that alternates between updating the shadow Q model and refining the adversarial perturbation generator. Experimental results on MuJoCo and Atari benchmarks demonstrate that SEBA can significantly reduce the cumulative rewards of targeted agents while preserving the visual fidelity of perturbations. Moreover, SEBA achieves this with a substantially lower number of environment interactions compared to previous black-box and white-box attack methods, highlighting its efficiency and effectiveness in compromising visual RL systems. <div>
arXiv:2511.09681v1 Announce Type: new 
Abstract: Visual reinforcement learning has achieved remarkable progress in visual control and robotics, but its vulnerability to adversarial perturbations remains underexplored. Most existing black-box attacks focus on vector-based or discrete-action RL, and their effectiveness on image-based continuous control is limited by the large action space and excessive environment queries. We propose SEBA, a sample-efficient framework for black-box adversarial attacks on visual RL agents. SEBA integrates a shadow Q model that estimates cumulative rewards under adversarial conditions, a generative adversarial network that produces visually imperceptible perturbations, and a world model that simulates environment dynamics to reduce real-world queries. Through a two-stage iterative training procedure that alternates between learning the shadow model and refining the generator, SEBA achieves strong attack performance while maintaining efficiency. Experiments on MuJoCo and Atari benchmarks show that SEBA significantly reduces cumulative rewards, preserves visual fidelity, and greatly decreases environment interactions compared to prior black-box and white-box methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConstrainedSQL: Training LLMs for Text2SQL via Constrained Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09693</link>
<guid>https://arxiv.org/abs/2511.09693</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Text2SQL, Reward Function, Constrained RL, Large Language Models<br /><br />Summary:<br /><br />This paper focuses on improving the reasoning capabilities of Text2SQL large language models (LLMs) using reinforcement learning (RL). The authors highlight that while advanced RL algorithms like GRPO and DAPO have been effective, their performance heavily depends on the design of the reward functions. Poorly designed rewards can lead to reward hacking, where models exploit flaws in the reward system without truly solving the task. To address this, the study introduces a constrained RL framework that integrates natural and interpretable reward signals alongside constraint signals. This approach aims to balance these different signals dynamically during training to prevent reward exploitation. The proposed framework is supported by theoretical guarantees that validate its efficacy. Experiments conducted on well-known Text2SQL datasets demonstrate that this method outperforms state-of-the-art RL-trained LLMs, showing significant improvements in task performance. The work emphasizes a more reliable and balanced reward mechanism that enhances model reasoning and robustness in Text2SQL tasks. <div>
arXiv:2511.09693v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated significant promise in enhancing the reasoning capabilities of Text2SQL LLMs, especially with advanced algorithms such as GRPO and DAPO. However, the performance of these methods is highly sensitive to the design of reward functions. Inappropriate rewards can lead to reward hacking, where models exploit loopholes in the reward structure to achieve high scores without genuinely solving the task. This work considers a constrained RL framework for Text2SQL that incorporates natural and interpretable reward and constraint signals, while dynamically balancing trade-offs among them during the training. We establish the theoretical guarantees of our constrained RL framework and our numerical experiments on the well-known Text2SQL datasets substantiate the improvement of our approach over the state-of-the-art RL-trained LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Hyperdimensional Computing with Modular Composite Representations</title>
<link>https://arxiv.org/abs/2511.09708</link>
<guid>https://arxiv.org/abs/2511.09708</guid>
<content:encoded><![CDATA[
<div> Keywords: Modular Composite Representation, High-Dimensional Vectors, Modular Arithmetic, Hardware Accelerator, Energy Efficiency  

<br /><br />Summary:  
The modular composite representation (MCR) is a computational model that encodes information using high-dimensional integer vectors based on modular arithmetic, generalizing the binary spatter code model to improve representational power with lower precision requirements. Despite its potential advantages, MCR has been underexplored, with few systematic analyses or comparisons to other vector representation methods. This work provides the first comprehensive evaluation of MCR, showing it balances capacity, accuracy, and hardware efficiency uniquely. Experimental results reveal that MCR surpasses binary and integer vector capacities while nearing the performance of complex-valued representations but with significantly less memory requirement. On a broad suite of 123 datasets, MCR consistently improves accuracy and can match binary spatter code performance using up to four times less memory. The study also introduces a tailored hardware accelerator for MCR, leveraging its suitability for digital logic implementation. Evaluation on basic operations and selected datasets reveals speedups up to 1000x and substantial energy savings versus software implementations. When accuracy is equalized to binary spatter codes, MCR demonstrates on average 3.08 times faster execution and 2.68 times lower energy consumption. These results confirm that despite increased operational complexity, MCR’s modular arithmetic and greater per-component precision reduce dimensionality, enabling a faster, more energy-efficient, and high-precision alternative when paired with dedicated hardware. <div>
arXiv:2511.09708v1 Announce Type: new 
Abstract: The modular composite representation (MCR) is a computing model that represents information with high-dimensional integer vectors using modular arithmetic. Originally proposed as a generalization of the binary spatter code model, it aims to provide higher representational power while remaining a lighter alternative to models requiring high-precision components. Despite this potential, MCR has received limited attention. Systematic analyses of its trade-offs and comparisons with other models are lacking, sustaining the perception that its added complexity outweighs the improved expressivity. In this work, we revisit MCR by presenting its first extensive evaluation, demonstrating that it achieves a unique balance of capacity, accuracy, and hardware efficiency. Experiments measuring capacity demonstrate that MCR outperforms binary and integer vectors while approaching complex-valued representations at a fraction of their memory footprint. Evaluation on 123 datasets confirms consistent accuracy gains and shows that MCR can match the performance of binary spatter codes using up to 4x less memory. We investigate the hardware realization of MCR by showing that it maps naturally to digital logic and by designing the first dedicated accelerator. Evaluations on basic operations and 7 selected datasets demonstrate a speedup of up to 3 orders of magnitude and significant energy reductions compared to software implementation. When matched for accuracy against binary spatter codes, MCR achieves on average 3.08x faster execution and 2.68x lower energy consumption. These findings demonstrate that, although MCR requires more sophisticated operations than binary spatter codes, its modular arithmetic and higher per-component precision enable lower dimensionality. When realized with dedicated hardware, this results in a faster, more energy-efficient, and high-precision alternative to existing models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing PDE Emulation with Equation-Aware Neural Operators</title>
<link>https://arxiv.org/abs/2511.09729</link>
<guid>https://arxiv.org/abs/2511.09729</guid>
<content:encoded><![CDATA[
<div> Partial differential equations, deep learning, surrogate models, PDE generalization, equation-aware emulation<br /><br />Summary:<br /><br />This paper addresses the computational challenges of solving partial differential equations (PDEs) by proposing a deep learning framework for equation-aware emulation. Unlike typical surrogate models that are specialized for a single PDE with fixed parameters, the authors introduce a method that generalizes to unseen PDEs by conditioning a neural network on an encoding vector that represents PDE terms and their coefficients. The framework is evaluated on a family of one-dimensional PDEs from the APEBench suite, demonstrating strong performance on parameter sets not encountered during training. Additionally, the model exhibits stability when rolling out predictions beyond the training time horizon. Notably, the approach generalizes well even to entirely unseen PDEs, showcasing its versatility. The work contributes to broader AI-driven efforts aiming to automate the creation of expert-level empirical software for scientific tasks that require scoring or quantitative evaluation. The authors further enhance reproducibility by providing both the data and codebase openly at the linked GitHub repository. This combination of generalization, stability, and open resources marks a significant advancement in surrogate modeling for scientific computing. <div>
arXiv:2511.09729v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) can be prohibitively expensive using traditional numerical methods. Deep learning-based surrogate models typically specialize in a single PDE with fixed parameters. We present a framework for equation-aware emulation that generalizes to unseen PDEs, conditioning a neural model on a vector encoding representing the terms in a PDE and their coefficients. We present a baseline of four distinct modeling technqiues, trained on a family of 1D PDEs from the APEBench suite. Our approach achieves strong performance on parameter sets held out from the training distribution, with strong stability for rollout beyond the training window, and generalization to an entirely unseen PDE. This work was developed as part of a broader effort exploring AI systems that automate the creation of expert-level empirical software for scorable scientific tasks. The data and codebase are available at https://github.com/google-research/generalized-pde-emulator.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2511.09731</link>
<guid>https://arxiv.org/abs/2511.09731</guid>
<content:encoded><![CDATA[
<div> Keywords: precipitation nowcasting, Conditional Flow Matching, diffusion models, radar-based forecasting, spatiotemporal modeling<br /><br />Summary:<br />1. The paper addresses radar-based precipitation nowcasting, which involves forecasting short-term precipitation fields from previous radar images, an important task for flood risk management and decision-making.<br />2. It identifies two ongoing challenges in this domain: handling the uncertainty inherent in atmospheric dynamics and efficiently modeling high-dimensional data.<br />3. While diffusion models have improved forecast sharpness and reliability, their iterative sampling process is computationally expensive, limiting their practicality for time-critical applications.<br />4. The authors propose FlowCast, the first application of Conditional Flow Matching (CFM) to precipitation nowcasting, which learns a direct noise-to-data mapping.<br />5. FlowCast allows significantly faster sample generation with fewer function evaluations compared to diffusion models.<br />6. Experimental results show that FlowCast sets a new state-of-the-art in predictive accuracy for precipitation nowcasting.<br />7. Direct comparisons demonstrate that the CFM objective not only surpasses a diffusion objective in accuracy but is also much more computationally efficient, requiring fewer sampling steps.<br />8. The work establishes Conditional Flow Matching as a powerful, practical alternative for high-dimensional spatiotemporal forecasting tasks.<br /><br /> <div>
arXiv:2511.09731v1 Announce Type: new 
Abstract: Radar-based precipitation nowcasting, the task of forecasting short-term precipitation fields from previous radar images, is a critical problem for flood risk management and decision-making. While deep learning has substantially advanced this field, two challenges remain fundamental: the uncertainty of atmospheric dynamics and the efficient modeling of high-dimensional data. Diffusion models have shown strong promise by producing sharp, reliable forecasts, but their iterative sampling process is computationally prohibitive for time-critical applications. We introduce FlowCast, the first model to apply Conditional Flow Matching (CFM) to precipitation nowcasting. Unlike diffusion, CFM learns a direct noise-to-data mapping, enabling rapid, high-fidelity sample generation with drastically fewer function evaluations. Our experiments demonstrate that FlowCast establishes a new state-of-the-art in predictive accuracy. A direct comparison further reveals the CFM objective is both more accurate and significantly more efficient than a diffusion objective on the same architecture, maintaining high performance with significantly fewer sampling steps. This work positions CFM as a powerful and practical alternative for high-dimensional spatiotemporal forecasting.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Heterogeneity and Forgotten Labels in Split Federated Learning</title>
<link>https://arxiv.org/abs/2511.09736</link>
<guid>https://arxiv.org/abs/2511.09736</guid>
<content:encoded><![CDATA[
<div> Keywords: Split Federated Learning, catastrophic forgetting, data heterogeneity, multi-head neural networks, Hydra  

<br /><br />Summary:  
1. The paper addresses Split Federated Learning (SFL), a collaborative training method where the model is divided into two parts—Part-1 trained locally on clients and Part-2 processed sequentially by a server from client activations.  
2. The study highlights the problem of catastrophic forgetting (CF) in SFL caused by data heterogeneity among clients, where local model updates diverge from global optima and the server’s sequential processing introduces forgetting effects similar to continual learning.  
3. It was observed that the model tends to perform better on classes seen later in the server’s processing sequence, indicating a label bias influenced by the order of data processing.  
4. The authors analyze critical factors in SFL that contribute to forgetting, focusing on how the processing order at the server and the choice of cut layer impact model performance and CF.  
5. To combat this issue, they propose Hydra, a novel mitigation strategy inspired by multi-head neural networks, specifically designed for the SFL environment.  
6. Extensive experiments demonstrate that Hydra surpasses existing baselines and competing methods from the literature, effectively reducing catastrophic forgetting and improving overall model accuracy on heterogeneous data distributions in SFL. <div>
arXiv:2511.09736v1 Announce Type: new 
Abstract: In Split Federated Learning (SFL), the clients collaboratively train a model with the help of a server by splitting the model into two parts. Part-1 is trained locally at each client and aggregated by the aggregator at the end of each round. Part-2 is trained at a server that sequentially processes the intermediate activations received from each client. We study the phenomenon of catastrophic forgetting (CF) in SFL in the presence of data heterogeneity. In detail, due to the nature of SFL, local updates of part-1 may drift away from global optima, while part-2 is sensitive to the processing sequence, similar to forgetting in continual learning (CL). Specifically, we observe that the trained model performs better in classes (labels) seen at the end of the sequence. We investigate this phenomenon with emphasis on key aspects of SFL, such as the processing order at the server and the cut layer. Based on our findings, we propose Hydra, a novel mitigation method inspired by multi-head neural networks and adapted for the SFL's setting. Extensive numerical evaluations show that Hydra outperforms baselines and methods from the literature.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Distribution Generalization with a SPARC: Racing 100 Unseen Vehicles with a Single Policy</title>
<link>https://arxiv.org/abs/2511.09737</link>
<guid>https://arxiv.org/abs/2511.09737</guid>
<content:encoded><![CDATA[
<div> Generalization, Contextual Reinforcement Learning, Out-of-Distribution, SPARC, Robust Control<br /><br />Summary:  
1. The paper addresses the challenge of generalizing reinforcement learning agents to unseen or out-of-distribution (OOD) environments, a critical issue in robotics and control applications such as self-driving cars and quadrupedal robots.  
2. It focuses on contextual reinforcement learning where agents must operate under varying contexts that differ from training conditions, without having explicit context information available at test time.  
3. Prior approaches commonly rely on a two-phase training strategy involving separate stages for a context encoder and a history adaptation module, which can be complex and cumbersome to implement.  
4. The authors propose SPARC, a novel single-phase adaptation method designed to simplify training and improve robust control by integrating the adaptation process seamlessly into one training phase.  
5. Experiments conducted in high-fidelity simulators, including Gran Turismo 7 for racing scenarios and MuJoCo with wind perturbations, demonstrate that SPARC offers more reliable and robust OOD generalization than existing methods. This work thus advances the practicality and effectiveness of reinforcement learning for real-world robotics tasks requiring adaptation to new, unseen conditions. <div>
arXiv:2511.09737v1 Announce Type: new 
Abstract: Generalization to unseen environments is a significant challenge in the field of robotics and control. In this work, we focus on contextual reinforcement learning, where agents act within environments with varying contexts, such as self-driving cars or quadrupedal robots that need to operate in different terrains or weather conditions than they were trained for. We tackle the critical task of generalizing to out-of-distribution (OOD) settings, without access to explicit context information at test time. Recent work has addressed this problem by training a context encoder and a history adaptation module in separate stages. While promising, this two-phase approach is cumbersome to implement and train. We simplify the methodology and introduce SPARC: single-phase adaptation for robust control. We test SPARC on varying contexts within the high-fidelity racing simulator Gran Turismo 7 and wind-perturbed MuJoCo environments, and find that it achieves reliable and robust OOD generalization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TawPipe: Topology-Aware Weight Pipeline Parallelism for Accelerating Long-Context Large Models Training</title>
<link>https://arxiv.org/abs/2511.09741</link>
<guid>https://arxiv.org/abs/2511.09741</guid>
<content:encoded><![CDATA[
<div> arXiv, large language models, pipeline parallelism, communication efficiency, distributed clusters  

<br /><br />Summary:  
Training large language models (LLMs) faces challenges due to limited device memory and expensive inter-device communication. Pipeline parallelism partitions the model across devices to reduce memory use, but it incurs high activation communication overhead proportional to sequence length, limiting efficiency for long contexts. Existing weight-passing methods such as WeiPipe reduce activation communication by transmitting weights instead, yet these methods suffer from redundant peer-to-peer (P2P) transfers and underutilized intra-node bandwidth. To address these shortcomings, TawPipe is proposed as a topology-aware weight pipeline parallelism technique designed to leverage hierarchical bandwidth in distributed clusters. TawPipe optimizes communication by grouping devices based on physical topology, enabling efficient intra-node collective and inter-node P2P communication. Each device is assigned a fixed shard of model weights and gradients, eliminating redundant transfers common in other approaches. Moreover, communication is overlapped with computation to hide communication latency. Unlike fully sharded data parallelism (FSDP) which uses global collective operations, TawPipe mostly confines communication within node boundaries, significantly reducing expensive cross-node traffic. Experiments conducted on up to 24 GPUs running LLaMA-style models demonstrate that TawPipe provides superior throughput and scalability compared to existing state-of-the-art baselines, making it a promising approach for efficient large model training. <div>
arXiv:2511.09741v1 Announce Type: new 
Abstract: Training large language models (LLMs) is fundamentally constrained by limited device memory and costly inter-device communication. Although pipeline parallelism alleviates memory pressure by partitioning models across devices, it incurs activation communication overhead that scales linearly with sequence length, limiting efficiency in long-context training. Recent weight-passing approaches (e.g., WeiPipe) mitigate this by transmitting model weights instead of activations, but suffer from redundant peer-to-peer (P2P) transfers and underutilized intra-node bandwidth. We propose TawPipe--topology-aware weight pipeline parallelism, which exploits hierarchical bandwidth in distributed clusters for improved communication efficiency. TawPipe: (i) groups devices based on topology to optimize intra-node collective and inter-node P2P communication; (ii) assigns each device a fixed shard of model weights and gradients, avoiding redundant transfers; and (iii) overlaps communication with computation to hide latency. Unlike global collective operations used in fully sharded data parallelism (FSDP), TawPipe confines most communication within node boundaries, significantly reducing cross-node traffic. Extensive experiments on up to 24 GPUs with LLaMA-style models show that TawPipe achieves superior throughput and scalability compared to state-of-the-art baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>History Rhymes: Macro-Contextual Retrieval for Robust Financial Forecasting</title>
<link>https://arxiv.org/abs/2511.09754</link>
<guid>https://arxiv.org/abs/2511.09754</guid>
<content:encoded><![CDATA[
<div> Keywords: financial markets, macro-contextual retrieval, out-of-distribution forecasting, multimodal embedding, regime shifts<br /><br />Summary:<br />1. Financial markets exhibit non-stationarity due to structural breaks and macroeconomic regime changes, causing forecasting models to fail in out-of-distribution (OOD) scenarios.<br />2. Traditional multimodal approaches that combine numerical indicators and textual sentiment lack adaptability to such distributional shifts.<br />3. The authors propose macro-contextual retrieval, a retrieval-augmented forecasting framework that grounds predictions in historically similar macroeconomic regimes by jointly embedding macro indicators and financial news sentiment in a shared similarity space.<br />4. This framework enables causal retrieval of precedent macroeconomic periods during inference without retraining.<br />5. Trained on 17 years of S&amp;P 500 data (2007-2023) and tested OOD on AAPL and XOM data from 2024, the approach reduces the gap between cross-validation and OOD performance.<br />6. The macro-conditioned retrieval method is the only approach to yield positive out-of-sample trading results, achieving PF=1.18 and Sharpe=0.95 on AAPL, and PF=1.16 and Sharpe=0.61 on XOM.<br />7. Baseline methods including static numeric, text-only, and naive multimodal models fail under regime shifts.<br />8. Retrieved historical neighbors provide interpretable evidence chains corresponding to recognizable macro contexts like inflationary periods or yield-curve inversions, enhancing causal interpretability and transparency.<br />9. This demonstrates that grounding forecasts in macro-aware retrieval can produce robust, explainable predictions under distributional changes.<br />10. The datasets, models, and code from this study are publicly released. <div>
arXiv:2511.09754v1 Announce Type: new 
Abstract: Financial markets are inherently non-stationary: structural breaks and macroeconomic regime shifts often cause forecasting models to fail when deployed out of distribution (OOD). Conventional multimodal approaches that simply fuse numerical indicators and textual sentiment rarely adapt to such shifts. We introduce macro-contextual retrieval, a retrieval-augmented forecasting framework that grounds each prediction in historically analogous macroeconomic regimes. The method jointly embeds macro indicators (e.g., CPI, unemployment, yield spread, GDP growth) and financial news sentiment in a shared similarity space, enabling causal retrieval of precedent periods during inference without retraining.
  Trained on seventeen years of S&amp;P 500 data (2007-2023) and evaluated OOD on AAPL (2024) and XOM (2024), the framework consistently narrows the CV to OOD performance gap. Macro-conditioned retrieval achieves the only positive out-of-sample trading outcomes (AAPL: PF=1.18, Sharpe=0.95; XOM: PF=1.16, Sharpe=0.61), while static numeric, text-only, and naive multimodal baselines collapse under regime shifts. Beyond metric gains, retrieved neighbors form interpretable evidence chains that correspond to recognizable macro contexts, such as inflationary or yield-curve inversion phases, supporting causal interpretability and transparency. By operationalizing the principle that "financial history may not repeat, but it often rhymes," this work demonstrates that macro-aware retrieval yields robust, explainable forecasts under distributional change.
  All datasets, models, and source code are publicly available.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is nasty noise actually harder than malicious noise?</title>
<link>https://arxiv.org/abs/2511.09763</link>
<guid>https://arxiv.org/abs/2511.09763</guid>
<content:encoded><![CDATA[
<div> malicious noise, nasty noise, computational learning theory, noise tolerance, cryptographic assumptions  

<br /><br />Summary:  
1. The paper studies the capabilities and limitations of computationally efficient algorithms for learning Boolean functions in the presence of adversarial noise, focusing on two noise models: malicious noise and nasty noise.  
2. Malicious noise allows an adversary to corrupt a random subset of examples, while nasty noise allows corruption of an adversarially chosen subset.  
3. The authors analyze both distribution-independent learning and fixed-distribution learning settings, revealing starkly different behaviors between them.  
4. For distribution-independent learning, they prove a strong equivalence: if a class of functions can be efficiently learned under a certain malicious noise rate η, it can also be learned under the same rate of nasty noise.  
5. Conversely, in the fixed-distribution setting, they demonstrate a potentially unbounded gap between the noise rates tolerated for malicious versus nasty noise, assuming standard cryptographic assumptions. For any large ratio r, there exists a concept class where the tolerated malicious noise rate exceeds that of nasty noise by a factor of r.  
6. To mitigate this negative fixed-distribution result, the paper introduces a natural class of algorithms named ICE (Ignore Contradictory Examples). They prove that for ICE learners, malicious noise and nasty noise are equivalent up to a factor of two in the noise rate.  
7. Finally, under the standard cryptographic assumption, they show this factor of two gap for ICE learners is necessary and cannot be improved. <div>
arXiv:2511.09763v1 Announce Type: new 
Abstract: We consider the relative abilities and limitations of computationally efficient algorithms for learning in the presence of noise, under two well-studied and challenging adversarial noise models for learning Boolean functions: malicious noise, in which an adversary can arbitrarily corrupt a random subset of examples given to the learner; and nasty noise, in which an adversary can arbitrarily corrupt an adversarially chosen subset of examples given to the learner.
  We consider both the distribution-independent and fixed-distribution settings. Our main results highlight a dramatic difference between these two settings: For distribution-independent learning, we prove a strong equivalence between the two noise models: If a class ${\cal C}$ of functions is efficiently learnable in the presence of $\eta$-rate malicious noise, then it is also efficiently learnable in the presence of $\eta$-rate nasty noise. In sharp contrast, for the fixed-distribution setting we show an arbitrarily large separation: Under a standard cryptographic assumption, for any arbitrarily large value $r$ there exists a concept class for which there is a ratio of $r$ between the rate $\eta_{malicious}$ of malicious noise that polynomial-time learning algorithms can tolerate, versus the rate $\eta_{nasty}$ of nasty noise that such learning algorithms can tolerate.
  To offset the negative result for the fixed-distribution setting, we define a broad and natural class of algorithms, namely those that ignore contradictory examples (ICE). We show that for these algorithms, malicious noise and nasty noise are equivalent up to a factor of two in the noise rate: Any efficient ICE learner that succeeds with $\eta$-rate malicious noise can be converted to an efficient learner that succeeds with $\eta/2$-rate nasty noise. We further show that the above factor of two is necessary, again under a standard cryptographic assumption.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroLingua: A Language-Inspired Hierarchical Framework for Multimodal Sleep Stage Classification Using EEG and EOG</title>
<link>https://arxiv.org/abs/2511.09773</link>
<guid>https://arxiv.org/abs/2511.09773</guid>
<content:encoded><![CDATA[
<div> Keywords: sleep stage classification, hierarchical modeling, multimodal fusion, Transformers, interpretability  

<br /><br />Summary:  
This paper introduces NeuroLingua, a novel language-inspired framework treating sleep as a structured physiological language for automated sleep stage classification from polysomnography data. 1) It segments each 30-second sleep epoch into overlapping 3-second subwindows called "tokens" using a CNN-based tokenizer to capture fine-grained temporal features. 2) The model applies dual-level Transformers to hierarchically model temporal dependencies: intra-segment encoding captures local relationships within tokens, while inter-segment integration aggregates information across seven consecutive epochs (3.5 minutes) for extended temporal context. 3) EEG and EOG channel data are embedded separately and fused through a Graph Convolutional Network, enhancing robust multimodal integration. 4) NeuroLingua was validated on the Sleep-EDF Expanded and ISRUC-Sleep datasets, achieving state-of-the-art results on Sleep-EDF (85.3% accuracy, 0.800 macro F1, 0.796 Cohen's kappa) and competitive results on ISRUC (81.9% accuracy, 0.802 macro F1, 0.755 kappa), outperforming or matching existing baselines on overall and per-class metrics. 5) The architecture’s attention mechanisms improve detection of clinically relevant sleep microevents, offering a strong basis for interpretability, explainability, and causal inference in sleep research. Overall, NeuroLingua advances automated sleep staging by unifying hierarchical temporal sequence modeling with multimodal fusion toward more transparent and clinically meaningful applications. <div>
arXiv:2511.09773v1 Announce Type: new 
Abstract: Automated sleep stage classification from polysomnography remains limited by the lack of expressive temporal hierarchies, challenges in multimodal EEG and EOG fusion, and the limited interpretability of deep learning models. We propose NeuroLingua, a language-inspired framework that conceptualizes sleep as a structured physiological language. Each 30-second epoch is decomposed into overlapping 3-second subwindows ("tokens") using a CNN-based tokenizer, enabling hierarchical temporal modeling through dual-level Transformers: intra-segment encoding of local dependencies and inter-segment integration across seven consecutive epochs (3.5 minutes) for extended context. Modality-specific embeddings from EEG and EOG channels are fused via a Graph Convolutional Network, facilitating robust multimodal integration. NeuroLingua is evaluated on the Sleep-EDF Expanded and ISRUC-Sleep datasets, achieving state-of-the-art results on Sleep-EDF (85.3% accuracy, 0.800 macro F1, and 0.796 Cohen's kappa) and competitive performance on ISRUC (81.9% accuracy, 0.802 macro F1, and 0.755 kappa), matching or exceeding published baselines in overall and per-class metrics. The architecture's attention mechanisms enhance the detection of clinically relevant sleep microevents, providing a principled foundation for future interpretability, explainability, and causal inference in sleep research. By framing sleep as a compositional language, NeuroLingua unifies hierarchical sequence modeling and multimodal fusion, advancing automated sleep staging toward more transparent and clinically meaningful applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hail to the Thief: Exploring Attacks and Defenses in Decentralised GRPO</title>
<link>https://arxiv.org/abs/2511.09780</link>
<guid>https://arxiv.org/abs/2511.09780</guid>
<content:encoded><![CDATA[
<div> Keywords: Group Relative Policy Optimization, Large Language Models, adversarial attack, decentralized training, defense mechanisms  

<br /><br />Summary:  
1. Group Relative Policy Optimization (GRPO) is effective in post-training Large Language Models (LLMs) using reinforcement learning on prompt completions.  
2. Due to low communication overhead, GRPO suits decentralized training where multiple nodes process and share prompt responses as strings concurrently.  
3. The work presents the first adversarial attack targeting decentralized GRPO systems, where malicious nodes inject harmful tokens to poison benign models.  
4. Both out-of-context and in-context attacks demonstrate high success rates (up to 100%) in corrupting local LLM post-training within as few as 50 iterations, shown through math and coding task examples.  
5. Two defense strategies are proposed based on whether all users train the same model or different models, effectively preventing these adversarial attacks with up to 100% attack stop rates. <div>
arXiv:2511.09780v1 Announce Type: new 
Abstract: Group Relative Policy Optimization (GRPO) has demonstrated great utilization in post-training of Large Language Models (LLMs). In GRPO, prompts are answered by the model and, through reinforcement learning, preferred completions are learnt. Owing to the small communication volume, GRPO is inherently suitable for decentralised training as the prompts can be concurrently answered by multiple nodes and then exchanged in the forms of strings. In this work, we present the first adversarial attack in decentralised GRPO. We demonstrate that malicious parties can poison such systems by injecting arbitrary malicious tokens in benign models in both out-of-context and in-context attacks. Using empirical examples of math and coding tasks, we show that adversarial attacks can easily poison the benign nodes, polluting their local LLM post-training, achieving attack success rates up to 100% in as few as 50 iterations. We propose two ways to defend against these attacks, depending on whether all users train the same model or different models. We show that these defenses can achieve stop rates of up to 100%, making the attack impossible.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Koopman Invariants as Drivers of Emergent Time-Series Clustering in Joint-Embedding Predictive Architectures</title>
<link>https://arxiv.org/abs/2511.09783</link>
<guid>https://arxiv.org/abs/2511.09783</guid>
<content:encoded><![CDATA[
<div> Koopman operator, Joint-Embedding Predictive Architectures, time-series clustering, invariant subspace, self-supervised learning<br /><br />Summary:<br /><br />This paper addresses the unexplained ability of Joint-Embedding Predictive Architectures (JEPAs), a type of self-supervised model, to cluster time-series data by their underlying dynamical regimes. The authors propose a theoretical explanation that the predictive objective in JEPA implicitly drives the model to learn the invariant subspace associated with the system’s Koopman operator. They prove that the idealized JEPA loss is minimized when the encoder learns the system’s regime indicator functions, which correspond to Koopman eigenfunctions. This theoretical claim is validated using synthetic datasets with known dynamical properties. A key finding is that restricting the JEPA’s linear predictor to act as a near-identity operator serves as a critical inductive bias. This constraint steers the encoder toward learning interpretable invariant representations, effectively disentangling the latent dynamics. Additionally, the work highlights the importance of this constraint in selecting meaningful solutions from a set of mathematically equivalent but entangled optima. Overall, the study clarifies a fundamental behavior of JEPAs, bridging modern self-supervised learning methods with dynamical systems theory and guiding the design of more robust and interpretable models for time-series analysis. <div>
arXiv:2511.09783v1 Announce Type: new 
Abstract: Joint-Embedding Predictive Architectures (JEPAs), a powerful class of self-supervised models, exhibit an unexplained ability to cluster time-series data by their underlying dynamical regimes. We propose a novel theoretical explanation for this phenomenon, hypothesizing that JEPA's predictive objective implicitly drives it to learn the invariant subspace of the system's Koopman operator. We prove that an idealized JEPA loss is minimized when the encoder represents the system's regime indicator functions, which are Koopman eigenfunctions. This theory was validated on synthetic data with known dynamics, demonstrating that constraining the JEPA's linear predictor to be a near-identity operator is the key inductive bias that forces the encoder to learn these invariants. We further discuss that this constraint is critical for selecting this interpretable solution from a class of mathematically equivalent but entangled optima, revealing the predictor's role in representation disentanglement. This work demystifies a key behavior of JEPAs, provides a principled connection between modern self-supervised learning and dynamical systems theory, and informs the design of more robust and interpretable time-series models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaReTS: A Multi-Task Framework Unifying Classification and Regression for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.09789</link>
<guid>https://arxiv.org/abs/2511.09789</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, multi-task learning, trend classification, regression, interpretable predictions<br /><br />Summary: This paper introduces CaReTS, a multi-task learning framework designed for multi-step time series forecasting that integrates classification and regression tasks. The framework features a dual-stream architecture: one branch classifies future stepwise trends, while the other estimates deviations from the latest target variable observation. This separation allows CaReTS to provide more interpretable predictions by disentangling macro-level trends from micro-level deviations. To effectively train the model, a multi-task loss function with uncertainty-aware weighting is proposed to balance the contributions of classification, regression, and deviation estimation tasks adaptively. The authors implement four variants of CaReTS (CaReTS1 to CaReTS4) that incorporate popular temporal modeling encoders such as CNNs, LSTMs, and Transformers, enabling flexibility in architecture choice. Experiments conducted on real-world datasets demonstrate that CaReTS consistently surpasses state-of-the-art models in forecasting accuracy and achieves superior trend classification performance. This work addresses both the accuracy and interpretability challenges in deep time series forecasting models, contributing a novel approach that unifies trend detection and deviation estimation within a single framework. <div>
arXiv:2511.09789v1 Announce Type: new 
Abstract: Recent advances in deep forecasting models have achieved remarkable performance, yet most approaches still struggle to provide both accurate predictions and interpretable insights into temporal dynamics. This paper proposes CaReTS, a novel multi-task learning framework that combines classification and regression tasks for multi-step time series forecasting problems. The framework adopts a dual-stream architecture, where a classification branch learns the stepwise trend into the future, while a regression branch estimates the corresponding deviations from the latest observation of the target variable. The dual-stream design provides more interpretable predictions by disentangling macro-level trends from micro-level deviations in the target variable. To enable effective learning in output prediction, deviation estimation, and trend classification, we design a multi-task loss with uncertainty-aware weighting to adaptively balance the contribution of each task. Furthermore, four variants (CaReTS1--4) are instantiated under this framework to incorporate mainstream temporal modelling encoders, including convolutional neural networks (CNNs), long short-term memory networks (LSTMs), and Transformers. Experiments on real-world datasets demonstrate that CaReTS outperforms state-of-the-art (SOTA) algorithms in forecasting accuracy, while achieving higher trend classification performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Monotonicity: Revisiting Factorization Principles in Multi-Agent Q-Learning</title>
<link>https://arxiv.org/abs/2511.09792</link>
<guid>https://arxiv.org/abs/2511.09792</guid>
<content:encoded><![CDATA[
<div> Keywords: value decomposition, multi-agent reinforcement learning, IGM consistency, dynamical systems, non-monotonic factorization<br /><br />Summary:  
Value decomposition is a fundamental technique in multi-agent reinforcement learning (MARL) that allows centralized training with decentralized execution by breaking down the global value function into local components. Ensuring individual-global-max (IGM) consistency traditionally involves monotonicity constraints that limit model expressiveness or softer surrogate methods that introduce added complexity. This work introduces a dynamical systems perspective by modeling learning as continuous-time gradient flow to analyze non-monotonic value decomposition. The authors prove that under approximately greedy exploration, any zero-loss equilibrium that violates IGM consistency is an unstable saddle point, while only IGM-consistent solutions act as stable attractors, ensuring convergence to optimal solutions. Extensive experiments on synthetic matrix games and real-world MARL benchmarks confirm that unconstrained, non-monotonic factorization reliably achieves IGM-optimal outcomes and outperforms monotonic baselines. Additionally, the study explores the impact of temporal-difference targets and exploration strategies, offering practical guidance for designing more effective value-based MARL algorithms in the future. <div>
arXiv:2511.09792v1 Announce Type: new 
Abstract: Value decomposition is a central approach in multi-agent reinforcement learning (MARL), enabling centralized training with decentralized execution by factorizing the global value function into local values. To ensure individual-global-max (IGM) consistency, existing methods either enforce monotonicity constraints, which limit expressive power, or adopt softer surrogates at the cost of algorithmic complexity. In this work, we present a dynamical systems analysis of non-monotonic value decomposition, modeling learning dynamics as continuous-time gradient flow. We prove that, under approximately greedy exploration, all zero-loss equilibria violating IGM consistency are unstable saddle points, while only IGM-consistent solutions are stable attractors of the learning dynamics. Extensive experiments on both synthetic matrix games and challenging MARL benchmarks demonstrate that unconstrained, non-monotonic factorization reliably recovers IGM-optimal solutions and consistently outperforms monotonic baselines. Additionally, we investigate the influence of temporal-difference targets and exploration strategies, providing actionable insights for the design of future value-based MARL algorithms.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constrained Best Arm Identification with Tests for Feasibility</title>
<link>https://arxiv.org/abs/2511.09808</link>
<guid>https://arxiv.org/abs/2511.09808</guid>
<content:encoded><![CDATA[
<div> Best arm identification, feasibility constraints, fixed confidence, sample complexity, asymptotic optimality  

<br /><br />Summary:  
This work addresses the problem of Best Arm Identification (BAI) under additional feasibility constraints, which is essential for real-world applications such as drug discovery where performance and safety criteria must be evaluated separately. Unlike prior work assuming simultaneous observation of performance and constraints, this paper considers settings where testing an arm’s performance or each of its feasibility constraints can be done independently. The authors formulate a feasible BAI problem where a decision-maker selects an arm and decides whether to test its performance or one of the constraints. They focus on the fixed confidence setting to identify the feasible arm with the highest performance with probability at least \(1 - \delta\). The paper proposes an efficient algorithm that adapts to the problem difficulty, eliminating arms based on either poor performance or infeasibility. The sample complexity of the algorithm is rigorously upper-bounded, and a matching lower bound is established, proving the algorithm is asymptotically optimal as \(\delta \to 0\). Empirical evaluations demonstrate that the algorithm outperforms state-of-the-art BAI methods on both synthetic and real-world datasets, validating its practical advantages. <div>
arXiv:2511.09808v1 Announce Type: new 
Abstract: Best arm identification (BAI) aims to identify the highest-performance arm among a set of $K$ arms by collecting stochastic samples from each arm. In real-world problems, the best arm needs to satisfy additional feasibility constraints. While there is limited prior work on BAI with feasibility constraints, they typically assume the performance and constraints are observed simultaneously on each pull of an arm. However, this assumption does not reflect most practical use cases, e.g., in drug discovery, we wish to find the most potent drug whose toxicity and solubility are below certain safety thresholds. These safety experiments can be conducted separately from the potency measurement. Thus, this requires designing BAI algorithms that not only decide which arm to pull but also decide whether to test for the arm's performance or feasibility. In this work, we study feasible BAI which allows a decision-maker to choose a tuple $(i,\ell)$, where $i\in [K]$ denotes an arm and $\ell$ denotes whether she wishes to test for its performance ($\ell=0$) or any of its $N$ feasibility constraints ($\ell\in[N]$). We focus on the fixed confidence setting, which is to identify the \textit{feasible} arm with the \textit{highest performance}, with a probability of at least $1-\delta$. We propose an efficient algorithm and upper-bound its sample complexity, showing our algorithm can naturally adapt to the problem's difficulty and eliminate arms by worse performance or infeasibility, whichever is easier. We complement this upper bound with a lower bound showing that our algorithm is \textit{asymptotically ($\delta\rightarrow 0$) optimal}. Finally, we empirically show that our algorithm outperforms other state-of-the-art BAI algorithms in both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence of Overparameterized Problems: Inherent Properties of the Compositional Structure of Neural Networks</title>
<link>https://arxiv.org/abs/2511.09810</link>
<guid>https://arxiv.org/abs/2511.09810</guid>
<content:encoded><![CDATA[
<div> Keywords: compositional structure, optimization landscape, overparameterized optimization, gradient flow, convergence acceleration  

<br /><br />Summary:  
This paper studies how the compositional structure of neural networks influences their optimization landscape and training dynamics, specifically focusing on overparameterized optimization problems modeled with linear activations. The authors analyze the gradient flow in such settings and demonstrate that global convergence can be guaranteed for any proper, real analytic cost function. When restricting the analysis to scalar-valued cost functions, they fully characterize the geometry of the landscape, revealing that essential structural features—such as the position and stability of saddle points—are universal across all admissible costs. This universality depends only on the overparameterized representation rather than the specific problem details. Additionally, the research introduces an imbalance metric that measures initialization conditions, showing that convergence speed can be dramatically improved depending on the initial imbalance. Finally, the paper extends these theoretical insights by discussing their possible generalization to neural networks with sigmoidal activations. Through a simple example, it is shown which geometric and dynamical properties persist beyond the linear activation scenario, suggesting broader applicability of the results. <div>
arXiv:2511.09810v1 Announce Type: new 
Abstract: This paper investigates how the compositional structure of neural networks shapes their optimization landscape and training dynamics. We analyze the gradient flow associated with overparameterized optimization problems, which can be interpreted as training a neural network with linear activations. Remarkably, we show that the global convergence properties can be derived for any cost function that is proper and real analytic. We then specialize the analysis to scalar-valued cost functions, where the geometry of the landscape can be fully characterized. In this setting, we demonstrate that key structural features -- such as the location and stability of saddle points -- are universal across all admissible costs, depending solely on the overparameterized representation rather than on problem-specific details. Moreover, we show that convergence can be arbitrarily accelerated depending on the initialization, as measured by an imbalance metric introduced in this work. Finally, we discuss how these insights may generalize to neural networks with sigmoidal activations, showing through a simple example which geometric and dynamical properties persist beyond the linear case.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMoFi: Step-wise Momentum Fusion for Split Federated Learning on Heterogeneous Data</title>
<link>https://arxiv.org/abs/2511.09828</link>
<guid>https://arxiv.org/abs/2511.09828</guid>
<content:encoded><![CDATA[
<div> Keywords: Split Federated Learning, data heterogeneity, momentum synchronization, Step-wise Momentum Fusion, gradient divergence<br /><br />Summary:  
This paper addresses the challenges posed by data heterogeneity in Split Federated Learning (SFL), a paradigm that utilizes central server resources to train partitions of a model distributed across clients. Data heterogeneity causes gradient divergence, which negatively affects the global model’s convergence speed and accuracy. To mitigate this, the authors introduce Step-wise Momentum Fusion (SMoFi), a lightweight and effective framework that synchronizes momentum buffers across server-side optimizers. SMoFi incorporates a staleness-aware alignment mechanism to control gradient divergence by imposing constraints on the server-side submodel’s gradient updates at each optimization step. Extensive experiments conducted on multiple real-world datasets demonstrate that SMoFi consistently enhances global model accuracy by up to 7.1% and speeds up convergence by as much as 10.25 times. The benefits of SMoFi become more pronounced as the number of participating clients increases and when training deeper learning models. Consequently, SMoFi is particularly advantageous for federated model training in resource-constrained environments, where efficiency and accuracy are critical. This work provides a promising direction for improving SFL performance amidst the prevalent challenge of data heterogeneity. <div>
arXiv:2511.09828v1 Announce Type: new 
Abstract: Split Federated Learning is a system-efficient federated learning paradigm that leverages the rich computing resources at a central server to train model partitions. Data heterogeneity across silos, however, presents a major challenge undermining the convergence speed and accuracy of the global model. This paper introduces Step-wise Momentum Fusion (SMoFi), an effective and lightweight framework that counteracts gradient divergence arising from data heterogeneity by synchronizing the momentum buffers across server-side optimizers. To control gradient divergence over the training process, we design a staleness-aware alignment mechanism that imposes constraints on gradient updates of the server-side submodel at each optimization step. Extensive validations on multiple real-world datasets show that SMoFi consistently improves global model accuracy (up to 7.1%) and convergence speed (up to 10.25$\times$). Furthermore, SMoFi has a greater impact with more clients involved and deeper learning models, making it particularly suitable for model training in resource-constrained contexts.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Intersections of Halfspaces under Factorizable Distribution</title>
<link>https://arxiv.org/abs/2511.09832</link>
<guid>https://arxiv.org/abs/2511.09832</guid>
<content:encoded><![CDATA[
<div> Learning intersections of halfspaces, polynomial time, statistical queries, factorizable distributions, Jennrich's Algorithm<br /><br />Summary:<br /><br />This paper addresses the long-standing open problem in Computational Learning Theory regarding whether intersections of two halfspaces can be learned in polynomial time with respect to the margin \(\gamma\) and dimension \(d\). Existing algorithms run in quasi-polynomial time \(d^{O(\log(1/\gamma))}\), and this complexity lower bound is known when using only correlational statistical queries (CSQ). The authors introduce a new algorithm that overcomes the CSQ hardness barrier by working within a broad class of factorizable distributions, a natural assumption that lies between distribution-specific and distribution-free settings. Under these distributions, while CSQ approaches still require quasi-polynomial time even for weak learning, their algorithm achieves polynomial time \(poly(d,1/\gamma)\) by leveraging more general statistical queries (SQ), thus demonstrating a clear separation between CSQ and SQ complexity in this simple PAC learning problem. The theoretical foundation involves a novel duality framework characterizing the moment tensor structure of the marginal distributions. Building on these insights, the paper proposes efficient algorithms combining a refined version of Jennrich's Algorithm and PCA on random projections of the moment tensor, along with a gradient-descent-based non-convex optimization method. This work significantly extends the class of tractable distributions for this problem. <div>
arXiv:2511.09832v1 Announce Type: new 
Abstract: Learning intersections of halfspaces is a central problem in Computational Learning Theory. Even for just two halfspaces, it remains a major open question whether learning is possible in polynomial time with respect to the margin $\gamma$ of the data points and their dimensionality $d$. The best-known algorithms run in quasi-polynomial time $d^{O(\log(1/\gamma))}$, and it has been shown that this complexity is unavoidable for any algorithm relying solely on correlational statistical queries (CSQ).
  In this work, we introduce a novel algorithm that provably circumvents the CSQ hardness barrier. Our approach applies to a broad class of distributions satisfying a natural, previously studied, factorizability assumption. Factorizable distributions lie between distribution-specific and distribution-free settings, and significantly extend previously known tractable cases. Under these distributions, we show that CSQ-based methods still require quasipolynomial time even for weakly learning, whereas our algorithm achieves $poly(d,1/\gamma)$ time by leveraging more general statistical queries (SQ), establishing a strong separation between CSQ and SQ for this simple realizable PAC learning problem.
  Our result is grounded in a rigorous analysis utilizing a novel duality framework that characterizes the moment tensor structure induced by the marginal distributions. Building on these structural insights, we propose new, efficient learning algorithms. These algorithms combine a refined variant of Jennrich's Algorithm with PCA over random projections of the moment tensor, along with a gradient-descent-based non-convex optimization framework.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ACT as Human: Multimodal Large Language Model Data Annotation with Critical Thinking</title>
<link>https://arxiv.org/abs/2511.09833</link>
<guid>https://arxiv.org/abs/2511.09833</guid>
<content:encoded><![CDATA[
<div> Keywords: Annotation with Critical Thinking, large language models, human annotation efficiency, multimodal understanding, loss function optimization<br /><br />Summary: This paper introduces the Annotation with Critical Thinking (ACT) data pipeline designed to improve supervised learning by enhancing annotation quality while drastically reducing human effort. First, ACT leverages large language models (LLMs) not only as annotators but also as critical judges to identify potential labeling errors, allowing humans to focus only on reviewing the most "suspicious" cases, thus increasing efficiency. Second, the approach is versatile and applicable across multiple domains such as natural language processing (NLP), computer vision (CV), and multimodal understanding by utilizing multimodal LLMs (MLLMs). Third, through extensive empirical studies, the authors distill 7 key insights related to improving annotation quality and cost-effectiveness, which are then translated into accessible guidelines for practical use. Fourth, a theoretical analysis is provided outlining modifications to the loss function that enable models trained on ACT-generated data to achieve comparable performance to those trained with fully human-labeled data. Finally, experimentation demonstrates that ACT can reduce the model performance gap to less than 2% on standard benchmarks while cutting human annotation costs by up to 90%, indicating a significant advancement in scalable, high-quality data annotation strategies. <div>
arXiv:2511.09833v1 Announce Type: new 
Abstract: Supervised learning relies on high-quality labeled data, but obtaining such data through human annotation is both expensive and time-consuming. Recent work explores using large language models (LLMs) for annotation, but LLM-generated labels still fall short of human-level quality. To address this problem, we propose the Annotation with Critical Thinking (ACT) data pipeline, where LLMs serve not only as annotators but also as judges to critically identify potential errors. Human effort is then directed towards reviewing only the most "suspicious" cases, significantly improving the human annotation efficiency. Our major contributions are as follows: (1) ACT is applicable to a wide range of domains, including natural language processing (NLP), computer vision (CV), and multimodal understanding, by leveraging multimodal-LLMs (MLLMs). (2) Through empirical studies, we derive 7 insights on how to enhance annotation quality while efficiently reducing the human cost, and then translate these findings into user-friendly guidelines. (3) We theoretically analyze how to modify the loss function so that models trained on ACT data achieve similar performance to those trained on fully human-annotated data. Our experiments show that the performance gap can be reduced to less than 2% on most benchmark datasets while saving up to 90% of human costs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Steering Pretrained Drafters during Speculative Decoding</title>
<link>https://arxiv.org/abs/2511.09844</link>
<guid>https://arxiv.org/abs/2511.09844</guid>
<content:encoded><![CDATA[
<div> Speculative decoding, language models, drafter-verifier alignment, pretrained drafters, steering vector<br /><br />Summary:<br /><br />This paper addresses the inefficiencies in speculative decoding for language model inference caused by misalignment between the drafter and verifier components, which limits the number of tokens accepted and reduces overall performance. Two common approaches to drafting are analyzed: small drafting heads trained from scratch, which are fast but less effective when verification latency dominates or input distributions shift, and pretrained drafters, which generate higher acceptance rates due to stronger generation quality but have higher latency. To improve acceptance rates without sacrificing speed, the authors propose a lightweight dynamic alignment mechanism that introduces a steering vector computed from the verifier’s hidden states into the pretrained drafter. This method enhances the synergy between drafter and verifier dynamically during decoding. The proposed approach outperforms existing offline alignment methods, such as distillation, by boosting accepted tokens by up to 35% under standard sampling and 22% under greedy sampling, all while adding negligible computational cost. Furthermore, this technique can be applied retroactively to existing pretrained models and architectures, facilitating quick integration into current workflows and accelerating language model inference efficiency with minimal changes. <div>
arXiv:2511.09844v1 Announce Type: new 
Abstract: Speculative decoding accelerates language model inference by separating generation into fast drafting and parallel verification. Its main limitation is drafter-verifier misalignment, which limits token acceptance and reduces overall effectiveness. While small drafting heads trained from scratch compensate with speed, they struggle when verification dominates latency or when inputs are out of distribution. In contrast, pretrained drafters, though slower, achieve higher acceptance rates thanks to stronger standalone generation capabilities, making them competitive when drafting latency is negligible relative to verification or communication overhead. In this work, we aim to improve the acceptance rates of pretrained drafters by introducing a lightweight dynamic alignment mechanism: a steering vector computed from the verifier's hidden states and injected into the pretrained drafter. Compared to existing offline alignment methods such as distillation, our approach boosts the number of accepted tokens by up to 35\% under standard sampling and 22\% under greedy sampling, all while incurring negligible computational overhead. Importantly, our approach can be retrofitted to existing architectures and pretrained models, enabling rapid adoption.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConSurv: Multimodal Continual Learning for Survival Analysis</title>
<link>https://arxiv.org/abs/2511.09853</link>
<guid>https://arxiv.org/abs/2511.09853</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, multimodal survival analysis, catastrophic forgetting, mixture of experts, feature constrained replay<br /><br />Summary:<br /><br />1. Survival prediction for cancer patients is critical in clinical practice to guide treatment and assess mortality risks, but models trained on single static datasets lack adaptability to dynamically evolving data.<br />2. Continual learning (CL) offers a way to update models over time with new datasets; however, existing CL methods primarily address unimodal data and face severe catastrophic forgetting when applied to survival prediction.<br />3. Multimodal data, such as whole slide images combined with genomics, provide richer, complementary information, yet current methods often overlook complex inter-modal relationships, harming performance.<br />4. To tackle these challenges, the authors propose ConSurv, the first multimodal continual learning (MMCL) framework tailored for survival analysis, which integrates two innovations: Multi-staged Mixture of Experts (MS-MoE) to learn shared and task-specific knowledge across modalities and learning stages, and Feature Constrained Replay (FCR) to reduce forgetting by limiting the deviation of previously learned features at encoder and fusion levels.<br />5. They introduce MSAIL, a new benchmark combining four datasets for a comprehensive evaluation of multimodal continual learning in survival prediction, with experimental results showing that ConSurv surpasses existing methods on multiple performance metrics. <div>
arXiv:2511.09853v1 Announce Type: new 
Abstract: Survival prediction of cancers is crucial for clinical practice, as it informs mortality risks and influences treatment plans. However, a static model trained on a single dataset fails to adapt to the dynamically evolving clinical environment and continuous data streams, limiting its practical utility. While continual learning (CL) offers a solution to learn dynamically from new datasets, existing CL methods primarily focus on unimodal inputs and suffer from severe catastrophic forgetting in survival prediction. In real-world scenarios, multimodal inputs often provide comprehensive and complementary information, such as whole slide images and genomics; and neglecting inter-modal correlations negatively impacts the performance. To address the two challenges of catastrophic forgetting and complex inter-modal interactions between gigapixel whole slide images and genomics, we propose ConSurv, the first multimodal continual learning (MMCL) method for survival analysis. ConSurv incorporates two key components: Multi-staged Mixture of Experts (MS-MoE) and Feature Constrained Replay (FCR). MS-MoE captures both task-shared and task-specific knowledge at different learning stages of the network, including two modality encoders and the modality fusion component, learning inter-modal relationships. FCR further enhances learned knowledge and mitigates forgetting by restricting feature deviation of previous data at different levels, including encoder-level features of two modalities and the fusion-level representations. Additionally, we introduce a new benchmark integrating four datasets, Multimodal Survival Analysis Incremental Learning (MSAIL), for comprehensive evaluation in the CL setting. Extensive experiments demonstrate that ConSurv outperforms competing methods across multiple metrics.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlearning Imperative: Securing Trustworthy and Responsible LLMs through Engineered Forgetting</title>
<link>https://arxiv.org/abs/2511.09855</link>
<guid>https://arxiv.org/abs/2511.09855</guid>
<content:encoded><![CDATA[
<div> Keywords: machine unlearning, large language models, privacy, adversarial recovery, regulatory frameworks<br /><br />Summary:  
This paper addresses the critical challenge of ensuring that large language models (LLMs) can permanently forget sensitive information once it has been used. It highlights that retraining models from scratch is prohibitively expensive and existing unlearning methods are often fragmented, unverifiable, and vulnerable to adversarial attempts at recovery. The survey reviews current techniques to evaluate whether forgetting has effectively occurred and examines the resilience of unlearned models against attacks. It discusses technical solutions such as differential privacy, homomorphic encryption, federated learning, and ephemeral memory as promising tools to enhance unlearning capabilities. Additionally, the paper emphasizes the importance of institutional safeguards, including auditing processes and regulatory frameworks, to build user trust, especially when model complexity or proprietary restrictions limit transparency. The review finds ongoing progress but identifies that robust, verifiable unlearning remains an unresolved problem in the field. The authors call for more efficient unlearning methods that avoid costly retraining, stronger defenses against adversarial recovery, and governance mechanisms that reinforce accountability. Ultimately, the study proposes integrating technical innovations with organizational policies to enable AI systems that can be required to forget, thereby protecting privacy and maintaining public trust in sensitive applications. <div>
arXiv:2511.09855v1 Announce Type: new 
Abstract: The growing use of large language models in sensitive domains has exposed a critical weakness: the inability to ensure that private information can be permanently forgotten. Yet these systems still lack reliable mechanisms to guarantee that sensitive information can be permanently removed once it has been used. Retraining from the beginning is prohibitively costly, and existing unlearning methods remain fragmented, difficult to verify, and often vulnerable to recovery. This paper surveys recent research on machine unlearning for LLMs and considers how far current approaches can address these challenges. We review methods for evaluating whether forgetting has occurred, the resilience of unlearned models against adversarial attacks, and mechanisms that can support user trust when model complexity or proprietary limits restrict transparency. Technical solutions such as differential privacy, homomorphic encryption, federated learning, and ephemeral memory are examined alongside institutional safeguards including auditing practices and regulatory frameworks. The review finds steady progress, but robust and verifiable unlearning is still unresolved. Efficient techniques that avoid costly retraining, stronger defenses against adversarial recovery, and governance structures that reinforce accountability are needed if LLMs are to be deployed safely in sensitive applications. By integrating technical and organizational perspectives, this study outlines a pathway toward AI systems that can be required to forget, while maintaining both privacy and public trust.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Guided Checkpoint Selection for Reinforcement Finetuning of Large Language Models</title>
<link>https://arxiv.org/abs/2511.09864</link>
<guid>https://arxiv.org/abs/2511.09864</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, checkpoint selection, uncertainty, large language models, generalization<br /><br />Summary:  
This article addresses the instability and high variance issues encountered during reinforcement learning (RL) fine-tuning of large language models (LLMs). It highlights the difficulty of selecting the best model checkpoint since evaluating each checkpoint on a validation set is computationally expensive and requires an appropriate validation dataset, while simply choosing the final checkpoint might not yield optimal results. To overcome these challenges, the authors propose an uncertainty-guided checkpoint selection (UGCS) method. UGCS leverages per-sample uncertainty to identify difficult question-answer pairs and ranks checkpoints based on their performance on these challenging samples. By averaging the rewards of the top uncertain samples over a short training period, UGCS generates a stable and discriminative signal for checkpoint selection without extra computation or forward passes. The method was tested on three datasets and three different large language models, demonstrating consistent selection of checkpoints that generalize better compared to traditional approaches relying on training or validation performance. Ultimately, the findings suggest that models exhibiting low uncertainty on their hardest tasks tend to be more reliable and robust overall. <div>
arXiv:2511.09864v1 Announce Type: new 
Abstract: Reinforcement learning (RL) finetuning is crucial to aligning large language models (LLMs), but the process is notoriously unstable and exhibits high variance across model checkpoints. In practice, selecting the best checkpoint is challenging: evaluating checkpoints on the validation set during training is computationally expensive and requires a good validation set, while relying on the final checkpoint provides no guarantee of good performance. We introduce an uncertainty-guided approach for checkpoint selection (UGCS) that avoids these pitfalls. Our method identifies hard question-answer pairs using per-sample uncertainty and ranks checkpoints by how well they handle these challenging cases. By averaging the rewards of the top-uncertain samples over a short training window, our method produces a stable and discriminative signal without additional forward passes or significant computation overhead. Experiments across three datasets and three LLMs demonstrate that it consistently identifies checkpoints with stronger generalization, outperforming traditional strategies such as relying on training or validation performance. These results highlight that models solving their hardest tasks with low uncertainty are the most reliable overall.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Expandable and Differentiable Dual Memories with Orthogonal Regularization for Exemplar-free Continual Learning</title>
<link>https://arxiv.org/abs/2511.09871</link>
<guid>https://arxiv.org/abs/2511.09871</guid>
<content:encoded><![CDATA[
<div> Keywords: continual learning, expandable memory, class-incremental learning, orthogonal regularization, feature extraction<br /><br />Summary:  
This paper addresses the limitations of current continual learning methods that treat sequential tasks in isolation, hindering the exploitation of useful relationships between tasks and causing redundant or overly distinct feature learning. To overcome this, the authors propose a novel fully differentiable, exemplar-free, and expandable framework consisting of two complementary memories: one that captures shared common features usable across all tasks, and another that integrates these shared features to learn discriminative, sample-specific characteristics. Both memory modules are differentiable, enabling autonomous learning of latent representations per sample. The approach includes a memory adjustment module that adaptively prunes essential memory slots and minimally expands capacity to accommodate new concepts in each task. Additionally, orthogonal regularization is employed to enforce geometric separation between preserved and newly learned memory components, mitigating interference. Experimental evaluation on CIFAR-10, CIFAR-100, and Tiny-ImageNet datasets demonstrates that the proposed method surpasses 14 state-of-the-art class-incremental learning methods, yielding final accuracies of 55.13%, 37.24%, and 30.11%, respectively. Further analysis confirms that effective knowledge integration and utilization leads to improved average performance across sequential tasks and achieves feature extraction quality approaching the upper bound, establishing a new benchmark in continual learning research. <div>
arXiv:2511.09871v1 Announce Type: new 
Abstract: Continual learning methods used to force neural networks to process sequential tasks in isolation, preventing them from leveraging useful inter-task relationships and causing them to repeatedly relearn similar features or overly differentiate them. To address this problem, we propose a fully differentiable, exemplar-free expandable method composed of two complementary memories: One learns common features that can be used across all tasks, and the other combines the shared features to learn discriminative characteristics unique to each sample. Both memories are differentiable so that the network can autonomously learn latent representations for each sample. For each task, the memory adjustment module adaptively prunes critical slots and minimally expands capacity to accommodate new concepts, and orthogonal regularization enforces geometric separation between preserved and newly learned memory components to prevent interference. Experiments on CIFAR-10, CIFAR-100, and Tiny-ImageNet show that the proposed method outperforms 14 state-of-the-art methods for class-incremental learning, achieving final accuracies of 55.13\%, 37.24\%, and 30.11\%, respectively. Additional analysis confirms that, through effective integration and utilization of knowledge, the proposed method can increase average performance across sequential tasks, and it produces feature extraction results closest to the upper bound, thus establishing a new milestone in continual learning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Anchor-Based Framework for Scalable Fair Clustering</title>
<link>https://arxiv.org/abs/2511.09889</link>
<guid>https://arxiv.org/abs/2511.09889</guid>
<content:encoded><![CDATA[
<div> Keywords: fair clustering, scalability, anchor sampling, optimization, ADMM

<br /><br />Summary:  
Fair clustering is essential to reduce bias in unsupervised learning, but existing algorithms often have quadratic or worse time complexity, limiting their use on large datasets. To address this, the paper proposes the Anchor-based Fair Clustering Framework (AFCF), which enables any fair clustering method to scale linearly with data size. AFCF begins by selecting a small, representative anchor set using a novel fair sampling strategy. Next, an existing fair clustering algorithm is applied to this anchor set, significantly reducing computational load. The framework’s key innovation is an anchor graph construction module that formulates an optimization problem to propagate cluster labels from anchors to the entire dataset while ensuring fairness through a group-label joint constraint. Theoretically, this constraint guarantees that the final clustering's fairness matches that of the anchors. The resulting optimization problem is efficiently solved using an ADMM-based algorithm. Extensive experiments on various large-scale benchmarks show AFCF can accelerate state-of-the-art fair clustering methods by orders of magnitude without sacrificing clustering quality or fairness. This work thus bridges the gap between fairness and scalability in clustering, making fair clustering practical for real-world large datasets. <div>
arXiv:2511.09889v1 Announce Type: new 
Abstract: Fair clustering is crucial for mitigating bias in unsupervised learning, yet existing algorithms often suffer from quadratic or super-quadratic computational complexity, rendering them impractical for large-scale datasets. To bridge this gap, we introduce the Anchor-based Fair Clustering Framework (AFCF), a novel, general, and plug-and-play framework that empowers arbitrary fair clustering algorithms with linear-time scalability. Our approach first selects a small but representative set of anchors using a novel fair sampling strategy. Then, any off-the-shelf fair clustering algorithm can be applied to this small anchor set. The core of our framework lies in a novel anchor graph construction module, where we formulate an optimization problem to propagate labels while preserving fairness. This is achieved through a carefully designed group-label joint constraint, which we prove theoretically ensures that the fairness of the final clustering on the entire dataset matches that of the anchor clustering. We solve this optimization efficiently using an ADMM-based algorithm. Extensive experiments on multiple large-scale benchmarks demonstrate that AFCF drastically accelerates state-of-the-art methods, which reduces computational time by orders of magnitude while maintaining strong clustering performance and fairness guarantees.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulator and Experience Enhanced Diffusion Model for Comprehensive ECG Generation</title>
<link>https://arxiv.org/abs/2511.09895</link>
<guid>https://arxiv.org/abs/2511.09895</guid>
<content:encoded><![CDATA[
<div> Cardiovascular Disease, Electrocardiogram Generation, Diffusion Models, Physiological Simulator, Clinical Knowledge<br /><br />Summary:<br /><br />1. Cardiovascular disease (CVD) remains a top cause of death worldwide, and electrocardiograms (ECGs) are essential for non-invasive cardiac evaluation, but large annotated ECG datasets are limited due to privacy, cost, and workflow issues. 2. Creating realistic ECG signals informed by clinical context is important to support mechanistic insights, data expansion, and privacy-preserving data sharing, yet methods to generate ECGs from text descriptions remain underdeveloped. 3. Existing diffusion-based text-to-ECG generation approaches often neglect the integration of physiological simulator knowledge and real-world clinical experience, which are critical to producing authentic ECG waveforms. 4. The proposed SE-Diff model innovatively incorporates a lightweight ODE-based physiological ECG simulator into the diffusion framework via a beat decoder and simulator-consistent constraints, introducing mechanistic priors to ensure physiologically sound signals. 5. Additionally, SE-Diff employs a large language model (LLM)-powered experience retrieval mechanism to embed practical clinical knowledge, enhancing guidance during ECG generation. 6. Experiments on real ECG datasets demonstrate that SE-Diff outperforms existing baselines in terms of both signal fidelity and semantic alignment between text input and generated ECGs. 7. Moreover, the combination of simulator-based and experience-based knowledge boosts downstream ECG classification performance, highlighting the utility of integrating mechanistic and experiential knowledge into generative models. <div>
arXiv:2511.09895v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) is a leading cause of mortality worldwide. Electrocardiograms (ECGs) are the most widely used non-invasive tool for cardiac assessment, yet large, well-annotated ECG corpora are scarce due to cost, privacy, and workflow constraints. Generating ECGs can be beneficial for the mechanistic understanding of cardiac electrical activity, enable the construction of large, heterogeneous, and unbiased datasets, and facilitate privacy-preserving data sharing. Generating realistic ECG signals from clinical context is important yet underexplored. Recent work has leveraged diffusion models for text-to-ECG generation, but two challenges remain: (i) existing methods often overlook the physiological simulator knowledge of cardiac activity; and (ii) they ignore broader, experience-based clinical knowledge grounded in real-world practice. To address these gaps, we propose SE-Diff, a novel physiological simulator and experience enhanced diffusion model for comprehensive ECG generation. SE-Diff integrates a lightweight ordinary differential equation (ODE)-based ECG simulator into the diffusion process via a beat decoder and simulator-consistent constraints, injecting mechanistic priors that promote physiologically plausible waveforms. In parallel, we design an LLM-powered experience retrieval-augmented strategy to inject clinical knowledge, providing more guidance for ECG generation. Extensive experiments on real-world ECG datasets demonstrate that SE-Diff improves both signal fidelity and text-ECG semantic alignment over baselines, proving its superiority for text-to-ECG generation. We further show that the simulator-based and experience-based knowledge also benefit downstream ECG classification.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explore and Establish Synergistic Effects Between Weight Pruning and Coreset Selection in Neural Network Training</title>
<link>https://arxiv.org/abs/2511.09901</link>
<guid>https://arxiv.org/abs/2511.09901</guid>
<content:encoded><![CDATA[
<div> Keywords: weight pruning, coreset selection, deep neural networks, computational efficiency, model instability<br /><br />Summary:<br /><br />This paper investigates the relationship between redundant model weights and training samples in deep neural networks, highlighting how noisy or redundant samples cause weights to be overtuned and complicate pruning. Conversely, irrelevant weights tend to overfit noisy data, diminishing the effectiveness of coreset selection. To leverage this interplay, the authors propose a Simultaneous Weight and Sample Tailoring mechanism (SWaST), which alternates between weight pruning and coreset selection to create a synergistic effect during training. A critical challenge identified is the "critical double-loss" phenomenon, where important weights and their supportive samples are simultaneously removed, causing model instability and performance degradation that is hard to recover. This problem arises from the lack of theoretical guarantees in pruning and coreset selection methods within deep learning, explaining why these approaches have traditionally been developed separately. SWaST addresses this by incorporating a state preservation mechanism that stabilizes joint optimization when pruning weights and selecting samples simultaneously. Extensive experiments demonstrate a strong synergy between the two paradigms, producing accuracy improvements of up to 17.83% while reducing computational cost measured in FLOPs by 10% to 90%, validating the effectiveness of the proposed approach across diverse pruning rates and coreset sizes. <div>
arXiv:2511.09901v1 Announce Type: new 
Abstract: Modern deep neural networks rely heavily on massive model weights and training samples, incurring substantial computational costs. Weight pruning and coreset selection are two emerging paradigms proposed to improve computational efficiency. In this paper, we first explore the interplay between redundant weights and training samples through a transparent analysis: redundant samples, particularly noisy ones, cause model weights to become unnecessarily overtuned to fit them, complicating the identification of irrelevant weights during pruning; conversely, irrelevant weights tend to overfit noisy data, undermining coreset selection effectiveness. To further investigate and harness this interplay in deep learning, we develop a Simultaneous Weight and Sample Tailoring mechanism (SWaST) that alternately performs weight pruning and coreset selection to establish a synergistic effect in training. During this investigation, we observe that when simultaneously removing a large number of weights and samples, a phenomenon we term critical double-loss can occur, where important weights and their supportive samples are mistakenly eliminated at the same time, leading to model instability and nearly irreversible degradation that cannot be recovered in subsequent training. Unlike classic machine learning models, this issue can arise in deep learning due to the lack of theoretical guarantees on the correctness of weight pruning and coreset selection, which explains why these paradigms are often developed independently. We mitigate this by integrating a state preservation mechanism into SWaST, enabling stable joint optimization. Extensive experiments reveal a strong synergy between pruning and coreset selection across varying prune rates and coreset sizes, delivering accuracy boosts of up to 17.83% alongside 10% to 90% FLOPs reductions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incremental Generation is Necessity and Sufficient for Universality in Flow-Based Modelling</title>
<link>https://arxiv.org/abs/2511.09902</link>
<guid>https://arxiv.org/abs/2511.09902</guid>
<content:encoded><![CDATA[
<div> Incremental generation, flow-based models, orientation-preserving homeomorphisms, universal approximation, Lipschitz maps<br /><br />Summary:  
This paper establishes a rigorous theoretical foundation for incremental flow-based denoising models, which have revolutionized generative modeling but lacked strong approximation-theoretic guarantees. It focuses on the largest natural class of self-maps compatible with denoising pipelines: the orientation-preserving homeomorphisms of the unit cube \([0,1]^d\). The authors demonstrate that incremental generation is both necessary and sufficient for achieving universal flow-based generation within this class. A novel topological-dynamical argument proves an impossibility theorem showing that single-step autonomous flows cannot universally approximate these maps regardless of network architecture or activation function, indicating their limitation. Conversely, by leveraging algebraic properties of autonomous flows, the paper shows that any orientation-preserving Lipschitz homeomorphism on \([0,1]^d\) can be approximated at a rate of \(\mathcal{O}(n^{-1/d})\) via a finite composition of such flows, with the required number depending only on the dimension \(d\). Under additional smoothness conditions, this approximation can be made dimension-independent and uniformly controlled. Finally, the authors extend these results by lifting the domain dimension to attain universal approximation for continuous functions and probability measures, enabling pushforwards of empirical measures with vanishing 1-Wasserstein error, thereby connecting theory with practical generative modeling. <div>
arXiv:2511.09902v1 Announce Type: new 
Abstract: Incremental flow-based denoising models have reshaped generative modelling, but their empirical advantage still lacks a rigorous approximation-theoretic foundation. We show that incremental generation is necessary and sufficient for universal flow-based generation on the largest natural class of self-maps of $[0,1]^d$ compatible with denoising pipelines, namely the orientation-preserving homeomorphisms of $[0,1]^d$. All our guarantees are uniform on the underlying maps and hence imply approximation both samplewise and in distribution.
  Using a new topological-dynamical argument, we first prove an impossibility theorem: the class of all single-step autonomous flows, independently of the architecture, width, depth, or Lipschitz activation of the underlying neural network, is meagre and therefore not universal in the space of orientation-preserving homeomorphisms of $[0,1]^d$. By exploiting algebraic properties of autonomous flows, we conversely show that every orientation-preserving Lipschitz homeomorphism on $[0,1]^d$ can be approximated at rate $\mathcal{O}(n^{-1/d})$ by a composition of at most $K_d$ such flows, where $K_d$ depends only on the dimension. Under additional smoothness assumptions, the approximation rate can be made dimension-free, and $K_d$ can be chosen uniformly over the class being approximated. Finally, by linearly lifting the domain into one higher dimension, we obtain structured universal approximation results for continuous functions and for probability measures on $[0,1]^d$, the latter realized as pushforwards of empirical measures with vanishing $1$-Wasserstein error.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Diversifying Dataset Distillation by Decoupling Architectural Priors</title>
<link>https://arxiv.org/abs/2511.09905</link>
<guid>https://arxiv.org/abs/2511.09905</guid>
<content:encoded><![CDATA[
<div> Dataset Distillation, Inductive Bias, Multi-Teacher Models, ImageNet-1K, Data Diversity<br /><br />Summary:<br /><br />This paper addresses limitations in dataset distillation (DD) methods that typically rely on a single teacher model, resulting in synthetic data with biased, overly smooth, and homogeneous samples that lack intra-class diversity and generalization capacity. The authors propose PRISM (PRIors from diverse Source Models), a novel framework designed to disentangle architectural priors during the data synthesis process. PRISM innovatively decouples the objectives of logit-matching and regularization by supervising them with different teacher architectures: a primary model guides the logit matching, while a stochastic subset of models aligns batch-normalization statistics. Experiments on ImageNet-1K demonstrate that PRISM consistently outperforms existing single-teacher methods (like SRe2L) and multi-teacher approaches (such as G-VBSM), particularly at low- and mid-images-per-class (IPC) regimes. The synthetic datasets produced by PRISM exhibit significantly richer intra-class diversity, verified through a substantial decrease in cosine similarity between feature representations. The paper further investigates strategies for selecting teacher models, comparing pre-distillation and intra-distillation approaches, and proposes a scalable cross-class batch formation technique that enables faster, parallelized synthesis. The authors intend to release the code following the review period, promoting reproducibility and further exploration. <div>
arXiv:2511.09905v1 Announce Type: new 
Abstract: Dataset distillation (DD) promises compact yet faithful synthetic data, but existing approaches often inherit the inductive bias of a single teacher model. As dataset size increases, this bias drives generation toward overly smooth, homogeneous samples, reducing intra-class diversity and limiting generalization. We present PRISM (PRIors from diverse Source Models), a framework that disentangles architectural priors during synthesis. PRISM decouples the logit-matching and regularization objectives, supervising them with different teacher architectures: a primary model for logits and a stochastic subset for batch-normalization (BN) alignment. On ImageNet-1K, PRISM consistently and reproducibly outperforms single-teacher methods (e.g., SRe2L) and recent multi-teacher variants (e.g., G-VBSM) at low- and mid-IPC regimes. The generated data also show significantly richer intra-class diversity, as reflected by a notable drop in cosine similarity between features. We further analyze teacher selection strategies (pre- vs. intra-distillation) and introduce a scalable cross-class batch formation scheme for fast parallel synthesis. Code will be released after the review period.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Multiple Missing Values-resistant Unsupervised Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.09917</link>
<guid>https://arxiv.org/abs/2511.09917</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised graph anomaly detection, missing values, dual-pathway encoder, imputation bias, latent space regularization<br /><br />Summary:<br /><br />This paper addresses the challenge of unsupervised graph anomaly detection (GAD) when node attributes and graph structure information are incomplete, a common real-world issue caused by privacy constraints, data collection errors, or dynamic graph changes. Traditional methods assume fully observed graphs and rely on imputation strategies that can bias the detection by "repairing" anomalies to appear normal. To tackle this, the authors propose M$^2$V-UGAD, a novel framework designed to be robust against multiple missing values in graphs. The approach utilizes a dual-pathway encoder that independently reconstructs missing node attributes and graph structure, preventing error propagation between views. These representations are then fused and regularized in a joint latent space configuration where normal nodes cluster compactly inside an inner manifold, while anomalies are pushed to an outer shell, improving discriminative capability. Furthermore, to reduce imputation bias, the model generates hard negative examples by sampling latent codes just outside the normal region and decoding them into realistic subgraphs and node features, sharpening the anomaly decision boundary. Extensive experiments on seven public datasets with varying missing data rates demonstrate that M$^2$V-UGAD consistently outperforms existing unsupervised GAD methods, confirming its effectiveness and robustness in incomplete graph scenarios. <div>
arXiv:2511.09917v1 Announce Type: new 
Abstract: Unsupervised graph anomaly detection (GAD) has received increasing attention in recent years, which aims to identify data anomalous patterns utilizing only unlabeled node information from graph-structured data. However, prevailing unsupervised GAD methods typically presuppose complete node attributes and structure information, a condition hardly satisfied in real-world scenarios owing to privacy, collection errors or dynamic node arrivals. Existing standard imputation schemes risk "repairing" rare anomalous nodes so that they appear normal, thereby introducing imputation bias into the detection process. In addition, when both node attributes and edges are missing simultaneously, estimation errors in one view can contaminate the other, causing cross-view interference that further undermines the detection performance. To overcome these challenges, we propose M$^2$V-UGAD, a multiple missing values-resistant unsupervised GAD framework on incomplete graphs. Specifically, a dual-pathway encoder is first proposed to independently reconstruct missing node attributes and graph structure, thereby preventing errors in one view from propagating to the other. The two pathways are then fused and regularized in a joint latent space so that normals occupy a compact inner manifold while anomalies reside on an outer shell. Lastly, to mitigate imputation bias, we sample latent codes just outside the normal region and decode them into realistic node features and subgraphs, providing hard negative examples that sharpen the decision boundary. Experiments on seven public benchmarks demonstrate that M$^2$V-UGAD consistently outperforms existing unsupervised GAD methods across varying missing rates.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Harnessing Bounded-Support Evolution Strategies for Policy Refinement</title>
<link>https://arxiv.org/abs/2511.09923</link>
<guid>https://arxiv.org/abs/2511.09923</guid>
<content:encoded><![CDATA[
<div> Keywords: Evolution Strategies, Triangular-Distribution ES, policy refinement, robotic manipulation, gradient-free updates<br /><br />Summary:<br />1. The paper addresses challenges in improving competent robot policies using on-policy reinforcement learning (RL), particularly due to noisy and low-signal gradients that hinder performance.<br />2. It revisits Evolution Strategies (ES) as an alternative policy-gradient proxy method and introduces localized exploration via bounded, antithetic triangular perturbations tailored for policy refinement.<br />3. The authors propose Triangular-Distribution ES (TD-ES), which combines bounded triangular noise with a centered-rank finite-difference estimator to produce more stable, parallelizable, and gradient-free updates.<br />4. The approach is integrated into a two-stage pipeline where PPO (Proximal Policy Optimization) is used for pretraining, followed by TD-ES for refinement, thereby maintaining early sample efficiency while enabling robust improvements in later training stages.<br />5. Experimental results across various robotic manipulation tasks demonstrate that TD-ES improves success rates by 26.5% relative to PPO alone and significantly reduces variance, providing an efficient and straightforward method for reliable policy refinement using less computational resources. <div>
arXiv:2511.09923v1 Announce Type: new 
Abstract: Improving competent robot policies with on-policy RL is often hampered by noisy, low-signal gradients. We revisit Evolution Strategies (ES) as a policy-gradient proxy and localize exploration with bounded, antithetic triangular perturbations, suitable for policy refinement. We propose Triangular-Distribution ES (TD-ES) which pairs bounded triangular noise with a centered-rank finite-difference estimator to deliver stable, parallelizable, gradient-free updates. In a two-stage pipeline -- PPO pretraining followed by TD-ES refinement -- this preserves early sample efficiency while enabling robust late-stage gains. Across a suite of robotic manipulation tasks, TD-ES raises success rates by 26.5% relative to PPO and greatly reduces variance, offering a simple, compute-light path to reliable refinement.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MDMLP-EIA: Multi-domain Dynamic MLPs with Energy Invariant Attention for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.09924</link>
<guid>https://arxiv.org/abs/2511.09924</guid>
<content:encoded><![CDATA[
<div> Time series forecasting, MLP, seasonal signals, energy invariant attention, dynamic capacity adjustment

<br /><br />Summary:  
Time series forecasting is vital for numerous fields, and while MLP-based models offer competitive performance to Transformers with fewer parameters and better robustness, they encounter challenges such as losing weak seasonal signals, limited capacity in weight-sharing MLPs, and poor channel fusion in channel-independent methods. To overcome these, the paper proposes MDMLP-EIA, featuring three main innovations. First, an adaptive fused dual-domain seasonal MLP distinguishes between strong and weak seasonal components, using a zero-initialized adaptive channel fusion strategy to reduce noise and improve integration of predictions. Second, an energy invariant attention mechanism is designed to focus adaptively on different feature channels in trend and seasonal predictions over time while keeping the total signal energy constant, enhancing robustness in the decomposition-prediction-reconstruction process. Third, a dynamic capacity adjustment mechanism is introduced for channel-independent MLPs that scales neuron count with the square root of the number of channels, ensuring adequate model capacity as channel dimensions grow. Extensive experiments on nine benchmark datasets show that MDMLP-EIA achieves state-of-the-art results in prediction accuracy and computational efficiency, validating the proposed approaches as effective solutions for improving MLP-based time series forecasting models. <div>
arXiv:2511.09924v1 Announce Type: new 
Abstract: Time series forecasting is essential across diverse domains. While MLP-based methods have gained attention for achieving Transformer-comparable performance with fewer parameters and better robustness, they face critical limitations including loss of weak seasonal signals, capacity constraints in weight-sharing MLPs, and insufficient channel fusion in channel-independent strategies. To address these challenges, we propose MDMLP-EIA (Multi-domain Dynamic MLPs with Energy Invariant Attention) with three key innovations. First, we develop an adaptive fused dual-domain seasonal MLP that categorizes seasonal signals into strong and weak components. It employs an adaptive zero-initialized channel fusion strategy to minimize noise interference while effectively integrating predictions. Second, we introduce an energy invariant attention mechanism that adaptively focuses on different feature channels within trend and seasonal predictions across time steps. This mechanism maintains constant total signal energy to align with the decomposition-prediction-reconstruction framework and enhance robustness against disturbances. Third, we propose a dynamic capacity adjustment mechanism for channel-independent MLPs. This mechanism scales neuron count with the square root of channel count, ensuring sufficient capacity as channels increase. Extensive experiments across nine benchmark datasets demonstrate that MDMLP-EIA achieves state-of-the-art performance in both prediction accuracy and computational efficiency.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEGAgent: A Unified Framework for Automated EEG Analysis Using Large Language Models</title>
<link>https://arxiv.org/abs/2511.09947</link>
<guid>https://arxiv.org/abs/2511.09947</guid>
<content:encoded><![CDATA[
<div> EEG analysis, large language models, multi-task, event detection, clinical applications<br /><br />Summary:<br /><br />This article introduces EEGAgent, a general-purpose framework designed to improve the scalability and generalizability of EEG analysis. EEGAgent leverages large language models (LLMs) to plan and schedule multiple specialized tools to automatically perform diverse EEG-related tasks. The framework addresses limitations of existing EEG models that are typically task-specific by supporting multi-task and continuous reasoning scenarios. EEGAgent incorporates key functions such as basic EEG information perception, spatiotemporal EEG exploration, event detection, user interaction, and automated report generation. To achieve these capabilities, a toolbox of various tools is developed, including those for EEG preprocessing, feature extraction, and event detection. The framework was evaluated on public EEG datasets, demonstrating flexibility and interpretability in EEG analysis processes. The results highlight the potential of EEGAgent to support complex and clinically relevant EEG evaluations, indicating strong promise for real-world clinical and cognitive research applications. <div>
arXiv:2511.09947v1 Announce Type: new 
Abstract: Scalable and generalizable analysis of brain activity is essential for advancing both clinical diagnostics and cognitive research. Electroencephalography (EEG), a non-invasive modality with high temporal resolution, has been widely used for brain states analysis. However, most existing EEG models are usually tailored for individual specific tasks, limiting their utility in realistic scenarios where EEG analysis often involves multi-task and continuous reasoning. In this work, we introduce EEGAgent, a general-purpose framework that leverages large language models (LLMs) to schedule and plan multiple tools to automatically complete EEG-related tasks. EEGAgent is capable of performing the key functions: EEG basic information perception, spatiotemporal EEG exploration, EEG event detection, interaction with users, and EEG report generation. To realize these capabilities, we design a toolbox composed of different tools for EEG preprocessing, feature extraction, event detection, etc. These capabilities were evaluated on public datasets, and our EEGAgent can support flexible and interpretable EEG analysis, highlighting its potential for real-world clinical applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Concept Drift Threshold Determination</title>
<link>https://arxiv.org/abs/2511.09953</link>
<guid>https://arxiv.org/abs/2511.09953</guid>
<content:encoded><![CDATA[
<div> Keywords: drift detection, dynamic threshold, model performance, adaptive strategy, machine learning<br /><br />Summary:<br /><br />This paper addresses a critical limitation in existing drift detection methods, which use a fixed detection threshold across all datasets and time periods. The authors note that model performance is highly sensitive to this static threshold, motivating the exploration of dynamic thresholding. They prove theoretically that an adaptive threshold, which changes over time based on data segments, can outperform any single fixed threshold strategy. The proof leverages the concept that combining the best-performing thresholds from individual data segments yields a dynamic strategy superior to any uniform threshold. Building on this theoretical foundation, the authors propose a Dynamic Threshold Determination (DTD) algorithm that enhances current drift detection frameworks by incorporating a novel comparison phase to guide threshold adjustments. Extensive empirical evaluations are conducted on a variety of synthetic and real-world datasets, covering both image and tabular data domains. Results demonstrate that their DTD algorithm significantly improves the performance of state-of-the-art drift detectors, reducing false alarms and late detections while maintaining model accuracy effectively. This advancement suggests that dynamically adjusting detection thresholds in response to data distribution changes is a promising direction for more robust drift detection in machine learning systems. <div>
arXiv:2511.09953v1 Announce Type: new 
Abstract: Existing drift detection methods focus on designing sensitive test statistics. They treat the detection threshold as a fixed hyperparameter, set once to balance false alarms and late detections, and applied uniformly across all datasets and over time. However, maintaining model performance is the key objective from the perspective of machine learning, and we observe that model performance is highly sensitive to this threshold. This observation inspires us to investigate whether a dynamic threshold could be provably better. In this paper, we prove that a threshold that adapts over time can outperform any single fixed threshold. The main idea of the proof is that a dynamic strategy, constructed by combining the best threshold from each individual data segment, is guaranteed to outperform any single threshold that apply to all segments. Based on the theorem, we propose a Dynamic Threshold Determination algorithm. It enhances existing drift detection frameworks with a novel comparison phase to inform how the threshold should be adjusted. Extensive experiments on a wide range of synthetic and real-world datasets, including both image and tabular data, validate that our approach substantially enhances the performance of state-of-the-art drift detectors.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Integrated Decision Support System for Real-Time Market Growth Forecasting and Multi-Source Content Diffusion Analytics</title>
<link>https://arxiv.org/abs/2511.09962</link>
<guid>https://arxiv.org/abs/2511.09962</guid>
<content:encoded><![CDATA[
<div> Keywords: AI-generated content, Decision Support System, Graph Neural Network, Temporal Transformer, marketing ROI<br /><br />Summary:<br /><br />The article addresses the challenges in predicting the diffusion and market impact of AI-generated content (AIGC) due to data heterogeneity, nonlinear propagation, and changing consumer behaviors. It proposes an AI-driven Decision Support System (DSS) that integrates diverse data sources such as social media streams, marketing expenditures, consumer engagement logs, and sentiment analysis to provide comprehensive insights. The core model utilizes a hybrid architecture combining Graph Neural Networks (GNN) to capture diffusion structures and Temporal Transformers to model temporal influence evolution through a dual-channel framework. Additionally, the system incorporates causal inference modules to separate the influence of marketing stimuli on return on investment (ROI) and market visibility, enhancing interpretability. The DSS is tested on large-scale datasets from platforms including Twitter, TikTok, and YouTube advertising, demonstrating superior performance over existing baseline methods across six evaluation metrics. The system not only improves prediction accuracy but also delivers real-time, interpretable insights for better marketing decision-making. Ultimately, the proposed framework offers an advanced tool for understanding and optimizing the spread and commercial outcomes of AIGC in digital marketing environments. <div>
arXiv:2511.09962v1 Announce Type: new 
Abstract: The rapid proliferation of AI-generated content (AIGC) has reshaped the dynamics of digital marketing and online consumer behavior. However, predicting the diffusion trajectory and market impact of such content remains challenging due to data heterogeneity, non linear propagation mechanisms, and evolving consumer interactions. This study proposes an AI driven Decision Support System (DSS) that integrates multi source data including social media streams, marketing expenditure records, consumer engagement logs, and sentiment dynamics using a hybrid Graph Neural Network (GNN) and Temporal Transformer framework. The model jointly learns the content diffusion structure and temporal influence evolution through a dual channel architecture, while causal inference modules disentangle the effects of marketing stimuli on return on investment (ROI) and market visibility. Experiments on large scale real-world datasets collected from multiple online platforms such as Twitter, TikTok, and YouTube advertising show that our system outperforms existing baselines in all six metrics. The proposed DSS enhances marketing decisions by providing interpretable real-time insights into AIGC driven content dissemination and market growth patterns.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MultiTab: A Scalable Foundation for Multitask Learning on Tabular Data</title>
<link>https://arxiv.org/abs/2511.09970</link>
<guid>https://arxiv.org/abs/2511.09970</guid>
<content:encoded><![CDATA[
<div> Keywords: multitask learning, tabular data, transformer, masked-attention, MultiTab-Net<br /><br />Summary:<br /><br />1. The paper addresses the challenge of improving multitask generalization in tabular data, which is widely used across industries like finance, healthcare, and e-commerce.<br />2. Existing multitask learning (MTL) methods for tabular data are mostly based on multi-layer perceptrons (MLPs), which are limited in modeling complex feature interactions and scaling effectively with large datasets.<br />3. To overcome these limitations, the authors propose MultiTab-Net, the first transformer architecture specifically designed for multitask learning on large tabular datasets.<br />4. MultiTab-Net introduces a novel multitask masked-attention mechanism that dynamically models dependencies between features while reducing task competition, enhancing multitask performance.<br />5. Experimental results demonstrate that MultiTab-Net consistently outperforms existing MTL architectures and single-task transformers on diverse datasets, including recommendation systems, socioeconomic census data, and physics datasets, across varying numbers of tasks and feature types.<br />6. The authors also contribute MultiTab-Bench, a synthetic dataset generator that allows systematic evaluation of multitask learning dynamics by adjusting task count, correlation, and complexity.<br />7. The code for MultiTab-Net and MultiTab-Bench has been made publicly available to foster further research and application. <div>
arXiv:2511.09970v1 Announce Type: new 
Abstract: Tabular data is the most abundant data type in the world, powering systems in finance, healthcare, e-commerce, and beyond. As tabular datasets grow and span multiple related targets, there is an increasing need to exploit shared task information for improved multitask generalization. Multitask learning (MTL) has emerged as a powerful way to improve generalization and efficiency, yet most existing work focuses narrowly on large-scale recommendation systems, leaving its potential in broader tabular domains largely underexplored. Also, existing MTL approaches for tabular data predominantly rely on multi-layer perceptron-based backbones, which struggle to capture complex feature interactions and often fail to scale when data is abundant, a limitation that transformer architectures have overcome in other domains. Motivated by this, we introduce MultiTab-Net, the first multitask transformer architecture specifically designed for large tabular data. MultiTab-Net employs a novel multitask masked-attention mechanism that dynamically models feature-feature dependencies while mitigating task competition. Through extensive experiments, we show that MultiTab-Net consistently achieves higher multitask gain than existing MTL architectures and single-task transformers across diverse domains including large-scale recommendation data, census-like socioeconomic data, and physics datasets, spanning a wide range of task counts, task types, and feature modalities. In addition, we contribute MultiTab-Bench, a generalized multitask synthetic dataset generator that enables systematic evaluation of multitask dynamics by tuning task count, task correlations, and relative task complexity. Our code is publicly available at https://github.com/Armanfard-Lab/MultiTab.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rediscovering the Lunar Equation of the Centre with AI Feynman via Embedded Physical Biases</title>
<link>https://arxiv.org/abs/2511.09979</link>
<guid>https://arxiv.org/abs/2511.09979</guid>
<content:encoded><![CDATA[
arXiv:2511.09979v1 Announce Type: new 
Abstract: This work explores using the physics-inspired AI Feynman symbolic regression algorithm to automatically rediscover a fundamental equation in astronomy -- the Equation of the Centre. Through the introduction of observational and inductive biases corresponding to the physical nature of the system through data preprocessing and search space restriction, AI Feynman was successful in recovering the first-order analytical form of this equation from lunar ephemerides data. However, this manual approach highlights a key limitation in its reliance on expert-driven coordinate system selection. We therefore propose an automated preprocessing extension to find the canonical coordinate system. Results demonstrate that targeted domain knowledge embedding enables symbolic regression to rediscover physical laws, but also highlight further challenges in constraining symbolic regression to derive physics equations when leveraging domain knowledge through tailored biases.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Multimodal Learning in the Open World</title>
<link>https://arxiv.org/abs/2511.09989</link>
<guid>https://arxiv.org/abs/2511.09989</guid>
<content:encoded><![CDATA[
arXiv:2511.09989v1 Announce Type: new 
Abstract: The rapid evolution of machine learning has propelled neural networks to unprecedented success across diverse domains. In particular, multimodal learning has emerged as a transformative paradigm, leveraging complementary information from heterogeneous data streams (e.g., text, vision, audio) to advance contextual reasoning and intelligent decision-making. Despite these advancements, current neural network-based models often fall short in open-world environments characterized by inherent unpredictability, where unpredictable environmental composition dynamics, incomplete modality inputs, and spurious distributions relations critically undermine system reliability. While humans naturally adapt to such dynamic, ambiguous scenarios, artificial intelligence systems exhibit stark limitations in robustness, particularly when processing multimodal signals under real-world complexity. This study investigates the fundamental challenge of multimodal learning robustness in open-world settings, aiming to bridge the gap between controlled experimental performance and practical deployment requirements.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Data-Dependent Learning Paradigm for Large Hypothesis Classes</title>
<link>https://arxiv.org/abs/2511.09996</link>
<guid>https://arxiv.org/abs/2511.09996</guid>
<content:encoded><![CDATA[
arXiv:2511.09996v1 Announce Type: new 
Abstract: We address the general task of learning with a set of candidate models that is too large to have a uniform convergence of empirical estimates to true losses. While the common approach to such challenges is SRM (or regularization) based learning algorithms, we propose a novel learning paradigm that relies on stronger incorporation of empirical data and requires less algorithmic decisions to be based on prior assumptions. We analyze the generalization capabilities of our approach and demonstrate its merits in several common learning assumptions, including similarity of close points, clustering of the domain into highly label-homogeneous regions, Lipschitzness assumptions of the labeling rule, and contrastive learning assumptions. Our approach allows utilizing such assumptions without the need to know their true parameters a priori.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DemoTuner: Efficient DBMS Knobs Tuning via LLM-Assisted Demonstration Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09998</link>
<guid>https://arxiv.org/abs/2511.09998</guid>
<content:encoded><![CDATA[
arXiv:2511.09998v1 Announce Type: new 
Abstract: The performance of modern DBMSs such as MySQL and PostgreSQL heavily depends on the configuration of performance-critical knobs. Manual tuning these knobs is laborious and inefficient due to the complex and high-dimensional nature of the configuration space. Among the automated tuning methods, reinforcement learning (RL)-based methods have recently sought to improve the DBMS knobs tuning process from several different perspectives. However, they still encounter challenges with slow convergence speed during offline training. In this paper, we mainly focus on how to leverage the valuable tuning hints contained in various textual documents such as DBMS manuals and web forums to improve the offline training of RL-based methods. To this end, we propose an efficient DBMS knobs tuning framework named DemoTuner via a novel LLM-assisted demonstration reinforcement learning method. Specifically, to comprehensively and accurately mine tuning hints from documents, we design a structured chain of thought prompt to employ LLMs to conduct a condition-aware tuning hints extraction task. To effectively integrate the mined tuning hints into RL agent training, we propose a hint-aware demonstration reinforcement learning algorithm HA-DDPGfD in DemoTuner. As far as we know, DemoTuner is the first work to introduce the demonstration reinforcement learning algorithm for DBMS knobs tuning. Experimental evaluations conducted on MySQL and PostgreSQL across various workloads demonstrate the significant advantages of DemoTuner in both performance improvement and online tuning cost reduction over three representative baselines including DB-BERT, GPTuner and CDBTune. Additionally, DemoTuner also exhibits superior adaptability to application scenarios with unknown workloads.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction as Interference: A Quantum-Inspired Aggregation Approach</title>
<link>https://arxiv.org/abs/2511.10018</link>
<guid>https://arxiv.org/abs/2511.10018</guid>
<content:encoded><![CDATA[
arXiv:2511.10018v1 Announce Type: new 
Abstract: Classical approaches often treat interaction as engineered product terms or as emergent patterns in flexible models, offering little control over how synergy or antagonism arises. We take a quantum-inspired view: following the Born rule (probability as squared amplitude), \emph{coherent} aggregation sums complex amplitudes before squaring, creating an interference cross-term, whereas an \emph{incoherent} proxy sums squared magnitudes and removes it. In a minimal linear-amplitude model, this cross-term equals the standard potential-outcome interaction contrast \(\Delta_{\mathrm{INT}}\) in a \(2\times 2\) factorial design, giving relative phase a direct, mechanism-level control over synergy versus antagonism.
  We instantiate this idea in a lightweight \emph{Interference Kernel Classifier} (IKC) and introduce two diagnostics: \emph{Coherent Gain} (log-likelihood gain of coherent over the incoherent proxy) and \emph{Interference Information} (the induced Kullback-Leibler gap). A controlled phase sweep recovers the identity. On a high-interaction synthetic task (XOR), IKC outperforms strong baselines under paired, budget-matched comparisons; on real tabular data (\emph{Adult} and \emph{Bank Marketing}) it is competitive overall but typically trails the most capacity-rich baseline in paired differences. Holding learned parameters fixed, toggling aggregation from incoherent to coherent consistently improves negative log-likelihood, Brier score, and expected calibration error, with positive Coherent Gain on both datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphSB: Boosting Imbalanced Node Classification on Graphs through Structural Balance</title>
<link>https://arxiv.org/abs/2511.10022</link>
<guid>https://arxiv.org/abs/2511.10022</guid>
<content:encoded><![CDATA[
arXiv:2511.10022v1 Announce Type: new 
Abstract: Imbalanced node classification is a critical challenge in graph learning, where most existing methods typically utilize Graph Neural Networks (GNNs) to learn node representations. These methods can be broadly categorized into the data-level and the algorithm-level. The former aims to synthesize minority-class nodes to mitigate quantity imbalance, while the latter tries to optimize the learning process to highlight minority classes. However, neither category addresses the inherently imbalanced graph structure, which is a fundamental factor that incurs majority-class dominance and minority-class assimilation in GNNs. Our theoretical analysis further supports this critical insight. Therefore, we propose GraphSB (Graph Structural Balance), a novel framework that incorporates Structural Balance as a key strategy to address the underlying imbalanced graph structure before node synthesis. Structural Balance performs a two-stage structure optimization: Structure Enhancement that adaptively builds similarity-based edges to strengthen connectivity of minority-class nodes, and Relation Diffusion that captures higher-order dependencies while amplifying signals from minority classes. Thus, GraphSB balances structural distribution before node synthesis, enabling more effective learning in GNNs. Extensive experiments demonstrate that GraphSB significantly outperforms the state-of-the-art methods. More importantly, the proposed Structural Balance can be seamlessly integrated into state-of-the-art methods as a simple plug-and-play module, increasing their accuracy by an average of 3.67\%.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SVD-NO: Learning PDE Solution Operators with SVD Integral Kernels</title>
<link>https://arxiv.org/abs/2511.10025</link>
<guid>https://arxiv.org/abs/2511.10025</guid>
<content:encoded><![CDATA[
arXiv:2511.10025v1 Announce Type: new 
Abstract: Neural operators have emerged as a promising paradigm for learning solution operators of partial differential equa- tions (PDEs) directly from data. Existing methods, such as those based on Fourier or graph techniques, make strong as- sumptions about the structure of the kernel integral opera- tor, assumptions which may limit expressivity. We present SVD-NO, a neural operator that explicitly parameterizes the kernel by its singular-value decomposition (SVD) and then carries out the integral directly in the low-rank basis. Two lightweight networks learn the left and right singular func- tions, a diagonal parameter matrix learns the singular values, and a Gram-matrix regularizer enforces orthonormality. As SVD-NO approximates the full kernel, it obtains a high de- gree of expressivity. Furthermore, due to its low-rank struc- ture the computational complexity of applying the operator remains reasonable, leading to a practical system. In exten- sive evaluations on five diverse benchmark equations, SVD- NO achieves a new state of the art. In particular, SVD-NO provides greater performance gains on PDEs whose solutions are highly spatially variable. The code of this work is publicly available at https://github.com/2noamk/SVDNO.git.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Latent Variable Structural Causal Model for Causal Discovery under External Interferences</title>
<link>https://arxiv.org/abs/2511.10031</link>
<guid>https://arxiv.org/abs/2511.10031</guid>
<content:encoded><![CDATA[
arXiv:2511.10031v1 Announce Type: new 
Abstract: Inferring causal relationships from observed data is an important task, yet it becomes challenging when the data is subject to various external interferences. Most of these interferences are the additional effects of external factors on observed variables. Since these external factors are often unknown, we introduce latent variables to represent these unobserved factors that affect the observed data. Specifically, to capture the causal strength and adjacency information, we propose a new temporal latent variable structural causal model, incorporating causal strength and adjacency coefficients that represent the causal relationships between variables. Considering that expert knowledge can provide information about unknown interferences in certain scenarios, we develop a method that facilitates the incorporation of prior knowledge into parameter learning based on Variational Inference, to guide the model estimation. Experimental results demonstrate the stability and accuracy of our proposed method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BuddyMoE: Exploiting Expert Redundancy to Accelerate Memory-Constrained Mixture-of-Experts Inference</title>
<link>https://arxiv.org/abs/2511.10054</link>
<guid>https://arxiv.org/abs/2511.10054</guid>
<content:encoded><![CDATA[
arXiv:2511.10054v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures scale language models by activating only a subset of specialized expert networks for each input token, thereby reducing the number of floating-point operations. However, the growing size of modern MoE models causes their full parameter sets to exceed GPU memory capacity; for example, Mixtral-8x7B has 45 billion parameters and requires 87 GB of memory even though only 14 billion parameters are used per token. Existing systems alleviate this limitation by offloading inactive experts to CPU memory, but transferring experts across the PCIe interconnect incurs significant latency (about 10 ms). Prefetching heuristics aim to hide this latency by predicting which experts are needed, but prefetch failures introduce significant stalls and amplify inference latency. In the event of a prefetch failure, prior work offers two primary solutions: either fetch the expert on demand, which incurs a long stall due to the PCIe bottleneck, or drop the expert from the computation, which significantly degrades model accuracy. The critical challenge, therefore, is to maintain both high inference speed and model accuracy when prefetching fails.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Static Structures to Ensembles: Studying and Harnessing Protein Structure Tokenization</title>
<link>https://arxiv.org/abs/2511.10056</link>
<guid>https://arxiv.org/abs/2511.10056</guid>
<content:encoded><![CDATA[
arXiv:2511.10056v1 Announce Type: new 
Abstract: Protein structure tokenization converts 3D structures into discrete or vectorized representations, enabling the integration of structural and sequence data. Despite many recent works on structure tokenization, the properties of the underlying discrete representations are not well understood. In this work, we first demonstrate that the successful utilization of structural tokens in a language model for structure prediction depends on using rich, pre-trained sequence embeddings to bridge the semantic gap between the sequence and structural "language". The analysis of the structural vocabulary itself then reveals significant semantic redundancy, where multiple distinct tokens correspond to nearly identical local geometries, acting as "structural synonyms". This redundancy, rather than being a flaw, can be exploited with a simple "synonym swap" strategy to generate diverse conformational ensembles by perturbing a predicted structure with its structural synonyms. This computationally lightweight method accurately recapitulates protein flexibility, performing competitively with state-of-the-art models. Our study provides fundamental insights into the nature of discrete protein structure representations and introduces a powerful, near-instantaneous method for modeling protein dynamics. Source code is available in https://github.com/IDEA-XL/TokenMD.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAQNAS: FLOPs-aware Hybrid Quantum Neural Architecture Search using Genetic Algorithm</title>
<link>https://arxiv.org/abs/2511.10062</link>
<guid>https://arxiv.org/abs/2511.10062</guid>
<content:encoded><![CDATA[
arXiv:2511.10062v1 Announce Type: new 
Abstract: Hybrid Quantum Neural Networks (HQNNs), which combine parameterized quantum circuits with classical neural layers, are emerging as promising models in the noisy intermediate-scale quantum (NISQ) era. While quantum circuits are not naturally measured in floating point operations (FLOPs), most HQNNs (in NISQ era) are still trained on classical simulators where FLOPs directly dictate runtime and scalability. Hence, FLOPs represent a practical and viable metric to measure the computational complexity of HQNNs. In this work, we introduce FAQNAS, a FLOPs-aware neural architecture search (NAS) framework that formulates HQNN design as a multi-objective optimization problem balancing accuracy and FLOPs. Unlike traditional approaches, FAQNAS explicitly incorporates FLOPs into the optimization objective, enabling the discovery of architectures that achieve strong performance while minimizing computational cost. Experiments on five benchmark datasets (MNIST, Digits, Wine, Breast Cancer, and Iris) show that quantum FLOPs dominate accuracy improvements, while classical FLOPs remain largely fixed. Pareto-optimal solutions reveal that competitive accuracy can often be achieved with significantly reduced computational cost compared to FLOPs-agnostic baselines. Our results establish FLOPs-awareness as a practical criterion for HQNN design in the NISQ era and as a scalable principle for future HQNN systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-Based Stochastic Optimization for Solving Large-Scale Urban Network Security Games</title>
<link>https://arxiv.org/abs/2511.10072</link>
<guid>https://arxiv.org/abs/2511.10072</guid>
<content:encoded><![CDATA[
arXiv:2511.10072v1 Announce Type: new 
Abstract: Urban Network Security Games (UNSGs), which model the strategic allocation of limited security resources on city road networks, are critical for urban safety. However, finding a Nash Equilibrium (NE) in large-scale UNSGs is challenging due to their massive and combinatorial action spaces. One common approach to addressing these games is the Policy-Space Response Oracle (PSRO) framework, which requires computing best responses (BR) at each iteration. However, precisely computing exact BRs is impractical in large-scale games, and employing reinforcement learning to approximate BRs inevitably introduces errors, which limits the overall effectiveness of the PSRO methods. Recent advancements in leveraging non-convex stochastic optimization to approximate an NE offer a promising alternative to the burdensome BR computation. However, utilizing existing stochastic optimization techniques with an unbiased loss function for UNSGs remains challenging because the action spaces are too vast to be effectively represented by neural networks. To address these issues, we introduce Tree-based Stochastic Optimization (TSO), a framework that bridges the gap between the stochastic optimization paradigm for NE-finding and the demands of UNSGs. Specifically, we employ the tree-based action representation that maps the whole action space onto a tree structure, addressing the challenge faced by neural networks in representing actions when the action space cannot be enumerated. We then incorporate this representation into the loss function and theoretically demonstrate its equivalence to the unbiased loss function. To further enhance the quality of the converged solution, we introduce a sample-and-prune mechanism that reduces the risk of being trapped in suboptimal local optima. Extensive experimental results indicate the superiority of TSO over other baseline algorithms in addressing the UNSGs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>eXIAA: eXplainable Injections for Adversarial Attack</title>
<link>https://arxiv.org/abs/2511.10088</link>
<guid>https://arxiv.org/abs/2511.10088</guid>
<content:encoded><![CDATA[
arXiv:2511.10088v1 Announce Type: new 
Abstract: Post-hoc explainability methods are a subset of Machine Learning (ML) that aim to provide a reason for why a model behaves in a certain way. In this paper, we show a new black-box model-agnostic adversarial attack for post-hoc explainable Artificial Intelligence (XAI), particularly in the image domain. The goal of the attack is to modify the original explanations while being undetected by the human eye and maintain the same predicted class. In contrast to previous methods, we do not require any access to the model or its weights, but only to the model's computed predictions and explanations. Additionally, the attack is accomplished in a single step while significantly changing the provided explanations, as demonstrated by empirical evaluation. The low requirements of our method expose a critical vulnerability in current explainability methods, raising concerns about their reliability in safety-critical applications. We systematically generate attacks based on the explanations generated by post-hoc explainability methods (saliency maps, integrated gradients, and DeepLIFT SHAP) for pretrained ResNet-18 and ViT-B16 on ImageNet. The results show that our attacks could lead to dramatically different explanations without changing the predictive probabilities. We validate the effectiveness of our attack, compute the induced change based on the explanation with mean absolute difference, and verify the closeness of the original image and the corrupted one with the Structural Similarity Index Measure (SSIM).
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T2IBias: Uncovering Societal Bias Encoded in the Latent Space of Text-to-Image Generative Models</title>
<link>https://arxiv.org/abs/2511.10089</link>
<guid>https://arxiv.org/abs/2511.10089</guid>
<content:encoded><![CDATA[
arXiv:2511.10089v1 Announce Type: new 
Abstract: Text-to-image (T2I) generative models are largely used in AI-powered real-world applications and value creation. However, their strategic deployment raises critical concerns for responsible AI management, particularly regarding the reproduction and amplification of race- and gender-related stereotypes that can undermine organizational ethics. In this work, we investigate whether such societal biases are systematically encoded within the pretrained latent spaces of state-of-the-art T2I models. We conduct an empirical study across the five most popular open-source models, using ten neutral, profession-related prompts to generate 100 images per profession, resulting in a dataset of 5,000 images evaluated by diverse human assessors representing different races and genders. We demonstrate that all five models encode and amplify pronounced societal skew: caregiving and nursing roles are consistently feminized, while high-status professions such as corporate CEO, politician, doctor, and lawyer are overwhelmingly represented by males and mostly White individuals. We further identify model-specific patterns, such as QWEN-Image's near-exclusive focus on East Asian outputs, Kandinsky's dominance of White individuals, and SDXL's comparatively broader but still biased distributions. These results provide critical insights for AI project managers and practitioners, enabling them to select equitable AI models and customized prompts that generate images in alignment with the principles of responsible AI. We conclude by discussing the risks of these biases and proposing actionable strategies for bias mitigation in building responsible GenAI systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does My Model Fail? Automatic Identification and Interpretation of Physical Plausibility Failure Modes with Matryoshka Transcoders</title>
<link>https://arxiv.org/abs/2511.10094</link>
<guid>https://arxiv.org/abs/2511.10094</guid>
<content:encoded><![CDATA[
arXiv:2511.10094v1 Announce Type: new 
Abstract: Although recent generative models are remarkably capable of producing instruction-following and realistic outputs, they remain prone to notable physical plausibility failures. Though critical in applications, these physical plausibility errors often escape detection by existing evaluation methods. Furthermore, no framework exists for automatically identifying and interpreting specific physical error patterns in natural language, preventing targeted model improvements. We introduce Matryoshka Transcoders, a novel framework for the automatic discovery and interpretation of physical plausibility features in generative models. Our approach extends the Matryoshka representation learning paradigm to transcoder architectures, enabling hierarchical sparse feature learning at multiple granularity levels. By training on intermediate representations from a physical plausibility classifier and leveraging large multimodal models for interpretation, our method identifies diverse physics-related failure modes without manual feature engineering, achieving superior feature relevance and feature accuracy compared to existing approaches. We utilize the discovered visual patterns to establish a benchmark for evaluating physical plausibility in generative models. Our analysis of eight state-of-the-art generative models provides valuable insights into how these models fail to follow physical constraints, paving the way for further model improvements.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RI-Loss: A Learnable Residual-Informed Loss for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.10130</link>
<guid>https://arxiv.org/abs/2511.10130</guid>
<content:encoded><![CDATA[
arXiv:2511.10130v1 Announce Type: new 
Abstract: Time series forecasting relies on predicting future values from historical data, yet most state-of-the-art approaches-including transformer and multilayer perceptron-based models-optimize using Mean Squared Error (MSE), which has two fundamental weaknesses: its point-wise error computation fails to capture temporal relationships, and it does not account for inherent noise in the data. To overcome these limitations, we introduce the Residual-Informed Loss (RI-Loss), a novel objective function based on the Hilbert-Schmidt Independence Criterion (HSIC). RI-Loss explicitly models noise structure by enforcing dependence between the residual sequence and a random time series, enabling more robust, noise-aware representations. Theoretically, we derive the first non-asymptotic HSIC bound with explicit double-sample complexity terms, achieving optimal convergence rates through Bernstein-type concentration inequalities and Rademacher complexity analysis. This provides rigorous guarantees for RI-Loss optimization while precisely quantifying kernel space interactions. Empirically, experiments across eight real-world benchmarks and five leading forecasting models demonstrate improvements in predictive performance, validating the effectiveness of our approach. Code will be made publicly available to ensure reproducibility.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EPO: Diverse and Realistic Protein Ensemble Generation via Energy Preference Optimization</title>
<link>https://arxiv.org/abs/2511.10165</link>
<guid>https://arxiv.org/abs/2511.10165</guid>
<content:encoded><![CDATA[
arXiv:2511.10165v1 Announce Type: new 
Abstract: Accurate exploration of protein conformational ensembles is essential for uncovering function but remains hard because molecular-dynamics (MD) simulations suffer from high computational costs and energy-barrier trapping. This paper presents Energy Preference Optimization (EPO), an online refinement algorithm that turns a pretrained protein ensemble generator into an energy-aware sampler without extra MD trajectories. Specifically, EPO leverages stochastic differential equation sampling to explore the conformational landscape and incorporates a novel energy-ranking mechanism based on list-wise preference optimization. Crucially, EPO introduces a practical upper bound to efficiently approximate the intractable probability of long sampling trajectories in continuous-time generative models, making it easily adaptable to existing pretrained generators. On Tetrapeptides, ATLAS, and Fast-Folding benchmarks, EPO successfully generates diverse and physically realistic ensembles, establishing a new state-of-the-art in nine evaluation metrics. These results demonstrate that energy-only preference signals can efficiently steer generative models toward thermodynamically consistent conformational ensembles, providing an alternative to long MD simulations and widening the applicability of learned potentials in structural biology and drug discovery.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Offline Reinforcement Learning via Quantum Metric Encoding</title>
<link>https://arxiv.org/abs/2511.10187</link>
<guid>https://arxiv.org/abs/2511.10187</guid>
<content:encoded><![CDATA[
arXiv:2511.10187v1 Announce Type: new 
Abstract: Reinforcement learning (RL) with limited samples is common in real-world applications. However, offline RL performance under this constraint is often suboptimal. We consider an alternative approach to dealing with limited samples by introducing the Quantum Metric Encoder (QME). In this methodology, instead of applying the RL framework directly on the original states and rewards, we embed the states into a more compact and meaningful representation, where the structure of the encoding is inspired by quantum circuits. For classical data, QME is a classically simulable, trainable unitary embedding and thus serves as a quantum-inspired module, on a classical device. For quantum data in the form of quantum states, QME can be implemented directly on quantum hardware, allowing for training without measurement or re-encoding.
  We evaluated QME on three datasets, each limited to 100 samples. We use Soft-Actor-Critic (SAC) and Implicit-Q-Learning (IQL), two well-known RL algorithms, to demonstrate the effectiveness of our approach. From the experimental results, we find that training offline RL agents on QME-embedded states with decoded rewards yields significantly better performance than training on the original states and rewards. On average across the three datasets, for maximum reward performance, we achieve a 116.2% improvement for SAC and 117.6% for IQL.
  We further investigate the $\Delta$-hyperbolicity of our framework, a geometric property of the state space known to be important for the RL training efficacy. The QME-embedded states exhibit low $\Delta$-hyperbolicity, suggesting that the improvement after embedding arises from the modified geometry of the state space induced by QME. Thus, the low $\Delta$-hyperbolicity and the corresponding effectiveness of QME could provide valuable information for developing efficient offline RL methods under limited-sample conditions.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Leveraging Sequential Structure in Animal Vocalizations</title>
<link>https://arxiv.org/abs/2511.10190</link>
<guid>https://arxiv.org/abs/2511.10190</guid>
<content:encoded><![CDATA[
arXiv:2511.10190v1 Announce Type: new 
Abstract: Animal vocalizations contain sequential structures that carry important communicative information, yet most computational bioacoustics studies average the extracted frame-level features across the temporal axis, discarding the order of the sub-units within a vocalization. This paper investigates whether discrete acoustic token sequences, derived through vector quantization and gumbel-softmax vector quantization of extracted self-supervised speech model representations can effectively capture and leverage temporal information. To that end, pairwise distance analysis of token sequences generated from HuBERT embeddings shows that they can discriminate call-types and callers across four bioacoustics datasets. Sequence classification experiments using $k$-Nearest Neighbour with Levenshtein distance show that the vector-quantized token sequences yield reasonable call-type and caller classification performances, and hold promise as alternative feature representations towards leveraging sequential information in animal vocalizations.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond MSE: Ordinal Cross-Entropy for Probabilistic Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.10200</link>
<guid>https://arxiv.org/abs/2511.10200</guid>
<content:encoded><![CDATA[
arXiv:2511.10200v1 Announce Type: new 
Abstract: Time series forecasting is an important task that involves analyzing temporal dependencies and underlying patterns (such as trends, cyclicality, and seasonality) in historical data to predict future values or trends. Current deep learning-based forecasting models primarily employ Mean Squared Error (MSE) loss functions for regression modeling. Despite enabling direct value prediction, this method offers no uncertainty estimation and exhibits poor outlier robustness. To address these limitations, we propose OCE-TS, a novel ordinal classification approach for time series forecasting that replaces MSE with Ordinal Cross-Entropy (OCE) loss, preserving prediction order while quantifying uncertainty through probability output. Specifically, OCE-TS begins by discretizing observed values into ordered intervals and deriving their probabilities via a parametric distribution as supervision signals. Using a simple linear model, we then predict probability distributions for each timestep. The OCE loss is computed between the cumulative distributions of predicted and ground-truth probabilities, explicitly preserving ordinal relationships among forecasted values. Through theoretical analysis using influence functions, we establish that cross-entropy (CE) loss exhibits superior stability and outlier robustness compared to MSE loss. Empirically, we compared OCE-TS with five baseline models-Autoformer, DLinear, iTransformer, TimeXer, and TimeBridge-on seven public time series datasets. Using MSE and Mean Absolute Error (MAE) as evaluation metrics, the results demonstrate that OCE-TS consistently outperforms benchmark models. The code will be published.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fractional neural attention for efficient multiscale sequence processing</title>
<link>https://arxiv.org/abs/2511.10208</link>
<guid>https://arxiv.org/abs/2511.10208</guid>
<content:encoded><![CDATA[
arXiv:2511.10208v1 Announce Type: new 
Abstract: Attention mechanisms underpin the computational power of Transformer models, which have achieved remarkable success across diverse domains. Yet understanding and extending the principles underlying self-attention remains a key challenge for advancing artificial intelligence. Drawing inspiration from the multiscale dynamics of biological attention and from dynamical systems theory, we introduce Fractional Neural Attention (FNA), a principled, neuroscience-inspired framework for multiscale information processing. FNA models token interactions through L\'evy diffusion governed by the fractional Laplacian, intrinsically realizing simultaneous short- and long-range dependencies across multiple scales. This mechanism yields greater expressivity and faster information mixing, advancing the foundational capacity of Transformers. Theoretically, we show that FNA's dynamics are governed by the fractional diffusion equation, and that the resulting attention networks exhibit larger spectral gaps and shorter path lengths -- mechanistic signatures of enhanced computational efficiency. Empirically, FNA achieves competitive text-classification performance even with a single layer and a single head; it also improves performance in image processing and neural machine translation. Finally, the diffusion map algorithm from geometric harmonics enables dimensionality reduction of FNA weights while preserving the intrinsic structure of embeddings and hidden states. Together, these results establish FNA as a principled mechanism connecting self-attention, stochastic dynamics, and geometry, providing an interpretable, biologically grounded foundation for powerful, neuroscience-inspired AI.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Out-of-Context Misinformation Detection via Variational Domain-Invariant Learning with Test-Time Training</title>
<link>https://arxiv.org/abs/2511.10213</link>
<guid>https://arxiv.org/abs/2511.10213</guid>
<content:encoded><![CDATA[
arXiv:2511.10213v1 Announce Type: new 
Abstract: Out-of-context misinformation (OOC) is a low-cost form of misinformation in news reports, which refers to place authentic images into out-of-context or fabricated image-text pairings. This problem has attracted significant attention from researchers in recent years. Current methods focus on assessing image-text consistency or generating explanations. However, these approaches assume that the training and test data are drawn from the same distribution. When encountering novel news domains, models tend to perform poorly due to the lack of prior knowledge. To address this challenge, we propose \textbf{VDT} to enhance the domain adaptation capability for OOC misinformation detection by learning domain-invariant features and test-time training mechanisms. Domain-Invariant Variational Align module is employed to jointly encodes source and target domain data to learn a separable distributional space domain-invariant features. For preserving semantic integrity, we utilize domain consistency constraint module to reconstruct the source and target domain latent distribution. During testing phase, we adopt the test-time training strategy and confidence-variance filtering module to dynamically updating the VAE encoder and classifier, facilitating the model's adaptation to the target domain distribution. Extensive experiments conducted on the benchmark dataset NewsCLIPpings demonstrate that our method outperforms state-of-the-art baselines under most domain adaptation settings.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedCure: Mitigating Participation Bias in Semi-Asynchronous Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2511.10227</link>
<guid>https://arxiv.org/abs/2511.10227</guid>
<content:encoded><![CDATA[
arXiv:2511.10227v1 Announce Type: new 
Abstract: While semi-asynchronous federated learning (SAFL) combines the efficiency of synchronous training with the flexibility of asynchronous updates, it inherently suffers from participation bias, which is further exacerbated by non-IID data distributions. More importantly, hierarchical architecture shifts participation from individual clients to client groups, thereby further intensifying this issue. Despite notable advancements in SAFL research, most existing works still focus on conventional cloud-end architectures while largely overlooking the critical impact of non-IID data on scheduling across the cloud-edge-client hierarchy. To tackle these challenges, we propose FedCure, an innovative semi-asynchronous Federated learning framework that leverages coalition construction and participation-aware scheduling to mitigate participation bias with non-IID data. Specifically, FedCure operates through three key rules: (1) a preference rule that optimizes coalition formation by maximizing collective benefits and establishing theoretically stable partitions to reduce non-IID-induced performance degradation; (2) a scheduling rule that integrates the virtual queue technique with Bayesian-estimated coalition dynamics, mitigating efficiency loss while ensuring mean rate stability; and (3) a resource allocation rule that enhances computational efficiency by optimizing client CPU frequencies based on estimated coalition dynamics while satisfying delay requirements. Comprehensive experiments on four real-world datasets demonstrate that FedCure improves accuracy by up to 5.1x compared with four state-of-the-art baselines, while significantly enhancing efficiency with the lowest coefficient of variation 0.0223 for per-round latency and maintaining long-term balance across diverse scenarios.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lost in Serialization: Invariance and Generalization of LLM Graph Reasoners</title>
<link>https://arxiv.org/abs/2511.10234</link>
<guid>https://arxiv.org/abs/2511.10234</guid>
<content:encoded><![CDATA[
arXiv:2511.10234v1 Announce Type: new 
Abstract: While promising, graph reasoners based on Large Language Models (LLMs) lack built-in invariance to symmetries in graph representations. Operating on sequential graph serializations, LLMs can produce different outputs under node reindexing, edge reordering, or formatting changes, raising robustness concerns. We systematically analyze these effects, studying how fine-tuning impacts encoding sensitivity as well generalization on unseen tasks. We propose a principled decomposition of graph serializations into node labeling, edge encoding, and syntax, and evaluate LLM robustness to variations of each of these factors on a comprehensive benchmarking suite. We also contribute a novel set of spectral tasks to further assess generalization abilities of fine-tuned reasoners. Results show that larger (non-fine-tuned) models are more robust. Fine-tuning reduces sensitivity to node relabeling but may increase it to variations in structure and format, while it does not consistently improve performance on unseen tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heuristic Transformer: Belief Augmented In-Context Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10251</link>
<guid>https://arxiv.org/abs/2511.10251</guid>
<content:encoded><![CDATA[
arXiv:2511.10251v1 Announce Type: new 
Abstract: Transformers have demonstrated exceptional in-context learning (ICL) capabilities, enabling applications across natural language processing, computer vision, and sequential decision-making. In reinforcement learning, ICL reframes learning as a supervised problem, facilitating task adaptation without parameter updates. Building on prior work leveraging transformers for sequential decision-making, we propose Heuristic Transformer (HT), an in-context reinforcement learning (ICRL) approach that augments the in-context dataset with a belief distribution over rewards to achieve better decision-making. Using a variational auto-encoder (VAE), a low-dimensional stochastic variable is learned to represent the posterior distribution over rewards, which is incorporated alongside an in-context dataset and query states as prompt to the transformer policy. We assess the performance of HT across the Darkroom, Miniworld, and MuJoCo environments, showing that it consistently surpasses comparable baselines in terms of both effectiveness and generalization. Our method presents a promising direction to bridge the gap between belief-based augmentations and transformer-based decision-making.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unitho: A Unified Multi-Task Framework for Computational Lithography</title>
<link>https://arxiv.org/abs/2511.10255</link>
<guid>https://arxiv.org/abs/2511.10255</guid>
<content:encoded><![CDATA[
arXiv:2511.10255v1 Announce Type: new 
Abstract: Reliable, generalizable data foundations are critical for enabling large-scale models in computational lithography. However, essential tasks-mask generation, rule violation detection, and layout optimization-are often handled in isolation, hindered by scarce datasets and limited modeling approaches. To address these challenges, we introduce Unitho, a unified multi-task large vision model built upon the Transformer architecture. Trained on a large-scale industrial lithography simulation dataset with hundreds of thousands of cases, Unitho supports end-to-end mask generation, lithography simulation, and rule violation detection. By enabling agile and high-fidelity lithography simulation, Unitho further facilitates the construction of robust data foundations for intelligent EDA. Experimental results validate its effectiveness and generalizability, with performance substantially surpassing academic baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Torch-Uncertainty: A Deep Learning Framework for Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2511.10282</link>
<guid>https://arxiv.org/abs/2511.10282</guid>
<content:encoded><![CDATA[
arXiv:2511.10282v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) have demonstrated remarkable performance across various domains, including computer vision and natural language processing. However, they often struggle to accurately quantify the uncertainty of their predictions, limiting their broader adoption in critical real-world applications. Uncertainty Quantification (UQ) for Deep Learning seeks to address this challenge by providing methods to improve the reliability of uncertainty estimates. Although numerous techniques have been proposed, a unified tool offering a seamless workflow to evaluate and integrate these methods remains lacking. To bridge this gap, we introduce Torch-Uncertainty, a PyTorch and Lightning-based framework designed to streamline DNN training and evaluation with UQ techniques and metrics. In this paper, we outline the foundational principles of our library and present comprehensive experimental results that benchmark a diverse set of UQ methods across classification, segmentation, and regression tasks. Our library is available at https://github.com/ENSTA-U2IS-AI/Torch-Uncertainty
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OutSafe-Bench: A Benchmark for Multimodal Offensive Content Detection in Large Language Models</title>
<link>https://arxiv.org/abs/2511.10287</link>
<guid>https://arxiv.org/abs/2511.10287</guid>
<content:encoded><![CDATA[
arXiv:2511.10287v1 Announce Type: new 
Abstract: Since Multimodal Large Language Models (MLLMs) are increasingly being integrated into everyday tools and intelligent agents, growing concerns have arisen regarding their possible output of unsafe contents, ranging from toxic language and biased imagery to privacy violations and harmful misinformation. Current safety benchmarks remain highly limited in both modality coverage and performance evaluations, often neglecting the extensive landscape of content safety. In this work, we introduce OutSafe-Bench, the first most comprehensive content safety evaluation test suite designed for the multimodal era. OutSafe-Bench includes a large-scale dataset that spans four modalities, featuring over 18,000 bilingual (Chinese and English) text prompts, 4,500 images, 450 audio clips and 450 videos, all systematically annotated across nine critical content risk categories. In addition to the dataset, we introduce a Multidimensional Cross Risk Score (MCRS), a novel metric designed to model and assess overlapping and correlated content risks across different categories. To ensure fair and robust evaluation, we propose FairScore, an explainable automated multi-reviewer weighted aggregation framework. FairScore selects top-performing models as adaptive juries, thereby mitigating biases from single-model judgments and enhancing overall evaluation reliability. Our evaluation of nine state-of-the-art MLLMs reveals persistent and substantial safety vulnerabilities, underscoring the pressing need for robust safeguards in MLLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PITE: Multi-Prototype Alignment for Individual Treatment Effect Estimation</title>
<link>https://arxiv.org/abs/2511.10320</link>
<guid>https://arxiv.org/abs/2511.10320</guid>
<content:encoded><![CDATA[
arXiv:2511.10320v1 Announce Type: new 
Abstract: Estimating Individual Treatment Effects (ITE) from observational data is challenging due to confounding bias. Most studies tackle this bias by balancing distributions globally, but ignore individual heterogeneity and fail to capture the local structure that represents the natural clustering among individuals, which ultimately compromises ITE estimation. While instance-level alignment methods consider heterogeneity, they similarly overlook the local structure information. To address these issues, we propose an end-to-end Multi-\textbf{P}rototype alignment method for \textbf{ITE} estimation (\textbf{PITE}). PITE effectively captures local structure within groups and enforces cross-group alignment, thereby achieving robust ITE estimation. Specifically, we first define prototypes as cluster centroids based on similar individuals under the same treatment. To identify local similarity and the distribution consistency, we perform instance-to-prototype matching to assign individuals to the nearest prototype within groups, and design a multi-prototype alignment strategy to encourage the matched prototypes to be close across treatment arms in the latent space. PITE not only reduces distribution shift through fine-grained, prototype-level alignment, but also preserves the local structures of treated and control groups, which provides meaningful constraints for ITE estimation. Extensive evaluations on benchmark datasets demonstrate that PITE outperforms 13 state-of-the-art methods, achieving more accurate and robust ITE estimation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training</title>
<link>https://arxiv.org/abs/2511.10333</link>
<guid>https://arxiv.org/abs/2511.10333</guid>
<content:encoded><![CDATA[
arXiv:2511.10333v1 Announce Type: new 
Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Decentralized Multi-armed Bandits: From Corruption-Resilience to Byzantine-Resilience</title>
<link>https://arxiv.org/abs/2511.10344</link>
<guid>https://arxiv.org/abs/2511.10344</guid>
<content:encoded><![CDATA[
arXiv:2511.10344v1 Announce Type: new 
Abstract: Decentralized cooperative multi-agent multi-armed bandits (DeCMA2B) considers how multiple agents collaborate in a decentralized multi-armed bandit setting. Though this problem has been extensively studied in previous work, most existing methods remain susceptible to various adversarial attacks. In this paper, we first study DeCMA2B with adversarial corruption, where an adversary can corrupt reward observations of all agents with a limited corruption budget. We propose a robust algorithm, called DeMABAR, which ensures that each agent's individual regret suffers only an additive term proportional to the corruption budget. Then we consider a more realistic scenario where the adversary can only attack a small number of agents. Our theoretical analysis shows that the DeMABAR algorithm can also almost completely eliminate the influence of adversarial attacks and is inherently robust in the Byzantine setting, where an unknown fraction of the agents can be Byzantine, i.e., may arbitrarily select arms and communicate wrong information. We also conduct numerical experiments to illustrate the robustness and effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Flow Equations for Deep Linear Neural Networks: A Survey from a Network Perspective</title>
<link>https://arxiv.org/abs/2511.10362</link>
<guid>https://arxiv.org/abs/2511.10362</guid>
<content:encoded><![CDATA[
arXiv:2511.10362v1 Announce Type: new 
Abstract: The paper surveys recent progresses in understanding the dynamics and loss landscape of the gradient flow equations associated to deep linear neural networks, i.e., the gradient descent training dynamics (in the limit when the step size goes to 0) of deep neural networks missing the activation functions and subject to quadratic loss functions. When formulated in terms of the adjacency matrix of the neural network, as we do in the paper, these gradient flow equations form a class of converging matrix ODEs which is nilpotent, polynomial, isospectral, and with conservation laws. The loss landscape is described in detail. It is characterized by infinitely many global minima and saddle points, both strict and nonstrict, but lacks local minima and maxima. The loss function itself is a positive semidefinite Lyapunov function for the gradient flow, and its level sets are unbounded invariant sets of critical points, with critical values that correspond to the amount of singular values of the input-output data learnt by the gradient along a certain trajectory. The adjacency matrix representation we use in the paper allows to highlight the existence of a quotient space structure in which each critical value of the loss function is represented only once, while all other critical points with the same critical value belong to the fiber associated to the quotient space. It also allows to easily determine stable and unstable submanifolds at the saddle points, even when the Hessian fails to obtain them.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Product distribution learning with imperfect advice</title>
<link>https://arxiv.org/abs/2511.10366</link>
<guid>https://arxiv.org/abs/2511.10366</guid>
<content:encoded><![CDATA[
arXiv:2511.10366v1 Announce Type: new 
Abstract: Given i.i.d.~samples from an unknown distribution $P$, the goal of distribution learning is to recover the parameters of a distribution that is close to $P$. When $P$ belongs to the class of product distributions on the Boolean hypercube $\{0,1\}^d$, it is known that $\Omega(d/\varepsilon^2)$ samples are necessary to learn $P$ within total variation (TV) distance $\varepsilon$. We revisit this problem when the learner is also given as advice the parameters of a product distribution $Q$. We show that there is an efficient algorithm to learn $P$ within TV distance $\varepsilon$ that has sample complexity $\tilde{O}(d^{1-\eta}/\varepsilon^2)$, if $\|\mathbf{p} - \mathbf{q}\|_1 < \varepsilon d^{0.5 - \Omega(\eta)}$. Here, $\mathbf{p}$ and $\mathbf{q}$ are the mean vectors of $P$ and $Q$ respectively, and no bound on $\|\mathbf{p} - \mathbf{q}\|_1$ is known to the algorithm a priori.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Kernel Power K-means: Scalable and Robust Clustering with Random Fourier Features and Possibilistic Method</title>
<link>https://arxiv.org/abs/2511.10392</link>
<guid>https://arxiv.org/abs/2511.10392</guid>
<content:encoded><![CDATA[
arXiv:2511.10392v1 Announce Type: new 
Abstract: Kernel power $k$-means (KPKM) leverages a family of means to mitigate local minima issues in kernel $k$-means. However, KPKM faces two key limitations: (1) the computational burden of the full kernel matrix restricts its use on extensive data, and (2) the lack of authentic centroid-sample assignment learning reduces its noise robustness. To overcome these challenges, we propose RFF-KPKM, introducing the first approximation theory for applying random Fourier features (RFF) to KPKM. RFF-KPKM employs RFF to generate efficient, low-dimensional feature maps, bypassing the need for the whole kernel matrix. Crucially, we are the first to establish strong theoretical guarantees for this combination: (1) an excess risk bound of $\mathcal{O}(\sqrt{k^3/n})$, (2) strong consistency with membership values, and (3) a $(1+\varepsilon)$ relative error bound achievable using the RFF of dimension $\mathrm{poly}(\varepsilon^{-1}\log k)$. Furthermore, to improve robustness and the ability to learn multiple kernels, we propose IP-RFF-MKPKM, an improved possibilistic RFF-based multiple kernel power $k$-means. IP-RFF-MKPKM ensures the scalability of MKPKM via RFF and refines cluster assignments by combining the merits of the possibilistic membership and fuzzy membership. Experiments on large-scale datasets demonstrate the superior efficiency and clustering accuracy of the proposed methods compared to the state-of-the-art alternatives.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentEvolver: Towards Efficient Self-Evolving Agent System</title>
<link>https://arxiv.org/abs/2511.10395</link>
<guid>https://arxiv.org/abs/2511.10395</guid>
<content:encoded><![CDATA[
arXiv:2511.10395v1 Announce Type: new 
Abstract: Autonomous agents powered by large language models (LLMs) have the potential to significantly enhance human productivity by reasoning, using tools, and executing complex tasks in diverse environments. However, current approaches to developing such agents remain costly and inefficient, as they typically require manually constructed task datasets and reinforcement learning (RL) pipelines with extensive random exploration. These limitations lead to prohibitively high data-construction costs, low exploration efficiency, and poor sample utilization. To address these challenges, we present AgentEvolver, a self-evolving agent system that leverages the semantic understanding and reasoning capabilities of LLMs to drive autonomous agent learning. AgentEvolver introduces three synergistic mechanisms: (i) self-questioning, which enables curiosity-driven task generation in novel environments, reducing dependence on handcrafted datasets; (ii) self-navigating, which improves exploration efficiency through experience reuse and hybrid policy guidance; and (iii) self-attributing, which enhances sample efficiency by assigning differentiated rewards to trajectory states and actions based on their contribution. By integrating these mechanisms into a unified framework, AgentEvolver enables scalable, cost-effective, and continual improvement of agent capabilities. Preliminary experiments indicate that AgentEvolver achieves more efficient exploration, better sample utilization, and faster adaptation compared to traditional RL-based baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking Dynamic Inter-Client Spatial Dependencies: A Federated Spatio-Temporal Graph Learning Method for Traffic Flow Forecasting</title>
<link>https://arxiv.org/abs/2511.10434</link>
<guid>https://arxiv.org/abs/2511.10434</guid>
<content:encoded><![CDATA[
arXiv:2511.10434v1 Announce Type: new 
Abstract: Spatio-temporal graphs are powerful tools for modeling complex dependencies in traffic time series. However, the distributed nature of real-world traffic data across multiple stakeholders poses significant challenges in modeling and reconstructing inter-client spatial dependencies while adhering to data locality constraints. Existing methods primarily address static dependencies, overlooking their dynamic nature and resulting in suboptimal performance. In response, we propose Federated Spatio-Temporal Graph with Dynamic Inter-Client Dependencies (FedSTGD), a framework designed to model and reconstruct dynamic inter-client spatial dependencies in federated learning. FedSTGD incorporates a federated nonlinear computation decomposition module to approximate complex graph operations. This is complemented by a graph node embedding augmentation module, which alleviates performance degradation arising from the decomposition. These modules are coordinated through a client-server collective learning protocol, which decomposes dynamic inter-client spatial dependency learning tasks into lightweight, parallelizable subtasks. Extensive experiments on four real-world datasets demonstrate that FedSTGD achieves superior performance over state-of-the-art baselines in terms of RMSE, MAE, and MAPE, approaching that of centralized baselines. Ablation studies confirm the contribution of each module in addressing dynamic inter-client spatial dependencies, while sensitivity analysis highlights the robustness of FedSTGD to variations in hyperparameters.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuronal Fluctuations: Learning Rates vs Participating Neurons</title>
<link>https://arxiv.org/abs/2511.10435</link>
<guid>https://arxiv.org/abs/2511.10435</guid>
<content:encoded><![CDATA[
arXiv:2511.10435v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) rely on inherent fluctuations in their internal parameters (weights and biases) to effectively navigate the complex optimization landscape and achieve robust performance. While these fluctuations are recognized as crucial for escaping local minima and improving generalization, their precise relationship with fundamental hyperparameters remains underexplored. A significant knowledge gap exists concerning how the learning rate, a critical parameter governing the training process, directly influences the dynamics of these neural fluctuations. This study systematically investigates the impact of varying learning rates on the magnitude and character of weight and bias fluctuations within a neural network. We trained a model using distinct learning rates and analyzed the corresponding parameter fluctuations in conjunction with the network's final accuracy. Our findings aim to establish a clear link between the learning rate's value, the resulting fluctuation patterns, and overall model performance. By doing so, we provide deeper insights into the optimization process, shedding light on how the learning rate mediates the crucial exploration-exploitation trade-off during training. This work contributes to a more nuanced understanding of hyperparameter tuning and the underlying mechanics of deep learning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Perturbation-based Explanations by Understanding the Role of Uncertainty Calibration</title>
<link>https://arxiv.org/abs/2511.10439</link>
<guid>https://arxiv.org/abs/2511.10439</guid>
<content:encoded><![CDATA[
arXiv:2511.10439v1 Announce Type: new 
Abstract: Perturbation-based explanations are widely utilized to enhance the transparency of machine-learning models in practice. However, their reliability is often compromised by the unknown model behavior under the specific perturbations used. This paper investigates the relationship between uncertainty calibration - the alignment of model confidence with actual accuracy - and perturbation-based explanations. We show that models systematically produce unreliable probability estimates when subjected to explainability-specific perturbations and theoretically prove that this directly undermines global and local explanation quality. To address this, we introduce ReCalX, a novel approach to recalibrate models for improved explanations while preserving their original predictions. Empirical evaluations across diverse models and datasets demonstrate that ReCalX consistently reduces perturbation-specific miscalibration most effectively while enhancing explanation robustness and the identification of globally important input features.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrinsic Dimensionality as a Model-Free Measure of Class Imbalance</title>
<link>https://arxiv.org/abs/2511.10475</link>
<guid>https://arxiv.org/abs/2511.10475</guid>
<content:encoded><![CDATA[
arXiv:2511.10475v1 Announce Type: new 
Abstract: Imbalance in classification tasks is commonly quantified by the cardinalities of examples across classes. This, however, disregards the presence of redundant examples and inherent differences in the learning difficulties of classes. Alternatively, one can use complex measures such as training loss and uncertainty, which, however, depend on training a machine learning model. Our paper proposes using data Intrinsic Dimensionality (ID) as an easy-to-compute, model-free measure of imbalance that can be seamlessly incorporated into various imbalance mitigation methods. Our results across five different datasets with a diverse range of imbalance ratios show that ID consistently outperforms cardinality-based re-weighting and re-sampling techniques used in the literature. Moreover, we show that combining ID with cardinality can further improve performance. Code: https://github.com/cagries/IDIM.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Panda: Test-Time Adaptation with Negative Data Augmentation</title>
<link>https://arxiv.org/abs/2511.10481</link>
<guid>https://arxiv.org/abs/2511.10481</guid>
<content:encoded><![CDATA[
arXiv:2511.10481v1 Announce Type: new 
Abstract: Pretrained VLMs exhibit strong zero-shot classification capabilities, but their predictions degrade significantly under common image corruptions. To improve robustness, many test-time adaptation (TTA) methods adopt positive data augmentation (PDA), which generates multiple views of each test sample to reduce prediction variance. However, these methods suffer from two key limitations. First, it introduces considerable computational overhead due to the large number of augmentations required per image. Second, it fails to mitigate prediction bias, where the model tends to predict certain classes disproportionately under corruption, as PDA operates on corrupted inputs and typically does not remove the corruption itself. To address these challenges, we propose Panda, a novel TTA method based on negative data augmentation (NDA). Unlike positive augmentations that preserve object semantics, Panda generates negative augmentations by disrupting semantic content. It divides images into patches and randomly assembles them from a shared patch pool. These negatively augmented images retain corruption-specific features while discarding object-relevant signals. We then subtract the mean feature of these negative samples from the original image feature, effectively suppressing corruption-related components while preserving class-relevant information. This mitigates prediction bias under distribution shifts. Panda allows augmentation to be shared across samples within a batch, resulting in minimal computational overhead. Panda can be seamlessly integrated into existing test-time adaptation frameworks and substantially improve their robustness. Our experiments indicate that Panda delivers superior performance compared to PDA methods, and a wide range of TTA methods exhibit significantly enhanced performance when integrated with Panda. Our code is available at https://github.com/ruxideng/Panda .
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weak Relation Enforcement for Kinematic-Informed Long-Term Stock Prediction with Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2511.10494</link>
<guid>https://arxiv.org/abs/2511.10494</guid>
<content:encoded><![CDATA[
arXiv:2511.10494v1 Announce Type: new 
Abstract: We propose loss function week enforcement of the velocity relations between time-series points in the Kinematic-Informed artificial Neural Networks (KINN) for long-term stock prediction. Problems of the series volatility, Out-of-Distribution (OOD) test data, and outliers in training data are addressed by (Artificial Neural Networks) ANN's learning not only future points prediction but also by learning velocity relations between the points, such a way as avoiding unrealistic spurious predictions. The presented loss function penalizes not only errors between predictions and supervised label data, but also errors between the next point prediction and the previous point plus velocity prediction. The loss function is tested on the multiple popular and exotic AR ANN architectures, and around fifteen years of Dow Jones function demonstrated statistically meaningful improvement across the normalization-sensitive activation functions prone to spurious behaviour in the OOD data conditions. Results show that such architecture addresses the issue of the normalization in the auto-regressive models that break the data topology by weakly enforcing the data neighbourhood proximity (relation) preservation during the ANN transformation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Holonorm</title>
<link>https://arxiv.org/abs/2511.10504</link>
<guid>https://arxiv.org/abs/2511.10504</guid>
<content:encoded><![CDATA[
arXiv:2511.10504v1 Announce Type: new 
Abstract: Normalization is a key point in transformer training . In Dynamic Tanh (DyT), the author demonstrated that Tanh can be used as an alternative layer normalization (LN) and confirmed the effectiveness of the idea. But Tanh itself faces orthogonality, linearity and distortion problems. Due to that, his proposition cannot be reliable. So we propose a Holonorm (hn) which has residual connections and nonlinearity. Holonorm is suitable for replacing Tanh in the context of normalization. Although the HoloNorm expression could be similar to the softsign function in dimension one, softsign is a componentwise function which is not good for tensors and vectors of great dimension. Holonorm preserves the orthogonality, the direction, the invertibility of the signal. Holonorm is also a suitable metric, maps all vectors into the open unit ball. This prevents exploding activations and improves stability in deep Transformer models. In this work, we have meticulously examined the normalization in transformers and say that Holonorm, a generalized form of softsign function suited as a normalization function first.Second, defined between 0 and 1 hn serves as a percentage, and $1 - \text{Holonorm}$ is its complement, making it better understandable in evaluating a model.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maximizing Efficiency of Dataset Compression for Machine Learning Potentials With Information Theory</title>
<link>https://arxiv.org/abs/2511.10561</link>
<guid>https://arxiv.org/abs/2511.10561</guid>
<content:encoded><![CDATA[
arXiv:2511.10561v1 Announce Type: new 
Abstract: Machine learning interatomic potentials (MLIPs) balance high accuracy and lower costs compared to density functional theory calculations, but their performance often depends on the size and diversity of training datasets. Large datasets improve model accuracy and generalization but are computationally expensive to produce and train on, while smaller datasets risk discarding rare but important atomic environments and compromising MLIP accuracy/reliability. Here, we develop an information-theoretical framework to quantify the efficiency of dataset compression methods and propose an algorithm that maximizes this efficiency. By framing atomistic dataset compression as an instance of the minimum set cover (MSC) problem over atom-centered environments, our method identifies the smallest subset of structures that contains as much information as possible from the original dataset while pruning redundant information. The approach is extensively demonstrated on the GAP-20 and TM23 datasets, and validated on 64 varied datasets from the ColabFit repository. Across all cases, MSC consistently retains outliers, preserves dataset diversity, and reproduces the long-tail distributions of forces even at high compression rates, outperforming other subsampling methods. Furthermore, MLIPs trained on MSC-compressed datasets exhibit reduced error for out-of-distribution data even in low-data regimes. We explain these results using an outlier analysis and show that such quantitative conclusions could not be achieved with conventional dimensionality reduction methods. The algorithm is implemented in the open-source QUESTS package and can be used for several tasks in atomistic modeling, from data subsampling, outlier detection, and training improved MLIPs at a lower cost.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oya: Deep Learning for Accurate Global Precipitation Estimation</title>
<link>https://arxiv.org/abs/2511.10562</link>
<guid>https://arxiv.org/abs/2511.10562</guid>
<content:encoded><![CDATA[
arXiv:2511.10562v1 Announce Type: new 
Abstract: Accurate precipitation estimation is critical for hydrological applications, especially in the Global South where ground-based observation networks are sparse and forecasting skill is limited. Existing satellite-based precipitation products often rely on the longwave infrared channel alone or are calibrated with data that can introduce significant errors, particularly at sub-daily timescales. This study introduces Oya, a novel real-time precipitation retrieval algorithm utilizing the full spectrum of visible and infrared (VIS-IR) observations from geostationary (GEO) satellites. Oya employs a two-stage deep learning approach, combining two U-Net models: one for precipitation detection and another for quantitative precipitation estimation (QPE), to address the inherent data imbalance between rain and no-rain events. The models are trained using high-resolution GPM Combined Radar-Radiometer Algorithm (CORRA) v07 data as ground truth and pre-trained on IMERG-Final retrievals to enhance robustness and mitigate overfitting due to the limited temporal sampling of CORRA. By leveraging multiple GEO satellites, Oya achieves quasi-global coverage and demonstrates superior performance compared to existing competitive regional and global precipitation baselines, offering a promising pathway to improved precipitation monitoring and forecasting.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Layer Norm on Memorization and Generalization in Transformers</title>
<link>https://arxiv.org/abs/2511.10566</link>
<guid>https://arxiv.org/abs/2511.10566</guid>
<content:encoded><![CDATA[
arXiv:2511.10566v1 Announce Type: new 
Abstract: Layer Normalization (LayerNorm) is one of the fundamental components in transformers that stabilizes training and improves optimization. In recent times, Pre-LayerNorm transformers have become the preferred choice over Post-LayerNorm transformers due to their stable gradient flow. However, the impact of LayerNorm on learning and memorization across these architectures remains unclear. In this work, we investigate how LayerNorm influences memorization and learning for Pre- and Post-LayerNorm transformers. We identify that LayerNorm serves as a key factor for stable learning in Pre-LayerNorm transformers, while in Post-LayerNorm transformers, it impacts memorization. Our analysis reveals that eliminating LayerNorm parameters in Pre-LayerNorm models exacerbates memorization and destabilizes learning, while in Post-LayerNorm models, it effectively mitigates memorization by restoring genuine labels. We further precisely identify that early layers LayerNorm are the most critical over middle/later layers and their influence varies across Pre and Post LayerNorm models. We have validated it through 13 models across 6 Vision and Language datasets. These insights shed new light on the role of LayerNorm in shaping memorization and learning in transformers.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Belief Net: A Filter-Based Framework for Learning Hidden Markov Models from Observations</title>
<link>https://arxiv.org/abs/2511.10571</link>
<guid>https://arxiv.org/abs/2511.10571</guid>
<content:encoded><![CDATA[
arXiv:2511.10571v1 Announce Type: new 
Abstract: Hidden Markov Models (HMMs) are fundamental for modeling sequential data, yet learning their parameters from observations remains challenging. Classical methods like the Baum-Welch (EM) algorithm are computationally intensive and prone to local optima, while modern spectral algorithms offer provable guarantees but may produce probability outputs outside valid ranges. This work introduces Belief Net, a novel framework that learns HMM parameters through gradient-based optimization by formulating the HMM's forward filter as a structured neural network. Unlike black-box Transformer models, Belief Net's learnable weights are explicitly the logits of the initial distribution, transition matrix, and emission matrix, ensuring full interpretability. The model processes observation sequences using a decoder-only architecture and is trained end-to-end with standard autoregressive next-observation prediction loss. On synthetic HMM data, Belief Net achieves superior convergence speed compared to Baum-Welch, successfully recovering parameters in both undercomplete and overcomplete settings where spectral methods fail. Comparisons with Transformer-based models are also presented on real-world language data.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Emotionally Intelligent and Responsible Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10573</link>
<guid>https://arxiv.org/abs/2511.10573</guid>
<content:encoded><![CDATA[
arXiv:2511.10573v1 Announce Type: new 
Abstract: Personalized decision systems in healthcare and behavioral support often rely on static rule-based or engagement-maximizing heuristics that overlook users' emotional context and ethical constraints. Such approaches risk recommending insensitive or unsafe interventions, especially in domains involving serious mental illness, substance use disorders, or depression. To address this limitation, we propose a Responsible Reinforcement Learning (RRL) framework that integrates emotional and contextual understanding with ethical considerations into the sequential decision-making process. RRL formulates personalization as a Constrained Markov Decision Process (CMDP), where the agent optimizes engagement and adherence while ensuring emotional alignment and ethical safety. We introduce a multi-objective reward function that explicitly balances short-term behavioral engagement with long-term user well-being, and define an emotion-informed state representation that captures fluctuations in emotional readiness, affect, and risk. The proposed architecture can be instantiated with any RL algorithm (e.g., DQN, PPO) augmented with safety constraints or Lagrangian regularization. Conceptually, this framework operationalizes empathy and responsibility within machine learning policy optimization, bridging safe RL, affective computing and responsible AI. We discuss the implications of this approach for human-centric domains such as behavioral health, education, and digital therapeutics, and outline simulation-based validation paths for future empirical work. This paper aims to initiate a methodological conversation about ethically aligned reinforcement learning for emotionally aware and trustworthy personalization systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Unified Sparse Dictionary Learning with Learnable Top-K LISTA and FISTA Encoders</title>
<link>https://arxiv.org/abs/2511.10575</link>
<guid>https://arxiv.org/abs/2511.10575</guid>
<content:encoded><![CDATA[
arXiv:2511.10575v1 Announce Type: new 
Abstract: We present a semi-unified sparse dictionary learning framework that bridges the gap between classical sparse models and modern deep architectures. Specifically, the method integrates strict Top-$K$ LISTA and its convex FISTA-based variant (LISTAConv) into the discriminative LC-KSVD2 model, enabling co-evolution between the sparse encoder and the dictionary under supervised or unsupervised regimes. This unified design retains the interpretability of traditional sparse coding while benefiting from efficient, differentiable training.
  We further establish a PALM-style convergence analysis for the convex variant, ensuring theoretical stability under block alternation. Experimentally, our method achieves 95.6\% on CIFAR-10, 86.3\% on CIFAR-100, and 88.5\% on TinyImageNet with faster convergence and lower memory cost ($<$4GB GPU). The results confirm that the proposed LC-KSVD2 + LISTA/LISTAConv pipeline offers an interpretable and computationally efficient alternative for modern deep architectures.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Robustness Certification through the Convex Hull of $\ell_0$ Attacks</title>
<link>https://arxiv.org/abs/2511.10576</link>
<guid>https://arxiv.org/abs/2511.10576</guid>
<content:encoded><![CDATA[
arXiv:2511.10576v1 Announce Type: new 
Abstract: Few-pixel attacks mislead a classifier by modifying a few pixels of an image. Their perturbation space is an $\ell_0$-ball, which is not convex, unlike $\ell_p$-balls for $p\geq1$. However, existing local robustness verifiers typically scale by relying on linear bound propagation, which captures convex perturbation spaces. We show that the convex hull of an $\ell_0$-ball is the intersection of its bounding box and an asymmetrically scaled $\ell_1$-like polytope. The volumes of the convex hull and this polytope are nearly equal as the input dimension increases. We then show a linear bound propagation that precisely computes bounds over the convex hull and is significantly tighter than bound propagations over the bounding box or our $\ell_1$-like polytope. This bound propagation scales the state-of-the-art $\ell_0$ verifier on its most challenging robustness benchmarks by 1.24x-7.07x, with a geometric mean of 3.16.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretrained Joint Predictions for Scalable Batch Bayesian Optimization of Molecular Designs</title>
<link>https://arxiv.org/abs/2511.10590</link>
<guid>https://arxiv.org/abs/2511.10590</guid>
<content:encoded><![CDATA[
arXiv:2511.10590v1 Announce Type: new 
Abstract: Batched synthesis and testing of molecular designs is the key bottleneck of drug development. There has been great interest in leveraging biomolecular foundation models as surrogates to accelerate this process. In this work, we show how to obtain scalable probabilistic surrogates of binding affinity for use in Batch Bayesian Optimization (Batch BO). This demands parallel acquisition functions that hedge between designs and the ability to rapidly sample from a joint predictive density to approximate them. Through the framework of Epistemic Neural Networks (ENNs), we obtain scalable joint predictive distributions of binding affinity on top of representations taken from large structure-informed models. Key to this work is an investigation into the importance of prior networks in ENNs and how to pretrain them on synthetic data to improve downstream performance in Batch BO. Their utility is demonstrated by rediscovering known potent EGFR inhibitors on a semi-synthetic benchmark in up to 5x fewer iterations, as well as potent inhibitors from a real-world small-molecule library in up to 10x fewer iterations, offering a promising solution for large-scale drug discovery applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithm Design and Stronger Guarantees for the Improving Multi-Armed Bandits Problem</title>
<link>https://arxiv.org/abs/2511.10619</link>
<guid>https://arxiv.org/abs/2511.10619</guid>
<content:encoded><![CDATA[
arXiv:2511.10619v1 Announce Type: new 
Abstract: The improving multi-armed bandits problem is a formal model for allocating effort under uncertainty, motivated by scenarios such as investing research effort into new technologies, performing clinical trials, and hyperparameter selection from learning curves. Each pull of an arm provides reward that increases monotonically with diminishing returns. A growing line of work has designed algorithms for improving bandits, albeit with somewhat pessimistic worst-case guarantees. Indeed, strong lower bounds of $\Omega(k)$ and $\Omega(\sqrt{k})$ multiplicative approximation factors are known for both deterministic and randomized algorithms (respectively) relative to the optimal arm, where $k$ is the number of bandit arms. In this work, we propose two new parameterized families of bandit algorithms and bound the sample complexity of learning the near-optimal algorithm from each family using offline data. The first family we define includes the optimal randomized algorithm from prior work. We show that an appropriately chosen algorithm from this family can achieve stronger guarantees, with optimal dependence on $k$, when the arm reward curves satisfy additional properties related to the strength of concavity. Our second family contains algorithms that both guarantee best-arm identification on well-behaved instances and revert to worst case guarantees on poorly-behaved instances. Taking a statistical learning perspective on the bandit rewards optimization problem, we achieve stronger data-dependent guarantees without the need for actually verifying whether the assumptions are satisfied.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthTools: A Framework for Scaling Synthetic Tools for Agent Development</title>
<link>https://arxiv.org/abs/2511.09572</link>
<guid>https://arxiv.org/abs/2511.09572</guid>
<content:encoded><![CDATA[
arXiv:2511.09572v1 Announce Type: cross 
Abstract: AI agents increasingly rely on external tools to solve complex, long-horizon tasks. Advancing such agents requires reproducible evaluation and large-scale training in controllable, diverse, and realistic tool-use environments. However, real-world APIs are limited in availability, domain coverage, and stability, often requiring access keys and imposing rate limits, which render them impractical for stable evaluation or scalable training. To address these challenges, we introduce SynthTools, a flexible and scalable framework for generating synthetic tool ecosystems. Our framework consists of three core components: Tool Generation for automatic and scalable creation of diverse tools, Tool Simulation to emulate realistic tool behaviors, and Tool Audit to ensure correctness and consistency of tool simulation. To illustrate its scalability, we show that SynthTools can readily produce toolsets that span twice as many domains and twice as many tools per domain as prior work. Furthermore, the tool simulation and tool audit components demonstrate strong reliability, achieving $94\%$ and $99\%$ accuracy respectively. Finally, we construct downstream tasks from the generated tools that even state-of-the-art models struggle to complete. By enabling scalable, diverse, and reliable tool ecosystems, SynthTools provides a practical path toward large-scale training and stable evaluation of tool-use agents. Our code is available at https://github.com/namkoong-lab/SynthTools.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prostate-VarBench: A Benchmark with Interpretable TabNet Framework for Prostate Cancer Variant Classification</title>
<link>https://arxiv.org/abs/2511.09576</link>
<guid>https://arxiv.org/abs/2511.09576</guid>
<content:encoded><![CDATA[
arXiv:2511.09576v1 Announce Type: cross 
Abstract: Variants of Uncertain Significance (VUS) limit the clinical utility of prostate cancer genomics by delaying diagnosis and therapy when evidence for pathogenicity or benignity is incomplete. Progress is further limited by inconsistent annotations across sources and the absence of a prostate-specific benchmark for fair comparison. We introduce Prostate-VarBench, a curated pipeline for creating prostate-specific benchmarks that integrates COSMIC (somatic cancer mutations), ClinVar (expert-curated clinical variants), and TCGA-PRAD (prostate tumor genomics from The Cancer Genome Atlas) into a harmonized dataset of 193,278 variants supporting patient- or gene-aware splits to prevent data leakage. To ensure data integrity, we corrected a Variant Effect Predictor (VEP) issue that merged multiple transcript records, introducing ambiguity in clinical significance fields. We then standardized 56 interpretable features across eight clinically relevant tiers, including population frequency, variant type, and clinical context. AlphaMissense pathogenicity scores were incorporated to enhance missense variant classification and reduce VUS uncertainty. Building on this resource, we trained an interpretable TabNet model to classify variant pathogenicity, whose step-wise sparse masks provide per-case rationales consistent with molecular tumor board review practices. On the held-out test set, the model achieved 89.9% accuracy with balanced class metrics, and the VEP correction yields an 6.5% absolute reduction in VUS.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Siegel Neural Networks</title>
<link>https://arxiv.org/abs/2511.09577</link>
<guid>https://arxiv.org/abs/2511.09577</guid>
<content:encoded><![CDATA[
arXiv:2511.09577v1 Announce Type: cross 
Abstract: Riemannian symmetric spaces (RSS) such as hyperbolic spaces and symmetric positive definite (SPD) manifolds have become popular spaces for representation learning. In this paper, we propose a novel approach for building discriminative neural networks on Siegel spaces, a family of RSS that is largely unexplored in machine learning tasks. For classification applications, one focus of recent works is the construction of multiclass logistic regression (MLR) and fully-connected (FC) layers for hyperbolic and SPD neural networks. Here we show how to build such layers for Siegel neural networks. Our approach relies on the quotient structure of those spaces and the notation of vector-valued distance on RSS. We demonstrate the relevance of our approach on two applications, i.e., radar clutter classification and node classification. Our results successfully demonstrate state-of-the-art performance across all datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.09605</link>
<guid>https://arxiv.org/abs/2511.09605</guid>
<content:encoded><![CDATA[
arXiv:2511.09605v1 Announce Type: cross 
Abstract: The growing number of medical tomography examinations has necessitated the development of automated methods capable of extracting comprehensive imaging features to facilitate downstream tasks such as tumor characterization, while assisting physicians in managing their growing workload. However, 3D medical image classification remains a challenging task due to the complex spatial relationships and long-range dependencies inherent in volumetric data. Training models from scratch suffers from low data regimes, and the absence of 3D large-scale multimodal datasets has limited the development of 3D medical imaging foundation models. Recent studies, however, have highlighted the potential of 2D vision foundation models, originally trained on natural images, as powerful feature extractors for medical image analysis. Despite these advances, existing approaches that apply 2D models to 3D volumes via slice-based decomposition remain suboptimal. Conventional volume slicing strategies, which rely on canonical planes such as axial, sagittal, or coronal, may inadequately capture the spatial extent of target structures when these are misaligned with standardized viewing planes. Furthermore, existing slice-wise aggregation strategies rarely account for preserving the volumetric structure, resulting in a loss of spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysis of the TAIGA-HiSCORE Data Using the Latent Space of Autoencoders</title>
<link>https://arxiv.org/abs/2511.09655</link>
<guid>https://arxiv.org/abs/2511.09655</guid>
<content:encoded><![CDATA[
arXiv:2511.09655v1 Announce Type: cross 
Abstract: The aim of extensive air shower (EAS) analysis is to reconstruct the physical parameters of the primary particle that initiated the shower. The TAIGA experiment is a hybrid detector system that combines several imaging atmospheric Cherenkov telescopes (IACTs) and an array of non-imaging Cherenkov detectors (TAIGA-HiSCORE) for EAS detection. Because the signals recorded by different detector types differ in physical nature, the direct merging of data is unfeasible, which complicates multimodal analysis. Currently, to analyze data from the IACTs and TAIGA-HiSCORE, a set of auxiliary parameters specific to each detector type is calculated from the recorded signals. These parameters are chosen empirically, so there is no certainty that they retain all important information and are the best suited for the respective problems. We propose to use autoencoders (AE) for the analysis of TAIGA experimental data and replace the conventionally used auxiliary parameters with the parameters of the AE latent space. The advantage of the AE latent space parameters is that they preserve essential physics from experimental data without prior assumptions. This approach also holds potential for enabling seamless integration of heterogeneous IACT and HiSCORE data through a joint latent space. To reconstruct the parameters of the primary particle of the EAS from the latent space of the AE, a separate artificial neural network is used. In this paper, the proposed approach is used to reconstruct the energy of the EAS primary particles based on Monte Carlo simulation data for TAIGA-HiSCORE. The dependence of the energy determination accuracy on the dimensionality of the latent space is analyzed, and these results are also compared with the results obtained by the conventional technique. It is shown that when using the AE latent space, the energy of the primary particle is reconstructed with satisfactory accuracy.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lithological Controls on the Permeability of Geologic Faults: Surrogate Modeling and Sensitivity Analysis</title>
<link>https://arxiv.org/abs/2511.09674</link>
<guid>https://arxiv.org/abs/2511.09674</guid>
<content:encoded><![CDATA[
arXiv:2511.09674v1 Announce Type: cross 
Abstract: Fault zones exhibit complex and heterogeneous permeability structures influenced by stratigraphic, compositional, and structural factors, making them critical yet uncertain components in subsurface flow modeling. In this study, we investigate how lithological controls influence fault permeability using the PREDICT framework: a probabilistic workflow that couples stochastic fault geometry generation, physically constrained material placement, and flow-based upscaling. The flow-based upscaling step, however, is a very computationally expensive component of the workflow and presents a major bottleneck that makes global sensitivity analysis (GSA) intractable, as it requires millions of model evaluations. To overcome this challenge, we develop a neural network surrogate to emulate the flow-based upscaling step. This surrogate model dramatically reduces the computational cost while maintaining high accuracy, thereby making GSA feasible. The surrogate-model-enabled GSA reveals new insights into the effects of lithological controls on fault permeability. In addition to identifying dominant parameters and negligible ones, the analysis uncovers significant nonlinear interactions between parameters that cannot be captured by traditional local sensitivity methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PriVi: Towards A General-Purpose Video Model For Primate Behavior In The Wild</title>
<link>https://arxiv.org/abs/2511.09675</link>
<guid>https://arxiv.org/abs/2511.09675</guid>
<content:encoded><![CDATA[
arXiv:2511.09675v1 Announce Type: cross 
Abstract: Non-human primates are our closest living relatives, and analyzing their behavior is central to research in cognition, evolution, and conservation. Computer vision could greatly aid this research, but existing methods often rely on human-centric pretrained models and focus on single datasets, which limits generalization. We address this limitation by shifting from a model-centric to a data-centric approach and introduce PriVi, a large-scale primate-centric video pretraining dataset. PriVi contains 424 hours of curated video, combining 174 hours from behavioral research across 11 settings with 250 hours of diverse web-sourced footage, assembled through a scalable data curation pipeline. We pretrain V-JEPA on PriVi to learn primate-specific representations and evaluate it using a lightweight frozen classifier. Across four benchmark datasets, ChimpACT, BaboonLand, PanAf500, and ChimpBehave, our approach consistently outperforms prior work, including fully finetuned baselines, and scales favorably with fewer labels. These results demonstrate that primate-centric pretraining substantially improves data efficiency and generalization, making it a promising approach for low-label applications. Code, models, and the majority of the dataset will be made available.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression</title>
<link>https://arxiv.org/abs/2511.09702</link>
<guid>https://arxiv.org/abs/2511.09702</guid>
<content:encoded><![CDATA[
arXiv:2511.09702v1 Announce Type: cross 
Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Mineral Modeling: Continent-Scale Mineral Prospecting via Geospatial Infilling</title>
<link>https://arxiv.org/abs/2511.09722</link>
<guid>https://arxiv.org/abs/2511.09722</guid>
<content:encoded><![CDATA[
arXiv:2511.09722v1 Announce Type: cross 
Abstract: Minerals play a critical role in the advanced energy technologies necessary for decarbonization, but characterizing mineral deposits hidden underground remains costly and challenging. Inspired by recent progress in generative modeling, we develop a learning method which infers the locations of minerals by masking and infilling geospatial maps of resource availability. We demonstrate this technique using mineral data for the conterminous United States, and train performant models, with the best achieving Dice coefficients of $0.31 \pm 0.01$ and recalls of $0.22 \pm 0.02$ on test data at 1$\times$1 mi$^2$ spatial resolution. One major advantage of our approach is that it can easily incorporate auxiliary data sources for prediction which may be more abundant than mineral data. We highlight the capabilities of our model by adding input layers derived from geophysical sources, along with a nation-wide ground survey of soils originally intended for agronomic purposes. We find that employing such auxiliary features can improve inference performance, while also enabling model evaluation in regions with no recorded minerals.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Data Fusion Labeler (dFL): Challenges and Solutions to Data Harmonization, Labeling, and Provenance in Fusion Energy</title>
<link>https://arxiv.org/abs/2511.09725</link>
<guid>https://arxiv.org/abs/2511.09725</guid>
<content:encoded><![CDATA[
arXiv:2511.09725v1 Announce Type: cross 
Abstract: Fusion energy research increasingly depends on the ability to integrate heterogeneous, multimodal datasets from high-resolution diagnostics, control systems, and multiscale simulations. The sheer volume and complexity of these datasets demand the development of new tools capable of systematically harmonizing and extracting knowledge across diverse modalities. The Data Fusion Labeler (dFL) is introduced as a unified workflow instrument that performs uncertainty-aware data harmonization, schema-compliant data fusion, and provenance-rich manual and automated labeling at scale. By embedding alignment, normalization, and labeling within a reproducible, operator-order-aware framework, dFL reduces time-to-analysis by greater than 50X (e.g., enabling >200 shots/hour to be consistently labeled rather than a handful per day), enhances label (and subsequently training) quality, and enables cross-device comparability. Case studies from DIII-D demonstrate its application to automated ELM detection and confinement regime classification, illustrating its potential as a core component of data-driven discovery, model validation, and real-time control in future burning plasma devices.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Baby Sophia: A Developmental Approach to Self-Exploration through Self-Touch and Hand Regard</title>
<link>https://arxiv.org/abs/2511.09727</link>
<guid>https://arxiv.org/abs/2511.09727</guid>
<content:encoded><![CDATA[
arXiv:2511.09727v1 Announce Type: cross 
Abstract: Inspired by infant development, we propose a Reinforcement Learning (RL) framework for autonomous self-exploration in a robotic agent, Baby Sophia, using the BabyBench simulation environment. The agent learns self-touch and hand regard behaviors through intrinsic rewards that mimic an infant's curiosity-driven exploration of its own body. For self-touch, high-dimensional tactile inputs are transformed into compact, meaningful representations, enabling efficient learning. The agent then discovers new tactile contacts through intrinsic rewards and curriculum learning that encourage broad body coverage, balance, and generalization. For hand regard, visual features of the hands, such as skin-color and shape, are learned through motor babbling. Then, intrinsic rewards encourage the agent to perform novel hand motions, and follow its hands with its gaze. A curriculum learning setup from single-hand to dual-hand training allows the agent to reach complex visual-motor coordination. The results of this work demonstrate that purely curiosity-based signals, with no external supervision, can drive coordinated multimodal learning, imitating an infant's progression from random motor babbling to purposeful behaviors.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fourier-Based Global Denoising Model for Smart Artifacts Removing of Microscopy Images</title>
<link>https://arxiv.org/abs/2511.09734</link>
<guid>https://arxiv.org/abs/2511.09734</guid>
<content:encoded><![CDATA[
arXiv:2511.09734v1 Announce Type: cross 
Abstract: Microscopy such as Scanning Tunneling Microscopy (STM), Atomic Force Microscopy (AFM) and Scanning Electron Microscopy (SEM) are essential tools in material imaging at micro- and nanoscale resolutions to extract physical knowledge and materials structure-property relationships. However, tuning microscopy controls (e.g. scanning speed, current setpoint, tip bias etc.) to obtain a high-quality of images is a non-trivial and time-consuming effort. On the other hand, with sub-standard images, the key features are not accurately discovered due to noise and artifacts, leading to erroneous analysis. Existing denoising models mostly build on generalizing the weak signals as noises while the strong signals are enhanced as key features, which is not always the case in microscopy images, thus can completely erase a significant amount of hidden physical information. To address these limitations, we propose a global denoising model (GDM) to smartly remove artifacts of microscopy images while preserving weaker but physically important features. The proposed model is developed based on 1) first designing a two-imaging input channel of non-pair and goal specific pre-processed images with user-defined trade-off information between two channels and 2) then integrating a loss function of pixel- and fast Fourier-transformed (FFT) based on training the U-net model. We compared the proposed GDM with the non-FFT denoising model over STM-generated images of Copper(Cu) and Silicon(Si) materials, AFM-generated Pantoea sp.YR343 bio-film images and SEM-generated plastic degradation images. We believe this proposed workflow can be extended to improve other microscopy image quality and will benefit the experimentalists with the proposed design flexibility to smartly tune via domain-experts preferences.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the Applicability of Natural Language Processing to Traditional Social Science Methodology: A Case Study in Identifying Strategic Signaling Patterns in Presidential Directives</title>
<link>https://arxiv.org/abs/2511.09738</link>
<guid>https://arxiv.org/abs/2511.09738</guid>
<content:encoded><![CDATA[
arXiv:2511.09738v1 Announce Type: cross 
Abstract: Our research investigates how Natural Language Processing (NLP) can be used to extract main topics from a larger corpus of written data, as applied to the case of identifying signaling themes in Presidential Directives (PDs) from the Reagan through Clinton administrations. Analysts and NLP both identified relevant documents, demonstrating the potential utility of NLPs in research involving large written corpuses. However, we also identified discrepancies between NLP and human-labeled results that indicate a need for more research to assess the validity of NLP in this use case. The research was conducted in 2023, and the rapidly evolving landscape of AIML means existing tools have improved and new tools have been developed; this research displays the inherent capabilities of a potentially dated AI tool in emerging social science applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Guided Exploration of Generative Model's Latent Space for Controlled Iris Image Augmentations</title>
<link>https://arxiv.org/abs/2511.09749</link>
<guid>https://arxiv.org/abs/2511.09749</guid>
<content:encoded><![CDATA[
arXiv:2511.09749v1 Announce Type: cross 
Abstract: Developing reliable iris recognition and presentation attack detection methods requires diverse datasets that capture realistic variations in iris features and a wide spectrum of anomalies. Because of the rich texture of iris images, which spans a wide range of spatial frequencies, synthesizing same-identity iris images while controlling specific attributes remains challenging. In this work, we introduce a new iris image augmentation strategy by traversing a generative model's latent space toward latent codes that represent same-identity samples but with some desired iris image properties manipulated. The latent space traversal is guided by a gradient of specific geometrical, textural, or quality-related iris image features (e.g., sharpness, pupil size, iris size, or pupil-to-iris ratio) and preserves the identity represented by the image being manipulated. The proposed approach can be easily extended to manipulate any attribute for which a differentiable loss term can be formulated. Additionally, our approach can use either randomly generated images using either a pre-train GAN model or real-world iris images. We can utilize GAN inversion to project any given iris image into the latent space and obtain its corresponding latent code.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Brian Intensify: An Adaptive Machine Learning Framework for Auditory EEG Stimulation and Cognitive Enhancement in FXS</title>
<link>https://arxiv.org/abs/2511.09765</link>
<guid>https://arxiv.org/abs/2511.09765</guid>
<content:encoded><![CDATA[
arXiv:2511.09765v1 Announce Type: cross 
Abstract: Neurodevelopmental disorders such as Fragile X Syndrome (FXS) and Autism Spectrum Disorder (ASD) are characterized by disrupted cortical oscillatory activity, particularly in the alpha and gamma frequency bands. These abnormalities are linked to deficits in attention, sensory processing, and cognitive function. In this work, we present an adaptive machine learning-based brain-computer interface (BCI) system designed to modulate neural oscillations through frequency-specific auditory stimulation to enhance cognitive readiness in individuals with FXS. EEG data were recorded from 38 participants using a 128-channel system under a stimulation paradigm consisting of a 30-second baseline (no stimulus) followed by 60-second auditory entrainment episodes at 7Hz, 9Hz, 11Hz, and 13Hz. A comprehensive analysis of power spectral features (Alpha, Gamma, Delta, Theta, Beta) and cross-frequency coupling metrics (Alpha-Gamma, Alpha-Beta, etc.) was conducted. The results identified Peak Alpha Power, Peak Gamma Power, and Alpha Power per second per channel as the most discriminative biomarkers. The 13Hz stimulation condition consistently elicited a significant increase in Alpha activity and suppression of Gamma activity, aligning with our optimization objective. A supervised machine learning framework was developed to predict EEG responses and dynamically adjust stimulation parameters, enabling real-time, subject-specific adaptation. This work establishes a novel EEG-driven optimization framework for cognitive neuromodulation, providing a foundational model for next-generation AI-integrated BCI systems aimed at personalized neurorehabilitation in FXS and related disorders.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelos Empiricos de Pos-Dupla Selecao por LASSO: Discussoes para Estudos do Transporte Aereo</title>
<link>https://arxiv.org/abs/2511.09767</link>
<guid>https://arxiv.org/abs/2511.09767</guid>
<content:encoded><![CDATA[
arXiv:2511.09767v1 Announce Type: cross 
Abstract: This paper presents and discusses forms of estimation by regularized regression and model selection using the LASSO method - Least Absolute Shrinkage and Selection Operator. LASSO is recognized as one of the main supervised learning methods applied to high-dimensional econometrics, allowing work with large volumes of data and multiple correlated controls. Conceptual issues related to the consequences of high dimensionality in modern econometrics and the principle of sparsity, which underpins regularization procedures, are addressed. The study examines the main post-double selection and post-regularization models, including variations applied to instrumental variable models. A brief description of the lassopack routine package, its syntaxes, and examples of HD, HDS (High-Dimension Sparse), and IV-HDS models, with combinations involving fixed effects estimators, is also presented. Finally, the potential application of the approach in research focused on air transport is discussed, with emphasis on an empirical study on the operational efficiency of airlines and aircraft fuel consumption.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProbLog4Fairness: A Neurosymbolic Approach to Modeling and Mitigating Bias</title>
<link>https://arxiv.org/abs/2511.09768</link>
<guid>https://arxiv.org/abs/2511.09768</guid>
<content:encoded><![CDATA[
arXiv:2511.09768v1 Announce Type: cross 
Abstract: Operationalizing definitions of fairness is difficult in practice, as multiple definitions can be incompatible while each being arguably desirable. Instead, it may be easier to directly describe algorithmic bias through ad-hoc assumptions specific to a particular real-world task, e.g., based on background information on systemic biases in its context. Such assumptions can, in turn, be used to mitigate this bias during training. Yet, a framework for incorporating such assumptions that is simultaneously principled, flexible, and interpretable is currently lacking.
  Our approach is to formalize bias assumptions as programs in ProbLog, a probabilistic logic programming language that allows for the description of probabilistic causal relationships through logic. Neurosymbolic extensions of ProbLog then allow for easy integration of these assumptions in a neural network's training process. We propose a set of templates to express different types of bias and show the versatility of our approach on synthetic tabular datasets with known biases. Using estimates of the bias distortions present, we also succeed in mitigating algorithmic bias in real-world tabular and image data. We conclude that ProbLog4Fairness outperforms baselines due to its ability to flexibly model the relevant bias assumptions, where other methods typically uphold a fixed bias type or notion of fairness.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetry aware Reynolds Averaged Navier Stokes turbulence models with equivariant neural networks</title>
<link>https://arxiv.org/abs/2511.09769</link>
<guid>https://arxiv.org/abs/2511.09769</guid>
<content:encoded><![CDATA[
arXiv:2511.09769v1 Announce Type: cross 
Abstract: Accurate and generalizable Reynolds-averaged Navier-Stokes (RANS) models for turbulent flows rely on effective closures. We introduce tensor-based, symmetry aware closures using equivariant neural networks (ENNs) and present an algorithm for enforcing algebraic contraction relations among tensor components. The modeling approach builds on the structure tensor framework introduced by Kassinos and Reynolds to learn closures in the rapid distortion theory setting. Experiments show that ENNs can effectively learn relationships involving high-order tensors, meeting or exceeding the performance of existing models in tasks such as predicting the rapid pressure-strain correlation. Our results show that ENNs provide a physically consistent alternative to classical tensor basis models, enabling end-to-end learning of unclosed terms in RANS and fast exploration of model dependencies.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Explainable AIoT Application via SHAP Entropy Regularization</title>
<link>https://arxiv.org/abs/2511.09775</link>
<guid>https://arxiv.org/abs/2511.09775</guid>
<content:encoded><![CDATA[
arXiv:2511.09775v1 Announce Type: cross 
Abstract: The widespread integration of Artificial Intelligence of Things (AIoT) in smart home environments has amplified the demand for transparent and interpretable machine learning models. To foster user trust and comply with emerging regulatory frameworks, the Explainable AI (XAI) methods, particularly post-hoc techniques such as SHapley Additive exPlanations (SHAP), and Local Interpretable Model-Agnostic Explanations (LIME), are widely employed to elucidate model behavior. However, recent studies have shown that these explanation methods can inadvertently expose sensitive user attributes and behavioral patterns, thereby introducing new privacy risks. To address these concerns, we propose a novel privacy-preserving approach based on SHAP entropy regularization to mitigate privacy leakage in explainable AIoT applications. Our method incorporates an entropy-based regularization objective that penalizes low-entropy SHAP attribution distributions during training, promoting a more uniform spread of feature contributions. To evaluate the effectiveness of our approach, we developed a suite of SHAP-based privacy attacks that strategically leverage model explanation outputs to infer sensitive information. We validate our method through comparative evaluations using these attacks alongside utility metrics on benchmark smart home energy consumption datasets. Experimental results demonstrate that SHAP entropy regularization substantially reduces privacy leakage compared to baseline models, while maintaining high predictive accuracy and faithful explanation fidelity. This work contributes to the development of privacy-preserving explainable AI techniques for secure and trustworthy AIoT applications.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Robust Task-Level Control Architecture for Learned Dynamical Systems</title>
<link>https://arxiv.org/abs/2511.09790</link>
<guid>https://arxiv.org/abs/2511.09790</guid>
<content:encoded><![CDATA[
arXiv:2511.09790v1 Announce Type: cross 
Abstract: Dynamical system (DS)-based learning from demonstration (LfD) is a powerful tool for generating motion plans in the operation (`task') space of robotic systems. However, the realization of the generated motion plans is often compromised by a ''task-execution mismatch'', where unmodeled dynamics, persistent disturbances, and system latency cause the robot's actual task-space state to diverge from the desired motion trajectory. We propose a novel task-level robust control architecture, L1-augmented Dynamical Systems (L1-DS), that explicitly handles the task-execution mismatch in tracking a nominal motion plan generated by any DS-based LfD scheme. Our framework augments any DS-based LfD model with a nominal stabilizing controller and an L1 adaptive controller. Furthermore, we introduce a windowed Dynamic Time Warping (DTW)-based target selector, which enables the nominal stabilizing controller to handle temporal misalignment for improved phase-consistent tracking. We demonstrate the efficacy of our architecture on the LASA and IROS handwriting datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized infinite dimensional Alpha-Procrustes based geometries</title>
<link>https://arxiv.org/abs/2511.09801</link>
<guid>https://arxiv.org/abs/2511.09801</guid>
<content:encoded><![CDATA[
arXiv:2511.09801v1 Announce Type: cross 
Abstract: This work extends the recently introduced Alpha-Procrustes family of Riemannian metrics for symmetric positive definite (SPD) matrices by incorporating generalized versions of the Bures-Wasserstein (GBW), Log-Euclidean, and Wasserstein distances. While the Alpha-Procrustes framework has unified many classical metrics in both finite- and infinite- dimensional settings, it previously lacked the structural components necessary to realize these generalized forms. We introduce a formalism based on unitized Hilbert-Schmidt operators and an extended Mahalanobis norm that allows the construction of robust, infinite-dimensional generalizations of GBW and Log-Hilbert-Schmidt distances. Our approach also incorporates a learnable regularization parameter that enhances geometric stability in high-dimensional comparisons. Preliminary experiments reproducing benchmarks from the literature demonstrate the improved performance of our generalized metrics, particularly in scenarios involving comparisons between datasets of varying dimension and scale. This work lays a theoretical and computational foundation for advancing robust geometric methods in machine learning, statistical inference, and functional data analysis.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.09809</link>
<guid>https://arxiv.org/abs/2511.09809</guid>
<content:encoded><![CDATA[
arXiv:2511.09809v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HierRouter: Coordinated Routing of Specialized Large Language Models via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.09873</link>
<guid>https://arxiv.org/abs/2511.09873</guid>
<content:encoded><![CDATA[
arXiv:2511.09873v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) deliver state-of-the-art performance across many tasks but impose high computational and memory costs, limiting their deployment in resource-constrained or real-time settings. To address this, we propose HierRouter, a hierarchical routing approach that dynamically assembles inference pipelines from a pool of specialized, lightweight language models. Formulated as a finite-horizon Markov Decision Process (MDP), our approach trains a Proximal Policy Optimization (PPO)-based reinforcement learning agent to iteratively select which models to invoke at each stage of multi-hop inference. The agent conditions on the evolving context and accumulated cost to make context-aware routing decisions. Experiments with three open-source candidate LLMs across six benchmarks, including QA, code generation, and mathematical reasoning, show that HierRouter improves response quality by up to 2.4x compared to using individual models independently, while incurring only a minimal additional inference cost on average. These results highlight the promise of hierarchical routing for cost-efficient, high-performance LLM inference. All codes can be found here https://github.com/ Nikunj-Gupta/hierouter.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EgoEMS: A High-Fidelity Multimodal Egocentric Dataset for Cognitive Assistance in Emergency Medical Services</title>
<link>https://arxiv.org/abs/2511.09894</link>
<guid>https://arxiv.org/abs/2511.09894</guid>
<content:encoded><![CDATA[
arXiv:2511.09894v1 Announce Type: cross 
Abstract: Emergency Medical Services (EMS) are critical to patient survival in emergencies, but first responders often face intense cognitive demands in high-stakes situations. AI cognitive assistants, acting as virtual partners, have the potential to ease this burden by supporting real-time data collection and decision making. In pursuit of this vision, we introduce EgoEMS, the first end-to-end, high-fidelity, multimodal, multiperson dataset capturing over 20 hours of realistic, procedural EMS activities from an egocentric view in 233 simulated emergency scenarios performed by 62 participants, including 46 EMS professionals. Developed in collaboration with EMS experts and aligned with national standards, EgoEMS is captured using an open-source, low-cost, and replicable data collection system and is annotated with keysteps, timestamped audio transcripts with speaker diarization, action quality metrics, and bounding boxes with segmentation masks. Emphasizing realism, the dataset includes responder-patient interactions reflecting real-world emergency dynamics. We also present a suite of benchmarks for real-time multimodal keystep recognition and action quality estimation, essential for developing AI support tools for EMS. We hope EgoEMS inspires the research community to push the boundaries of intelligent EMS systems and ultimately contribute to improved patient outcomes.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theory and computation for structured variational inference</title>
<link>https://arxiv.org/abs/2511.09897</link>
<guid>https://arxiv.org/abs/2511.09897</guid>
<content:encoded><![CDATA[
arXiv:2511.09897v1 Announce Type: cross 
Abstract: Structured variational inference constitutes a core methodology in modern statistical applications. Unlike mean-field variational inference, the approximate posterior is assumed to have interdependent structure. We consider the natural setting of star-structured variational inference, where a root variable impacts all the other ones. We prove the first results for existence, uniqueness, and self-consistency of the variational approximation. In turn, we derive quantitative approximation error bounds for the variational approximation to the posterior, extending prior work from the mean-field setting to the star-structured setting. We also develop a gradient-based algorithm with provable guarantees for computing the variational approximation using ideas from optimal transport theory. We explore the implications of our results for Gaussian measures and hierarchical Bayesian models, including generalized linear models with location family priors and spike-and-slab priors with one-dimensional debiasing. As a by-product of our analysis, we develop new stability results for star-separable transport maps which might be of independent interest.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond empirical models: Discovering new constitutive laws in solids with graph-based equation discovery</title>
<link>https://arxiv.org/abs/2511.09906</link>
<guid>https://arxiv.org/abs/2511.09906</guid>
<content:encoded><![CDATA[
arXiv:2511.09906v1 Announce Type: cross 
Abstract: Constitutive models are fundamental to solid mechanics and materials science, underpinning the quantitative description and prediction of material responses under diverse loading conditions. Traditional phenomenological models, which are derived through empirical fitting, often lack generalizability and rely heavily on expert intuition and predefined functional forms. In this work, we propose a graph-based equation discovery framework for the automated discovery of constitutive laws directly from multisource experimental data. This framework expresses equations as directed graphs, where nodes represent operators and variables, edges denote computational relations, and edge features encode parametric dependencies. This enables the generation and optimization of free-form symbolic expressions with undetermined material-specific parameters. Through the proposed framework, we have discovered new constitutive models for strain-rate effects in alloy steel materials and the deformation behavior of lithium metal. Compared with conventional empirical models, these new models exhibit compact analytical structures and achieve higher accuracy. The proposed graph-based equation discovery framework provides a generalizable and interpretable approach for data-driven scientific modelling, particularly in contexts where traditional empirical formulations are inadequate for representing complex physical phenomena.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence of Four-Layer Matrix Factorization under Random Initialization</title>
<link>https://arxiv.org/abs/2511.09925</link>
<guid>https://arxiv.org/abs/2511.09925</guid>
<content:encoded><![CDATA[
arXiv:2511.09925v1 Announce Type: cross 
Abstract: Gradient descent dynamics on the deep matrix factorization problem is extensively studied as a simplified theoretical model for deep neural networks. Although the convergence theory for two-layer matrix factorization is well-established, no global convergence guarantee for general deep matrix factorization under random initialization has been established to date. To address this gap, we provide a polynomial-time global convergence guarantee for randomly initialized gradient descent on four-layer matrix factorization, given certain conditions on the target matrix and a standard balanced regularization term. Our analysis employs new techniques to show saddle-avoidance properties of gradient decent dynamics, and extends previous theories to characterize the change in eigenvalues of layer weights.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptViG: Adaptive Vision GNN with Exponential Decay Gating</title>
<link>https://arxiv.org/abs/2511.09942</link>
<guid>https://arxiv.org/abs/2511.09942</guid>
<content:encoded><![CDATA[
arXiv:2511.09942v1 Announce Type: cross 
Abstract: Vision Graph Neural Networks (ViGs) offer a new direction for advancements in vision architectures. While powerful, ViGs often face substantial computational challenges stemming from their graph construction phase, which can hinder their efficiency. To address this issue we propose AdaptViG, an efficient and powerful hybrid Vision GNN that introduces a novel graph construction mechanism called Adaptive Graph Convolution. This mechanism builds upon a highly efficient static axial scaffold and a dynamic, content-aware gating strategy called Exponential Decay Gating. This gating mechanism selectively weighs long-range connections based on feature similarity. Furthermore, AdaptViG employs a hybrid strategy, utilizing our efficient gating mechanism in the early stages and a full Global Attention block in the final stage for maximum feature aggregation. Our method achieves a new state-of-the-art trade-off between accuracy and efficiency among Vision GNNs. For instance, our AdaptViG-M achieves 82.6% top-1 accuracy, outperforming ViG-B by 0.3% while using 80% fewer parameters and 84% fewer GMACs. On downstream tasks, AdaptViG-M obtains 45.8 mIoU, 44.8 APbox, and 41.1 APmask, surpassing the much larger EfficientFormer-L7 by 0.7 mIoU, 2.2 APbox, and 2.1 APmask, respectively, with 78% fewer parameters.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Role of Advanced Computer Architectures in Accelerating Artificial Intelligence Workloads</title>
<link>https://arxiv.org/abs/2511.10010</link>
<guid>https://arxiv.org/abs/2511.10010</guid>
<content:encoded><![CDATA[
arXiv:2511.10010v1 Announce Type: cross 
Abstract: The remarkable progress in Artificial Intelligence (AI) is foundation-ally linked to a concurrent revolution in computer architecture. As AI models, particularly Deep Neural Networks (DNNs), have grown in complexity, their massive computational demands have pushed traditional architectures to their limits. This paper provides a structured review of this co-evolution, analyzing the architectural landscape designed to accelerate modern AI workloads. We explore the dominant architectural paradigms Graphics Processing Units (GPUs), Appli-cation-Specific Integrated Circuits (ASICs), and Field-Programmable Gate Ar-rays (FPGAs) by breaking down their design philosophies, key features, and per-formance trade-offs. The core principles essential for performance and energy efficiency, including dataflow optimization, advanced memory hierarchies, spar-sity, and quantization, are analyzed. Furthermore, this paper looks ahead to emerging technologies such as Processing-in-Memory (PIM) and neuromorphic computing, which may redefine future computation. By synthesizing architec-tural principles with quantitative performance data from industry-standard benchmarks, this survey presents a comprehensive picture of the AI accelerator landscape. We conclude that AI and computer architecture are in a symbiotic relationship, where hardware-software co-design is no longer an optimization but a necessity for future progress in computing.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent In-context Coordination via Decentralized Memory Retrieval</title>
<link>https://arxiv.org/abs/2511.10030</link>
<guid>https://arxiv.org/abs/2511.10030</guid>
<content:encoded><![CDATA[
arXiv:2511.10030v1 Announce Type: cross 
Abstract: Large transformer models, trained on diverse datasets, have demonstrated impressive few-shot performance on previously unseen tasks without requiring parameter updates. This capability has also been explored in Reinforcement Learning (RL), where agents interact with the environment to retrieve context and maximize cumulative rewards, showcasing strong adaptability in complex settings. However, in cooperative Multi-Agent Reinforcement Learning (MARL), where agents must coordinate toward a shared goal, decentralized policy deployment can lead to mismatches in task alignment and reward assignment, limiting the efficiency of policy adaptation. To address this challenge, we introduce Multi-agent In-context Coordination via Decentralized Memory Retrieval (MAICC), a novel approach designed to enhance coordination by fast adaptation. Our method involves training a centralized embedding model to capture fine-grained trajectory representations, followed by decentralized models that approximate the centralized one to obtain team-level task information. Based on the learned embeddings, relevant trajectories are retrieved as context, which, combined with the agents' current sub-trajectories, inform decision-making. During decentralized execution, we introduce a novel memory mechanism that effectively balances test-time online data with offline memory. Based on the constructed memory, we propose a hybrid utility score that incorporates both individual- and team-level returns, ensuring credit assignment across agents. Extensive experiments on cooperative MARL benchmarks, including Level-Based Foraging (LBF) and SMAC (v1/v2), show that MAICC enables faster adaptation to unseen tasks compared to existing methods. Code is available at https://github.com/LAMDA-RL/MAICC.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed Machine Learning for Static Friction Modeling in Robotic Manipulators Based on Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2511.10079</link>
<guid>https://arxiv.org/abs/2511.10079</guid>
<content:encoded><![CDATA[
arXiv:2511.10079v1 Announce Type: cross 
Abstract: Friction modeling plays a crucial role in achieving high-precision motion control in robotic operating systems. Traditional static friction models (such as the Stribeck model) are widely used due to their simple forms; however, they typically require predefined functional assumptions, which poses significant challenges when dealing with unknown functional structures. To address this issue, this paper proposes a physics-inspired machine learning approach based on the Kolmogorov Arnold Network (KAN) for static friction modeling of robotic joints. The method integrates spline activation functions with a symbolic regression mechanism, enabling model simplification and physical expression extraction through pruning and attribute scoring, while maintaining both high prediction accuracy and interpretability. We first validate the method's capability to accurately identify key parameters under known functional models, and further demonstrate its robustness and generalization ability under conditions with unknown functional structures and noisy data. Experiments conducted on both synthetic data and real friction data collected from a six-degree-of-freedom industrial manipulator show that the proposed method achieves a coefficient of determination greater than 0.95 across various tasks and successfully extracts concise and physically meaningful friction expressions. This study provides a new perspective for interpretable and data-driven robotic friction modeling with promising engineering applicability.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Towards Unified Expressive Policy Optimization for Robust Robot Learning</title>
<link>https://arxiv.org/abs/2511.10087</link>
<guid>https://arxiv.org/abs/2511.10087</guid>
<content:encoded><![CDATA[
arXiv:2511.10087v1 Announce Type: cross 
Abstract: Offline-to-online reinforcement learning (O2O-RL) has emerged as a promising paradigm for safe and efficient robotic policy deployment but suffers from two fundamental challenges: limited coverage of multimodal behaviors and distributional shifts during online adaptation. We propose UEPO, a unified generative framework inspired by large language model pretraining and fine-tuning strategies. Our contributions are threefold: (1) a multi-seed dynamics-aware diffusion policy that efficiently captures diverse modalities without training multiple models; (2) a dynamic divergence regularization mechanism that enforces physically meaningful policy diversity; and (3) a diffusion-based data augmentation module that enhances dynamics model generalization. On the D4RL benchmark, UEPO achieves +5.9\% absolute improvement over Uni-O4 on locomotion tasks and +12.4\% on dexterous manipulation, demonstrating strong generalization and scalability.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing to Unseen Disaster Events: A Causal View</title>
<link>https://arxiv.org/abs/2511.10120</link>
<guid>https://arxiv.org/abs/2511.10120</guid>
<content:encoded><![CDATA[
arXiv:2511.10120v1 Announce Type: cross 
Abstract: Due to the rapid growth of social media platforms, these tools have become essential for monitoring information during ongoing disaster events. However, extracting valuable insights requires real-time processing of vast amounts of data. A major challenge in existing systems is their exposure to event-related biases, which negatively affects their ability to generalize to emerging events. While recent advancements in debiasing and causal learning offer promising solutions, they remain underexplored in the disaster event domain. In this work, we approach bias mitigation through a causal lens and propose a method to reduce event- and domain-related biases, enhancing generalization to future events. Our approach outperforms multiple baselines by up to +1.9% F1 and significantly improves a PLM-based classifier across three disaster classification tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DenoGrad: Deep Gradient Denoising Framework for Enhancing the Performance of Interpretable AI Models</title>
<link>https://arxiv.org/abs/2511.10161</link>
<guid>https://arxiv.org/abs/2511.10161</guid>
<content:encoded><![CDATA[
arXiv:2511.10161v1 Announce Type: cross 
Abstract: The performance of Machine Learning (ML) models, particularly those operating within the Interpretable Artificial Intelligence (Interpretable AI) framework, is significantly affected by the presence of noise in both training and production data. Denoising has therefore become a critical preprocessing step, typically categorized into instance removal and instance correction techniques. However, existing correction approaches often degrade performance or oversimplify the problem by altering the original data distribution. This leads to unrealistic scenarios and biased models, which is particularly problematic in contexts where interpretable AI models are employed, as their interpretability depends on the fidelity of the underlying data patterns. In this paper, we argue that defining noise independently of the solution may be ineffective, as its nature can vary significantly across tasks and datasets. Using a task-specific high quality solution as a reference can provide a more precise and adaptable noise definition. To this end, we propose DenoGrad, a novel Gradient-based instance Denoiser framework that leverages gradients from an accurate Deep Learning (DL) model trained on the target data -- regardless of the specific task -- to detect and adjust noisy samples. Unlike conventional approaches, DenoGrad dynamically corrects noisy instances, preserving problem's data distribution, and improving AI models robustness. DenoGrad is validated on both tabular and time series datasets under various noise settings against the state-of-the-art. DenoGrad outperforms existing denoising strategies, enhancing the performance of interpretable IA models while standing out as the only high quality approach that preserves the original data distribution.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Synthetic and Real Routing Problems via LLM-Guided Instance Generation and Progressive Adaptation</title>
<link>https://arxiv.org/abs/2511.10233</link>
<guid>https://arxiv.org/abs/2511.10233</guid>
<content:encoded><![CDATA[
arXiv:2511.10233v1 Announce Type: cross 
Abstract: Recent advances in Neural Combinatorial Optimization (NCO) methods have significantly improved the capability of neural solvers to handle synthetic routing instances. Nonetheless, existing neural solvers typically struggle to generalize effectively from synthetic, uniformly-distributed training data to real-world VRP scenarios, including widely recognized benchmark instances from TSPLib and CVRPLib. To bridge this generalization gap, we present Evolutionary Realistic Instance Synthesis (EvoReal), which leverages an evolutionary module guided by large language models (LLMs) to generate synthetic instances characterized by diverse and realistic structural patterns. Specifically, the evolutionary module produces synthetic instances whose structural attributes statistically mimics those observed in authentic real-world instances. Subsequently, pre-trained NCO models are progressively refined, firstly aligning them with these structurally enriched synthetic distributions and then further adapting them through direct fine-tuning on actual benchmark instances. Extensive experimental evaluations demonstrate that EvoReal markedly improves the generalization capabilities of state-of-the-art neural solvers, yielding a notable reduced performance gap compared to the optimal solutions on the TSPLib (1.05%) and CVRPLib (2.71%) benchmarks across a broad spectrum of problem scales.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Model-Based Reinforcement Learning for Sample-Efficient IoT Channel Access</title>
<link>https://arxiv.org/abs/2511.10291</link>
<guid>https://arxiv.org/abs/2511.10291</guid>
<content:encoded><![CDATA[
arXiv:2511.10291v1 Announce Type: cross 
Abstract: Despite the advantages of multi-agent reinforcement learning (MARL) for wireless use case such as medium access control (MAC), their real-world deployment in Internet of Things (IoT) is hindered by their sample inefficiency. To alleviate this challenge, one can leverage model-based reinforcement learning (MBRL) solutions, however, conventional MBRL approaches rely on black-box models that are not interpretable and cannot reason. In contrast, in this paper, a novel causal model-based MARL framework is developed by leveraging tools from causal learn- ing. In particular, the proposed model can explicitly represent causal dependencies between network variables using structural causal models (SCMs) and attention-based inference networks. Interpretable causal models are then developed to capture how MAC control messages influence observations, how transmission actions determine outcomes, and how channel observations affect rewards. Data augmentation techniques are then used to generate synthetic rollouts using the learned causal model for policy optimization via proximal policy optimization (PPO). Analytical results demonstrate exponential sample complexity gains of causal MBRL over black-box approaches. Extensive simulations demonstrate that, on average, the proposed approach can reduce environment interactions by 58%, and yield faster convergence compared to model-free baselines. The proposed approach inherently is also shown to provide interpretable scheduling decisions via attention-based causal attribution, revealing which network conditions drive the policy. The resulting combination of sample efficiency and interpretability establishes causal MBRL as a practical approach for resource-constrained wireless systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fault Detection in Solar Thermal Systems using Probabilistic Reconstructions</title>
<link>https://arxiv.org/abs/2511.10296</link>
<guid>https://arxiv.org/abs/2511.10296</guid>
<content:encoded><![CDATA[
arXiv:2511.10296v1 Announce Type: cross 
Abstract: Solar thermal systems (STS) present a promising avenue for low-carbon heat generation, with a well-running system providing heat at minimal cost and carbon emissions. However, STS can exhibit faults due to improper installation, maintenance, or operation, often resulting in a substantial reduction in efficiency or even damage to the system. As monitoring at the individual level is economically prohibitive for small-scale systems, automated monitoring and fault detection should be used to address such issues. Recent advances in data-driven anomaly detection, particularly in time series analysis, offer a cost-effective solution by leveraging existing sensors to identify abnormal system states.
  Here, we propose a probabilistic reconstruction-based framework for anomaly detection. We evaluate our method on the publicly available PaSTS dataset of operational domestic STS, which features real-world complexities and diverse fault types. Our experiments show that reconstruction-based methods can detect faults in domestic STS both qualitatively and quantitatively, while generalizing to previously unseen systems. We also demonstrate that our model outperforms both simple and more complex deep learning baselines. Additionally, we show that heteroscedastic uncertainty estimation is essential to fault detection performance. Finally, we discuss the engineering overhead required to unlock these improvements and make a case for simple deep learning models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Evaluation of Deep Neural Networks for Pedestrian Detection</title>
<link>https://arxiv.org/abs/2511.10308</link>
<guid>https://arxiv.org/abs/2511.10308</guid>
<content:encoded><![CDATA[
arXiv:2511.10308v1 Announce Type: cross 
Abstract: Reliable pedestrian detection represents a crucial step towards automated driving systems. However, the current performance benchmarks exhibit weaknesses. The currently applied metrics for various subsets of a validation dataset prohibit a realistic performance evaluation of a DNN for pedestrian detection. As image segmentation supplies fine-grained information about a street scene, it can serve as a starting point to automatically distinguish between different types of errors during the evaluation of a pedestrian detector. In this work, eight different error categories for pedestrian detection are proposed and new metrics are proposed for performance comparison along these error categories. We use the new metrics to compare various backbones for a simplified version of the APD, and show a more fine-grained and robust way to compare models with each other especially in terms of safety-critical performance. We achieve SOTA on CityPersons-reasonable (without extra training data) by using a rather simple architecture.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SHRUG-FM: Reliability-Aware Foundation Models for Earth Observation</title>
<link>https://arxiv.org/abs/2511.10370</link>
<guid>https://arxiv.org/abs/2511.10370</guid>
<content:encoded><![CDATA[
arXiv:2511.10370v1 Announce Type: cross 
Abstract: Geospatial foundation models for Earth observation often fail to perform reliably in environments underrepresented during pretraining. We introduce SHRUG-FM, a framework for reliability-aware prediction that integrates three complementary signals: out-of-distribution (OOD) detection in the input space, OOD detection in the embedding space and task-specific predictive uncertainty. Applied to burn scar segmentation, SHRUG-FM shows that OOD scores correlate with lower performance in specific environmental conditions, while uncertainty-based flags help discard many poorly performing predictions. Linking these flags to land cover attributes from HydroATLAS shows that failures are not random but concentrated in certain geographies, such as low-elevation zones and large river areas, likely due to underrepresentation in pretraining data. SHRUG-FM provides a pathway toward safer and more interpretable deployment of GFMs in climate-sensitive applications, helping bridge the gap between benchmark performance and real-world reliability.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator Models for Continuous-Time Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.10383</link>
<guid>https://arxiv.org/abs/2511.10383</guid>
<content:encoded><![CDATA[
arXiv:2511.10383v1 Announce Type: cross 
Abstract: Continuous-time stochastic processes underlie many natural and engineered systems. In healthcare, autonomous driving, and industrial control, direct interaction with the environment is often unsafe or impractical, motivating offline reinforcement learning from historical data. However, there is limited statistical understanding of the approximation errors inherent in learning policies from offline datasets. We address this by linking reinforcement learning to the Hamilton-Jacobi-Bellman equation and proposing an operator-theoretic algorithm based on a simple dynamic programming recursion. Specifically, we represent our world model in terms of the infinitesimal generator of controlled diffusion processes learned in a reproducing kernel Hilbert space. By integrating statistical learning methods and operator theory, we establish global convergence of the value function and derive finite-sample guarantees with bounds tied to system properties such as smoothness and stability. Our theoretical and numerical results indicate that operator-based approaches may hold promise in solving offline reinforcement learning using continuous-time optimal control.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics informed Transformer-VAE for biophysical parameter estimation: PROSAIL model inversion in Sentinel-2 imagery</title>
<link>https://arxiv.org/abs/2511.10387</link>
<guid>https://arxiv.org/abs/2511.10387</guid>
<content:encoded><![CDATA[
arXiv:2511.10387v1 Announce Type: cross 
Abstract: Accurate retrieval of vegetation biophysical variables from satellite imagery is crucial for ecosystem monitoring and agricultural management. In this work, we propose a physics-informed Transformer-VAE architecture to invert the PROSAIL radiative transfer model for simultaneous estimation of key canopy parameters from Sentinel-2 data. Unlike previous hybrid approaches that require real satellite images for self-supevised training. Our model is trained exclusively on simulated data, yet achieves performance on par with state-of-the-art methods that utilize real imagery. The Transformer-VAE incorporates the PROSAIL model as a differentiable physical decoder, ensuring that inferred latent variables correspond to physically plausible leaf and canopy properties. We demonstrate retrieval of leaf area index (LAI) and canopy chlorophyll content (CCC) on real-world field datasets (FRM4Veg and BelSAR) with accuracy comparable to models trained with real Sentinel-2 data. Our method requires no in-situ labels or calibration on real images, offering a cost-effective and self-supervised solution for global vegetation monitoring. The proposed approach illustrates how integrating physical models with advanced deep networks can improve the inversion of RTMs, opening new prospects for large-scale, physically-constrained remote sensing of vegetation traits.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Analogical Inference from Boolean to Continuous Domains</title>
<link>https://arxiv.org/abs/2511.10416</link>
<guid>https://arxiv.org/abs/2511.10416</guid>
<content:encoded><![CDATA[
arXiv:2511.10416v1 Announce Type: cross 
Abstract: Analogical reasoning is a powerful inductive mechanism, widely used in human cognition and increasingly applied in artificial intelligence. Formal frameworks for analogical inference have been developed for Boolean domains, where inference is provably sound for affine functions and approximately correct for functions close to affine. These results have informed the design of analogy-based classifiers. However, they do not extend to regression tasks or continuous domains. In this paper, we revisit analogical inference from a foundational perspective. We first present a counterexample showing that existing generalization bounds fail even in the Boolean setting. We then introduce a unified framework for analogical reasoning in real-valued domains based on parameterized analogies defined via generalized means. This model subsumes both Boolean classification and regression, and supports analogical inference over continuous functions. We characterize the class of analogy-preserving functions in this setting and derive both worst-case and average-case error bounds under smoothness assumptions. Our results offer a general theory of analogical inference across discrete and continuous domains.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Completion of partial structures using Patterson maps with the CrysFormer machine learning model</title>
<link>https://arxiv.org/abs/2511.10440</link>
<guid>https://arxiv.org/abs/2511.10440</guid>
<content:encoded><![CDATA[
arXiv:2511.10440v1 Announce Type: cross 
Abstract: Protein structure determination has long been one of the primary challenges of structural biology, to which deep machine learning (ML)-based approaches have increasingly been applied. However, these ML models generally do not incorporate the experimental measurements directly, such as X-ray crystallographic diffraction data. To this end, we explore an approach that more tightly couples these traditional crystallographic and recent ML-based methods, by training a hybrid 3-d vision transformer and convolutional network on inputs from both domains. We make use of two distinct input constructs / Patterson maps, which are directly obtainable from crystallographic data, and ``partial structure'' template maps derived from predicted structures deposited in the AlphaFold Protein Structure Database with subsequently omitted residues. With these, we predict electron density maps that are then post-processed into atomic models through standard crystallographic refinement processes. Introducing an initial dataset of small protein fragments taken from Protein Data Bank entries and placing them in hypothetical crystal settings, we demonstrate that our method is effective at both improving the phases of the crystallographic structure factors and completing the regions missing from partial structure templates, as well as improving the agreement of the electron density maps with the ground truth atomic structures.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuum Dropout for Neural Differential Equations</title>
<link>https://arxiv.org/abs/2511.10446</link>
<guid>https://arxiv.org/abs/2511.10446</guid>
<content:encoded><![CDATA[
arXiv:2511.10446v1 Announce Type: cross 
Abstract: Neural Differential Equations (NDEs) excel at modeling continuous-time dynamics, effectively handling challenges such as irregular observations, missing values, and noise. Despite their advantages, NDEs face a fundamental challenge in adopting dropout, a cornerstone of deep learning regularization, making them susceptible to overfitting. To address this research gap, we introduce Continuum Dropout, a universally applicable regularization technique for NDEs built upon the theory of alternating renewal processes. Continuum Dropout formulates the on-off mechanism of dropout as a stochastic process that alternates between active (evolution) and inactive (paused) states in continuous time. This provides a principled approach to prevent overfitting and enhance the generalization capabilities of NDEs. Moreover, Continuum Dropout offers a structured framework to quantify predictive uncertainty via Monte Carlo sampling at test time. Through extensive experiments, we demonstrate that Continuum Dropout outperforms existing regularization methods for NDEs, achieving superior performance on various time series and image classification tasks. It also yields better-calibrated and more trustworthy probability estimates, highlighting its effectiveness for uncertainty-aware modeling.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OpenSR-SRGAN: A Flexible Super-Resolution Framework for Multispectral Earth Observation Data</title>
<link>https://arxiv.org/abs/2511.10461</link>
<guid>https://arxiv.org/abs/2511.10461</guid>
<content:encoded><![CDATA[
arXiv:2511.10461v1 Announce Type: cross 
Abstract: We present OpenSR-SRGAN, an open and modular framework for single-image super-resolution in Earth Observation. The software provides a unified implementation of SRGAN-style models that is easy to configure, extend, and apply to multispectral satellite data such as Sentinel-2. Instead of requiring users to modify model code, OpenSR-SRGAN exposes generators, discriminators, loss functions, and training schedules through concise configuration files, making it straightforward to switch between architectures, scale factors, and band setups. The framework is designed as a practical tool and benchmark implementation rather than a state-of-the-art model. It ships with ready-to-use configurations for common remote sensing scenarios, sensible default settings for adversarial training, and built-in hooks for logging, validation, and large-scene inference. By turning GAN-based super-resolution into a configuration-driven workflow, OpenSR-SRGAN lowers the entry barrier for researchers and practitioners who wish to experiment with SRGANs, compare models in a reproducible way, and deploy super-resolution pipelines across diverse Earth-observation datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Waste It: Guiding Generative Recommenders with Structured Human Priors via Multi-head Decoding</title>
<link>https://arxiv.org/abs/2511.10492</link>
<guid>https://arxiv.org/abs/2511.10492</guid>
<content:encoded><![CDATA[
arXiv:2511.10492v1 Announce Type: cross 
Abstract: Optimizing recommender systems for objectives beyond accuracy, such as diversity, novelty, and personalization, is crucial for long-term user satisfaction. To this end, industrial practitioners have accumulated vast amounts of structured domain knowledge, which we term human priors (e.g., item taxonomies, temporal patterns). This knowledge is typically applied through post-hoc adjustments during ranking or post-ranking. However, this approach remains decoupled from the core model learning, which is particularly undesirable as the industry shifts to end-to-end generative recommendation foundation models. On the other hand, many methods targeting these beyond-accuracy objectives often require architecture-specific modifications and discard these valuable human priors by learning user intent in a fully unsupervised manner.
  Instead of discarding the human priors accumulated over years of practice, we introduce a backbone-agnostic framework that seamlessly integrates these human priors directly into the end-to-end training of generative recommenders. With lightweight, prior-conditioned adapter heads inspired by efficient LLM decoding strategies, our approach guides the model to disentangle user intent along human-understandable axes (e.g., interaction types, long- vs. short-term interests). We also introduce a hierarchical composition strategy for modeling complex interactions across different prior types. Extensive experiments on three large-scale datasets demonstrate that our method significantly enhances both accuracy and beyond-accuracy objectives. We also show that human priors allow the backbone model to more effectively leverage longer context lengths and larger model sizes.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edge Machine Learning for Cluster Counting in Next-Generation Drift Chambers</title>
<link>https://arxiv.org/abs/2511.10540</link>
<guid>https://arxiv.org/abs/2511.10540</guid>
<content:encoded><![CDATA[
arXiv:2511.10540v1 Announce Type: cross 
Abstract: Drift chambers have long been central to collider tracking, but future machines like a Higgs factory motivate higher granularity and cluster counting for particle ID, posing new data processing challenges. Machine learning (ML) at the "edge", or in cell-level readout, can dramatically reduce the off-detector data rate for high-granularity drift chambers by performing cluster counting at-source. We present machine learning algorithms for cluster counting in real-time readout of future drift chambers. These algorithms outperform traditional derivative-based techniques based on achievable pion-kaon separation. When synthesized to FPGA resources, they can achieve latencies consistent with real-time operation in a future Higgs factory scenario, thus advancing both R&amp;D for future collider detectors as well as hardware-based ML for edge applications in high energy physics.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two Americas of Well-Being: Divergent Rural-Urban Patterns of Life Satisfaction and Happiness from 2.6 B Social Media Posts</title>
<link>https://arxiv.org/abs/2511.10542</link>
<guid>https://arxiv.org/abs/2511.10542</guid>
<content:encoded><![CDATA[
arXiv:2511.10542v1 Announce Type: cross 
Abstract: Using 2.6 billion geolocated social-media posts (2014-2022) and a fine-tuned generative language model, we construct county-level indicators of life satisfaction and happiness for the United States. We document an apparent rural-urban paradox: rural counties express higher life satisfaction while urban counties exhibit greater happiness. We reconcile this by treating the two as distinct layers of subjective well-being, evaluative vs. hedonic, showing that each maps differently onto place, politics, and time. Republican-leaning areas appear more satisfied in evaluative terms, but partisan gaps in happiness largely flatten outside major metros, indicating context-dependent political effects. Temporal shocks dominate the hedonic layer: happiness falls sharply during 2020-2022, whereas life satisfaction moves more modestly. These patterns are robust across logistic and OLS specifications and align with well-being theory. Interpreted as associations for the population of social-media posts, the results show that large-scale, language-based indicators can resolve conflicting findings about the rural-urban divide by distinguishing the type of well-being expressed, offering a transparent, reproducible complement to traditional surveys.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Diversity in Image Generation via Attribute-Conditional Human Evaluation</title>
<link>https://arxiv.org/abs/2511.10547</link>
<guid>https://arxiv.org/abs/2511.10547</guid>
<content:encoded><![CDATA[
arXiv:2511.10547v1 Announce Type: cross 
Abstract: Despite advances in generation quality, current text-to-image (T2I) models often lack diversity, generating homogeneous outputs. This work introduces a framework to address the need for robust diversity evaluation in T2I models. Our framework systematically assesses diversity by evaluating individual concepts and their relevant factors of variation. Key contributions include: (1) a novel human evaluation template for nuanced diversity assessment; (2) a curated prompt set covering diverse concepts with their identified factors of variation (e.g. prompt: An image of an apple, factor of variation: color); and (3) a methodology for comparing models in terms of human annotations via binomial tests.
  Furthermore, we rigorously compare various image embeddings for diversity measurement. Notably, our principled approach enables ranking of T2I models by diversity, identifying categories where they particularly struggle. This research offers a robust methodology and insights, paving the way for improvements in T2I model diversity and metric development.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-Level Contextual Bandits for Individualized Resource Allocation under Delayed Feedback</title>
<link>https://arxiv.org/abs/2511.10572</link>
<guid>https://arxiv.org/abs/2511.10572</guid>
<content:encoded><![CDATA[
arXiv:2511.10572v1 Announce Type: cross 
Abstract: Equitably allocating limited resources in high-stakes domains-such as education, employment, and healthcare-requires balancing short-term utility with long-term impact, while accounting for delayed outcomes, hidden heterogeneity, and ethical constraints. However, most learning-based allocation frameworks either assume immediate feedback or ignore the complex interplay between individual characteristics and intervention dynamics. We propose a novel bi-level contextual bandit framework for individualized resource allocation under delayed feedback, designed to operate in real-world settings with dynamic populations, capacity constraints, and time-sensitive impact. At the meta level, the model optimizes subgroup-level budget allocations to satisfy fairness and operational constraints. At the base level, it identifies the most responsive individuals within each group using a neural network trained on observational data, while respecting cooldown windows and delayed treatment effects modeled via resource-specific delay kernels. By explicitly modeling temporal dynamics and feedback delays, the algorithm continually refines its policy as new data arrive, enabling more responsive and adaptive decision-making. We validate our approach on two real-world datasets from education and workforce development, showing that it achieves higher cumulative outcomes, better adapts to delay structures, and ensures equitable distribution across subgroups. Our results highlight the potential of delay-aware, data-driven decision-making systems to improve institutional policy and social welfare.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multitask GLocal OBIA-Mamba for Sentinel-2 Landcover Mapping</title>
<link>https://arxiv.org/abs/2511.10604</link>
<guid>https://arxiv.org/abs/2511.10604</guid>
<content:encoded><![CDATA[
arXiv:2511.10604v1 Announce Type: cross 
Abstract: Although Sentinel-2 based land use and land cover (LULC) classification is critical for various environmental monitoring applications, it is a very difficult task due to some key data challenges (e.g., spatial heterogeneity, context information, signature ambiguity). This paper presents a novel Multitask Glocal OBIA-Mamba (MSOM) for enhanced Sentinel-2 classification with the following contributions. First, an object-based image analysis (OBIA) Mamba model (OBIA-Mamba) is designed to reduce redundant computation without compromising fine-grained details by using superpixels as Mamba tokens. Second, a global-local (GLocal) dual-branch convolutional neural network (CNN)-mamba architecture is designed to jointly model local spatial detail and global contextual information. Third, a multitask optimization framework is designed to employ dual loss functions to balance local precision with global consistency. The proposed approach is tested on Sentinel-2 imagery in Alberta, Canada, in comparison with several advanced classification approaches, and the results demonstrate that the proposed approach achieves higher classification accuracy and finer details that the other state-of-the-art methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know Your Limits: Entropy Estimation Modeling for Compression and Generalization</title>
<link>https://arxiv.org/abs/2511.10618</link>
<guid>https://arxiv.org/abs/2511.10618</guid>
<content:encoded><![CDATA[
arXiv:2511.10618v1 Announce Type: cross 
Abstract: Language prediction is constrained by informational entropy intrinsic to language, such that there exists a limit to how accurate any language model can become and equivalently a lower bound to language compression. The most efficient language compression algorithms today are causal (next token prediction) large language models, but the use of these models to form accurate estimates of language entropy is currently computationally infeasible. We introduce encoder-augmented causal decoder model architectures that exhibit superior training efficiency characteristics and achieve higher compression than causal transformers even when trained on modest hardware. We demonstrate how entropy estimates can be obtained on a per-token basis, and show that the generalization of models trained to approach the entropy of their training data necessarily exceeds the generalization of models trained to minimize loss beyond this value. We show empirically that causal models trained to approach but not exceed estimated per-token entropies exhibit greater generalization than models trained without taking entropy into account.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSR: Socratic Self-Refine for Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2511.10621</link>
<guid>https://arxiv.org/abs/2511.10621</guid>
<content:encoded><![CDATA[
arXiv:2511.10621v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable reasoning abilities, yet existing test-time frameworks often rely on coarse self-verification and self-correction, limiting their effectiveness on complex tasks. In this paper, we propose Socratic Self-Refine (SSR), a novel framework for fine-grained evaluation and precise refinement of LLM reasoning. Our proposed SSR decomposes model responses into verifiable (sub-question, sub-answer) pairs, enabling step-level confidence estimation through controlled re-solving and self-consistency checks. By pinpointing unreliable steps and iteratively refining them, SSR produces more accurate and interpretable reasoning chains. Empirical results across five reasoning benchmarks and three LLMs show that SSR consistently outperforms state-of-the-art iterative self-refinement baselines. Beyond performance gains, SSR provides a principled black-box approach for evaluating and understanding the internal reasoning processes of LLMs. Code is available at https://github.com/SalesforceAIResearch/socratic-self-refine-reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Solutions to Non-Convex Functional Constrained Problems with Hidden Convexity</title>
<link>https://arxiv.org/abs/2511.10626</link>
<guid>https://arxiv.org/abs/2511.10626</guid>
<content:encoded><![CDATA[
arXiv:2511.10626v1 Announce Type: cross 
Abstract: Constrained non-convex optimization is fundamentally challenging, as global solutions are generally intractable and constraint qualifications may not hold. However, in many applications, including safe policy optimization in control and reinforcement learning, such problems possess hidden convexity, meaning they can be reformulated as convex programs via a nonlinear invertible transformation. Typically such transformations are implicit or unknown, making the direct link with the convex program impossible. On the other hand, (sub-)gradients with respect to the original variables are often accessible or can be easily estimated, which motivates algorithms that operate directly in the original (non-convex) problem space using standard (sub-)gradient oracles. In this work, we develop the first algorithms to provably solve such non-convex problems to global minima. First, using a modified inexact proximal point method, we establish global last-iterate convergence guarantees with $\widetilde{\mathcal{O}}(\varepsilon^{-3})$ oracle complexity in non-smooth setting. For smooth problems, we propose a new bundle-level type method based on linearly constrained quadratic subproblems, improving the oracle complexity to $\widetilde{\mathcal{O}}(\varepsilon^{-1})$. Surprisingly, despite non-convexity, our methodology does not require any constraint qualifications, can handle hidden convex equality constraints, and achieves complexities matching those for solving unconstrained hidden convex optimization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Querying Labeled Time Series Data with Scenario Programs</title>
<link>https://arxiv.org/abs/2511.10627</link>
<guid>https://arxiv.org/abs/2511.10627</guid>
<content:encoded><![CDATA[
arXiv:2511.10627v1 Announce Type: cross 
Abstract: Simulation-based testing has become a crucial complement to road testing for ensuring the safety of cyber physical systems (CPS). As a result, significant research efforts have been directed toward identifying failure scenarios within simulation environments. However, a critical question remains. Are the AV failure scenarios discovered in simulation reproducible on actual systems in the real world? The sim-to-real gap caused by differences between simulated and real sensor data means that failure scenarios identified in simulation might either be artifacts of synthetic sensor data or actual issues that also occur with real sensor data. To address this, an effective approach to validating simulated failure scenarios is to locate occurrences of these scenarios within real-world datasets and verify whether the failure persists on the datasets. To this end, we introduce a formal definition of how labeled time series sensor data can match an abstract scenario, represented as a scenario program using the Scenic probabilistic programming language. We present a querying algorithm that, given a scenario program and a labeled dataset, identifies the subset of data that matches the specified scenario. Our experiment shows that our algorithm is more accurate and orders of magnitude faster in querying scenarios than the state-of-the-art commercial vision large language models, and can scale with the duration of queried time series data.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instella: Fully Open Language Models with Stellar Performance</title>
<link>https://arxiv.org/abs/2511.10628</link>
<guid>https://arxiv.org/abs/2511.10628</guid>
<content:encoded><![CDATA[
arXiv:2511.10628v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated remarkable performance across a wide range of tasks, yet the majority of high-performing models remain closed-source or partially open, limiting transparency and reproducibility. In this work, we introduce Instella, a family of fully open three billion parameter language models trained entirely on openly available data and codebase. Powered by AMD Instinct MI300X GPUs, Instella is developed through large-scale pre-training, general-purpose instruction tuning, and alignment with human preferences. Despite using substantially fewer pre-training tokens than many contemporaries, Instella achieves state-of-the-art results among fully open models and is competitive with leading open-weight models of comparable size. We further release two specialized variants: Instella-Long, capable of handling context lengths up to 128K tokens, and Instella-Math, a reasoning-focused model enhanced through supervised fine-tuning and reinforcement learning on mathematical tasks. Together, these contributions establish Instella as a transparent, performant, and versatile alternative for the community, advancing the goal of open and reproducible language modeling research.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robot Crash Course: Learning Soft and Stylized Falling</title>
<link>https://arxiv.org/abs/2511.10635</link>
<guid>https://arxiv.org/abs/2511.10635</guid>
<content:encoded><![CDATA[
arXiv:2511.10635v1 Announce Type: cross 
Abstract: Despite recent advances in robust locomotion, bipedal robots operating in the real world remain at risk of falling. While most research focuses on preventing such events, we instead concentrate on the phenomenon of falling itself. Specifically, we aim to reduce physical damage to the robot while providing users with control over a robot's end pose. To this end, we propose a robot agnostic reward function that balances the achievement of a desired end pose with impact minimization and the protection of critical robot parts during reinforcement learning. To make the policy robust to a broad range of initial falling conditions and to enable the specification of an arbitrary and unseen end pose at inference time, we introduce a simulation-based sampling strategy of initial and end poses. Through simulated and real-world experiments, our work demonstrates that even bipedal robots can perform controlled, soft falls.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer in Reinforcement Learning via Regret Bounds for Learning Agents</title>
<link>https://arxiv.org/abs/2202.01182</link>
<guid>https://arxiv.org/abs/2202.01182</guid>
<content:encoded><![CDATA[
arXiv:2202.01182v2 Announce Type: replace 
Abstract: We present an approach for the quantification of the usefulness of transfer in reinforcement learning via regret bounds for a multi-agent setting. Considering a number of $\aleph$ agents operating in the same Markov decision process, however possibly with different reward functions, we consider the regret each agent suffers with respect to an optimal policy maximizing her average reward. We show that when the agents share their observations the total regret of all agents is smaller by a factor of $\sqrt{\aleph}$ compared to the case when each agent has to rely on the information collected by herself. This result demonstrates how considering the regret in multi-agent settings can provide theoretical bounds on the benefit of sharing observations in transfer learning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reassessing feature-based Android malware detection in a contemporary context</title>
<link>https://arxiv.org/abs/2301.12778</link>
<guid>https://arxiv.org/abs/2301.12778</guid>
<content:encoded><![CDATA[
arXiv:2301.12778v4 Announce Type: replace 
Abstract: We report the findings of a reimplementation of 18 foundational studies in feature-based machine learning for Android malware detection, published during the period 2013-2023. These studies are reevaluated on a level playing field using a contemporary Android environment and a balanced dataset of 124,000 applications. Our findings show that feature-based approaches can still achieve detection accuracies beyond 98%, despite a considerable increase in the size of the underlying Android feature sets. We observe that features derived through dynamic analysis yield only a small benefit over those derived from static analysis, and that simpler models often out-perform more complex models. We also find that API calls and opcodes are the most productive static features within our evaluation context, network traffic is the most predictive dynamic feature, and that ensemble models provide an efficient means of combining models trained on static and dynamic features. Together, these findings suggest that simple, fast machine learning approaches can still be an effective basis for malware detection, despite the increasing focus on slower, more expensive machine learning models in the literature.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effector: A Python package for regional explanations</title>
<link>https://arxiv.org/abs/2404.02629</link>
<guid>https://arxiv.org/abs/2404.02629</guid>
<content:encoded><![CDATA[
arXiv:2404.02629v2 Announce Type: replace 
Abstract: Effector is a Python package for interpreting machine learning (ML) models that are trained on tabular data through global and regional feature effects. Global effects, like Partial Dependence Plot (PDP) and Accumulated Local Effects (ALE), are widely used for explaining tabular ML models due to their simplicity -- each feature's average influence on the prediction is summarized by a single 1D plot. However, when features are interacting, global effects can be misleading. Regional effects address this by partitioning the input space into disjoint subregions with minimal interactions within each and computing a separate regional effect per subspace. Regional effects are then visualized by a set of 1D plots per feature. Effector provides efficient implementations of state-of-the-art global and regional feature effects methods under a unified API. The package integrates seamlessly with major ML libraries like scikit-learn and PyTorch. It is designed to be modular and extensible, and comes with comprehensive documentation and tutorials. Effector is an open-source project publicly available on Github at https://github.com/givasile/effector.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lipschitz-Regularized Critics Lead to Policy Robustness Against Transition Dynamics Uncertainty</title>
<link>https://arxiv.org/abs/2404.13879</link>
<guid>https://arxiv.org/abs/2404.13879</guid>
<content:encoded><![CDATA[
arXiv:2404.13879v3 Announce Type: replace 
Abstract: Uncertainties in transition dynamics pose a critical challenge in reinforcement learning (RL), often resulting in performance degradation of trained policies when deployed on hardware. Many robust RL approaches follow two strategies: enforcing smoothness in actor or actor-critic modules with Lipschitz regularization, or learning robust Bellman operators. However, the first strategy does not investigate the impact of critic-only Lipschitz regularization on policy robustness, while the second lacks comprehensive validation in real-world scenarios. Building on this gap and prior work, we propose PPO-PGDLC, an algorithm based on Proximal Policy Optimization (PPO) that integrates Projected Gradient Descent (PGD) with a Lipschitz-regularized critic (LC). The PGD component calculates the adversarial state within an uncertainty set to approximate the robust Bellman operator, and the Lipschitz-regularized critic further improves the smoothness of learned policies. Experimental results on two classic control tasks and one real-world robotic locomotion task demonstrates that, compared to several baseline algorithms, PPO-PGDLC achieves better performance and predicts smoother actions under environmental perturbations.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution Learning Meets Graph Structure Sampling</title>
<link>https://arxiv.org/abs/2405.07914</link>
<guid>https://arxiv.org/abs/2405.07914</guid>
<content:encoded><![CDATA[
arXiv:2405.07914v2 Announce Type: replace 
Abstract: This work establishes a novel link between the problem of PAC-learning high-dimensional graphical models and the task of (efficient) counting and sampling of graph structures, using an online learning framework.
  We observe that if we apply the exponentially weighted average (EWA) or randomized weighted majority (RWM) forecasters on a sequence of samples from a distribution P using the log loss function, the average regret incurred by the forecaster's predictions can be used to bound the expected KL divergence between P and the predictions. Known regret bounds for EWA and RWM then yield new sample complexity bounds for learning Bayes nets. Moreover, these algorithms can be made computationally efficient for several interesting classes of Bayes nets. Specifically, we give a new sample-optimal and polynomial time learning algorithm with respect to trees of unknown structure and the first polynomial sample and time algorithm for learning with respect to Bayes nets over a given chordal skeleton.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Caption, Create, Continue: Continual Learning with Pre-trained Generative Vision-Language Models</title>
<link>https://arxiv.org/abs/2409.17806</link>
<guid>https://arxiv.org/abs/2409.17806</guid>
<content:encoded><![CDATA[
arXiv:2409.17806v2 Announce Type: replace 
Abstract: Continual learning (CL) enables models to adapt to evolving data streams without catastrophic forgetting, a fundamental requirement for real-world AI systems. However, the current methods often depend on large replay buffers or heavily annotated datasets which are impractical due to storage, privacy, and cost constraints. We propose CLTS (Continual Learning via Text-Image Synergy), a novel class-incremental framework that mitigates forgetting without storing real task data. CLTS leverages pre-trained vision-language models, BLIP (Bootstrapping Language-Image Pre-training) for caption generation and stable diffusion for sample generation. Each task is handled by a dedicated Task Head, while a Task Router learns to assign inputs to the correct Task Head using the generated data. On three benchmark datasets, CLTS improves average task accuracy by up to 54% and achieves 63 times better memory efficiency compared to four recent continual learning baselines, demonstrating improved retention and adaptability. CLTS introduces a novel perspective by integrating generative text-image augmentation for scalable continual learning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matryoshka Pilot: Learning to Drive Black-Box LLMs with LLMs</title>
<link>https://arxiv.org/abs/2410.20749</link>
<guid>https://arxiv.org/abs/2410.20749</guid>
<content:encoded><![CDATA[
arXiv:2410.20749v4 Announce Type: replace 
Abstract: Despite the impressive generative abilities of black-box large language models (LLMs), their inherent opacity hinders further advancements in capabilities such as reasoning, planning, and personalization. Existing works aim to enhance LLM capabilities via domain-specific adaptation, which require additional training on accessible model parameters, an infeasible option for black-box LLMs. To address this challenge, we introduce Matryoshka Pilot (M-Pilot), a lightweight white-box LLM controller that guides a large-scale black-box LLM generator by decomposing complex tasks into a series of intermediate outputs. Specifically, we consider the black-box LLM as an environment, with M-Pilot serving as a policy to provide intermediate guidance through prompts for driving the black-box LLM. M-Pilot is trained to pivot the outputs of the black-box LLM aligning with preferences during iterative interaction, which enables controllable multi-turn generation and self-improvement in optimizing intermediate guidance. Empirical evaluations on diverse tasks demonstrate that our method effectively enhances the capabilities of black-box LLMs in complex, long-horizon tasks. Our code is publicly available at: https://github.com/lichangh20/Matryoshka.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing the Vulnerability of Decentralized Learning to Membership Inference Attacks Through the Lens of Graph Mixing</title>
<link>https://arxiv.org/abs/2412.12837</link>
<guid>https://arxiv.org/abs/2412.12837</guid>
<content:encoded><![CDATA[
arXiv:2412.12837v4 Announce Type: replace 
Abstract: The primary promise of decentralized learning is to allow users to engage in the training of machine learning models in a collaborative manner while keeping their data on their premises and without relying on any central entity. However, this paradigm necessitates the exchange of model parameters or gradients between peers. Such exchanges can be exploited to infer sensitive information about training data, which is achieved through privacy attacks (e.g., Membership Inference Attacks -- MIA). In order to devise effective defense mechanisms, it is important to understand the factors that increase/reduce the vulnerability of a given decentralized learning architecture to MIA. In this study, we extensively explore the vulnerability to MIA of various decentralized learning architectures by varying the graph structure (e.g., number of neighbors), the graph dynamics, and the aggregation strategy, across diverse datasets and data distributions. Our key finding, which to the best of our knowledge we are the first to report, is that the vulnerability to MIA is heavily correlated to (i) the local model mixing strategy performed by each node upon reception of models from neighboring nodes and (ii) the global mixing properties of the communication graph. We illustrate these results experimentally using four datasets and by theoretically analyzing the mixing properties of various decentralized architectures. We also empirically show that enhancing mixing properties is highly beneficial when combined with other privacy-preserving techniques such as Differential Privacy. Our paper draws a set of lessons learned for devising decentralized learning systems that reduce by design the vulnerability to MIA.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Neural ODEs for Gene Regulatory Network Discovery under Perturbations</title>
<link>https://arxiv.org/abs/2501.02409</link>
<guid>https://arxiv.org/abs/2501.02409</guid>
<content:encoded><![CDATA[
arXiv:2501.02409v5 Announce Type: replace 
Abstract: Modern high-throughput biological datasets with thousands of perturbations provide the opportunity for large-scale discovery of causal graphs that represent the regulatory interactions between genes. Differentiable causal graphical models have been proposed to infer a gene regulatory network (GRN) from large scale interventional datasets, capturing the causal gene regulatory relationships from genetic perturbations. However, existing models are limited in their expressivity and scalability while failing to address the dynamic nature of biological processes such as cellular differentiation. We propose PerturbODE, a novel framework that incorporates biologically informative neural ordinary differential equations (neural ODEs) to model cell state trajectories under perturbations and derive the causal GRN from the neural ODE's parameters. We demonstrate PerturbODE's efficacy in trajectory prediction and GRN inference across simulated and real over-expression datasets.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DarkFarseer: Robust Spatio-temporal Kriging under Graph Sparsity and Noise</title>
<link>https://arxiv.org/abs/2501.02808</link>
<guid>https://arxiv.org/abs/2501.02808</guid>
<content:encoded><![CDATA[
arXiv:2501.02808v2 Announce Type: replace 
Abstract: With the rapid growth of the Internet of Things and Cyber-Physical Systems, widespread sensor deployment has become essential. However, the high costs of building sensor networks limit their scale and coverage, making fine-grained deployment challenging. Inductive Spatio-Temporal Kriging (ISK) addresses this issue by introducing virtual sensors. Based on graph neural networks (GNNs) extracting the relationships between physical and virtual sensors, ISK can infer the measurements of virtual sensors from physical sensors. However, current ISK methods rely on conventional message-passing mechanisms and network architectures, without effectively extracting spatio-temporal features of physical sensors and focusing on representing virtual sensors. Additionally, existing graph construction methods face issues of sparse and noisy connections, destroying ISK performance. To address these issues, we propose DarkFarseer, a novel ISK framework with three key components. First, we propose the Neighbor Hidden Style Enhancement module with a style transfer strategy to enhance the representation of virtual nodes in a temporal-then-spatial manner to better extract the spatial relationships between physical and virtual nodes. Second, we propose Virtual-Component Contrastive Learning, which aims to enrich the node representation by establishing the association between the patterns of virtual nodes and the regional patterns within graph components. Lastly, we design a Similarity-Based Graph Denoising Strategy, which reduces the connectivity strength of noisy connections around virtual nodes and their neighbors based on their temporal information and regional spatial patterns. Extensive experiments demonstrate that DarkFarseer significantly outperforms existing ISK methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preconditioned Inexact Stochastic ADMM for Deep Model</title>
<link>https://arxiv.org/abs/2502.10784</link>
<guid>https://arxiv.org/abs/2502.10784</guid>
<content:encoded><![CDATA[
arXiv:2502.10784v4 Announce Type: replace 
Abstract: The recent advancement of foundation models (FMs) has brought about a paradigm shift, revolutionizing various sectors worldwide. The popular optimizers used to train these models are stochastic gradient descent-based algorithms, which face inherent limitations, such as slow convergence and stringent assumptions for convergence. In particular, data heterogeneity arising from distributed settings poses significant challenges to their theoretical and numerical performance. This paper develops an algorithm, PISA (Preconditioned Inexact Stochastic Alternating Direction Method of Multipliers). Grounded in rigorous theoretical guarantees, the algorithm converges under the sole assumption of Lipschitz continuity of the gradient on a bounded region, thereby removing the need for other conditions commonly imposed by stochastic methods. This capability enables the proposed algorithm to tackle the challenge of data heterogeneity effectively. Moreover, the algorithmic architecture enables scalable parallel computing and supports various preconditions, such as second-order information, second moment, and orthogonalized momentum by Newton-Schulz iterations. Incorporating the latter two preconditions in PISA yields two computationally efficient variants: SISA and NSISA. Comprehensive experimental evaluations for training or fine-tuning diverse deep models, including vision models, large language models, reinforcement learning models, generative adversarial networks, and recurrent neural networks, demonstrate superior numerical performance of SISA and NSISA compared to various state-of-the-art optimizers.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRDP: Progressively Refined Differentiable Physics</title>
<link>https://arxiv.org/abs/2502.19611</link>
<guid>https://arxiv.org/abs/2502.19611</guid>
<content:encoded><![CDATA[
arXiv:2502.19611v2 Announce Type: replace 
Abstract: The physics solvers employed for neural network training are primarily iterative, and hence, differentiating through them introduces a severe computational burden as iterations grow large. Inspired by works in bilevel optimization, we show that full accuracy of the network is achievable through physics significantly coarser than fully converged solvers. We propose Progressively Refined Differentiable Physics (PRDP), an approach that identifies the level of physics refinement sufficient for full training accuracy. By beginning with coarse physics, adaptively refining it during training, and stopping refinement at the level adequate for training, it enables significant compute savings without sacrificing network accuracy. Our focus is on differentiating iterative linear solvers for sparsely discretized differential operators, which are fundamental to scientific computing. PRDP is applicable to both unrolled and implicit differentiation. We validate its performance on a variety of learning scenarios involving differentiable physics solvers such as inverse problems, autoregressive neural emulators, and correction-based neural-hybrid solvers. In the challenging example of emulating the Navier-Stokes equations, we reduce training time by 62%.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Overlap-aware meta-learning attention to enhance hypergraph neural networks for node classification</title>
<link>https://arxiv.org/abs/2503.07961</link>
<guid>https://arxiv.org/abs/2503.07961</guid>
<content:encoded><![CDATA[
arXiv:2503.07961v2 Announce Type: replace 
Abstract: Although hypergraph neural networks (HGNNs) have emerged as a powerful framework for analyzing complex datasets, their practical performance often remains limited. On one hand, existing networks typically employ a single type of attention mechanism, focusing on either structural or feature similarities during message passing. On the other hand, assuming that all nodes in current hypergraph models have the same level of overlap may lead to suboptimal generalization. To overcome these limitations, we propose a novel framework, overlap-aware meta-learning attention for hypergraph neural networks (OMA-HGNN). First, we introduce a hypergraph attention mechanism that integrates both structural and feature similarities. Specifically, we linearly combine their respective losses with weighted factors for the HGNN model. Second, we partition nodes into different tasks based on their diverse overlap levels and develop a multi-task Meta-Weight-Net (MWN) to determine the corresponding weighted factors. Third, we jointly train the internal MWN model with the losses from the external HGNN model and train the external model with the weighted factors from the internal model. To evaluate the effectiveness of OMA-HGNN, we conducted experiments on six real-world datasets and benchmarked its perfor-mance against nine state-of-the-art methods for node classification. The results demonstrate that OMA-HGNN excels in learning superior node representations and outperforms these baselines.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ELECTRA: A Cartesian Network for 3D Charge Density Prediction with Floating Orbitals</title>
<link>https://arxiv.org/abs/2503.08305</link>
<guid>https://arxiv.org/abs/2503.08305</guid>
<content:encoded><![CDATA[
arXiv:2503.08305v3 Announce Type: replace 
Abstract: We present the Electronic Tensor Reconstruction Algorithm (ELECTRA) - an equivariant model for predicting electronic charge densities using floating orbitals. Floating orbitals are a long-standing concept in the quantum chemistry community that promises more compact and accurate representations by placing orbitals freely in space, as opposed to centering all orbitals at the position of atoms. Finding the ideal placement of these orbitals requires extensive domain knowledge, though, which thus far has prevented widespread adoption. We solve this in a data-driven manner by training a Cartesian tensor network to predict the orbital positions along with orbital coefficients. This is made possible through a symmetry-breaking mechanism that is used to learn position displacements with lower symmetry than the input molecule while preserving the rotation equivariance of the charge density itself. Inspired by recent successes of Gaussian Splatting in representing densities in space, we are using Gaussian orbitals and predicting their weights and covariance matrices. Our method achieves a state-of-the-art balance between computational efficiency and predictive accuracy on established benchmarks. Furthermore, ELECTRA is able to lower the compute time required to arrive at converged DFT solutions - initializing calculations using our predicted densities yields an average 50.72 \% reduction in self-consistent field (SCF) iterations on unseen molecules.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Game Solving via Asymmetry of Step Sizes</title>
<link>https://arxiv.org/abs/2503.12770</link>
<guid>https://arxiv.org/abs/2503.12770</guid>
<content:encoded><![CDATA[
arXiv:2503.12770v2 Announce Type: replace 
Abstract: Counterfactual Regret Minimization (CFR) algorithms are widely used to compute a Nash equilibrium (NE) in two-player zero-sum imperfect-information extensive-form games (IIGs). Among them, Predictive CFR$^+$ (PCFR$^+$) is particularly powerful, achieving an exceptionally fast empirical convergence rate via the prediction in many games.However, the empirical convergence rate of PCFR$^+$ would significantly degrade if the prediction is inaccurate, leading to unstable performance on certain IIGs. To enhance the robustness of PCFR$^+$, we propose Asymmetric PCFR$^+$ (APCFR$^+$), which employs an adaptive asymmetry of step sizes between the updates of implicit and explicit accumulated counterfactual regrets to mitigate the impact of the prediction inaccuracy on convergence. We present a theoretical analysis demonstrating why APCFR$^+$ can enhance the robustness. To the best of our knowledge, we are the first to propose the asymmetry of step sizes, a simple yet novel technique that effectively improves the robustness of PCFR$^+$. Then, to reduce the difficulty of implementing APCFR$^+$ caused by the adaptive asymmetry, we propose a simplified version of APCFR$^+$ called Simple APCFR$^+$ (SAPCFR$^+$), which uses a fixed asymmetry of step sizes to enable only a single-line modification compared to original PCFR$^+$.Experimental results on five standard IIG benchmarks and two heads-up no-limit Texas Hold' em (HUNL) Subagems show that (i) both APCFR$^+$ and SAPCFR$^+$ outperform PCFR$^+$ in most of the tested games, (ii) SAPCFR$^+$ achieves a comparable empirical convergence rate with APCFR$^+$,and (iii) our approach can be generalized to improve other CFR algorithms, e.g., Discount CFR (DCFR).
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unique Hard Attention: A Tale of Two Sides</title>
<link>https://arxiv.org/abs/2503.14615</link>
<guid>https://arxiv.org/abs/2503.14615</guid>
<content:encoded><![CDATA[
arXiv:2503.14615v3 Announce Type: replace 
Abstract: Understanding the expressive power of transformers has recently attracted attention, as it offers insights into their abilities and limitations. Many studies analyze unique hard attention transformers, where attention selects a single position that maximizes the attention scores. When multiple positions achieve the maximum score, either the rightmost or the leftmost of those is chosen. In this paper, we highlight the importance of this seeming triviality. Recently, finite-precision transformers with both leftmost- and rightmost-hard attention were shown to be equivalent to Linear Temporal Logic (LTL). We show that this no longer holds with only leftmost-hard attention -- in that case, they correspond to a \emph{strictly weaker} fragment of LTL. Furthermore, we show that models with leftmost-hard attention are equivalent to \emph{soft} attention, suggesting they may better approximate real-world transformers than right-attention models. These findings refine the landscape of transformer expressivity and underscore the role of attention directionality.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient quantification on large-scale networks</title>
<link>https://arxiv.org/abs/2503.15267</link>
<guid>https://arxiv.org/abs/2503.15267</guid>
<content:encoded><![CDATA[
arXiv:2503.15267v2 Announce Type: replace 
Abstract: Network quantification (NQ) is the problem of estimating the proportions of nodes belonging to each class in subsets of unlabelled graph nodes. When prior probability shift is at play, this task cannot be effectively addressed by first classifying the nodes and then counting the class predictions. In addition, unlike non-relational quantification, NQ demands enhanced flexibility in order to capture a broad range of connectivity patterns, resilience to the challenge of heterophily, and scalability to large networks. In order to meet these stringent requirements, we introduce XNQ, a novel method that synergizes the flexibility and efficiency of the unsupervised node embeddings computed by randomized recursive Graph Neural Networks, with an Expectation-Maximization algorithm that provides a robust quantification-aware adjustment to the output probabilities of a calibrated node classifier. In an extensive evaluation, in which we also validate the design choices underpinning XNQ through comprehensive ablation experiments, we find that XNQ consistently and significantly improves on the best network quantification methods to date, thereby setting the new state of the art for this challenging task. XNQ also provides a training speed-up of up to 10x-100x over other methods based on graph learning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-PINNs: Epistemic Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2503.19333</link>
<guid>https://arxiv.org/abs/2503.19333</guid>
<content:encoded><![CDATA[
arXiv:2503.19333v2 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have demonstrated promise as a framework for solving forward and inverse problems involving partial differential equations. Despite recent progress in the field, it remains challenging to quantify uncertainty in these networks. While techniques such as Bayesian PINNs (B-PINNs) provide a principled approach to capturing epistemic uncertainty through Bayesian inference, they can be computationally expensive for large-scale applications. In this work, we propose Epistemic Physics-Informed Neural Networks (E-PINNs), a framework that uses a small network, the epinet, to efficiently quantify epistemic uncertainty in PINNs. The proposed approach works as an add-on to existing, pre-trained PINNs with a small computational overhead. We demonstrate the applicability of the proposed framework in various test cases and compare the results with B-PINNs using Hamiltonian Monte Carlo (HMC) posterior estimation and dropout-equipped PINNs (Dropout-PINNs). In our experiments, E-PINNs achieve calibrated coverage with competitive sharpness at substantially lower cost. We demonstrate that when B-PINNs produce narrower bands, they under-cover in our tests. E-PINNs also show better calibration than Dropout-PINNs in these examples, indicating a favorable accuracy-efficiency trade-off.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why do zeroes happen? A model-based approach for demand classification</title>
<link>https://arxiv.org/abs/2504.05894</link>
<guid>https://arxiv.org/abs/2504.05894</guid>
<content:encoded><![CDATA[
arXiv:2504.05894v2 Announce Type: replace 
Abstract: Effective demand forecasting is critical for inventory management, production planning, and decision making across industries. Selecting the appropriate model and suitable features to efficiently capture patterns in the data is one of the main challenges in demand forecasting. In reality, this becomes even more complicated when the recorded sales have zeroes, which can happen naturally or due to some anomalies, such as stockouts and recording errors. Mistreating the zeroes can lead to the application of inappropriate forecasting methods, and thus leading to poor decision making. Furthermore, the demand itself can have different fundamental characteristics, and being able to distinguish one type from another might bring substantial benefits in terms of accuracy and thus decision making. We propose a two-stage model-based classification framework that in the first step, identifies artificially occurring zeroes, and in the second, classifies demand to one of the possible types: regular/intermittent, intermittent smooth/lumpy, fractional/count. The framework relies on statistical modelling and information criteria. We argue that different types of demand need different features, and show empirically that they tend to increase the accuracy of the forecasting methods and reduce inventory costs compared to those applied directly to the dataset without the generated features and the two-stage framework.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructing an Optimal Behavior Basis for the Option Keyboard</title>
<link>https://arxiv.org/abs/2505.00787</link>
<guid>https://arxiv.org/abs/2505.00787</guid>
<content:encoded><![CDATA[
arXiv:2505.00787v2 Announce Type: replace 
Abstract: Multi-task reinforcement learning aims to quickly identify solutions for new tasks with minimal or no additional interaction with the environment. Generalized Policy Improvement (GPI) addresses this by combining a set of base policies to produce a new one that is at least as good -- though not necessarily optimal -- as any individual base policy. Optimality can be ensured, particularly in the linear-reward case, via techniques that compute a Convex Coverage Set (CCS). However, these are computationally expensive and do not scale to complex domains. The Option Keyboard (OK) improves upon GPI by producing policies that are at least as good -- and often better. It achieves this through a learned meta-policy that dynamically combines base policies. However, its performance critically depends on the choice of base policies. This raises a key question: is there an optimal set of base policies -- an optimal behavior basis -- that enables zero-shot identification of optimal solutions for any linear tasks? We solve this open problem by introducing a novel method that efficiently constructs such an optimal behavior basis. We show that it significantly reduces the number of base policies needed to ensure optimality in new tasks. We also prove that it is strictly more expressive than a CCS, enabling particular classes of non-linear tasks to be solved optimally. We empirically evaluate our technique in challenging domains and show that it outperforms state-of-the-art approaches, increasingly so as task complexity increases.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OODTE: A Differential Testing Engine for the ONNX Optimizer</title>
<link>https://arxiv.org/abs/2505.01892</link>
<guid>https://arxiv.org/abs/2505.01892</guid>
<content:encoded><![CDATA[
arXiv:2505.01892v3 Announce Type: replace 
Abstract: With over 760 stars on GitHub and being part of the official ONNX repository, the ONNX Optimizer is the default tool for applying graph-based optimizations to ONNX models. Despite its widespread use, its ability to maintain model accuracy during optimization has not been thoroughly investigated. In this work, we present OODTE, a utility designed to automatically and comprehensively evaluate the correctness of the ONNX Optimizer. OODTE adopts a straightforward yet powerful differential testing and evaluation methodology, which can be readily adapted for use with other compiler optimizers. Specifically, OODTE takes a collection of ONNX models, applies optimizations, and executes both the original and optimized versions across a user-defined input set, automatically capturing any issues encountered during optimization. When discrepancies in accuracy arise, OODTE iteratively isolates the responsible optimization pass by repeating the process at a finer granularity. We applied OODTE to 130 well-known models from the official ONNX Model Hub, spanning diverse tasks including classification, object detection, semantic segmentation, text summarization, question answering, and sentiment analysis. Our evaluation revealed that 9.2% of the model instances either caused the optimizer to crash or led to the generation of invalid models using default optimization strategies. Additionally, 30% of classification models and 16.6% of object detection and segmentation models exhibited differing outputs across original and optimized versions, whereas models focused on text-related tasks were generally robust to optimization. OODTE uncovered 15 issues-14 previously unknown-affecting 9 of 47 optimization passes and the optimizer overall. All issues were reported to the ONNX Optimizer team. OODTE offers a simple but effective framework for validating AI model optimizers, applicable beyond the ONNX ecosystem.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashKAT: Understanding and Addressing Performance Bottlenecks in the Kolmogorov-Arnold Transformer</title>
<link>https://arxiv.org/abs/2505.13813</link>
<guid>https://arxiv.org/abs/2505.13813</guid>
<content:encoded><![CDATA[
arXiv:2505.13813v2 Announce Type: replace 
Abstract: The Kolmogorov-Arnold Network (KAN) has been gaining popularity as an alternative to the multi-layer perceptron (MLP) with its increased expressiveness and interpretability. Even so, the KAN suffers from being orders of magnitude slower due to its increased computational cost and training instability, limiting its applicability to larger-scale tasks. Recently, the Kolmogorov-Arnold Transformer (KAT) has been proposed, which can achieve FLOPs similar to the traditional Transformer with MLPs by leveraging Group-Rational KAN (GR-KAN). Unfortunately, despite the comparable FLOPs, our testing reveals that the KAT is still 123x slower in training speeds, indicating that there are other performance bottlenecks beyond FLOPs. In this paper, we conduct a series of experiments to understand the root cause of the slowdown in KAT. We uncover that the slowdown can be isolated to memory stalls, linked more specifically to inefficient gradient accumulations in the backward pass of GR-KAN. To address this memory bottleneck, we propose FlashKAT, which minimizes accesses to slow memory and the usage of atomic adds through a restructured kernel. Evaluations demonstrate that FlashKAT can achieve a training speedup of 86.5x compared with the state-of-the-art KAT, while reducing rounding errors in the computation of the gradients.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finding separatrices of dynamical flows with Deep Koopman Eigenfunctions</title>
<link>https://arxiv.org/abs/2505.15231</link>
<guid>https://arxiv.org/abs/2505.15231</guid>
<content:encoded><![CDATA[
arXiv:2505.15231v2 Announce Type: replace 
Abstract: Many natural systems, including neural circuits involved in decision making, are modeled as high-dimensional dynamical systems with multiple stable states. While existing analytical tools primarily describe behavior near stable equilibria, characterizing separatrices--the manifolds that delineate boundaries between different basins of attraction--remains challenging, particularly in high-dimensional settings. Here, we introduce a numerical framework leveraging Koopman Theory combined with Deep Neural Networks to effectively characterize separatrices. Specifically, we approximate Koopman Eigenfunctions (KEFs) associated with real positive eigenvalues, which vanish precisely at the separatrices. Utilizing these scalar KEFs, optimization methods efficiently locate separatrices even in complex systems. We demonstrate our approach on synthetic benchmarks, ecological network models, and high-dimensional recurrent neural networks trained on either neuroscience-inspired tasks or fit to real neural data. Moreover, we illustrate the practical utility of our method by designing optimal perturbations that can shift systems across separatrices, enabling predictions relevant to optogenetic stimulation experiments in neuroscience.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent Markov Entanglement</title>
<link>https://arxiv.org/abs/2506.02385</link>
<guid>https://arxiv.org/abs/2506.02385</guid>
<content:encoded><![CDATA[
arXiv:2506.02385v3 Announce Type: replace 
Abstract: Value decomposition has long been a fundamental technique in multi-agent dynamic programming and reinforcement learning (RL). Specifically, the value function of a global state $(s_1,s_2,\ldots,s_N)$ is often approximated as the sum of local functions: $V(s_1,s_2,\ldots,s_N)\approx\sum_{i=1}^N V_i(s_i)$. This approach traces back to the index policy in restless multi-armed bandit problems and has found various applications in modern RL systems. However, the theoretical justification for why this decomposition works so effectively remains underexplored.
  In this paper, we uncover the underlying mathematical structure that enables value decomposition. We demonstrate that a multi-agent Markov decision process (MDP) permits value decomposition if and only if its transition matrix is not "entangled" -- a concept analogous to quantum entanglement in quantum physics. Drawing inspiration from how physicists measure quantum entanglement, we introduce how to measure the "Markov entanglement" for multi-agent MDPs and show that this measure can be used to bound the decomposition error in general multi-agent MDPs.
  Using the concept of Markov entanglement, we proved that a widely-used class of index policies is weakly entangled and enjoys a sublinear $\mathcal O(\sqrt{N})$ scale of decomposition error for $N$-agent systems. Finally, we show how Markov entanglement can be efficiently estimated in practice, providing practitioners with an empirical proxy for the quality of value decomposition.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DuoGPT: Training-free Dual Sparsity through Activation-aware Pruning in LLMs</title>
<link>https://arxiv.org/abs/2506.20194</link>
<guid>https://arxiv.org/abs/2506.20194</guid>
<content:encoded><![CDATA[
arXiv:2506.20194v3 Announce Type: replace 
Abstract: Large language models (LLMs) deliver strong performance but are difficult to deploy due to high memory and compute costs. While pruning reduces these demands, most methods ignore activation sparsity observed at runtime. We reinterpret activation sparsity as dynamic structured weight sparsity and propose DuoGPT, a unified framework that constructs dual-sparse (spMspV) workloads by combining unstructured weight pruning with activation sparsity. To preserve accuracy, we extend the Optimal Brain Compression (OBC) framework with activation-aware calibration and introduce output residuals from the dense model as correction terms. We further optimize the solution for efficient GPU execution, enabling scalability to billion-parameter LLMs. Evaluations on LLaMA-2 and LLaMA-3 show that DuoGPT outperforms state-of-the-art structured pruning methods by up to 9.17% accuracy at an iso-speedup of 1.39$\times$ compared to the baseline dense model. Code is available at Github.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Linear Mode Connectivity for Transformers</title>
<link>https://arxiv.org/abs/2506.22712</link>
<guid>https://arxiv.org/abs/2506.22712</guid>
<content:encoded><![CDATA[
arXiv:2506.22712v2 Announce Type: replace 
Abstract: Understanding the geometry of neural network loss landscapes is a central question in deep learning, with implications for generalization and optimization. A striking phenomenon is linear mode connectivity (LMC), where independently trained models can be connected by low- or zero-loss paths despite appearing to lie in separate loss basins. However, this is often obscured by symmetries in parameter space -- such as neuron permutations -- which make functionally equivalent models appear dissimilar. Prior work has predominantly focused on neuron reordering through permutations, but such approaches are limited in scope and fail to capture the richer symmetries exhibited by modern architectures such as Transformers. In this work, we introduce a unified framework that captures four symmetry classes -- permutations, semi-permutations, orthogonal transformations, and general invertible maps -- broadening the set of valid reparameterizations and subsuming many previous approaches as special cases. Crucially, this generalization enables, for the first time, the discovery of low- and zero-barrier linear interpolation paths between independently trained Vision Transformers and GPT-2 models. Furthermore, our framework extends beyond pairwise alignment to multi-model and width-heterogeneous settings, enabling alignment across architectures of different sizes. These results reveal deeper structure in the loss landscape and underscore the importance of symmetry-aware analysis for understanding model space geometry.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection</title>
<link>https://arxiv.org/abs/2506.22837</link>
<guid>https://arxiv.org/abs/2506.22837</guid>
<content:encoded><![CDATA[
arXiv:2506.22837v2 Announce Type: replace 
Abstract: The recently proposed xLSTM is a powerful model that leverages expressive multiplicative gating and residual connections, providing the temporal capacity needed for long-horizon forecasting and representation learning. This architecture has demonstrated success in time series forecasting, lossless compression, and even large-scale language modeling tasks, where its linear memory footprint and fast inference make it a viable alternative to Transformers. Despite its growing popularity, no prior work has explored xLSTM for anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the first anomaly detection method that integrates a full encoder-decoder xLSTM architecture, purpose-built for multivariate time series data. Our encoder processes input sequences to capture historical context, while the decoder is devised in two separate variants of the method. In the forecasting approach, the decoder iteratively generates forecasted future values xLSTMAD-F, while the reconstruction approach reconstructs the input time series from its encoded counterpart xLSTMAD-R. We investigate the performance of two loss functions: Mean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider local reconstruction fidelity and global sequence alignment, respectively. We evaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17 real-world datasets, using state-of-the-art challenging metrics such as VUS-PR. In our results, xLSTM showcases state-of-the-art accuracy, outperforming 23 popular anomaly detection baselines. Our paper is the first work revealing the powerful modeling capabilities of xLSTM for anomaly detection, paving the way for exciting new developments on this subject. Our code is available at: https://github.com/Nyderx/xlstmad
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-grained Token Allocation Via Operation Pruning for Efficient MLLMs</title>
<link>https://arxiv.org/abs/2507.02909</link>
<guid>https://arxiv.org/abs/2507.02909</guid>
<content:encoded><![CDATA[
arXiv:2507.02909v2 Announce Type: replace 
Abstract: Token reduction accelerates Multimodal Large Language Models (MLLMs) by reducing excessive tokens, but overlooks structural redundancy differences, where critical and redundant modules process identical token loads. For fine-grained computation control, we define an ``operation" as the computation for a module to process a group of tokens and introduce the operation pruning framework to enable modules to selectively process tokens. Built on this framework, we propose Depth-wise Operation Pruning (DOP), a data-driven method that searches for strategies to prune redundant operations and save computational budget for critical modules to process more tokens than uniform allocation by minimizing divergence from the original model's output probability distribution on a small validation set while satisfying computational constraints. For efficient optimization, DOP applies depth-wise pruning to reduce policy space and uses an additive approximation to minimize required validation runs. Depth-wise pruning partitions operations by module type and token group, and prunes operations in deeper layers before those in shallower layers within each module-group pair. The additive approximation obtains individual divergences by independently varying each policy parameter, and then sums them to approximate the joint divergence of simultaneously changing all policy parameters, reducing required validation runs from exponential to linear with respect to the number of policy parameters. Comprehensive evaluations show that DOP establishes new state-of-the-art performance across 6 MLLMs and 13 benchmarks against 12 baselines. On LLaVA-Next-7B, DOP achieves 86\% TFLOPS reduction and 83\% latency reduction on real GPU with only 1\% performance loss. Our extensive ablation studies further demonstrate DOP's data and time efficiency as well as strong generalization capabilities.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning for Sustainable Rice Production: Region-Scale Monitoring of Water-Saving Practices in Punjab, India</title>
<link>https://arxiv.org/abs/2507.08605</link>
<guid>https://arxiv.org/abs/2507.08605</guid>
<content:encoded><![CDATA[
arXiv:2507.08605v2 Announce Type: replace 
Abstract: Rice cultivation supplies half the world's population with staple food, while also being a major driver of freshwater depletion--consuming roughly a quarter of global freshwater--and accounting for approx. 48% of greenhouse gas emissions from croplands. In regions like Punjab, India, where groundwater levels are plummeting at 41.6 cm/year, adopting water-saving rice farming practices is critical. Direct-Seeded Rice (DSR) and Alternate Wetting and Drying (AWD) can cut irrigation water use by 20-40% without hurting yields, yet lack of spatial data on adoption impedes effective adaptation policy and climate action. We present a machine learning framework to bridge this data gap by monitoring sustainable rice farming at scale. In collaboration with agronomy experts and a large-scale farmer training program, we obtained ground-truth data from 1,400 fields across Punjab. Leveraging this partnership, we developed a novel dimensional classification approach that decouples sowing and irrigation practices, achieving F1 scores of 0.8 and 0.74 respectively, solely employing Sentinel-1 satellite imagery. Explainability analysis reveals that DSR classification is robust while AWD classification depends primarily on planting schedule differences, as Sentinel-1's 12-day revisit frequency cannot capture the higher frequency irrigation cycles characteristic of AWD practices. Applying this model across 3 million fields reveals spatial heterogeneity in adoption at the state level, highlighting gaps and opportunities for policy targeting. Our district-level adoption rates correlate well with government estimates (Spearman's $\rho$=0.69 and Rank Biased Overlap=0.77). This study provides policymakers and sustainability programs a powerful tool to track practice adoption, inform targeted interventions, and drive data-driven policies for water conservation and climate mitigation at regional scale.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyperEvent: A Strong Baseline for Dynamic Link Prediction via Relative Structural Encoding</title>
<link>https://arxiv.org/abs/2507.11836</link>
<guid>https://arxiv.org/abs/2507.11836</guid>
<content:encoded><![CDATA[
arXiv:2507.11836v3 Announce Type: replace 
Abstract: Learning representations for continuous-time dynamic graphs is critical for dynamic link prediction. While recent methods have become increasingly complex, the field lacks a strong and informative baseline to reliably gauge progress. This paper proposes HyperEvent, a simple approach that captures relative structural patterns in event sequences through an intuitive encoding mechanism. As a straightforward baseline, HyperEvent leverages relative structural encoding to identify meaningful event sequences without complex parameterization. By combining these interpretable features with a lightweight transformer classifier, HyperEvent reframes link prediction as event structure recognition. Despite its simplicity, HyperEvent achieves competitive results across multiple benchmarks, often matching the performance of more complex models. This work demonstrates that effective modeling can be achieved through simple structural encoding, providing a clear reference point for evaluating future advancements.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Application-Specific Component-Aware Structured Pruning of Deep Neural Networks in Control via Soft Coefficient Optimization</title>
<link>https://arxiv.org/abs/2507.14882</link>
<guid>https://arxiv.org/abs/2507.14882</guid>
<content:encoded><![CDATA[
arXiv:2507.14882v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) offer significant flexibility and robust performance. This makes them ideal for building not only system models but also advanced neural network controllers (NNCs). However, their high complexity and computational needs often limit their use. Various model compression strategies have been developed over the past few decades to address these issues. These strategies are effective for general DNNs but do not directly apply to NNCs. NNCs need both size reduction and the retention of key application-specific performance features. In structured pruning, which removes groups of related elements, standard importance metrics often fail to protect these critical characteristics. In this paper, we introduce a novel framework for calculating importance metrics in pruning groups. This framework not only shrinks the model size but also considers various application-specific constraints. To find the best pruning coefficient for each group, we evaluate two approaches. The first approach involves simple exploration through grid search. The second utilizes gradient descent optimization, aiming to balance compression and task performance. We test our method in two use cases: one on an MNIST autoencoder and the other on a Temporal Difference Model Predictive Control (TDMPC) agent. Results show that the method effectively maintains application-relevant performance while achieving a significant reduction in model size.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Democratizing Tabular Data Access with an Open$\unicode{x2013}$Source Synthetic$\unicode{x2013}$Data SDK</title>
<link>https://arxiv.org/abs/2508.00718</link>
<guid>https://arxiv.org/abs/2508.00718</guid>
<content:encoded><![CDATA[
arXiv:2508.00718v2 Announce Type: replace 
Abstract: Machine learning development critically depends on access to high-quality data. However, increasing restrictions due to privacy, proprietary interests, and ethical concerns have created significant barriers to data accessibility. Synthetic data offers a viable solution by enabling safe, broad data usage without compromising sensitive information. This paper presents the MOSTLY AI Synthetic Data Software Development Kit (SDK), an open-source toolkit designed specifically for synthesizing high-quality tabular data. The SDK integrates robust features such as differential privacy guarantees, fairness-aware data generation, and automated quality assurance into a flexible and accessible Python interface. Leveraging the TabularARGN autoregressive framework, the SDK supports diverse data types and complex multi-table and sequential datasets, delivering competitive performance with notable improvements in speed and usability. Currently deployed both as a cloud service and locally installable software, the SDK has seen rapid adoption, highlighting its practicality in addressing real-world data bottlenecks and promoting widespread data democratization.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Sliced Fused Gromov-Wasserstein Distance</title>
<link>https://arxiv.org/abs/2508.02364</link>
<guid>https://arxiv.org/abs/2508.02364</guid>
<content:encoded><![CDATA[
arXiv:2508.02364v2 Announce Type: replace 
Abstract: The Gromov--Wasserstein (GW) distance and its fused extension (FGW) are powerful tools for comparing heterogeneous data. Their computation is, however, challenging since both distances are based on non-convex, quadratic optimal transport (OT) problems. Leveraging 1D OT, a sliced version of GW has been proposed to lower the computational burden. Unfortunately, this sliced version is restricted to Euclidean geometry and loses invariance to isometries, strongly limiting its application in practice. To overcome these issues, we propose a novel slicing technique for GW as well as for FGW that is based on an appropriate lower bound, hierarchical OT, and suitable quadrature rules for the underlying 1D OT problems. Our novel sliced FGW significantly reduces the numerical effort while remaining invariant to isometric transformations and allowing the comparison of arbitrary geometries. We show that our new distance actually defines a pseudo-metric for structured spaces that bounds FGW from below and study its interpolation properties between sliced Wasserstein and GW. Since we avoid the underlying quadratic program, our sliced distance is numerically more robust and reliable than the original GW and FGW distance; especially in the context of shape retrieval and graph isomorphism testing.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Deterministic and Stochastic Diffusion-Model Samplers: A Simple Analysis in Wasserstein Distance</title>
<link>https://arxiv.org/abs/2508.03210</link>
<guid>https://arxiv.org/abs/2508.03210</guid>
<content:encoded><![CDATA[
arXiv:2508.03210v2 Announce Type: replace 
Abstract: We provide new convergence guarantees in Wasserstein distance for diffusion-based generative models, covering both stochastic (DDPM-like) and deterministic (DDIM-like) sampling methods. We introduce a simple framework to analyze discretization, initialization, and score estimation errors. Notably, we derive the first Wasserstein convergence bound for the Heun sampler and improve existing results for the Euler sampler of the probability flow ODE. Our analysis emphasizes the importance of spatial regularity of the learned score function and argues for controlling the score error with respect to the true reverse process, in line with denoising score matching. We also incorporate recent results on smoothed Wasserstein distances to sharpen initialization error bounds.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control</title>
<link>https://arxiv.org/abs/2509.01720</link>
<guid>https://arxiv.org/abs/2509.01720</guid>
<content:encoded><![CDATA[
arXiv:2509.01720v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) using foundation models for policy approximations in multi-turn tasks remains challenging. We identify two main limitations related to sparse reward settings and policy gradient updates, based on which we formulate a key insight: updates from positive samples with high returns typically do not require policy regularisation, whereas updates from negative samples, reflecting undesirable behaviour, can harm model performance. This paper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RL algorithm evaluated on mobile app control tasks. SoLS improves sample efficiency when fine-tuning foundation models for user interface navigation via a modified off-policy actor-critic approach, applying direct policy updates for positive samples and conservative, regularised updates for negative ones to prevent model degradation. We augment SoLS with Successful Transition Replay (STR), which prioritises learning from successful interactions, further improving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark, where it significantly outperforms existing methods (at least 17% relative increase), including prompt-engineering and RL approaches, while requiring substantially fewer computational resources than GPT-4o-based methods with 5-60x faster inference.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
<link>https://arxiv.org/abs/2509.04449</link>
<guid>https://arxiv.org/abs/2509.04449</guid>
<content:encoded><![CDATA[
arXiv:2509.04449v2 Announce Type: replace 
Abstract: We present ChronoGraph, a graph-structured multivariate time series forecasting dataset built from real-world production microservices. Each node is a service that emits a multivariate stream of system-level performance metrics, capturing CPU, memory, and network usage patterns, while directed edges encode dependencies between services. The primary task is forecasting future values of these signals at the service level. In addition, ChronoGraph provides expert-annotated incident windows as anomaly labels, enabling evaluation of anomaly detection methods and assessment of forecast robustness during operational disruptions. Compared to existing benchmarks from industrial control systems or traffic and air-quality domains, ChronoGraph uniquely combines (i) multivariate time series, (ii) an explicit, machine-readable dependency graph, and (iii) anomaly labels aligned with real incidents. We report baseline results spanning forecasting models, pretrained time-series foundation models, and standard anomaly detectors. ChronoGraph offers a realistic benchmark for studying structure-aware forecasting and incident-aware evaluation in microservice systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning</title>
<link>https://arxiv.org/abs/2509.11816</link>
<guid>https://arxiv.org/abs/2509.11816</guid>
<content:encoded><![CDATA[
arXiv:2509.11816v2 Announce Type: replace 
Abstract: Current unlearning and safety training methods consistently fail to remove dangerous knowledge from language models. We identify the root cause - unlearning targets representations which are too general - and develop a highly selective technique that unlearns robustly while preserving general performance.
  Our method performs PCA on activations and module-output gradients to identify subspaces containing common representations, then collapses these subspaces before computing unlearning updates, a technique we term Collapse of Irrelevant Representations (CIR). This avoids unlearning general knowledge and targets only representations specific to the facts being unlearned.
  When unlearning bio- and cyber-hazardous facts from Llama-3.1-8B, we achieve over 30x greater reduction in post-attack accuracy than the best baseline (Circuit Breakers), while disrupting general performance 30x less, and using less than 3 GPU-seconds per fact.
  Thus, by disentangling harmful and benign capabilities at the level of representations, CIR enables robust and non-disruptive unlearning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference Offloading for Cost-Sensitive Binary Classification at the Edge</title>
<link>https://arxiv.org/abs/2509.15674</link>
<guid>https://arxiv.org/abs/2509.15674</guid>
<content:encoded><![CDATA[
arXiv:2509.15674v2 Announce Type: replace 
Abstract: We focus on a binary classification problem in an edge intelligence system where false negatives are more costly than false positives. The system has a compact, locally deployed model, which is supplemented by a larger, remote model, which is accessible via the network by incurring an offloading cost. For each sample, our system first uses the locally deployed model for inference. Based on the output of the local model, the sample may be offloaded to the remote model. This work aims to understand the fundamental trade-off between classification accuracy and the offloading costs within such a hierarchical inference (HI) system. To optimise this system, we propose an online learning framework that continuously adapts a pair of thresholds on the local model's confidence scores. These thresholds determine the prediction of the local model and whether a sample is classified locally or offloaded to the remote model. We present a closed-form solution for the setting where the local model is calibrated. For the more general case of uncalibrated models, we introduce H2T2, an online two-threshold hierarchical inference policy, and prove it achieves sublinear regret. H2T2 is model-agnostic, requires no training, and learns during the inference phase using limited feedback. Simulations on real-world datasets show that H2T2 consistently outperforms naive and single-threshold HI policies, sometimes even surpassing offline optima. The policy also demonstrates robustness to distribution shifts and adapts effectively to mismatched classifiers.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Clinical Classification with Kolgomorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2509.16750</link>
<guid>https://arxiv.org/abs/2509.16750</guid>
<content:encoded><![CDATA[
arXiv:2509.16750v2 Announce Type: replace 
Abstract: Why should a clinician trust an Artificial Intelligence (AI) prediction? Despite the increasing accuracy of machine learning methods in medicine, the lack of transparency continues to hinder their adoption in clinical practice. In this work, we explore Kolmogorov-Arnold Networks (KANs) for clinical classification tasks on tabular data. In contrast to traditional neural networks, KANs are function-based architectures that offer intrinsic interpretability through transparent, symbolic representations. We introduce \emph{Logistic-KAN}, a flexible generalization of logistic regression, and \emph{Kolmogorov-Arnold Additive Model (KAAM)}, a simplified additive variant that delivers transparent, symbolic formulas. Unlike ``black-box'' models that require post-hoc explainability tools, our models support built-in patient-level insights, intuitive visualizations, and nearest-patient retrieval. Across multiple health datasets, our models match or outperform standard baselines, while remaining fully interpretable. These results position KANs as a promising step toward trustworthy AI that clinicians can understand, audit, and act upon. We release the code for reproducibility in \codeurl.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Practical Multi-label Causal Discovery in High-Dimensional Event Sequences via One-Shot Graph Aggregation</title>
<link>https://arxiv.org/abs/2509.19112</link>
<guid>https://arxiv.org/abs/2509.19112</guid>
<content:encoded><![CDATA[
arXiv:2509.19112v2 Announce Type: replace 
Abstract: Understanding causality in event sequences where outcome labels such as diseases or system failures arise from preceding events like symptoms or error codes is critical. Yet remains an unsolved challenge across domains like healthcare or vehicle diagnostics. We introduce CARGO, a scalable multi-label causal discovery method for sparse, high-dimensional event sequences comprising of thousands of unique event types. Using two pretrained causal Transformers as domain-specific foundation models for event sequences. CARGO infers in parallel, per sequence one-shot causal graphs and aggregates them using an adaptive frequency fusion to reconstruct the global Markov boundaries of labels. This two-stage approach enables efficient probabilistic reasoning at scale while bypassing the intractable cost of full-dataset conditional independence testing. Our results on a challenging real-world automotive fault prediction dataset with over 29,100 unique event types and 474 imbalanced labels demonstrate CARGO's ability to perform structured reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Shot Multi-Label Causal Discovery in High-Dimensional Event Sequences</title>
<link>https://arxiv.org/abs/2509.23213</link>
<guid>https://arxiv.org/abs/2509.23213</guid>
<content:encoded><![CDATA[
arXiv:2509.23213v2 Announce Type: replace 
Abstract: Understanding causality in event sequences with thousands of sparse event types is critical in domains such as healthcare, cybersecurity, or vehicle diagnostics, yet current methods fail to scale. We present OSCAR, a one-shot causal autoregressive method that infers per-sequence Markov Boundaries using two pretrained Transformers as density estimators. This enables efficient, parallel causal discovery without costly global CI testing. On a real-world automotive dataset with 29,100 events and 474 labels, OSCAR recovers interpretable causal structures in minutes, while classical methods fail to scale, enabling practical scientific diagnostics at production scale.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Scale Latent Dynamics for Recurrent-Depth Transformers</title>
<link>https://arxiv.org/abs/2509.23314</link>
<guid>https://arxiv.org/abs/2509.23314</guid>
<content:encoded><![CDATA[
arXiv:2509.23314v2 Announce Type: replace 
Abstract: Recurrent-depth transformers scale test-time compute by iterating latent computations before emitting tokens. We study the geometry of these iterates and argue for a simple, two-scale operational picture: (i) within a looped block, updates act as small-scale refinements; (ii) across consecutive blocks, states undergo a larger-scale drift. Across training, our measurements show that loop steps become smaller and increasingly orthogonal to one another, indicating better local modeling of fine structure rather than merely pushing in a single direction. These dynamics motivate an early-exit mechanism based on the model's second-order difference in step-size, which we show is superior in terms of performance, stability and time-efficiency, when compared to the KL-divergence exit strategy of Geiping et al. and its naive first-order counterpart.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superposition disentanglement of neural representations reveals hidden alignment</title>
<link>https://arxiv.org/abs/2510.03186</link>
<guid>https://arxiv.org/abs/2510.03186</guid>
<content:encoded><![CDATA[
arXiv:2510.03186v2 Announce Type: replace 
Abstract: The superposition hypothesis states that single neurons may participate in representing multiple features in order for the neural network to represent more features than it has neurons. In neuroscience and AI, representational alignment metrics measure the extent to which different deep neural networks (DNNs) or brains represent similar information. In this work, we explore a critical question: does superposition interact with alignment metrics in any undesirable way? We hypothesize that models which represent the same features in different superposition arrangements, i.e., their neurons have different linear combinations of the features, will interfere with predictive mapping metrics (semi-matching, soft-matching, linear regression), producing lower alignment than expected. We develop a theory for how permutation metrics are dependent on superposition arrangements. This is tested by training sparse autoencoders (SAEs) to disentangle superposition in toy models, where alignment scores are shown to typically increase when a model's base neurons are replaced with its sparse overcomplete latent codes. We find similar increases for DNN-DNN and DNN-brain linear regression alignment in the visual domain. Our results suggest that superposition disentanglement is necessary for mapping metrics to uncover the true representational alignment between neural networks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Time Series Forecasting through Selective Representation Spaces: A Patch Perspective</title>
<link>https://arxiv.org/abs/2510.14510</link>
<guid>https://arxiv.org/abs/2510.14510</guid>
<content:encoded><![CDATA[
arXiv:2510.14510v3 Announce Type: replace 
Abstract: Time Series Forecasting has made significant progress with the help of Patching technique, which partitions time series into multiple patches to effectively retain contextual semantic information into a representation space beneficial for modeling long-term dependencies. However, conventional patching partitions a time series into adjacent patches, which causes a fixed representation space, thus resulting in insufficiently expressful representations. In this paper, we pioneer the exploration of constructing a selective representation space to flexibly include the most informative patches for forecasting. Specifically, we propose the Selective Representation Space (SRS) module, which utilizes the learnable Selective Patching and Dynamic Reassembly techniques to adaptively select and shuffle the patches from the contextual time series, aiming at fully exploiting the information of contextual time series to enhance the forecasting performance of patch-based models. To demonstrate the effectiveness of SRS module, we propose a simple yet effective SRSNet consisting of SRS and an MLP head, which achieves state-of-the-art performance on real-world datasets from multiple domains. Furthermore, as a novel plugin-and-play module, SRS can also enhance the performance of existing patch-based models. The resources are available at https://github.com/decisionintelligence/SRSNet.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BATIS: Bayesian Approaches for Targeted Improvement of Species Distribution Models</title>
<link>https://arxiv.org/abs/2510.19749</link>
<guid>https://arxiv.org/abs/2510.19749</guid>
<content:encoded><![CDATA[
arXiv:2510.19749v2 Announce Type: replace 
Abstract: Species distribution models (SDMs), which aim to predict species occurrence based on environmental variables, are widely used to monitor and respond to biodiversity change. Recent deep learning advances for SDMs have been shown to perform well on complex and heterogeneous datasets, but their effectiveness remains limited by spatial biases in the data. In this paper, we revisit deep SDMs from a Bayesian perspective and introduce BATIS, a novel and practical framework wherein prior predictions are updated iteratively using limited observational data. Models must appropriately capture both aleatoric and epistemic uncertainty to effectively combine fine-grained local insights with broader ecological patterns. We benchmark an extensive set of uncertainty quantification approaches on a novel dataset including citizen science observations from the eBird platform. Our empirical study shows how Bayesian deep learning approaches can greatly improve the reliability of SDMs in data-scarce locations, which can contribute to ecological understanding and conservation efforts.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Good GRACEs: Principled Teacher Selection for Knowledge Distillation</title>
<link>https://arxiv.org/abs/2511.02833</link>
<guid>https://arxiv.org/abs/2511.02833</guid>
<content:encoded><![CDATA[
arXiv:2511.02833v2 Announce Type: replace 
Abstract: Knowledge distillation is an efficient strategy to use data generated by large "teacher" language models to train smaller capable "student" models, but selecting the optimal teacher for a specific student-task combination requires expensive trial-and-error. We propose a lightweight score called GRACE to quantify how effective a teacher will be for post-training a student model. GRACE measures distributional properties of the student's gradients without access to a verifier, teacher logits, teacher internals, or test data. From an information-theoretic perspective, GRACE connects to leave-one-out stability of gradient-based algorithms, which controls the generalization performance of the distilled students. On GSM8K and MATH, GRACE correlates strongly (up to 86% Spearman correlation) with the performance of the distilled LLaMA and OLMo students. In particular, training a student using the GRACE-selected teacher can improve the performance by up to 7.4% over naively using the best-performing teacher. Further, GRACE can provide guidance on crucial design choices in distillation, including (1) the best temperature to use when generating from the teacher, (2) the best teacher to use given a size constraint, and (3) the best teacher to use within a specific model family. Altogether, our findings demonstrate that GRACE can efficiently and effectively identify a strongly compatible teacher for a given student and provide fine-grained guidance on how to perform distillation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ScaleDL: Towards Scalable and Efficient Runtime Prediction for Distributed Deep Learning Workloads</title>
<link>https://arxiv.org/abs/2511.04162</link>
<guid>https://arxiv.org/abs/2511.04162</guid>
<content:encoded><![CDATA[
arXiv:2511.04162v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) form the cornerstone of modern AI services, supporting a wide range of applications, including autonomous driving, chatbots, and recommendation systems. As models increase in size and complexity, DNN workloads such as training and inference tasks impose unprecedented demands on distributed computing resources, making accurate runtime prediction essential for optimizing development and resource allocation. Traditional methods rely on additive computational unit models, limiting their accuracy and generalizability. In contrast, graph-enhanced modeling improves performance but significantly increases data collection costs. Therefore, there is a critical need for a method that strikes a balance between accuracy, generalizability, and data collection costs. To address these challenges, we propose ScaleDL, a novel runtime prediction framework that combines nonlinear layer-wise modeling with graph neural network (GNN)-based cross-layer interaction mechanism, enabling accurate DNN runtime prediction and hierarchical generalizability across different network architectures. Additionally, we employ the D-optimal method to reduce data collection costs. Experiments on the workloads of five popular DNN models demonstrate that ScaleDL enhances runtime prediction accuracy and generalizability, achieving 6 times lower MRE and 5 times lower RMSE compared to baseline models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral methods for Neural Integral Equations</title>
<link>https://arxiv.org/abs/2312.05654</link>
<guid>https://arxiv.org/abs/2312.05654</guid>
<content:encoded><![CDATA[
arXiv:2312.05654v4 Announce Type: replace-cross 
Abstract: Neural integral equations are deep learning models based on the theory of integral equations, where the model consists of an integral operator and the corresponding equation (of the second kind) which is learned through an optimization procedure. This approach allows to leverage the nonlocal properties of integral operators in machine learning, but it is computationally expensive. In this article, we introduce a framework for neural integral equations based on spectral methods that allows us to learn an operator in the spectral domain, resulting in a cheaper computational cost, as well as in high interpolation accuracy. We study the properties of our methods and show various theoretical guarantees regarding the approximation capabilities of the model, and convergence to solutions of the numerical methods. We provide numerical experiments to demonstrate the practical effectiveness of the resulting model.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Scalable Black-Box Variational Inference with Structured Variational Families</title>
<link>https://arxiv.org/abs/2401.10989</link>
<guid>https://arxiv.org/abs/2401.10989</guid>
<content:encoded><![CDATA[
arXiv:2401.10989v4 Announce Type: replace-cross 
Abstract: Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean-field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}\left(N\right)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on large-scale hierarchical models.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Regression Trees Know Calculus</title>
<link>https://arxiv.org/abs/2405.13846</link>
<guid>https://arxiv.org/abs/2405.13846</guid>
<content:encoded><![CDATA[
arXiv:2405.13846v4 Announce Type: replace-cross 
Abstract: Regression trees have emerged as a preeminent tool for solving real-world regression problems due to their ability to deal with nonlinearities, interaction effects and sharp discontinuities. In this article, we rather study regression trees applied to well-behaved, differentiable functions, and determine the relationship between node parameters and the local gradient of the function being approximated. We find a simple estimate of the gradient which can be efficiently computed using quantities exposed by popular tree learning libraries. This allows the tools developed in the context of differentiable algorithms, like neural nets and Gaussian processes, to be deployed to tree-based models. To demonstrate this, we study measures of model sensitivity defined in terms of integrals of gradients and demonstrate how to compute them for regression trees using the proposed gradient estimates. Quantitative and qualitative numerical experiments reveal the capability of gradients estimated by regression trees to improve predictive analysis, solve tasks in uncertainty quantification, and provide interpretation of model behavior.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attri-Net: A Globally and Locally Inherently Interpretable Model for Multi-Label Classification Using Class-Specific Counterfactuals</title>
<link>https://arxiv.org/abs/2406.05477</link>
<guid>https://arxiv.org/abs/2406.05477</guid>
<content:encoded><![CDATA[
arXiv:2406.05477v2 Announce Type: replace-cross 
Abstract: Interpretability is crucial for machine learning algorithms in high-stakes medical applications. However, high-performing neural networks typically cannot explain their predictions. Post-hoc explanation methods provide a way to understand neural networks but have been shown to suffer from conceptual problems. Moreover, current research largely focuses on providing local explanations for individual samples rather than global explanations for the model itself. In this paper, we propose Attri-Net, an inherently interpretable model for multi-label classification that provides local and global explanations. Attri-Net first counterfactually generates class-specific attribution maps to highlight the disease evidence, then performs classification with logistic regression classifiers based solely on the attribution maps. Local explanations for each prediction can be obtained by interpreting the attribution maps weighted by the classifiers' weights. Global explanation of whole model can be obtained by jointly considering learned average representations of the attribution maps for each class (called the class centers) and the weights of the linear classifiers. To ensure the model is ``right for the right reason", we further introduce a mechanism to guide the model's explanations to align with human knowledge. Our comprehensive evaluations show that Attri-Net can generate high-quality explanations consistent with clinical knowledge while not sacrificing classification performance.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models</title>
<link>https://arxiv.org/abs/2410.08207</link>
<guid>https://arxiv.org/abs/2410.08207</guid>
<content:encoded><![CDATA[
arXiv:2410.08207v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models have achieved success in tasks like image generation and masked language modeling but face limitations in controlled content editing. We introduce DICE (Discrete Inversion for Controllable Editing), the first approach to enable precise inversion for discrete diffusion models, including multinomial diffusion and masked generative models. By recording noise sequences and masking patterns during the reverse diffusion process, DICE enables accurate reconstruction and flexible editing of discrete data without the need for predefined masks or attention manipulation. We demonstrate the effectiveness of DICE across both image and text domains, evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results show that DICE preserves high data fidelity while enhancing editing capabilities, offering new opportunities for fine-grained content manipulation in discrete spaces.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reducing the Scope of Language Models</title>
<link>https://arxiv.org/abs/2410.21597</link>
<guid>https://arxiv.org/abs/2410.21597</guid>
<content:encoded><![CDATA[
arXiv:2410.21597v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are deployed in a wide variety of user-facing applications. Typically, these deployments have some specific purpose, like answering questions grounded on documentation or acting as coding assistants, but they require general language understanding. In such deployments, LLMs should respond only to queries that align with the intended purpose and reject all other requests, such as generating poetry or answering questions about physics, a task we refer to as `scoping'. We conduct a comprehensive empirical evaluation of various methods, ranging from prompting, fine-tuning to preference learning and the recently proposed general alignment technique known as Circuit Breakers (CB). Across three families of language models and a broad variety of tasks, we show that it is possible to scope language models. We examine scoping for multiple topics, and fine-grained topics. We ablate diversity of irrelevant queries, layer different techniques, conduct adversarial evaluations and more. Among other results, we find that when diverse examples of irrelevant queries are available, simple supervised fine-tuning produces the best results, but when such diversity is low, Circuit Breakers perform quite well. One can often get the benefits of both methods by layering them in succession. We intend our study to serve as a practitioner's guide to scoping LLMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaloChallenge 2022: A Community Challenge for Fast Calorimeter Simulation</title>
<link>https://arxiv.org/abs/2410.21611</link>
<guid>https://arxiv.org/abs/2410.21611</guid>
<content:encoded><![CDATA[
arXiv:2410.21611v2 Announce Type: replace-cross 
Abstract: We present the results of the "Fast Calorimeter Simulation Challenge 2022" - the CaloChallenge. We study state-of-the-art generative models on four calorimeter shower datasets of increasing dimensionality, ranging from a few hundred voxels to a few tens of thousand voxels. The 31 individual submissions span a wide range of current popular generative architectures, including Variational AutoEncoders (VAEs), Generative Adversarial Networks (GANs), Normalizing Flows, Diffusion models, and models based on Conditional Flow Matching. We compare all submissions in terms of quality of generated calorimeter showers, as well as shower generation time and model size. To assess the quality we use a broad range of different metrics including differences in 1-dimensional histograms of observables, KPD/FPD scores, AUCs of binary classifiers, and the log-posterior of a multiclass classifier. The results of the CaloChallenge provide the most complete and comprehensive survey of cutting-edge approaches to calorimeter fast simulation to date. In addition, our work provides a uniquely detailed perspective on the important problem of how to evaluate generative models. As such, the results presented here should be applicable for other domains that use generative AI and require fast and faithful generation of samples in a large phase space.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image-based Outlier Synthesis With Training Data</title>
<link>https://arxiv.org/abs/2411.10794</link>
<guid>https://arxiv.org/abs/2411.10794</guid>
<content:encoded><![CDATA[
arXiv:2411.10794v4 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection is critical to ensure the safe deployment of deep learning models in critical applications. Deep learning models can often misidentify OOD samples as in-distribution (ID) samples. This vulnerability worsens in the presence of spurious correlation in the training set. Likewise, in fine-grained classification settings, detection of fine-grained OOD samples becomes inherently challenging due to their high similarity to ID samples. However, current research on OOD detection has focused instead largely on relatively easier (conventional) cases. Even the few recent works addressing these challenging cases rely on carefully curated or synthesized outliers, ultimately requiring external data. This motivates our central research question: ``Can we innovate OOD detection training framework for fine-grained and spurious settings \textbf{without requiring any external data at all?}" In this work, we present a unified \textbf{A}pproach to \textbf{S}purious, fine-grained, and \textbf{C}onventional \textbf{OOD D}etection (\textbf{\ASCOOD}) that eliminates the reliance on external data. First, we synthesize virtual outliers from ID data by approximating the destruction of invariant features. Specifically, we propose to add gradient attribution values to ID inputs to disrupt invariant features while amplifying true-class logit, thereby synthesizing challenging near-manifold virtual outliers. Then, we simultaneously incentivize ID classification and predictive uncertainty towards virtual outliers. For this, we further propose to leverage standardized features with z-score normalization. ASCOOD effectively mitigates impact of spurious correlations and encourages capturing fine-grained attributes. Extensive experiments across \textbf{7} datasets and and comparisons with \textbf{30+} methods demonstrate merit of ASCOOD in spurious, fine-grained and conventional settings.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Evaluation Choices Distort the Outcome of Generative Drug Discovery</title>
<link>https://arxiv.org/abs/2501.05457</link>
<guid>https://arxiv.org/abs/2501.05457</guid>
<content:encoded><![CDATA[
arXiv:2501.05457v2 Announce Type: replace-cross 
Abstract: "How to evaluate the de novo designs proposed by a generative model?" Despite the transformative potential of generative deep learning in drug discovery, this seemingly simple question has no clear answer. The absence of standardized guidelines challenges both the benchmarking of generative approaches and the selection of molecules for prospective studies. In this work, we take a fresh - critical and constructive - perspective on de novo design evaluation. By training chemical language models, we analyze approximately 1 billion molecule designs and discover principles consistent across different neural networks and datasets. We uncover a key confounder: the size of the generated molecular library significantly impacts evaluation outcomes, often leading to misleading model comparisons. We find increasing the number of designs as a remedy and propose new and compute-efficient metrics to compute at large-scale. We also identify critical pitfalls in commonly used metrics - such as uniqueness and distributional similarity - that can distort assessments of generative performance. To address these issues, we propose new and refined strategies for reliable model comparison and design evaluation. Furthermore, when examining molecule selection and sampling strategies, our findings reveal the constraints to diversify the generated libraries and draw new parallels and distinctions between deep learning and drug discovery. We anticipate our findings to help reshape evaluation pipelines in generative drug discovery, paving the way for more reliable and reproducible generative modeling approaches.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Higher-Order Couplings with Neural Networks</title>
<link>https://arxiv.org/abs/2501.06108</link>
<guid>https://arxiv.org/abs/2501.06108</guid>
<content:encoded><![CDATA[
arXiv:2501.06108v5 Announce Type: replace-cross 
Abstract: Maximum entropy methods, rooted in the inverse Ising/Potts problem from statistical physics, are widely used to model pairwise interactions in complex systems across disciplines such as bioinformatics and neuroscience. While successful, these approaches often fail to capture higher-order interactions that are critical for understanding collective behavior. In contrast, modern machine learning methods can model such interactions, but their interpretability often comes at a prohibitive computational cost. Restricted Boltzmann Machines (RBMs) provide a computationally efficient alternative by encoding statistical correlations through hidden units in a bipartite architecture. In this work, we introduce a method that maps RBMs onto generalized Potts models, enabling the systematic extraction of interactions up to arbitrary order. Leveraging large-$N$ approximations, made tractable by the RBM's structure, we extract effective many-body couplings with minimal computational effort. We further propose a robust framework for recovering higher-order interactions in more complex generative models, and introduce a simple gauge-fixing scheme for the effective Potts representation. Validation on synthetic data demonstrates accurate recovery of two- and three-body interactions. Applied to protein sequence data, our method reconstructs contact maps with high fidelity and outperforms state-of-the-art inverse Potts models. These results establish RBMs as a powerful and efficient tool for modeling higher-order structure in high-dimensional categorical data.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semiparametric Double Reinforcement Learning with Applications to Long-Term Causal Inference</title>
<link>https://arxiv.org/abs/2501.06926</link>
<guid>https://arxiv.org/abs/2501.06926</guid>
<content:encoded><![CDATA[
arXiv:2501.06926v4 Announce Type: replace-cross 
Abstract: Double Reinforcement Learning (DRL) enables efficient inference for policy values in nonparametric Markov decision processes (MDPs), but existing methods face two major obstacles: (1) they require stringent intertemporal overlap conditions on state trajectories, and (2) they rely on estimating high-dimensional occupancy density ratios. Motivated by problems in long-term causal inference, we extend DRL to a semiparametric setting and develop doubly robust, automatic estimators for general linear functionals of the Q-function in infinite-horizon, time-homogeneous MDPs. By imposing structure on the Q-function, we relax the overlap conditions required by nonparametric methods and obtain efficiency gains. The second obstacle--density-ratio estimation--typically requires computationally expensive and unstable min-max optimization. To address both challenges, we introduce superefficient nonparametric estimators whose limiting variance falls below the generalized Cramer-Rao bound. These estimators treat the Q-function as a one-dimensional summary of the state-action process, reducing high-dimensional overlap requirements to a single-dimensional condition. The procedure is simple to implement: estimate and calibrate the Q-function using fitted Q-iteration, then plug the result into the target functional, thereby avoiding density-ratio estimation altogether.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked Images</title>
<link>https://arxiv.org/abs/2502.00266</link>
<guid>https://arxiv.org/abs/2502.00266</guid>
<content:encoded><![CDATA[
arXiv:2502.00266v2 Announce Type: replace-cross 
Abstract: Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies perceptual inputs, potentially offering significant advantages in concept learning with large-scale Transformer models. To this end, we propose Multi-layer Concept Map (MCM), the first work to devise an efficient concept learning method based on masked images. In particular, we introduce an asymmetric concept learning architecture by establishing correlations between different encoder and decoder layers, updating concept tokens using backward gradients from reconstruction tasks. The learned concept tokens at various levels of granularity help either reconstruct the masked image patches by filling in gaps or guide the reconstruction results in a direction that reflects specific concepts. Moreover, we present both quantitative and qualitative results across a wide range of metrics, demonstrating that MCM significantly reduces computational costs by training on fewer than 75% of the total image patches while enhancing concept prediction performance. Additionally, editing specific concept tokens in the latent space enables targeted image generation from masked images, aligning both the visible contextual patches and the provided concepts. By further adjusting the testing time mask ratio, we could produce a range of reconstructions that blend the visible patches with the provided concepts, proportional to the chosen ratios.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance Reduction via Resampling and Experience Replay</title>
<link>https://arxiv.org/abs/2502.00520</link>
<guid>https://arxiv.org/abs/2502.00520</guid>
<content:encoded><![CDATA[
arXiv:2502.00520v2 Announce Type: replace-cross 
Abstract: Experience replay is a foundational technique in reinforcement learning that enhances learning stability by storing past experiences in a replay buffer and reusing them during training. Despite its practical success, its theoretical properties remain underexplored. In this paper, we present a theoretical framework that models experience replay using resampled $U$- and $V$-statistics, providing rigorous variance reduction guarantees. We apply this framework to policy evaluation tasks using the Least-Squares Temporal Difference (LSTD) algorithm and a Partial Differential Equation (PDE)-based model-free algorithm, demonstrating significant improvements in stability and efficiency, particularly in data-scarce scenarios. Beyond policy evaluation, we extend the framework to kernel ridge regression, showing that the experience replay-based method reduces the computational cost from the traditional $O(n^3)$ in time to as low as $O(n^2)$ in time while simultaneously reducing variance. Extensive numerical experiments validate our theoretical findings, demonstrating the broad applicability and effectiveness of experience replay in diverse machine learning tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Negative Dependence as a toolbox for machine learning : review and new developments</title>
<link>https://arxiv.org/abs/2502.07285</link>
<guid>https://arxiv.org/abs/2502.07285</guid>
<content:encoded><![CDATA[
arXiv:2502.07285v3 Announce Type: replace-cross 
Abstract: Negative dependence is becoming a key driver in advancing learning capabilities beyond the limits of traditional independence. Recent developments have evidenced support towards negatively dependent systems as a learning paradigm in a broad range of fundamental machine learning challenges including optimization, sampling, dimensionality reduction and sparse signal recovery, often surpassing the performance of current methods based on statistical independence. The most popular negatively dependent model has been that of determinantal point processes (DPPs), which have their origins in quantum theory. However, other models, such as perturbed lattice models, strongly Rayleigh measures, zeros of random functions have gained salience in various learning applications. In this article, we review this burgeoning field of research, as it has developed over the past two decades or so. We also present new results on applications of DPPs to the parsimonious representation of neural networks. In the limited scope of the article, we mostly focus on aspects of this area to which the authors contributed over the recent years, including applications to Monte Carlo methods, coresets and stochastic gradient descent, stochastic networks, signal processing and connections to quantum computation. However, starting from basics of negative dependence for the uninitiated reader, extensive references are provided to a broad swath of related developments which could not be covered within our limited scope. While existing works and reviews generally focus on specific negatively dependent models (e.g. DPPs), a notable feature of this article is that it addresses negative dependence as a machine learning methodology as a whole. In this vein, it covers within its span an array of negatively dependent models and their applications well beyond DPPs, thereby putting forward a very general and rather unique perspective.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Foundational Atomistic Models Reliable for Finite-Temperature Molecular Dynamics?</title>
<link>https://arxiv.org/abs/2503.08207</link>
<guid>https://arxiv.org/abs/2503.08207</guid>
<content:encoded><![CDATA[
arXiv:2503.08207v3 Announce Type: replace-cross 
Abstract: Machine learning force fields have emerged as promising tools for molecular dynamics (MD) simulations, potentially offering quantum-mechanical accuracy with the efficiency of classical MD. Inspired by foundational large language models, recent years have seen considerable progress in developing foundational atomistic models, sometimes referred to as universal force fields, designed to cover most elements in the periodic table. This Perspective adopts a practitioner's viewpoint to ask a critical question: Are these foundational atomistic models reliable for one of their most compelling applications, in particular simulating finite-temperature dynamics? Instead of a broad benchmark, we use the canonical ferroelectric-paraelectric phase transition in PbTiO$_3$ as a focused case study to evaluate prominent foundational atomistic models. Our findings suggest a potential disconnect between static accuracy and dynamic reliability. While 0 K properties are often well-reproduced, we observed that the models can struggle to consistently capture the correct phase transition, sometimes exhibiting simulation instabilities. We believe these challenges may stem from inherent biases in training data and a limited description of anharmonicity. These observed shortcomings, though demonstrated on a single system, appear to point to broader, systemic challenges that can be addressed with targeted fine-tuning. This Perspective serves not to rank models, but to initiate a crucial discussion on the practical readiness of foundational atomistic models and to explore future directions for their improvement.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForAug: Recombining Foregrounds and Backgrounds to Improve Vision Transformer Training with Bias Mitigation</title>
<link>https://arxiv.org/abs/2503.09399</link>
<guid>https://arxiv.org/abs/2503.09399</guid>
<content:encoded><![CDATA[
arXiv:2503.09399v2 Announce Type: replace-cross 
Abstract: Transformers, particularly Vision Transformers (ViTs), have achieved state-of-the-art performance in large-scale image classification. However, they often require large amounts of data and can exhibit biases that limit their robustness and generalizability. This paper introduces ForAug, a novel data augmentation scheme that addresses these challenges and explicitly includes inductive biases, which commonly are part of the neural network architecture, into the training data. ForAug is constructed by using pretrained foundation models to separate and recombine foreground objects with different backgrounds, enabling fine-grained control over image composition during training. It thus increases the data diversity and effective number of training samples. We demonstrate that training on ForNet, the application of ForAug to ImageNet, significantly improves the accuracy of ViTs and other architectures by up to 4.5 percentage points (p.p.) on ImageNet and 7.3 p.p. on downstream tasks. Importantly, ForAug enables novel ways of analyzing model behavior and quantifying biases. Namely, we introduce metrics for background robustness, foreground focus, center bias, and size bias and show that training on ForNet substantially reduces these biases compared to training on ImageNet. In summary, ForAug provides a valuable tool for analyzing and mitigating biases, enabling the development of more robust and reliable computer vision models. Our code and dataset are publicly available at https://github.com/tobna/ForAug.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Evaluation of Secure Code Generation</title>
<link>https://arxiv.org/abs/2503.15554</link>
<guid>https://arxiv.org/abs/2503.15554</guid>
<content:encoded><![CDATA[
arXiv:2503.15554v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are widely used in software development. However, the code generated by LLMs often contains vulnerabilities. Several secure code generation methods have been proposed to address this issue, but their current evaluation schemes leave several concerns unaddressed. Specifically, most existing studies evaluate security and functional correctness separately, using different datasets. That is, they assess vulnerabilities using security-related code datasets while validating functionality with general code datasets. In addition, prior research primarily relies on a single static analyzer, CodeQL, to detect vulnerabilities in generated code, which limits the scope of security evaluation.
  In this work, we conduct a comprehensive study to systematically assess the improvements introduced by four state-of-the-art secure code generation techniques. Specifically, we apply both security inspection and functionality validation to the same generated code and evaluate these two aspects together. We also employ three popular static analyzers and two LLMs to identify potential vulnerabilities in the generated code. Our study reveals that existing techniques often compromise the functionality of generated code to enhance security. Their overall performance remains limited when evaluating security and functionality together. In fact, many techniques even degrade the performance of the base LLM by more than 50%. Our further inspection reveals that these techniques often either remove vulnerable lines of code entirely or generate ``garbage code'' that is unrelated to the intended task. Moreover, the commonly used static analyzer CodeQL fails to detect several vulnerabilities, further obscuring the actual security improvements achieved by existing techniques.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth Matters: Multimodal RGB-D Perception for Robust Autonomous Agents</title>
<link>https://arxiv.org/abs/2503.16711</link>
<guid>https://arxiv.org/abs/2503.16711</guid>
<content:encoded><![CDATA[
arXiv:2503.16711v3 Announce Type: replace-cross 
Abstract: Autonomous agents that rely purely on perception to make real-time control decisions require efficient and robust architectures. In this work, we demonstrate that augmenting RGB input with depth information significantly enhances our agents' ability to predict steering commands compared to using RGB alone. We benchmark lightweight recurrent controllers that leverage the fused RGB-D features for sequential decision-making. To train our models, we collect high-quality data using a small-scale autonomous car controlled by an expert driver via a physical steering wheel, capturing varying levels of steering difficulty. Our models were successfully deployed on real hardware and inherently avoided dynamic and static obstacles, under out-of-distribution conditions. Specifically, our findings reveal that the early fusion of depth data results in a highly robust controller, which remains effective even with frame drops and increased noise levels, without compromising the network's focus on the task.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning</title>
<link>https://arxiv.org/abs/2506.01939</link>
<guid>https://arxiv.org/abs/2506.01939</guid>
<content:encoded><![CDATA[
arXiv:2506.01939v2 Announce Type: replace-cross 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonlinear Causal Discovery through a Sequential Edge Orientation Approach</title>
<link>https://arxiv.org/abs/2506.05590</link>
<guid>https://arxiv.org/abs/2506.05590</guid>
<content:encoded><![CDATA[
arXiv:2506.05590v2 Announce Type: replace-cross 
Abstract: Recent advances have established the identifiability of a directed acyclic graph (DAG) under additive noise models (ANMs), spurring the development of various causal discovery methods. However, most existing methods make restrictive model assumptions, rely heavily on general independence tests, or require substantial computational time. To address these limitations, we propose a sequential procedure to orient undirected edges in a completed partial DAG (CPDAG), representing an equivalence class of DAGs, by leveraging the pairwise additive noise model (PANM) to identify their causal directions. We prove that this procedure can recover the true causal DAG assuming a restricted ANM. Building on this result, we develop a novel constraint-based algorithm for learning causal DAGs under nonlinear ANMs. Given an estimated CPDAG, we develop a ranking procedure that sorts undirected edges by their adherence to the PANM, which defines an evaluation order of the edges. To determine the edge direction, we devise a statistical test that compares the log-likelihood values, evaluated with respect to the competing directions, of a sub-graph comprising just the candidate nodes and their identified parents in the partial DAG. We further establish the structural learning consistency of our algorithm in the large-sample limit. Extensive experiments on synthetic and real-world datasets demonstrate that our method is computationally efficient, robust to model misspecification, and consistently outperforms many existing nonlinear DAG learning methods.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Onboard Mission Replanning for Adaptive Cooperative Multi-Robot Systems</title>
<link>https://arxiv.org/abs/2506.06094</link>
<guid>https://arxiv.org/abs/2506.06094</guid>
<content:encoded><![CDATA[
arXiv:2506.06094v4 Announce Type: replace-cross 
Abstract: Cooperative autonomous robotic systems have significant potential for executing complex multi-task missions across space, air, ground, and maritime domains. But they commonly operate in remote, dynamic and hazardous environments, requiring rapid in-mission adaptation without reliance on fragile or slow communication links to centralised compute. Fast, on-board replanning algorithms are therefore needed to enhance resilience. Reinforcement Learning shows strong promise for efficiently solving mission planning tasks when formulated as Travelling Salesperson Problems (TSPs), but existing methods: 1) are unsuitable for replanning, where agents do not start at a single location; 2) do not allow cooperation between agents; 3) are unable to model tasks with variable durations; or 4) lack practical considerations for on-board deployment. Here we define the Cooperative Mission Replanning Problem as a novel variant of multiple TSP with adaptations to overcome these issues, and develop a new encoder/decoder-based model using Graph Attention Networks and Attention Models to solve it effectively and efficiently. Using a simple example of cooperative drones, we show our replanner consistently (90% of the time) maintains performance within 10% of the state-of-the-art LKH3 heuristic solver, whilst running 85-370 times faster on a Raspberry Pi. This work paves the way for increased resilience in autonomous multi-agent systems.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic Empirical Study</title>
<link>https://arxiv.org/abs/2506.19794</link>
<guid>https://arxiv.org/abs/2506.19794</guid>
<content:encoded><![CDATA[
arXiv:2506.19794v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) hold promise in automating data analysis tasks, yet open-source models face significant limitations in these kinds of reasoning-intensive scenarios. In this work, we investigate strategies to enhance the data analysis capabilities of open-source LLMs. By curating a seed dataset of diverse, realistic scenarios, we evaluate model behavior across three core dimensions: data understanding, code generation, and strategic planning. Our analysis reveals three key findings: (1) Strategic planning quality serves as the primary determinant of model performance; (2) Interaction design and task complexity significantly influence reasoning capabilities; (3) Data quality demonstrates a greater impact than diversity in achieving optimal performance. We leverage these insights to develop a data synthesis methodology, demonstrating significant improvements in open-source LLMs' analytical reasoning capabilities. Code is available at https://github.com/zjunlp/DataMind.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transforming Calabi-Yau Constructions: Generating New Calabi-Yau Manifolds with Transformers</title>
<link>https://arxiv.org/abs/2507.03732</link>
<guid>https://arxiv.org/abs/2507.03732</guid>
<content:encoded><![CDATA[
arXiv:2507.03732v2 Announce Type: replace-cross 
Abstract: Fine, regular, and star triangulations (FRSTs) of four-dimensional reflexive polytopes give rise to toric varieties, within which generic anticanonical hypersurfaces yield smooth Calabi-Yau threefolds. We introduce CYTransformer, a deep learning model based on the transformer architecture, to automate the generation of FRSTs. We demonstrate that CYTransformer efficiently and unbiasedly samples FRSTs for polytopes across a range of sizes, and can self-improve through retraining on its own output. These results lay the foundation for AICY: a community-driven platform designed to combine self-improving machine learning models with a continuously expanding database to explore and catalog the Calabi-Yau landscape.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Surrogate Quantum Circuit Design for the Lattice Boltzmann Collision Operator</title>
<link>https://arxiv.org/abs/2507.12256</link>
<guid>https://arxiv.org/abs/2507.12256</guid>
<content:encoded><![CDATA[
arXiv:2507.12256v2 Announce Type: replace-cross 
Abstract: This study introduces a framework for learning a low-depth surrogate quantum circuit (SQC) that approximates the nonlinear, dissipative, and hence non-unitary Bhatnagar-Gross-Krook (BGK) collision operator in the lattice Boltzmann method (LBM) for the D2Q9 lattice. By appropriately selecting the quantum state encoding, circuit architecture, and measurement protocol, non-unitary dynamics emerge naturally within the physical population space. This approach removes the need for probabilistic algorithms relying on any ancilla qubits and post-selection to reproduce dissipation, or for multiple state copies to capture nonlinearity. The SQC is designed to preserve key physical properties of the BGK operator, including mass conservation, scale equivariance, and D8 equivariance, while momentum conservation is encouraged through penalization in the training loss. When compiled to the IBM Heron quantum processor's native gate set, assuming all-to-all qubit connectivity, the circuit requires only 724 native gates and operates locally on the velocity register, making it independent of the lattice size. The learned SQC is validated on two benchmark cases, the Taylor-Green vortex decay and the lid-driven cavity, showing accurate reproduction of vortex decay and flow recirculation. While integration of the SQC into a quantum LBM framework presently requires measurement and re-initialization at each timestep, the necessary steps towards a measurement-free formulation are outlined.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training</title>
<link>https://arxiv.org/abs/2507.20067</link>
<guid>https://arxiv.org/abs/2507.20067</guid>
<content:encoded><![CDATA[
arXiv:2507.20067v2 Announce Type: replace-cross 
Abstract: Inference-time alignment enables large language models (LLMs) to generate outputs aligned with end-user preferences without further training. Recent post-training methods achieve this by using small guidance models to modify token generation during inference. These methods typically optimize a reward function KL-regularized by the original LLM taken as the reference policy. A critical limitation, however, is their dependence on a pre-trained reward model, which requires fitting to human preference feedback--a potentially unstable process. In contrast, we introduce PITA, a novel framework that integrates preference feedback directly into the LLM's token generation, eliminating the need for a reward model. PITA learns a small preference-based guidance policy to modify token probabilities at inference time without LLM fine-tuning, reducing computational cost and bypassing the pre-trained reward model dependency. The problem is framed as identifying an underlying preference distribution, solved through stochastic search and iterative refinement of the preference-based guidance model. We evaluate PITA across diverse tasks, including mathematical reasoning and sentiment classification, demonstrating its effectiveness in aligning LLM outputs with user preferences.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Debiasing Machine Learning Predictions for Causal Inference Without Additional Ground Truth Data: "One Map, Many Trials" in Satellite-Driven Poverty Analysis</title>
<link>https://arxiv.org/abs/2508.01341</link>
<guid>https://arxiv.org/abs/2508.01341</guid>
<content:encoded><![CDATA[
arXiv:2508.01341v2 Announce Type: replace-cross 
Abstract: Machine learning models trained on Earth observation data, such as satellite imagery, have demonstrated significant promise in predicting household-level wealth indices, enabling the creation of high-resolution wealth maps that can be leveraged across multiple causal trials while addressing chronic data scarcity in global development research. However, because standard training objectives prioritize overall predictive accuracy, these predictions often suffer from shrinkage toward the mean, leading to attenuated estimates of causal treatment effects and limiting their utility in policy evaluations. Existing debiasing methods, such as Prediction-Powered Inference (PPI), can handle this attenuation bias but require additional fresh ground-truth data at the downstream stage of causal inference, which restricts their applicability in data-scarce environments. We introduce and evaluate two post-hoc correction methods -- Linear Calibration Correction (LCC) and a Tweedie's correction approach -- that substantially reduce shrinkage-induced prediction bias without relying on newly collected labeled data. LCC applies a simple linear transformation estimated on a held-out calibration split; Tweedie's method locally de-shrink predictions using density score estimates and a noise scale learned upstream. We provide practical diagnostics for when a correction is warranted and discuss practical limitations. Across analytical results, simulations, and experiments with Demographic and Health Surveys (DHS) data, both approaches reduce attenuation; Tweedie's correction yields nearly unbiased treatment-effect estimates, enabling a "one map, many trials" paradigm. Although we demonstrate on EO-ML wealth mapping, the methods are not geospatial-specific: they apply to any setting where imputed outcomes are reused downstream (e.g., pollution indices, population density, or LLM-derived indicators).
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimQFL: A Quantum Federated Learning Simulator with Real-Time Visualization</title>
<link>https://arxiv.org/abs/2508.12477</link>
<guid>https://arxiv.org/abs/2508.12477</guid>
<content:encoded><![CDATA[
arXiv:2508.12477v2 Announce Type: replace-cross 
Abstract: Quantum federated learning (QFL) is an emerging field that has the potential to revolutionize computation by taking advantage of quantum physics concepts in a distributed machine learning (ML) environment. However, the majority of available quantum simulators are primarily built for general quantum circuit simulation and do not include integrated support for machine learning tasks such as training, evaluation, and iterative optimization. Furthermore, designing and assessing quantum learning algorithms is still a difficult and resource-intensive task. Real-time updates are essential for observing model convergence, debugging quantum circuits, and making conscious choices during training with the use of limited resources. Furthermore, most current simulators fail to support the integration of user-specific data for training purposes, undermining the main purpose of using a simulator. In this study, we introduce SimQFL, a customized simulator that simplifies and accelerates QFL experiments in quantum network applications. SimQFL supports real-time, epoch-wise output development and visualization, allowing researchers to monitor the process of learning across each training round. Furthermore, SimQFL offers an intuitive and visually appealing interface that facilitates ease of use and seamless execution. Users can customize key variables such as the number of epochs, learning rates, number of clients, and quantum hyperparameters such as qubits and quantum layers, making the simulator suitable for various QFL applications. The system gives immediate feedback following each epoch by showing intermediate outcomes and dynamically illustrating learning curves. SimQFL is a practical and interactive platform enabling academics and developers to prototype, analyze, and tune quantum neural networks with greater transparency and control in distributed quantum networks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural-Network Chemical Emulator for First-Star Formation: Robust Iterative Predictions over a Wide Density Range</title>
<link>https://arxiv.org/abs/2508.16114</link>
<guid>https://arxiv.org/abs/2508.16114</guid>
<content:encoded><![CDATA[
arXiv:2508.16114v2 Announce Type: replace-cross 
Abstract: We present a neural-network emulator for the thermal and chemical evolution in Population III star formation. The emulator accurately reproduces the thermochemical evolution over a wide density range spanning 21 orders of magnitude (10$^{-3}$-10$^{18}$ cm$^{-3}$), tracking six primordial species: H, H$_2$, e$^{-}$, H$^{+}$, H$^{-}$, and H$_2^{+}$. To handle the broad dynamic range, we partition the density range into five subregions and train separate deep operator networks (DeepONets) in each region. When applied to randomly sampled thermochemical states, the emulator achieves relative errors below 10% in over 90% of cases for both temperature and chemical abundances (except for the rare species H$_2^{+}$). The emulator is roughly ten times faster on a CPU and more than 1000 times faster for batched predictions on a GPU, compared with conventional numerical integration. Furthermore, to ensure robust predictions under many iterations, we introduce a novel timescale-based update method, where a short-timestep update of each variable is computed by rescaling the predicted change over a longer timestep equal to its characteristic variation timescale. In one-zone collapse calculations, the results from the timescale-based method agree well with traditional numerical integration even with many iterations at a timestep as short as 10$^{-4}$ of the free-fall time. This proof-of-concept study suggests the potential for neural network-based chemical emulators to accelerate hydrodynamic simulations of star formation.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroLad: 2D-to-3D Microstructure Reconstruction and Generation via Latent Diffusion and Score Distillation</title>
<link>https://arxiv.org/abs/2508.20138</link>
<guid>https://arxiv.org/abs/2508.20138</guid>
<content:encoded><![CDATA[
arXiv:2508.20138v4 Announce Type: replace-cross 
Abstract: A major obstacle to establishing reliable structure-property (SP) linkages in materials engineering is the scarcity of diverse 3D microstructure datasets. Limited dataset availability and insufficient control over the analysis and design space restrict the variety of achievable microstructure morphologies, hindering progress in solving the inverse (property-to-structure) design problem. To address these challenges, we introduce MicroLad, a latent diffusion framework specifically designed for reconstructing 3D microstructures from 2D data. Trained on 2D images and employing multi-plane denoising diffusion sampling in the latent space, the framework reliably generates stable and coherent 3D volumes that remain statistically consistent with the original data. While this reconstruction capability enables dimensionality expansion (2D-to-3D) for generating statistically equivalent 3D samples from 2D data, effective exploration of microstructure design requires methods to guide the generation process toward specific objectives. To achieve this, MicroLad integrates score distillation sampling (SDS), which combines a differentiable score loss with microstructural descriptor-matching and property-alignment terms. This approach updates encoded 2D slices of the 3D volume in the latent space, enabling robust inverse-controlled 2D-to-3D microstructure generation. Consequently, the method facilitates exploration of an expanded 3D microstructure analysis and design space in terms of both microstructural descriptors and material properties.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RINO: Renormalization Group Invariance with No Labels</title>
<link>https://arxiv.org/abs/2509.07486</link>
<guid>https://arxiv.org/abs/2509.07486</guid>
<content:encoded><![CDATA[
arXiv:2509.07486v3 Announce Type: replace-cross 
Abstract: A common challenge with supervised machine learning (ML) in high energy physics (HEP) is the reliance on simulations for labeled data, which can often mismodel the underlying collision or detector response. To help mitigate this problem of domain shift, we propose RINO (Renormalization Group Invariance with No Labels), a self-supervised learning approach that can instead pretrain models directly on collision data, learning embeddings invariant to renormalization group flow scales. In this work, we pretrain a transformer-based model on jets originating from quantum chromodynamic (QCD) interactions from the JetClass dataset, emulating real QCD-dominated experimental data, and then finetune on the JetNet dataset -- emulating simulations -- for the task of identifying jets originating from top quark decays. RINO demonstrates improved generalization from the JetNet training data to JetClass data compared to supervised training on JetNet from scratch, demonstrating the potential for RINO pretraining on real collision data followed by fine-tuning on small, high-quality MC datasets, to improve the robustness of ML models in HEP.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations</title>
<link>https://arxiv.org/abs/2509.09651</link>
<guid>https://arxiv.org/abs/2509.09651</guid>
<content:encoded><![CDATA[
arXiv:2509.09651v2 Announce Type: replace-cross 
Abstract: We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Capabilities to Performance: Evaluating Key Functional Properties of LLM Architectures in Penetration Testing</title>
<link>https://arxiv.org/abs/2509.14289</link>
<guid>https://arxiv.org/abs/2509.14289</guid>
<content:encoded><![CDATA[
arXiv:2509.14289v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used to automate or augment penetration testing, but their effectiveness and reliability across attack phases remain unclear. We present a comprehensive evaluation of multiple LLM-based agents, from single-agent to modular designs, across realistic penetration testing scenarios, measuring empirical performance and recurring failure patterns. We also isolate the impact of five core functional capabilities via targeted augmentations: Global Context Memory (GCM), Inter-Agent Messaging (IAM), Context-Conditioned Invocation (CCI), Adaptive Planning (AP), and Real-Time Monitoring (RTM). These interventions support, respectively: (i) context coherence and retention, (ii) inter-component coordination and state management, (iii) tool use accuracy and selective execution, (iv) multi-step strategic planning, error detection, and recovery, and (v) real-time dynamic responsiveness. Our results show that while some architectures natively exhibit subsets of these properties, targeted augmentations substantially improve modular agent performance, especially in complex, multi-step, and real-time penetration testing tasks.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VisualMimic: Visual Humanoid Loco-Manipulation via Motion Tracking and Generation</title>
<link>https://arxiv.org/abs/2509.20322</link>
<guid>https://arxiv.org/abs/2509.20322</guid>
<content:encoded><![CDATA[
arXiv:2509.20322v2 Announce Type: replace-cross 
Abstract: Humanoid loco-manipulation in unstructured environments demands tight integration of egocentric perception and whole-body control. However, existing approaches either depend on external motion capture systems or fail to generalize across diverse tasks. We introduce VisualMimic, a visual sim-to-real framework that unifies egocentric vision with hierarchical whole-body control for humanoid robots. VisualMimic combines a task-agnostic low-level keypoint tracker -- trained from human motion data via a teacher-student scheme -- with a task-specific high-level policy that generates keypoint commands from visual and proprioceptive input. To ensure stable training, we inject noise into the low-level policy and clip high-level actions using human motion statistics. VisualMimic enables zero-shot transfer of visuomotor policies trained in simulation to real humanoid robots, accomplishing a wide range of loco-manipulation tasks such as box lifting, pushing, football dribbling, and kicking. Beyond controlled laboratory settings, our policies also generalize robustly to outdoor environments. Videos are available at: https://visualmimic.github.io .
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-to-Scene with Large Reasoning Models</title>
<link>https://arxiv.org/abs/2509.26091</link>
<guid>https://arxiv.org/abs/2509.26091</guid>
<content:encoded><![CDATA[
arXiv:2509.26091v2 Announce Type: replace-cross 
Abstract: Prompt-driven scene synthesis allows users to generate complete 3D environments from textual descriptions. Current text-to-scene methods often struggle with complex geometries and object transformations, and tend to show weak adherence to complex instructions. We address these limitations by introducing Reason-3D, a text-to-scene model powered by large reasoning models (LRMs). Reason-3D integrates object retrieval using captions covering physical, functional, and contextual attributes. Reason-3D then places the selected objects based on implicit and explicit layout constraints, and refines their positions with collision-aware spatial reasoning. Evaluated on instructions ranging from simple to complex indoor configurations, Reason-3D significantly outperforms previous methods in human-rated visual fidelity, adherence to constraints, and asset retrieval quality. Beyond its contribution to the field of text-to-scene generation, our work showcases the advanced spatial reasoning abilities of modern LRMs. Additionally, we release the codebase to further the research in object retrieval and placement with LRMs.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models</title>
<link>https://arxiv.org/abs/2510.01252</link>
<guid>https://arxiv.org/abs/2510.01252</guid>
<content:encoded><![CDATA[
arXiv:2510.01252v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are trained on massive, unstructured corpora, making it unclear which social patterns and biases they absorb and later reproduce. Existing evaluations typically examine outputs or activations, but rarely connect them back to the pre-training data. We introduce a pipeline that couples LLMs with sparse autoencoders (SAEs) to trace how different themes are encoded during training. As a controlled case study, we trained a GPT-style model on 37 nineteenth-century novels by ten female authors, a corpus centered on themes such as gender, marriage, class, and morality. By applying SAEs across layers and probing with eleven social and moral categories, we mapped sparse features to human-interpretable concepts. The analysis revealed stable thematic backbones (most prominently around gender and kinship) and showed how associations expand and entangle with depth. More broadly, we argue that the LLM+SAEs pipeline offers a scalable framework for auditing how cultural assumptions from the data are embedded in model representations.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computing Wasserstein Barycenters through Gradient Flows</title>
<link>https://arxiv.org/abs/2510.04602</link>
<guid>https://arxiv.org/abs/2510.04602</guid>
<content:encoded><![CDATA[
arXiv:2510.04602v2 Announce Type: replace-cross 
Abstract: Wasserstein barycenters provide a powerful tool for aggregating probability measures, while leveraging the geometry of their ambient space. Existing discrete methods suffer from poor scalability, as they require access to the complete set of samples from input measures. We address this issue by recasting the original barycenter problem as a gradient flow in the Wasserstein space. Our approach offers two advantages. First, we achieve scalability by sampling mini-batches from the input measures. Second, we incorporate functionals over probability measures, which regularize the barycenter problem through internal, potential, and interaction energies. We present two algorithms for empirical and Gaussian mixture measures, providing convergence guarantees under the Polyak-{\L}ojasiewicz inequality. Experimental validation on toy datasets and domain adaptation benchmarks show that our methods outperform previous discrete and neural net-based methods for computing Wasserstein barycenters.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis</title>
<link>https://arxiv.org/abs/2510.17425</link>
<guid>https://arxiv.org/abs/2510.17425</guid>
<content:encoded><![CDATA[
arXiv:2510.17425v2 Announce Type: replace-cross 
Abstract: Addressing climate change effectively requires more than cataloguing the number of policies in place; it calls for tools that can reveal their thematic priorities and their tangible impacts on development outcomes. Existing assessments often rely on qualitative descriptions or composite indices, which can mask crucial differences between key domains such as mitigation, adaptation, disaster risk management, and loss and damage. To bridge this gap, we develop a quantitative indicator of climate policy orientation by applying a multilingual transformer-based language model to official national policy documents, achieving a classification accuracy of 0.90 (F1-score). Linking these indicators with World Bank development data in panel regressions reveals that mitigation policies are associated with higher GDP and GNI; disaster risk management correlates with greater GNI and debt but reduced foreign direct investment; adaptation and loss and damage show limited measurable effects. This integrated NLP-econometric framework enables comparable, theme-specific analysis of climate governance, offering a scalable method to monitor progress, evaluate trade-offs, and align policy emphasis with development goals.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Current Detectors Catch Face-to-Voice Deepfake Attacks?</title>
<link>https://arxiv.org/abs/2510.21004</link>
<guid>https://arxiv.org/abs/2510.21004</guid>
<content:encoded><![CDATA[
arXiv:2510.21004v2 Announce Type: replace-cross 
Abstract: The rapid advancement of generative models has enabled the creation of increasingly stealthy synthetic voices, commonly referred to as audio deepfakes. A recent technique, FOICE [USENIX'24], demonstrates a particularly alarming capability: generating a victim's voice from a single facial image, without requiring any voice sample. By exploiting correlations between facial and vocal features, FOICE produces synthetic voices realistic enough to bypass industry-standard authentication systems, including WeChat Voiceprint and Microsoft Azure. This raises serious security concerns, as facial images are far easier for adversaries to obtain than voice samples, dramatically lowering the barrier to large-scale attacks. In this work, we investigate two core research questions: (RQ1) can state-of-the-art audio deepfake detectors reliably detect FOICE-generated speech under clean and noisy conditions, and (RQ2) whether fine-tuning these detectors on FOICE data improves detection without overfitting, thereby preserving robustness to unseen voice generators such as SpeechT5.
  Our study makes three contributions. First, we present the first systematic evaluation of FOICE detection, showing that leading detectors consistently fail under both standard and noisy conditions. Second, we introduce targeted fine-tuning strategies that capture FOICE-specific artifacts, yielding significant accuracy improvements. Third, we assess generalization after fine-tuning, revealing trade-offs between specialization to FOICE and robustness to unseen synthesis pipelines. These findings expose fundamental weaknesses in today's defenses and motivate new architectures and training protocols for next-generation audio deepfake detection.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Information Ordering and Differential Privacy</title>
<link>https://arxiv.org/abs/2511.01467</link>
<guid>https://arxiv.org/abs/2511.01467</guid>
<content:encoded><![CDATA[
arXiv:2511.01467v2 Announce Type: replace-cross 
Abstract: We study quantum differential privacy (QDP) by defining a notion of the order of informativeness between two pairs of quantum states. In particular, we show that if the hypothesis testing divergence of the one pair dominates over that of the other pair, then this dominance holds for every $f$-divergence. This approach completely characterizes $(\varepsilon,\delta)$-QDP mechanisms by identifying the most informative $(\varepsilon,\delta)$-DP quantum state pairs. We apply this to analyze the stability of quantum differentially private learning algorithms, generalizing classical results to the case $\delta>0$. Additionally, we study precise limits for privatized hypothesis testing and privatized quantum parameter estimation, including tight upper-bounds on the quantum Fisher information under QDP. Finally, we establish near-optimal contraction bounds for differentially private quantum channels with respect to the hockey-stick divergence.
]]></content:encoded>
<pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Certifiable Machine Learning-Based Pipeline to Predict Fatigue Life of Aircraft Structures</title>
<link>https://arxiv.org/abs/2509.10227</link>
<guid>https://arxiv.org/abs/2509.10227</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Fatigue Life Prediction, Aircraft, Aerospace, Wing

Summary:
- Fatigue life prediction is critical in aircraft design and operation for safety.
- Traditional methods are time-consuming and complex, involving multiple steps and collaboration.
- Machine learning offers faster and more accurate fatigue life estimation.
- A ML-based pipeline is presented for estimating fatigue life in aircraft wing locations.
- The pipeline reduces costly simulations, computational resources, and human effort. 

<br /><br />Summary: <div>
arXiv:2509.10227v3 Announce Type: replace 
Abstract: Fatigue life prediction is essential in both the design and operational phases of any aircraft, and in this sense safety in the aerospace industry requires early detection of fatigue cracks to prevent in-flight failures. Robust and precise fatigue life predictors are thus essential to ensure safety. Traditional engineering methods, while reliable, are time consuming and involve complex workflows, including steps such as conducting several Finite Element Method (FEM) simulations, deriving the expected loading spectrum, and applying cycle counting techniques like peak-valley or rainflow counting. These steps often require collaboration between multiple teams and tools, added to the computational time and effort required to achieve fatigue life predictions. Machine learning (ML) offers a promising complement to traditional fatigue life estimation methods, enabling faster iterations and generalization, providing quick estimates that guide decisions alongside conventional simulations.
  In this paper, we present a ML-based pipeline that aims to estimate the fatigue life of different aircraft wing locations given the flight parameters of the different missions that the aircraft will be operating throughout its operational life. We validate the pipeline in a realistic use case of fatigue life estimation, yielding accurate predictions alongside a thorough statistical validation and uncertainty quantification. Our pipeline constitutes a complement to traditional methodologies by reducing the amount of costly simulations and, thereby, lowering the required computational and human resources.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt-Based Safety Guidance Is Ineffective for Unlearned Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2511.04834</link>
<guid>https://arxiv.org/abs/2511.04834</guid>
<content:encoded><![CDATA[
<div> Concept inversion, text-to-image generative models, harmful content, defense performance, negative prompts

Summary:
Recent advancements in text-to-image generative models have raised concerns about the potential for generating harmful content when given malicious input prompts. Two main approaches have been proposed to address this issue: fine-tuning models to unlearn harmful concepts and training-free methods utilizing negative prompts. However, combining these approaches often results in minimal or decreased defense performance, indicating an inherent incompatibility between the two paradigms. In response, a novel method is proposed that replaces negative prompts with implicit negative embeddings obtained through concept inversion. This method requires no modification to existing approaches and can be seamlessly integrated into current pipelines. Experimental validation on nudity and violence benchmarks shows consistent improvements in defense success rate while maintaining the essential semantics of input prompts. The proposed method effectively mitigates the challenges of combining fine-tuning and training-free methods to enhance defense against harmful content generation. 

<br /><br />Summary: <div>
arXiv:2511.04834v2 Announce Type: replace 
Abstract: Recent advances in text-to-image generative models have raised concerns about their potential to produce harmful content when provided with malicious input text prompts. To address this issue, two main approaches have emerged: (1) fine-tuning the model to unlearn harmful concepts and (2) training-free guidance methods that leverage negative prompts. However, we observe that combining these two orthogonal approaches often leads to marginal or even degraded defense performance. This observation indicates a critical incompatibility between two paradigms, which hinders their combined effectiveness. In this work, we address this issue by proposing a conceptually simple yet experimentally robust method: replacing the negative prompts used in training-free methods with implicit negative embeddings obtained through concept inversion. Our method requires no modification to either approach and can be easily integrated into existing pipelines. We experimentally validate its effectiveness on nudity and violence benchmarks, demonstrating consistent improvements in defense success rate while preserving the core semantics of input prompts.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActiTect: A Generalizable Machine Learning Pipeline for REM Sleep Behavior Disorder Screening through Standardized Actigraphy</title>
<link>https://arxiv.org/abs/2511.05221</link>
<guid>https://arxiv.org/abs/2511.05221</guid>
<content:encoded><![CDATA[
<div> Keywords: isolated rapid eye movement sleep behavior disorder (iRBD), alpha-synucleinopathies, actigraphy recordings, machine learning, wearable devices 

Summary: 
The study presents ActiTect, an open-source machine learning tool for automated detection of isolated rapid eye movement sleep behavior disorder (iRBD) from actigraphy recordings. The tool utilizes robust preprocessing and automated sleep-wake detection to extract motion features characterizing activity patterns. Model development on a cohort of 78 individuals demonstrated strong discrimination (AUROC = 0.95) under cross-validation. Generalization was confirmed on blinded local and external test sets, with consistent performance across datasets. Key predictive features remained reproducible across datasets, supporting the final pooled multi-center model's robustness. The tool's open-source nature promotes widespread adoption and collaborative improvements in RBD detection using wearable devices. 

<br /><br />Summary: <div>
arXiv:2511.05221v2 Announce Type: replace 
Abstract: Isolated rapid eye movement sleep behavior disorder (iRBD) is a major prodromal marker of $\alpha$-synucleinopathies, often preceding the clinical onset of Parkinson's disease, dementia with Lewy bodies, or multiple system atrophy. While wrist-worn actimeters hold significant potential for detecting RBD in large-scale screening efforts by capturing abnormal nocturnal movements, they become inoperable without a reliable and efficient analysis pipeline. This study presents ActiTect, a fully automated, open-source machine learning tool to identify RBD from actigraphy recordings. To ensure generalizability across heterogeneous acquisition settings, our pipeline includes robust preprocessing and automated sleep-wake detection to harmonize multi-device data and extract physiologically interpretable motion features characterizing activity patterns. Model development was conducted on a cohort of 78 individuals, yielding strong discrimination under nested cross-validation (AUROC = 0.95). Generalization was confirmed on a blinded local test set (n = 31, AUROC = 0.86) and on two independent external cohorts (n = 113, AUROC = 0.84; n = 57, AUROC = 0.94). To assess real-world robustness, leave-one-dataset-out cross-validation across the internal and external cohorts demonstrated consistent performance (AUROC range = 0.84-0.89). A complementary stability analysis showed that key predictive features remained reproducible across datasets, supporting the final pooled multi-center model as a robust pre-trained resource for broader deployment. By being open-source and easy to use, our tool promotes widespread adoption and facilitates independent validation and collaborative improvements, thereby advancing the field toward a unified and generalizable RBD detection model using wearable devices.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models</title>
<link>https://arxiv.org/abs/2506.22552</link>
<guid>https://arxiv.org/abs/2506.22552</guid>
<content:encoded><![CDATA[
<div> Neural climate emulators, linear response theory, multiscale systems, reduced-order models, stochastic approaches <br />
Summary: This study explores the challenges in developing data-driven models of multiscale systems in climate science and applied mathematics. It suggests that neural climate emulators may struggle to reproduce forced responses due to the need for appropriate coarse-grained representations and parameterizations of unresolved processes. The study advocates for reduced-order models tailored to specific processes and scales as alternatives. A neural model developed for analyzing the joint variability of surface temperature field and radiative fluxes is able to infer a noise process from data, reproduce system probability distribution, and enable causal studies through forced responses. The study highlights the importance of using response theory as a framework for guiding model design in data-driven modeling of multiscale physical systems. <br /><br /> <div>
arXiv:2506.22552v5 Announce Type: replace-cross 
Abstract: A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green's function experiments. To investigate the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and probe causal mechanisms. We argue that the ability of multiscale systems' emulators to reproduce perturbed statistics depends critically on (i) identifying an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. For low-frequency climate dynamics, these insights highlight reduced-order models, tailored to specific processes and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application, developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system's probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. These results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory as a principled framework to guide model design.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning</title>
<link>https://arxiv.org/abs/2508.10419</link>
<guid>https://arxiv.org/abs/2508.10419</guid>
<content:encoded><![CDATA[
<div> Keywords: narrative comprehension, retrieval-augmented generation, long-context reasoning, dynamic memory, iterative querying<br /><br />Summary:<br /><br />This paper addresses the challenge of narrative comprehension in long stories and novels, which involve complex plotlines and evolving relationships. Traditional retrieval-augmented generation (RAG) methods are limited by their stateless, single-step retrieval processes that fail to capture interconnected relations effectively over long contexts. To overcome these limitations, the authors propose ComoRAG, a novel framework inspired by human cognition that treats narrative reasoning as an iterative process involving continuous interaction between new evidence acquisition and integration with past knowledge. ComoRAG performs multiple reasoning cycles, generating probing queries to explore new evidence and updating a dynamic global memory workspace that consolidates information. This dynamic memory supports coherent context formation, enabling more effective query resolution. The framework was evaluated on four demanding long-context narrative benchmarks consisting of over 200,000 tokens. Experimental results indicate that ComoRAG consistently outperforms strong RAG baselines, achieving up to an 11% relative improvement. Analysis further shows that ComoRAG excels particularly on complex queries that require comprehension of global context, demonstrating the effectiveness of its cognitively inspired, stateful retrieval paradigm. The authors have made the source code publicly available for further research and application. <div>
arXiv:2508.10419v3 Announce Type: replace-cross 
Abstract: Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and its high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods could fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition on reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global context comprehension, offering a principled, cognitively motivated paradigm towards retrieval-based stateful reasoning. Our framework is made publicly available at https://github.com/EternityJune25/ComoRAG.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-adaptive weighting and sampling for physics-informed neural networks</title>
<link>https://arxiv.org/abs/2511.05452</link>
<guid>https://arxiv.org/abs/2511.05452</guid>
<content:encoded><![CDATA[
<div> sampling, weighting, physics-informed neural networks, PDEs, training efficiency
Summary: 
The study introduces a hybrid method combining adaptive sampling and weighting to improve the performance of physics-informed neural networks (PINNs) for solving complex partial differential equations (PDEs). The adaptive sampling identifies training points in regions with rapid solution variation, while adaptive weighting balances convergence across points. Results demonstrate that using either method alone is inadequate for precise predictions, especially with limited training points. The effectiveness of each method varies depending on the problem. However, combining both strategies consistently enhances prediction accuracy and training efficiency, providing a robust solution for solving PDEs with PINNs. <div>
arXiv:2511.05452v2 Announce Type: replace-cross 
Abstract: Physics-informed deep learning has emerged as a promising framework for solving partial differential equations (PDEs). Nevertheless, training these models on complex problems remains challenging, often leading to limited accuracy and efficiency. In this work, we introduce a hybrid adaptive sampling and weighting method to enhance the performance of physics-informed neural networks (PINNs). The adaptive sampling component identifies training points in regions where the solution exhibits rapid variation, while the adaptive weighting component balances the convergence rate across training points. Numerical experiments show that applying only adaptive sampling or only adaptive weighting is insufficient to consistently achieve accurate predictions, particularly when training points are scarce. Since each method emphasizes different aspects of the solution, their effectiveness is problem dependent. By combining both strategies, the proposed framework consistently improves prediction accuracy and training efficiency, offering a more robust approach for solving PDEs with PINNs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Lightweight CNN-Attention-BiLSTM Architecture for Multi-Class Arrhythmia Classification on Standard and Wearable ECGs</title>
<link>https://arxiv.org/abs/2511.08650</link>
<guid>https://arxiv.org/abs/2511.08650</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiac arrhythmias, deep learning, ECG classification, class-weighted loss, real-time deployment

Summary: 
This study introduces a lightweight deep learning model for the early and accurate detection of cardiac arrhythmias from both 12-lead and single-lead ECGs. The model combines 1D Convolutional Neural Networks (CNN), attention mechanisms, and Bidirectional Long Short-Term Memory (BiLSTM) to classify arrhythmias with superior accuracy and F1-scores compared to baseline models. To address class imbalance, a class-weighted loss is implemented. With only 0.945 million parameters, the model is well-suited for real-time deployment in wearable health monitoring systems. The evaluation on the CPSC 2018 dataset shows promising results, showcasing the potential for efficient and effective arrhythmia classification using deep learning techniques.<br /><br />Summary: <div>
arXiv:2511.08650v1 Announce Type: new 
Abstract: Early and accurate detection of cardiac arrhythmias is vital for timely diagnosis and intervention. We propose a lightweight deep learning model combining 1D Convolutional Neural Networks (CNN), attention mechanisms, and Bidirectional Long Short-Term Memory (BiLSTM) for classifying arrhythmias from both 12-lead and single-lead ECGs. Evaluated on the CPSC 2018 dataset, the model addresses class imbalance using a class-weighted loss and demonstrates superior accuracy and F1- scores over baseline models. With only 0.945 million parameters, our model is well-suited for real-time deployment in wearable health monitoring systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Training Speed of Tiny Recursive Models via Curriculum Guided Adaptive Recursion</title>
<link>https://arxiv.org/abs/2511.08653</link>
<guid>https://arxiv.org/abs/2511.08653</guid>
<content:encoded><![CDATA[
<div> Recursive reasoning, curriculum learning, architectural depth, training efficiency, Sudoku

<br /><br />Summary:  
The paper introduces CGAR, an innovative training methodology for recursive reasoning models that significantly reduces computational cost by applying curriculum learning to architectural depth instead of traditional data order. It consists of two key components: Progressive Depth Curriculum, which gradually increases recursion depth during training to avoid early overfitting and reduce computational time, and Hierarchical Supervision Weighting, which assigns exponentially decreasing weights to supervision steps to match the diminishing gradient magnitude. On the Sudoku-Extreme dataset with over 423,000 puzzles, CGAR achieves a 1.71x speedup in training time, cutting it from 10.93 to 6.38 hours with only a minimal accuracy decrease of 0.63% (from 86.65% to 86.02%). Ablation studies show that Progressive Depth Curriculum alone provides a 2.26x speedup while maintaining competitive accuracy at 85.47%, representing a rare Pareto improvement where training efficiency and solution quality both increase. Additionally, models trained with CGAR demonstrate improved inference efficiency, achieving 100% halting accuracy and requiring 11% fewer reasoning steps. This work highlights how the application of an architectural depth curriculum can make training recursive reasoning models more accessible on modest hardware without sacrificing performance. The authors provide code and pretrained models publicly. <div>
arXiv:2511.08653v1 Announce Type: new 
Abstract: Recursive reasoning models achieve remarkable performance on complex reasoning tasks through iterative refinement, enabling tiny networks to match large language models thousands of times their size. However, training remains computationally expensive, prior work reporting approximately 36 GPU-hours per dataset, limiting broader adoption and research. We propose CGAR, a novel training methodology that applies curriculum learning to architectural depth rather than traditional data ordering. CGAR introduces two synergistic components: Progressive Depth Curriculum dynamically adjusts recursion depth from shallow to deep configurations during training, preventing early overfitting while reducing computational cost, and Hierarchical Supervision Weighting applies exponentially decaying importance to supervision steps, aligning loss weighting with observed gradient magnitude decay. On Sudoku-Extreme with 423,168 test puzzles, CGAR achieves 1.71x training speedup (10.93 to 6.38 hours, 42% cost reduction) with only 0.63% accuracy drop (86.65% to 86.02%). Systematic ablations reveal Progressive Depth Curriculum alone achieves 2.26x speedup with 85.47% accuracy, demonstrating a rare Pareto improvement where architectural curriculum simultaneously enhances training efficiency and solution quality. CGAR-trained models exhibit superior inference efficiency with 100% halting accuracy and 11% fewer reasoning steps. Our work demonstrates that principled curriculum on architectural depth enables efficient training of recursive reasoning models on modest hardware. Code and models: https://github.com/Kaleemullahqasim/CGAR and https://huggingface.co/Kaleemullah/trm-cgar-sudoku
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning the Basis: A Kolmogorov-Arnold Network Approach Embedding Green's Function Priors</title>
<link>https://arxiv.org/abs/2511.08655</link>
<guid>https://arxiv.org/abs/2511.08655</guid>
<content:encoded><![CDATA[
<div> Method of Moments, Rao-Wilton-Glisson basis, Kolmogorov-Arnold representation theorem, physics-informed neural network, electromagnetic modeling<br /><br />Summary:<br /><br />This paper addresses the limitations of the traditional Method of Moments (MoM) in electromagnetic modeling, which relies on static, geometry-fixed basis functions like the Rao-Wilton-Glisson (RWG) basis. The authors reframe electromagnetic modeling by introducing a learnable basis representation framework instead of solving for coefficients on a fixed basis. They reveal that the RWG basis can be viewed as a static, piecewise-linear instantiation of the Kolmogorov-Arnold representation theorem. Building on this insight, they propose PhyKAN, a physics-informed Kolmogorov-Arnold Network that generalizes RWG into a flexible, adaptive basis family. PhyKAN is derived from the Electric Field Integral Equation (EFIE) and combines a local KAN branch with a global branch incorporating Green’s function priors to ensure physical consistency in the model. Experimental results on canonical geometries demonstrate that PhyKAN achieves very low reconstruction errors (sub-0.01) and provides accurate, unsupervised radar cross section predictions. The approach offers an interpretable and physics-consistent connection between classical electromagnetic solvers and modern neural network architectures, potentially advancing both modeling accuracy and efficiency in computational electromagnetics. <div>
arXiv:2511.08655v1 Announce Type: new 
Abstract: The Method of Moments (MoM) is constrained by the usage of static, geometry-defined basis functions, such as the Rao-Wilton-Glisson (RWG) basis. This letter reframes electromagnetic modeling around a learnable basis representation rather than solving for the coefficients over a fixed basis. We first show that the RWG basis is essentially a static and piecewise-linear realization of the Kolmogorov-Arnold representation theorem. Inspired by this insight, we propose PhyKAN, a physics-informed Kolmogorov-Arnold Network (KAN) that generalizes RWG into a learnable and adaptive basis family. Derived from the EFIE, PhyKAN integrates a local KAN branch with a global branch embedded with Green's function priors to preserve physical consistency. It is demonstrated that, across canonical geometries, PhyKAN achieves sub-0.01 reconstruction errors as well as accurate, unsupervised radar cross section predictions, offering an interpretable, physics-consistent bridge between classical solvers and modern neural network models for electromagnetic modeling.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabPFN-2.5: Advancing the State of the Art in Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2511.08667</link>
<guid>https://arxiv.org/abs/2511.08667</guid>
<content:encoded><![CDATA[
<div> Keywords: TabPFN-2.5, tabular foundation model, TabArena benchmark, distillation engine, XGBoost

<br /><br />Summary:  
1. TabPFN-2.5 is the next-generation tabular foundation model designed to handle datasets with up to 50,000 data points and 2,000 features, representing a 20x increase in capacity over its predecessor, TabPFNv2.  
2. It leads the industry-standard TabArena benchmark, which includes datasets with up to 100,000 training points, surpassing tuned tree-based models and matching the performance of the heavily tuned AutoGluon 1.4 ensemble.  
3. On small to medium-sized classification datasets (≤10,000 points, 500 features), TabPFN-2.5 achieves a 100% win rate against default XGBoost, and maintains high win rates (87% classification, 85% regression) on larger datasets reaching 100K samples and 2K features.  
4. For production scenarios, a newly introduced distillation engine converts the large TabPFN-2.5 model into compact multilayer perceptrons (MLP) or tree ensembles that retain most of the accuracy but have drastically reduced latency and are easier to deploy.  
5. By releasing TabPFN-2.5, the authors expect to significantly improve the performance and efficiency of many existing applications and methods built on the TabPFN ecosystem, expanding its impact across diverse tabular AI use cases. <div>
arXiv:2511.08667v1 Announce Type: new 
Abstract: The first tabular foundation model, TabPFN, and its successor TabPFNv2 have impacted tabular AI substantially, with dozens of methods building on it and hundreds of applications across different use cases. This report introduces TabPFN-2.5, the next generation of our tabular foundation model, built for datasets with up to 50,000 data points and 2,000 features, a 20x increase in data cells compared to TabPFNv2. TabPFN-2.5 is now the leading method for the industry standard benchmark TabArena (which contains datasets with up to 100,000 training data points), substantially outperforming tuned tree-based models and matching the accuracy of AutoGluon 1.4, a complex four-hour tuned ensemble that even includes the previous TabPFNv2. Remarkably, default TabPFN-2.5 has a 100% win rate against default XGBoost on small to medium-sized classification datasets (<=10,000 data points, 500 features) and a 87% win rate on larger datasets up to 100K samples and 2K features (85% for regression). For production use cases, we introduce a new distillation engine that converts TabPFN-2.5 into a compact MLP or tree ensemble, preserving most of its accuracy while delivering orders-of-magnitude lower latency and plug-and-play deployment. This new release will immediately strengthen the performance of the many applications and methods already built on the TabPFN ecosystem.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEGNet: A Physics-Embedded Graph Network for Long-Term Stable Multiphysics Simulation</title>
<link>https://arxiv.org/abs/2511.08697</link>
<guid>https://arxiv.org/abs/2511.08697</guid>
<content:encoded><![CDATA[
<div> Graph Network, Physics-Embedded, PDE-guided, Data-driven, Multi-scale <br />
<br />Summary:
PEGNet is a novel Physics-Embedded Graph Network that enhances traditional numerical solvers by incorporating PDE-guided message passing. This new architecture embeds key PDE dynamics such as convection, viscosity, and diffusion into distinct message functions, ensuring better integration of physical constraints in forward propagation. PEGNet also employs a hierarchical structure to capture multi-scale features and includes physical regularization in the loss function to enforce adherence to governing physics. The model was evaluated on respiratory airflow and drug delivery datasets, showcasing improved long-term prediction accuracy and physical consistency compared to existing methods. This approach has the potential to revolutionize simulations of physical phenomena, offering accurate and efficient solutions for complex multiphysics problems. <div>
arXiv:2511.08697v1 Announce Type: new 
Abstract: Accurate and efficient simulations of physical phenomena governed by partial differential equations (PDEs) are important for scientific and engineering progress. While traditional numerical solvers are powerful, they are often computationally expensive. Recently, data-driven methods have emerged as alternatives, but they frequently suffer from error accumulation and limited physical consistency, especially in multiphysics and complex geometries. To address these challenges, we propose PEGNet, a Physics-Embedded Graph Network that incorporates PDE-guided message passing to redesign the graph neural network architecture. By embedding key PDE dynamics like convection, viscosity, and diffusion into distinct message functions, the model naturally integrates physical constraints into its forward propagation, producing more stable and physically consistent solutions. Additionally, a hierarchical architecture is employed to capture multi-scale features, and physical regularization is integrated into the loss function to further enforce adherence to governing physics. We evaluated PEGNet on benchmarks, including custom datasets for respiratory airflow and drug delivery, showing significant improvements in long-term prediction accuracy and physical consistency over existing methods. Our code is available at https://github.com/Yanghuoshan/PEGNet.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAIRPLAI: A Human-in-the-Loop Approach to Fair and Private Machine Learning</title>
<link>https://arxiv.org/abs/2511.08702</link>
<guid>https://arxiv.org/abs/2511.08702</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, differential privacy, human oversight, machine learning, stakeholder input<br /><br />Summary:<br /><br />This paper addresses the challenge of balancing accuracy, fairness, and privacy in machine learning systems that impact critical areas such as healthcare, finance, hiring, and public services. It notes that achieving all three objectives is complex since differential privacy may exacerbate disparities, fairness interventions often require sensitive data prohibited by privacy, and existing automated pipelines fail to incorporate human judgment in fairness decisions. To overcome these challenges, the authors introduce FAIRPLAI (Fair and Private Learning with Active Human Influence), a framework that integrates human oversight into machine learning design and deployment. FAIRPLAI operates by constructing privacy-fairness frontiers to transparently illustrate trade-offs between accuracy, privacy, and group outcomes. It also enables interactive input from stakeholders, allowing decision-makers to choose fairness criteria and operating points aligned with their specific domains. Additionally, FAIRPLAI includes a differentially private auditing loop, permitting humans to review explanations and edge cases securely without jeopardizing individual privacy. Experimental results on benchmark datasets demonstrate that FAIRPLAI maintains strong privacy guarantees while reducing fairness disparities compared to automated baselines. Ultimately, FAIRPLAI offers a user-friendly, interpretable approach for practitioners to simultaneously manage accuracy, privacy, and fairness, embedding human judgment where it is most crucial and promoting responsible, trustworthy machine learning applications. <div>
arXiv:2511.08702v1 Announce Type: new 
Abstract: As machine learning systems move from theory to practice, they are increasingly tasked with decisions that affect healthcare access, financial opportunities, hiring, and public services. In these contexts, accuracy is only one piece of the puzzle - models must also be fair to different groups, protect individual privacy, and remain accountable to stakeholders. Achieving all three is difficult: differential privacy can unintentionally worsen disparities, fairness interventions often rely on sensitive data that privacy restricts, and automated pipelines ignore that fairness is ultimately a human and contextual judgment. We introduce FAIRPLAI (Fair and Private Learning with Active Human Influence), a practical framework that integrates human oversight into the design and deployment of machine learning systems. FAIRPLAI works in three ways: (1) it constructs privacy-fairness frontiers that make trade-offs between accuracy, privacy guarantees, and group outcomes transparent; (2) it enables interactive stakeholder input, allowing decision-makers to select fairness criteria and operating points that reflect their domain needs; and (3) it embeds a differentially private auditing loop, giving humans the ability to review explanations and edge cases without compromising individual data security. Applied to benchmark datasets, FAIRPLAI consistently preserves strong privacy protections while reducing fairness disparities relative to automated baselines. More importantly, it provides a straightforward, interpretable process for practitioners to manage competing demands of accuracy, privacy, and fairness in socially impactful applications. By embedding human judgment where it matters most, FAIRPLAI offers a pathway to machine learning systems that are effective, responsible, and trustworthy in practice. GitHub: https://github.com/Li1Davey/Fairplai
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benevolent Dictators? On LLM Agent Behavior in Dictator Games</title>
<link>https://arxiv.org/abs/2511.08721</link>
<guid>https://arxiv.org/abs/2511.08721</guid>
<content:encoded><![CDATA[
<div> Keywords: ultimatum game, dictator game, Large Language Models, system prompt, fairness

Summary: 
The study introduces the LLM agent behavior study (LLM-ABS) framework to investigate the impact of system prompts on the behavior of Large Language Models in behavioral science experiments like the ultimatum and dictator games. By using neutral prompt variations, the framework aims to provide more reliable insights into agent preferences and analyze linguistic features in model responses. The research reveals that LLM agents often show a preference for fairness, highlighting the significant influence of the system prompt on their behavior. Linguistic analysis of model responses uncovers variations in expression patterns. Despite challenges related to prompt sensitivity, the proposed framework establishes a solid foundation for studying the behavior of LLM agents in complex scenarios. The code artifacts for the study are available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2511.08721v1 Announce Type: new 
Abstract: In behavioral sciences, experiments such as the ultimatum game are conducted to assess preferences for fairness or self-interest of study participants. In the dictator game, a simplified version of the ultimatum game where only one of two players makes a single decision, the dictator unilaterally decides how to split a fixed sum of money between themselves and the other player. Although recent studies have explored behavioral patterns of AI agents based on Large Language Models (LLMs) instructed to adopt different personas, we question the robustness of these results. In particular, many of these studies overlook the role of the system prompt - the underlying instructions that shape the model's behavior - and do not account for how sensitive results can be to slight changes in prompts. However, a robust baseline is essential when studying highly complex behavioral aspects of LLMs. To overcome previous limitations, we propose the LLM agent behavior study (LLM-ABS) framework to (i) explore how different system prompts influence model behavior, (ii) get more reliable insights into agent preferences by using neutral prompt variations, and (iii) analyze linguistic features in responses to open-ended instructions by LLM agents to better understand the reasoning behind their behavior. We found that agents often exhibit a strong preference for fairness, as well as a significant impact of the system prompt on their behavior. From a linguistic perspective, we identify that models express their responses differently. Although prompt sensitivity remains a persistent challenge, our proposed framework demonstrates a robust foundation for LLM agent behavior studies. Our code artifacts are available at https://github.com/andreaseinwiller/LLM-ABS.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Macroscopic Emission Modeling of Urban Traffic Using Probe Vehicle Data: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2511.08722</link>
<guid>https://arxiv.org/abs/2511.08722</guid>
<content:encoded><![CDATA[
<div> machine learning, urban congestion, emissions, traffic data, urban areas <br />
Summary: <br />
Urban congestion leads to inefficient vehicle movement, increased greenhouse gas emissions, and urban air pollution. A new study leverages large-scale traffic and emission data from probe vehicles in U.S. urban areas to predict network-wide emission rates using machine learning methods. This approach provides insights into the relationship between emissions and traffic variables, allowing for real-time monitoring of region-wide emissions and optimization of travel demand allocation to reduce congestion and emissions. The data-driven eMFD models developed in this study offer a deeper understanding of the factors influencing emission rates, such as network characteristics, infrastructure, land use, and vehicle features. These findings enable transportation authorities to measure carbon emissions from urban transport, make informed traffic management decisions, and plan strategies to mitigate network-wide emissions. <div>
arXiv:2511.08722v1 Announce Type: new 
Abstract: Urban congestions cause inefficient movement of vehicles and exacerbate greenhouse gas emissions and urban air pollution. Macroscopic emission fundamental diagram (eMFD)captures an orderly relationship among emission and aggregated traffic variables at the network level, allowing for real-time monitoring of region-wide emissions and optimal allocation of travel demand to existing networks, reducing urban congestion and associated emissions. However, empirically derived eMFD models are sparse due to historical data limitation. Leveraging a large-scale and granular traffic and emission data derived from probe vehicles, this study is the first to apply machine learning methods to predict the network wide emission rate to traffic relationship in U.S. urban areas at a large scale. The analysis framework and insights developed in this work generate data-driven eMFDs and a deeper understanding of their location dependence on network, infrastructure, land use, and vehicle characteristics, enabling transportation authorities to measure carbon emissions from urban transport of given travel demand and optimize location specific traffic management and planning decisions to mitigate network-wide emissions.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gromov-Wasserstein Graph Coarsening</title>
<link>https://arxiv.org/abs/2511.08733</link>
<guid>https://arxiv.org/abs/2511.08733</guid>
<content:encoded><![CDATA[
<div> Gromov-Wasserstein geometry, graph coarsening, Greedy Pair Coarsening, $k$-means Greedy Pair Coarsening, distortion measure

Summary:
The study focuses on graph coarsening using Gromov-Wasserstein geometry. Two algorithms, Greedy Pair Coarsening (GPC) and $k$-means Greedy Pair Coarsening (KGPC), are proposed to merge nodes based on distortion measures. GPC method iteratively merges nodes to minimize distortion locally, while KGPC clusters nodes based on pairwise distortion metrics. Optimal coarsening is guaranteed under certain conditions, and the algorithms are validated on large datasets and clustering tasks. Results demonstrate superior performance compared to existing methods across various parameters and scenarios. <div>
arXiv:2511.08733v1 Announce Type: new 
Abstract: We study the problem of graph coarsening within the Gromov-Wasserstein geometry. Specifically, we propose two algorithms that leverage a novel representation of the distortion induced by merging pairs of nodes. The first method, termed Greedy Pair Coarsening (GPC), iteratively merges pairs of nodes that locally minimize a measure of distortion until the desired size is achieved. The second method, termed $k$-means Greedy Pair Coarsening (KGPC), leverages clustering based on pairwise distortion metrics to directly merge clusters of nodes. We provide conditions guaranteeing optimal coarsening for our methods and validate their performance on six large-scale datasets and a downstream clustering task. Results show that the proposed methods outperform existing approaches on a wide range of parameters and scenarios.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hey Pentti, We Did (More of) It!: A Vector-Symbolic Lisp With Residue Arithmetic</title>
<link>https://arxiv.org/abs/2511.08767</link>
<guid>https://arxiv.org/abs/2511.08767</guid>
<content:encoded><![CDATA[
<div> FHRRs, VSA, Lisp 1.5, RHC, neural network states <br />
Summary: 
The article introduces Frequency-domain Holographic Reduced Representations (FHRRs) to extend a Vector-Symbolic Architecture (VSA) encoding of Lisp 1.5 with arithmetic operations using Residue Hyperdimensional Computing (RHC). By encoding a Turing-complete syntax in a high-dimensional vector space, the neural network states can have structured representations that are interpretable. The enhanced expressive power of the VSA encoding can be applied to machine learning tasks. The emphasis on encoding structured representations and designing neural networks that are sensitive to the structure of their representations is geared towards developing more intelligent agents. The goal is to create neural networks capable of handling complex structures and fostering more general intelligence. <br /><br /> <div>
arXiv:2511.08767v1 Announce Type: new 
Abstract: Using Frequency-domain Holographic Reduced Representations (FHRRs), we extend a Vector-Symbolic Architecture (VSA) encoding of Lisp 1.5 with primitives for arithmetic operations using Residue Hyperdimensional Computing (RHC). Encoding a Turing-complete syntax over a high-dimensional vector space increases the expressivity of neural network states, enabling network states to contain arbitrarily structured representations that are inherently interpretable. We discuss the potential applications of the VSA encoding in machine learning tasks, as well as the importance of encoding structured representations and designing neural networks whose behavior is sensitive to the structure of their representations in virtue of attaining more general intelligent agents than exist at present.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generalized Bias-Variance Decomposition for Bregman Divergences</title>
<link>https://arxiv.org/abs/2511.08789</link>
<guid>https://arxiv.org/abs/2511.08789</guid>
<content:encoded><![CDATA[
<div> Bias-variance decomposition, Bregman divergence, prediction error, exponential families, maximum likelihood estimation<br /><br />Summary:<br /><br />1. The article discusses the classical bias-variance decomposition, a foundational concept in statistics and machine learning, typically formulated for squared error loss.<br />2. It generalizes this decomposition to cases where the prediction error is measured by a Bregman divergence instead of squared error.<br />3. This generalization is especially important in the context of maximum likelihood estimation involving exponential family distributions, where Bregman divergences naturally arise.<br />4. Although the generalized bias-variance decomposition for Bregman divergences is known in the literature, a clear and self-contained derivation had not been previously available.<br />5. The author provides such a derivation primarily for pedagogical purposes, supplementing the note with additional discussion and references to relevant prior work, improving clarity and accessibility for readers. <div>
arXiv:2511.08789v1 Announce Type: new 
Abstract: The bias-variance decomposition is a central result in statistics and machine learning, but is typically presented only for the squared error. We present a generalization of the bias-variance decomposition where the prediction error is a Bregman divergence, which is relevant to maximum likelihood estimation with exponential families. While the result is already known, there was not previously a clear, standalone derivation, so we provide one for pedagogical purposes. A version of this note previously appeared on the author's personal website without context. Here we provide additional discussion and references to the relevant prior literature.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BayesQ: Uncertainty-Guided Bayesian Quantization</title>
<link>https://arxiv.org/abs/2511.08821</link>
<guid>https://arxiv.org/abs/2511.08821</guid>
<content:encoded><![CDATA[
<div> Keywords: BayesQ, post-training quantization, uncertainty, posterior expected loss, mixed precision<br /><br />Summary:<br /><br />1. The paper introduces BayesQ, a novel post-training quantization (PTQ) framework that optimizes quantization by minimizing the posterior expected loss, accounting for uncertainty in model weights. <br />2. BayesQ models weight uncertainty using a lightweight Gaussian posterior (default diagonal Laplace, with optional K-FAC/low-rank approximations) and whitens weights based on posterior covariance. <br />3. It designs quantization codebooks to minimize posterior-expected distortion and efficiently allocates mixed precision through a greedy knapsack strategy that maximizes marginal expected-loss reduction per bit under a global bit budget. <br />4. For scalar quantizers, closed-form tables derived from posterior-expected mean squared error (MSE) are used, while task-specific proxies are estimated via brief Monte Carlo runs on small calibration datasets. <br />5. An optional calibration-only distillation step is included to better align the quantized model with the posterior predictive teacher. <br />6. Experimental results show that at comparable average bits per weight (3.0/3.5/4.0), BayesQ consistently outperforms strong PTQ baselines like GPTQ on ResNet-50 (ImageNet) and BERT-base (GLUE), achieving up to +1.5% top-1 accuracy improvement on RN50 and +1.1 GLUE points on BERT. <br />7. The preprocessing overhead of BayesQ is similar to that of GPTQ, making it both practical and effective. <br />8. Overall, BayesQ offers a principled, uncertainty-aware risk minimization approach to low-bit quantization in a post-training setting. <div>
arXiv:2511.08821v1 Announce Type: new 
Abstract: We present BayesQ, an uncertainty-guided post-training quantization framework that is the first to optimize quantization under the posterior expected loss. BayesQ fits a lightweight Gaussian posterior over weights (diagonal Laplace by default; optional K-FAC/low-rank), whitens by the posterior covariance, designs codebooks to minimize posterior-expected distortion, and allocates mixed precision via a greedy knapsack that maximizes marginal expected-loss reduction per bit under a global budget. For scalar quantizers, posterior-expected MSE yields closed-form tables; task-aware proxies are handled by short Monte Carlo on a small calibration set. An optional calibration-only distillation aligns the quantized model with the posterior predictive teacher. At matched average bits/weight of 3.0/3.5/4.0, BayesQ improves over strong PTQ baselines on ResNet-50 (ImageNet) and BERT-base (GLUE) e.g., vs. GPTQ by $+1.5/+0.7/+0.3$ top-1 percentage points on RN50 and $+1.1/+0.4/+0.2$ GLUE points on BERT, while requiring one-time preprocessing comparable to a GPTQ pass. BayesQ reframes low-bit quantization as uncertainty-aware risk minimization in a practical, post-training pipeline.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning for Characterizing System Stability</title>
<link>https://arxiv.org/abs/2511.08831</link>
<guid>https://arxiv.org/abs/2511.08831</guid>
<content:encoded><![CDATA[
<div> machine learning, stability analysis, Lyapunov function, dynamical systems, aerospace 

Summary:
- The paper addresses the challenge of determining stability regions for complex dynamical systems, especially in aerospace applications.
- It introduces a new physics-informed machine learning method, LyapInf, to infer a Lyapunov function from system trajectory data without explicit knowledge of system equations.
- LyapInf uses a quadratic form for the unknown Lyapunov function and minimizes the average residual of the Zubov equation to characterize an estimated stability region.
- Numerical results show that the method successfully identifies a near-maximal ellipsoid of the stability region without prior knowledge of the system equations.
- This approach allows for efficient stability analysis and characterization of stability regions for dynamical systems treated as black boxes in practical applications. 

<br /><br />Summary: <div>
arXiv:2511.08831v1 Announce Type: new 
Abstract: In the design and operation of complex dynamical systems, it is essential to ensure that all state trajectories of the dynamical system converge to a desired equilibrium within a guaranteed stability region. Yet, for many practical systems -- especially in aerospace -- this region cannot be determined a priori and is often challenging to compute. One of the most common methods for computing the stability region is to identify a Lyapunov function. A Lyapunov function is a positive function whose time derivative along system trajectories is non-positive, which provides a sufficient condition for stability and characterizes an estimated stability region. However, existing methods of characterizing a stability region via a Lyapunov function often rely on explicit knowledge of the system governing equations. In this work, we present a new physics-informed machine learning method of characterizing an estimated stability region by inferring a Lyapunov function from system trajectory data that treats the dynamical system as a black box and does not require explicit knowledge of the system governing equations. In our presented Lyapunov function Inference method (LyapInf), we propose a quadratic form for the unknown Lyapunov function and fit the unknown quadratic operator to system trajectory data by minimizing the average residual of the Zubov equation, a first-order partial differential equation whose solution yields a Lyapunov function. The inferred quadratic Lyapunov function can then characterize an ellipsoidal estimate of the stability region. Numerical results on benchmark examples demonstrate that our physics-informed stability analysis method successfully characterizes a near-maximal ellipsoid of the system stability region associated with the inferred Lyapunov function without requiring knowledge of the system governing equations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIGER-MARL: Enhancing Multi-Agent Reinforcement Learning with Temporal Information through Graph-based Embeddings and Representations</title>
<link>https://arxiv.org/abs/2511.08832</link>
<guid>https://arxiv.org/abs/2511.08832</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-agent reinforcement learning, Temporal graphs, Graph embeddings, Temporal attention, Coordination strategies  

<br /><br />Summary:  
This paper introduces TIGER, a novel approach to enhance multi-agent reinforcement learning (MARL) by explicitly modeling the temporal evolution of inter-agent coordination structures. Unlike existing MARL methods that rely on static or per-step relational graphs, TIGER captures dynamic dependencies by constructing temporal graphs that connect agents’ current and historical interactions over time. A temporal attention-based encoder is then used to aggregate information across these evolving structural and temporal neighborhoods, producing time-aware embeddings that inform cooperative policy learning. The approach addresses the natural changes in agent interactions as they adapt, move, or reorganize cooperation strategies, which is critical for achieving robust and adaptive coordination. TIGER’s effectiveness is demonstrated through extensive experiments on two coordination-intensive benchmarks, showing consistent outperformance compared to various value-decomposition and graph-based MARL baselines in terms of both task performance and sample efficiency. Additionally, comprehensive ablation studies highlight the impact of key design elements, illustrating how structural and temporal factors jointly influence effective policy learning in MARL. The authors also provide open-source implementation code to facilitate further research and application within the MARL community. <div>
arXiv:2511.08832v1 Announce Type: new 
Abstract: In this paper, we propose capturing and utilizing \textit{Temporal Information through Graph-based Embeddings and Representations} or \textbf{TIGER} to enhance multi-agent reinforcement learning (MARL). We explicitly model how inter-agent coordination structures evolve over time. While most MARL approaches rely on static or per-step relational graphs, they overlook the temporal evolution of interactions that naturally arise as agents adapt, move, or reorganize cooperation strategies. Capturing such evolving dependencies is key to achieving robust and adaptive coordination. To this end, TIGER constructs dynamic temporal graphs of MARL agents, connecting their current and historical interactions. It then employs a temporal attention-based encoder to aggregate information across these structural and temporal neighborhoods, yielding time-aware agent embeddings that guide cooperative policy learning. Through extensive experiments on two coordination-intensive benchmarks, we show that TIGER consistently outperforms diverse value-decomposition and graph-based MARL baselines in task performance and sample efficiency. Furthermore, we conduct comprehensive ablation studies to isolate the impact of key design parameters in TIGER, revealing how structural and temporal factors can jointly shape effective policy learning in MARL. All codes can be found here: https://github.com/Nikunj-Gupta/tiger-marl.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing DPSGD via Per-Sample Momentum and Low-Pass Filtering</title>
<link>https://arxiv.org/abs/2511.08841</link>
<guid>https://arxiv.org/abs/2511.08841</guid>
<content:encoded><![CDATA[
<div> Keywords: Differentially Private Stochastic Gradient Descent, DPSGD, differential privacy, DP noise, clipping bias <br />
Summary: 
The paper introduces a new method, DP-PMLF, which aims to address the challenges of maintaining model accuracy while maintaining differential privacy in training deep neural networks. DP-PMLF combines per-sample momentum with a low-pass filtering strategy to mitigate DP noise and clipping bias simultaneously. By using per-sample momentum to smooth gradient estimates before clipping and applying a low-pass filter to reduce DP noise post-processing, DP-PMLF demonstrates improved convergence rates under rigorous DP guarantees. The empirical evaluations show that DP-PMLF significantly improves the privacy-utility trade-off compared to existing DPSGD variants. This approach provides a novel solution to the trade-off between privacy and accuracy in training deep neural networks with differential privacy. <br /> <div>
arXiv:2511.08841v1 Announce Type: new 
Abstract: Differentially Private Stochastic Gradient Descent (DPSGD) is widely used to train deep neural networks with formal privacy guarantees. However, the addition of differential privacy (DP) often degrades model accuracy by introducing both noise and bias. Existing techniques typically address only one of these issues, as reducing DP noise can exacerbate clipping bias and vice-versa. In this paper, we propose a novel method, \emph{DP-PMLF}, which integrates per-sample momentum with a low-pass filtering strategy to simultaneously mitigate DP noise and clipping bias. Our approach uses per-sample momentum to smooth gradient estimates prior to clipping, thereby reducing sampling variance. It further employs a post-processing low-pass filter to attenuate high-frequency DP noise without consuming additional privacy budget. We provide a theoretical analysis demonstrating an improved convergence rate under rigorous DP guarantees, and our empirical evaluations reveal that DP-PMLF significantly enhances the privacy-utility trade-off compared to several state-of-the-art DPSGD variants.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On topological descriptors for graph products</title>
<link>https://arxiv.org/abs/2511.08846</link>
<guid>https://arxiv.org/abs/2511.08846</guid>
<content:encoded><![CDATA[
<div> filtrations, topological descriptors, Euler characteristic, persistent homology, graph products <br />
<br />
Topological descriptors are important for capturing structural information in relational data. This study investigates the impact of various filtrations on the product of graphs on topological descriptors like Euler characteristic (EC) and persistent homology (PH). The study finds that EC has limited expressive power on general color-based filtrations while PH descriptors on graph products provide more information compared to individual graphs. Algorithms are developed to compute PH diagrams for vertex- and edge-level filtrations on graph products. The study includes empirical investigations on runtime analysis, expressivity, and graph classification performance to support the theoretical analysis. Overall, this research demonstrates the potential for powerful graph persistent descriptors through product filtrations.<br /><br />Summary: <div>
arXiv:2511.08846v1 Announce Type: new 
Abstract: Topological descriptors have been increasingly utilized for capturing multiscale structural information in relational data. In this work, we consider various filtrations on the (box) product of graphs and the effect on their outputs on the topological descriptors - the Euler characteristic (EC) and persistent homology (PH). In particular, we establish a complete characterization of the expressive power of EC on general color-based filtrations. We also show that the PH descriptors of (virtual) graph products contain strictly more information than the computation on individual graphs, whereas EC does not. Additionally, we provide algorithms to compute the PH diagrams of the product of vertex- and edge-level filtrations on the graph product. We also substantiate our theoretical analysis with empirical investigations on runtime analysis, expressivity, and graph classification performance. Overall, this work paves way for powerful graph persistent descriptors via product filtrations. Code is available at https://github.com/Aalto-QuML/tda_graph_product.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Graph Super-resolution: Dual Frameworks for Topological Fidelity</title>
<link>https://arxiv.org/abs/2511.08853</link>
<guid>https://arxiv.org/abs/2511.08853</guid>
<content:encoded><![CDATA[
<div> Graph super-resolution, GNN-based approaches, Bi-SR, DEFEND, brain connectome dataset<br />
Summary:<br />
Graph super-resolution aims to infer high-resolution graphs from low-resolution counterparts, crucial for resource-constrained fields like medicine. Current GNN-based methods have limitations like ignoring graph structure and relying on node representations for edge weights. The Bi-SR framework introduces a bipartite graph connecting LR and HR nodes for structure-aware node super-resolution. Meanwhile, DEFEND learns edge representations by mapping HR edges to nodes in a dual graph to enable edge inference through standard GNNs. Evaluations on a real-world brain connectome dataset show state-of-the-art performance across various topological measures. Additionally, new simulated datasets capturing different topologies and LR-HR relationships facilitate comprehensive benchmarking efforts for graph super-resolution approaches.<br /> 
Summary: <div>
arXiv:2511.08853v1 Announce Type: new 
Abstract: Graph super-resolution, the task of inferring high-resolution (HR) graphs from low-resolution (LR) counterparts, is an underexplored yet crucial research direction that circumvents the need for costly data acquisition. This makes it especially desirable for resource-constrained fields such as the medical domain. While recent GNN-based approaches show promise, they suffer from two key limitations: (1) matrix-based node super-resolution that disregards graph structure and lacks permutation invariance; and (2) reliance on node representations to infer edge weights, which limits scalability and expressivity. In this work, we propose two GNN-agnostic frameworks to address these issues. First, Bi-SR introduces a bipartite graph connecting LR and HR nodes to enable structure-aware node super-resolution that preserves topology and permutation invariance. Second, DEFEND learns edge representations by mapping HR edges to nodes of a dual graph, allowing edge inference via standard node-based GNNs. We evaluate both frameworks on a real-world brain connectome dataset, where they achieve state-of-the-art performance across seven topological measures. To support generalization, we introduce twelve new simulated datasets that capture diverse topologies and LR-HR relationships. These enable comprehensive benchmarking of graph super-resolution methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposition of Small Transformer Models</title>
<link>https://arxiv.org/abs/2511.08854</link>
<guid>https://arxiv.org/abs/2511.08854</guid>
<content:encoded><![CDATA[
<div> transformer models, stochastic parameter decomposition, interpretable concepts, sequential data, causal importance function

Summary:<br />
This article discusses the extension of Stochastic Parameter Decomposition (SPD) to Transformer models for interpretability. The updated causal importance function and new loss function developed for sequential data allow for successful decomposition of a toy induction-head model and GPT-2-small model. The study demonstrates that SPD can accurately identify interpretable concepts like "golf" and "basketball" within the parameters of the GPT-2-small model. By bridging the gap between traditional models and modern Transformer models, this work presents a promising approach for analyzing and intervening in complex neural network structures. The results showcase the potential of SPD in uncovering interpretable parameter-space mechanisms within sophisticated models, paving the way for future applications in real-world scenarios. 

<br />Summary: <div>
arXiv:2511.08854v1 Announce Type: new 
Abstract: Recent work in mechanistic interpretability has shown that decomposing models in parameter space may yield clean handles for analysis and intervention. Previous methods have demonstrated successful applications on a wide range of toy models, but the gap to "real models" has not yet been bridged. In this work, we extend Stochastic Parameter Decomposition (SPD) to Transformer models, proposing an updated causal importance function suited for sequential data and a new loss function. We demonstrate that SPD can successfully decompose a toy induction-head model and recover the expected 2-step circuit. We also show that applying SPD to GPT-2-small can successfully locate subcomponents corresponding to interpretable concepts like "golf" and "basketball". These results take the first step in the direction of extending SPD to modern models, and show that we can use the method to surface interpretable parameter-space mechanisms.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ForeSWE: Forecasting Snow-Water Equivalent with an Uncertainty-Aware Attention Model</title>
<link>https://arxiv.org/abs/2511.08856</link>
<guid>https://arxiv.org/abs/2511.08856</guid>
<content:encoded><![CDATA[
<div> Keywords: Snow-Water Equivalent, spatio-temporal forecasting, deep learning, Gaussian process, uncertainty estimation  

<br /><br />Summary:  
1. The paper addresses the challenge of forecasting Snow-Water Equivalent (SWE), a crucial measure used for estimating the water content in snowpacks within snow-dominant watersheds.  
2. SWE forecasting is difficult due to its spatio-temporal variability influenced by a variety of environmental and topographical factors, which classical methods have not fully exploited.  
3. The authors introduce ForeSWE, a novel probabilistic spatio-temporal forecasting model that combines deep learning techniques with classical probabilistic models.  
4. ForeSWE incorporates an attention mechanism to effectively capture and integrate spatiotemporal features and interactions.  
5. A Gaussian process module is integrated to provide principled uncertainty quantification alongside the point forecasts, enhancing decision-making confidence.  
6. The model was evaluated on a dataset collected from 512 SNOTEL stations across the Western United States.  
7. Results demonstrate significant improvements in both forecast accuracy and the quality of prediction intervals when compared to existing SWE forecasting methods.  
8. The study validates the effectiveness of combining deep learning with classical probabilistic models in environmental forecasting tasks, particularly in delivering meaningful uncertainty estimates.  
9. The presented findings establish a foundation for practical deployment and ongoing feedback within the water management community to aid complex water resource decisions. <div>
arXiv:2511.08856v1 Announce Type: new 
Abstract: Various complex water management decisions are made in snow-dominant watersheds with the knowledge of Snow-Water Equivalent (SWE) -- a key measure widely used to estimate the water content of a snowpack. However, forecasting SWE is challenging because SWE is influenced by various factors including topography and an array of environmental conditions, and has therefore been observed to be spatio-temporally variable. Classical approaches to SWE forecasting have not adequately utilized these spatial/temporal correlations, nor do they provide uncertainty estimates -- which can be of significant value to the decision maker. In this paper, we present ForeSWE, a new probabilistic spatio-temporal forecasting model that integrates deep learning and classical probabilistic techniques. The resulting model features a combination of an attention mechanism to integrate spatiotemporal features and interactions, alongside a Gaussian process module that provides principled quantification of prediction uncertainty. We evaluate the model on data from 512 Snow Telemetry (SNOTEL) stations in the Western US. The results show significant improvements in both forecasting accuracy and prediction interval compared to existing approaches. The results also serve to highlight the efficacy in uncertainty estimates between different approaches. Collectively, these findings have provided a platform for deployment and feedback by the water management community.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-X: Device-Agnostic and Noise-Robust Foundation Model for EEG</title>
<link>https://arxiv.org/abs/2511.08861</link>
<guid>https://arxiv.org/abs/2511.08861</guid>
<content:encoded><![CDATA[
<div> Keywords: EEG representation learning, device-agnostic, noise robustness, location-based channel embedding, dictionary-inspired convolutional transformation<br /><br />Summary:  
The paper presents EEG-X, a foundation model designed for robust and generalizable EEG representation learning. It addresses two main challenges in EEG analysis: variability across datasets due to different devices and configurations, and the inherently low signal-to-noise ratio (SNR) of EEG data. EEG-X introduces a novel location-based channel embedding that encodes spatial information to improve generalization across domains and tasks, enabling the model to handle varying electrode layouts, channel numbers, and recording lengths. To enhance noise robustness, EEG-X applies a noise-aware masking and reconstruction approach in both raw and latent data spaces. Unlike prior methods that reconstruct noisy raw EEG signals, EEG-X trains on denoised signals derived from artifact removal, thereby focusing on neural activity rather than extraneous noise. Additionally, EEG-X incorporates a dictionary-inspired convolutional transformation (DiCT) layer that maps signals into a structured feature space before computing reconstruction loss, effectively reducing sensitivity to noise while capturing frequency- and shape-based similarities. Experimental results across multiple EEG datasets from diverse devices demonstrate EEG-X’s superior performance over existing state-of-the-art methods in multiple downstream tasks. Significantly, it excels in cross-domain scenarios with differing electrode layouts between pre-training and downstream datasets. The code and models are publicly available at the provided GitHub repository. <div>
arXiv:2511.08861v1 Announce Type: new 
Abstract: Foundation models for EEG analysis are still in their infancy, limited by two key challenges: (1) variability across datasets caused by differences in recording devices and configurations, and (2) the low signal-to-noise ratio (SNR) of EEG, where brain signals are often buried under artifacts and non-brain sources. To address these challenges, we present EEG-X, a device-agnostic and noise-robust foundation model for EEG representation learning. EEG-X introduces a novel location-based channel embedding that encodes spatial information and improves generalization across domains and tasks by allowing the model to handle varying channel numbers, combinations, and recording lengths. To enhance robustness against noise, EEG-X employs a noise-aware masking and reconstruction strategy in both raw and latent spaces. Unlike previous models that mask and reconstruct raw noisy EEG signals, EEG-X is trained to reconstruct denoised signals obtained through an artifact removal process, ensuring that the learned representations focus on neural activity rather than noise. To further enhance reconstruction-based pretraining, EEG-X introduces a dictionary-inspired convolutional transformation (DiCT) layer that projects signals into a structured feature space before computing reconstruction (MSE) loss, reducing noise sensitivity and capturing frequency- and shape-aware similarities. Experiments on datasets collected from diverse devices show that EEG-X outperforms state-of-the-art methods across multiple downstream EEG tasks and excels in cross-domain settings where pre-trained and downstream datasets differ in electrode layouts. The models and code are available at: https://github.com/Emotiv/EEG-X
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer-Based Sleep Stage Classification Enhanced by Clinical Information</title>
<link>https://arxiv.org/abs/2511.08864</link>
<guid>https://arxiv.org/abs/2511.08864</guid>
<content:encoded><![CDATA[
<div> Keywords: sleep staging, polysomnography, Transformer, clinical metadata, event annotations<br /><br />Summary:<br />1. Manual sleep staging from polysomnography (PSG) is time-consuming and subject to variability among scorers, motivating automation through deep learning.<br />2. The authors propose a novel two-stage architecture combining a Transformer-based per-epoch encoder with a 1D CNN aggregator to improve automated sleep staging.<br />3. The model explicitly incorporates contextual information including subject-level clinical metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing) to enhance performance.<br />4. Using a large dataset from the Sleep Heart Health Study (n=8,357), they demonstrate that integrating context significantly improves staging accuracy compared to a baseline that uses only PSG signals.<br />5. Their final model attains macro-F1 and micro-F1 scores of 0.8031 and 0.9051, respectively, outperforming the baseline scores of 0.7745 and 0.8774, with the event annotations providing the largest accuracy gains.<br />6. Feature fusion was shown to be more effective than multi-task learning approaches predicting auxiliary labels.<br />7. This approach enhances both model performance and interpretability without changing the PSG setup or needing additional sensors.<br />8. These findings support a practical, scalable framework for context-aware, expert-aligned automated sleep staging systems. <div>
arXiv:2511.08864v1 Announce Type: new 
Abstract: Manual sleep staging from polysomnography (PSG) is labor-intensive and prone to inter-scorer variability. While recent deep learning models have advanced automated staging, most rely solely on raw PSG signals and neglect contextual cues used by human experts. We propose a two-stage architecture that combines a Transformer-based per-epoch encoder with a 1D CNN aggregator, and systematically investigates the effect of incorporating explicit context: subject-level clinical metadata (age, sex, BMI) and per-epoch expert event annotations (apneas, desaturations, arousals, periodic breathing). Using the Sleep Heart Health Study (SHHS) cohort (n=8,357), we demonstrate that contextual fusion substantially improves staging accuracy. Compared to a PSG-only baseline (macro-F1 0.7745, micro-F1 0.8774), our final model achieves macro-F1 0.8031 and micro-F1 0.9051, with event annotations contributing the largest gains. Notably, feature fusion outperforms multi-task alternatives that predict the same auxiliary labels. These results highlight that augmenting learned representations with clinically meaningful features enhances both performance and interpretability, without modifying the PSG montage or requiring additional sensors. Our findings support a practical and scalable path toward context-aware, expert-aligned sleep staging systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covariance Scattering Transforms</title>
<link>https://arxiv.org/abs/2511.08878</link>
<guid>https://arxiv.org/abs/2511.08878</guid>
<content:encoded><![CDATA[
<div> PCA, Principal Component Analysis, CoVariance Neural Networks, CSTs, age prediction <br />
Summary: <br />
The article introduces Covariance Scattering Transforms (CSTs) as a technique to improve upon the limitations of Principal Component Analysis (PCA) and CoVariance Neural Networks (VNNs) in capturing information in low-variance directions and ensuring stability in low-sample regimes. CSTs are deep untrained networks that apply filters localized in the covariance spectrum to input data, producing hierarchical representations via nonlinearities. The filters, termed covariance wavelets, capture specific covariance spectral patterns. CSTs offer improved computational and memory efficiency through a pruning mechanism and exhibit enhanced stability in finite-sample covariance estimations compared to PCA. Experimental results on age prediction from cortical thickness measurements demonstrate that CSTs produce stable representations in low-data settings similar to VNNs but without the need for training, leading to comparable or better predictions compared to more complex learning models. <div>
arXiv:2511.08878v1 Announce Type: new 
Abstract: Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Predictability as a Fast Reliability Indicator for Time Series Forecasting Model Selection</title>
<link>https://arxiv.org/abs/2511.08884</link>
<guid>https://arxiv.org/abs/2511.08884</guid>
<content:encoded><![CDATA[
<div> Spectral predictability, time series forecasting, model selection, foundation models, computational efficiency<br /><br />Summary:<br /><br />1. The paper addresses the challenge practitioners face in selecting appropriate time series forecasting models due to the high computational cost of validating numerous models and the risk of poor performance from choosing the wrong model.<br />2. The authors introduce spectral predictability, denoted as Ω, a simple signal processing metric that reliably stratifies the performance of different model families.<br />3. Controlled experiments are conducted across four diverse domains, and the analysis is further extended to 51 models and 28 datasets from the GIFT-Eval benchmark.<br />4. Results show that large time series foundation models (TSFMs) consistently outperform lightweight, task-trained baseline models when Ω is high, but this advantage disappears when Ω is low.<br />5. Since Ω can be computed in seconds per dataset, it provides a rapid assessment tool to determine whether a dataset is suitable for TSFMs or if simpler, less expensive models suffice.<br />6. Overall, Ω serves as a practical first-pass filter to reduce the validation cost for practitioners, emphasizing the need for models that handle genuinely difficult low-Ω problems instead of just optimizing for easier high-Ω cases. <div>
arXiv:2511.08884v1 Announce Type: new 
Abstract: Practitioners deploying time series forecasting models face a dilemma: exhaustively validating dozens of models is computationally prohibitive, yet choosing the wrong model risks poor performance. We show that spectral predictability~$\Omega$ -- a simple signal processing metric -- systematically stratifies model family performance, enabling fast model selection. We conduct controlled experiments in four different domains, then further expand our analysis to 51 models and 28 datasets from the GIFT-Eval benchmark. We find that large time series foundation models (TSFMs) systematically outperform lightweight task-trained baselines when $\Omega$ is high, while their advantage vanishes as $\Omega$ drops. Computing $\Omega$ takes seconds per dataset, enabling practitioners to quickly assess whether their data suits TSFM approaches or whether simpler, cheaper models suffice. We demonstrate that $\Omega$ stratifies model performance predictably, offering a practical first-pass filter that reduces validation costs while highlighting the need for models that excel on genuinely difficult (low-$\Omega$) problems rather than merely optimizing easy ones.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis</title>
<link>https://arxiv.org/abs/2511.08887</link>
<guid>https://arxiv.org/abs/2511.08887</guid>
<content:encoded><![CDATA[
<div> Stroke diagnosis, fairness, domain-adversarial training, group distributionally robust optimization, medical AI<br /><br />Summary:<br /><br />1. Stroke is an urgent cerebrovascular condition where rapid and accurate diagnosis greatly improves patient outcomes. 2. Current automated diagnostic methods often face fairness challenges, leading to unequal performance across different demographic groups such as age, gender, and posture, which may worsen healthcare disparities. 3. The authors propose FAST-CAD, a novel framework integrating domain-adversarial training (DAT) and group distributionally robust optimization (Group-DRO) to ensure fair and accurate non-contact stroke diagnosis. 4. FAST-CAD leverages self-supervised encoders combined with adversarial domain discrimination to learn representations that are invariant to demographic differences, while Group-DRO focuses on minimizing the worst-case risk among all demographic subgroups. 5. The method is theoretically grounded, providing convergence guarantees and fairness bounds based on domain adaptation and minimax fairness theory. 6. A custom multimodal dataset covering 12 demographic subgroups is curated for evaluation. 7. Experimental results show that FAST-CAD delivers superior diagnostic accuracy with consistent fairness across demographic groups, supported by theoretical analysis demonstrating the advantages of the unified DAT + Group-DRO approach. 8. This research advances both practical applications and theoretical understanding of fairness in medical AI systems. <div>
arXiv:2511.08887v1 Announce Type: new 
Abstract: Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. In this work we propose FAST-CAD, a theoretically grounded framework that combines domain-adversarial training (DAT) with group distributionally robust optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. Our approach is built on domain adaptation and minimax fairness theory and provides convergence guarantees and fairness bounds. We curate a multimodal dataset covering 12 demographic subgroups defined by age, gender, and posture. FAST-CAD employs self-supervised encoders with adversarial domain discrimination to learn demographic-invariant representations, while Group-DRO optimizes worst-group risk to ensure robust performance across all subgroups. Extensive experiments show that our method achieves superior diagnostic performance while maintaining fairness across demographic groups, and our theoretical analysis supports the effectiveness of the unified DAT + Group-DRO framework. This work provides both practical advances and theoretical insights for fair medical AI systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weaver: Kronecker Product Approximations of Spatiotemporal Attention for Traffic Network Forecasting</title>
<link>https://arxiv.org/abs/2511.08888</link>
<guid>https://arxiv.org/abs/2511.08888</guid>
<content:encoded><![CDATA[
<div> Transformer-based architectures, spatiotemporal forecasting, transportation networks, attention-based model, Kronecker product approximations


Summary:<br /><br />
- Weaver introduces a novel attention-based model for spatiotemporal forecasting on transportation networks.
- It applies Kronecker product approximations to decompose spatiotemporal attention into temporal and spatial maps, improving efficiency.
- The model utilizes Valence Attention with the continuous Tanimoto coefficient to capture real-world traffic dynamics.
- The Traffic Phase Dictionary is introduced for self-conditioning to fully utilize the model's learning capacity.
- Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance in training efficiency and accuracy. <div>
arXiv:2511.08888v1 Announce Type: new 
Abstract: Spatiotemporal forecasting on transportation networks is a complex task that requires understanding how traffic nodes interact within a dynamic, evolving system dictated by traffic flow dynamics and social behavioral patterns. The importance of transportation networks and ITS for modern mobility and commerce necessitates forecasting models that are not only accurate but also interpretable, efficient, and robust under structural or temporal perturbations. Recent approaches, particularly Transformer-based architectures, have improved predictive performance but often at the cost of high computational overhead and diminished architectural interpretability. In this work, we introduce Weaver, a novel attention-based model that applies Kronecker product approximations (KPA) to decompose the PN X PN spatiotemporal attention of O(P^2N^2) complexity into local P X P temporal and N X N spatial attention maps. This Kronecker attention map enables our Parallel-Kronecker Matrix-Vector product (P2-KMV) for efficient spatiotemporal message passing with O(P^2N + N^2P) complexity. To capture real-world traffic dynamics, we address the importance of negative edges in modeling traffic behavior by introducing Valence Attention using the continuous Tanimoto coefficient (CTC), which provides properties conducive to precise latent graph generation and training stability. To fully utilize the model's learning capacity, we introduce the Traffic Phase Dictionary for self-conditioning. Evaluations on PEMS-BAY and METR-LA show that Weaver achieves competitive performance across model categories while training more efficiently.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepDR: an integrated deep-learning model web server for drug repositioning</title>
<link>https://arxiv.org/abs/2511.08921</link>
<guid>https://arxiv.org/abs/2511.08921</guid>
<content:encoded><![CDATA[
<div> Keywords: drug repositioning, deep learning, integrated platform, knowledge graph, computational automation

Summary:
DeepDR is an innovative platform that utilizes deep learning models for drug repositioning tasks. It integrates various established models and an extensive knowledge graph with data from multiple databases and scientific publications. DeepDR recommends candidate drugs based on disease- and target-specific information, facilitating the identification of new indications for approved drugs. The platform offers detailed descriptions of recommended drugs and visualizes key patterns through interpretability via the knowledge graph. DeepDR is accessible to all users without registration and aims to provide a user-friendly, systematic, and highly accurate solution for both experimental and computational scientists seeking novel drug indications. <div>
arXiv:2511.08921v1 Announce Type: new 
Abstract: Background: Identifying new indications for approved drugs is a complex and time-consuming process that requires extensive knowledge of pharmacology, clinical data, and advanced computational methods. Recently, deep learning (DL) methods have shown their capability for the accurate prediction of drug repositioning. However, implementing DL-based modeling requires in-depth domain knowledge and proficient programming skills. Results: In this application, we introduce DeepDR, the first integrated platform that combines a variety of established DL-based models for disease- and target-specific drug repositioning tasks. DeepDR leverages invaluable experience to recommend candidate drugs, which covers more than 15 networks and a comprehensive knowledge graph that includes 5.9 million edges across 107 types of relationships connecting drugs, diseases, proteins/genes, pathways, and expression from six existing databases and a large scientific corpus of 24 million PubMed publications. Additionally, the recommended results include detailed descriptions of the recommended drugs and visualize key patterns with interpretability through a knowledge graph. Conclusion: DeepDR is free and open to all users without the requirement of registration. We believe it can provide an easy-to-use, systematic, highly accurate, and computationally automated platform for both experimental and computational scientists.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Policies with Value-Conditional Optimization for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.08922</link>
<guid>https://arxiv.org/abs/2511.08922</guid>
<content:encoded><![CDATA[
<div> Keywords: Offline Reinforcement Learning, Diffusion Models, Value Overestimation, Policy Improvement, D4RL Benchmark

<br /><br />Summary:  
This paper addresses the issue of value overestimation caused by out-of-distribution (OOD) actions in offline reinforcement learning, which limits policy performance. It proposes DIVO (DIffusion policies with Value-conditional Optimization), a novel method that leverages diffusion models to generate high-quality, in-distribution state-action samples, thereby improving policy learning efficiency. DIVO introduces a binary-weighted mechanism using advantage values from the offline dataset to guide the diffusion model's training; this selectively expands the action space towards higher-advantage regions while maintaining alignment with the original dataset distribution. During policy improvement, DIVO dynamically filters actions with high return potential from the diffusion model, balancing conservatism and explorability in policy optimization. The approach is evaluated on the D4RL benchmark, showing significant performance gains in locomotion tasks and outperforming state-of-the-art methods in the challenging AntMaze environment, which features sparse rewards. Overall, DIVO demonstrates an effective way to reduce over-conservatism and improve policy expressiveness and efficiency in offline RL through value-conditional diffusion modeling. <div>
arXiv:2511.08922v1 Announce Type: new 
Abstract: In offline reinforcement learning, value overestimation caused by out-of-distribution (OOD) actions significantly limits policy performance. Recently, diffusion models have been leveraged for their strong distribution-matching capabilities, enforcing conservatism through behavior policy constraints. However, existing methods often apply indiscriminate regularization to redundant actions in low-quality datasets, resulting in excessive conservatism and an imbalance between the expressiveness and efficiency of diffusion modeling. To address these issues, we propose DIffusion policies with Value-conditional Optimization (DIVO), a novel approach that leverages diffusion models to generate high-quality, broadly covered in-distribution state-action samples while facilitating efficient policy improvement. Specifically, DIVO introduces a binary-weighted mechanism that utilizes the advantage values of actions in the offline dataset to guide diffusion model training. This enables a more precise alignment with the dataset's distribution while selectively expanding the boundaries of high-advantage actions. During policy improvement, DIVO dynamically filters high-return-potential actions from the diffusion model, effectively guiding the learned policy toward better performance. This approach achieves a critical balance between conservatism and explorability in offline RL. We evaluate DIVO on the D4RL benchmark and compare it against state-of-the-art baselines. Empirical results demonstrate that DIVO achieves superior performance, delivering significant improvements in average returns across locomotion tasks and outperforming existing methods in the challenging AntMaze domain, where sparse rewards pose a major difficulty.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TransactionGPT</title>
<link>https://arxiv.org/abs/2511.08939</link>
<guid>https://arxiv.org/abs/2511.08939</guid>
<content:encoded><![CDATA[
<div> Keywords: TransactionGPT, 3D-Transformer, payment transactions, downstream tasks, LLM embeddings  

<br /><br />Summary:  
This paper introduces TransactionGPT (TGPT), a foundation model designed for consumer transaction data from one of the world’s largest payment networks. TGPT employs a novel 3D-Transformer architecture specifically developed to capture the complex dynamics of payment transaction data by enhancing modality fusion and computational efficiency. The model is trained on billions of real-world transactions and supports multiple downstream tasks such as prediction and classification. Experimental results demonstrate that TGPT significantly outperforms competitive production models and various baselines in both classification accuracy and transaction generation. The study evaluates TGPT using diverse company transaction datasets across several tasks to provide a comprehensive empirical assessment of its effectiveness and efficiency compared to established methods. Furthermore, the authors investigate integrating embeddings derived from large language models (LLMs) within TGPT and compare its performance to fine-tuned LLMs. Results show TGPT achieves superior predictive accuracy along with faster training and inference times. The paper concludes that the architectural innovations and practical insights presented have the potential to advance foundation models tailored to transaction-like data and inspire further research in this emerging domain. <div>
arXiv:2511.08939v1 Announce Type: new 
Abstract: We present TransactionGPT (TGPT), a foundation model for consumer transaction data within one of world's largest payment networks. TGPT is designed to understand and generate transaction trajectories while simultaneously supporting a variety of downstream prediction and classification tasks. We introduce a novel 3D-Transformer architecture specifically tailored for capturing the complex dynamics in payment transaction data. This architecture incorporates design innovations that enhance modality fusion and computational efficiency, while seamlessly enabling joint optimization with downstream objectives. Trained on billion-scale real-world transactions, TGPT significantly improves downstream classification performance against a competitive production model and exhibits advantages over baselines in generating future transactions. We conduct extensive empirical evaluations utilizing a diverse collection of company transaction datasets spanning multiple downstream tasks, thereby enabling a thorough assessment of TGPT's effectiveness and efficiency in comparison to established methodologies. Furthermore, we examine the incorporation of LLM-derived embeddings within TGPT and benchmark its performance against fine-tuned LLMs, demonstrating that TGPT achieves superior predictive accuracy as well as faster training and inference. We anticipate that the architectural innovations and practical guidelines from this work will advance foundation models for transaction-like data and catalyze future research in this emerging field.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QIBONN: A Quantum-Inspired Bilevel Optimizer for Neural Networks on Tabular Classification</title>
<link>https://arxiv.org/abs/2511.08940</link>
<guid>https://arxiv.org/abs/2511.08940</guid>
<content:encoded><![CDATA[
<div> Optimization, Neural Networks, Tabular Data, Quantum-Inspired, Bilevel Framework <br />
<br />
Summary: 
The article introduces the Quantum-Inspired Bilevel Optimizer for Neural Networks (QIBONN), a framework designed for hyperparameter optimization (HPO) on tabular data. QIBONN addresses the challenges of large non-convex search spaces and costly exhaustive tuning by utilizing a qubit-based representation that encompasses feature selection, architectural hyperparameters, and regularization. It combines deterministic quantum-inspired rotations with stochastic qubit mutations guided by a global attractor to strike a balance between exploration and exploitation within a fixed evaluation budget. Experiments conducted under single-qubit bit-flip noise show that QIBONN is competitive with established methods such as classical tree-based methods and both classical and quantum-inspired HPO algorithms when considering the same tuning budget. <div>
arXiv:2511.08940v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO) for neural networks on tabular data is critical to a wide range of applications, yet it remains challenging due to large, non-convex search spaces and the cost of exhaustive tuning. We introduce the Quantum-Inspired Bilevel Optimizer for Neural Networks (QIBONN), a bilevel framework that encodes feature selection, architectural hyperparameters, and regularization in a unified qubit-based representation. By combining deterministic quantum-inspired rotations with stochastic qubit mutations guided by a global attractor, QIBONN balances exploration and exploitation under a fixed evaluation budget. We conduct systematic experiments under single-qubit bit-flip noise (0.1\%--1\%) emulated by an IBM-Q backend. Results on 13 real-world datasets indicate that QIBONN is competitive with established methods, including classical tree-based methods and both classical/quantum-inspired HPO algorithms under the same tuning budget.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Backdoor Removal by Reconstructing Trigger-Activated Changes in Latent Representation</title>
<link>https://arxiv.org/abs/2511.08944</link>
<guid>https://arxiv.org/abs/2511.08944</guid>
<content:encoded><![CDATA[
<div> backdoor attacks, machine learning models, defense methods, optimization problem, perturbations
Summary: 
This article introduces a novel method for defending against backdoor attacks in machine learning models. Backdoor attacks are a serious threat as they cause models to misclassify poisoned data while behaving normally on clean data. Existing defenses struggle to accurately identify backdoor neurons. The proposed method focuses on accurately reconstructing Trigger-Activated Changes (TAC) values in the latent representation. By formulating the minimal perturbation needed to force clean data into a specific class as a convex quadratic optimization problem, accurate TAC values can be obtained. The poisoned class is identified by detecting small perturbation norms, and this information is used in fine-tuning to remove backdoors. Experimental results on various datasets and architectures show that this approach outperforms existing defense methods, achieving superior backdoor suppression while maintaining high clean accuracy. 
<br /><br />Summary: <div>
arXiv:2511.08944v1 Announce Type: new 
Abstract: Backdoor attacks pose a critical threat to machine learning models, causing them to behave normally on clean data but misclassify poisoned data into a poisoned class. Existing defenses often attempt to identify and remove backdoor neurons based on Trigger-Activated Changes (TAC) which is the activation differences between clean and poisoned data. These methods suffer from low precision in identifying true backdoor neurons due to inaccurate estimation of TAC values. In this work, we propose a novel backdoor removal method by accurately reconstructing TAC values in the latent representation. Specifically, we formulate the minimal perturbation that forces clean data to be classified into a specific class as a convex quadratic optimization problem, whose optimal solution serves as a surrogate for TAC. We then identify the poisoned class by detecting statistically small $L^2$ norms of perturbations and leverage the perturbation of the poisoned class in fine-tuning to remove backdoors. Experiments on CIFAR-10, GTSRB, and TinyImageNet demonstrated that our approach consistently achieves superior backdoor suppression with high clean accuracy across different attack types, datasets, and architectures, outperforming existing defense methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Conditional VAE with approximation using Normalizing Flows</title>
<link>https://arxiv.org/abs/2511.08946</link>
<guid>https://arxiv.org/abs/2511.08946</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoders, Generative Adversarial Networks, Diffusion Models, Conditional Variational Autoencoders, Normalizing Flows<br /><br />Summary:<br /><br />1. Variational Autoencoders (VAEs) and Generative Adversarial Networks (GANs) were the state-of-the-art generative models until 2022 but have since been surpassed by diffusion-based models. As a result, efforts to improve these traditional models have slowed down. <br /><br />2. This work revisits image generation using Conditional Variational Autoencoders (CVAEs), aiming to incorporate specific attributes into generated images through conditioning. <br /><br />3. A known issue with VAEs is that they often produce blurry images with limited diversity. To address this, the authors adopt a method that treats the variance of the Gaussian decoder as a learnable parameter during training, improving image sharpness and diversity. <br /><br />4. Previous studies on CVAEs typically assumed that the conditional distribution of the latent space given the labels matches the prior distribution; however, this assumption does not hold true in practice. <br /><br />5. The authors propose estimating the conditional latent distribution using normalizing flows, which improves image generation quality. Their approach reduces the Fréchet Inception Distance (FID) by 5% and increases the log-likelihood by 7.7% compared to previous methods, demonstrating measurable performance gains. <div>
arXiv:2511.08946v1 Announce Type: new 
Abstract: Variational Autoencoders and Generative Adversarial Networks remained the state-of-the-art (SOTA) generative models until 2022. Now they are superseded by diffusion based models. Efforts to improve traditional models have stagnated as a result. In old-school fashion, we explore image generation with conditional Variational Autoencoders (CVAE) to incorporate desired attributes within the images. VAEs are known to produce blurry images with less diversity, we refer a method that solve this issue by leveraging the variance of the gaussian decoder as a learnable parameter during training. Previous works on CVAEs assumed that the conditional distribution of the latent space given the labels is equal to the prior distribution, which is not the case in reality. We show that estimating it using normalizing flows results in better image generation than existing methods by reducing the FID by 5% and increasing log likelihood by 7.7% than the previous case.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Mixture of Experts For Large Language Models</title>
<link>https://arxiv.org/abs/2511.08968</link>
<guid>https://arxiv.org/abs/2511.08968</guid>
<content:encoded><![CDATA[
<div> Bayesian Mixture of Experts, Uncertainty Estimation, Large Language Models, Mixture-of-Experts, Calibration Error<br />
Summary:
Bayesian Mixture of Experts (Bayesian-MoE) is introduced as a framework for precise uncertainty estimation in fine-tuned large language models (LLMs) by employing a structured Laplace approximation. This method enhances calibrated uncertainty estimation without altering the original training process or adding new parameters, focusing on the existing expert pathways in Mixture-of-Experts models. Bayesian-MoE utilizes the modular design of MoE models for manageable block-wise posterior estimation and employs Kronecker-factored low-rank approximations to model curvature. Through experiments on common-sense reasoning benchmarks with Qwen1.5-MoE and DeepSeek-MoE, Bayesian-MoE showcases improved expected calibration error (ECE) and negative log-likelihood (NLL) over baseline approaches, affirming its efficacy in facilitating dependable downstream decision-making.<br /><br />Summary: <div>
arXiv:2511.08968v1 Announce Type: new 
Abstract: We present Bayesian Mixture of Experts (Bayesian-MoE), a post-hoc uncertainty estimation framework for fine-tuned large language models (LLMs) based on Mixture-of-Experts architectures. Our method applies a structured Laplace approximation to the second linear layer of each expert, enabling calibrated uncertainty estimation without modifying the original training procedure or introducing new parameters. Unlike prior approaches, which apply Bayesian inference to added adapter modules, Bayesian-MoE directly targets the expert pathways already present in MoE models, leveraging their modular design for tractable block-wise posterior estimation. We use Kronecker-factored low-rank approximations to model curvature and derive scalable estimates of predictive uncertainty and marginal likelihood. Experiments on common-sense reasoning benchmarks with Qwen1.5-MoE and DeepSeek-MoE demonstrate that Bayesian-MoE improves both expected calibration error (ECE) and negative log-likelihood (NLL) over baselines, confirming its effectiveness for reliable downstream decision-making.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Sinkhorn Routing for Improved Sparse Mixture of Experts</title>
<link>https://arxiv.org/abs/2511.08972</link>
<guid>https://arxiv.org/abs/2511.08972</guid>
<content:encoded><![CDATA[
<div> Sparse Mixture-of-Experts, SMoE, optimal transport problem, expert diversity, Selective Sinkhorn Routing<br />
Summary:<br />
Sparse Mixture-of-Experts (SMoE) architecture efficiently increases model capacity without added inference costs. However, existing models use auxiliary losses and parameters for expert diversity without directly aligning objectives. This work formulates token-to-expert assignment as an optimal transport problem with balancing constraints, improving SMoE performance without extra balancing losses by deriving gating scores from the transport map. Selective Sinkhorn Routing (SSR) is proposed as a routing mechanism that enhances token assignments and expert selection, replacing auxiliary losses with lightweight Sinkhorn-based routing. SSR achieves faster training, higher accuracy, and better robustness to input corruption in language modeling and image classification tasks.<br /><br />Summary: <div>
arXiv:2511.08972v1 Announce Type: new 
Abstract: Sparse Mixture-of-Experts (SMoE) has gained prominence as a scalable and computationally efficient architecture, enabling significant growth in model capacity without incurring additional inference costs. However, existing SMoE models often rely on auxiliary losses (e.g., z-loss, load balancing) and additional trainable parameters (e.g., noisy gating) to encourage expert diversity, leading to objective misalignment and increased model complexity. Moreover, existing Sinkhorn-based methods suffer from significant training overhead due to their heavy reliance on the computationally expensive Sinkhorn algorithm. In this work, we formulate token-to-expert assignment as an optimal transport problem, incorporating constraints to ensure balanced expert utilization. We demonstrate that introducing a minimal degree of optimal transport-based routing enhances SMoE performance without requiring auxiliary balancing losses. Unlike previous methods, our approach derives gating scores directly from the transport map, enabling more effective token-to-expert balancing, supported by both theoretical analysis and empirical results. Building on these insights, we propose Selective Sinkhorn Routing (SSR), a routing mechanism that replaces auxiliary loss with lightweight Sinkhorn-based routing. SSR promotes balanced token assignments while preserving flexibility in expert selection. Across both language modeling and image classification tasks, SSR achieves faster training, higher accuracy, and greater robustness to input corruption.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data reuse enables cost-efficient randomized trials of medical AI models</title>
<link>https://arxiv.org/abs/2511.08986</link>
<guid>https://arxiv.org/abs/2511.08986</guid>
<content:encoded><![CDATA[
<div> Randomized controlled trials, medical artificial intelligence, BRIDGE design, data reuse, AI-based risk models <br />
<br />
Summary: Randomized controlled trials are crucial for validating medical AI tools but are costly and time-consuming. The BRIDGE design proposes reusing data from previous AI trials when old and updated models show similar predictions, reducing the need for new trials. A checklist helps assess data reuse validity. Real-world datasets across various conditions show high concordance between AI models. Simulation studies on breast cancer screening demonstrate a 46.6% reduction in enrollment requirements and cost savings while maintaining statistical power. This adaptive trial design allows for more efficient and cost-effective validation of AI models, accelerating their integration into clinical practice. <div>
arXiv:2511.08986v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) are indispensable for establishing the clinical value of medical artificial-intelligence (AI) tools, yet their high cost and long timelines hinder timely validation as new models emerge rapidly. Here, we propose BRIDGE, a data-reuse RCT design for AI-based risk models. AI risk models support a broad range of interventions, including screening, treatment selection, and clinical alerts. BRIDGE trials recycle participant-level data from completed trials of AI models when legacy and updated models make concordant predictions, thereby reducing the enrollment requirement for subsequent trials. We provide a practical checklist for investigators to assess whether reusing data from previous trials allows for valid causal inference and preserves type I error. Using real-world datasets across breast cancer, cardiovascular disease, and sepsis, we demonstrate concordance between successive AI models, with up to 64.8% overlap in top 5% high-risk cohorts. We then simulate a series of breast cancer screening studies, where our design reduced required enrollment by 46.6%--saving over US$2.8 million--while maintaining 80% power. By transforming trials into adaptive, modular studies, our proposed design makes Level I evidence generation feasible for every model iteration, thereby accelerating cost-effective translation of AI into routine care.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast $k$-means clustering in Riemannian manifolds via Fr\'{e}chet maps: Applications to large-dimensional SPD matrices</title>
<link>https://arxiv.org/abs/2511.08993</link>
<guid>https://arxiv.org/abs/2511.08993</guid>
<content:encoded><![CDATA[
<div> Clustering, p-Fréchet map, SPD matrices, manifold learning, dimensionality reduction

<br /><br />Summary:  
This paper introduces a novel and efficient framework for clustering data that lie on high-dimensional, non-Euclidean manifolds, addressing the computational difficulties of standard intrinsic methods. The main innovation is the use of the $p$-Fréchet map, which is a function that embeds data from a generic metric space or manifold $\mathcal{M}$ into a lower-dimensional Euclidean space $\mathbb{R}^\ell$ using a carefully selected set of reference points. This embedding enables the application of conventional Euclidean clustering algorithms such as k-means, which are computationally more efficient and simpler to implement than intrinsic manifold-based techniques. The authors rigorously analyze the mathematical properties of this $p$-Fréchet map both in Euclidean settings and in the particularly challenging manifold of symmetric positive definite matrices $\mathit{SPD}(n)$. Through extensive experiments on synthetic and real $\mathit{SPD}(n)$ data, the proposed method demonstrates substantial performance improvements, achieving runtime reductions of up to two orders of magnitude compared to intrinsic approaches. Importantly, these efficiency gains do not compromise clustering accuracy; the method maintains high-quality results even in difficult scenarios where existing alternatives struggle or fail, highlighting its robustness and practical relevance for manifold data clustering problems. <div>
arXiv:2511.08993v1 Announce Type: new 
Abstract: We introduce a novel, efficient framework for clustering data on high-dimensional, non-Euclidean manifolds that overcomes the computational challenges associated with standard intrinsic methods. The key innovation is the use of the $p$-Fr\'{e}chet map $F^p : \mathcal{M} \to \mathbb{R}^\ell$ -- defined on a generic metric space $\mathcal{M}$ -- which embeds the manifold data into a lower-dimensional Euclidean space $\mathbb{R}^\ell$ using a set of reference points $\{r_i\}_{i=1}^\ell$, $r_i \in \mathcal{M}$. Once embedded, we can efficiently and accurately apply standard Euclidean clustering techniques such as k-means. We rigorously analyze the mathematical properties of $F^p$ in the Euclidean space and the challenging manifold of $n \times n$ symmetric positive definite matrices $\mathit{SPD}(n)$. Extensive numerical experiments using synthetic and real $\mathit{SPD}(n)$ data demonstrate significant performance gains: our method reduces runtime by up to two orders of magnitude compared to intrinsic manifold-based approaches, all while maintaining high clustering accuracy, including scenarios where existing alternative methods struggle or fail.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLAD: Federated Learning for LLM-based Autonomous Driving in Vehicle-Edge-Cloud Networks</title>
<link>https://arxiv.org/abs/2511.09025</link>
<guid>https://arxiv.org/abs/2511.09025</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Large Language Models, Autonomous Driving, Privacy Concerns, Communication Efficiency <br />
Summary:<br />Large Language Models (LLMs) have shown promise in autonomous driving (AD) but face challenges like high computation costs and privacy concerns. Federated Learning (FL) offers a solution by enabling collaborative model training without sharing raw data. The Federated LLM-based Autonomous Driving (FLAD) framework leverages distributed sensory data from autonomous vehicles (AVs) in various environments, with innovations including a collaborative architecture for reduced communication delay and data privacy, intelligent parallelized training with communication scheduling for efficiency, and knowledge distillation for personalized LLMs based on edge data. An FLAD testbed with NVIDIA Jetsons overcomes implementation challenges like resource constraints and dynamic model partitions. Experimental results show FLAD outperforms in AD performance while efficiently utilizing distributed vehicular resources, paving the way for collaborative AD model training and knowledge sharing in the future. <br /> <div>
arXiv:2511.09025v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have impressive data fusion and reasoning capabilities for autonomous driving (AD). However, training LLMs for AD faces significant challenges including high computation transmission costs, and privacy concerns associated with sensitive driving data. Federated Learning (FL) is promising for enabling autonomous vehicles (AVs) to collaboratively train models without sharing raw data. We present Federated LLM-based Autonomous Driving (FLAD), an FL framework that leverages distributed multimodal sensory data across AVs in heterogeneous environment. FLAD has three key innovations: (1) a cloud-edge-vehicle collaborative architecture that reduces communication delay and preserving data privacy; (2) an intelligent parallelized collaborative training with a communication scheduling mechanism that optimizes training efficiency, leveraging end-devices otherwise having insufficient resources for model training; and (3) a knowledge distillation method that personalizes LLM according to heterogeneous edge data. In addition, we prototype FLAD in a testbed with NVIDIA Jetsons, overcoming practical implementation challenges including CPU/GPU memory sharing in resource-constrained devices, dynamic model partitions, and fault-tolerant training.Extensive experimental evaluation demonstrates that FLAD achieves superior end-to-end AD performance while efficiently utilizing distributed vehicular resources, opening up new possibilities for future collaborative AD model training and knowledge sharing.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSDWC: Federated Synergistic Dual-Representation Weak Causal Learning for OOD</title>
<link>https://arxiv.org/abs/2511.09036</link>
<guid>https://arxiv.org/abs/2511.09036</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, data privacy, causal inference, semantic representations, generalization error<br />
Summary:<br />
Federated learning (FL) has become popular due to its ability to protect data privacy and leverage computational infrastructure. However, data distribution differences can impact its reliability in real-world scenarios. A proposed solution, FedSDWC, integrates invariant and variant features to infer causal semantic representations, enhancing FL's generalization capabilities and outperforming existing methods like FedICON. The method overcomes limitations by modeling weak causal influence between features, establishing a theoretical error bound and relationship with client prior distributions. Experiments on benchmark datasets show FedSDWC's superiority in handling covariate and semantic shifts, surpassing FedICON by 3.04% on CIFAR-10 and 8.11% on CIFAR-100.<br /> <div>
arXiv:2511.09036v1 Announce Type: new 
Abstract: Amid growing demands for data privacy and advances in computational infrastructure, federated learning (FL) has emerged as a prominent distributed learning paradigm. Nevertheless, differences in data distribution (such as covariate and semantic shifts) severely affect its reliability in real-world deployments. To address this issue, we propose FedSDWC, a causal inference method that integrates both invariant and variant features. FedSDWC infers causal semantic representations by modeling the weak causal influence between invariant and variant features, effectively overcoming the limitations of existing invariant learning methods in accurately capturing invariant features and directly constructing causal representations. This approach significantly enhances FL's ability to generalize and detect OOD data. Theoretically, we derive FedSDWC's generalization error bound under specific conditions and, for the first time, establish its relationship with client prior distributions. Moreover, extensive experiments conducted on multiple benchmark datasets validate the superior performance of FedSDWC in handling covariate and semantic shifts. For example, FedSDWC outperforms FedICON, the next best baseline, by an average of 3.04% on CIFAR-10 and 8.11% on CIFAR-100.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Few-Shot Learning for Audio-Visual Stress Detection</title>
<link>https://arxiv.org/abs/2511.09039</link>
<guid>https://arxiv.org/abs/2511.09039</guid>
<content:encoded><![CDATA[
<div> FairM2S, stress detection, fairness, meta-learning, gender bias, dataset <br />
Summary:<br />
FairM2S is a meta-learning framework designed to address gender bias in AI-driven stress detection using audio-visual data. It incorporates Equalized Odds constraints during both training and adaptation phases, employing adversarial gradient masking to mitigate bias. FairM2S achieves 78.1% accuracy while reducing Equal Opportunity to 0.06, showcasing significant fairness gains. The release of SAVSD, a smartphone-captured dataset with gender annotations, supports fairness research in real-world scenarios. FairM2S is positioned as a leading approach for equitable and scalable few-shot stress detection in mental health AI. The dataset and framework are made publicly available with this paper.<br /> <div>
arXiv:2511.09039v1 Announce Type: new 
Abstract: Fairness in AI-driven stress detection is critical for equitable mental healthcare, yet existing models frequently exhibit gender bias, particularly in data-scarce scenarios. To address this, we propose FairM2S, a fairness-aware meta-learning framework for stress detection leveraging audio-visual data. FairM2S integrates Equalized Odds constraints during both meta-training and adaptation phases, employing adversarial gradient masking and fairness-constrained meta-updates to effectively mitigate bias. Evaluated against five state-of-the-art baselines, FairM2S achieves 78.1% accuracy while reducing the Equal Opportunity to 0.06, demonstrating substantial fairness gains. We also release SAVSD, a smartphone-captured dataset with gender annotations, designed to support fairness research in low-resource, real-world contexts. Together, these contributions position FairM2S as a state-of-the-art approach for equitable and scalable few-shot stress detection in mental health AI. We release our dataset and FairM2S publicly with this paper.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoGNN: Quantifying and Mitigating Semantic Drift in Text-Attributed Graphs</title>
<link>https://arxiv.org/abs/2511.09042</link>
<guid>https://arxiv.org/abs/2511.09042</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph neural networks, text--attributed graphs, semantic drift, manifold structure, Geodesic Aggregation

Summary: 
This paper discusses the issue of semantic drift in graph neural networks operating on text--attributed graphs. Traditional linear aggregation methods distort the geometrically structured representation spaces of modern pretrained language models, leading to semantic drift where aggregated representations deviate from their intrinsic manifold. To address this problem, the authors introduce a novel metric to measure semantic drift and propose Geodesic Aggregation, a mechanism that aggregates neighbor information along geodesics on the unit sphere to maintain fidelity to the semantic manifold. They present GeoGNN, an implementation of this approach that integrates spherical attention and manifold interpolation. Extensive experiments demonstrate that GeoGNN effectively mitigates semantic drift and outperforms existing methods in text--attributed graph learning tasks. The results emphasize the importance of manifold--aware aggregation for preserving semantic information in graph neural networks. 

<br /><br />Summary: <div>
arXiv:2511.09042v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) on text--attributed graphs (TAGs) typically encode node texts using pretrained language models (PLMs) and propagate these embeddings through linear neighborhood aggregation. However, the representation spaces of modern PLMs are highly non--linear and geometrically structured, where textual embeddings reside on curved semantic manifolds rather than flat Euclidean spaces. Linear aggregation on such manifolds inevitably distorts geometry and causes semantic drift--a phenomenon where aggregated representations deviate from the intrinsic manifold, losing semantic fidelity and expressive power. To quantitatively investigate this problem, this work introduces a local PCA--based metric that measures the degree of semantic drift and provides the first quantitative framework to analyze how different aggregation mechanisms affect manifold structure. Building upon these insights, we propose Geodesic Aggregation, a manifold--aware mechanism that aggregates neighbor information along geodesics via log--exp mappings on the unit sphere, ensuring that representations remain faithful to the semantic manifold during message passing. We further develop GeoGNN, a practical instantiation that integrates spherical attention with manifold interpolation. Extensive experiments across four benchmark datasets and multiple text encoders show that GeoGNN substantially mitigates semantic drift and consistently outperforms strong baselines, establishing the importance of manifold--aware aggregation in text--attributed graph learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference is More Than Comparisons: Rethinking Dueling Bandits with Augmented Human Feedback</title>
<link>https://arxiv.org/abs/2511.09047</link>
<guid>https://arxiv.org/abs/2511.09047</guid>
<content:encoded><![CDATA[
<div> sparse feedback, preference elicitation, dueling bandit, regret analysis, performance trade-off

Summary: 
The article introduces a new approach to Interactive Preference Elicitation (IPE) using Dueling Bandit (DB) algorithms. These algorithms aim to optimize decision-making in IPE by making pairwise comparisons. The current challenge with DB algorithms is sparse human feedback. Traditional methods rely heavily on parametric reward models, which can be limited by assumptions and misspecifications. This article proposes an alternative approach based on feedback augmentation. By introducing augmented confidence bounds and incorporating augmented human feedback, the model-free DB framework is enhanced. The algorithm shows competitive performance in various IPE benchmarks, such as recommendation systems and response optimization for large language models. Through regret analysis, the multi-factored performance trade-off is analyzed, demonstrating the potential efficiency of the proposed approach in a wide range of applications. <div>
arXiv:2511.09047v1 Announce Type: new 
Abstract: Interactive preference elicitation (IPE) aims to substantially reduce human effort while acquiring human preferences in wide personalization systems. Dueling bandit (DB) algorithms enable optimal decision-making in IPE building on pairwise comparisons. However, they remain inefficient when human feedback is sparse. Existing methods address sparsity by heavily relying on parametric reward models, whose rigid assumptions are vulnerable to misspecification. In contrast, we explore an alternative perspective based on feedback augmentation, and introduce critical improvements to the model-free DB framework. Specifically, we introduce augmented confidence bounds to integrate augmented human feedback under generalized concentration properties, and analyze the multi-factored performance trade-off via regret analysis. Our prototype algorithm achieves competitive performance across several IPE benchmarks, including recommendation, multi-objective optimization, and response optimization for large language models, demonstrating the potential of our approach for provably efficient IPE in broader applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guaranteeing Conservation of Integrals with Projection in Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2511.09048</link>
<guid>https://arxiv.org/abs/2511.09048</guid>
<content:encoded><![CDATA[
<div> Keywords: projection method, Physics-Informed Neural Networks (PINNs), conservation of integral quantities, partial differential equations (PDEs), non-linear optimization

Summary: 
The article proposes a novel projection method to ensure the conservation of integral quantities in Physics-Informed Neural Networks (PINNs). While PINNs use a soft constraint to enforce partial differential equations (PDEs) structure, this can lead to violations of physical laws in the discovered solution. The projection method guarantees the conservation of linear and quadratic integrals separately and jointly by solving constrained non-linear optimization problems. The introduced method, called PINN-Proj, significantly reduces the error in conservation of quantities and marginally decreases the PDE solution error. Evidence suggests that the projection enhances convergence by improving the loss landscape conditioning. This approach offers a general framework to ensure the conservation of any integral quantity in a PINN if a feasible solution exists. 

<br /><br />Summary: <div>
arXiv:2511.09048v1 Announce Type: new 
Abstract: We propose a novel projection method that guarantees the conservation of integral quantities in Physics-Informed Neural Networks (PINNs). While the soft constraint that PINNs use to enforce the structure of partial differential equations (PDEs) enables necessary flexibility during training, it also permits the discovered solution to violate physical laws. To address this, we introduce a projection method that guarantees the conservation of the linear and quadratic integrals, both separately and jointly. We derived the projection formulae by solving constrained non-linear optimization problems and found that our PINN modified with the projection, which we call PINN-Proj, reduced the error in the conservation of these quantities by three to four orders of magnitude compared to the soft constraint and marginally reduced the PDE solution error. We also found evidence that the projection improved convergence through improving the conditioning of the loss landscape. Our method holds promise as a general framework to guarantee the conservation of any integral quantity in a PINN if a tractable solution exists.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Break the Tie: Learning Cluster-Customized Category Relationships for Categorical Data Clustering</title>
<link>https://arxiv.org/abs/2511.09049</link>
<guid>https://arxiv.org/abs/2511.09049</guid>
<content:encoded><![CDATA[
<div> distance metrics, categorical attributes, cluster analysis, learnable relationships, clustering accuracy  
Summary:  
- Categorical attributes pose challenges in cluster analysis due to lack of defined relationships between attribute values.  
- Existing distance metrics assume fixed topological relationships, limiting adaptability to different cluster structures.  
- This paper introduces a method to learn customizable distance metrics for improved clustering performance.  
- The learned category relationships enhance algorithm fit and are compatible with Euclidean distance metrics.  
- Experimental results on real datasets demonstrate significantly higher clustering accuracy compared to current best-performing methods.   <div>
arXiv:2511.09049v1 Announce Type: new 
Abstract: Categorical attributes with qualitative values are ubiquitous in cluster analysis of real datasets. Unlike the Euclidean distance of numerical attributes, the categorical attributes lack well-defined relationships of their possible values (also called categories interchangeably), which hampers the exploration of compact categorical data clusters. Although most attempts are made for developing appropriate distance metrics, they typically assume a fixed topological relationship between categories when learning distance metrics, which limits their adaptability to varying cluster structures and often leads to suboptimal clustering performance. This paper, therefore, breaks the intrinsic relationship tie of attribute categories and learns customized distance metrics suitable for flexibly and accurately revealing various cluster distributions. As a result, the fitting ability of the clustering algorithm is significantly enhanced, benefiting from the learnable category relationships. Moreover, the learned category relationships are proved to be Euclidean distance metric-compatible, enabling a seamless extension to mixed datasets that include both numerical and categorical attributes. Comparative experiments on 12 real benchmark datasets with significance tests show the superior clustering accuracy of the proposed method with an average ranking of 1.25, which is significantly higher than the 5.21 ranking of the current best-performing method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Human-Corrected Labels Learning: Enhancing Labels Quality via Human Correction of VLMs Discrepancies</title>
<link>https://arxiv.org/abs/2511.09063</link>
<guid>https://arxiv.org/abs/2511.09063</guid>
<content:encoded><![CDATA[
<div> Keywords: Vision-Language Models, Human-Corrected Labels, label quality, error correction mechanisms, weak supervision

Summary:
Vision-Language Models (VLMs) have been used for data annotation but face issues with low-quality labels and lack of error correction mechanisms. To address this, Human-Corrected Labels (HCLs) are introduced to efficiently correct VLM-generated noisy labels. HCL strategically involves human correction only for instances with discrepancies, leading to higher-quality annotations and reduced labor costs. A risk-consistent estimator is theoretically derived to incorporate human-corrected labels and VLM predictions for classifier training. Additionally, a conditional probability method is proposed to estimate label distribution using VLM outputs and model predictions. Extensive experiments show that the approach achieves superior classification performance and is robust to label noise, demonstrating the effectiveness of HCL in weak supervision scenarios. The code for this work is available at https://github.com/Lilianach24/HCL.git<br /><br />Summary: <div>
arXiv:2511.09063v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs), with their powerful content generation capabilities, have been successfully applied to data annotation processes. However, the VLM-generated labels exhibit dual limitations: low quality (i.e., label noise) and absence of error correction mechanisms. To enhance label quality, we propose Human-Corrected Labels (HCLs), a novel setting that efficient human correction for VLM-generated noisy labels. As shown in Figure 1(b), HCL strategically deploys human correction only for instances with VLM discrepancies, achieving both higher-quality annotations and reduced labor costs. Specifically, we theoretically derive a risk-consistent estimator that incorporates both human-corrected labels and VLM predictions to train classifiers. Besides, we further propose a conditional probability method to estimate the label distribution using a combination of VLM outputs and model predictions. Extensive experiments demonstrate that our approach achieves superior classification performance and is robust to label noise, validating the effectiveness of HCL in practical weak supervision scenarios. Code https://github.com/Lilianach24/HCL.git
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Factorization-in-Loop: Proximal Fill-in Minimization for Sparse Matrix Reordering</title>
<link>https://arxiv.org/abs/2511.09093</link>
<guid>https://arxiv.org/abs/2511.09093</guid>
<content:encoded><![CDATA[
<div> learning, reordering network, fill-ins reduction, sparse matrices, LU factorization 

Summary:
- The article addresses the issue of fill-ins in the LU factorization of large sparse matrices, which can increase memory usage and computational time.
- Learning a reordering network is proposed to minimize the \(l_1\) norm of triangular factors to approximate fill-ins.
- A graph encoder is used to predict row or column node scores for reordering.
- Reparameterization techniques are designed to bridge the gap between predicted node scores and resultant triangular factors.
- The proposed method shows a 20% reduction in fill-in number and a 17.8% reduction in LU factorization time compared to state-of-the-art baselines.

<br /><br />Summary: <div>
arXiv:2511.09093v1 Announce Type: new 
Abstract: Fill-ins are new nonzero elements in the summation of the upper and lower triangular factors generated during LU factorization. For large sparse matrices, they will increase the memory usage and computational time, and be reduced through proper row or column arrangement, namely matrix reordering. Finding a row or column permutation with the minimal fill-ins is NP-hard, and surrogate objectives are designed to derive fill-in reduction permutations or learn a reordering function. However, there is no theoretical guarantee between the golden criterion and these surrogate objectives. Here we propose to learn a reordering network by minimizing \(l_1\) norm of triangular factors of the reordered matrix to approximate the exact number of fill-ins. The reordering network utilizes a graph encoder to predict row or column node scores. For inference, it is easy and fast to derive the permutation from sorting algorithms for matrices. For gradient based optimization, there is a large gap between the predicted node scores and resultant triangular factors in the optimization objective. To bridge the gap, we first design two reparameterization techniques to obtain the permutation matrix from node scores. The matrix is reordered by multiplying the permutation matrix. Then we introduce the factorization process into the objective function to arrive at target triangular factors. The overall objective function is optimized with the alternating direction method of multipliers and proximal gradient descent. Experimental results on benchmark sparse matrix collection SuiteSparse show the fill-in number and LU factorization time reduction of our proposed method is 20% and 17.8% compared with state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPM: Federated Learning Using Second-order Optimization with Preconditioned Mixing of Local Parameters</title>
<link>https://arxiv.org/abs/2511.09100</link>
<guid>https://arxiv.org/abs/2511.09100</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, second-order optimization, preconditioned mixing, convergence, heterogeneous data<br /><br />Summary:<br /><br />1. This paper introduces Federated Preconditioned Mixing (FedPM), a new method in Federated Learning (FL) that incorporates second-order optimization techniques.<br /><br />2. Existing methods such as LocalNewton, LTDA, and FedSophia apply second-order optimization by conducting iterative updates on clients and using simple mixing of parameters on the server, but they encounter problems due to drift in local preconditioners, causing convergence issues especially in heterogeneous data environments.<br /><br />3. FedPM addresses this by decomposing the ideal second-order update, which uses globally preconditioned global gradients, into two components: parameter mixing on the server and local updates on clients. This approach enables preconditioned mixing of local parameters on the server and reduces the drift in local preconditioners.<br /><br />4. The authors provide a theoretical analysis showing that FedPM achieves superlinear convergence rates for strongly convex objectives when only a single local update is performed.<br /><br />5. Extensive experiments demonstrate that FedPM significantly improves test accuracy compared to traditional simple mixing methods, effectively exploiting the advantages of second-order optimization in federated learning.<br /><br /> <div>
arXiv:2511.09100v1 Announce Type: new 
Abstract: We propose Federated Preconditioned Mixing (FedPM), a novel Federated Learning (FL) method that leverages second-order optimization. Prior methods--such as LocalNewton, LTDA, and FedSophia--have incorporated second-order optimization in FL by performing iterative local updates on clients and applying simple mixing of local parameters on the server. However, these methods often suffer from drift in local preconditioners, which significantly disrupts the convergence of parameter training, particularly in heterogeneous data settings. To overcome this issue, we refine the update rules by decomposing the ideal second-order update--computed using globally preconditioned global gradients--into parameter mixing on the server and local parameter updates on clients. As a result, our FedPM introduces preconditioned mixing of local parameters on the server, effectively mitigating drift in local preconditioners.
  We provide a theoretical convergence analysis demonstrating a superlinear rate for strongly convex objectives in scenarios involving a single local update. To demonstrate the practical benefits of FedPM, we conducted extensive experiments. The results showed significant improvements with FedPM in the test accuracy compared to conventional methods incorporating simple mixing, fully leveraging the potential of second-order optimization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cost-Minimized Label-Flipping Poisoning Attack to LLM Alignment</title>
<link>https://arxiv.org/abs/2511.09105</link>
<guid>https://arxiv.org/abs/2511.09105</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, data poisoning, RLHF, label flipping, convex optimization<br /><br />Summary: This paper investigates vulnerabilities of large language models (LLMs) during the reinforcement learning with human feedback (RLHF) and direct preference optimization (DPO) alignment processes, focusing on data poisoning attacks. Specifically, it studies the minimal-cost poisoning attacks achievable by flipping preference labels without altering the textual outputs compared during alignment. The authors formulate this problem as a convex optimization problem constrained linearly and derive theoretical lower and upper bounds on the minimum attack cost required to steer the LLM’s policy toward an attacker-specified target. They also introduce a post-processing method that optimizes existing label-flipping attacks by reducing the number of flipped labels necessary while maintaining the attack’s effectiveness. Empirical evaluations reveal that this cost-minimization approach significantly lowers poisoning costs compared to baseline methods, especially when the reward model's feature dimension is small relative to the dataset size. These theoretical and empirical insights expose fundamental weaknesses in RLHF/DPO pipelines and provide practical tools to assess the robustness of LLM training against low-cost label flipping attacks, emphasizing a need for more secure alignment methodologies. <div>
arXiv:2511.09105v1 Announce Type: new 
Abstract: Large language models (LLMs) are increasingly deployed in real-world systems, making it critical to understand their vulnerabilities. While data poisoning attacks during RLHF/DPO alignment have been studied empirically, their theoretical foundations remain unclear. We investigate the minimum-cost poisoning attack required to steer an LLM's policy toward an attacker's target by flipping preference labels during RLHF/DPO, without altering the compared outputs. We formulate this as a convex optimization problem with linear constraints, deriving lower and upper bounds on the minimum attack cost. As a byproduct of this theoretical analysis, we show that any existing label-flipping attack can be post-processed via our proposed method to reduce the number of label flips required while preserving the intended poisoning effect. Empirical results demonstrate that this cost-minimization post-processing can significantly reduce poisoning costs over baselines, particularly when the reward model's feature dimension is small relative to the dataset size. These findings highlight fundamental vulnerabilities in RLHF/DPO pipelines and provide tools to evaluate their robustness against low-cost poisoning attacks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks</title>
<link>https://arxiv.org/abs/2511.09114</link>
<guid>https://arxiv.org/abs/2511.09114</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, cyber defence, graph neural networks, generalisability, Proximal Policy Optimisation<br /><br />Summary:<br /><br />This paper addresses the challenge of adapting deep reinforcement learning agents to defend computer networks with varying topology and size without retraining. The authors introduce Topological Extensions for Reinforcement Learning Agents (TERLA), which leverage heterogeneous graph neural network layers to create fixed-size latent embeddings representing network states. This fixed-size representation enhances the agent’s ability to generalize across different network configurations. Additionally, TERLA incorporates a reduced, fixed-size, semantically meaningful, and interpretable action space to improve action efficiency. The approach is applied to the standard Proximal Policy Optimisation (PPO) reinforcement learning model and evaluated within the Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4 environment, which simulates realistic network conditions including Intrusion Detection System (IDS) events and multiple defending agents. Results show that TERLA agents maintain comparable defensive performance to vanilla PPO agents while achieving higher action efficiency. Moreover, the generalisability of TERLA agents is confirmed by deploying a single agent multiple times to defend networks with different sizes and topologies, where it consistently demonstrates better defensive performance and efficiency. These contributions suggest TERLA offers a scalable and practical solution for autonomous cyber defence in dynamic real-world networks. <div>
arXiv:2511.09114v1 Announce Type: new 
Abstract: Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trusted Multi-view Learning for Long-tailed Classification</title>
<link>https://arxiv.org/abs/2511.09138</link>
<guid>https://arxiv.org/abs/2511.09138</guid>
<content:encoded><![CDATA[
<div> Keywords: class imbalance, multi-view learning, long-tailed classification, opinion aggregation, pseudo-data generation<br /><br />Summary:<br /><br />1. This paper addresses the challenging problem of class imbalance specifically in multi-view learning settings, focusing on long-tailed classification where some classes have very few samples.<br /><br />2. The authors propose TMLC (Trusted Multi-view Long-tailed Classification), a framework designed to improve trustworthiness and performance by focusing on two main components: opinion aggregation and pseudo-data generation.<br /><br />3. For opinion aggregation, inspired by Social Identity Theory, TMLC incorporates a group consensus mechanism that steers decision-making toward the majority group's preferred outcomes, enhancing reliability in multi-view predictions.<br /><br />4. In pseudo-data generation, the paper introduces a novel distance metric to modify the SMOTE algorithm for multi-view data, alongside an uncertainty-guided generation module to create high-quality synthetic samples that better address class imbalance.<br /><br />5. Extensive experiments on various long-tailed multi-view datasets demonstrate that TMLC achieves superior performance compared to existing methods, effectively mitigating the adverse effects of class imbalance. The authors provide their code publicly for reproducibility and further research at https://github.com/cncq-tang/TMLC. <div>
arXiv:2511.09138v1 Announce Type: new 
Abstract: Class imbalance has been extensively studied in single-view scenarios; however, addressing this challenge in multi-view contexts remains an open problem, with even scarcer research focusing on trustworthy solutions. In this paper, we tackle a particularly challenging class imbalance problem in multi-view scenarios: long-tailed classification. We propose TMLC, a Trusted Multi-view Long-tailed Classification framework, which makes contributions on two critical aspects: opinion aggregation and pseudo-data generation. Specifically, inspired by Social Identity Theory, we design a group consensus opinion aggregation mechanism that guides decision making toward the direction favored by the majority of the group. In terms of pseudo-data generation, we introduce a novel distance metric to adapt SMOTE for multi-view scenarios and develop an uncertainty-guided data generation module that produces high-quality pseudo-data, effectively mitigating the adverse effects of class imbalance. Extensive experiments on long-tailed multi-view datasets demonstrate that our model is capable of achieving superior performance. The code is released at https://github.com/cncq-tang/TMLC.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Global and Local Bounds in Gaussian Process Regression via Chaining</title>
<link>https://arxiv.org/abs/2511.09144</link>
<guid>https://arxiv.org/abs/2511.09144</guid>
<content:encoded><![CDATA[
<div> Gaussian process regression, uncertainty quantification, chaining method, kernel bounds, safety-critical applications<br /><br />Summary:<br /><br />1. The paper addresses limitations in existing Gaussian process regression (GPR) uncertainty bounds, which typically require knowledge of specific input features and depend on posterior mean and variance or hyperparameter tuning.<br />2. It proposes a novel chaining-based framework to estimate upper and lower bounds on the expected extreme values of the model over unseen data, without needing access to particular input locations.<br />3. The framework is further refined for commonly used kernels, including RBF and Matérn kernels, providing tighter bounds than generic methods.<br />4. Numerical tightness of the bounds is improved by avoiding analytical relaxations, enhancing practical utility.<br />5. The authors develop a new method for local uncertainty quantification that leverages the chaining geometry via partition diameters. This approach adapts to local data structure without relying on posterior variance scaling.<br />6. Experimental results on both synthetic and real-world datasets validate the theoretical improvements and demonstrate that the proposed method outperforms existing uncertainty bounds approaches in terms of robustness and accuracy.<br /><br />This study contributes a robust global and local uncertainty quantification framework in GPR suitable for safety-critical applications where reliable uncertainty estimates are essential. <div>
arXiv:2511.09144v1 Announce Type: new 
Abstract: Gaussian process regression (GPR) is a popular nonparametric Bayesian method that provides predictive uncertainty estimates and is widely used in safety-critical applications. While prior research has introduced various uncertainty bounds, most existing approaches require access to specific input features and rely on posterior mean and variance estimates or tuning hyperparameters. These limitations hinder robustness and fail to capture the model's global behavior in expectation. To address these limitations, we propose a chaining-based framework for estimating upper and lower bounds on the expected extreme values over unseen data, without requiring access to specific input locations. We provide kernel-specific refinements for commonly used kernels such as RBF and Mat\'ern, in which our bounds are tighter than generic constructions. We further improve numerical tightness by avoiding analytical relaxations. In addition to global estimation, we also develop a novel method for local uncertainty quantification at specified inputs. This approach leverages chaining geometry through partition diameters, adapting to local structure without relying on posterior variance scaling. Our experimental results validate the theoretical findings and demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Agents to Communicate Entirely in Latent Space</title>
<link>https://arxiv.org/abs/2511.09149</link>
<guid>https://arxiv.org/abs/2511.09149</guid>
<content:encoded><![CDATA[
<div> Keywords: latent communication, large language models, multi-agent systems, compression, chain-of-thought prompting  

<br /><br />Summary:  
This paper addresses a fundamental limitation in communication between large language model (LLM)-based agents, which traditionally rely on natural language to transmit information. Natural language restricts the depth and nuance of the rich internal latent states of LLMs, hindering effective collaborative problem-solving. To overcome this, the authors propose Interlat (Inter-agent Latent Space Communication), a novel paradigm that enables direct transmission of the LLMs’ last hidden states, conceptualized as a representation of the agent's "mind." Interlat also introduces an additional latent space compression mechanism that further reduces communication overhead through end-to-end latent reasoning, preserving essential information. Experimental evaluation shows that Interlat significantly outperforms fine-tuned chain-of-thought (CoT) prompting techniques and single-agent baselines by fostering more exploratory behavior and effectively utilizing latent internal data. Moreover, the compression step not only accelerates inference but also maintains competitive performance, validating the approach’s efficiency. This work serves as a feasibility study for fully latent space-based inter-agent communication, highlighting promising directions for future research in communication protocols among LLM agents and advancing multi-agent collaboration beyond natural language constraints. <div>
arXiv:2511.09149v1 Announce Type: new 
Abstract: While natural language is the de facto communication medium for LLM-based agents, it presents a fundamental constraint. The process of downsampling rich, internal latent states into discrete tokens inherently limits the depth and nuance of information that can be transmitted, thereby hindering collaborative problem-solving. Inspired by human mind-reading, we propose Interlat (Inter-agent Latent Space Communication), a paradigm that leverages the last hidden states of an LLM as a representation of its mind for direct transmission (termed latent communication). An additional compression process further compresses latent communication via entirely latent space reasoning. Experiments demonstrate that Interlat outperforms both fine-tuned chain-of-thought (CoT) prompting and single-agent baselines, promoting more exploratory behavior and enabling genuine utilization of latent information. Further compression not only substantially accelerates inference but also maintains competitive performance through an efficient information-preserving mechanism. We position this work as a feasibility study of entirely latent space inter-agent communication, and our results highlight its potential, offering valuable insights for future research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Feature Selection Through Group Discovery</title>
<link>https://arxiv.org/abs/2511.09166</link>
<guid>https://arxiv.org/abs/2511.09166</guid>
<content:encoded><![CDATA[
arXiv:2511.09166v1 Announce Type: new 
Abstract: Unsupervised feature selection (FS) is essential for high-dimensional learning tasks where labels are not available. It helps reduce noise, improve generalization, and enhance interpretability. However, most existing unsupervised FS methods evaluate features in isolation, even though informative signals often emerge from groups of related features. For example, adjacent pixels, functionally connected brain regions, or correlated financial indicators tend to act together, making independent evaluation suboptimal. Although some methods attempt to capture group structure, they typically rely on predefined partitions or label supervision, limiting their applicability. We propose GroupFS, an end-to-end, fully differentiable framework that jointly discovers latent feature groups and selects the most informative groups among them, without relying on fixed a priori groups or label supervision. GroupFS enforces Laplacian smoothness on both feature and sample graphs and applies a group sparsity regularizer to learn a compact, structured representation. Across nine benchmarks spanning images, tabular data, and biological datasets, GroupFS consistently outperforms state-of-the-art unsupervised FS in clustering and selects groups of features that align with meaningful patterns.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compact Memory for Continual Logistic Regression</title>
<link>https://arxiv.org/abs/2511.09167</link>
<guid>https://arxiv.org/abs/2511.09167</guid>
<content:encoded><![CDATA[
arXiv:2511.09167v1 Announce Type: new 
Abstract: Despite recent progress, continual learning still does not match the performance of batch training. To avoid catastrophic forgetting, we need to build compact memory of essential past knowledge, but no clear solution has yet emerged, even for shallow neural networks with just one or two layers. In this paper, we present a new method to build compact memory for logistic regression. Our method is based on a result by Khan and Swaroop [2021] who show the existence of optimal memory for such models. We formulate the search for the optimal memory as Hessian-matching and propose a probabilistic PCA method to estimate them. Our approach can drastically improve accuracy compared to Experience Replay. For instance, on Split-ImageNet, we get 60% accuracy compared to 30% obtained by replay with memory-size equivalent to 0.3% of the data size. Increasing the memory size to 2% further boosts the accuracy to 74%, closing the gap to the batch accuracy of 77.6% on this task. Our work opens a new direction for building compact memory that can also be useful in the future for continual deep learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization</title>
<link>https://arxiv.org/abs/2511.09173</link>
<guid>https://arxiv.org/abs/2511.09173</guid>
<content:encoded><![CDATA[
arXiv:2511.09173v1 Announce Type: new 
Abstract: Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FSampler: Training Free Acceleration of Diffusion Sampling via Epsilon Extrapolation</title>
<link>https://arxiv.org/abs/2511.09180</link>
<guid>https://arxiv.org/abs/2511.09180</guid>
<content:encoded><![CDATA[
arXiv:2511.09180v1 Announce Type: new 
Abstract: FSampler is a training free, sampler agnostic execution layer that accelerates diffusion sampling by reducing the number of function evaluations (NFE). FSampler maintains a short history of denoising signals (epsilon) from recent real model calls and extrapolates the next epsilon using finite difference predictors at second order, third order, or fourth order, falling back to lower order when history is insufficient. On selected steps the predicted epsilon substitutes the model call while keeping each sampler's update rule unchanged. Predicted epsilons are validated for finiteness and magnitude; a learning stabilizer rescales predictions on skipped steps to correct drift, and an optional gradient estimation stabilizer compensates local curvature. Protected windows, periodic anchors, and a cap on consecutive skips bound deviation over the trajectory. Operating at the sampler level, FSampler integrates with Euler/DDIM, DPM++ 2M/2S, LMS/AB2, and RES family exponential multistep methods and drops into standard workflows. FLUX.1 dev, Qwen Image, and Wan 2.2, FSampler reduces time by 8 to 22% and model calls by 15 to 25% at high fidelity (Structural Similarity Index (SSIM) 0.95 to 0.99), without altering sampler formulas. With an aggressive adaptive gate, reductions can reach 45 to 50% fewer model calls at lower fidelity (SSIM 0.73 to 0.74).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterated Population Based Training with Task-Agnostic Restarts</title>
<link>https://arxiv.org/abs/2511.09190</link>
<guid>https://arxiv.org/abs/2511.09190</guid>
<content:encoded><![CDATA[
arXiv:2511.09190v1 Announce Type: new 
Abstract: Hyperparameter Optimization (HPO) can lift the burden of tuning hyperparameters (HPs) of neural networks. HPO algorithms from the Population Based Training (PBT) family are efficient thanks to dynamically adjusting HPs every few steps of the weight optimization. Recent results indicate that the number of steps between HP updates is an important meta-HP of all PBT variants that can substantially affect their performance. Yet, no method or intuition is available for efficiently setting its value. We introduce Iterated Population Based Training (IPBT), a novel PBT variant that automatically adjusts this HP via restarts that reuse weight information in a task-agnostic way and leverage time-varying Bayesian optimization to reinitialize HPs. Evaluation on 8 image classification and reinforcement learning tasks shows that, on average, our algorithm matches or outperforms 5 previous PBT variants and other HPO algorithms (random search, ASHA, SMAC3), without requiring a budget increase or any changes to its HPs. The source code is available at https://github.com/AwesomeLemon/IPBT.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sure! Here's a short and concise title for your paper: "Contamination in Generated Text Detection Benchmarks"</title>
<link>https://arxiv.org/abs/2511.09200</link>
<guid>https://arxiv.org/abs/2511.09200</guid>
<content:encoded><![CDATA[
arXiv:2511.09200v1 Announce Type: new 
Abstract: Large language models are increasingly used for many applications. To prevent illicit use, it is desirable to be able to detect AI-generated text. Training and evaluation of such detectors critically depend on suitable benchmark datasets. Several groups took on the tedious work of collecting, curating, and publishing large and diverse datasets for this task. However, it remains an open challenge to ensure high quality in all relevant aspects of such a dataset. For example, the DetectRL benchmark exhibits relatively simple patterns of AI-generation in 98.5% of the Claude-LLM data. These patterns may include introductory words such as "Sure! Here is the academic article abstract:", or instances where the LLM rejects the prompted task. In this work, we demonstrate that detectors trained on such data use such patterns as shortcuts, which facilitates spoofing attacks on the trained detectors. We consequently reprocessed the DetectRL dataset with several cleansing operations. Experiments show that such data cleansing makes direct attacks more difficult. The reprocessed dataset is publicly available.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Mean-Shift Clustering</title>
<link>https://arxiv.org/abs/2511.09202</link>
<guid>https://arxiv.org/abs/2511.09202</guid>
<content:encoded><![CDATA[
arXiv:2511.09202v1 Announce Type: new 
Abstract: We present a stochastic version of the mean-shift clustering algorithm. In this stochastic version a randomly chosen sequence of data points move according to partial gradient ascent steps of the objective function. Theoretical results illustrating the convergence of the proposed approach, and its relative performances is evaluated on synthesized 2-dimensional samples generated by a Gaussian mixture distribution and compared with state-of-the-art methods. It can be observed that in most cases the stochastic mean-shift clustering outperforms the standard mean-shift. We also illustrate as a practical application the use of the presented method for speaker clustering.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoCo-MILP: Inter-Variable Contrastive and Intra-Constraint Competitive MILP Solution Prediction</title>
<link>https://arxiv.org/abs/2511.09209</link>
<guid>https://arxiv.org/abs/2511.09209</guid>
<content:encoded><![CDATA[
arXiv:2511.09209v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) is a cornerstone of combinatorial optimization, yet solving large-scale instances remains a significant computational challenge. Recently, Graph Neural Networks (GNNs) have shown promise in accelerating MILP solvers by predicting high-quality solutions. However, we identify that existing methods misalign with the intrinsic structure of MILP problems at two levels. At the leaning objective level, the Binary Cross-Entropy (BCE) loss treats variables independently, neglecting their relative priority and yielding plausible logits. At the model architecture level, standard GNN message passing inherently smooths the representations across variables, missing the natural competitive relationships within constraints. To address these challenges, we propose CoCo-MILP, which explicitly models inter-variable Contrast and intra-constraint Competition for advanced MILP solution prediction. At the objective level, CoCo-MILP introduces the Inter-Variable Contrastive Loss (VCL), which explicitly maximizes the embedding margin between variables assigned one versus zero. At the architectural level, we design an Intra-Constraint Competitive GNN layer that, instead of homogenizing features, learns to differentiate representations of competing variables within a constraint, capturing their exclusionary nature. Experimental results on standard benchmarks demonstrate that CoCo-MILP significantly outperforms existing learning-based approaches, reducing the solution gap by up to 68.12% compared to traditional solvers. Our code is available at https://github.com/happypu326/CoCo-MILP.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Free Clustering via Self-Supervised Consensus Maximization (Extended Version)</title>
<link>https://arxiv.org/abs/2511.09211</link>
<guid>https://arxiv.org/abs/2511.09211</guid>
<content:encoded><![CDATA[
arXiv:2511.09211v1 Announce Type: new 
Abstract: Clustering is a fundamental task in unsupervised learning, but most existing methods heavily rely on hyperparameters such as the number of clusters or other sensitive settings, limiting their applicability in real-world scenarios. To address this long-standing challenge, we propose a novel and fully parameter-free clustering framework via Self-supervised Consensus Maximization, named SCMax. Our framework performs hierarchical agglomerative clustering and cluster evaluation in a single, integrated process. At each step of agglomeration, it creates a new, structure-aware data representation through a self-supervised learning task guided by the current clustering structure. We then introduce a nearest neighbor consensus score, which measures the agreement between the nearest neighbor-based merge decisions suggested by the original representation and the self-supervised one. The moment at which consensus maximization occurs can serve as a criterion for determining the optimal number of clusters. Extensive experiments on multiple datasets demonstrate that the proposed framework outperforms existing clustering approaches designed for scenarios with an unknown number of clusters.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable protein design through Feynman-Kac steering</title>
<link>https://arxiv.org/abs/2511.09216</link>
<guid>https://arxiv.org/abs/2511.09216</guid>
<content:encoded><![CDATA[
arXiv:2511.09216v1 Announce Type: new 
Abstract: Diffusion-based models have recently enabled the generation of realistic and diverse protein structures, yet they remain limited in their ability to steer outcomes toward specific functional or biochemical objectives, such as binding affinity or sequence composition. Here we extend the Feynman-Kac (FK) steering framework, an inference-time control approach, to diffusion-based protein design. By coupling FK steering with structure generation, the method guides sampling toward desirable structural or energetic features while maintaining the diversity of the underlying diffusion process. To enable simultaneous generation of both sequence and structure properties, rewards are computed on models refined through ProteinMPNN and all-atom relaxation. Applied to binder design, FK steering consistently improves predicted interface energetics across diverse targets with minimal computational overhead. More broadly, this work demonstrates that inference-time FK control generalizes diffusion-based protein design to arbitrary, non-differentiable, and reward-agnostic objectives, providing a unified and model-independent framework for guided molecular generation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Planning in Branch-and-Bound: Model-Based Reinforcement Learning for Exact Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2511.09219</link>
<guid>https://arxiv.org/abs/2511.09219</guid>
<content:encoded><![CDATA[
arXiv:2511.09219v1 Announce Type: new 
Abstract: Mixed-Integer Linear Programming (MILP) lies at the core of many real-world combinatorial optimization (CO) problems, traditionally solved by branch-and-bound (B&amp;B). A key driver influencing B&amp;B solvers efficiency is the variable selection heuristic that guides branching decisions. Looking to move beyond static, hand-crafted heuristics, recent work has explored adapting traditional reinforcement learning (RL) algorithms to the B&amp;B setting, aiming to learn branching strategies tailored to specific MILP distributions. In parallel, RL agents have achieved remarkable success in board games, a very specific type of combinatorial problems, by leveraging environment simulators to plan via Monte Carlo Tree Search (MCTS). Building on these developments, we introduce Plan-and-Branch-and-Bound (PlanB&amp;B), a model-based reinforcement learning (MBRL) agent that leverages a learned internal model of the B&amp;B dynamics to discover improved branching strategies. Computational experiments empirically validate our approach, with our MBRL branching agent outperforming previous state-of-the-art RL methods across four standard MILP benchmarks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Distributed Training Architecture For Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2511.09261</link>
<guid>https://arxiv.org/abs/2511.09261</guid>
<content:encoded><![CDATA[
arXiv:2511.09261v1 Announce Type: new 
Abstract: In recent years, graph neural networks (GNNs) have been widely applied in tackling combinatorial optimization problems. However, existing methods still suffer from limited accuracy when addressing that on complex graphs and exhibit poor scalability, since full training requires loading the whole adjacent matrix and all embeddings at a time, the it may results in out of memory of a single machine. This limitation significantly restricts their applicability to large-scale scenarios. To address these challenges, we propose a distributed GNN-based training framework for combinatorial optimization. In details, firstly, large graph is partition into several small subgraphs. Then the individual subgraphs are full trained, providing a foundation for efficient local optimization. Finally, reinforcement learning (RL) are employed to take actions according to GNN output, to make sure the restrictions between cross nodes can be learned. Extensive experiments are conducted on both real large-scale social network datasets (e.g., Facebook, Youtube) and synthetically generated high-complexity graphs, which demonstrate that our framework outperforms state-of-the-art approaches in both solution quality and computational efficiency. Moreover, the experiments on large graph instances also validate the scalability of the model.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-step Predictive Coding Leads To Simplicity Bias</title>
<link>https://arxiv.org/abs/2511.09290</link>
<guid>https://arxiv.org/abs/2511.09290</guid>
<content:encoded><![CDATA[
arXiv:2511.09290v1 Announce Type: new 
Abstract: Predictive coding is a framework for understanding the formation of low-dimensional internal representations mirroring the environment's latent structure. The conditions under which such representations emerge remain unclear. In this work, we investigate how the prediction horizon and network depth shape the solutions of predictive coding tasks. Using a minimal abstract setting inspired by prior work, we show empirically and theoretically that sufficiently deep networks trained with multi-step prediction horizons consistently recover the underlying latent structure, a phenomenon explained through the Ordinary Least Squares estimator structure and biases in learning dynamics. We then extend these insights to nonlinear networks and complex datasets, including piecewise linear functions, MNIST, multiple latent states and higher dimensional state geometries. Our results provide a principled understanding of when and why predictive coding induces structured representations, bridging the gap between empirical observations and theoretical foundations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GuardFed: A Trustworthy Federated Learning Framework Against Dual-Facet Attacks</title>
<link>https://arxiv.org/abs/2511.09294</link>
<guid>https://arxiv.org/abs/2511.09294</guid>
<content:encoded><![CDATA[
arXiv:2511.09294v1 Announce Type: new 
Abstract: Federated learning (FL) enables privacy-preserving collaborative model training but remains vulnerable to adversarial behaviors that compromise model utility or fairness across sensitive groups. While extensive studies have examined attacks targeting either objective, strategies that simultaneously degrade both utility and fairness remain largely unexplored. To bridge this gap, we introduce the Dual-Facet Attack (DFA), a novel threat model that concurrently undermines predictive accuracy and group fairness. Two variants, Synchronous DFA (S-DFA) and Split DFA (Sp-DFA), are further proposed to capture distinct real-world collusion scenarios. Experimental results show that existing robust FL defenses, including hybrid aggregation schemes, fail to resist DFAs effectively. To counter these threats, we propose GuardFed, a self-adaptive defense framework that maintains a fairness-aware reference model using a small amount of clean server data augmented with synthetic samples. In each training round, GuardFed computes a dual-perspective trust score for every client by jointly evaluating its utility deviation and fairness degradation, thereby enabling selective aggregation of trustworthy updates. Extensive experiments on real-world datasets demonstrate that GuardFed consistently preserves both accuracy and fairness under diverse non-IID and adversarial conditions, achieving state-of-the-art performance compared with existing robust FL methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiently Transforming Neural Networks into Decision Trees: A Path to Ground Truth Explanations with RENTT</title>
<link>https://arxiv.org/abs/2511.09299</link>
<guid>https://arxiv.org/abs/2511.09299</guid>
<content:encoded><![CDATA[
arXiv:2511.09299v1 Announce Type: new 
Abstract: Although neural networks are a powerful tool, their widespread use is hindered by the opacity of their decisions and their black-box nature, which result in a lack of trustworthiness. To alleviate this problem, methods in the field of explainable Artificial Intelligence try to unveil how such automated decisions are made. But explainable AI methods are often plagued by missing faithfulness/correctness, meaning that they sometimes provide explanations that do not align with the neural network's decision and logic. Recently, transformations to decision trees have been proposed to overcome such problems. Unfortunately, they typically lack exactness, scalability, or interpretability as the size of the neural network grows. Thus, we generalize these previous results, especially by considering convolutional neural networks, recurrent neural networks, non-ReLU activation functions, and bias terms. Our findings are accompanied by rigorous proofs and we present a novel algorithm RENTT (Runtime Efficient Network to Tree Transformation) designed to compute an exact equivalent decision tree representation of neural networks in a manner that is both runtime and memory efficient. The resulting decision trees are multivariate and thus, possibly too complex to understand. To alleviate this problem, we also provide a method to calculate the ground truth feature importance for neural networks via the equivalent decision trees - for entire models (global), specific input regions (regional), or single decisions (local). All theoretical results are supported by detailed numerical experiments that emphasize two key aspects: the computational efficiency and scalability of our algorithm, and that only RENTT succeeds in uncovering ground truth explanations compared to conventional approximation methods like LIME and SHAP. All code is available at https://github.com/HelenaM23/RENTT .
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tensor Residual Circuit Neural Network Factorized with Matrix Product Operation</title>
<link>https://arxiv.org/abs/2511.09315</link>
<guid>https://arxiv.org/abs/2511.09315</guid>
<content:encoded><![CDATA[
arXiv:2511.09315v1 Announce Type: new 
Abstract: It is challenging to reduce the complexity of neural networks while maintaining their generalization ability and robustness, especially for practical applications. Conventional solutions for this problem incorporate quantum-inspired neural networks with Kronecker products and hybrid tensor neural networks with MPO factorization and fully-connected layers. Nonetheless, the generalization power and robustness of the fully-connected layers are not as outstanding as circuit models in quantum computing. In this paper, we propose a novel tensor circuit neural network (TCNN) that takes advantage of the characteristics of tensor neural networks and residual circuit models to achieve generalization ability and robustness with low complexity. The proposed activation operation and parallelism of the circuit in complex number field improves its non-linearity and efficiency for feature learning. Moreover, since the feature information exists in the parameters in both the real and imaginary parts in TCNN, an information fusion layer is proposed for merging features stored in those parameters to enhance the generalization capability. Experimental results confirm that TCNN showcases more outstanding generalization and robustness with its average accuracies on various datasets 2\%-3\% higher than those of the state-of-the-art compared models. More significantly, while other models fail to learn features under noise parameter attacking, TCNN still showcases prominent learning capability owing to its ability to prevent gradient explosion. Furthermore, it is comparable to the compared models on the number of trainable parameters and the CPU running time. An ablation study also indicates the advantage of the activation operation, the parallelism architecture and the information fusion layer.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture-of-Channels: Exploiting Sparse FFNs for Efficient LLMs Pre-Training and Inference</title>
<link>https://arxiv.org/abs/2511.09323</link>
<guid>https://arxiv.org/abs/2511.09323</guid>
<content:encoded><![CDATA[
arXiv:2511.09323v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable success across diverse artificial intelligence tasks, driven by scaling laws that correlate model size and training data with performance improvements. However, this scaling paradigm incurs substantial memory overhead, creating significant challenges for both training and inference. While existing research has primarily addressed parameter and optimizer state memory reduction, activation memory-particularly from feed-forward networks (FFNs)-has become the critical bottleneck, especially when FlashAttention is implemented. In this work, we conduct a detailed memory profiling of LLMs and identify FFN activations as the predominant source to activation memory overhead. Motivated by this, we introduce Mixture-of-Channels (MoC), a novel FFN architecture that selectively activates only the Top-K most relevant channels per token determined by SwiGLU's native gating mechanism. MoC substantially reduces activation memory during pre-training and improves inference efficiency by reducing memory access through partial weight loading into GPU SRAM. Extensive experiments validate that MoC delivers significant memory savings and throughput gains while maintaining competitive model performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARBLE: Multi-Armed Restless Bandits in Latent Markovian Environment</title>
<link>https://arxiv.org/abs/2511.09324</link>
<guid>https://arxiv.org/abs/2511.09324</guid>
<content:encoded><![CDATA[
arXiv:2511.09324v1 Announce Type: new 
Abstract: Restless Multi-Armed Bandits (RMABs) are powerful models for decision-making under uncertainty, yet classical formulations typically assume fixed dynamics, an assumption often violated in nonstationary environments. We introduce MARBLE (Multi-Armed Restless Bandits in a Latent Markovian Environment), which augments RMABs with a latent Markov state that induces nonstationary behavior. In MARBLE, each arm evolves according to a latent environment state that switches over time, making policy learning substantially more challenging. We further introduce the Markov-Averaged Indexability (MAI) criterion as a relaxed indexability assumption and prove that, despite unobserved regime switches, under the MAI criterion, synchronous Q-learning with Whittle Indices (QWI) converges almost surely to the optimal Q-function and the corresponding Whittle indices. We validate MARBLE on a calibrated simulator-embedded (digital twin) recommender system, where QWI consistently adapts to a shifting latent state and converges to an optimal policy, empirically corroborating our theoretical findings.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAMMA_FLOW: Guided Analysis of Multi-label spectra by MAtrix Factorization for Lightweight Operational Workflows</title>
<link>https://arxiv.org/abs/2511.09326</link>
<guid>https://arxiv.org/abs/2511.09326</guid>
<content:encoded><![CDATA[
arXiv:2511.09326v1 Announce Type: new 
Abstract: GAMMA_FLOW is an open-source Python package for real-time analysis of spectral data. It supports classification, denoising, decomposition, and outlier detection of both single- and multi-component spectra. Instead of relying on large, computationally intensive models, it employs a supervised approach to non-negative matrix factorization (NMF) for dimensionality reduction. This ensures a fast, efficient, and adaptable analysis while reducing computational costs. gamma_flow achieves classification accuracies above 90% and enables reliable automated spectral interpretation. Originally developed for gamma-ray spectra, it is applicable to any type of one-dimensional spectral data. As an open and flexible alternative to proprietary software, it supports various applications in research and industry.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-Based Feature Attribution for Explaining the Predictions of Any Classifier</title>
<link>https://arxiv.org/abs/2511.09332</link>
<guid>https://arxiv.org/abs/2511.09332</guid>
<content:encoded><![CDATA[
arXiv:2511.09332v1 Announce Type: new 
Abstract: The proliferation of complex, black-box AI models has intensified the need for techniques that can explain their decisions. Feature attribution methods have become a popular solution for providing post-hoc explanations, yet the field has historically lacked a formal problem definition. This paper addresses this gap by introducing a formal definition for the problem of feature attribution, which stipulates that explanations be supported by an underlying probability distribution represented by the given dataset. Our analysis reveals that many existing model-agnostic methods fail to meet this criterion, while even those that do often possess other limitations. To overcome these challenges, we propose Distributional Feature Attribution eXplanations (DFAX), a novel, model-agnostic method for feature attribution. DFAX is the first feature attribution method to explain classifier predictions directly based on the data distribution. We show through extensive experiments that DFAX is more effective and efficient than state-of-the-art baselines.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Decision Trees to Boolean Logic: A Fast and Unified SHAP Algorithm</title>
<link>https://arxiv.org/abs/2511.09376</link>
<guid>https://arxiv.org/abs/2511.09376</guid>
<content:encoded><![CDATA[
arXiv:2511.09376v1 Announce Type: new 
Abstract: SHapley Additive exPlanations (SHAP) is a key tool for interpreting decision tree ensembles by assigning contribution values to features. It is widely used in finance, advertising, medicine, and other domains. Two main approaches to SHAP calculation exist: Path-Dependent SHAP, which leverages the tree structure for efficiency, and Background SHAP, which uses a background dataset to estimate feature distributions.
  We introduce WOODELF, a SHAP algorithm that integrates decision trees, game theory, and Boolean logic into a unified framework. For each consumer, WOODELF constructs a pseudo-Boolean formula that captures their feature values, the structure of the decision tree ensemble, and the entire background dataset. It then leverages this representation to compute Background SHAP in linear time. WOODELF can also compute Path-Dependent SHAP, Shapley interaction values, Banzhaf values, and Banzhaf interaction values.
  WOODELF is designed to run efficiently on CPU and GPU hardware alike. Available via the WOODELF Python package, it is implemented using NumPy, SciPy, and CuPy without relying on custom C++ or CUDA code. This design enables fast performance and seamless integration into existing frameworks, supporting large-scale computation of SHAP and other game-theoretic values in practice.
  For example, on a dataset with 3,000,000 rows, 5,000,000 background samples, and 127 features, WOODELF computed all Background Shapley values in 162 seconds on CPU and 16 seconds on GPU - compared to 44 minutes required by the best method on any hardware platform, representing 16x and 165x speedups, respectively.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion-based Sinogram Interpolation for Limited Angle PET</title>
<link>https://arxiv.org/abs/2511.09383</link>
<guid>https://arxiv.org/abs/2511.09383</guid>
<content:encoded><![CDATA[
arXiv:2511.09383v1 Announce Type: new 
Abstract: Accurate PET imaging increasingly requires methods that support unconstrained detector layouts from walk-through designs to long-axial rings where gaps and open sides lead to severely undersampled sinograms. Instead of constraining the hardware to form complete cylinders, we propose treating the missing lines-of-responses as a learnable prior. Data-driven approaches, particularly generative models, offer a promising pathway to recover this missing information. In this work, we explore the use of conditional diffusion models to interpolate sparsely sampled sinograms, paving the way for novel, cost-efficient, and patient-friendly PET geometries in real clinical settings.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Potent but Stealthy: Rethink Profile Pollution against Sequential Recommendation via Bi-level Constrained Reinforcement Paradigm</title>
<link>https://arxiv.org/abs/2511.09392</link>
<guid>https://arxiv.org/abs/2511.09392</guid>
<content:encoded><![CDATA[
arXiv:2511.09392v1 Announce Type: new 
Abstract: Sequential Recommenders, which exploit dynamic user intents through interaction sequences, is vulnerable to adversarial attacks. While existing attacks primarily rely on data poisoning, they require large-scale user access or fake profiles thus lacking practicality. In this paper, we focus on the Profile Pollution Attack that subtly contaminates partial user interactions to induce targeted mispredictions. Previous PPA methods suffer from two limitations, i.e., i) over-reliance on sequence horizon impact restricts fine-grained perturbations on item transitions, and ii) holistic modifications cause detectable distribution shifts. To address these challenges, we propose a constrained reinforcement driven attack CREAT that synergizes a bi-level optimization framework with multi-reward reinforcement learning to balance adversarial efficacy and stealthiness. We first develop a Pattern Balanced Rewarding Policy, which integrates pattern inversion rewards to invert critical patterns and distribution consistency rewards to minimize detectable shifts via unbalanced co-optimal transport. Then we employ a Constrained Group Relative Reinforcement Learning paradigm, enabling step-wise perturbations through dynamic barrier constraints and group-shared experience replay, achieving targeted pollution with minimal detectability. Extensive experiments demonstrate the effectiveness of CREAT.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Abstract Gradient Training: A Unified Certification Framework for Data Poisoning, Unlearning, and Differential Privacy</title>
<link>https://arxiv.org/abs/2511.09400</link>
<guid>https://arxiv.org/abs/2511.09400</guid>
<content:encoded><![CDATA[
arXiv:2511.09400v1 Announce Type: new 
Abstract: The impact of inference-time data perturbation (e.g., adversarial attacks) has been extensively studied in machine learning, leading to well-established certification techniques for adversarial robustness. In contrast, certifying models against training data perturbations remains a relatively under-explored area. These perturbations can arise in three critical contexts: adversarial data poisoning, where an adversary manipulates training samples to corrupt model performance; machine unlearning, which requires certifying model behavior under the removal of specific training data; and differential privacy, where guarantees must be given with respect to substituting individual data points. This work introduces Abstract Gradient Training (AGT), a unified framework for certifying robustness of a given model and training procedure to training data perturbations, including bounded perturbations, the removal of data points, and the addition of new samples. By bounding the reachable set of parameters, i.e., establishing provable parameter-space bounds, AGT provides a formal approach to analyzing the behavior of models trained via first-order optimization methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatio-Temporal Graph Unlearning</title>
<link>https://arxiv.org/abs/2511.09404</link>
<guid>https://arxiv.org/abs/2511.09404</guid>
<content:encoded><![CDATA[
arXiv:2511.09404v1 Announce Type: new 
Abstract: Spatio-temporal graphs are widely used in modeling complex dynamic processes such as traffic forecasting, molecular dynamics, and healthcare monitoring. Recently, stringent privacy regulations such as GDPR and CCPA have introduced significant new challenges for existing spatio-temporal graph models, requiring complete unlearning of unauthorized data. Since each node in a spatio-temporal graph diffuses information globally across both spatial and temporal dimensions, existing unlearning methods primarily designed for static graphs and localized data removal cannot efficiently erase a single node without incurring costs nearly equivalent to full model retraining. Therefore, an effective approach for complete spatio-temporal graph unlearning is a pressing need. To address this, we propose CallosumNet, a divide-and-conquer spatio-temporal graph unlearning framework inspired by the corpus callosum structure that facilitates communication between the brain's two hemispheres. CallosumNet incorporates two novel techniques: (1) Enhanced Subgraph Construction (ESC), which adaptively constructs multiple localized subgraphs based on several factors, including biologically-inspired virtual ganglions; and (2) Global Ganglion Bridging (GGB), which reconstructs global spatio-temporal dependencies from these localized subgraphs, effectively restoring the full graph representation. Empirical results on four diverse real-world datasets show that CallosumNet achieves complete unlearning with only 1%-2% relative MAE loss compared to the gold model, significantly outperforming state-of-the-art baselines. Ablation studies verify the effectiveness of both proposed techniques.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probing then Editing: A Push-Pull Framework for Retain-Free Machine Unlearning in Industrial IoT</title>
<link>https://arxiv.org/abs/2511.09414</link>
<guid>https://arxiv.org/abs/2511.09414</guid>
<content:encoded><![CDATA[
arXiv:2511.09414v1 Announce Type: new 
Abstract: In dynamic Industrial Internet of Things (IIoT) environments, models need the ability to selectively forget outdated or erroneous knowledge. However, existing methods typically rely on retain data to constrain model behavior, which increases computational and energy burdens and conflicts with industrial data silos and privacy compliance requirements. To address this, we propose a novel retain-free unlearning framework, referred to as Probing then Editing (PTE). PTE frames unlearning as a probe-edit process: first, it probes the decision boundary neighborhood of the model on the to-be-forgotten class via gradient ascent and generates corresponding editing instructions using the model's own predictions. Subsequently, a push-pull collaborative optimization is performed: the push branch actively dismantles the decision region of the target class using the editing instructions, while the pull branch applies masked knowledge distillation to anchor the model's knowledge on retained classes to their original states. Benefiting from this mechanism, PTE achieves efficient and balanced knowledge editing using only the to-be-forgotten data and the original model. Experimental results demonstrate that PTE achieves an excellent balance between unlearning effectiveness and model utility across multiple general and industrial benchmarks such as CWRU and SCUT-FD.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformer Semantic Genetic Programming for d-dimensional Symbolic Regression Problems</title>
<link>https://arxiv.org/abs/2511.09416</link>
<guid>https://arxiv.org/abs/2511.09416</guid>
<content:encoded><![CDATA[
arXiv:2511.09416v1 Announce Type: new 
Abstract: Transformer Semantic Genetic Programming (TSGP) is a semantic search approach that uses a pre-trained transformer model as a variation operator to generate offspring programs with controlled semantic similarity to a given parent. Unlike other semantic GP approaches that rely on fixed syntactic transformations, TSGP aims to learn diverse structural variations that lead to solutions with similar semantics. We find that a single transformer model trained on millions of programs is able to generalize across symbolic regression problems of varying dimension. Evaluated on 24 real-world and synthetic datasets, TSGP significantly outperforms standard GP, SLIM_GSGP, Deep Symbolic Regression, and Denoising Autoencoder GP, achieving an average rank of 1.58 across all benchmarks. Moreover, TSGP produces more compact solutions than SLIM_GSGP, despite its higher accuracy. In addition, the target semantic distance $\mathrm{SD}_t$ is able to control the step size in the semantic space: small values of $\mathrm{SD}_t$ enable consistent improvement in fitness but often lead to larger programs, while larger values promote faster convergence and compactness. Thus, $\mathrm{SD}_t$ provides an effective mechanism for balancing exploration and exploitation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Several Supporting Evidences for the Adaptive Feature Program</title>
<link>https://arxiv.org/abs/2511.09425</link>
<guid>https://arxiv.org/abs/2511.09425</guid>
<content:encoded><![CDATA[
arXiv:2511.09425v1 Announce Type: new 
Abstract: Theoretically exploring the advantages of neural networks might be one of the most challenging problems in the AI era. An adaptive feature program has recently been proposed to analyze the feature learning characteristic property of neural networks in a more abstract way. Motivated by the celebrated Le Cam equivalence, we advocate the over-parametrized sequence models to further simplify the analysis of the training dynamics of adaptive feature program and present several supporting evidences for the adaptive feature program. More precisely, after having introduced the feature error measure (FEM) to characterize the quality of the learned feature, we show that the FEM is decreasing during the training process of several concrete adaptive feature models including linear regression, single/multiple index models, etc. We believe that this hints at the potential successes of the adaptive feature program.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Equivariance Meets Mechanistic Interpretability: Equivariant Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2511.09432</link>
<guid>https://arxiv.org/abs/2511.09432</guid>
<content:encoded><![CDATA[
arXiv:2511.09432v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have proven useful in disentangling the opaque activations of neural networks, primarily large language models, into sets of interpretable features. However, adapting them to domains beyond language, such as scientific data with group symmetries, introduces challenges that can hinder their effectiveness. We show that incorporating such group symmetries into the SAEs yields features more useful in downstream tasks. More specifically, we train autoencoders on synthetic images and find that a single matrix can explain how their activations transform as the images are rotated. Building on this, we develop adaptively equivariant SAEs that can adapt to the base model's level of equivariance. These adaptive SAEs discover features that lead to superior probing performance compared to regular SAEs, demonstrating the value of incorporating symmetries in mechanistic interpretability tools.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Guided Dynamic-UMAP for Personalized Federated Graph Learning</title>
<link>https://arxiv.org/abs/2511.09438</link>
<guid>https://arxiv.org/abs/2511.09438</guid>
<content:encoded><![CDATA[
arXiv:2511.09438v1 Announce Type: new 
Abstract: We propose a method that uses large language models to assist graph machine learning under personalization and privacy constraints. The approach combines data augmentation for sparse graphs, prompt and instruction tuning to adapt foundation models to graph tasks, and in-context learning to supply few-shot graph reasoning signals. These signals parameterize a Dynamic UMAP manifold of client-specific graph embeddings inside a Bayesian variational objective for personalized federated learning. The method supports node classification and link prediction in low-resource settings and aligns language model latent representations with graph structure via a cross-modal regularizer. We outline a convergence argument for the variational aggregation procedure, describe a differential privacy threat model based on a moments accountant, and present applications to knowledge graph completion, recommendation-style link prediction, and citation and product graphs. We also discuss evaluation considerations for benchmarking LLM-assisted graph machine learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How does the Performance of the Data-driven Traffic Flow Forecasting Models deteriorate with Increasing Forecasting Horizon? An Extensive Approach Considering Statistical, Machine Learning and Deep Learning Models</title>
<link>https://arxiv.org/abs/2511.09450</link>
<guid>https://arxiv.org/abs/2511.09450</guid>
<content:encoded><![CDATA[
arXiv:2511.09450v1 Announce Type: new 
Abstract: With rapid urbanization in recent decades, traffic congestion has intensified due to increased movement of people and goods. As planning shifts from demand-based to supply-oriented strategies, Intelligent Transportation Systems (ITS) have become essential for managing traffic within existing infrastructure. A core ITS function is traffic forecasting, enabling proactive measures like ramp metering, signal control, and dynamic routing through platforms such as Google Maps. This study assesses the performance of statistical, machine learning (ML), and deep learning (DL) models in forecasting traffic speed and flow using real-world data from California's Harbor Freeway, sourced from the Caltrans Performance Measurement System (PeMS). Each model was evaluated over 20 forecasting windows (up to 1 hour 40 minutes) using RMSE, MAE, and R-Square metrics. Results show ANFIS-GP performs best at early windows with RMSE of 0.038, MAE of 0.0276, and R-Square of 0.9983, while Bi-LSTM is more robust for medium-term prediction due to its capacity to model long-range temporal dependencies, achieving RMSE of 0.1863, MAE of 0.0833, and R-Square of 0.987 at a forecasting of 20. The degradation in model performance was quantified using logarithmic transformation, with slope values used to measure robustness. Among DL models, Bi-LSTM had the flattest slope (0.0454 RMSE, 0.0545 MAE for flow), whereas ANFIS-GP had 0.1058 for RMSE and 0.1037 for flow MAE. The study concludes by identifying hybrid models as a promising future direction.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Explainability in Solar Energetic Particle Event Prediction: A Global Feature Mapping Approach</title>
<link>https://arxiv.org/abs/2511.09475</link>
<guid>https://arxiv.org/abs/2511.09475</guid>
<content:encoded><![CDATA[
arXiv:2511.09475v1 Announce Type: new 
Abstract: Solar energetic particle (SEP) events, as one of the most prominent manifestations of solar activity, can generate severe hazardous radiation when accelerated by solar flares or shock waves formed aside from coronal mass ejections (CMEs). However, most existing data-driven methods used for SEP predictions are operated as black-box models, making it challenging for solar physicists to interpret the results and understand the underlying physical causes of such events rather than just obtain a prediction. To address this challenge, we propose a novel framework that integrates global explanations and ad-hoc feature mapping to enhance model transparency and provide deeper insights into the decision-making process. We validate our approach using a dataset of 341 SEP events, including 244 significant (>=10 MeV) proton events exceeding the Space Weather Prediction Center S1 threshold, spanning solar cycles 22, 23, and 24. Furthermore, we present an explainability-focused case study of major SEP events, demonstrating how our method improves explainability and facilitates a more physics-informed understanding of SEP event prediction.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Planning via Embedding Arithmetic: A Contrastive Approach to Strategic Reasoning</title>
<link>https://arxiv.org/abs/2511.09477</link>
<guid>https://arxiv.org/abs/2511.09477</guid>
<content:encoded><![CDATA[
arXiv:2511.09477v1 Announce Type: new 
Abstract: Planning in high-dimensional decision spaces is increasingly being studied through the lens of learned representations. Rather than training policies or value heads, we investigate whether planning can be carried out directly in an evaluation-aligned embedding space. We introduce SOLIS, which learns such a space using supervised contrastive learning. In this representation, outcome similarity is captured by proximity, and a single global advantage vector orients the space from losing to winning regions. Candidate actions are then ranked according to their alignment with this direction, reducing planning to vector operations in latent space. We demonstrate this approach in chess, where SOLIS uses only a shallow search guided by the learned embedding to reach competitive strength under constrained conditions. More broadly, our results suggest that evaluation-aligned latent planning offers a lightweight alternative to traditional dynamics models or policy learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting</title>
<link>https://arxiv.org/abs/2511.09478</link>
<guid>https://arxiv.org/abs/2511.09478</guid>
<content:encoded><![CDATA[
arXiv:2511.09478v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has demonstrated considerable potential for enhancing reasoning in large language models (LLMs). However, existing methods suffer from Gradient Starvation and Policy Degradation when training directly on samples with mixed difficulty. To mitigate this, prior approaches leverage Chain-of-Thought (CoT) data, but the construction of high-quality CoT annotations remains labor-intensive. Alternatively, curriculum learning strategies have been explored but frequently encounter challenges, such as difficulty mismatch, reliance on manual curriculum design, and catastrophic forgetting. To address these issues, we propose AdaCuRL, a Adaptive Curriculum Reinforcement Learning framework that integrates coarse-to-fine difficulty estimation with adaptive curriculum scheduling. This approach dynamically aligns data difficulty with model capability and incorporates a data revisitation mechanism to mitigate catastrophic forgetting. Furthermore, AdaCuRL employs adaptive reference and sparse KL strategies to prevent Policy Degradation. Extensive experiments across diverse reasoning benchmarks demonstrate that AdaCuRL consistently achieves significant performance improvements on both LLMs and MLLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PDAC: Efficient Coreset Selection for Continual Learning via Probability Density Awareness</title>
<link>https://arxiv.org/abs/2511.09487</link>
<guid>https://arxiv.org/abs/2511.09487</guid>
<content:encoded><![CDATA[
arXiv:2511.09487v1 Announce Type: new 
Abstract: Rehearsal-based Continual Learning (CL) maintains a limited memory buffer to store replay samples for knowledge retention, making these approaches heavily reliant on the quality of the stored samples. Current Rehearsal-based CL methods typically construct the memory buffer by selecting a representative subset (referred to as coresets), aiming to approximate the training efficacy of the full dataset with minimal storage overhead. However, mainstream Coreset Selection (CS) methods generally formulate the CS problem as a bi-level optimization problem that relies on numerous inner and outer iterations to solve, leading to substantial computational cost thus limiting their practical efficiency. In this paper, we aim to provide a more efficient selection logic and scheme for coreset construction. To this end, we first analyze the Mean Squared Error (MSE) between the buffer-trained model and the Bayes-optimal model through the perspective of localized error decomposition to investigate the contribution of samples from different regions to MSE suppression. Further theoretical and experimental analyses demonstrate that samples with high probability density play a dominant role in error suppression. Inspired by this, we propose the Probability Density-Aware Coreset (PDAC) method. PDAC leverages the Projected Gaussian Mixture (PGM) model to estimate each sample's joint density, enabling efficient density-prioritized buffer selection. Finally, we introduce the streaming Expectation Maximization (EM) algorithm to enhance the adaptability of PGM parameters to streaming data, yielding Streaming PDAC (SPDAC) for streaming scenarios. Extensive comparative experiments show that our methods outperforms other baselines across various CL settings while ensuring favorable efficiency.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoSynth: Automated Workflow Optimization for High-Quality Synthetic Dataset Generation via Monte Carlo Tree Search</title>
<link>https://arxiv.org/abs/2511.09488</link>
<guid>https://arxiv.org/abs/2511.09488</guid>
<content:encoded><![CDATA[
arXiv:2511.09488v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) of large language models (LLMs) for specialized tasks requires high-quality datasets, but manual curation is prohibitively expensive. Synthetic data generation offers scalability, but its effectiveness relies on complex, multi-stage workflows, integrating prompt engineering and model orchestration. Existing automated workflow methods face a cold start problem: they require labeled datasets for reward modeling, which is especially problematic for subjective, open-ended tasks with no objective ground truth. We introduce AutoSynth, a framework that automates workflow discovery and optimization without reference datasets by reframing the problem as a Monte Carlo Tree Search guided by a novel dataset-free hybrid reward. This reward enables meta-learning through two LLM-as-judge components: one evaluates sample quality using dynamically generated task-specific metrics, and another assesses workflow code and prompt quality. Experiments on subjective educational tasks show that while expert-designed workflows achieve higher human preference rates (96-99% win rates vs. AutoSynth's 40-51%), models trained on AutoSynth-generated data dramatically outperform baselines (40-51% vs. 2-5%) and match or surpass expert workflows on certain metrics, suggesting discovery of quality dimensions beyond human intuition. These results are achieved while reducing human effort from 5-7 hours to just 30 minutes (>90% reduction). AutoSynth tackles the cold start issue in data-centric AI, offering a scalable, cost-effective method for subjective LLM tasks. Code: https://github.com/bisz9918-maker/AutoSynth.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quasi-Newton Compatible Actor-Critic for Deterministic Policies</title>
<link>https://arxiv.org/abs/2511.09509</link>
<guid>https://arxiv.org/abs/2511.09509</guid>
<content:encoded><![CDATA[
arXiv:2511.09509v1 Announce Type: new 
Abstract: In this paper, we propose a second-order deterministic actor-critic framework in reinforcement learning that extends the classical deterministic policy gradient method to exploit curvature information of the performance function. Building on the concept of compatible function approximation for the critic, we introduce a quadratic critic that simultaneously preserves the true policy gradient and an approximation of the performance Hessian. A least-squares temporal difference learning scheme is then developed to estimate the quadratic critic parameters efficiently. This construction enables a quasi-Newton actor update using information learned by the critic, yielding faster convergence compared to first-order methods. The proposed approach is general and applicable to any differentiable policy class. Numerical examples demonstrate that the method achieves improved convergence and performance over standard deterministic actor-critic baselines.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GenePheno: Interpretable Gene Knockout-Induced Phenotype Abnormality Prediction from Gene Sequences</title>
<link>https://arxiv.org/abs/2511.09512</link>
<guid>https://arxiv.org/abs/2511.09512</guid>
<content:encoded><![CDATA[
arXiv:2511.09512v1 Announce Type: new 
Abstract: Exploring how genetic sequences shape phenotypes is a fundamental challenge in biology and a key step toward scalable, hypothesis-driven experimentation. The task is complicated by the large modality gap between sequences and phenotypes, as well as the pleiotropic nature of gene-phenotype relationships. Existing sequence-based efforts focus on the degree to which variants of specific genes alter a limited set of phenotypes, while general gene knockout induced phenotype abnormality prediction methods heavily rely on curated genetic information as inputs, which limits scalability and generalizability. As a result, the task of broadly predicting the presence of multiple phenotype abnormalities under gene knockout directly from gene sequences remains underexplored. We introduce GenePheno, the first interpretable multi-label prediction framework that predicts knockout induced phenotypic abnormalities from gene sequences. GenePheno employs a contrastive multi-label learning objective that captures inter-phenotype correlations, complemented by an exclusive regularization that enforces biological consistency. It further incorporates a gene function bottleneck layer, offering human interpretable concepts that reflect functional mechanisms behind phenotype formation. To support progress in this area, we curate four datasets with canonical gene sequences as input and multi-label phenotypic abnormalities induced by gene knockouts as targets. Across these datasets, GenePheno achieves state-of-the-art gene-centric Fmax and phenotype-centric AUC, and case studies demonstrate its ability to reveal gene functional mechanisms.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-Driven Digital-Time-Domain Inference Architectures for Tsetlin Machines</title>
<link>https://arxiv.org/abs/2511.09527</link>
<guid>https://arxiv.org/abs/2511.09527</guid>
<content:encoded><![CDATA[
arXiv:2511.09527v1 Announce Type: new 
Abstract: Machine learning fits model parameters to approximate input-output mappings, predicting unknown samples. However, these models often require extensive arithmetic computations during inference, increasing latency and power consumption. This paper proposes a digital-time-domain computing approach for Tsetlin machine (TM) inference process to address these challenges. This approach leverages a delay accumulation mechanism to mitigate the costly arithmetic sums of classes and employs a Winner-Takes-All scheme to replace conventional magnitude comparators. Specifically, a Hamming distance-driven time-domain scheme is implemented for multi-class TMs. Furthermore, differential delay paths, combined with a leading-ones-detector logarithmic delay compression digital-time-domain scheme, are utilised for the coalesced TMs, accommodating both binary-signed and exponential-scale delay accumulation issues. Compared to the functionally equivalent, post-implementation digital TM architecture baseline, the proposed architecture demonstrates orders-of-magnitude improvements in energy efficiency and throughput.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SiDGen: Structure-informed Diffusion for Generative modeling of Ligands for Proteins</title>
<link>https://arxiv.org/abs/2511.09529</link>
<guid>https://arxiv.org/abs/2511.09529</guid>
<content:encoded><![CDATA[
arXiv:2511.09529v1 Announce Type: new 
Abstract: Designing ligands that are both chemically valid and structurally compatible with protein binding pockets is a key bottleneck in computational drug discovery. Existing approaches either ignore structural context or rely on expensive, memory-intensive encoding that limits throughput and scalability. We present SiDGen (Structure-informed Diffusion Generator), a protein-conditioned diffusion framework that integrates masked SMILES generation with lightweight folding-derived features for pocket awareness. To balance expressivity with efficiency, SiDGen supports two conditioning pathways: a streamlined mode that pools coarse structural signals from protein embeddings and a full mode that injects localized pairwise biases for stronger coupling. A coarse-stride folding mechanism with nearest-neighbor upsampling alleviates the quadratic memory costs of pair tensors, enabling training on realistic sequence lengths. Learning stability is maintained through in-loop chemical validity checks and an invalidity penalty, while large-scale training efficiency is restored \textit{via} selective compilation, dataloader tuning, and gradient accumulation. In automated benchmarks, SiDGen generates ligands with high validity, uniqueness, and novelty, while achieving competitive performance in docking-based evaluations and maintaining reasonable molecular properties. These results demonstrate that SiDGen can deliver scalable, pocket-aware molecular design, providing a practical route to conditional generation for high-throughput drug discovery.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NSL-MT: Linguistically Informed Negative Samples for Efficient Machine Translation in Low-Resource Languages</title>
<link>https://arxiv.org/abs/2511.09537</link>
<guid>https://arxiv.org/abs/2511.09537</guid>
<content:encoded><![CDATA[
arXiv:2511.09537v1 Announce Type: new 
Abstract: We introduce Negative Space Learning MT (NSL-MT), a training method that teaches models what not to generate by encoding linguistic constraints as severity-weighted penalties in the loss function. NSL-MT increases limited parallel data with synthetically generated violations of target language grammar, explicitly penalizing the model when it assigns high probability to these linguistically invalid outputs. We demonstrate that NSL-MT delivers improvements across all architectures: 3-12\% BLEU gains for well-performing models and 56-89\% gains for models lacking descent initial support. Furthermore, NSL-MT provides a 5x data efficiency multiplier -- training with 1,000 examples matches or exceeds normal training with 5,000 examples. Thus, NSL-MT provides a data-efficient alternative training method for settings where there is limited annotated parallel corporas.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extrapolation to infinite model space of no-core shell model calculations using machine learning</title>
<link>https://arxiv.org/abs/2511.05061</link>
<guid>https://arxiv.org/abs/2511.05061</guid>
<content:encoded><![CDATA[
arXiv:2511.05061v1 Announce Type: cross 
Abstract: An ensemble of neural networks is employed to extrapolate no-core shell model (NCSM) results to infinite model space for light nuclei. We present a review of our neural network extrapolations of the NCSM results obtained with the Daejeon16 NN interaction in different model spaces and with different values of the NCSM basis parameter $\hbar\Omega$ for energies of nuclear states and root-mean-square (rms) radii of proton, neutron and matter distributions in light nuclei. The method yields convergent predictions with quantifiable uncertainties. Ground-state energies for $^{6}$Li, $^{6}$He, and the unbound $^{6}$Be, as well as the excited $(3^{+},0)$ and $(0^{+},1)$ states of $^{6}$Li, are obtained within a few hundred keV of experiment. The extrapolated radii of bound states converge well. In contrast, radii of unbound states in $^{6}$Be and $^{6}$Li do not stabilize.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Federated Learning for U.S. State-Level Financial Distress Modeling</title>
<link>https://arxiv.org/abs/2511.08588</link>
<guid>https://arxiv.org/abs/2511.08588</guid>
<content:encoded><![CDATA[
arXiv:2511.08588v1 Announce Type: cross 
Abstract: We present the first application of federated learning (FL) to the U.S. National Financial Capability Study, introducing an interpretable framework for predicting consumer financial distress across all 50 states and the District of Columbia without centralizing sensitive data. Our cross-silo FL setup treats each state as a distinct data silo, simulating real-world governance in nationwide financial systems. Unlike prior work, our approach integrates two complementary explainable AI techniques to identify both global (nationwide) and local (state-specific) predictors of financial hardship, such as contact from debt collection agencies. We develop a machine learning model specifically suited for highly categorical, imbalanced survey data. This work delivers a scalable, regulation-compliant blueprint for early warning systems in finance, demonstrating how FL can power socially responsible AI applications in consumer credit risk and financial inclusion.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GMTRouter: Personalized LLM Router over Multi-turn User Interactions</title>
<link>https://arxiv.org/abs/2511.08590</link>
<guid>https://arxiv.org/abs/2511.08590</guid>
<content:encoded><![CDATA[
arXiv:2511.08590v1 Announce Type: cross 
Abstract: Large Language Model (LLM) routing has demonstrated strong capability in balancing response quality with computational cost. As users exhibit diverse preferences, personalization has attracted increasing attention in LLM routing, since even identical queries may require different models to generate responses tailored to individual needs. However, existing approaches are not fully personalized and often fail to capture the complex interactions between specific users and LLMs. Moreover, user preference data is typically scarce, noisy, and inconsistent in format, which limits the effectiveness of methods that rely solely on user-specific data. To address these challenges, we propose GMTRouter, which represents multi-turn user-LLM interactions as a heterogeneous graph with four node types: user, LLM, query, and response, thereby preserving the rich relational structure of the interaction. Through a tailored message-passing mechanism, GMTRouter learns to capture user preferences from few-shot data within a lightweight inductive graph learning framework, enabling effective personalization. Extensive experiments demonstrate that GMTRouter consistently outperforms strong baselines, achieving 0.9 to 21.6 percent higher accuracy and 0.006 to 0.309 higher AUC across multiple datasets. More importantly, we demonstrate that GMTRouter can adapt to new users and evolving preferences using only few-shot data, without extensive fine-tuning. The code for GMTRouter is publicly available at https://github.com/ulab-uiuc/GMTRouter.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Case Study: Transformer-Based Solution for the Automatic Digitization of Gas Plants</title>
<link>https://arxiv.org/abs/2511.08609</link>
<guid>https://arxiv.org/abs/2511.08609</guid>
<content:encoded><![CDATA[
arXiv:2511.08609v1 Announce Type: cross 
Abstract: The energy transition is a key theme of the last decades to determine a future of eco-sustainability, and an area of such importance cannot disregard digitization, innovation and the new technological tools available. This is the context in which the Generative Artificial Intelligence models described in this paper are positioned, developed by Engineering Ingegneria Informatica SpA in order to automate the plant structures acquisition of SNAM energy infrastructure, a leading gas transportation company in Italy and Europe. The digitization of a gas plant consists in registering all its relevant information through the interpretation of the related documentation. The aim of this work is therefore to design an effective solution based on Artificial Intelligence techniques to automate the extraction of the information necessary for the digitization of a plant, in order to streamline the daily work of MGM users. The solution received the P&amp;ID of the plant as input, each one in pdf format, and uses OCR, Vision LLM, Object Detection, Relational Reasoning and optimization algorithms to return an output consisting of two sets of information: a structured overview of the relevant design data and the hierarchical framework of the plant. To achieve convincing results, we extend a state-of-the-art model for Scene Graph Generation introducing a brand new Transformer architecture with the aim of deepening the analysis of the complex relations between the plant's components. The synergistic use of the listed AI-based technologies allowed to overcome many obstacles arising from the high variety of data, due to the lack of standardization. An accuracy of 91\% has been achieved in the extraction of textual information relating to design data. Regarding the plants topology, 93\% of components are correctly identified and the hierarchical structure is extracted with an accuracy around 80\%.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoE-GraphSAGE-Based Integrated Evaluation of Transient Rotor Angle and Voltage Stability in Power Systems</title>
<link>https://arxiv.org/abs/2511.08610</link>
<guid>https://arxiv.org/abs/2511.08610</guid>
<content:encoded><![CDATA[
arXiv:2511.08610v1 Announce Type: cross 
Abstract: The large-scale integration of renewable energy and power electronic devices has increased the complexity of power system stability, making transient stability assessment more challenging. Conventional methods are limited in both accuracy and computational efficiency. To address these challenges, this paper proposes MoE-GraphSAGE, a graph neural network framework based on the MoE for unified TAS and TVS assessment. The framework leverages GraphSAGE to capture the power grid's spatiotemporal topological features and employs multi-expert networks with a gating mechanism to model distinct instability modes jointly. Experimental results on the IEEE 39-bus system demonstrate that MoE-GraphSAGE achieves superior accuracy and efficiency, offering an effective solution for online multi-task transient stability assessment in complex power systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning based Modelling of Throttleable Engine Dynamics for Lunar Landing Mission</title>
<link>https://arxiv.org/abs/2511.08612</link>
<guid>https://arxiv.org/abs/2511.08612</guid>
<content:encoded><![CDATA[
arXiv:2511.08612v1 Announce Type: cross 
Abstract: Typical lunar landing missions involve multiple phases of braking to achieve soft-landing. The propulsion system configuration for these missions consists of throttleable engines. This configuration involves complex interconnected hydraulic, mechanical, and pneumatic components each exhibiting non-linear dynamic characteristics. Accurate modelling of the propulsion dynamics is essential for analyzing closed-loop guidance and control schemes during descent. This paper presents a learning-based system identification approach for modelling of throttleable engine dynamics using data obtained from high-fidelity propulsion model. The developed model is validated with experimental results and used for closed-loop guidance and control simulations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Drone Multi-View Dataset and Deep Learning Framework for Pedestrian Detection and Tracking</title>
<link>https://arxiv.org/abs/2511.08615</link>
<guid>https://arxiv.org/abs/2511.08615</guid>
<content:encoded><![CDATA[
arXiv:2511.08615v1 Announce Type: cross 
Abstract: Multi-drone surveillance systems offer enhanced coverage and robustness for pedestrian tracking, yet existing approaches struggle with dynamic camera positions and complex occlusions. This paper introduces MATRIX (Multi-Aerial TRacking In compleX environments), a comprehensive dataset featuring synchronized footage from eight drones with continuously changing positions, and a novel deep learning framework for multi-view detection and tracking. Unlike existing datasets that rely on static cameras or limited drone coverage, MATRIX provides a challenging scenario with 40 pedestrians and a significant architectural obstruction in an urban environment. Our framework addresses the unique challenges of dynamic drone-based surveillance through real-time camera calibration, feature-based image registration, and multi-view feature fusion in bird's-eye-view (BEV) representation. Experimental results demonstrate that while static camera methods maintain over 90\% detection and tracking precision and accuracy metrics in a simplified MATRIX environment without an obstruction, 10 pedestrians and a much smaller observational area, their performance significantly degrades in the complex environment. Our proposed approach maintains robust performance with $\sim$90\% detection and tracking accuracy, as well as successfully tracks $\sim$80\% of trajectories under challenging conditions. Transfer learning experiments reveal strong generalization capabilities, with the pretrained model achieving much higher detection and tracking accuracy performance compared to training the model from scratch. Additionally, systematic camera dropout experiments reveal graceful performance degradation, demonstrating practical robustness for real-world deployments where camera failures may occur. The MATRIX dataset and framework provide essential benchmarks for advancing dynamic multi-view surveillance systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reasoning on Time-Series for Financial Technical Analysis</title>
<link>https://arxiv.org/abs/2511.08616</link>
<guid>https://arxiv.org/abs/2511.08616</guid>
<content:encoded><![CDATA[
arXiv:2511.08616v1 Announce Type: cross 
Abstract: While Large Language Models have been used to produce interpretable stock forecasts, they mainly focus on analyzing textual reports but not historical price data, also known as Technical Analysis. This task is challenging as it switches between domains: the stock price inputs and outputs lie in the time-series domain, while the reasoning step should be in natural language. In this work, we introduce Verbal Technical Analysis (VTA), a novel framework that combine verbal and latent reasoning to produce stock time-series forecasts that are both accurate and interpretable. To reason over time-series, we convert stock price data into textual annotations and optimize the reasoning trace using an inverse Mean Squared Error (MSE) reward objective. To produce time-series outputs from textual reasoning, we condition the outputs of a time-series backbone model on the reasoning-based attributes. Experiments on stock datasets across U.S., Chinese, and European markets show that VTA achieves state-of-the-art forecasting accuracy, while the reasoning traces also perform well on evaluation by industry experts.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn More, Forget Less: A Gradient-Aware Data Selection Approach for LLM</title>
<link>https://arxiv.org/abs/2511.08620</link>
<guid>https://arxiv.org/abs/2511.08620</guid>
<content:encoded><![CDATA[
arXiv:2511.08620v1 Announce Type: cross 
Abstract: Despite large language models (LLMs) have achieved impressive achievements across numerous tasks, supervised fine-tuning (SFT) remains essential for adapting these models to specialized domains. However, SFT for domain specialization can be resource-intensive and sometimes leads to a deterioration in performance over general capabilities due to catastrophic forgetting (CF). To address these issues, we propose a self-adaptive gradient-aware data selection approach (GrADS) for supervised fine-tuning of LLMs, which identifies effective subsets of training data by analyzing gradients obtained from a preliminary training phase. Specifically, we design self-guided criteria that leverage the magnitude and statistical distribution of gradients to prioritize examples that contribute the most to the model's learning process. This approach enables the acquisition of representative samples that enhance LLMs understanding of domain-specific tasks. Through extensive experimentation with various LLMs across diverse domains such as medicine, law, and finance, GrADS has demonstrated significant efficiency and cost-effectiveness. Remarkably, utilizing merely 5% of the selected GrADS data, LLMs already surpass the performance of those fine-tuned on the entire dataset, and increasing to 50% of the data results in significant improvements! With catastrophic forgetting substantially mitigated simultaneously. We will release our code for GrADS later.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-period Learning for Financial Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08622</link>
<guid>https://arxiv.org/abs/2511.08622</guid>
<content:encoded><![CDATA[
arXiv:2511.08622v1 Announce Type: cross 
Abstract: Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (TSF). However, current TSF models either use only single-period input, or lack customized designs for addressing multi-period characteristics. In this paper, we propose a Multi-period Learning Framework (MLF) to enhance financial TSF performance. MLF considers both TSF's accuracy and efficiency requirements. Specifically, we design three new modules to better integrate the multi-period inputs for improving accuracy: (i) Inter-period Redundancy Filtering (IRF), that removes the information redundancy between periods for accurate self-attention modeling, (ii) Learnable Weighted-average Integration (LWI), that effectively integrates multi-period forecasts, (iii) Multi-period self-Adaptive Patching (MAP), that mitigates the bias towards certain periods by setting the same number of patches across all periods. Furthermore, we propose a Patch Squeeze module to reduce the number of patches in self-attention modeling for maximized efficiency. MLF incorporates multiple inputs with varying lengths (periods) to achieve better accuracy and reduces the costs of selecting input lengths during training. The codes and datasets are available at https://github.com/Meteor-Stars/MLF.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Field Interface-Aware Neural Operators for Multiphase Flow Simulation</title>
<link>https://arxiv.org/abs/2511.08625</link>
<guid>https://arxiv.org/abs/2511.08625</guid>
<content:encoded><![CDATA[
arXiv:2511.08625v1 Announce Type: cross 
Abstract: Multiphase flow systems, with their complex dynamics, field discontinuities, and interphase interactions, pose significant computational challenges for traditional numerical solvers. While neural operators offer efficient alternatives, they often struggle to achieve high-resolution numerical accuracy in these systems. This limitation primarily stems from the inherent spatial heterogeneity and the scarcity of high-quality training data in multiphase flows. In this work, we propose the Interface Information-Aware Neural Operator (IANO), a novel framework that explicitly leverages interface information as a physical prior to enhance the prediction accuracy. The IANO architecture introduces two key components: 1) An interface-aware multiple function encoding mechanism jointly models multiple physical fields and interfaces, thus capturing the high-frequency physical features at the interface. 2) A geometry-aware positional encoding mechanism further establishes the relationship between interface information, physical variables, and spatial positions, enabling it to achieve pointwise super-resolution prediction even in the low-data regimes. Experimental results demonstrate that IANO outperforms baselines by $\sim$10\% in accuracy for multiphase flow simulations while maintaining robustness under data-scarce and noise-perturbed conditions.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-to-Move: Training-Free Motion Controlled Video Generation via Dual-Clock Denoising</title>
<link>https://arxiv.org/abs/2511.08633</link>
<guid>https://arxiv.org/abs/2511.08633</guid>
<content:encoded><![CDATA[
arXiv:2511.08633v1 Announce Type: cross 
Abstract: Diffusion-based video generation can create realistic videos, yet existing image- and text-based conditioning fails to offer precise motion control. Prior methods for motion-conditioned synthesis typically require model-specific fine-tuning, which is computationally expensive and restrictive. We introduce Time-to-Move (TTM), a training-free, plug-and-play framework for motion- and appearance-controlled video generation with image-to-video (I2V) diffusion models. Our key insight is to use crude reference animations obtained through user-friendly manipulations such as cut-and-drag or depth-based reprojection. Motivated by SDEdit's use of coarse layout cues for image editing, we treat the crude animations as coarse motion cues and adapt the mechanism to the video domain. We preserve appearance with image conditioning and introduce dual-clock denoising, a region-dependent strategy that enforces strong alignment in motion-specified regions while allowing flexibility elsewhere, balancing fidelity to user intent with natural dynamics. This lightweight modification of the sampling process incurs no additional training or runtime cost and is compatible with any backbone. Extensive experiments on object and camera motion benchmarks show that TTM matches or exceeds existing training-based baselines in realism and motion control. Beyond this, TTM introduces a unique capability: precise appearance control through pixel-level conditioning, exceeding the limits of text-only prompting. Visit our project page for video examples and code: https://time-to-move.github.io/.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Suicidal Ideation in Text with Interpretable Deep Learning: A CNN-BiGRU with Attention Mechanism</title>
<link>https://arxiv.org/abs/2511.08636</link>
<guid>https://arxiv.org/abs/2511.08636</guid>
<content:encoded><![CDATA[
arXiv:2511.08636v1 Announce Type: cross 
Abstract: Worldwide, suicide is the second leading cause of death for adolescents with past suicide attempts to be an important predictor for increased future suicides. While some people with suicidal thoughts may try to suppress them, many signal their intentions in social media platforms. To address these issues, we propose a new type of hybrid deep learning scheme, i.e., the combination of a CNN architecture and a BiGRU technique, which can accurately identify the patterns of suicidal ideation from SN datasets. Also, we apply Explainable AI methods using SHapley Additive exPlanations to interpret the prediction results and verifying the model reliability. This integration of CNN local feature extraction, BiGRU bidirectional sequence modeling, attention mechanisms, and SHAP interpretability provides a comprehensive framework for suicide detection. Training and evaluation of the system were performed on a publicly available dataset. Several performance metrics were used for evaluating model performance. Our method was found to have achieved 93.97 accuracy in experimental results. Comparative study to different state-of-the-art Machine Learning and DL models and existing literature demonstrates the superiority of our proposed technique over all the competing methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern Recognition of Scrap Plastic Misclassification in Global Trade Data</title>
<link>https://arxiv.org/abs/2511.08638</link>
<guid>https://arxiv.org/abs/2511.08638</guid>
<content:encoded><![CDATA[
arXiv:2511.08638v1 Announce Type: cross 
Abstract: We propose an interpretable machine learning framework to help identify trade data discrepancies that are challenging to detect with traditional methods. Our system analyzes trade data to find a novel inverse price-volume signature, a pattern where reported volumes increase as average unit prices decrease.
  The model achieves 0.9375 accuracy and was validated by comparing large-scale UN data with detailed firm-level data, confirming that the risk signatures are consistent. This scalable tool provides customs authorities with a transparent, data-driven method to shift from conventional to priority-based inspection protocols, translating complex data into actionable intelligence to support international environmental policies.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compact Artificial Neural Network Models for Predicting Protein Residue - RNA Base Binding</title>
<link>https://arxiv.org/abs/2511.08648</link>
<guid>https://arxiv.org/abs/2511.08648</guid>
<content:encoded><![CDATA[
arXiv:2511.08648v1 Announce Type: cross 
Abstract: Large Artificial Neural Network (ANN) models have demonstrated success in various domains, including general text and image generation, drug discovery, and protein-RNA (ribonucleic acid) binding tasks. However, these models typically demand substantial computational resources, time, and data for effective training. Given that such extensive resources are often inaccessible to many researchers and that life sciences data sets are frequently limited, we investigated whether small ANN models could achieve acceptable accuracy in protein-RNA prediction. We experimented with shallow feed-forward ANNs comprising two hidden layers and various non-linearities. These models did not utilize explicit structural information; instead, a sliding window approach was employed to implicitly consider the context of neighboring residues and bases. We explored different training techniques to address the issue of highly unbalanced data. Among the seven most popular non-linearities for feed-forward ANNs, only three: Rectified Linear Unit (ReLU), Gated Linear Unit (GLU), and Hyperbolic Tangent (Tanh) yielded converging models. Common re-balancing techniques, such as under- and over-sampling of training sets, proved ineffective, whereas increasing the volume of training data and using model ensembles significantly improved performance. The optimal context window size, balancing both false negative and false positive errors, was found to be approximately 30 residues and bases. Our findings indicate that high-accuracy protein-RNA binding prediction is achievable using computing hardware accessible to most educational and research institutions.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>"It Looks All the Same to Me": Cross-index Training for Long-term Financial Series Prediction</title>
<link>https://arxiv.org/abs/2511.08658</link>
<guid>https://arxiv.org/abs/2511.08658</guid>
<content:encoded><![CDATA[
arXiv:2511.08658v1 Announce Type: cross 
Abstract: We investigate a number of Artificial Neural Network architectures (well-known and more ``exotic'') in application to the long-term financial time-series forecasts of indexes on different global markets. The particular area of interest of this research is to examine the correlation of these indexes' behaviour in terms of Machine Learning algorithms cross-training. Would training an algorithm on an index from one global market produce similar or even better accuracy when such a model is applied for predicting another index from a different market? The demonstrated predominately positive answer to this question is another argument in favour of the long-debated Efficient Market Hypothesis of Eugene Fama.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical and Performant Enhancements for Maximization of Algebraic Connectivity</title>
<link>https://arxiv.org/abs/2511.08694</link>
<guid>https://arxiv.org/abs/2511.08694</guid>
<content:encoded><![CDATA[
arXiv:2511.08694v1 Announce Type: cross 
Abstract: Long-term state estimation over graphs remains challenging as current graph estimation methods scale poorly on large, long-term graphs. To address this, our work advances a current state-of-the-art graph sparsification algorithm, maximizing algebraic connectivity (MAC). MAC is a sparsification method that preserves estimation performance by maximizing the algebraic connectivity, a spectral graph property that is directly connected to the estimation error. Unfortunately, MAC remains computationally prohibitive for online use and requires users to manually pre-specify a connectivity-preserving edge set. Our contributions close these gaps along three complementary fronts: we develop a specialized solver for algebraic connectivity that yields an average 2x runtime speedup; we investigate advanced step size strategies for MAC's optimization procedure to enhance both convergence speed and solution quality; and we propose automatic schemes that guarantee graph connectivity without requiring manual specification of edges. Together, these contributions make MAC more scalable, reliable, and suitable for real-time estimation applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Hardware Trojan Insertion in Industrial-Scale Designs</title>
<link>https://arxiv.org/abs/2511.08703</link>
<guid>https://arxiv.org/abs/2511.08703</guid>
<content:encoded><![CDATA[
arXiv:2511.08703v1 Announce Type: cross 
Abstract: Industrial Systems-on-Chips (SoCs) often comprise hundreds of thousands to millions of nets and millions to tens of millions of connectivity edges, making empirical evaluation of hardware-Trojan (HT) detectors on realistic designs both necessary and difficult. Public benchmarks remain significantly smaller and hand-crafted, while releasing truly malicious RTL raises ethical and operational risks. This work presents an automated and scalable methodology for generating HT-like patterns in industry-scale netlists whose purpose is to stress-test detection tools without altering user-visible functionality. The pipeline (i) parses large gate-level designs into connectivity graphs, (ii) explores rare regions using SCOAP testability metrics, and (iii) applies parameterized, function-preserving graph transformations to synthesize trigger-payload pairs that mimic the statistical footprint of stealthy HTs. When evaluated on the benchmarks generated in this work, representative state-of-the-art graph-learning models fail to detect Trojans. The framework closes the evaluation gap between academic circuits and modern SoCs by providing reproducible challenge instances that advance security research without sharing step-by-step attack instructions.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking generative image pretraining: How far are we from scaling up next-pixel prediction?</title>
<link>https://arxiv.org/abs/2511.08704</link>
<guid>https://arxiv.org/abs/2511.08704</guid>
<content:encoded><![CDATA[
arXiv:2511.08704v1 Announce Type: cross 
Abstract: This paper investigates the scaling properties of autoregressive next-pixel prediction, a simple, end-to-end yet under-explored framework for unified vision models. Starting with images at resolutions of 32x32, we train a family of Transformers using IsoFlops profiles across compute budgets up to 7e19 FLOPs and evaluate three distinct target metrics: next-pixel prediction objective, ImageNet classification accuracy, and generation quality measured by Fr'echet Distance. First, optimal scaling strategy is critically task-dependent. At a fixed 32x32 resolution alone, the optimal scaling properties for image classification and image generation diverge, where generation optimal setup requires the data size grow three to five times faster than for the classification optimal setup. Second, as image resolution increases, the optimal scaling strategy indicates that the model size must grow much faster than data size. Surprisingly, by projecting our findings, we discover that the primary bottleneck is compute rather than the amount of training data. As compute continues to grow four to five times annually, we forecast the feasibility of pixel-by-pixel modeling of images within the next five years.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Control of the Future via Prospective Foraging</title>
<link>https://arxiv.org/abs/2511.08717</link>
<guid>https://arxiv.org/abs/2511.08717</guid>
<content:encoded><![CDATA[
arXiv:2511.08717v1 Announce Type: cross 
Abstract: Optimal control of the future is the next frontier for AI. Current approaches to this problem are typically rooted in either reinforcement learning or online learning. While powerful, these frameworks for learning are mathematically distinct from Probably Approximately Correct (PAC) learning, which has been the workhorse for the recent technological achievements in AI. We therefore build on the prior work of prospective learning, an extension of PAC learning (without control) in non-stationary environments (De Silva et al., 2023; Silva et al., 2024; Bai et al., 2026). Here, we further extend the PAC learning framework to address learning and control in non-stationary environments. Using this framework, called ''Prospective Control'', we prove that under certain fairly general assumptions, empirical risk minimization (ERM) asymptotically achieves the Bayes optimal policy. We then consider a specific instance of prospective control, foraging, which is a canonical task for any mobile agent, be it natural or artificial. We illustrate that existing reinforcement learning algorithms fail to learn in these non-stationary environments, and even with modifications, they are orders of magnitude less efficient than our prospective foraging agents. Code is available at: https://github.com/neurodata/ProspectiveLearningwithControl.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical considerations when designing an online learning algorithm for an app-based mHealth intervention</title>
<link>https://arxiv.org/abs/2511.08719</link>
<guid>https://arxiv.org/abs/2511.08719</guid>
<content:encoded><![CDATA[
arXiv:2511.08719v1 Announce Type: cross 
Abstract: The ubiquitous nature of mobile health (mHealth) technology has expanded opportunities for the integration of reinforcement learning into traditional clinical trial designs, allowing researchers to learn individualized treatment policies during the study. LowSalt4Life 2 (LS4L2) is a recent trial aimed at reducing sodium intake among hypertensive individuals through an app-based intervention. A reinforcement learning algorithm, which was deployed in one of the trial arms, was designed to send reminder notifications to promote app engagement in contexts where the notification would be effective, i.e., when a participant is likely to open the app in the next 30-minute and not when prior data suggested reduced effectiveness. Such an algorithm can improve app-based mHealth interventions by reducing participant burden and more effectively promoting behavior change. We encountered various challenges during the implementation of the learning algorithm, which we present as a template to solving challenges in future trials that deploy reinforcement learning algorithms. We provide template solutions based on LS4L2 for solving the key challenges of (i) defining a relevant reward, (ii) determining a meaningful timescale for optimization, (iii) specifying a robust statistical model that allows for automation, (iv) balancing model flexibility with computational cost, and (v) addressing missing values in gradually collected data.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intuitive Programming, Adaptive Task Planning, and Dynamic Role Allocation in Human-Robot Collaboration</title>
<link>https://arxiv.org/abs/2511.08732</link>
<guid>https://arxiv.org/abs/2511.08732</guid>
<content:encoded><![CDATA[
arXiv:2511.08732v1 Announce Type: cross 
Abstract: Remarkable capabilities have been achieved by robotics and AI, mastering complex tasks and environments. Yet, humans often remain passive observers, fascinated but uncertain how to engage. Robots, in turn, cannot reach their full potential in human-populated environments without effectively modeling human states and intentions and adapting their behavior. To achieve a synergistic human-robot collaboration (HRC), a continuous information flow should be established: humans must intuitively communicate instructions, share expertise, and express needs. In parallel, robots must clearly convey their internal state and forthcoming actions to keep users informed, comfortable, and in control. This review identifies and connects key components enabling intuitive information exchange and skill transfer between humans and robots. We examine the full interaction pipeline: from the human-to-robot communication bridge translating multimodal inputs into robot-understandable representations, through adaptive planning and role allocation, to the control layer and feedback mechanisms to close the loop. Finally, we highlight trends and promising directions toward more adaptive, accessible HRC.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Deep Learning-Based Method for Fully Coupled Non-Markovian FBSDEs with Applications</title>
<link>https://arxiv.org/abs/2511.08735</link>
<guid>https://arxiv.org/abs/2511.08735</guid>
<content:encoded><![CDATA[
arXiv:2511.08735v1 Announce Type: cross 
Abstract: In this work, we extend deep learning-based numerical methods to fully coupled forward-backward stochastic differential equations (FBSDEs) within a non-Markovian framework. Error estimates and convergence are provided. In contrast to the existing literature, our approach not only analyzes the non-Markovian framework but also addresses fully coupled settings, in which both the drift and diffusion coefficients of the forward process may be random and depend on the backward components $Y$ and $Z$. Furthermore, we illustrate the practical applicability of our framework by addressing utility maximization problems under rough volatility, which are solved numerically with the proposed deep learning-based methods.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Symbolic Algebras for the Abstraction and Reasoning Corpus</title>
<link>https://arxiv.org/abs/2511.08747</link>
<guid>https://arxiv.org/abs/2511.08747</guid>
<content:encoded><![CDATA[
arXiv:2511.08747v1 Announce Type: cross 
Abstract: The Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) is a generative, few-shot fluid intelligence benchmark. Although humans effortlessly solve ARC-AGI, it remains extremely difficult for even the most advanced artificial intelligence systems. Inspired by methods for modelling human intelligence spanning neuroscience to psychology, we propose a cognitively plausible ARC-AGI solver. Our solver integrates System 1 intuitions with System 2 reasoning in an efficient and interpretable process using neurosymbolic methods based on Vector Symbolic Algebras (VSAs). Our solver works by object-centric program synthesis, leveraging VSAs to represent abstract objects, guide solution search, and enable sample-efficient neural learning. Preliminary results indicate success, with our solver scoring 10.8% on ARC-AGI-1-Train and 3.0% on ARC-AGI-1-Eval. Additionally, our solver performs well on simpler benchmarks, scoring 94.5% on Sort-of-ARC and 83.1% on 1D-ARC -- the latter outperforming GPT-4 at a tiny fraction of the computational cost. Importantly, our approach is unique; we believe we are the first to apply VSAs to ARC-AGI and have developed the most cognitively plausible ARC-AGI solver yet. Our code is available at: https://github.com/ijoffe/ARC-VSA-2025.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WATSON-Net: Vetting, Validation, and Analysis of Transits from Space Observations with Neural Networks</title>
<link>https://arxiv.org/abs/2511.08768</link>
<guid>https://arxiv.org/abs/2511.08768</guid>
<content:encoded><![CDATA[
arXiv:2511.08768v1 Announce Type: cross 
Abstract: Context. As the number of detected transiting exoplanet candidates continues to grow, the need for robust and scalable automated tools to prioritize or validate them has become increasingly critical. Among the most promising solutions, deep learning models offer the ability to interpret complex diagnostic metrics traditionally used in the vetting process. Aims. In this work, we present WATSON-Net, a new open-source neural network classifier and data preparation package designed to compete with current state-of-the-art tools for vetting and validation of transiting exoplanet signals from space-based missions. Methods. Trained on Kepler Q1-Q17 DR25 data using 10-fold cross-validation, WATSON-Net produces ten independent models, each evaluated on dedicated validation and test sets. The ten models are calibrated and prepared to be extensible for TESS data by standardizing the input pipeline, allowing for performance assessment across different space missions. Results. For Kepler targets, WATSON-Net achieves a recall-at-precision of 0.99 (R@P0.99) of 0.903, ranking second, with only the ExoMiner network performing better (R@P0.99 = 0.936). For TESS signals, WATSON-Net emerges as the best-performing non-fine-tuned machine learning classifier, achieving a precision of 0.93 and a recall of 0.76 on a test set comprising confirmed planets and false positives. Both the model and its data preparation tools are publicly available in the dearwatson Python package, fully open-source and integrated into the vetting engine of the SHERLOCK pipeline.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Probably Approximately Correct Learning Model in Computational Learning Theory</title>
<link>https://arxiv.org/abs/2511.08791</link>
<guid>https://arxiv.org/abs/2511.08791</guid>
<content:encoded><![CDATA[
arXiv:2511.08791v1 Announce Type: cross 
Abstract: This survey paper gives an overview of various known results on learning classes of Boolean functions in Valiant's Probably Approximately Correct (PAC) learning model and its commonly studied variants.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effects of label noise on the classification of outlier observations</title>
<link>https://arxiv.org/abs/2511.08808</link>
<guid>https://arxiv.org/abs/2511.08808</guid>
<content:encoded><![CDATA[
arXiv:2511.08808v1 Announce Type: cross 
Abstract: This study investigates the impact of adding noise to the training set classes in classification tasks using the BCOPS algorithm (Balanced and Conformal Optimized Prediction Sets), proposed by Guan & Tibshirani (2022). The BCOPS algorithm is an application of conformal prediction combined with a machine learning method to construct prediction sets such that the probability of the true class being included in the prediction set for a test observation meets a specified coverage guarantee. An observation is considered an outlier if its true class is not present in the training set. The study employs both synthetic and real datasets and conducts experiments to evaluate the prediction abstention rate for outlier observations and the model's robustness in this previously untested scenario. The results indicate that the addition of noise, even in small amounts, can have a significant effect on model performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural-Operator Preconditioned Newton Method for Accelerated Nonlinear Solvers</title>
<link>https://arxiv.org/abs/2511.08811</link>
<guid>https://arxiv.org/abs/2511.08811</guid>
<content:encoded><![CDATA[
arXiv:2511.08811v1 Announce Type: cross 
Abstract: We propose a novel neural preconditioned Newton (NP-Newton) method for solving parametric nonlinear systems of equations. To overcome the stagnation or instability of Newton iterations caused by unbalanced nonlinearities, we introduce a fixed-point neural operator (FPNO) that learns the direct mapping from the current iterate to the solution by emulating fixed-point iterations. Unlike traditional line-search or trust-region algorithms, the proposed FPNO adaptively employs negative step sizes to effectively mitigate the effects of unbalanced nonlinearities. Through numerical experiments we demonstrate the computational efficiency and robustness of the proposed NP-Newton method across multiple real-world applications, especially for very strong nonlinearities.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments</title>
<link>https://arxiv.org/abs/2511.08851</link>
<guid>https://arxiv.org/abs/2511.08851</guid>
<content:encoded><![CDATA[
arXiv:2511.08851v1 Announce Type: cross 
Abstract: In this paper, a measurement-driven framework is proposed for early radio link failure (RLF) prediction in 5G non-standalone (NSA) railway environments. Using 10 Hz metro-train traces with serving and neighbor-cell indicators, we benchmark six models, namely CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under varied observation windows and prediction horizons. When the observation window is three seconds, TimesNet attains the highest F1 score with a three-second prediction horizon, while CNN provides a favorable accuracy-latency tradeoff with a two-second horizon, enabling proactive actions such as redundancy and adaptive handovers. The results indicate that deep temporal models can anticipate reliability degradations several seconds in advance using lightweight features available on commercial devices, offering a practical path to early-warning control in 5G-based railway systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares</title>
<link>https://arxiv.org/abs/2511.08852</link>
<guid>https://arxiv.org/abs/2511.08852</guid>
<content:encoded><![CDATA[
arXiv:2511.08852v1 Announce Type: cross 
Abstract: In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations. Unlike conventional geometry or CSI-dependent approaches, the policy learns directly from uplink pilot responses and geometry features, enabling robust localization without explicit CSI estimation. An augmented WLS jointly estimates position and receiver clock bias, improving numerical stability under dynamic beam geometry. Across representative scenarios, the proposed method reduces the mean positioning error by 99.3% compared with the geometry-based baseline, achieving 0.395 m RMSE with near real-time inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When is a System Discoverable from Data? Discovery Requires Chaos</title>
<link>https://arxiv.org/abs/2511.08860</link>
<guid>https://arxiv.org/abs/2511.08860</guid>
<content:encoded><![CDATA[
arXiv:2511.08860v1 Announce Type: cross 
Abstract: The deep learning revolution has spurred a rise in advances of using AI in sciences. Within physical sciences the main focus has been on discovery of dynamical systems from observational data. Yet the reliability of learned surrogates and symbolic models is often undermined by the fundamental problem of non-uniqueness. The resulting models may fit the available data perfectly, but lack genuine predictive power. This raises the question: under what conditions can the systems governing equations be uniquely identified from a finite set of observations? We show, counter-intuitively, that chaos, typically associated with unpredictability, is crucial for ensuring a system is discoverable in the space of continuous or analytic functions. The prevalence of chaotic systems in benchmark datasets may have inadvertently obscured this fundamental limitation.
  More concretely, we show that systems chaotic on their entire domain are discoverable from a single trajectory within the space of continuous functions, and systems chaotic on a strange attractor are analytically discoverable under a geometric condition on the attractor. As a consequence, we demonstrate for the first time that the classical Lorenz system is analytically discoverable. Moreover, we establish that analytic discoverability is impossible in the presence of first integrals, common in real-world systems. These findings help explain the success of data-driven methods in inherently chaotic domains like weather forecasting, while revealing a significant challenge for engineering applications like digital twins, where stable, predictable behavior is desired. For these non-chaotic systems, we find that while trajectory data alone is insufficient, certain prior physical knowledge can help ensure discoverability. These findings warrant a critical re-evaluation of the fundamental assumptions underpinning purely data-driven discovery.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Histopathologic Glioblastoma Sub-regions with EfficientNet</title>
<link>https://arxiv.org/abs/2511.08896</link>
<guid>https://arxiv.org/abs/2511.08896</guid>
<content:encoded><![CDATA[
arXiv:2511.08896v1 Announce Type: cross 
Abstract: Glioblastoma (GBM) is the most common aggressive, fast-growing brain tumor, with a grim prognosis. Despite clinical diagnostic advancements, there have not been any substantial improvements to patient prognosis. Histopathological assessment of excised tumors is the first line of clinical diagnostic routine. We hypothesize that automated, robust, and accurate identification of distinct histological sub-regions within GBM could contribute to morphologically understanding this disease at scale. In this study, we designed a four-step deep learning approach to classify six (6) histopathological regions and quantitatively evaluated it on the BraTS-Path 2024 challenge dataset, which includes digitized Hematoxylin \& Eosin (H\&amp;E) stained GBM tissue sections annotated for six distinct regions. We used the challenge's publicly available training dataset to develop and evaluate the effectiveness of several variants of EfficientNet architectures (i.e., B0, B1, B2, B3, B4). EfficientNet-B1 and EfficientNet-B4 achieved the best performance, achieving an F1 score of 0.98 in a 5-fold cross-validation configuration using the BraTS-Path training set. The quantitative performance evaluation of our proposed approach with EfficientNet-B1 on the BraTS-Path hold-out validation data and the final hidden testing data yielded F1 scores of 0.546 and 0.517, respectively, for the associated 6-class classification task. The difference in the performance on training, validation, and testing data highlights the challenge of developing models that generalize well to new data, which is crucial for clinical applications. The source code of the proposed approach can be found at the GitHub repository of Indiana University Division of Computational Pathology: https://github.com/IUCompPath/brats-path-2024-enet.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boosting Adversarial Transferability via Ensemble Non-Attention</title>
<link>https://arxiv.org/abs/2511.08937</link>
<guid>https://arxiv.org/abs/2511.08937</guid>
<content:encoded><![CDATA[
arXiv:2511.08937v1 Announce Type: cross 
Abstract: Ensemble attacks integrate the outputs of surrogate models with diverse architectures, which can be combined with various gradient-based attacks to improve adversarial transferability. However, previous work shows unsatisfactory attack performance when transferring across heterogeneous model architectures. The main reason is that the gradient update directions of heterogeneous surrogate models differ widely, making it hard to reduce the gradient variance of ensemble models while making the best of individual model. To tackle this challenge, we design a novel ensemble attack, NAMEA, which for the first time integrates the gradients from the non-attention areas of ensemble models into the iterative gradient optimization process. Our design is inspired by the observation that the attention areas of heterogeneous models vary sharply, thus the non-attention areas of ViTs are likely to be the focus of CNNs and vice versa. Therefore, we merge the gradients respectively from the attention and non-attention areas of ensemble models so as to fuse the transfer information of CNNs and ViTs. Specifically, we pioneer a new way of decoupling the gradients of non-attention areas from those of attention areas, while merging gradients by meta-learning. Empirical evaluations on ImageNet dataset indicate that NAMEA outperforms AdaEA and SMER, the state-of-the-art ensemble attacks by an average of 15.0% and 9.6%, respectively. This work is the first attempt to explore the power of ensemble non-attention in boosting cross-architecture transferability, providing new insights into launching ensemble attacks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MicroEvoEval: A Systematic Evaluation Framework for Image-Based Microstructure Evolution Prediction</title>
<link>https://arxiv.org/abs/2511.08955</link>
<guid>https://arxiv.org/abs/2511.08955</guid>
<content:encoded><![CDATA[
arXiv:2511.08955v1 Announce Type: cross 
Abstract: Simulating microstructure evolution (MicroEvo) is vital for materials design but demands high numerical accuracy, efficiency, and physical fidelity. Although recent studies on deep learning (DL) offer a promising alternative to traditional solvers, the field lacks standardized benchmarks. Existing studies are flawed due to a lack of comparing specialized MicroEvo DL models with state-of-the-art spatio-temporal architectures, an overemphasis on numerical accuracy over physical fidelity, and a failure to analyze error propagation over time. To address these gaps, we introduce MicroEvoEval, the first comprehensive benchmark for image-based microstructure evolution prediction. We evaluate 14 models, encompassing both domain-specific and general-purpose architectures, across four representative MicroEvo tasks with datasets specifically structured for both short- and long-term assessment. Our multi-faceted evaluation framework goes beyond numerical accuracy and computational cost, incorporating a curated set of structure-preserving metrics to assess physical fidelity. Our extensive evaluations yield several key insights. Notably, we find that modern architectures (e.g., VMamba), not only achieve superior long-term stability and physical fidelity but also operate with an order-of-magnitude greater computational efficiency. The results highlight the necessity of holistic evaluation and identify these modern architectures as a highly promising direction for developing efficient and reliable surrogate models in data-driven materials science.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Finite Difference Approximation of Second Order Regularization of Neural-SDFs</title>
<link>https://arxiv.org/abs/2511.08980</link>
<guid>https://arxiv.org/abs/2511.08980</guid>
<content:encoded><![CDATA[
arXiv:2511.08980v1 Announce Type: cross 
Abstract: We introduce a finite-difference framework for curvature regularization in neural signed distance field (SDF) learning. Existing approaches enforce curvature priors using full Hessian information obtained via second-order automatic differentiation, which is accurate but computationally expensive. Others reduced this overhead by avoiding explicit Hessian assembly, but still required higher-order differentiation. In contrast, our method replaces these operations with lightweight finite-difference stencils that approximate second derivatives using the well known Taylor expansion with a truncation error of O(h^2), and can serve as drop-in replacements for Gaussian curvature and rank-deficiency losses. Experiments demonstrate that our finite-difference variants achieve reconstruction fidelity comparable to their automatic-differentiation counterparts, while reducing GPU memory usage and training time by up to a factor of two. Additional tests on sparse, incomplete, and non-CAD data confirm that the proposed formulation is robust and general, offering an efficient and scalable alternative for curvature-aware SDF learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepTracer: Tracing Stolen Model via Deep Coupled Watermarks</title>
<link>https://arxiv.org/abs/2511.08985</link>
<guid>https://arxiv.org/abs/2511.08985</guid>
<content:encoded><![CDATA[
arXiv:2511.08985v1 Announce Type: cross 
Abstract: Model watermarking techniques can embed watermark information into the protected model for ownership declaration by constructing specific input-output pairs. However, existing watermarks are easily removed when facing model stealing attacks, and make it difficult for model owners to effectively verify the copyright of stolen models. In this paper, we analyze the root cause of the failure of current watermarking methods under model stealing scenarios and then explore potential solutions. Specifically, we introduce a robust watermarking framework, DeepTracer, which leverages a novel watermark samples construction method and a same-class coupling loss constraint. DeepTracer can incur a high-coupling model between watermark task and primary task that makes adversaries inevitably learn the hidden watermark task when stealing the primary task functionality. Furthermore, we propose an effective watermark samples filtering mechanism that elaborately select watermark key samples used in model ownership verification to enhance the reliability of watermarks. Extensive experiments across multiple datasets and models demonstrate that our method surpasses existing approaches in defending against various model stealing attacks, as well as watermark attacks, and achieves new state-of-the-art effectiveness and robustness.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Sampling for Active Statistical Inference</title>
<link>https://arxiv.org/abs/2511.08991</link>
<guid>https://arxiv.org/abs/2511.08991</guid>
<content:encoded><![CDATA[
arXiv:2511.08991v1 Announce Type: cross 
Abstract: Active statistical inference is a new method for inference with AI-assisted data collection. Given a budget on the number of labeled data points that can be collected and assuming access to an AI predictive model, the basic idea is to improve estimation accuracy by prioritizing the collection of labels where the model is most uncertain. The drawback, however, is that inaccurate uncertainty estimates can make active sampling produce highly noisy results, potentially worse than those from naive uniform sampling. In this work, we present robust sampling strategies for active statistical inference. Robust sampling ensures that the resulting estimator is never worse than the estimator using uniform sampling. Furthermore, with reliable uncertainty estimates, the estimator usually outperforms standard active inference. This is achieved by optimally interpolating between uniform and active sampling, depending on the quality of the uncertainty scores, and by using ideas from robust optimization. We demonstrate the utility of the method on a series of real datasets from computational social science and survey research.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalisable prediction model of surgical case duration: multicentre development and temporal validation</title>
<link>https://arxiv.org/abs/2511.08994</link>
<guid>https://arxiv.org/abs/2511.08994</guid>
<content:encoded><![CDATA[
arXiv:2511.08994v1 Announce Type: cross 
Abstract: Background: Accurate prediction of surgical case duration underpins operating room (OR) scheduling, yet existing models often depend on site- or surgeon-specific inputs and rarely undergo external validation, limiting generalisability.
  Methods: We undertook a retrospective multicentre study using routinely collected perioperative data from two general hospitals in Japan (development: 1 January 2021-31 December 2023; temporal test: 1 January-31 December 2024). Elective weekday procedures with American Society of Anesthesiologists (ASA) Physical Status 1-4 were included. Pre-specified preoperative predictors comprised surgical context (year, month, weekday, scheduled duration, general anaesthesia indicator, body position) and patient factors (sex, age, body mass index, allergy, infection, comorbidity, ASA). Missing data were addressed by multiple imputation by chained equations. Four learners (elastic-net, generalised additive models, random forest, gradient-boosted trees) were tuned within internal-external cross-validation (IECV; leave-one-cluster-out by centre-year) and combined by stacked generalisation to predict log-transformed duration.
  Results: We analysed 63,206 procedures (development 45,647; temporal test 17,559). Cluster-specific and pooled errors and calibrations from IECV are provided with consistent performance across centres and years. In the 2024 temporal test cohort, calibration was good (intercept 0.423, 95%CI 0.372 to 0.474; slope 0.921, 95%CI 0.911 to 0.932).
  Conclusions: A stacked machine-learning model using only widely available preoperative variables achieved accurate, well-calibrated predictions in temporal external validation, supporting transportability across sites and over time. Such general-purpose tools may improve OR scheduling without relying on idiosyncratic inputs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence and Stability Analysis of Self-Consuming Generative Models with Heterogeneous Human Curation</title>
<link>https://arxiv.org/abs/2511.09002</link>
<guid>https://arxiv.org/abs/2511.09002</guid>
<content:encoded><![CDATA[
arXiv:2511.09002v1 Announce Type: cross 
Abstract: Self-consuming generative models have received significant attention over the last few years. In this paper, we study a self-consuming generative model with heterogeneous preferences that is a generalization of the model in Ferbach et al. (2024). The model is retrained round by round using real data and its previous-round synthetic outputs. The asymptotic behavior of the retraining dynamics is investigated across four regimes using different techniques including the nonlinear Perron--Frobenius theory. Our analyses improve upon that of Ferbach et al. (2024) and provide convergence results in settings where the well-known Banach contraction mapping arguments do not apply. Stability and non-stability results regarding the retraining dynamics are also given.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neurosymbolic Approach to Natural Language Formalization and Verification</title>
<link>https://arxiv.org/abs/2511.09008</link>
<guid>https://arxiv.org/abs/2511.09008</guid>
<content:encoded><![CDATA[
arXiv:2511.09008v1 Announce Type: cross 
Abstract: Large Language Models perform well at natural language interpretation and reasoning, but their inherent stochasticity limits their adoption in regulated industries like finance and healthcare that operate under strict policies. To address this limitation, we present a two-stage neurosymbolic framework that (1) uses LLMs with optional human guidance to formalize natural language policies, allowing fine-grained control of the formalization process, and (2) uses inference-time autoformalization to validate logical correctness of natural language statements against those policies. When correctness is paramount, we perform multiple redundant formalization steps at inference time, cross checking the formalizations for semantic equivalence. Our benchmarks demonstrate that our approach exceeds 99% soundness, indicating a near-zero false positive rate in identifying logical validity. Our approach produces auditable logical artifacts that substantiate the verification outcomes and can be used to improve the original text.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assumed Density Filtering and Smoothing with Neural Network Surrogate Models</title>
<link>https://arxiv.org/abs/2511.09016</link>
<guid>https://arxiv.org/abs/2511.09016</guid>
<content:encoded><![CDATA[
arXiv:2511.09016v1 Announce Type: cross 
Abstract: The Kalman filter and Rauch-Tung-Striebel (RTS) smoother are optimal for state estimation in linear dynamic systems. With nonlinear systems, the challenge consists in how to propagate uncertainty through the state transitions and output function. For the case of a neural network model, we enable accurate uncertainty propagation using a recent state-of-the-art analytic formula for computing the mean and covariance of a deep neural network with Gaussian input. We argue that cross entropy is a more appropriate performance metric than RMSE for evaluating the accuracy of filters and smoothers. We demonstrate the superiority of our method for state estimation on a stochastic Lorenz system and a Wiener system, and find that our method enables more optimal linear quadratic regulation when the state estimate is used for feedback.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepVRegulome: DNABERT-based deep-learning framework for predicting the functional impact of short genomic variants on the human regulome</title>
<link>https://arxiv.org/abs/2511.09026</link>
<guid>https://arxiv.org/abs/2511.09026</guid>
<content:encoded><![CDATA[
arXiv:2511.09026v1 Announce Type: cross 
Abstract: Whole-genome sequencing (WGS) has revealed numerous non-coding short variants whose functional impacts remain poorly understood. Despite recent advances in deep-learning genomic approaches, accurately predicting and prioritizing clinically relevant mutations in gene regulatory regions remains a major challenge. Here we introduce Deep VRegulome, a deep-learning method for prediction and interpretation of functionally disruptive variants in the human regulome, which combines 700 DNABERT fine-tuned models, trained on vast amounts of ENCODE gene regulatory regions, with variant scoring, motif analysis, attention-based visualization, and survival analysis. We showcase its application on TCGA glioblastoma WGS dataset in prioritizing survival-associated mutations and regulatory regions. The analysis identified 572 splice-disrupting and 9,837 transcription-factor binding site altering mutations occurring in greater than 10% of glioblastoma samples. Survival analysis linked 1352 mutations and 563 disrupted regulatory regions to patient outcomes, enabling stratification via non-coding mutation signatures. All the code, fine-tuned models, and an interactive data portal are publicly available.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PAN: A World Model for General, Interactable, and Long-Horizon World Simulation</title>
<link>https://arxiv.org/abs/2511.09057</link>
<guid>https://arxiv.org/abs/2511.09057</guid>
<content:encoded><![CDATA[
arXiv:2511.09057v1 Announce Type: cross 
Abstract: A world model enables an intelligent agent to imagine, predict, and reason about how the world evolves in response to its actions, and accordingly to plan and strategize. While recent video generation models produce realistic visual sequences, they typically operate in the prompt-to-full-video manner without causal control, interactivity, or long-horizon consistency required for purposeful reasoning. Existing world modeling efforts, on the other hand, often focus on restricted domains (e.g., physical, game, or 3D-scene dynamics) with limited depth and controllability, and struggle to generalize across diverse environments and interaction formats. In this work, we introduce PAN, a general, interactable, and long-horizon world model that predicts future world states through high-quality video simulation conditioned on history and natural language actions. PAN employs the Generative Latent Prediction (GLP) architecture that combines an autoregressive latent dynamics backbone based on a large language model (LLM), which grounds simulation in extensive text-based knowledge and enables conditioning on language-specified actions, with a video diffusion decoder that reconstructs perceptually detailed and temporally coherent visual observations, to achieve a unification between latent space reasoning (imagination) and realizable world dynamics (reality). Trained on large-scale video-action pairs spanning diverse domains, PAN supports open-domain, action-conditioned simulation with coherent, long-term dynamics. Extensive experiments show that PAN achieves strong performance in action-conditioned world simulation, long-horizon forecasting, and simulative reasoning compared to other video generators and world models, taking a step towards general world models that enable predictive simulation of future world states for reasoning and acting.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VAE-Based Synthetic EMG Generation with Mix-Consistency Loss for Recognizing Unseen Motion Combinations</title>
<link>https://arxiv.org/abs/2511.09060</link>
<guid>https://arxiv.org/abs/2511.09060</guid>
<content:encoded><![CDATA[
arXiv:2511.09060v1 Announce Type: cross 
Abstract: Electromyogram (EMG)-based motion classification using machine learning has been widely employed in applications such as prosthesis control. While previous studies have explored generating synthetic patterns of combined motions to reduce training data requirements, these methods assume that combined motions can be represented as linear combinations of basic motions. However, this assumption often fails due to complex neuromuscular phenomena such as muscle co-contraction, resulting in low-fidelity synthetic signals and degraded classification performance. To address this limitation, we propose a novel method that learns to synthesize combined motion patterns in a structured latent space. Specifically, we employ a variational autoencoder (VAE) to encode EMG signals into a low-dimensional representation and introduce a mixconsistency loss that structures the latent space such that combined motions are embedded between their constituent basic motions. Synthetic patterns are then generated within this structured latent space and used to train classifiers for recognizing unseen combined motions. We validated our approach through upper-limb motion classification experiments with eight healthy participants. The results demonstrate that our method outperforms input-space synthesis approaches, achieving approximately 30% improvement in accuracy.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Validate Generative Models: a Goodness-of-Fit Approach</title>
<link>https://arxiv.org/abs/2511.09118</link>
<guid>https://arxiv.org/abs/2511.09118</guid>
<content:encoded><![CDATA[
arXiv:2511.09118v1 Announce Type: cross 
Abstract: Generative models are increasingly central to scientific workflows, yet their systematic use and interpretation require a proper understanding of their limitations through rigorous validation. Classic approaches struggle with scalability, statistical power, or interpretability when applied to high-dimensional data, making it difficult to certify the reliability of these models in realistic, high-dimensional scientific settings. Here, we propose the use of the New Physics Learning Machine (NPLM), a learning based approach to goodness-of-fit testing inspired by the Neyman-Pearson construction, to test generative networks trained on high-dimensional scientific data. We demonstrate the performance of NPLM for validation in two benchmark cases: generative models trained on mixtures of Gaussian models with increasing dimensionality, and a public end-to-end generator for the Large Hadron Collider called FlashSim, trained on jet data, typical in the field of high-energy physics. We demonstrate that the NPLM can serve as a powerful validation method while also providing a means to diagnose sub-optimally modeled regions of the data.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoopTool: Closing the Data-Training Loop for Robust LLM Tool Calls</title>
<link>https://arxiv.org/abs/2511.09148</link>
<guid>https://arxiv.org/abs/2511.09148</guid>
<content:encoded><![CDATA[
arXiv:2511.09148v1 Announce Type: cross 
Abstract: Augmenting Large Language Models (LLMs) with external tools enables them to execute complex, multi-step tasks. However, tool learning is hampered by the static synthetic data pipelines where data generation and model training are executed as two separate, non-interactive processes. This approach fails to adaptively focus on a model's specific weaknesses and allows noisy labels to persist, degrading training efficiency. We introduce LoopTool, a fully automated, model-aware data evolution framework that closes this loop by tightly integrating data synthesis and model training. LoopTool iteratively refines both the data and the model through three synergistic modules: (1) Greedy Capability Probing (GCP) diagnoses the model's mastered and failed capabilities; (2) Judgement-Guided Label Verification (JGLV) uses an open-source judge model to find and correct annotation errors, progressively purifying the dataset; and (3) Error-Driven Data Expansion (EDDE) generates new, challenging samples based on identified failures. This closed-loop process operates within a cost-effective, open-source ecosystem, eliminating dependence on expensive closed-source APIs. Experiments show that our 8B model trained with LoopTool significantly surpasses its 32B data generator and achieves new state-of-the-art results on the BFCL-v3 and ACEBench benchmarks for its scale. Our work demonstrates that closed-loop, self-refining data pipelines can dramatically enhance the tool-use capabilities of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Mixed-Integer Optimization with Neural Constraints via Dual Decomposition</title>
<link>https://arxiv.org/abs/2511.09186</link>
<guid>https://arxiv.org/abs/2511.09186</guid>
<content:encoded><![CDATA[
arXiv:2511.09186v1 Announce Type: cross 
Abstract: Embedding deep neural networks (NNs) into mixed-integer programs (MIPs) is attractive for decision making with learned constraints, yet state-of-the-art monolithic linearisations blow up in size and quickly become intractable. In this paper, we introduce a novel dual-decomposition framework that relaxes the single coupling equality u=x with an augmented Lagrange multiplier and splits the problem into a vanilla MIP and a constrained NN block. Each part is tackled by the solver that suits it best-branch and cut for the MIP subproblem, first-order optimisation for the NN subproblem-so the model remains modular, the number of integer variables never grows with network depth, and the per-iteration cost scales only linearly with the NN size. On the public \textsc{SurrogateLIB} benchmark, our method proves \textbf{scalable}, \textbf{modular}, and \textbf{adaptable}: it runs \(120\times\) faster than an exact Big-M formulation on the largest test case; the NN sub-solver can be swapped from a log-barrier interior step to a projected-gradient routine with no code changes and identical objective value; and swapping the MLP for an LSTM backbone still completes the full optimisation in 47s without any bespoke adaptation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource-Efficient Variational Quantum Classifier</title>
<link>https://arxiv.org/abs/2511.09204</link>
<guid>https://arxiv.org/abs/2511.09204</guid>
<content:encoded><![CDATA[
arXiv:2511.09204v1 Announce Type: cross 
Abstract: Quantum computing promises a revolution in information processing, with significant potential for machine learning and classification tasks. However, achieving this potential requires overcoming several fundamental challenges. One key limitation arises at the prediction stage, where the intrinsic randomness of quantum model outputs necessitates repeated executions, resulting in substantial overhead. To overcome this, we propose a novel measurement strategy for a variational quantum classifier that allows us to define the unambiguous quantum classifier. This strategy achieves near-deterministic predictions while maintaining competitive classification accuracy in noisy environments, all with significantly fewer quantum circuit executions. Although this approach entails a slight reduction in performance, it represents a favorable trade-off for improved resource efficiency. We further validate our theoretical model with supporting experimental results.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Least-Squares Optimization for Data-Driven Predictive Control: A Geometric Approach</title>
<link>https://arxiv.org/abs/2511.09242</link>
<guid>https://arxiv.org/abs/2511.09242</guid>
<content:encoded><![CDATA[
arXiv:2511.09242v1 Announce Type: cross 
Abstract: The paper studies a geometrically robust least-squares problem that extends classical and norm-based robust formulations. Rather than minimizing residual error for fixed or perturbed data, we interpret least-squares as enforcing approximate subspace inclusion between measured and true data spaces. The uncertainty in this geometric relation is modeled as a metric ball on the Grassmannian manifold, leading to a min-max problem over Euclidean and manifold variables. The inner maximization admits a closed-form solution, enabling an efficient algorithm with a transparent geometric interpretation. Applied to robust finite-horizon linear-quadratic tracking in data-enabled predictive control, the method improves upon existing robust least-squares formulations, achieving stronger robustness and favorable scaling under small uncertainty.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Model Training to Model Raising - A call to reform AI model training paradigms from post-hoc alignment to intrinsic, identity-based development</title>
<link>https://arxiv.org/abs/2511.09287</link>
<guid>https://arxiv.org/abs/2511.09287</guid>
<content:encoded><![CDATA[
arXiv:2511.09287v1 Announce Type: cross 
Abstract: Current AI training methods align models with human values only after their core capabilities have been established, resulting in models that are easily misaligned and lack deep-rooted value systems. We propose a paradigm shift from "model training" to "model raising", in which alignment is woven into a model's development from the start. We identify several key components for this paradigm, all centered around redesigning the training corpus: reframing training data from a first-person perspective, recontextualizing information as lived experience, simulating social interactions, and scaffolding the ordering of training data. We expect that this redesign of the training corpus will lead to an early commitment to values from the first training token onward, such that knowledge, skills, and values are intrinsically much harder to separate. In an ecosystem in which large language model capabilities start overtaking human capabilities in many tasks, this seems to us like a critical need.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptDel: Adaptable Deletion Rate Randomized Smoothing for Certified Robustness</title>
<link>https://arxiv.org/abs/2511.09316</link>
<guid>https://arxiv.org/abs/2511.09316</guid>
<content:encoded><![CDATA[
arXiv:2511.09316v1 Announce Type: cross 
Abstract: We consider the problem of certified robustness for sequence classification against edit distance perturbations. Naturally occurring inputs of varying lengths (e.g., sentences in natural language processing tasks) present a challenge to current methods that employ fixed-rate deletion mechanisms and lead to suboptimal performance. To this end, we introduce AdaptDel methods with adaptable deletion rates that dynamically adjust based on input properties. We extend the theoretical framework of randomized smoothing to variable-rate deletion, ensuring sound certification with respect to edit distance. We achieve strong empirical results in natural language tasks, observing up to 30 orders of magnitude improvement to median cardinality of the certified region, over state-of-the-art certifications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Routesplain: Towards Faithful and Intervenable Routing for Software-related Tasks</title>
<link>https://arxiv.org/abs/2511.09373</link>
<guid>https://arxiv.org/abs/2511.09373</guid>
<content:encoded><![CDATA[
arXiv:2511.09373v1 Announce Type: cross 
Abstract: LLMs now tackle a wide range of software-related tasks, yet we show that their performance varies markedly both across and within these tasks. Routing user queries to the appropriate LLMs can therefore help improve response quality while reducing cost. Prior work, however, has focused mainly on general-purpose LLM routing via black-box models. We introduce Routesplain, the first LLM router for software-related tasks, including multilingual code generation and repair, input/output prediction, and computer science QA. Unlike existing routing approaches, Routesplain first extracts human-interpretable concepts from each query (e.g., task, domain, reasoning complexity) and only routes based on these concepts, thereby providing intelligible, faithful rationales. We evaluate Routesplain on 16 state-of-the-art LLMs across eight software-related tasks; Routesplain outperforms individual models both in terms of accuracy and cost, and equals or surpasses all black-box baselines, with concept-level intervention highlighting avenues for further router improvements.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 2025 Planning Performance of Frontier Large Language Models</title>
<link>https://arxiv.org/abs/2511.09378</link>
<guid>https://arxiv.org/abs/2511.09378</guid>
<content:encoded><![CDATA[
arXiv:2511.09378v1 Announce Type: cross 
Abstract: The capacity of Large Language Models (LLMs) for reasoning remains an active area of research, with the capabilities of frontier models continually advancing. We provide an updated evaluation of the end-to-end planning performance of three frontier LLMs as of 2025, where models are prompted to generate a plan from PDDL domain and task descriptions. We evaluate DeepSeek R1, Gemini 2.5 Pro, GPT-5 and as reference the planner LAMA on a subset of domains from the most recent Learning Track of the International Planning Competition. Our results show that on standard PDDL domains, the performance of GPT-5 in terms of solved tasks is competitive with LAMA. When the PDDL domains and tasks are obfuscated to test for pure reasoning, the performance of all LLMs degrades, though less severely than previously reported for other models. These results show substantial improvements over prior generations of LLMs, reducing the performance gap to planners on a challenging benchmark.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIG5-TPoT: Predicting BIG Five Personality Traits, Facets, and Items Through Targeted Preselection of Texts</title>
<link>https://arxiv.org/abs/2511.09426</link>
<guid>https://arxiv.org/abs/2511.09426</guid>
<content:encoded><![CDATA[
arXiv:2511.09426v1 Announce Type: cross 
Abstract: Predicting an individual's personalities from their generated texts is a challenging task, especially when the text volume is large. In this paper, we introduce a straightforward yet effective novel strategy called targeted preselection of texts (TPoT). This method semantically filters the texts as input to a deep learning model, specifically designed to predict a Big Five personality trait, facet, or item, referred to as the BIG5-TPoT model. By selecting texts that are semantically relevant to a particular trait, facet, or item, this strategy not only addresses the issue of input text limits in large language models but also improves the Mean Absolute Error and accuracy metrics in predictions for the Stream of Consciousness Essays dataset.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially and Distributionally Robust Virtual Energy Storage Systems via the Scenario Approach</title>
<link>https://arxiv.org/abs/2511.09427</link>
<guid>https://arxiv.org/abs/2511.09427</guid>
<content:encoded><![CDATA[
arXiv:2511.09427v1 Announce Type: cross 
Abstract: We propose an optimization model where a parking lot manager (PLM) can aggregate parked EV batteries to provide virtual energy storage services that are provably robust under uncertain EV departures and state-of-charge caps. Our formulation yields a data-driven convex optimization problem where a prosumer community agrees on a contract with the PLM for the provision of storage services over a finite horizon. Leveraging recent results in the scenario approach, we certify out-of-sample constraint safety. Furthermore, we enable a tunable profit-risk trade-off through scenario relaxation and extend our model to account for robustness to adversarial perturbations and distributional shifts over Wasserstein-based ambiguity sets. All the approaches are accompanied by tight finite-sample certificates. Numerical studies demonstrate the out-of-sample and out-of-distribution constraint satisfaction of our proposed model compared to the developed theoretical guarantees, showing their effectiveness and potential in robust and efficient virtual energy services.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MCAD: Multimodal Context-Aware Audio Description Generation For Soccer</title>
<link>https://arxiv.org/abs/2511.09448</link>
<guid>https://arxiv.org/abs/2511.09448</guid>
<content:encoded><![CDATA[
arXiv:2511.09448v1 Announce Type: cross 
Abstract: Audio Descriptions (AD) are essential for making visual content accessible to individuals with visual impairments. Recent works have shown a promising step towards automating AD, but they have been limited to describing high-quality movie content using human-annotated ground truth AD in the process. In this work, we present an end-to-end pipeline, MCAD, that extends AD generation beyond movies to the domain of sports, with a focus on soccer games, without relying on ground truth AD. To address the absence of domain-specific AD datasets, we fine-tune a Video Large Language Model on publicly available movie AD datasets so that it learns the narrative structure and conventions of AD. During inference, MCAD incorporates multimodal contextual cues such as player identities, soccer events and actions, and commentary from the game. These cues, combined with input prompts to the fine-tuned VideoLLM, allow the system to produce complete AD text for each video segment. We further introduce a new evaluation metric, ARGE-AD, designed to accurately assess the quality of generated AD. ARGE-AD evaluates the generated AD for the presence of five characteristics: (i) usage of people's names, (ii) mention of actions and events, (iii) appropriate length of AD, (iv) absence of pronouns, and (v) overlap from commentary or subtitles. We present an in-depth analysis of our approach on both movie and soccer datasets. We also validate the use of this metric to quantitatively comment on the quality of generated AD using our metric across domains. Additionally, we contribute audio descriptions for 100 soccer game clips annotated by two AD experts.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branching Flows: Discrete, Continuous, and Manifold Flow Matching with Splits and Deletions</title>
<link>https://arxiv.org/abs/2511.09465</link>
<guid>https://arxiv.org/abs/2511.09465</guid>
<content:encoded><![CDATA[
arXiv:2511.09465v1 Announce Type: cross 
Abstract: Diffusion and flow matching approaches to generative modeling have shown promise in domains where the state space is continuous, such as image generation or protein folding & design, and discrete, exemplified by diffusion large language models. They offer a natural fit when the number of elements in a state is fixed in advance (e.g. images), but require ad hoc solutions when, for example, the length of a response from a large language model, or the number of amino acids in a protein chain is not known a priori.
  Here we propose Branching Flows, a generative modeling framework that, like diffusion and flow matching approaches, transports a simple distribution to the data distribution. But in Branching Flows, the elements in the state evolve over a forest of binary trees, branching and dying stochastically with rates that are learned by the model. This allows the model to control, during generation, the number of elements in the sequence. We also show that Branching Flows can compose with any flow matching base process on discrete sets, continuous Euclidean spaces, smooth manifolds, and `multimodal' product spaces that mix these components. We demonstrate this in three domains: small molecule generation (multimodal), antibody sequence generation (discrete), and protein backbone generation (multimodal), and show that Branching Flows is a capable distribution learner with a stable learning objective, and that it enables new capabilities.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A general framework for adaptive nonparametric dimensionality reduction</title>
<link>https://arxiv.org/abs/2511.09486</link>
<guid>https://arxiv.org/abs/2511.09486</guid>
<content:encoded><![CDATA[
arXiv:2511.09486v1 Announce Type: cross 
Abstract: Dimensionality reduction is a fundamental task in modern data science. Several projection methods specifically tailored to take into account the non-linearity of the data via local embeddings have been proposed. Such methods are often based on local neighbourhood structures and require tuning the number of neighbours that define this local structure, and the dimensionality of the lower-dimensional space onto which the data are projected. Such choices critically influence the quality of the resulting embedding. In this paper, we exploit a recently proposed intrinsic dimension estimator which also returns the optimal locally adaptive neighbourhood sizes according to some desirable criteria. In principle, this adaptive framework can be employed to perform an optimal hyper-parameter tuning of any dimensionality reduction algorithm that relies on local neighbourhood structures. Numerical experiments on both real-world and simulated datasets show that the proposed method can be used to significantly improve well-known projection methods when employed for various learning tasks, with improvements measurable through both quantitative metrics and the quality of low-dimensional visualizations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consensus Sampling for Safer Generative AI</title>
<link>https://arxiv.org/abs/2511.09493</link>
<guid>https://arxiv.org/abs/2511.09493</guid>
<content:encoded><![CDATA[
arXiv:2511.09493v1 Announce Type: cross 
Abstract: Many approaches to AI safety rely on inspecting model outputs or activations, yet certain risks are inherently undetectable by inspection alone. We propose a complementary, architecture-agnostic approach that enhances safety through the aggregation of multiple generative models, with the aggregated model inheriting its safety from the safest subset of a given size among them. Specifically, we present a consensus sampling algorithm that, given $k$ models and a prompt, achieves risk competitive with the average risk of the safest $s$ of the $k$ models, where $s$ is a chosen parameter, while abstaining when there is insufficient agreement between them. The approach leverages the models' ability to compute output probabilities, and we bound the probability of abstention when sufficiently many models are safe and exhibit adequate agreement. The algorithm is inspired by the provable copyright protection algorithm of Vyas et al. (2023). It requires some overlap among safe models, offers no protection when all models are unsafe, and may accumulate risk over repeated use. Nonetheless, our results provide a new, model-agnostic approach for AI safety by amplifying safety guarantees from an unknown subset of models within a collection to that of a single reliable model.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional Shrinkage I: Universal Denoisers in Multi-Dimensions</title>
<link>https://arxiv.org/abs/2511.09500</link>
<guid>https://arxiv.org/abs/2511.09500</guid>
<content:encoded><![CDATA[
arXiv:2511.09500v1 Announce Type: cross 
Abstract: We revisit the problem of denoising from noisy measurements where only the noise level is known, not the noise distribution. In multi-dimensions, independent noise $Z$ corrupts the signal $X$, resulting in the noisy measurement $Y = X + \sigma Z$, where $\sigma \in (0, 1)$ is a known noise level. Our goal is to recover the underlying signal distribution $P_X$ from denoising $P_Y$. We propose and analyze universal denoisers that are agnostic to a wide range of signal and noise distributions. Our distributional denoisers offer order-of-magnitude improvements over the Bayes-optimal denoiser derived from Tweedie's formula, if the focus is on the entire distribution $P_X$ rather than on individual realizations of $X$. Our denoisers shrink $P_Y$ toward $P_X$ optimally, achieving $O(\sigma^4)$ and $O(\sigma^6)$ accuracy in matching generalized moments and density functions. Inspired by optimal transport theory, the proposed denoisers are optimal in approximating the Monge-Amp\`ere equation with higher-order accuracy, and can be implemented efficiently via score matching.
  Let $q$ represent the density of $P_Y$; for optimal distributional denoising, we recommend replacing the Bayes-optimal denoiser, \[ \mathbf{T}^*(y) = y + \sigma^2 \nabla \log q(y), \] with denoisers exhibiting less aggressive distributional shrinkage, \[ \mathbf{T}_1(y) = y + \frac{\sigma^2}{2} \nabla \log q(y), \] \[ \mathbf{T}_2(y) = y + \frac{\sigma^2}{2} \nabla \log q(y) - \frac{\sigma^4}{8} \nabla \left( \frac{1}{2} \| \nabla \log q(y) \|^2 + \nabla \cdot \nabla \log q(y) \right) . \]
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Inference Beyond a Single Node: From Bottlenecks to Mitigations with Fast All-Reduce Communication</title>
<link>https://arxiv.org/abs/2511.09557</link>
<guid>https://arxiv.org/abs/2511.09557</guid>
<content:encoded><![CDATA[
arXiv:2511.09557v1 Announce Type: cross 
Abstract: As large language models (LLMs) continue to grow in size, distributed inference has become increasingly important. Model-parallel strategies must now efficiently scale not only across multiple GPUs but also across multiple nodes. In this work, we present a detailed performance study of multi-node distributed inference using LLMs on GPU-based supercomputers. We conduct experiments with several state-of-the-art inference engines alongside YALIS, a research-oriented prototype engine designed for controlled experimentation. We analyze the strong-scaling behavior of different model-parallel schemes and identify key bottlenecks. Since all-reduce operations are a common performance bottleneck, we develop NVRAR, a hierarchical all-reduce algorithm based on recursive doubling with NVSHMEM. NVRAR achieves up to 1.9x-3.6x lower latency than NCCL for message sizes between 128 KB and 2 MB on HPE Slingshot and InfiniBand interconnects. Integrated into YALIS, NVRAR achieves up to a 1.72x reduction in end-to-end batch latency for the Llama 3.1 405B model in multi-node decode-heavy workloads using tensor parallelism.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFG: Internet-Scale Guidance for Functional Grasping Generation</title>
<link>https://arxiv.org/abs/2511.09558</link>
<guid>https://arxiv.org/abs/2511.09558</guid>
<content:encoded><![CDATA[
arXiv:2511.09558v1 Announce Type: cross 
Abstract: Large Vision Models trained on internet-scale data have demonstrated strong capabilities in segmenting and semantically understanding object parts, even in cluttered, crowded scenes. However, while these models can direct a robot toward the general region of an object, they lack the geometric understanding required to precisely control dexterous robotic hands for 3D grasping. To overcome this, our key insight is to leverage simulation with a force-closure grasping generation pipeline that understands local geometries of the hand and object in the scene. Because this pipeline is slow and requires ground-truth observations, the resulting data is distilled into a diffusion model that operates in real-time on camera point clouds. By combining the global semantic understanding of internet-scale models with the geometric precision of a simulation-based locally-aware force-closure, \our achieves high-performance semantic grasping without any manually collected training data. For visualizations of this please visit our website at https://ifgrasping.github.io/
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReactionTeam: Teaming Experts for Divergent Thinking Beyond Typical Reaction Patterns</title>
<link>https://arxiv.org/abs/2310.04674</link>
<guid>https://arxiv.org/abs/2310.04674</guid>
<content:encoded><![CDATA[
arXiv:2310.04674v3 Announce Type: replace 
Abstract: Reaction prediction, a critical task in synthetic chemistry, is to predict the outcome of a reaction based on given reactants. Generative models like Transformer have typically been employed to predict the reaction product. However, these likelihood-maximization models overlooked the inherent stochastic nature of chemical reactions, such as the multiple ways electrons can be redistributed among atoms during the reaction process. In scenarios where similar reactants could follow different electron redistribution patterns, these models typically predict the most common outcomes, neglecting less frequent but potentially crucial reaction patterns. These overlooked patterns, though rare, can lead to innovative methods for designing synthetic routes and significantly advance synthesis techniques. To address these limitations, we build a team of expert models to capture diverse plausible reaction outcomes for the same reactants, mimicking the divergent thinking of chemists. The proposed framework, ReactionTeam, is composed of specialized expert models, each trained to capture a distinct type of electron redistribution pattern in reaction, and a ranking expert that evaluates and orders the generated predictions. Experimental results across two widely used datasets and different data settings demonstrate that our proposed method achieves significantly better performance compared to existing state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSAI: Conditional Self-Attention Imputation for Healthcare Time-series</title>
<link>https://arxiv.org/abs/2312.16713</link>
<guid>https://arxiv.org/abs/2312.16713</guid>
<content:encoded><![CDATA[
arXiv:2312.16713v5 Announce Type: replace 
Abstract: We introduce the Conditional Self-Attention Imputation (CSAI) model, a novel recurrent neural network architecture designed to address the challenges of complex missing data patterns in multivariate time series derived from hospital electronic health records (EHRs). CSAI extends state-of-the-art neural network-based imputation by introducing key modifications specific to EHR data: a) attention-based hidden state initialisation to capture both long- and short-range temporal dependencies prevalent in EHRs, b) domain-informed temporal decay to mimic clinical data recording patterns, and c) a non-uniform masking strategy that models non-random missingness by calibrating weights according to both temporal and cross-sectional data characteristics. Comprehensive evaluation across four EHR benchmark datasets demonstrates CSAI's effectiveness compared to state-of-the-art architectures in data restoration and downstream tasks. CSAI is integrated into PyPOTS, an open-source Python toolbox designed for machine learning tasks on partially observed time series. This work significantly advances the state of neural network imputation applied to EHRs by more closely aligning algorithmic imputation with clinical realities.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nearly Optimal Algorithms for Contextual Dueling Bandits from Adversarial Feedback</title>
<link>https://arxiv.org/abs/2404.10776</link>
<guid>https://arxiv.org/abs/2404.10776</guid>
<content:encoded><![CDATA[
arXiv:2404.10776v3 Announce Type: replace 
Abstract: Learning from human feedback plays an important role in aligning generative models, such as large language models (LLM). However, the effectiveness of this approach can be influenced by adversaries, who may intentionally provide misleading preferences to manipulate the output in an undesirable or harmful direction. To tackle this challenge, we study a specific model within this problem domain--contextual dueling bandits with adversarial feedback, where the true preference label can be flipped by an adversary. We propose an algorithm, namely robust contextual dueling bandits, which is based on uncertainty-weighted maximum likelihood estimation. Our algorithm achieves an $\tilde O(d\sqrt{T}/\kappa+dC/\kappa)$ regret bound, where $T$ is the number of rounds, $d$ is the dimension of the context, $\kappa$ is the lower bound of the derivative of the link function, and $ 0 \le C \le T$ is the total number of adversarial feedback. We also prove a lower bound to show that our regret bound is nearly optimal, both in scenarios with and without ($C=0$) adversarial feedback. Our work is the first to achieve nearly minimax optimal regret for dueling bandits in the presence of adversarial preference feedback. Additionally, for the sigmoid link function, we develop a novel algorithm that takes into account the effect of local derivatives in maximum likelihood estimation (MLE) analysis through a refined method for estimating the link function's derivative. This method helps us to eliminate the $\kappa$ dependence in the leading term with respect to $T$, which reduces the exponential dependence on the parameter radius $B$ to a polynomial dependence. We conduct experiments to evaluate our proposed algorithm against various types of adversarial feedback. Experimental results demonstrate its superiority over the state-of-the-art dueling bandit algorithms in the presence of adversarial feedback.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Data Analysis for Growing Data</title>
<link>https://arxiv.org/abs/2405.13375</link>
<guid>https://arxiv.org/abs/2405.13375</guid>
<content:encoded><![CDATA[
arXiv:2405.13375v2 Announce Type: replace 
Abstract: Reuse of data in adaptive workflows poses challenges regarding overfitting and the statistical validity of results. Previous work has demonstrated that interacting with data via differentially private algorithms can mitigate overfitting, achieving worst-case generalization guarantees with asymptotically optimal data requirements. However, such past work assumes data is static and cannot accommodate situations where data grows over time. In this paper we address this gap, presenting the first generalization bounds for adaptive analysis on dynamic data. We allow the analyst to adaptively schedule their queries conditioned on the current size of the data, in addition to previous queries and responses. We also incorporate time-varying empirical accuracy bounds and mechanisms, allowing for tighter guarantees as data accumulates. In a batched query setting, the asymptotic data requirements of our bound grows with the square-root of the number of adaptive queries, matching prior works' improvement over data splitting for the static setting. We instantiate our bound for statistical queries with the clipped Gaussian mechanism, where it empirically outperforms baselines composed from static bounds.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Information Theoretic Evaluation Metric For Strong Unlearning</title>
<link>https://arxiv.org/abs/2405.17878</link>
<guid>https://arxiv.org/abs/2405.17878</guid>
<content:encoded><![CDATA[
arXiv:2405.17878v3 Announce Type: replace 
Abstract: Machine unlearning (MU) aims to remove the influence of specific data from trained models, addressing privacy concerns and ensuring compliance with regulations such as the ``right to be forgotten.'' Evaluating strong unlearning, where the unlearned model is indistinguishable from one retrained without the forgetting data, remains a significant challenge in deep neural networks (DNNs). Common black-box metrics, such as variants of membership inference attacks and accuracy comparisons, primarily assess model outputs but often fail to capture residual information in intermediate layers. To bridge this gap, we introduce the Information Difference Index (IDI), a novel white-box metric inspired by information theory. IDI quantifies retained information in intermediate features by measuring mutual information between those features and the labels to be forgotten, offering a more comprehensive assessment of unlearning efficacy. Our experiments demonstrate that IDI effectively measures the degree of unlearning across various datasets and architectures, providing a reliable tool for evaluating strong unlearning in DNNs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeVAE: A Variational Autoencoder Approach for Discrete Online Anomaly Detection in Variable-state Multivariate Time-series Data</title>
<link>https://arxiv.org/abs/2407.06849</link>
<guid>https://arxiv.org/abs/2407.06849</guid>
<content:encoded><![CDATA[
arXiv:2407.06849v2 Announce Type: replace 
Abstract: As attention to recorded data grows in the realm of automotive testing and manual evaluation reaches its limits, there is a growing need for automatic online anomaly detection. This real-world data is complex in many ways and requires the modelling of testee behaviour. To address this, we propose a temporal variational autoencoder (TeVAE) that can detect anomalies with minimal false positives when trained on unlabelled data. Our approach also avoids the bypass phenomenon and introduces a new method to remap individual windows to a continuous time series. Furthermore, we propose metrics to evaluate the detection delay and root-cause capability of our approach and present results from experiments on a real-world industrial data set. When properly configured, TeVAE flags anomalies only 6% of the time wrongly and detects 65% of anomalies present. It also has the potential to perform well with a smaller training and validation subset but requires a more sophisticated threshold estimation method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Scope Experts at Test: Generalizing Deeper Graph Neural Networks with Shallow Variants</title>
<link>https://arxiv.org/abs/2409.06998</link>
<guid>https://arxiv.org/abs/2409.06998</guid>
<content:encoded><![CDATA[
arXiv:2409.06998v4 Announce Type: replace 
Abstract: Heterophilous graphs, where dissimilar nodes tend to connect, pose a challenge for graph neural networks (GNNs). Increasing the GNN depth can expand the scope (i.e., receptive field), potentially finding homophily from the higher-order neighborhoods. However, GNNs suffer from performance degradation as depth increases. Despite having better expressivity, state-of-the-art deeper GNNs achieve only marginal improvements compared to their shallow variants. Through theoretical and empirical analysis, we systematically demonstrate a shift in GNN generalization preferences across nodes with different homophily levels as depth increases. This creates a disparity in generalization patterns between GNN models with varying depth. Based on these findings, we propose to improve deeper GNN generalization while maintaining high expressivity by Mixture of scope experts at test (Moscat). Experimental results show that Moscat works flexibly with various GNNs across a wide range of datasets while significantly improving accuracy. Our code is available at (https://github.com/Hydrapse/moscat).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExDBN: Learning Dynamic Bayesian Networks using Extended Mixed-Integer Programming Formulations</title>
<link>https://arxiv.org/abs/2410.16100</link>
<guid>https://arxiv.org/abs/2410.16100</guid>
<content:encoded><![CDATA[
arXiv:2410.16100v3 Announce Type: replace 
Abstract: Causal learning from data has received much attention recently. Bayesian networks can be used to capture causal relationships. There, one recovers a weighted directed acyclic graph in which random variables are represented by vertices, and the weights associated with each edge represent the strengths of the causal relationships between them. This concept is extended to capture dynamic effects by introducing a dependency on past data, which may be captured by the structural equation model. This formalism is utilized in the present contribution to propose a score-based learning algorithm. A mixed-integer quadratic program is formulated and an algorithmic solution proposed, in which the pre-generation of exponentially many acyclicity constraints is avoided by utilizing the so-called branch-and-cut (``lazy constraint'') method. Comparing the novel approach to the state-of-the-art, we show that the proposed approach turns out to produce more accurate results when applied to small and medium-sized synthetic instances containing up to 80 time series. Lastly, two interesting applications in bioscience and finance, to which the method is directly applied, further stress the importance of developing highly accurate, globally convergent solvers that can handle instances of modest size.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Distribution Learning for Graph Classification</title>
<link>https://arxiv.org/abs/2411.15206</link>
<guid>https://arxiv.org/abs/2411.15206</guid>
<content:encoded><![CDATA[
arXiv:2411.15206v3 Announce Type: replace 
Abstract: Leveraging the diversity and quantity of data provided by various graph-structured data augmentations while preserving intrinsic semantic information is challenging. Additionally, successive layers in graph neural network (GNN) tend to produce more similar node embeddings, while graph contrastive learning aims to increase the dissimilarity between negative pairs of node embeddings. This inevitably results in a conflict between the message-passing mechanism (MPM) of GNNs and the contrastive learning (CL) of negative pairs via intraviews. In this paper, we propose a conditional distribution learning (CDL) method that learns graph representations from graph-structured data for semisupervised graph classification. Specifically, we present an end-to-end graph representation learning model to align the conditional distributions of weakly and strongly augmented features over the original features. This alignment enables the CDL model to effectively preserve intrinsic semantic information when both weak and strong augmentations are applied to graph-structured data. To avoid the conflict between the MPM and the CL of negative pairs, positive pairs of node representations are retained for measuring the similarity between the original features and the corresponding weakly augmented features. Extensive experiments with several benchmark graph datasets demonstrate the effectiveness of the proposed CDL method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified Training with Branch-and-Bound for Lyapunov-stable Neural Control</title>
<link>https://arxiv.org/abs/2411.18235</link>
<guid>https://arxiv.org/abs/2411.18235</guid>
<content:encoded><![CDATA[
arXiv:2411.18235v2 Announce Type: replace 
Abstract: We study the problem of learning verifiably Lyapunov-stable neural controllers that provably satisfy the Lyapunov asymptotic stability condition within a region-of-attraction (ROA). Unlike previous works that adopted counterexample-guided training without considering the computation of verification in training, we introduce Certified Training with Branch-and-Bound (CT-BaB), a new certified training framework that optimizes certified bounds, thereby reducing the discrepancy between training and test-time verification that also computes certified bounds. To achieve a relatively global guarantee on an entire input region-of-interest, we propose a training-time BaB technique that maintains a dynamic training dataset and adaptively splits hard input subregions into smaller ones, to tighten certified bounds and ease the training. Meanwhile, subregions created by the training-time BaB also inform test-time verification, for a more efficient training-aware verification. We demonstrate that CT-BaB yields verification-friendly models that can be more efficiently verified at test time while achieving stronger verifiable guarantees with larger ROA. On the largest output-feedback 2D Quadrotor system experimented, CT-BaB reduces verification time by over 11X relative to the previous state-of-the-art baseline while achieving 164X larger ROA.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Constrained Neural Differential Equation Framework for Data-Driven Snowpack Simulation</title>
<link>https://arxiv.org/abs/2412.06819</link>
<guid>https://arxiv.org/abs/2412.06819</guid>
<content:encoded><![CDATA[
arXiv:2412.06819v3 Announce Type: replace 
Abstract: This paper presents a physics-constrained neural differential equation framework for parameterization, and employs it to model the time evolution of seasonal snow depth given hydrometeorological forcings. When trained on data from multiple SNOTEL sites, the parameterization predicts daily snow depth with under 9% median error and Nash Sutcliffe Efficiencies over 0.94 across a wide variety of snow climates. The parameterization also generalizes to new sites not seen during training, which is not often true for calibrated snow models. Requiring the parameterization to predict snow water equivalent in addition to snow depth only increases error to ~12%. The structure of the approach guarantees the satisfaction of physical constraints, enables these constraints during model training, and allows modeling at different temporal resolutions without additional retraining of the parameterization. These benefits hold potential in climate modeling, and could extend to other dynamical systems with physical constraints.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Transfer Learning: A Survey</title>
<link>https://arxiv.org/abs/2412.14116</link>
<guid>https://arxiv.org/abs/2412.14116</guid>
<content:encoded><![CDATA[
arXiv:2412.14116v2 Announce Type: replace 
Abstract: Transfer learning aims to transfer knowledge or information from a source domain to a relevant target domain. In this paper, we understand transfer learning from the perspectives of knowledge transferability and trustworthiness. This involves two research questions: How is knowledge transferability quantitatively measured and enhanced across domains? Can we trust the transferred knowledge in the transfer learning process? To answer these questions, this paper provides a comprehensive review of trustworthy transfer learning from various aspects, including problem definitions, theoretical analysis, empirical algorithms, and real-world applications. Specifically, we summarize recent theories and algorithms for understanding knowledge transferability under (within-domain) IID and non-IID assumptions. In addition to knowledge transferability, we review the impact of trustworthiness on transfer learning, e.g., whether the transferred knowledge is adversarially robust or algorithmically fair, how to transfer the knowledge under privacy-preserving constraints, etc. Beyond discussing the current advancements, we highlight the open questions and future directions for understanding transfer learning in a reliable and trustworthy manner.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoG: Towards automatic graph construction from tabular data</title>
<link>https://arxiv.org/abs/2501.15282</link>
<guid>https://arxiv.org/abs/2501.15282</guid>
<content:encoded><![CDATA[
arXiv:2501.15282v4 Announce Type: replace 
Abstract: Recent years have witnessed significant advancements in graph machine learning (GML), with its applications spanning numerous domains. However, the focus of GML has predominantly been on developing powerful models, often overlooking a crucial initial step: constructing suitable graphs from common data formats, such as tabular data. This construction process is fundamental to applying graph-based models, yet it remains largely understudied and lacks formalization. Our research aims to address this gap by formalizing the graph construction problem and proposing an effective solution. We identify two critical challenges to achieve this goal: 1. The absence of dedicated datasets to formalize and evaluate the effectiveness of graph construction methods, and 2. Existing automatic construction methods can only be applied to some specific cases, while tedious human engineering is required to generate high-quality graphs. To tackle these challenges, we present a two-fold contribution. First, we introduce a set of datasets to formalize and evaluate graph construction methods. Second, we propose an LLM-based solution, AutoG, automatically generating high-quality graph schemas without human intervention. The experimental results demonstrate that the quality of constructed graphs is critical to downstream task performance, and AutoG can generate high-quality graphs that rival those produced by human experts. Our code can be accessible from https://github.com/amazon-science/Automatic-Table-to-Graph-Generation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contrastive Learning for Connectome Classification</title>
<link>https://arxiv.org/abs/2502.05109</link>
<guid>https://arxiv.org/abs/2502.05109</guid>
<content:encoded><![CDATA[
arXiv:2502.05109v2 Announce Type: replace 
Abstract: With recent advancements in non-invasive techniques for measuring brain activity, such as magnetic resonance imaging (MRI), the study of structural and functional brain networks through graph signal processing (GSP) has gained notable prominence. GSP stands as a key tool in unraveling the interplay between the brain's function and structure, enabling the analysis of graphs defined by the connections between regions of interest -- referred to as connectomes in this context. Our work represents a further step in this direction by exploring supervised contrastive learning methods within the realm of graph representation learning. The main objective of this approach is to generate subject-level (i.e., graph-level) vector representations that bring together subjects sharing the same label while separating those with different labels. These connectome embeddings are derived from a graph neural network Encoder-Decoder architecture, which jointly considers structural and functional connectivity. By leveraging data augmentation techniques, the proposed framework achieves state-of-the-art performance in a gender classification task using Human Connectome Project data. More broadly, our connectome-centric methodological advances support the promising prospect of using GSP to discover more about brain function, with potential impact to understanding heterogeneity in the neurodegeneration for precision medicine and diagnosis.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Thompson Sampling via Generation of Missing Data</title>
<link>https://arxiv.org/abs/2502.07064</link>
<guid>https://arxiv.org/abs/2502.07064</guid>
<content:encoded><![CDATA[
arXiv:2502.07064v2 Announce Type: replace 
Abstract: We introduce a framework for Thompson sampling (TS) contextual bandit algorithms, in which the algorithm's ability to quantify uncertainty and make decisions depends on the quality of a generative model that is learned offline. Instead of viewing uncertainty in the environment as arising from unobservable latent parameters, our algorithm treats uncertainty as stemming from missing, but potentially observable outcomes (including both future and counterfactual outcomes). If these outcomes were all observed, one could simply make decisions using an "oracle" policy fit on the complete dataset. Inspired by this conceptualization, at each decision-time, our algorithm uses a generative model to probabilistically impute missing outcomes, fits a policy using the imputed complete dataset, and uses that policy to select the next action. We formally show that this algorithm is a generative formulation of TS and establish a state-of-the-art regret bound. Notably, our regret bound depends on the generative model only through the quality of its offline prediction loss, and applies to any method of fitting the "oracle" policy.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Message Passing Experts with Routing Entropy Regularization for Node Classification</title>
<link>https://arxiv.org/abs/2502.08083</link>
<guid>https://arxiv.org/abs/2502.08083</guid>
<content:encoded><![CDATA[
arXiv:2502.08083v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) have achieved significant progress in graph-based learning tasks, yet their performance often deteriorates when facing heterophilous structures where connected nodes differ substantially in features and labels. To address this limitation, we propose GNNMoE, a novel entropy-driven mixture of message-passing experts framework that enables node-level adaptive representation learning. GNNMoE decomposes message passing into propagation and transformation operations and integrates them through multiple expert networks guided by a hybrid routing mechanism. And a routing entropy regularization dynamically adjusts soft weighting and soft top-$k$ routing, allowing GNNMoE to flexibly adapt to diverse neighborhood contexts. Extensive experiments on twelve benchmark datasets demonstrate that GNNMoE consistently outperforms SOTA node classification methods, while maintaining scalability and interpretability. This work provides a unified and principled approach for achieving fine-grained, personalized node representation learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultrametric Cluster Hierarchies: I Want 'em All!</title>
<link>https://arxiv.org/abs/2502.14018</link>
<guid>https://arxiv.org/abs/2502.14018</guid>
<content:encoded><![CDATA[
arXiv:2502.14018v2 Announce Type: replace 
Abstract: Hierarchical clustering is a powerful tool for exploratory data analysis, organizing data into a tree of clusterings from which a partition can be chosen. This paper generalizes these ideas by proving that, for any reasonable hierarchy, one can optimally solve any center-based clustering objective over it (such as $k$-means). Moreover, these solutions can be found exceedingly quickly and are themselves necessarily hierarchical. Thus, given a cluster tree, we show that one can quickly access a plethora of new, equally meaningful hierarchies. Just as in standard hierarchical clustering, one can then choose any desired partition from these new hierarchies. We conclude by verifying the utility of our proposed techniques across datasets, hierarchies, and partitioning schemes.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple and Effective Reinforcement Learning Method for Text-to-Image Diffusion Fine-tuning</title>
<link>https://arxiv.org/abs/2503.00897</link>
<guid>https://arxiv.org/abs/2503.00897</guid>
<content:encoded><![CDATA[
arXiv:2503.00897v5 Announce Type: replace 
Abstract: Reinforcement learning (RL)-based fine-tuning has emerged as a powerful approach for aligning diffusion models with black-box objectives. Proximal policy optimization (PPO) is the most popular choice of method for policy optimization. While effective in terms of performance, PPO is highly sensitive to hyper-parameters and involves substantial computational overhead. REINFORCE, on the other hand, mitigates some computational complexities such as high memory overhead and sensitive hyper-parameter tuning, but has suboptimal performance due to high-variance and sample inefficiency. While the variance of the REINFORCE can be reduced by sampling multiple actions per input prompt and using a baseline correction term, it still suffers from sample inefficiency. To address these challenges, we systematically analyze the efficiency-effectiveness trade-off between REINFORCE and PPO, and propose leave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP combines variance reduction techniques from REINFORCE, such as sampling multiple actions per input prompt and a baseline correction term, with the robustness and sample efficiency of PPO via clipping and importance sampling. Our results demonstrate that LOOP effectively improves diffusion models on various black-box objectives, and achieves a better balance between computational efficiency and performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Well Can Differential Privacy Be Audited in One Run?</title>
<link>https://arxiv.org/abs/2503.07199</link>
<guid>https://arxiv.org/abs/2503.07199</guid>
<content:encoded><![CDATA[
arXiv:2503.07199v3 Announce Type: replace 
Abstract: Recent methods for auditing the privacy of machine learning algorithms have improved computational efficiency by simultaneously intervening on multiple training examples in a single training run. Steinke et al. (2024) prove that one-run auditing indeed lower bounds the true privacy parameter of the audited algorithm, and give impressive empirical results. Their work leaves open the question of how precisely one-run auditing can uncover the true privacy parameter of an algorithm, and how that precision depends on the audited algorithm. In this work, we characterize the maximum achievable efficacy of one-run auditing and show that the key barrier to its efficacy is interference between the observable effects of different data elements. We present new conceptual approaches to minimize this barrier, towards improving the performance of one-run auditing of real machine learning algorithms.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynLlama: Generating Synthesizable Molecules and Their Analogs with Large Language Models</title>
<link>https://arxiv.org/abs/2503.12602</link>
<guid>https://arxiv.org/abs/2503.12602</guid>
<content:encoded><![CDATA[
arXiv:2503.12602v5 Announce Type: replace 
Abstract: Generative machine learning models for exploring chemical space have shown immense promise, but many molecules they generate are too difficult to synthesize, making them impractical for further investigation or development. In this work, we present a novel approach by fine-tuning Meta's Llama3 Large Language Models (LLMs) to create SynLlama, which generates full synthetic pathways made of commonly accessible building blocks and robust organic reaction templates. SynLlama explores a large synthesizable space using significantly less data, and offers strong performance in both forward and bottom-up synthesis planning compared to other state-of-the-art methods. We find that SynLlama, even without training on external building blocks, can effectively generalize to unseen yet purchasable building blocks, meaning that its reconstruction capabilities extend to a broader synthesizable chemical space than the training data. We also demonstrate the use of SynLlama in a pharmaceutical context for synthesis planning of analog molecules and hit expansion leads for proposed inhibitors of target proteins, offering medicinal chemists a valuable tool for discovery.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What's Producible May Not Be Reachable: Measuring the Steerability of Generative Models</title>
<link>https://arxiv.org/abs/2503.17482</link>
<guid>https://arxiv.org/abs/2503.17482</guid>
<content:encoded><![CDATA[
arXiv:2503.17482v2 Announce Type: replace 
Abstract: How should we evaluate the quality of generative models? Many existing metrics focus on a model's producibility, i.e. the quality and breadth of outputs it can generate. However, the actual value from using a generative model stems not just from what it can produce but whether a user with a specific goal can produce an output that satisfies that goal. We refer to this property as steerability. In this paper, we first introduce a mathematical decomposition for quantifying steerability independently from producibility. Steerability is more challenging to evaluate than producibility because it requires knowing a user's goals. We address this issue by creating a benchmark task that relies on one key idea: sample an output from a generative model and ask users to reproduce it. We implement this benchmark in user studies of text-to-image and large language models. Despite the ability of these models to produce high-quality outputs, they all perform poorly on steerability. These results suggest that we need to focus on improving the steerability of generative models. We show such improvements are indeed possible: simple image-based steering mechanisms achieve more than 2x improvement on this benchmark.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Policy Optimization</title>
<link>https://arxiv.org/abs/2503.19037</link>
<guid>https://arxiv.org/abs/2503.19037</guid>
<content:encoded><![CDATA[
arXiv:2503.19037v3 Announce Type: replace 
Abstract: On-policy reinforcement learning (RL) algorithms are widely used for their strong asymptotic performance and training stability, but they struggle to scale with larger batch sizes, as additional parallel environments yield redundant data due to limited policy-induced diversity. In contrast, Evolutionary Algorithms (EAs) scale naturally and encourage exploration via randomized population-based search, but are often sample-inefficient. We propose Evolutionary Policy Optimization (EPO), a hybrid algorithm that combines the scalability and diversity of EAs with the performance and stability of policy gradients. EPO maintains a population of agents conditioned on latent variables, shares actor-critic network parameters for coherence and memory efficiency, and aggregates diverse experiences into a master agent. Across tasks in dexterous manipulation, legged locomotion, and classic control, EPO outperforms state-of-the-art baselines in sample efficiency, asymptotic performance, and scalability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination</title>
<link>https://arxiv.org/abs/2503.22454</link>
<guid>https://arxiv.org/abs/2503.22454</guid>
<content:encoded><![CDATA[
arXiv:2503.22454v2 Announce Type: replace 
Abstract: Fairness studies of algorithmic decision-making systems often simplify complex decision processes, such as bail or loan approvals, into binary classification tasks. However, these approaches overlook that such decisions are not inherently binary (e.g., approve or not approve bail or loan); they also involve non-binary treatment decisions (e.g., bail conditions or loan terms) that can influence the downstream outcomes (e.g., loan repayment or reoffending). In this paper, we argue that non-binary treatment decisions are integral to the decision process and controlled by decision-makers and, therefore, should be central to fairness analyses in algorithmic decision-making. We propose a causal framework that extends fairness analyses and explicitly distinguishes between decision-subjects' covariates and the treatment decisions. This specification allows decision-makers to use our framework to (i) measure treatment disparity and its downstream effects in historical data and, using counterfactual reasoning, (ii) mitigate the impact of past unfair treatment decisions when automating decision-making. We use our framework to empirically analyze four widely used loan approval datasets to reveal potential disparity in non-binary treatment decisions and their discriminatory impact on outcomes, highlighting the need to incorporate treatment decisions in fairness assessments. Moreover, by intervening in treatment decisions, we show that our framework effectively mitigates treatment discrimination from historical data to ensure fair risk score estimation and (non-binary) decision-making processes that benefit all stakeholders.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TAMIS: Tailored Membership Inference Attacks on Synthetic Data</title>
<link>https://arxiv.org/abs/2504.00758</link>
<guid>https://arxiv.org/abs/2504.00758</guid>
<content:encoded><![CDATA[
arXiv:2504.00758v2 Announce Type: replace 
Abstract: Membership Inference Attacks (MIA) enable to empirically assess the privacy of a machine learning algorithm. In this paper, we propose TAMIS, a novel MIA against differentially-private synthetic data generation methods that rely on graphical models. This attack builds upon MAMA-MIA, a recently-published state-of-the-art method. It lowers its computational cost and requires less attacker knowledge. Our attack is the product of a two-fold improvement. First, we recover the graphical model having generated a synthetic dataset by using solely that dataset, rather than shadow-modeling over an auxiliary one. This proves less costly and more performant. Second, we introduce a more mathematically-grounded attack score, that provides a natural threshold for binary predictions. In our experiments, TAMIS achieves better or similar performance as MAMA-MIA on replicas of the SNAKE challenge.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks</title>
<link>https://arxiv.org/abs/2504.04277</link>
<guid>https://arxiv.org/abs/2504.04277</guid>
<content:encoded><![CDATA[
arXiv:2504.04277v3 Announce Type: replace 
Abstract: Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5\% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphRAFT: Retrieval Augmented Fine-Tuning for Knowledge Graphs on Graph Databases</title>
<link>https://arxiv.org/abs/2504.05478</link>
<guid>https://arxiv.org/abs/2504.05478</guid>
<content:encoded><![CDATA[
arXiv:2504.05478v2 Announce Type: replace 
Abstract: Large language models have shown remarkable language processing and reasoning ability but are prone to hallucinate when asked about private data. Retrieval-augmented generation (RAG) retrieves relevant data that fit into an LLM's context window and prompts the LLM for an answer. GraphRAG extends this approach to structured Knowledge Graphs (KGs) and questions regarding entities multiple hops away. The majority of recent GraphRAG methods either overlook the retrieval step or have ad hoc retrieval processes that are abstract or inefficient. This prevents them from being adopted when the KGs are stored in graph databases supporting graph query languages. In this work, we present GraphRAFT, a retrieve-and-reason framework that finetunes LLMs to generate provably correct Cypher queries to retrieve high-quality subgraph contexts and produce accurate answers. Our method is the first such solution that can be taken off-the-shelf and used on KGs stored in native graph DBs. Benchmarks suggest that our method is sample-efficient and scales with the availability of training data. Our method achieves significantly better results than all state-of-the-art models across all four standard metrics on two challenging Q&amp;As on large text-attributed KGs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repetitive Contrastive Learning Enhances Mamba's Selectivity in Time Series Prediction</title>
<link>https://arxiv.org/abs/2504.09185</link>
<guid>https://arxiv.org/abs/2504.09185</guid>
<content:encoded><![CDATA[
arXiv:2504.09185v2 Announce Type: replace 
Abstract: Long sequence prediction is a key challenge in time series forecasting. While Mamba-based models have shown strong performance due to their sequence selection capabilities, they still struggle with insufficient focus on critical time steps and incomplete noise suppression, caused by limited selective abilities. To address this, we introduce Repetitive Contrastive Learning (RCL), a token-level contrastive pretraining framework aimed at enhancing Mamba's selective capabilities. RCL pretrains a single Mamba block to strengthen its selective abilities and then transfers these pretrained parameters to initialize Mamba blocks in various backbone models, improving their temporal prediction performance. RCL uses sequence augmentation with Gaussian noise and applies inter-sequence and intra-sequence contrastive learning to help the Mamba module prioritize information-rich time steps while ignoring noisy ones. Extensive experiments show that RCL consistently boosts the performance of backbone models, surpassing existing methods and achieving state-of-the-art results. Additionally, we propose two metrics to quantify Mamba's selective capabilities, providing theoretical, qualitative, and quantitative evidence for the improvements brought by RCL.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integration Matters for Learning PDEs with Backwards SDEs</title>
<link>https://arxiv.org/abs/2505.01078</link>
<guid>https://arxiv.org/abs/2505.01078</guid>
<content:encoded><![CDATA[
arXiv:2505.01078v2 Announce Type: replace 
Abstract: Backward stochastic differential equation (BSDE)-based deep learning methods provide an alternative to Physics-Informed Neural Networks (PINNs) for solving high-dimensional partial differential equations (PDEs), offering potential algorithmic advantages in settings such as stochastic optimal control, where the PDEs of interest are tied to an underlying dynamical system. However, standard BSDE-based solvers have empirically been shown to underperform relative to PINNs in the literature. In this paper, we identify the root cause of this performance gap as a discretization bias introduced by the standard Euler-Maruyama (EM) integration scheme applied to one-step self-consistency BSDE losses, which shifts the optimization landscape off target. We find that this bias cannot be satisfactorily addressed through finer step-sizes or multi-step self-consistency losses. To properly handle this issue, we propose a Stratonovich-based BSDE formulation, which we implement with stochastic Heun integration. We show that our proposed approach completely eliminates the bias issues faced by EM integration. Furthermore, our empirical results show that our Heun-based BSDE method consistently outperforms EM-based variants and achieves competitive results with PINNs across multiple high-dimensional benchmarks. Our findings highlight the critical role of integration schemes in BSDE-based PDE solvers, an algorithmic detail that has received little attention thus far in the literature.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RefiDiff: Progressive Refinement Diffusion for Efficient Missing Data Imputation</title>
<link>https://arxiv.org/abs/2505.14451</link>
<guid>https://arxiv.org/abs/2505.14451</guid>
<content:encoded><![CDATA[
arXiv:2505.14451v2 Announce Type: replace 
Abstract: Missing values in high-dimensional, mixed-type datasets pose significant challenges for data imputation, particularly under Missing Not At Random (MNAR) mechanisms. Existing methods struggle to integrate local and global data characteristics, limiting performance in MNAR and high-dimensional settings. We propose an innovative framework, RefiDiff, combining local machine learning predictions with a novel Mamba-based denoising network efficiently capturing long-range dependencies among features and samples with low computational complexity. RefiDiff bridges the predictive and generative paradigms of imputation, leveraging pre-refinement for initial warm-up imputations and post-refinement to polish results, enhancing stability and accuracy. By encoding mixed-type data into unified tokens, RefiDiff enables robust imputation without architectural or hyperparameter tuning. RefiDiff outperforms state-of-the-art (SOTA) methods across missing-value settings, demonstrating strong performance in MNAR settings and superior out-of-sample generalization. Extensive evaluations on nine real-world datasets demonstrate its robustness, scalability, and effectiveness in handling complex missingness patterns.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polar Sparsity: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity</title>
<link>https://arxiv.org/abs/2505.14884</link>
<guid>https://arxiv.org/abs/2505.14884</guid>
<content:encoded><![CDATA[
arXiv:2505.14884v3 Announce Type: replace 
Abstract: Accelerating large language model (LLM) inference is critical for real-world deployments requiring high throughput and low latency. Contextual sparsity, where each token dynamically activates only a small subset of the model parameters, shows promise but does not scale to large batch sizes due to union of active neurons quickly approaching dense computation. We introduce Polar Sparsity, highlighting a key shift in sparsity importance from MLP to Attention layers as we scale batch size and sequence length. While MLP layers become more compute-efficient under batching, their sparsity vanishes. In contrast, attention becomes increasingly more expensive at scale, while their head sparsity remains stable and batch-invariant. We develop Selective Head Attention with hardware-efficient, sparsity-aware GPU kernels, delivering up to \(2.2\times\) end-to-end speedups for models like OPT, LLaMA-2 \& 3, Qwen, Mistral across various batch sizes and sequence lengths without compromising accuracy. To our knowledge, this is the first work to demonstrate that contextual sparsity can scale effectively to large batch sizes, delivering substantial inference acceleration with minimal changes, making Polar Sparsity practical for large-scale, high-throughput LLM deployment systems. Our code is available at: https://github.com/susavlsh10/Polar-Sparsity.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Bellman Residual Minimization: A Simple Value-Based Method for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.15311</link>
<guid>https://arxiv.org/abs/2505.15311</guid>
<content:encoded><![CDATA[
arXiv:2505.15311v2 Announce Type: replace 
Abstract: Policy-based methods currently dominate reinforcement learning (RL) pipelines for large language model (LLM) reasoning, leaving value-based approaches largely unexplored. We revisit the classical paradigm of Bellman Residual Minimization and introduce Trajectory Bellman Residual Minimization (TBRM), an algorithm that naturally adapts this idea to LLMs, yielding a simple yet effective off-policy algorithm that optimizes a single trajectory-level Bellman objective using the model's own logits as $Q$-values. TBRM removes the need for critics, importance-sampling ratios, or clipping, and operates with only one rollout per prompt. We prove convergence to the near-optimal KL-regularized policy from arbitrary off-policy data via an improved change-of-trajectory-measure analysis. Experiments on standard mathematical-reasoning benchmarks show that TBRM consistently outperforms policy-based baselines, like PPO and GRPO, with comparable or lower computational and memory overhead. Our results indicate that value-based RL might be a principled and efficient alternative for enhancing reasoning capabilities in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solver-Free Decision-Focused Learning for Linear Optimization Problems</title>
<link>https://arxiv.org/abs/2505.22224</link>
<guid>https://arxiv.org/abs/2505.22224</guid>
<content:encoded><![CDATA[
arXiv:2505.22224v2 Announce Type: replace 
Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An empirical study of task and feature correlations in the reuse of pre-trained models</title>
<link>https://arxiv.org/abs/2506.01975</link>
<guid>https://arxiv.org/abs/2506.01975</guid>
<content:encoded><![CDATA[
arXiv:2506.01975v3 Announce Type: replace 
Abstract: Pre-trained neural networks are commonly used and reused in the machine learning community. Alice trains a model for a particular task, and a part of her neural network is reused by Bob for a different task, often to great effect. To what can we ascribe Bob's success? This paper introduces an experimental setup through which factors contributing to Bob's empirical success could be studied in silico. As a result, we demonstrate that Bob might just be lucky: his task accuracy increases monotonically with the correlation between his task and Alice's. Even when Bob has provably uncorrelated tasks and input features from Alice's pre-trained network, he can achieve significantly better than random performance due to Alice's choice of network and optimizer. When there is little correlation between tasks, only reusing lower pre-trained layers is preferable, and we hypothesize the converse: that the optimal number of retrained layers is indicative of task and feature correlation. Finally, we show in controlled real-world scenarios that Bob can effectively reuse Alice's pre-trained network if there are semantic correlations between his and Alice's task.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RiemannFormer: A Framework for Attention in Curved Spaces</title>
<link>https://arxiv.org/abs/2506.07405</link>
<guid>https://arxiv.org/abs/2506.07405</guid>
<content:encoded><![CDATA[
arXiv:2506.07405v2 Announce Type: replace 
Abstract: This research endeavors to offer insights into unlocking the further potential of transformer-based architectures. One of the primary motivations is to offer a geometric interpretation for the attention mechanism in transformers. In our framework, the attention mainly involves metric tensors, tangent spaces, inner product, and how they relate to each other. These quantities and structures at discrete positions are intricately interconnected via the parallel transport of tangent vectors. To make the learning process more efficient, we reduce the number of parameters through ingenious predefined configurations. Moreover, we introduce an explicit mechanism to highlight a neighborhood by attenuating the remote values, given that transformers inherently neglect local inductive bias. Experimental results demonstrate that our modules deliver significant performance improvements relative to the baseline. More evaluation experiments on visual and large language models will be launched successively.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edit Flows: Flow Matching with Edit Operations</title>
<link>https://arxiv.org/abs/2506.09018</link>
<guid>https://arxiv.org/abs/2506.09018</guid>
<content:encoded><![CDATA[
arXiv:2506.09018v3 Announce Type: replace 
Abstract: Autoregressive generative models naturally generate variable-length sequences, while non-autoregressive models struggle, often imposing rigid, token-wise structures. We propose Edit Flows, a non-autoregressive model that overcomes these limitations by defining a discrete flow over sequences through edit operations$\unicode{x2013}$insertions, deletions, and substitutions. By modeling these operations within a Continuous-time Markov Chain over the sequence space, Edit Flows enable flexible, position-relative generation that aligns more closely with the structure of sequence data. Our training method leverages an expanded state space with auxiliary variables, making the learning process efficient and tractable. Empirical results show that Edit Flows outperforms both autoregressive and mask models on image captioning and significantly outperforms the mask construction in text and code generation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STOAT: Spatial-Temporal Probabilistic Causal Inference Network</title>
<link>https://arxiv.org/abs/2506.09544</link>
<guid>https://arxiv.org/abs/2506.09544</guid>
<content:encoded><![CDATA[
arXiv:2506.09544v3 Announce Type: replace 
Abstract: Spatial-temporal causal time series (STC-TS) involve region-specific temporal observations driven by causally relevant covariates and interconnected across geographic or network-based spaces. Existing methods often model spatial and temporal dynamics independently and overlook causality-driven probabilistic forecasting, limiting their predictive power. To address this, we propose STOAT (Spatial-Temporal Probabilistic Causal Inference Network), a novel framework for probabilistic forecasting in STC-TS. The proposed method extends a causal inference approach by incorporating a spatial relation matrix that encodes interregional dependencies (e.g. proximity or connectivity), enabling spatially informed causal effect estimation. The resulting latent series are processed by deep probabilistic models to estimate the parameters of the distributions, enabling calibrated uncertainty modeling. We further explore multiple output distributions (e.g., Gaussian, Student's-$t$, Laplace) to capture region-specific variability. Experiments on COVID-19 data across six countries demonstrate that STOAT outperforms state-of-the-art probabilistic forecasting models (DeepAR, DeepVAR, Deep State Space Model, etc.) in key metrics, particularly in regions with strong spatial dependencies. By bridging causal inference and geospatial probabilistic forecasting, STOAT offers a generalizable framework for complex spatial-temporal tasks, such as epidemic management.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flat Channels to Infinity in Neural Loss Landscapes</title>
<link>https://arxiv.org/abs/2506.14951</link>
<guid>https://arxiv.org/abs/2506.14951</guid>
<content:encoded><![CDATA[
arXiv:2506.14951v3 Announce Type: replace 
Abstract: The loss landscapes of neural networks contain minima and saddle points that may be connected in flat regions or appear in isolation. We identify and characterize a special structure in the loss landscape: channels along which the loss decreases extremely slowly, while the output weights of at least two neurons, $a_i$ and $a_j$, diverge to $\pm$infinity, and their input weight vectors, $\mathbf{w_i}$ and $\mathbf{w_j}$, become equal to each other. At convergence, the two neurons implement a gated linear unit: $a_i\sigma(\mathbf{w_i} \cdot \mathbf{x}) + a_j\sigma(\mathbf{w_j} \cdot \mathbf{x}) \rightarrow \sigma(\mathbf{w} \cdot \mathbf{x}) + (\mathbf{v} \cdot \mathbf{x}) \sigma'(\mathbf{w} \cdot \mathbf{x})$. Geometrically, these channels to infinity are asymptotically parallel to symmetry-induced lines of critical points. Gradient flow solvers, and related optimization methods like SGD or ADAM, reach the channels with high probability in diverse regression settings, but without careful inspection they look like flat local minima with finite parameter values. Our characterization provides a comprehensive picture of these quasi-flat regions in terms of gradient dynamics, geometry, and functional interpretation. The emergence of gated linear units at the end of the channels highlights a surprising aspect of the computational capabilities of fully connected layers.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Capturing Polysemanticity with PRISM: A Multi-Concept Feature Description Framework</title>
<link>https://arxiv.org/abs/2506.15538</link>
<guid>https://arxiv.org/abs/2506.15538</guid>
<content:encoded><![CDATA[
arXiv:2506.15538v4 Announce Type: replace 
Abstract: Automated interpretability research aims to identify concepts encoded in neural network features to enhance human understanding of model behavior. Within the context of large language models (LLMs) for natural language processing (NLP), current automated neuron-level feature description methods face two key challenges: limited robustness and the assumption that each neuron encodes a single concept (monosemanticity), despite increasing evidence of polysemanticity. This assumption restricts the expressiveness of feature descriptions and limits their ability to capture the full range of behaviors encoded in model internals. To address this, we introduce Polysemantic FeatuRe Identification and Scoring Method (PRISM), a novel framework specifically designed to capture the complexity of features in LLMs. Unlike approaches that assign a single description per neuron, common in many automated interpretability methods in NLP, PRISM produces more nuanced descriptions that account for both monosemantic and polysemantic behavior. We apply PRISM to LLMs and, through extensive benchmarking against existing methods, demonstrate that our approach produces more accurate and faithful feature descriptions, improving both overall description quality (via a description score) and the ability to capture distinct concepts when polysemanticity is present (via a polysemanticity score).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Do Latent Action Models Actually Learn?</title>
<link>https://arxiv.org/abs/2506.15691</link>
<guid>https://arxiv.org/abs/2506.15691</guid>
<content:encoded><![CDATA[
arXiv:2506.15691v3 Announce Type: replace 
Abstract: Latent action models (LAMs) aim to learn action-relevant changes from unlabeled videos by compressing changes between frames as latents. However, differences between video frames can be caused by controllable changes as well as exogenous noise, leading to an important concern -- do latents capture the changes caused by actions or irrelevant noise? This paper studies this issue analytically, presenting a linear model that encapsulates the essence of LAM learning, while being tractable.This provides several insights, including connections between LAM and principal component analysis (PCA), desiderata of the data-generating policy, and justification of strategies to encourage learning controllable changes using data augmentation, data cleaning, and auxiliary action-prediction. We also provide illustrative results based on numerical simulation, shedding light on the specific structure of observations, actions, and noise in data that influence LAM learning.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Non-Linear Representation Dilemma: Is Causal Abstraction Enough for Mechanistic Interpretability?</title>
<link>https://arxiv.org/abs/2507.08802</link>
<guid>https://arxiv.org/abs/2507.08802</guid>
<content:encoded><![CDATA[
arXiv:2507.08802v2 Announce Type: replace 
Abstract: The concept of causal abstraction got recently popularised to demystify the opaque decision-making processes of machine learning models; in short, a neural network can be abstracted as a higher-level algorithm if there exists a function which allows us to map between them. Notably, most interpretability papers implement these maps as linear functions, motivated by the linear representation hypothesis: the idea that features are encoded linearly in a model's representations. However, this linearity constraint is not required by the definition of causal abstraction. In this work, we critically examine the concept of causal abstraction by considering arbitrarily powerful alignment maps. In particular, we prove that under reasonable assumptions, any neural network can be mapped to any algorithm, rendering this unrestricted notion of causal abstraction trivial and uninformative. We complement these theoretical findings with empirical evidence, demonstrating that it is possible to perfectly map models to algorithms even when these models are incapable of solving the actual task; e.g., on an experiment using randomly initialised language models, our alignment maps reach 100\% interchange-intervention accuracy on the indirect object identification task. This raises the non-linear representation dilemma: if we lift the linearity constraint imposed to alignment maps in causal abstraction analyses, we are left with no principled way to balance the inherent trade-off between these maps' complexity and accuracy. Together, these results suggest an answer to our title's question: causal abstraction is not enough for mechanistic interpretability, as it becomes vacuous without assumptions about how models encode information. Studying the connection between this information-encoding assumption and causal abstraction should lead to exciting future work.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery</title>
<link>https://arxiv.org/abs/2507.08977</link>
<guid>https://arxiv.org/abs/2507.08977</guid>
<content:encoded><![CDATA[
arXiv:2507.08977v2 Announce Type: replace 
Abstract: Scientific modeling faces a tradeoff: mechanistic models provide scientific grounding but struggle with real-world complexity, while machine learning models achieve strong predictive performance but require large labeled datasets and are not interpretable. We introduce Simulation-Grounded Neural Networks (SGNNs), which use mechanistic simulations as training data for neural networks. SGNNs are pretrained on synthetic corpora spanning diverse model structures, parameter regimes, stochasticity, and observational artifacts. Simulation-grounded learning has been applied in multiple domains (e.g., surrogate models in physics, forecasting in epidemiology). We provide a unified framework for simulation-grounded learning and evaluated SGNNs across scientific disciplines and modeling tasks. We found that SGNNs were successful across domains: for prediction tasks, they nearly tripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield prediction error by one-third, and maintained accuracy in ecological forecasting where task-specific models failed. For inference tasks, SGNNs also accurately classified the source of information spread in simulated social networks and enabled supervised learning for unobservable targets, such as estimating COVID-19 transmissibility more accurately than traditional methods even in early outbreaks. Finally, SGNNs enable back-to-simulation attribution, a form of mechanistic interpretability. Back-to-simulation attribution matches real-world observations to the training simulations the model considers most similar, identifying which mechanistic processes the model believes best explain the observed data. By providing a unified framework for simulation-grounded learning, we establish when and how mechanistic simulations can serve as effective training data for robust, interpretable scientific inference.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3-Net: A Cost-Effective Graph-Free MLP-Based Model for Traffic Prediction</title>
<link>https://arxiv.org/abs/2508.08543</link>
<guid>https://arxiv.org/abs/2508.08543</guid>
<content:encoded><![CDATA[
arXiv:2508.08543v3 Announce Type: replace 
Abstract: Achieving accurate traffic prediction is a fundamental but crucial task in the development of current intelligent transportation systems.Most of the mainstream methods that have made breakthroughs in traffic prediction rely on spatio-temporal graph neural networks, spatio-temporal attention mechanisms, etc. The main challenges of the existing deep learning approaches are that they either depend on a complete traffic network structure or require intricate model designs to capture complex spatio-temporal dependencies. These limitations pose significant challenges for the efficient deployment and operation of deep learning models on large-scale datasets. To address these challenges, we propose a cost-effective graph-free Multilayer Perceptron (MLP) based model M3-Net for traffic prediction. Our proposed model not only employs time series and spatio-temporal embeddings for efficient feature processing but also first introduces a novel MLP-Mixer architecture with a mixture of experts (MoE) mechanism. Extensive experiments conducted on multiple real datasets demonstrate the superiority of the proposed model in terms of prediction performance and lightweight deployment.Our code is available at https://github.com/jinguangyin/M3_NET
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>P-DRUM: Post-hoc Descriptor-based Residual Uncertainty Modeling for Machine Learning Potentials</title>
<link>https://arxiv.org/abs/2509.02927</link>
<guid>https://arxiv.org/abs/2509.02927</guid>
<content:encoded><![CDATA[
arXiv:2509.02927v2 Announce Type: replace 
Abstract: Ensemble method is considered the gold standard for uncertainty quantification (UQ) in machine learning interatomic potentials (MLIPs). However, their high computational cost can limit its practicality. Alternative techniques, such as Monte Carlo dropout and deep kernel learning, have been proposed to improve computational efficiency; however, some of these methods cannot be applied to already trained models and may affect the prediction accuracy. In this paper, we propose a simple and efficient post-hoc framework for UQ that leverages the descriptor of a trained graph neural network potential to estimate residual errors. We refer to this method as post-hoc descriptor-based residual uncertainty modeling (P-DRUM). P-DRUM models the discrepancy between MLIP predictions and ground truth values, allowing these residuals to act as proxies for prediction uncertainty. We explore multiple variants of P-DRUM and benchmark them against established UQ methods, evaluating both their effectiveness and limitations.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2509.18542</link>
<guid>https://arxiv.org/abs/2509.18542</guid>
<content:encoded><![CDATA[
arXiv:2509.18542v2 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To mitigate the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Qwen2.5-Coder and Qwen2). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a stage of post-training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RouterArena: An Open Platform for Comprehensive Comparison of LLM Routers</title>
<link>https://arxiv.org/abs/2510.00202</link>
<guid>https://arxiv.org/abs/2510.00202</guid>
<content:encoded><![CDATA[
arXiv:2510.00202v2 Announce Type: replace 
Abstract: Today's LLM ecosystem comprises a wide spectrum of models that differ in size, capability, and cost. No single model is optimal for all scenarios; hence, LLM routers have become essential for selecting the most appropriate model under varying circumstances. However, the rapid emergence of various routers makes choosing the right one increasingly challenging. To address this problem, we need a comprehensive router comparison and a standardized leaderboard, similar to those available for models. In this work, we introduce RouterArena, the first open platform enabling comprehensive comparison of LLM routers. RouterArena has (1) a principally constructed dataset with broad knowledge domain coverage, (2) distinguishable difficulty levels for each domain, (3) an extensive list of evaluation metrics, and (4) an automated framework for leaderboard updates. Leveraging our framework, we have produced the initial leaderboard with detailed metrics comparison as shown in Figure 1. Our framework for evaluating new routers is on https://github.com/RouteWorks/RouterArena
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation</title>
<link>https://arxiv.org/abs/2510.00478</link>
<guid>https://arxiv.org/abs/2510.00478</guid>
<content:encoded><![CDATA[
arXiv:2510.00478v3 Announce Type: replace 
Abstract: Recent work on latent diffusion models (LDMs) has focused almost exclusively on generative tasks, leaving their potential for discriminative transfer largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a novel LDM-based framework for a more practical variant of source-free domain adaptation (SFDA): the source provider may share not only a pre-trained classifier but also an auxiliary latent diffusion module, trained once on the source data and never exposing raw source samples. DVD encodes each source feature's label information into its latent vicinity by fitting a Gaussian prior over its k-nearest neighbors and training the diffusion network to drift noisy samples back to label-consistent representations. During adaptation, we sample from each target feature's latent vicinity, apply the frozen diffusion module to generate source-like cues, and use a simple InfoNCE loss to align the target encoder to these cues, explicitly transferring decision boundaries without source access. Across standard SFDA benchmarks, DVD outperforms state-of-the-art methods. We further show that the same latent diffusion module enhances the source classifier's accuracy on in-domain data and boosts performance in supervised classification and domain generalization experiments. DVD thus reinterprets LDMs as practical, privacy-preserving bridges for explicit knowledge transfer, addressing a core challenge in source-free domain adaptation that prior methods have yet to solve.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Token Hidden Reward: Steering Exploration-Exploitation in Group Relative Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.03669</link>
<guid>https://arxiv.org/abs/2510.03669</guid>
<content:encoded><![CDATA[
arXiv:2510.03669v3 Announce Type: replace 
Abstract: Reinforcement learning with verifiable rewards has significantly advanced the reasoning capabilities of large language models, yet how to explicitly steer training toward exploration or exploitation remains an open problem. We introduce Token Hidden Reward (THR), a token-level metric that quantifies each token's influence on the likelihood of correct responses under Group Relative Policy Optimization (GRPO). We find that training dynamics are dominated by a small subset of tokens with high absolute THR values. Most interestingly, tokens with positive THR strengthen confidence in correct outputs, thus favoring exploitation, while tokens with negative THR preserve probability mass for alternative outputs, enabling exploration. This insight suggests a natural intervention: a THR-guided reweighting algorithm that modulates GRPO's learning signals to explicitly bias training toward exploitation or exploration. We validate the efficacy of this algorithm on diverse math reasoning benchmarks. By amplifying tokens with positive THR value and weakening negative ones, our algorithm improves greedy-decoding accuracy, favoring exploitation. The reverse strategy yields consistent gains in Pass@K accuracy, favoring exploration. We further demonstrate that our algorithm integrates seamlessly with other RL objectives such as GSPO and generalizes across architectures including Llama. These findings establish THR as a principled and fine-grained mechanism for dynamically controlling exploration and exploitation in RL-tuned LLMs, providing new tools for targeted fine-tuning in reasoning-intensive applications.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heterogeneous Point Set Transformers for Segmentation of Multiple View Particle Detectors</title>
<link>https://arxiv.org/abs/2510.09659</link>
<guid>https://arxiv.org/abs/2510.09659</guid>
<content:encoded><![CDATA[
arXiv:2510.09659v2 Announce Type: replace 
Abstract: NOvA is a long-baseline neutrino oscillation experiment that detects neutrino particles from the NuMI beam at Fermilab. Before data from this experiment can be used in analyses, raw hits in the detector must be matched to their source particles, and the type of each particle must be identified. This task has commonly been done using a mix of traditional clustering approaches and convolutional neural networks (CNNs). Due to the construction of the detector, the data is presented as two sparse 2D images: an XZ and a YZ view of the detector, rather than a 3D representation. We propose a point set neural network that operates on the sparse matrices with an operation that mixes information from both views. Our model uses less than 10% of the memory required using previous methods while achieving a 96.8% AUC score, a higher score than obtained when both views are processed independently (85.4%).
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jet Functors and Weil Algebras in Automatic Differentiation: A Geometric Analysis</title>
<link>https://arxiv.org/abs/2510.14342</link>
<guid>https://arxiv.org/abs/2510.14342</guid>
<content:encoded><![CDATA[
arXiv:2510.14342v2 Announce Type: replace 
Abstract: We present a differential-geometric formulation of automatic differentiation (AD) based on jet functors and Weil algebras. In this framework, forward- and reverse-mode differentiation arise naturally as pushforward and cotangent pullback, while higher-order differentiation corresponds to evaluation in a Weil algebra. This construction provides a unified, coordinate-free view of derivative propagation and clarifies the algebraic structure underlying AD. All results are realized in modern JAX code, where the Weil-mode formulation computes all mixed derivatives in a single forward pass with cost linear in the algebra dimension. The resulting implementation achieves algebraically exact and numerically stable differentiation with predictable scaling, demonstrating that geometric abstraction can yield more efficient and transparent computational differentiation systems. Code is available at https://git.nilu.no/geometric-ad/jet-weil-ad
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Bounds for Rank-sparse Neural Networks</title>
<link>https://arxiv.org/abs/2510.21945</link>
<guid>https://arxiv.org/abs/2510.21945</guid>
<content:encoded><![CDATA[
arXiv:2510.21945v2 Announce Type: replace 
Abstract: It has been recently observed in much of the literature that neural networks exhibit a bottleneck rank property: for larger depths, the activation and weights of neural networks trained with gradient-based methods tend to be of approximately low rank. In fact, the rank of the activations of each layer converges to a fixed value referred to as the ``bottleneck rank'', which is the minimum rank required to represent the training data. This perspective is in line with the observation that regularizing linear networks (without activations) with weight decay is equivalent to minimizing the Schatten $p$ quasi norm of the neural network. In this paper we investigate the implications of this phenomenon for generalization. More specifically, we prove generalization bounds for neural networks which exploit the approximate low rank structure of the weight matrices if present. The final results rely on the Schatten $p$ quasi norms of the weight matrices: for small $p$, the bounds exhibit a sample complexity $ \widetilde{O}(WrL^2)$ where $W$ and $L$ are the width and depth of the neural network respectively and where $r$ is the rank of the weight matrices. As $p$ increases, the bound behaves more like a norm-based bound instead.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Diffusion Language Models via Unpaired Preference Optimization</title>
<link>https://arxiv.org/abs/2510.23658</link>
<guid>https://arxiv.org/abs/2510.23658</guid>
<content:encoded><![CDATA[
arXiv:2510.23658v2 Announce Type: replace 
Abstract: Diffusion language models (dLLMs) are an emerging alternative to autoregressive (AR) generators, but aligning them to human preferences is challenging because sequence log-likelihoods are intractable and pairwise preference data are costly to collect. We introduce ELBO-KTO, which combines an ELBO surrogate for diffusion log-likelihoods with a prospect-theoretic, unpaired preference objective (Kahneman Tversky Optimization, KTO). We analyze the bias and variance induced by the ELBO substitution and employ variance-reduction practices that stabilize gradients during training. Applied to LLaDA-8B-Instruct, ELBO-KTO yields 65.9% and 62.3% adjusted win rates on kto-mix-14k and UltraFeedback-Binary, respectively, versus the base model under an automatic LLM judge. Across downstream tasks, including GSM8K, MMLU, and additional reasoning/knowledge benchmarks, ELBO-KTO trained on UltraFeedback-Binary performs on par with or better than the base model under identical decoding. This establishes unpaired preference optimization as a viable alternative to pairwise alignment in diffusion LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive EEG-based stroke diagnosis with a GRU-TCN classifier and deep Q-learning thresholding</title>
<link>https://arxiv.org/abs/2510.24889</link>
<guid>https://arxiv.org/abs/2510.24889</guid>
<content:encoded><![CDATA[
arXiv:2510.24889v3 Announce Type: replace 
Abstract: Rapid triage of suspected stroke needs accurate, bedside-deployable tools; EEG is promising but underused at first contact. We present an adaptive multitask EEG classifier that converts 32-channel signals to power spectral density features (Welch), uses a recurrent-convolutional network (GRU-TCN) to predict stroke type (healthy, ischemic, hemorrhagic), hemispheric lateralization, and severity, and applies a deep Q-network (DQN) to tune decision thresholds in real time. Using a patient-wise split of the UCLH Stroke EIT/EEG data set (44 recordings; about 26 acute stroke, 10 controls), the primary outcome was stroke-type performance; secondary outcomes were severity and lateralization. The baseline GRU-TCN reached 89.3% accuracy (F1 92.8%) for stroke type, about 96.9% (F1 95.9%) for severity, and about 96.7% (F1 97.4%) for lateralization. With DQN threshold adaptation, stroke-type accuracy increased to about 98.0% (F1 97.7%). We also tested robustness on an independent, low-density EEG cohort (ZJU4H) and report paired patient-level statistics. Analyses follow STARD 2015 guidance for diagnostic accuracy studies (index test: GRU-TCN+DQN; reference standard: radiology/clinical diagnosis; patient-wise evaluation). Adaptive thresholding shifts the operating point to clinically preferred sensitivity-specificity trade-offs, while integrated scalp-map and spectral visualizations support interpretability.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Data Reveals Generalization Gaps in Correlated Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2510.25759</link>
<guid>https://arxiv.org/abs/2510.25759</guid>
<content:encoded><![CDATA[
arXiv:2510.25759v2 Announce Type: replace 
Abstract: Multiple instance learning (MIL) is often used in medical imaging to classify high-resolution 2D images by processing patches or classify 3D volumes by processing slices. However, conventional MIL approaches treat instances separately, ignoring contextual relationships such as the appearance of nearby patches or slices that can be essential in real applications. We design a synthetic classification task where accounting for adjacent instance features is crucial for accurate prediction. We demonstrate the limitations of off-the-shelf MIL approaches by quantifying their performance compared to the optimal Bayes estimator for this task, which is available in closed-form. We empirically show that newer correlated MIL methods still do not achieve the best possible performance when trained with ten thousand training samples, each containing many instances.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Optimization and Generalization Bounds for Stochastic Gauss-Newton in Overparameterized Models</title>
<link>https://arxiv.org/abs/2511.03972</link>
<guid>https://arxiv.org/abs/2511.03972</guid>
<content:encoded><![CDATA[
arXiv:2511.03972v2 Announce Type: replace 
Abstract: An important question in deep learning is how higher-order optimization methods affect generalization. In this work, we analyze a stochastic Gauss-Newton (SGN) method with Levenberg-Marquardt damping and mini-batch sampling for training overparameterized deep neural networks with smooth activations in a regression setting. Our theoretical contributions are twofold. First, we establish finite-time convergence bounds via a variable-metric analysis in parameter space, with explicit dependencies on the batch size, network width and depth. Second, we derive non-asymptotic generalization bounds for SGN using uniform stability in the overparameterized regime, characterizing the impact of curvature, batch size, and overparameterization on generalization performance. Our theoretical results identify a favorable generalization regime for SGN in which a larger minimum eigenvalue of the Gauss-Newton matrix along the optimization path yields tighter stability bounds.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-Aware Tensor Decomposition for Compression of Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2511.04494</link>
<guid>https://arxiv.org/abs/2511.04494</guid>
<content:encoded><![CDATA[
arXiv:2511.04494v2 Announce Type: replace 
Abstract: Neural networks are widely used for image-related tasks but typically demand considerable computing power. Once a network has been trained, however, its memory- and compute-footprint can be reduced by compression. In this work, we focus on compression through tensorization and low-rank representations. Whereas classical approaches search for a low-rank approximation by minimizing an isotropic norm such as the Frobenius norm in weight-space, we use data-informed norms that measure the error in function space. Concretely, we minimize the change in the layer's output distribution, which can be expressed as $\lVert (W - \widetilde{W}) \Sigma^{1/2}\rVert_F$ where $\Sigma^{1/2}$ is the square root of the covariance matrix of the layer's input and $W$, $\widetilde{W}$ are the original and compressed weights. We propose new alternating least square algorithms for the two most common tensor decompositions (Tucker-2 and CPD) that directly optimize the new norm. Unlike conventional compression pipelines, which almost always require post-compression fine-tuning, our data-informed approach often achieves competitive accuracy without any fine-tuning. We further show that the same covariance-based norm can be transferred from one dataset to another with only a minor accuracy drop, enabling compression even when the original training dataset is unavailable. Experiments on several CNN architectures (ResNet-18/50, and GoogLeNet) and datasets (ImageNet, FGVC-Aircraft, Cifar10, and Cifar100) confirm the advantages of the proposed method.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Background Invariance Testing According to Semantic Proximity</title>
<link>https://arxiv.org/abs/2208.09286</link>
<guid>https://arxiv.org/abs/2208.09286</guid>
<content:encoded><![CDATA[
arXiv:2208.09286v2 Announce Type: replace-cross 
Abstract: In many applications, machine-learned (ML) models are required to hold some invariance qualities, such as rotation, size, and intensity invariance. Among these, testing for background invariance presents a significant challenge due to the vast and complex data space it encompasses. To evaluate invariance qualities, we first use a visualization-based testing framework which allows human analysts to assess and make informed decisions about the invariance properties of ML models. We show that such informative testing framework is preferred as ML models with the same global statistics (e.g., accuracy scores) can behave differently and have different visualized testing patterns. However, such human analysts might not lead to consistent decisions without a systematic sampling approach to select representative testing suites. In this work, we present a technical solution for selecting background scenes according to their semantic proximity to a target image that contains a foreground object being tested. We construct an ontology for storing knowledge about relationships among different objects using association analysis. This ontology enables an efficient and meaningful search for background scenes of different semantic distances to a target image, enabling the selection of a test suite that is both diverse and reasonable. Compared with other testing techniques, e.g., random sampling, nearest neighbors, or other sampled test suites by visual-language models (VLMs), our method achieved a superior balance between diversity and consistency of human annotations, thereby enhancing the reliability and comprehensiveness of background invariance testing.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc travel time and path choice model estimation subsumed</title>
<link>https://arxiv.org/abs/2210.14351</link>
<guid>https://arxiv.org/abs/2210.14351</guid>
<content:encoded><![CDATA[
arXiv:2210.14351v2 Announce Type: replace-cross 
Abstract: We address the problem of simultaneously estimating arc travel times in a network \emph{and} parameters of route choice models for strategic and tactical network planning purposes. Hitherto, these interdependent tasks have been approached separately in the literature on road traffic networks. We illustrate that ignoring this interdependence can lead to erroneous route choice model parameter estimates. We propose a method for maximum likelihood estimation to solve the simultaneous estimation problem that is applicable to any differentiable route choice model. Moreover, our approach allows to naturally mix observations at varying levels of granularity, including noisy or partial path data. Numerical results based on real taxi data from New York City show strong performance of our method, even in comparison to a benchmark method focused solely on arc travel time estimation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Ensemble Learning for Sector Rotation: A Gradient-Free Framework</title>
<link>https://arxiv.org/abs/2304.09947</link>
<guid>https://arxiv.org/abs/2304.09947</guid>
<content:encoded><![CDATA[
arXiv:2304.09947v2 Announce Type: replace-cross 
Abstract: We propose a gradient-free online ensemble learning algorithm that dynamically combines forecasts from a heterogeneous set of machine learning models based on their recent predictive performance, measured by out-of-sample R-squared. The ensemble is model-agnostic, requires no gradient access, and is designed for sequential forecasting under nonstationarity. It adaptively reweights 16 constituent models-three linear benchmarks (OLS, PCR, LASSO) and thirteen nonlinear learners including Random Forests, Gradient-Boosted Trees, and a hierarchy of neural networks (NN1-NN12). We apply the framework to sector rotation, using sector-level features aggregated from firm characteristics. Empirically, sector returns are more predictable and stable than individual asset returns, making them suitable for cross-sectional forecasting. The algorithm constructs sector-specific ensembles that assign adaptive weights in a rolling-window fashion, guided by forecast accuracy. Our key theoretical result bounds the online forecast regret directly in terms of realized out-of-sample R-squared, providing an interpretable guarantee that the ensemble performs nearly as well as the best model in hindsight. Empirically, the ensemble consistently outperforms individual models, equal-weighted averages, and traditional offline ensembles, delivering higher predictive accuracy, stronger risk-adjusted returns, and robustness across macroeconomic regimes, including during the COVID-19 crisis.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formalizing and Benchmarking Prompt Injection Attacks and Defenses</title>
<link>https://arxiv.org/abs/2310.12815</link>
<guid>https://arxiv.org/abs/2310.12815</guid>
<content:encoded><![CDATA[
arXiv:2310.12815v5 Announce Type: replace-cross 
Abstract: A prompt injection attack aims to inject malicious instruction/data into the input of an LLM-Integrated Application such that it produces results as an attacker desires. Existing works are limited to case studies. As a result, the literature lacks a systematic understanding of prompt injection attacks and their defenses. We aim to bridge the gap in this work. In particular, we propose a framework to formalize prompt injection attacks. Existing attacks are special cases in our framework. Moreover, based on our framework, we design a new attack by combining existing ones. Using our framework, we conduct a systematic evaluation on 5 prompt injection attacks and 10 defenses with 10 LLMs and 7 tasks. Our work provides a common benchmark for quantitatively evaluating future prompt injection attacks and defenses. To facilitate research on this topic, we make our platform public at https://github.com/liu00222/Open-Prompt-Injection.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bandit Convex Optimisation</title>
<link>https://arxiv.org/abs/2402.06535</link>
<guid>https://arxiv.org/abs/2402.06535</guid>
<content:encoded><![CDATA[
arXiv:2402.06535v5 Announce Type: replace-cross 
Abstract: Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. This book covers the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proximal Oracles for Optimization and Sampling</title>
<link>https://arxiv.org/abs/2404.02239</link>
<guid>https://arxiv.org/abs/2404.02239</guid>
<content:encoded><![CDATA[
arXiv:2404.02239v3 Announce Type: replace-cross 
Abstract: We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either H\"older smooth or in hybrid form as the finite sum of H\"older smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish its iteration-complexity under both H\"older smoothness and hybrid settings using novel convergence analysis, yielding results that are new to the literature. We further propose an adaptive proximal bundle method for non-smooth optimization that employs an aggressive adaptive stepsize strategy, which adjusts stepsizes only when necessary and never rejects iterates. The proposed method is universal since it does not need any problem parameters as input. Additionally, we provide an exact implementation of a proximal sampling oracle, analogous to the proximal map in optimization, along with simple complexity analyses for both the H\"older smooth and hybrid cases, using a novel technique based on a modified Gaussian integral. Finally, we combine this proximal sampling oracle and ASF to obtain a Markov chain Monte Carlo method with non-asymptotic complexity bounds for sampling in H\"older smooth and hybrid settings.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating Non-Markovian Open Quantum Dynamics with Neural Quantum States</title>
<link>https://arxiv.org/abs/2404.11093</link>
<guid>https://arxiv.org/abs/2404.11093</guid>
<content:encoded><![CDATA[
arXiv:2404.11093v3 Announce Type: replace-cross 
Abstract: Reducing computational scaling for simulating non-Markovian dissipative dynamics using artificial neural networks is both a major focus and formidable challenge in open quantum systems. To enable neural quantum states (NQSs), we encode environmental memory in dissipatons (quasiparticles with characteristic lifetimes), yielding the dissipaton-embedded quantum master equation (DQME). The resulting NQS-DQME framework achieves compact representation of many-body correlations and non-Markovian memory. Benchmarking against numerically exact hierarchical equations of motion confirms NQS-DQME maintains comparable accuracy while enhancing scalability and interpretability. This methodology opens new paths to explore non-Markovian open quantum dynamics in previously intractable systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Waveform Design for Over-the-Air Computing</title>
<link>https://arxiv.org/abs/2405.20877</link>
<guid>https://arxiv.org/abs/2405.20877</guid>
<content:encoded><![CDATA[
arXiv:2405.20877v2 Announce Type: replace-cross 
Abstract: In response to the increasing number of devices expected in next-generation networks, a shift to over-the-air (OTA) computing has been proposed. By leveraging the superposition of multiple access channels, OTA computing enables efficient resource management by supporting simultaneous uncoded transmission in the time and frequency domains. To advance the integration of OTA computing, our study presents a theoretical analysis that addresses practical issues encountered in current digital communication transceivers, such as transmitter synchronization (sync) errors and intersymbol interference (ISI). To this end, we investigate the theoretical mean squared error (MSE) for OTA transmission under sync errors and ISI, while also exploring methods for minimizing the MSE in OTA transmission. Using alternating optimization, we also derive optimal power policies for both the devices and the base station. In addition, we propose a novel deep neural network (DNN)-based approach to design waveforms that improve OTA transmission performance under sync errors and ISI. To ensure a fair comparison with existing waveforms such as raised cosine (RC) and better-than-raised-cosine (BTRC), we incorporate a custom loss function that integrates energy and bandwidth constraints along with practical design considerations such as waveform symmetry. Simulation results validate our theoretical analysis and demonstrate performance gains of the designed pulse over RC and BTRC waveforms. To facilitate testing of our results without the need to rebuild the DNN structure, we also provide curve-fitting parameters for the selected DNN-based waveforms.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ElicitationGPT: Text Elicitation Mechanisms via Language Models</title>
<link>https://arxiv.org/abs/2406.09363</link>
<guid>https://arxiv.org/abs/2406.09363</guid>
<content:encoded><![CDATA[
arXiv:2406.09363v3 Announce Type: replace-cross 
Abstract: Scoring rules evaluate probabilistic forecasts of an unknown state against the realized state and are a fundamental building block in the incentivized elicitation of information. This paper develops mechanisms for scoring elicited text against ground truth text by reducing the textual information elicitation problem to a forecast elicitation problem, via domain-knowledge-free queries to a large language model (specifically ChatGPT), and empirically evaluates their alignment with human preferences. Our theoretical analysis shows that the reduction achieves provable properness via black-box language models. The empirical evaluation is conducted on peer reviews from a peer-grading dataset, in comparison to manual instructor scores for the peer reviews.
  Our results suggest a paradigm of algorithmic artificial intelligence that may be useful for developing artificial intelligence technologies with provable guarantees.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmileyLlama: Modifying Large Language Models for Directed Chemical Space Exploration</title>
<link>https://arxiv.org/abs/2409.02231</link>
<guid>https://arxiv.org/abs/2409.02231</guid>
<content:encoded><![CDATA[
arXiv:2409.02231v4 Announce Type: replace-cross 
Abstract: Here we show that a general-purpose large language model (LLM) chatbot, Llama-3.1-8B-Instruct, can be transformed via supervised fine-tuning of engineered prompts into a chemical language model (CLM), SmileyLlama, for molecule generation. We benchmark SmileyLlama by comparing it to CLMs trained from scratch on large amounts of ChEMBL data for their ability to generate valid and novel drug-like molecules. We also use direct preference optimization to both improve SmileyLlama's adherence to a prompt and to generate molecules within the iMiner reinforcement learning framework to predict new drug molecules with optimized 3D conformations and high binding affinity to drug targets, illustrated with the SARS-Cov-2 Main Protease. This overall framework allows a LLM to speak directly as a CLM which can generate molecules with user-specified properties, rather than acting only as a chatbot with knowledge of chemistry or as a helpful virtual assistant. While our dataset and analyses are geared toward drug discovery, this general procedure can be extended to other chemical applications such as chemical synthesis.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dataset-Free Weight-Initialization on Restricted Boltzmann Machine</title>
<link>https://arxiv.org/abs/2409.07708</link>
<guid>https://arxiv.org/abs/2409.07708</guid>
<content:encoded><![CDATA[
arXiv:2409.07708v4 Announce Type: replace-cross 
Abstract: In feed-forward neural networks, dataset-free weight-initialization methods such as LeCun, Xavier (or Glorot), and He initializations have been developed. These methods randomly determine the initial values of weight parameters based on specific distributions (e.g., Gaussian or uniform distributions) without using training datasets. To the best of the authors' knowledge, such a dataset-free weight-initialization method is yet to be developed for restricted Boltzmann machines (RBMs), which are probabilistic neural networks consisting of two layers. In this study, we derive a dataset-free weight-initialization method for Bernoulli--Bernoulli RBMs based on statistical mechanical analysis. In the proposed weight-initialization method, the weight parameters are drawn from a Gaussian distribution with zero mean. The standard deviation of the Gaussian distribution is optimized based on our hypothesis that a standard deviation providing a larger layer correlation (LC) between the two layers improves the learning efficiency. The expression of the LC is derived based on a statistical mechanical analysis. The optimal value of the standard deviation corresponds to the maximum point of the LC. The proposed weight-initialization method is identical to Xavier initialization in a specific case (i.e., when the sizes of the two layers are the same, the random variables of the layers are $\{-1,1\}$-binary, and all bias parameters are zero). The validity of the proposed weight-initialization method is demonstrated in numerical experiments using a toy and real-world datasets.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey of Low-bit Large Language Models: Basics, Systems, and Algorithms</title>
<link>https://arxiv.org/abs/2409.16694</link>
<guid>https://arxiv.org/abs/2409.16694</guid>
<content:encoded><![CDATA[
arXiv:2409.16694v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable advancements in natural language processing, showcasing exceptional performance across various tasks. However, the expensive memory and computational requirements present significant challenges for their practical deployment. Low-bit quantization has emerged as a critical approach to mitigate these challenges by reducing the bit-width of model parameters, activations, and gradients, thus decreasing memory usage and computational demands. This paper presents a comprehensive survey of low-bit quantization methods tailored for LLMs, covering the fundamental principles, system implementations, and algorithmic strategies. An overview of basic concepts and new data formats specific to low-bit LLMs is first introduced, followed by a review of frameworks and systems that facilitate low-bit LLMs across various hardware platforms. Then, we categorize and analyze techniques and toolkits for efficient low-bit training and inference of LLMs. Finally, we conclude with a discussion of future trends and potential advancements of low-bit LLMs. Our systematic overview from basic, system, and algorithm perspectives can offer valuable insights and guidelines for future works to enhance the efficiency and applicability of LLMs through low-bit quantization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CART: Compositional Auto-Regressive Transformer for Image Generation</title>
<link>https://arxiv.org/abs/2411.10180</link>
<guid>https://arxiv.org/abs/2411.10180</guid>
<content:encoded><![CDATA[
arXiv:2411.10180v3 Announce Type: replace-cross 
Abstract: We propose a novel Auto-Regressive (AR) image generation approach that models images as hierarchical compositions of interpretable visual layers. While AR models have achieved transformative success in language modeling, replicating this success in vision tasks remains challenging due to inherent spatial dependencies in images. Addressing the unique challenges of vision tasks, our method (CART) adds image details iteratively via semantically meaningful decompositions. We demonstrate the flexibility and generality of CART by applying it across three distinct decomposition strategies: (i) Base-Detail Decomposition (Mumford-Shah smoothness), (ii) Intrinsic Decomposition (albedo/shading), and (iii) Specularity Decomposition (diffuse/specular). This next-detail strategy outperforms traditional next-token and next-scale approaches, improving controllability, semantic interpretability, and resolution scalability. Experiments show CART generates visually compelling results while enabling structured image manipulation, opening new directions for controllable generative modeling via physically or perceptually motivated image factorization.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training and Evaluating Language Models with Template-based Data Generation</title>
<link>https://arxiv.org/abs/2411.18104</link>
<guid>https://arxiv.org/abs/2411.18104</guid>
<content:encoded><![CDATA[
arXiv:2411.18104v5 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) such as GPT-3, PaLM, and Llama has significantly transformed natural language processing, showcasing remarkable capabilities in understanding and generating language. However, a fundamental bottleneck persists: these models often struggle with tasks requiring complex, multi-step reasoning, particularly in mathematical problem-solving. This deficiency stems from the critical scarcity of large-scale, high-quality, domain-specific datasets necessary for cultivating sophisticated reasoning abilities. To overcome this challenge, we introduce Template-based Data Generation (TDG), a novel and scalable paradigm that harnesses frontier LLMs (GPT-4) to automatically generate parameterized meta-templates, which in turn synthesize a virtually infinite stream of high-quality problems and solutions. Using this paradigm, we create TemplateMath Part I: TemplateGSM, a foundational dataset of over 7 million synthetically generated grade school math problems. Each problem is accompanied by a programmatically verifiable solution, offering an unprecedented level of quality at scale. This resource not only resolves the data scarcity issue for supervised fine-tuning but also provides a robust mechanism for model alignment through Reinforcement Learning with Verifiable Rewards (RLVR). Our approach elevates data augmentation by leveraging GPT-4 to generate meta-templates, ensuring diverse and complex problem structures. By providing a scalable solution to the data and verification bottleneck, TDG and TemplateGSM pave the way for a new generation of LLMs with powerful, reliable reasoning skills. Project Page: https://github.com/iiis-ai/TemplateMath
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models</title>
<link>https://arxiv.org/abs/2412.20586</link>
<guid>https://arxiv.org/abs/2412.20586</guid>
<content:encoded><![CDATA[
arXiv:2412.20586v2 Announce Type: replace-cross 
Abstract: Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GreedyPixel: Fine-Grained Black-Box Adversarial Attack Via Greedy Algorithm</title>
<link>https://arxiv.org/abs/2501.14230</link>
<guid>https://arxiv.org/abs/2501.14230</guid>
<content:encoded><![CDATA[
arXiv:2501.14230v3 Announce Type: replace-cross 
Abstract: Deep neural networks are highly vulnerable to adversarial examples, which are inputs with small, carefully crafted perturbations that cause misclassification--making adversarial attacks a critical tool for evaluating robustness. Existing black-box methods typically entail a trade-off between precision and flexibility: pixel-sparse attacks (e.g., single- or few-pixel attacks) provide fine-grained control but lack adaptability, whereas patch- or frequency-based attacks improve efficiency or transferability, but at the cost of producing larger and less precise perturbations. We present GreedyPixel, a fine-grained black-box attack method that performs brute-force-style, per-pixel greedy optimization guided by a surrogate-derived priority map and refined by means of query feedback. It evaluates each coordinate directly without any gradient information, guaranteeing monotonic loss reduction and convergence to a coordinate-wise optimum, while also yielding near white-box-level precision and pixel-wise sparsity and perceptual quality. On the CIFAR-10 and ImageNet datasets, spanning convolutional neural networks (CNNs) and Transformer models, GreedyPixel achieved state-of-the-art success rates with visually imperceptible perturbations, effectively bridging the gap between black-box practicality and white-box performance. The implementation is available at https://github.com/azrealwang/greedypixel.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Variational Inference for Bayesian Mixture Models</title>
<link>https://arxiv.org/abs/2502.12684</link>
<guid>https://arxiv.org/abs/2502.12684</guid>
<content:encoded><![CDATA[
arXiv:2502.12684v2 Announce Type: replace-cross 
Abstract: We present a federated learning approach for Bayesian model-based clustering of large-scale binary and categorical datasets. We introduce a principled 'divide and conquer' inference procedure using variational inference with local merge and delete moves within batches of the data in parallel, followed by 'global' merge moves across batches to find global clustering structures. We show that these merge moves require only summaries of the data in each batch, enabling federated learning across local nodes without requiring the full dataset to be shared. Empirical results on simulated and benchmark datasets demonstrate that our method performs well in comparison to existing clustering algorithms. We validate the practical utility of the method by applying it to large scale electronic health record (EHR) data.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UDora: A Unified Red Teaming Framework against LLM Agents by Dynamically Hijacking Their Own Reasoning</title>
<link>https://arxiv.org/abs/2503.01908</link>
<guid>https://arxiv.org/abs/2503.01908</guid>
<content:encoded><![CDATA[
arXiv:2503.01908v3 Announce Type: replace-cross 
Abstract: Large Language Model (LLM) agents equipped with external tools have become increasingly powerful for complex tasks such as web shopping, automated email replies, and financial trading. However, these advancements amplify the risks of adversarial attacks, especially when agents can access sensitive external functionalities. Nevertheless, manipulating LLM agents into performing targeted malicious actions or invoking specific tools remains challenging, as these agents extensively reason or plan before executing final actions. In this work, we present UDora, a unified red teaming framework designed for LLM agents that dynamically hijacks the agent's reasoning processes to compel malicious behavior. Specifically, UDora first generates the model's reasoning trace for the given task, then automatically identifies optimal points within this trace to insert targeted perturbations. The resulting perturbed reasoning is then used as a surrogate response for optimization. By iteratively applying this process, the LLM agent will then be induced to undertake designated malicious actions or to invoke specific malicious tools. Our approach demonstrates superior effectiveness compared to existing methods across three LLM agent datasets. The code is available at https://github.com/AI-secure/UDora.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning conformational ensembles of proteins based on backbone geometry</title>
<link>https://arxiv.org/abs/2503.05738</link>
<guid>https://arxiv.org/abs/2503.05738</guid>
<content:encoded><![CDATA[
arXiv:2503.05738v2 Announce Type: replace-cross 
Abstract: Deep generative models have recently been proposed for sampling protein conformations from the Boltzmann distribution, as an alternative to often prohibitively expensive Molecular Dynamics simulations. However, current state-of-the-art approaches rely on fine-tuning pre-trained folding models and evolutionary sequence information, limiting their applicability and efficiency, and introducing potential biases. In this work, we propose a flow matching model for sampling protein conformations based solely on backbone geometry - BBFlow. We introduce a geometric encoding of the backbone equilibrium structure as input and propose to condition not only the flow but also the prior distribution on the respective equilibrium structure, eliminating the need for evolutionary information. The resulting model is orders of magnitudes faster than current state-of-the-art approaches at comparable accuracy, is transferable to multi-chain proteins, and can be trained from scratch in a few GPU days. In our experiments, we demonstrate that the proposed model achieves competitive performance with reduced inference time, across not only an established benchmark of naturally occurring proteins but also de novo proteins, for which evolutionary information is scarce or absent. BBFlow is available at https://github.com/graeter-group/bbflow.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Are Unreliable for Cyber Threat Intelligence</title>
<link>https://arxiv.org/abs/2503.23175</link>
<guid>https://arxiv.org/abs/2503.23175</guid>
<content:encoded><![CDATA[
arXiv:2503.23175v4 Announce Type: replace-cross 
Abstract: Several recent works have argued that Large Language Models (LLMs) can be used to tame the data deluge in the cybersecurity field, by improving the automation of Cyber Threat Intelligence (CTI) tasks. This work presents an evaluation methodology that other than allowing to test LLMs on CTI tasks when using zero-shot learning, few-shot learning and fine-tuning, also allows to quantify their consistency and their confidence level. We run experiments with three state-of-the-art LLMs and a dataset of 350 threat intelligence reports and present new evidence of potential security risks in relying on LLMs for CTI. We show how LLMs cannot guarantee sufficient performance on real-size reports while also being inconsistent and overconfident. Few-shot learning and fine-tuning only partially improve the results, thus posing doubts about the possibility of using LLMs for CTI scenarios, where labelled datasets are lacking and where confidence is a fundamental factor.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrystalFormer-RL: Reinforcement Fine-Tuning for Materials Design</title>
<link>https://arxiv.org/abs/2504.02367</link>
<guid>https://arxiv.org/abs/2504.02367</guid>
<content:encoded><![CDATA[
arXiv:2504.02367v2 Announce Type: replace-cross 
Abstract: Reinforcement fine-tuning played an instrumental role in enhancing the instruction-following and reasoning abilities of large language models. In this work, we employ reinforcement fine-tuning for materials design, in which discriminative machine learning models are used to provide rewards to the autoregressive transformer-based materials generative model CrystalFormer. By optimizing the reward signals-such as energy above the convex hull and material properties figures of merit-reinforcement fine-tuning infuses knowledge from discriminative models into generative models. The resulting model, CrystalFormer-RL, shows enhanced stability in generated crystals and successfully discovers crystals with desirable yet conflicting material properties, such as substantial dielectric constant and band gap simultaneously. Notably, we observe that reinforcement fine-tuning not only enables the property-guided material design but also unlocks property-based material retrieval behavior of pretrained generative model. The present framework opens an exciting gateway to the synergies of the machine learning ecosystem for materials design.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limits of Discrete Energy of Families of Increasing Sets</title>
<link>https://arxiv.org/abs/2504.11302</link>
<guid>https://arxiv.org/abs/2504.11302</guid>
<content:encoded><![CDATA[
arXiv:2504.11302v4 Announce Type: replace-cross 
Abstract: The Hausdorff dimension of a set can be detected using the Riesz energy. Here, we consider situations where a sequence of points, $\{x_n\}$, ``fills in'' a set $E \subset \mathbb{R}^d$ in an appropriate sense and investigate the degree to which the discrete analog to the Riesz energy of these sets can be used to bound the Hausdorff dimension of $E$. We also discuss applications to data science and Erd\H{o}s/Falconer type problems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bayesian Approach to Segmentation with Noisy Labels via Spatially Correlated Distributions</title>
<link>https://arxiv.org/abs/2504.14795</link>
<guid>https://arxiv.org/abs/2504.14795</guid>
<content:encoded><![CDATA[
arXiv:2504.14795v2 Announce Type: replace-cross 
Abstract: In semantic segmentation, the accuracy of models heavily depends on the high-quality annotations. However, in many practical scenarios, such as medical imaging and remote sensing, obtaining true annotations is not straightforward and usually requires significant human labor. Relying on human labor often introduces annotation errors, including mislabeling, omissions, and inconsistency between annotators. In the case of remote sensing, differences in procurement time can lead to misaligned ground-truth annotations. These label errors are not independently distributed, and instead usually appear in spatially connected regions where adjacent pixels are more likely to share the same errors.To address these issues, we propose an approximate Bayesian estimation based on a probabilistic model that assumes training data include label errors, incorporating the tendency for these errors to occur with spatial correlations between adjacent pixels. However, Bayesian inference for such spatially correlated discrete variables is notoriously intractable. To overcome this fundamental challenge, we introduce a novel class of probabilistic models, which we term the ELBO-Computable Correlated Discrete Distribution (ECCD). By representing the discrete dependencies through a continuous latent Gaussian field with a Kac-Murdock-Szeg\"{o} (KMS) structured covariance, our framework enables scalable and efficient variational inference for problems previously considered computationally prohibitive. Through experiments on multiple segmentation tasks, we confirm that leveraging the spatial correlation of label errors significantly improves performance. Notably, in specific tasks such as lung segmentation, the proposed method achieves performance comparable to training with clean labels under moderate noise levels. Code is available at https://github.com/pfnet-research/Bayesian_SpatialCorr.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers</title>
<link>https://arxiv.org/abs/2504.19254</link>
<guid>https://arxiv.org/abs/2504.19254</guid>
<content:encoded><![CDATA[
arXiv:2504.19254v4 Announce Type: replace-cross 
Abstract: Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we outline a versatile framework for closed-book hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we propose a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Symmetry Discovery and Enforcement Using Infinitesimal Generators of Multi-parameter Group Actions</title>
<link>https://arxiv.org/abs/2505.08219</link>
<guid>https://arxiv.org/abs/2505.08219</guid>
<content:encoded><![CDATA[
arXiv:2505.08219v2 Announce Type: replace-cross 
Abstract: Symmetry-informed machine learning can exhibit advantages over machine learning which fails to account for symmetry. In the context of continuous symmetry detection, current state of the art experiments are largely limited to detecting affine transformations. Herein, we outline a computationally efficient framework for discovering infinitesimal generators of multi-parameter group actions which are not generally affine transformations. This framework accommodates the automatic discovery of the number of linearly independent infinitesimal generators. We build upon recent work in continuous symmetry discovery by extending to neural networks and by restricting the symmetry search space to infinitesimal isometries. We also introduce symmetry enforcement of smooth models using vector field regularization, thereby improving model generalization. The notion of vector field similarity is also generalized for non-Euclidean Riemannian metric tensors.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich Document Understanding</title>
<link>https://arxiv.org/abs/2505.17330</link>
<guid>https://arxiv.org/abs/2505.17330</guid>
<content:encoded><![CDATA[
arXiv:2505.17330v2 Announce Type: replace-cross 
Abstract: In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable and efficient model architecture for visually rich document understanding (VRDU) in few-shot settings. FS-DAG leverages domain-specific and language/vision specific backbones within a modular framework to adapt to diverse document types with minimal data. The model is robust to practical challenges such as handling OCR errors, misspellings, and domain shifts, which are critical in real-world deployments. FS-DAG is highly performant with less than 90M parameters, making it well-suited for complex real-world applications for Information Extraction (IE) tasks where computational resources are limited. We demonstrate FS-DAG's capability through extensive experiments for information extraction task, showing significant improvements in convergence speed and performance compared to state-of-the-art methods. Additionally, this work highlights the ongoing progress in developing smaller, more efficient models that do not compromise on performance. Code : https://github.com/oracle-samples/fs-dag
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Diffusion Schr\"odinger Bridge in Astrophysical Observational Inversions</title>
<link>https://arxiv.org/abs/2506.08065</link>
<guid>https://arxiv.org/abs/2506.08065</guid>
<content:encoded><![CDATA[
arXiv:2506.08065v4 Announce Type: replace-cross 
Abstract: We study Diffusion Schr\"odinger Bridge (DSB) models in the context of dynamical astrophysical systems, specifically tackling observational inverse prediction tasks within Giant Molecular Clouds (GMCs) for star formation. We introduce the Astro-DSB model, a variant of DSB with the pairwise domain assumption tailored for astrophysical dynamics. By investigating its learning process and prediction performance in both physically simulated data and in real observations (the Taurus B213 data), we present two main takeaways. First, from the astrophysical perspective, our proposed paired DSB method improves interpretability, learning efficiency, and prediction performance over conventional astrostatistical and other machine learning methods. Second, from the generative modeling perspective, probabilistic generative modeling reveals improvements over discriminative pixel-to-pixel modeling in Out-Of-Distribution (OOD) testing cases of physical simulations with unseen initial conditions and different dominant physical processes. Our study expands research into diffusion models beyond the traditional visual synthesis application and provides evidence of the models' learning abilities beyond pure data statistics, paving a path for future physics-aware generative models which can align dynamics between machine learning and real (astro)physical systems.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models</title>
<link>https://arxiv.org/abs/2506.22493</link>
<guid>https://arxiv.org/abs/2506.22493</guid>
<content:encoded><![CDATA[
arXiv:2506.22493v4 Announce Type: replace-cross 
Abstract: The Political Compass Test (PCT) and similar surveys are commonly used to assess political bias in auto-regressive LLMs. Our rigorous statistical experiments show that while changes to standard generation parameters have minimal effect on PCT scores, prompt phrasing and fine-tuning individually and together can significantly influence results. Interestingly, fine-tuning on politically rich vs. neutral datasets does not lead to different shifts in scores. We also generalize these findings to a similar popular test called 8 Values. Humans do not change their responses to questions when prompted differently (``answer this question'' vs ``state your opinion''), or after exposure to politically neutral text, such as mathematical formulae. But the fact that the models do so raises concerns about the validity of these tests for measuring model bias, and paves the way for deeper exploration into how political and social views are encoded in LLMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
arXiv:2507.12482v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation but remain constrained by inference-time context and lack structured reasoning over code, leaving debugging largely unsolved. While Claude 4.5 Sonnet and Claude 4.1 Opus exceed 70% on code synthesis benchmarks, they achieve under 15% on real debugging tasks. We introduce Kodezi Chronos, a language model purpose-built for debugging that integrates Adaptive Graph-Guided Retrieval to traverse codebases up to 10 million lines, Persistent Debug Memory trained on over 15 million sessions, and a seven-layer fix-test-refine architecture. On 5,000 real-world scenarios, Chronos achieves 67.3% fix accuracy compared to 14.2% and 13.8% for Claude 4.1 Opus and GPT-4.1, respectively. On SWE-bench Lite, Chronos reaches a state-of-the-art 80.33% resolution rate (241 of 300), outperforming the next best system by 20 points and achieving repository-specific highs of 96.1% on Sympy and 90.4% on Django. Chronos reduces debugging time by 40% and iterations by 65%, resolving complex multi-file and cross-repository bugs. It remains limited on hardware-dependent and dynamic language errors. Chronos will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tight Bounds for Answering Adaptively Chosen Concentrated Queries</title>
<link>https://arxiv.org/abs/2507.13700</link>
<guid>https://arxiv.org/abs/2507.13700</guid>
<content:encoded><![CDATA[
arXiv:2507.13700v2 Announce Type: replace-cross 
Abstract: Most work on adaptive data analysis assumes that samples in the dataset are independent. When correlations are allowed, even the non-adaptive setting can become intractable, unless some structural constraints are imposed. To address this, Bassily and Freund [2016] introduced the elegant framework of concentrated queries, which requires the analyst to restrict itself to queries that are concentrated around their expected value. While this assumption makes the problem trivial in the non-adaptive setting, in the adaptive setting it remains quite challenging. In fact, all known algorithms in this framework support significantly fewer queries than in the independent case: At most $O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the independent setting.
  In this work, we prove that this utility gap is inherent under the current formulation of the concentrated queries framework, assuming some natural conditions on the algorithm. Additionally, we present a simplified version of the best-known algorithms that match our impossibility result.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Touch in the Wild: Learning Fine-Grained Manipulation with a Portable Visuo-Tactile Gripper</title>
<link>https://arxiv.org/abs/2507.15062</link>
<guid>https://arxiv.org/abs/2507.15062</guid>
<content:encoded><![CDATA[
arXiv:2507.15062v2 Announce Type: replace-cross 
Abstract: Handheld grippers are increasingly used to collect human demonstrations due to their ease of deployment and versatility. However, most existing designs lack tactile sensing, despite the critical role of tactile feedback in precise manipulation. We present a portable, lightweight gripper with integrated tactile sensors that enables synchronized collection of visual and tactile data in diverse, real-world, and in-the-wild settings. Building on this hardware, we propose a cross-modal representation learning framework that integrates visual and tactile signals while preserving their distinct characteristics. The learning procedure allows the emergence of interpretable representations that consistently focus on contacting regions relevant for physical interactions. When used for downstream manipulation tasks, these representations enable more efficient and effective policy learning, supporting precise robotic manipulation based on multimodal feedback. We validate our approach on fine-grained tasks such as test tube insertion and pipette-based fluid transfer, demonstrating improved accuracy and robustness under external disturbances. Our project page is available at https://binghao-huang.github.io/touch_in_the_wild/ .
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian preference elicitation for decision support in multiobjective optimization</title>
<link>https://arxiv.org/abs/2507.16999</link>
<guid>https://arxiv.org/abs/2507.16999</guid>
<content:encoded><![CDATA[
arXiv:2507.16999v2 Announce Type: replace-cross 
Abstract: We present a novel approach to help decision-makers efficiently identify preferred solutions from the Pareto set of a multi-objective optimization problem. Our method uses a Bayesian model to estimate the decision-maker's utility function based on pairwise comparisons. Aided by this model, a principled elicitation strategy selects queries interactively to balance exploration and exploitation, guiding the discovery of high-utility solutions. The approach is flexible: it can be used interactively or a posteriori after estimating the Pareto front through standard multi-objective optimization techniques. Additionally, at the end of the elicitation phase, it generates a reduced menu of high-quality solutions, simplifying the decision-making process. Through experiments on test problems with up to nine objectives, our method demonstrates superior performance in finding high-utility solutions with a small number of queries. We also provide an open-source implementation of our method to support its adoption by the broader community.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding street network morphologies and their correlation to travel mode choice</title>
<link>https://arxiv.org/abs/2507.19648</link>
<guid>https://arxiv.org/abs/2507.19648</guid>
<content:encoded><![CDATA[
arXiv:2507.19648v2 Announce Type: replace-cross 
Abstract: Urban morphology has long been recognized as a factor shaping human mobility, yet comparative and formal classifications of urban form across metropolitan areas remain limited. Building on theoretical principles of urban structure and advances in unsupervised learning, we systematically classified the built environment of nine U.S. metropolitan areas using structural indicators such as density, connectivity, and spatial configuration. The resulting morphological types were linked to mobility patterns through descriptive statistics, marginal effects estimation, and post hoc statistical testing. Here we show that distinct urban forms are systematically associated with different mobility behaviors, such as reticular morphologies being linked to significantly higher public transport use (marginal effect = 0.49) and reduced car dependence (-0.41), while organic forms are associated with increased car usage (0.44), and substantial declines in public transport (-0.47) and active mobility (-0.30). These effects are statistically robust (p < 1e-19), highlighting that the spatial configuration of urban areas plays a fundamental role in shaping transportation choices. Our findings extend previous work by offering a reproducible framework for classifying urban form and demonstrate the added value of morphological analysis in comparative urban research. These results suggest that urban form should be treated as a key variable in mobility planning and provide empirical support for incorporating spatial typologies into sustainable urban policy design.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amorphous Solid Model of Vectorial Hopfield Neural Networks</title>
<link>https://arxiv.org/abs/2507.22787</link>
<guid>https://arxiv.org/abs/2507.22787</guid>
<content:encoded><![CDATA[
arXiv:2507.22787v2 Announce Type: replace-cross 
Abstract: We introduce a three-dimensional vectorial extension of the Hopfield associative-memory model in which each neuron is a unit vector on $S^2$ and synaptic couplings are $3\times 3$ blocks generated through a vectorial Hebbian rule. The resulting block-structured operator is mathematically analogous to the Hessian of amorphous solids and induces a rigid energy landscape with deep minima for stored patterns. Simulations and spectral analysis demonstrate that the vectorial network substantially outperforms the classical binary Hopfield model: it exhibits a memory capacity that grows linearly with connectivity, a persistent spectral gap separating stored patterns from noise, and significantly enlarged basins of attraction. These results show that geometric constraints combined with amorphous-solid-inspired structure yield associative memories with markedly enhanced stability and retrieval performance.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Veli: Unsupervised Method and Unified Benchmark for Low-Cost Air Quality Sensor Correction</title>
<link>https://arxiv.org/abs/2508.02724</link>
<guid>https://arxiv.org/abs/2508.02724</guid>
<content:encoded><![CDATA[
arXiv:2508.02724v2 Announce Type: replace-cross 
Abstract: Urban air pollution is a major health crisis causing millions of premature deaths annually, underscoring the urgent need for accurate and scalable monitoring of air quality (AQ). While low-cost sensors (LCS) offer a scalable alternative to expensive reference-grade stations, their readings are affected by drift, calibration errors, and environmental interference. To address these challenges, we introduce Veli (Reference-free Variational Estimation via Latent Inference), an unsupervised Bayesian model that leverages variational inference to correct LCS readings without requiring co-location with reference stations, eliminating a major deployment barrier. Specifically, Veli constructs a disentangled representation of the LCS readings, effectively separating the true pollutant reading from the sensor noise. To build our model and address the lack of standardized benchmarks in AQ monitoring, we also introduce the Air Quality Sensor Data Repository (AQ-SDR). AQ-SDR is the largest AQ sensor benchmark to date, with readings from 23,737 LCS and reference stations across multiple regions. Veli demonstrates strong generalization across both in-distribution and out-of-distribution settings, effectively handling sensor drift and erratic sensor behavior. Code for model and dataset will be made public when this paper is published.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushdown Reward Machines for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.06894</link>
<guid>https://arxiv.org/abs/2508.06894</guid>
<content:encoded><![CDATA[
arXiv:2508.06894v2 Announce Type: replace-cross 
Abstract: Reward machines (RMs) are automata structures that encode (non-Markovian) reward functions for reinforcement learning (RL). RMs can reward any behaviour representable in regular languages and, when paired with RL algorithms that exploit RM structure, have been shown to significantly improve sample efficiency in many domains. In this work, we present pushdown reward machines (pdRMs), an extension of reward machines based on deterministic pushdown automata. pdRMs can recognise and reward temporally extended behaviours representable in deterministic context-free languages, making them more expressive than reward machines. We introduce two variants of pdRM-based policies, one which has access to the entire stack of the pdRM, and one which can only access the top $k$ symbols (for a given constant $k$) of the stack. We propose a procedure to check when the two kinds of policies (for a given environment, pdRM, and constant $k$) achieve the same optimal state values. We then provide theoretical results establishing the expressive power of pdRMs, and space complexity results for the proposed learning problems. Lastly, we propose an approach for off-policy RL algorithms that exploits counterfactual experiences with pdRMs. We conclude by providing experimental results showing how agents can be trained to perform tasks representable in deterministic context-free languages using pdRMs.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Hallucinations in Large Language Models via Causal Reasoning</title>
<link>https://arxiv.org/abs/2508.12495</link>
<guid>https://arxiv.org/abs/2508.12495</guid>
<content:encoded><![CDATA[
arXiv:2508.12495v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) exhibit logically inconsistent hallucinations that appear coherent yet violate reasoning principles, with recent research suggesting an inverse relationship between causal reasoning capabilities and such hallucinations. However, existing reasoning approaches in LLMs, such as Chain-of-Thought (CoT) and its graph-based variants, operate at the linguistic token level rather than modeling the underlying causal relationships between variables, lacking the ability to represent conditional independencies or satisfy causal identification assumptions. To bridge this gap, we introduce causal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning framework that trains LLMs to explicitly construct variable-level directed acyclic graph (DAG) and then perform reasoning over it. Moreover, we present a dataset comprising 25,368 samples (CausalDR), where each sample includes an input question, explicit causal DAG, graph-based reasoning trace, and validated answer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves the causal reasoning capability with the state-of-the-art 95.33% accuracy on CLADDER (surpassing human performance of 94.8% for the first time) and reduces the hallucination on HaluEval with 10% improvements. It demonstrates that explicit causal structure modeling in LLMs can effectively mitigate logical inconsistencies in LLM outputs. Code is available at https://github.com/MrLYG/CDCR-SFT.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI models capture realistic sea-ice evolution from days to decades</title>
<link>https://arxiv.org/abs/2508.14984</link>
<guid>https://arxiv.org/abs/2508.14984</guid>
<content:encoded><![CDATA[
arXiv:2508.14984v2 Announce Type: replace-cross 
Abstract: Sea ice plays an important role in stabilising the Earth system. Yet, representing its dynamics remains a major challenge for models, as the underlying processes are scale-invariant and highly anisotropic. This poses a dilemma: physics-based models that faithfully reproduce the observed dynamics are computationally costly, while efficient AI models sacrifice realism. Here, to resolve this dilemma, we introduce GenSIM, the first generative AI model to predict the evolution of the full Arctic sea-ice state at 12-hour increments. Trained for sub-daily forecasting on 20 years of sea-ice-ocean simulation data, GenSIM makes realistic predictions for 30 years, while reproducing the dynamical properties of sea ice with its leads and ridges and capturing long-term trends in the sea-ice volume. Notably, although solely driven by atmospheric reanalysis, GenSIM implicitly learns hidden signatures of multi-year ice-ocean interaction. Therefore, generative AI can extrapolate from sub-daily forecasts to decadal simulations, while retaining physical consistency.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning</title>
<link>https://arxiv.org/abs/2508.15212</link>
<guid>https://arxiv.org/abs/2508.15212</guid>
<content:encoded><![CDATA[
arXiv:2508.15212v3 Announce Type: replace-cross 
Abstract: Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulation-based inference of yeast centromeres</title>
<link>https://arxiv.org/abs/2509.00200</link>
<guid>https://arxiv.org/abs/2509.00200</guid>
<content:encoded><![CDATA[
arXiv:2509.00200v2 Announce Type: replace-cross 
Abstract: The chromatin folding and the spatial arrangement of chromosomes in the cell play a crucial role in DNA replication and genes expression. An improper chromatin folding could lead to malfunctions and, over time, diseases. For eukaryotes, centromeres are essential for proper chromosome segregation and folding. Despite extensive research using de novo sequencing of genomes and annotation analysis, centromere locations in yeasts remain difficult to infer and are still unknown in most species. Recently, genome-wide chromosome conformation capture coupled with next-generation sequencing (Hi-C) has become one of the leading methods to investigate chromosome structures. Some recent studies have used Hi-C data to give a point estimate of each centromere, but those approaches highly rely on a good pre-localization. Here, we present a novel approach that infers in a stochastic manner the locations of all centromeres in budding yeast based on both the experimental Hi-C map and simulated contact maps.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KoopMotion: Learning Almost Divergence Free Koopman Flow Fields for Motion Planning</title>
<link>https://arxiv.org/abs/2509.09074</link>
<guid>https://arxiv.org/abs/2509.09074</guid>
<content:encoded><![CDATA[
arXiv:2509.09074v2 Announce Type: replace-cross 
Abstract: In this work, we propose a novel flow field-based motion planning method that drives a robot from any initial state to a desired reference trajectory such that it converges to the trajectory's end point. Despite demonstrated efficacy in using Koopman operator theory for modeling dynamical systems, Koopman does not inherently enforce convergence to desired trajectories nor to specified goals - a requirement when learning from demonstrations (LfD). We present KoopMotion which represents motion flow fields as dynamical systems, parameterized by Koopman Operators to mimic desired trajectories, and leverages the divergence properties of the learnt flow fields to obtain smooth motion fields that converge to a desired reference trajectory when a robot is placed away from the desired trajectory, and tracks the trajectory until the end point. To demonstrate the effectiveness of our approach, we show evaluations of KoopMotion on the LASA human handwriting dataset and a 3D manipulator end-effector trajectory dataset, including spectral analysis. We also perform experiments on a physical robot, verifying KoopMotion on a miniature autonomous surface vehicle operating in a non-static fluid flow environment. Our approach is highly sample efficient in both space and time, requiring only 3\% of the LASA dataset to generate dense motion plans. Additionally, KoopMotion provides a significant improvement over baselines when comparing metrics that measure spatial and temporal dynamics modeling efficacy. Code at: \href{https://alicekl.github.io/koop-motion/}{\color{blue}{https://alicekl.github.io/koop-motion}}.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tree-OPO: Off-policy Monte Carlo Tree-Guided Advantage Optimization for Multistep Reasoning</title>
<link>https://arxiv.org/abs/2509.09284</link>
<guid>https://arxiv.org/abs/2509.09284</guid>
<content:encoded><![CDATA[
arXiv:2509.09284v2 Announce Type: replace-cross 
Abstract: Recent advances in reasoning with large language models (LLMs) have shown the effectiveness of Monte Carlo Tree Search (MCTS) for generating high-quality intermediate trajectories, particularly in math and symbolic domains. Inspired by this, we explore how MCTS-derived trajectories-traditionally used for training value or reward models-can be repurposed to improve policy optimization in preference-based reinforcement learning (RL). Specifically, we focus on Group Relative Policy Optimization (GRPO), a recent algorithm that enables preference-consistent policy learning without value networks. We reframe GRPO into a staged training paradigm, leveraging a teacher's MCTS rollouts to construct a tree-structured curriculum of prefixes. This introduces the novel challenge of computing advantages for training samples that originate from different prefixes, each with a distinct expected return. To address this, we propose Staged Advantage Estimation (SAE), a framework for computing low-variance, prefix-aware advantages by projecting rewards onto a constraint set that respects the tree's hierarchy. Our empirical results on mathematical reasoning tasks show that SAE improves final accuracy over standard GRPO. This outcome is grounded in our theoretical analysis, which confirms that SAE reduces gradient variance-a principled path to improved sample efficiency. We demonstrate this through practical SAE implementations, comparing efficient heuristics against a formal quadratic program.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spacing Test for Fused Lasso</title>
<link>https://arxiv.org/abs/2509.14229</link>
<guid>https://arxiv.org/abs/2509.14229</guid>
<content:encoded><![CDATA[
arXiv:2509.14229v3 Announce Type: replace-cross 
Abstract: Detecting changepoints in a one-dimensional signal is a classical yet fundamental problem. The fused lasso provides an elegant convex formulation that produces a stepwise estimate of the mean, but quantifying the uncertainty of the detected changepoints remains difficult. Post-selection inference (PSI) offers a principled way to compute valid $p$-values after a data-driven selection, but its application to the fused lasso has been considered computationally cumbersome, requiring the tracking of many ``hit'' and ``leave'' events along the regularization path. In this paper, we show that the one-dimensional fused lasso has a surprisingly simple geometry: each changepoint enters in a strictly one-sided fashion, and there are no leave events. This structure implies that the so-called \emph{conservative spacing test} of Tibshirani et al.\ (2016), previously regarded as an approximation, is in fact \emph{exact}. The truncation region in the selective law reduces to a single lower bound given by the next knot on the LARS path. As a result, the exact selective $p$-value takes a closed form identical to the simple spacing statistic used in the LARS/lasso setting, with no additional computation. This finding establishes one of the rare cases in which an exact PSI procedure for the generalized lasso admits a closed-form pivot. We further validate the result by simulations and real data, confirming both exact calibration and high power.
  Keywords: fused lasso; changepoint detection; post-selection inference; spacing test; monotone LASSO
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconquering Bell sampling on qudits: stabilizer learning and testing, quantum pseudorandomness bounds, and more</title>
<link>https://arxiv.org/abs/2510.06848</link>
<guid>https://arxiv.org/abs/2510.06848</guid>
<content:encoded><![CDATA[
arXiv:2510.06848v2 Announce Type: replace-cross 
Abstract: Bell sampling is a simple yet powerful tool based on measuring two copies of a quantum state in the Bell basis, and has found applications in a plethora of problems related to stabiliser states and measures of magic. However, it was not known how to generalise the procedure from qubits to $d$-level systems -- qudits -- for all dimensions $d > 2$ in a useful way. Indeed, a prior work of the authors (arXiv'24) showed that the natural extension of Bell sampling to arbitrary dimensions fails to provide meaningful information about the quantum states being measured. In this paper, we overcome the difficulties encountered in previous works and develop a useful generalisation of Bell sampling to qudits of all $d\geq 2$. At the heart of our primitive is a new unitary, based on Lagrange's four-square theorem, that maps four copies of any stabiliser state $|\mathcal{S}\rangle$ to four copies of its complex conjugate $|\mathcal{S}^\ast\rangle$ (up to some Pauli operator), which may be of independent interest. We then demonstrate the utility of our new Bell sampling technique by lifting several known results from qubits to qudits for any $d\geq 2$:
  1. Learning stabiliser states in $O(n^3)$ time with $O(n)$ samples;
  2. Solving the Hidden Stabiliser Group Problem in $\tilde{O}(n^3/\varepsilon)$ time with $\tilde{O}(n/\varepsilon)$ samples;
  3. Testing whether $|\psi\rangle$ has stabiliser size at least $d^t$ or is $\varepsilon$-far from all such states in $\tilde{O}(n^3/\varepsilon)$ time with $\tilde{O}(n/\varepsilon)$ samples;
  4. Clifford circuits with at most $n/2$ single-qudit non-Clifford gates cannot prepare pseudorandom states;
  5. Testing whether $|\psi\rangle$ has stabiliser fidelity at least $1-\varepsilon_1$ or at most $1-\varepsilon_2$ with $O(d^2/\varepsilon_2)$ samples if $\varepsilon_1 = 0$ or $O(d^2/\varepsilon_2^2)$ samples if $\varepsilon_1 = O(d^{-2})$.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v2 Announce Type: replace-cross 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-envisioning Euclid Galaxy Morphology: Identifying and Interpreting Features with Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2510.23749</link>
<guid>https://arxiv.org/abs/2510.23749</guid>
<content:encoded><![CDATA[
arXiv:2510.23749v2 Announce Type: replace-cross 
Abstract: Sparse Autoencoders (SAEs) can efficiently identify candidate monosemantic features from pretrained neural networks for galaxy morphology. We demonstrate this on Euclid Q1 images using both supervised (Zoobot) and new self-supervised (MAE) models. Our publicly released MAE achieves superhuman image reconstruction performance. While a Principal Component Analysis (PCA) on the supervised model primarily identifies features already aligned with the Galaxy Zoo decision tree, SAEs can identify interpretable features outside of this framework. SAE features also show stronger alignment than PCA with Galaxy Zoo labels. Although challenges in interpretability remain, SAEs provide a powerful engine for discovering astrophysical phenomena beyond the confines of human-defined classification.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-Adaptive PINNs with Applications to Phase Transitions</title>
<link>https://arxiv.org/abs/2510.23999</link>
<guid>https://arxiv.org/abs/2510.23999</guid>
<content:encoded><![CDATA[
arXiv:2510.23999v3 Announce Type: replace-cross 
Abstract: We propose an adaptive sampling method for the training of Physics Informed Neural Networks (PINNs) which allows for sampling based on an arbitrary problem-specific heuristic which may depend on the network and its gradients. In particular we focus our analysis on the Allen-Cahn equations, attempting to accurately resolve the characteristic interfacial regions using a PINN without any post-hoc resampling. In experiments, we show the effectiveness of these methods over residual-adaptive frameworks.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AttnCache: Accelerating Self-Attention Inference for LLM Prefill via Attention Cache</title>
<link>https://arxiv.org/abs/2510.25979</link>
<guid>https://arxiv.org/abs/2510.25979</guid>
<content:encoded><![CDATA[
arXiv:2510.25979v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely used in generative applications such as chatting, code generation, and reasoning. However, many realworld workloads such as classification, question answering, recommendation, and text embedding rely solely on the prefill stage of inference, where the model encodes input sequences without performing autoregressive decoding. In these prefill only scenarios, the self-attention computation becomes the primary performance bottleneck due to its quadratic complexity with respect to sequence length. In this paper, we observe that semantically different sentences often produce similar attention maps across layers and heads. Building on this insight, we propose AttnCache, a framework that accelerates the prefill stage of LLM inference by retrieving and reusing similar attention maps. Based on an attention map memorization database, AttnCache employs efficient caching and similarity search techniques to identify and reuse pre-cached attention maps during inference, thereby reducing the computational overhead of self-attention. Experimental results show that AttnCache achieves an average of 1.2x end-to-end and 2x attention speedup on CPU, and 1.6x end-to-end and 3x attention speedup on GPU, with negligible accuracy degradation.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>e1: Learning Adaptive Control of Reasoning Effort</title>
<link>https://arxiv.org/abs/2510.27042</link>
<guid>https://arxiv.org/abs/2510.27042</guid>
<content:encoded><![CDATA[
arXiv:2510.27042v2 Announce Type: replace-cross 
Abstract: Increasing the thinking budget of AI models can significantly improve accuracy, but not all questions warrant the same amount of reasoning. Users may prefer to allocate different amounts of reasoning effort depending on how they value output quality versus latency and cost. To leverage this tradeoff effectively, users need fine-grained control over the amount of thinking used for a particular query, but few approaches enable such control. Existing methods require users to specify the absolute number of desired tokens, but this requires knowing the difficulty of the problem beforehand to appropriately set the token budget for a query. To address these issues, we propose Adaptive Effort Control, a self-adaptive reinforcement learning method that trains models to use a user-specified fraction of tokens relative to the current average chain-of-thought length for each query. This approach eliminates dataset- and phase-specific tuning while producing better cost-accuracy tradeoff curves compared to standard methods. Users can dynamically adjust the cost-accuracy trade-off through a continuous effort parameter specified at inference time. We observe that the model automatically learns to allocate resources proportionally to the task difficulty and, across model scales ranging from 1.5B to 32B parameters, our approach enables a 2-3x reduction in chain-of-thought length while maintaining or improving performance relative to the base model used for RL training.
]]></content:encoded>
<pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning with Gramian Angular Fields for Privacy-Preserving ECG Classification on Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2511.03753</link>
<guid>https://arxiv.org/abs/2511.03753</guid>
<content:encoded><![CDATA[
<div> FL-GAF, ECG classification, federated learning, IoT healthcare, privacy-preserving

Summary:<br />
- The study presents a federated learning framework for ECG classification in IoT healthcare, using GAF images for feature extraction and privacy preservation.
- The approach enables efficient CNN-based classification while keeping sensitive data local to devices, ensuring privacy in healthcare monitoring.
- Experimental validation across heterogeneous IoT devices shows high classification accuracy of 95.18% in a multi-client setup, outperforming single-client baselines.
- The framework is deployed on a server, laptop, and Raspberry Pi 4, demonstrating feasibility in realistic IoT settings for edge-cloud integration.
- Despite the added complexity of GAF transformations, the model maintains efficient resource utilization and communication overhead, supporting secure edge deployments in smart health systems.

Summary: <div>
arXiv:2511.03753v2 Announce Type: replace 
Abstract: This study presents a federated learning (FL) framework for privacy-preserving electrocardiogram (ECG) classification in Internet of Things (IoT) healthcare environments. By transforming 1D ECG signals into 2D Gramian Angular Field (GAF) images, the proposed approach enables efficient feature extraction through Convolutional Neural Networks (CNNs) while ensuring that sensitive medical data remain local to each device. This work is among the first to experimentally validate GAF-based federated ECG classification across heterogeneous IoT devices, quantifying both performance and communication efficiency. To evaluate feasibility in realistic IoT settings, we deployed the framework across a server, a laptop, and a resource-constrained Raspberry Pi 4, reflecting edge-cloud integration in IoT ecosystems. Experimental results demonstrate that the FL-GAF model achieves a high classification accuracy of 95.18% in a multi-client setup, significantly outperforming a single-client baseline in both accuracy and training time. Despite the added computational complexity of GAF transformations, the framework maintains efficient resource utilization and communication overhead. These findings highlight the potential of lightweight, privacy-preserving AI for IoT-based healthcare monitoring, supporting scalable and secure edge deployments in smart health systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alternative Fairness and Accuracy Optimization in Criminal Justice</title>
<link>https://arxiv.org/abs/2511.04505</link>
<guid>https://arxiv.org/abs/2511.04505</guid>
<content:encoded><![CDATA[
<div> Keywords: Algorithmic fairness, criminal justice, group fairness, error loss, predictive accuracy

Summary: 
Algorithmic fairness in criminal justice is a rapidly evolving field, with unresolved concepts such as group, individual, and process fairness. The proposed modification to standard group fairness involves minimizing a weighted error loss while maintaining small differences in false negative rates across protected groups. This approach simplifies finding solutions, potentially improving predictive accuracy and highlighting the ethical consideration of error costs. Critiques including biased data, latent affirmative action, and subgroup constraints are addressed. A practical framework for deployment in public decision systems focuses on need-based decisions, transparency, accountability, and narrowly tailored definitions and solutions. This framework aims to align technical design with legitimacy and provide actionable guidance for agencies utilizing risk assessment tools.<br /><br />Summary: <div>
arXiv:2511.04505v3 Announce Type: replace 
Abstract: Algorithmic fairness has grown rapidly as a research area, yet key concepts remain unsettled, especially in criminal justice. We review group, individual, and process fairness and map the conditions under which they conflict. We then develop a simple modification to standard group fairness. Rather than exact parity across protected groups, we minimize a weighted error loss while keeping differences in false negative rates within a small tolerance. This makes solutions easier to find, can raise predictive accuracy, and surfaces the ethical choice of error costs. We situate this proposal within three classes of critique: biased and incomplete data, latent affirmative action, and the explosion of subgroup constraints. Finally, we offer a practical framework for deployment in public decision systems built on three pillars: need-based decisions, Transparency and accountability, and narrowly tailored definitions and solutions. Together, these elements link technical design to legitimacy and provide actionable guidance for agencies that use risk assessment and related tools.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMPILOT: Harnessing Transformer Models for Auto Parallelization to Shared Memory Computing Paradigms</title>
<link>https://arxiv.org/abs/2511.03866</link>
<guid>https://arxiv.org/abs/2511.03866</guid>
<content:encoded><![CDATA[
<div> transforming code, large language models, code translation, OpenMP, parallelization
<br />
Recent advancements in large language models (LLMs) have greatly improved code translation capabilities, particularly in converting C++ code to OpenMP for shared-memory parallelization. The OMPILOT encoder-decoder transformer is specifically designed for this task, utilizing custom pre-training objectives to incorporate parallel construct semantics and a combination of unsupervised and supervised learning techniques to enhance translation accuracy. Unlike previous methods focusing on loop-level transformations, OMPILOT operates at the function level to capture a broader semantic context. To evaluate the translation quality, a new metric called OMPBLEU is introduced, specifically tailored to assess the correctness and quality of OpenMP parallel constructs. Overall, the use of LLMs like OMPILOT streamlines cross-language code conversion, reduces development overhead, and accelerates legacy code migration.
<br /><br />Summary: <div>
arXiv:2511.03866v2 Announce Type: replace-cross 
Abstract: Recent advances in large language models (LLMs) have significantly accelerated progress in code translation, enabling more accurate and efficient transformation across programming languages. While originally developed for natural language processing, LLMs have shown strong capabilities in modeling programming language syntax and semantics, outperforming traditional rule-based systems in both accuracy and flexibility. These models have streamlined cross-language conversion, reduced development overhead, and accelerated legacy code migration. In this paper, we introduce OMPILOT, a novel domain-specific encoder-decoder transformer tailored for translating C++ code into OpenMP, enabling effective shared-memory parallelization. OMPILOT leverages custom pre-training objectives that incorporate the semantics of parallel constructs and combines both unsupervised and supervised learning strategies to improve code translation robustness. Unlike previous work that focused primarily on loop-level transformations, OMPILOT operates at the function level to capture a wider semantic context. To evaluate our approach, we propose OMPBLEU, a novel composite metric specifically crafted to assess the correctness and quality of OpenMP parallel constructs, addressing limitations in conventional translation metrics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Classification of Infrequent Labels by Reducing Variability in Label Distribution</title>
<link>https://arxiv.org/abs/2511.07459</link>
<guid>https://arxiv.org/abs/2511.07459</guid>
<content:encoded><![CDATA[
<div> Keywords: Extreme Classification, Siamese architecture, infrequent categories, label inconsistency, multi-intent datasets

Summary:
LEVER is a novel solution for addressing the challenges of underperforming infrequent categories in Extreme Classification tasks. It utilizes a robust Siamese-style architecture to transfer knowledge and reduce label inconsistency in One-vs-All classifiers, resulting in improved performance for infrequent categories with sparse samples. Comprehensive testing on multiple XC datasets showcases significant enhancements in handling infrequent categories, setting a new standard in the field. The paper also introduces two new multi-intent datasets, providing valuable resources for future XC research. Overall, LEVER's approach demonstrates the potential to transform the classification performance of infrequent categories and opens up new opportunities for advancing research in Extreme Classification. 

<br /><br />Summary: <div>
arXiv:2511.07459v1 Announce Type: new 
Abstract: This paper presents a novel solution, LEVER, designed to address the challenges posed by underperforming infrequent categories in Extreme Classification (XC) tasks. Infrequent categories, often characterized by sparse samples, suffer from high label inconsistency, which undermines classification performance. LEVER mitigates this problem by adopting a robust Siamese-style architecture, leveraging knowledge transfer to reduce label inconsistency and enhance the performance of One-vs-All classifiers. Comprehensive testing across multiple XC datasets reveals substantial improvements in the handling of infrequent categories, setting a new benchmark for the field. Additionally, the paper introduces two newly created multi-intent datasets, offering essential resources for future XC research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Slimmable NAM: Neural Amp Models with adjustable runtime computational cost</title>
<link>https://arxiv.org/abs/2511.07470</link>
<guid>https://arxiv.org/abs/2511.07470</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Amp Models, slimmable, computational cost, real-time demonstration, audio effect plug-in <br />
Summary: <br />
This work introduces "slimmable Neural Amp Models," which allow for easy adjustment of size and computational cost without the need for additional training. The models offer flexibility for musicians to balance accuracy and computational resources as needed. Performance comparisons with standard baselines show the method's effectiveness. Additionally, a real-time demonstration showcases the model in action within an audio effect plug-in. These slimmable models offer a practical solution for musicians seeking versatile and efficient neural amp models for their audio processing needs. <div>
arXiv:2511.07470v1 Announce Type: new 
Abstract: This work demonstrates "slimmable Neural Amp Models", whose size and computational cost can be changed without additional training and with negligible computational overhead, enabling musicians to easily trade off between the accuracy and compute of the models they are using. The method's performance is quantified against commonly-used baselines, and a real-time demonstration of the model in an audio effect plug-in is developed.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Personalized Quantum Federated Learning for Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.07471</link>
<guid>https://arxiv.org/abs/2511.07471</guid>
<content:encoded><![CDATA[
<div> personalized quantum federated learning, anomaly detection, quantum clients, quantum processing, quantum-centric personalization

Summary:<br /><br />Anomaly detection in various applications relies on context and limited anomaly-labeled data. Quantum federated learning (QFL) distributes model training among multiple quantum clients, eliminating the need for centralized quantum storage and processing. However, the heterogeneity in clients' hardware capabilities, noise levels, and data representation hinder global model training, especially with imbalanced or non-identically distributed data. To tackle this, personalized quantum federated learning (PQFL) is introduced for anomaly detection. PQFL enhances local model training at clients using parameterized quantum circuits and classical optimizers, adapting each client's model to their hardware characteristics and data representation. Experimental results demonstrate PQFL's effectiveness in improving anomaly detection accuracy, reducing false errors by up to 23% and achieving significant gains in AUROC and AUPR. The framework showcases scalability and efficacy in practical quantum federated settings.<br /><br />Summary: <div>
arXiv:2511.07471v1 Announce Type: new 
Abstract: Anomaly detection has a significant impact on applications such as video surveillance, medical diagnostics, and industrial monitoring, where anomalies frequently depend on context and anomaly-labeled data are limited. Quantum federated learning (QFL) overcomes these concerns by distributing model training among several quantum clients, consequently eliminating the requirement for centralized quantum storage and processing. However, in real-life quantum networks, clients frequently differ in terms of hardware capabilities, circuit designs, noise levels, and how classical data is encoded or preprocessed into quantum states. These differences create inherent heterogeneity across clients - not just in their data distributions, but also in their quantum processing behaviors. As a result, training a single global model becomes ineffective, especially when clients handle imbalanced or non-identically distributed (non-IID) data. To address this, we propose a new framework called personalized quantum federated learning (PQFL) for anomaly detection. PQFL enhances local model training at quantum clients using parameterized quantum circuits and classical optimizers, while introducing a quantum-centric personalization strategy that adapts each client's model to its own hardware characteristics and data representation. Extensive experiments show that PQFL significantly improves anomaly detection accuracy under diverse and realistic conditions. Compared to state-of-the-art methods, PQFL reduces false errors by up to 23%, and achieves gains of 24.2% in AUROC and 20.5% in AUPR, highlighting its effectiveness and scalability in practical quantum federated settings.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate Variational Autoencoder</title>
<link>https://arxiv.org/abs/2511.07472</link>
<guid>https://arxiv.org/abs/2511.07472</guid>
<content:encoded><![CDATA[
<div> Variational Autoencoder, Multivariate, Gaussian tractability, Full-covariance family, Reparameterization <br />
Summary: <br />
The Multivariate Variational Autoencoder (MVAE) is introduced as a variant of VAE that maintains Gaussian tractability while allowing non-diagonal posterior covariance. MVAE utilizes a global coupling matrix and per-sample diagonal scales to generate dataset-wide latent correlations and local uncertainty modulation, respectively. It offers analytic KL divergence calculations and a practical reparameterization method. In various datasets like MNIST, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently outperforms diagonal-covariance VAEs in terms of reconstruction accuracy, calibration metrics, and unsupervised structure learning. The MVAE model also displays smoother and more coherent latent factor representations with sharper details. A fully reproducible implementation with training scripts is released for wider adoption and evaluation. <br /> <div>
arXiv:2511.07472v1 Announce Type: new 
Abstract: We present the Multivariate Variational Autoencoder (MVAE), a VAE variant that preserves Gaussian tractability while lifting the diagonal posterior restriction. MVAE factorizes each posterior covariance, where a \emph{global} coupling matrix $\mathbf{C}$ induces dataset-wide latent correlations and \emph{per-sample} diagonal scales modulate local uncertainty. This yields a full-covariance family with analytic KL and an efficient reparameterization via $\mathbf{L}=\mathbf{C}\mathrm{diag}(\boldsymbol{\sigma})$. Across Larochelle-style MNIST variants, Fashion-MNIST, CIFAR-10, and CIFAR-100, MVAE consistently matches or improves reconstruction (MSE~$\downarrow$) and delivers robust gains in calibration (NLL/Brier/ECE~$\downarrow$) and unsupervised structure (NMI/ARI~$\uparrow$) relative to diagonal-covariance VAEs with matched capacity, especially at mid-range latent sizes. Latent-plane visualizations further indicate smoother, more coherent factor traversals and sharper local detail. We release a fully reproducible implementation with training/evaluation scripts and sweep utilities to facilitate fair comparison and reuse.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RELEAP: Reinforcement-Enhanced Label-Efficient Active Phenotyping for Electronic Health Records</title>
<link>https://arxiv.org/abs/2511.07473</link>
<guid>https://arxiv.org/abs/2511.07473</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, active learning, electronic health record phenotyping, risk prediction, noisy-label baselines <br />
Summary: The study introduces RELEAP, a reinforcement learning-based active learning framework for electronic health record (EHR) phenotyping. It aims to refine phenotypes for better risk prediction by using downstream prediction performance as feedback. RELEAP outperformed noisy-label baselines and single-strategy active learning methods in predicting incident lung cancer risk in a Duke University Health System cohort. It increased logistic AUC from 0.774 to 0.805 and survival C-index from 0.718 to 0.752, showing smoother and more stable gains. By linking phenotype refinement to prediction outcomes, RELEAP learns which samples improve downstream discrimination and calibration, offering a more principled approach to active learning. This optimized method reduces manual chart review, enhances the reliability of EHR-based risk prediction, and provides a scalable, label-efficient paradigm for improving healthcare outcomes. <br /><br /> <div>
arXiv:2511.07473v1 Announce Type: new 
Abstract: Objective: Electronic health record (EHR) phenotyping often relies on noisy proxy labels, which undermine the reliability of downstream risk prediction. Active learning can reduce annotation costs, but most rely on fixed heuristics and do not ensure that phenotype refinement improves prediction performance. Our goal was to develop a framework that directly uses downstream prediction performance as feedback to guide phenotype correction and sample selection under constrained labeling budgets.
  Materials and Methods: We propose Reinforcement-Enhanced Label-Efficient Active Phenotyping (RELEAP), a reinforcement learning-based active learning framework. RELEAP adaptively integrates multiple querying strategies and, unlike prior methods, updates its policy based on feedback from downstream models. We evaluated RELEAP on a de-identified Duke University Health System (DUHS) cohort (2014-2024) for incident lung cancer risk prediction, using logistic regression and penalized Cox survival models. Performance was benchmarked against noisy-label baselines and single-strategy active learning.
  Results: RELEAP consistently outperformed all baselines. Logistic AUC increased from 0.774 to 0.805 and survival C-index from 0.718 to 0.752. Using downstream performance as feedback, RELEAP produced smoother and more stable gains than heuristic methods under the same labeling budget.
  Discussion: By linking phenotype refinement to prediction outcomes, RELEAP learns which samples most improve downstream discrimination and calibration, offering a more principled alternative to fixed active learning rules.
  Conclusion: RELEAP optimizes phenotype correction through downstream feedback, offering a scalable, label-efficient paradigm that reduces manual chart review and enhances the reliability of EHR-based risk prediction.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Reconstruction Attacks on Pretrained Versus Full Fine-tuned Large Language Model Embeddings on Homo Sapiens Splice Sites Genomic Data</title>
<link>https://arxiv.org/abs/2511.07481</link>
<guid>https://arxiv.org/abs/2511.07481</guid>
<content:encoded><![CDATA[
<div> Keywords: embedding reconstruction, language models, genomic sequences, fine-tuning, privacy protection

Summary: 
This study explores embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, focusing on the impact of fine-tuning on vulnerability. The research builds upon previous work by analyzing pretrained and fine-tuned model embeddings using the HS3D genomic dataset. Specialized tokenization mechanisms for DNA sequences are implemented to enhance model processing capabilities. The study reveals that fine-tuning strengthens resistance to reconstruction attacks in various LLM architectures, such as XLNet, GPT-2, and BERT. Task-specific optimization is identified as a potential privacy enhancement mechanism for protecting sensitive genomic data. The results emphasize the importance of implementing advanced protective measures for language models handling genomic information and suggest that fine-tuning could be a valuable technique for enhancing privacy. 

<br /><br />Summary: <div>
arXiv:2511.07481v1 Announce Type: new 
Abstract: This study investigates embedding reconstruction attacks in large language models (LLMs) applied to genomic sequences, with a specific focus on how fine-tuning affects vulnerability to these attacks. Building upon Pan et al.'s seminal work demonstrating that embeddings from pretrained language models can leak sensitive information, we conduct a comprehensive analysis using the HS3D genomic dataset to determine whether task-specific optimization strengthens or weakens privacy protections. Our research extends Pan et al.'s work in three significant dimensions. First, we apply their reconstruction attack pipeline to pretrained and fine-tuned model embeddings, addressing a critical gap in their methodology that did not specify embedding types. Second, we implement specialized tokenization mechanisms tailored specifically for DNA sequences, enhancing the model's ability to process genomic data, as these models are pretrained on natural language and not DNA. Third, we perform a detailed comparative analysis examining position-specific, nucleotide-type, and privacy changes between pretrained and fine-tuned embeddings. We assess embeddings vulnerabilities across different types and dimensions, providing deeper insights into how task adaptation shifts privacy risks throughout genomic sequences. Our findings show a clear distinction in reconstruction vulnerability between pretrained and fine-tuned embeddings. Notably, fine-tuning strengthens resistance to reconstruction attacks in multiple architectures -- XLNet (+19.8\%), GPT-2 (+9.8\%), and BERT (+7.8\%) -- pointing to task-specific optimization as a potential privacy enhancement mechanism. These results highlight the need for advanced protective mechanisms for language models processing sensitive genomic data, while highlighting fine-tuning as a potential privacy-enhancing technique worth further exploration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alignment-Constrained Dynamic Pruning for LLMs: Identifying and Preserving Alignment-Critical Circuits</title>
<link>https://arxiv.org/abs/2511.07482</link>
<guid>https://arxiv.org/abs/2511.07482</guid>
<content:encoded><![CDATA[
<div> dynamic pruning, large language models, alignment degradation, safety-critical circuits, inference efficiency

Summary:
Alignment-Aware Probe Pruning (AAPP) addresses the challenges faced by Large Language Models in efficient deployment. By building upon Probe Pruning, AAPP dynamically prunes circuits during inference to preserve alignment-relevant circuits, improving refusal rates by 50% at matched compute. This method enhances safety-critical circuit preservation by adaptively selecting circuits based on alignment, mitigating alignment degradation issues caused by dynamic pruning. Experiments conducted on various large language models demonstrate the effectiveness of AAPP in enabling efficient yet safety-preserving deployment of LLMs. AAPP offers a solution to the deployment challenges posed by the computational resources required for inference in Large Language Models, ensuring alignment vulnerabilities are addressed effectively. <br /><br />Summary: <div>
arXiv:2511.07482v1 Announce Type: new 
Abstract: Large Language Models require substantial computational resources for inference, posing deployment challenges. While dynamic pruning offers superior efficiency over static methods through adaptive circuit selection, it exacerbates alignment degradation by retaining only input-dependent safety-critical circuit preservation across diverse inputs. As a result, addressing these heightened alignment vulnerabilities remains critical. We introduce Alignment-Aware Probe Pruning (AAPP), a dynamic structured pruning method that adaptively preserves alignment-relevant circuits during inference, building upon Probe Pruning. Experiments on LLaMA 2-7B, Qwen2.5-14B-Instruct, and Gemma-3-12B-IT show AAPP improves refusal rates by 50\% at matched compute, enabling efficient yet safety-preserving LLM deployment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Forecasting of Human Behavior using Generative AI and Causal Graphs</title>
<link>https://arxiv.org/abs/2511.07484</link>
<guid>https://arxiv.org/abs/2511.07484</guid>
<content:encoded><![CDATA[
<div> framework, counterfactual user behavior forecasting, structural causal models, transformer-based generative artificial intelligence, causal graphs

Summary:
The study introduces a novel framework for counterfactual user behavior forecasting. It combines structural causal models with transformer-based generative AI to simulate fictitious scenarios. By creating causal graphs, the method maps the connections between user interactions, adoption metrics, and product features. Using generative models conditioned on causal variables, the framework generates realistic behavioral trajectories under counterfactual conditions. Tested on various datasets, including web interactions and e-commerce, the methodology surpasses traditional forecasting and uplift modeling techniques. It enables product teams to simulate and evaluate interventions before implementation. The framework's enhanced interpretability through causal path visualization further aids in decision-making. <div>
arXiv:2511.07484v1 Announce Type: new 
Abstract: This study presents a novel framework for counterfactual user behavior forecasting that combines structural causal models with transformer-based generative artificial intelligence. To model fictitious situations, the method creates causal graphs that map the connections between user interactions, adoption metrics, and product features. The framework generates realistic behavioral trajectories under counterfactual conditions by using generative models that are conditioned on causal variables. Tested on datasets from web interactions, mobile applications, and e-commerce, the methodology outperforms conventional forecasting and uplift modeling techniques. Product teams can effectively simulate and assess possible interventions prior to deployment thanks to the framework improved interpretability through causal path visualization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Are Learning Biases Equivalent? A Unifying Framework for Fairness, Robustness, and Distribution Shift</title>
<link>https://arxiv.org/abs/2511.07485</link>
<guid>https://arxiv.org/abs/2511.07485</guid>
<content:encoded><![CDATA[
<div> spurious correlations, subpopulation shift, class imbalance, fairness violations, information-theoretic measures
Summary:
This paper introduces a unified theoretical framework to understand the various failure modes of machine learning systems, such as unfairness, brittleness, and poor performance. By formalizing biases as violations of conditional independence using information-theoretic measures, the study establishes equivalence conditions between spurious correlations, subpopulation shift, class imbalance, and fairness violations. The theory predicts that a spurious correlation of strength $\alpha$ leads to equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+\alpha)/(1-\alpha)$. Empirical validation across multiple datasets and architectures confirms the predicted equivalences within a 3% accuracy margin. This research paves the way for the systematic transfer of debiasing techniques across different problem domains, bridging the gap between fairness, robustness, and distribution shifts in machine learning. 
<br /><br />Summary: <div>
arXiv:2511.07485v1 Announce Type: new 
Abstract: Machine learning systems exhibit diverse failure modes: unfairness toward protected groups, brittleness to spurious correlations, poor performance on minority sub-populations, which are typically studied in isolation by distinct research communities. We propose a unifying theoretical framework that characterizes when different bias mechanisms produce quantitatively equivalent effects on model performance. By formalizing biases as violations of conditional independence through information-theoretic measures, we prove formal equivalence conditions relating spurious correlations, subpopulation shift, class imbalance, and fairness violations. Our theory predicts that a spurious correlation of strength $\alpha$ produces equivalent worst-group accuracy degradation as a sub-population imbalance ratio $r \approx (1+\alpha)/(1-\alpha)$ under feature overlap assumptions. Empirical validation in six datasets and three architectures confirms that predicted equivalences hold within the accuracy of the worst group 3\%, enabling the principled transfer of debiasing methods across problem domains. This work bridges the literature on fairness, robustness, and distribution shifts under a common perspective.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Efficient Sample Complexity for Robust CMDP</title>
<link>https://arxiv.org/abs/2511.07486</link>
<guid>https://arxiv.org/abs/2511.07486</guid>
<content:encoded><![CDATA[
<div> Robust constrained Markov decision processes (RCMDPs), policy optimization, sample complexity guarantees, augmented state space, Robust constrained Value iteration (RCVI) algorithm<br />
<br />
Summary:
This study addresses learning policies that maximize cumulative reward while meeting safety constraints in environments where the real dynamics may differ from the nominal model. The authors introduce the concept of Robust constrained Markov decision processes (RCMDPs) which require maximizing reward under the worst-case dynamics within an uncertainty set. The research demonstrates that Markovian policies may not be optimal under rectangular uncertainty sets. To combat this, an augmented state space that includes the remaining utility budget is introduced. The novel Robust constrained Value iteration (RCVI) algorithm is proposed with a sample complexity guarantee of $\mathcal{\tilde{O}}(|S||A|H^5/\epsilon^2)$ achieving at most $\epsilon$ violation. This is the first sample complexity guarantee for RCMDPs. Empirical results support the effectiveness of the approach. <div>
arXiv:2511.07486v1 Announce Type: new 
Abstract: We study the problem of learning policies that maximize cumulative reward while satisfying safety constraints, even when the real environment differs from a simulator or nominal model. We focus on robust constrained Markov decision processes (RCMDPs), where the agent must maximize reward while ensuring cumulative utility exceeds a threshold under the worst-case dynamics within an uncertainty set. While recent works have established finite-time iteration complexity guarantees for RCMDPs using policy optimization, their sample complexity guarantees remain largely unexplored. In this paper, we first show that Markovian policies may fail to be optimal even under rectangular uncertainty sets unlike the {\em unconstrained} robust MDP. To address this, we introduce an augmented state space that incorporates the remaining utility budget into the state representation. Building on this formulation, we propose a novel Robust constrained Value iteration (RCVI) algorithm with a sample complexity of $\mathcal{\tilde{O}}(|S||A|H^5/\epsilon^2)$ achieving at most $\epsilon$ violation using a generative model where $|S|$ and $|A|$ denote the sizes of the state and action spaces, respectively, and $H$ is the episode length. To the best of our knowledge, this is the {\em first sample complexity guarantee} for RCMDP. Empirical results further validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Methodological Precedence in Health Tech: Why ML/Big Data Analysis Must Follow Basic Epidemiological Consistency. A Case Study</title>
<link>https://arxiv.org/abs/2511.07500</link>
<guid>https://arxiv.org/abs/2511.07500</guid>
<content:encoded><![CDATA[
<div> Keywords: advanced analytical tools, Machine Learning, Big Data, cohort study, selection bias

Summary:
The article discusses the importance of ensuring basic methodological coherence in health research before implementing advanced analytical tools such as Machine Learning (ML) and Big Data analysis. It presents a case study highlighting the significance of verifying fundamental design choices to avoid misleading or contradictory results. By applying standard statistical methods and national benchmarks to a vaccine outcomes and psychiatric events cohort study, the authors identify severe paradoxes and mathematical artifacts due to uncorrected selection bias. The study emphasizes the necessity of utilizing robust methods like Propensity Score Matching for valid causal inference in complex health research involving administrative data. The findings underscore the critical role of foundational epidemiological consistency before employing advanced statistical modeling approaches.<br /><br />Summary: <div>
arXiv:2511.07500v1 Announce Type: new 
Abstract: The integration of advanced analytical tools, including Machine Learning (ML) and massive data processing, has revolutionized health research, promising unprecedented accuracy in diagnosis and risk prediction. However, the rigor of these complex methods is fundamentally dependent on the quality and integrity of the underlying datasets and the validity of their statistical design. We propose an emblematic case where advanced analysis (ML/Big Data) must necessarily be subsequent to the verification of basic methodological coherence. This study highlights a crucial cautionary principle: sophisticated analyses amplify, rather than correct, severe methodological flaws rooted in basic design choices, leading to misleading or contradictory findings. By applying simple, standard descriptive statistical methods and established national epidemiological benchmarks to a recently published cohort study on vaccine outcomes and psychiatric events, we expose multiple, statistically irreconcilable paradoxes. These paradoxes, including an implausible risk reduction for a chronic disorder in a high-risk group and contradictory incidence rate comparisons, definitively invalidate the reported hazard ratios (HRs). We demonstrate that the observed effects are mathematical artifacts stemming from an uncorrected selection bias in the cohort construction. This analysis serves as a robust reminder that even the most complex health studies must first pass the test of basic epidemiological consistency before any conclusion drawn from subsequent advanced ML or statistical modeling can be considered valid or publishable. We conclude that robust methods, such as Propensity Score Matching, are essential for achieving valid causal inference from administrative data in the absence of randomization
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>N-ReLU: Zero-Mean Stochastic Extension of ReLU</title>
<link>https://arxiv.org/abs/2511.07559</link>
<guid>https://arxiv.org/abs/2511.07559</guid>
<content:encoded><![CDATA[
<div> Keywords: Activation functions, Neural networks, ReLU, N-ReLU, Gaussian noise<br />
Summary:<br />
Activation functions play a crucial role in enabling nonlinear representations in deep neural networks. The standard ReLU activation function often faces the issue of dead or inactive neurons due to its hard zero cutoff. To address this problem, N-ReLU is introduced as a zero-mean stochastic extension of ReLU. By replacing negative activations with Gaussian noise while maintaining the same expected output, N-ReLU ensures gradient flow in inactive regions and acts as a regularizer during training. Experimental results on the MNIST dataset using various neural network architectures demonstrate that N-ReLU achieves comparable accuracy to other popular activation functions like LeakyReLU and GELU at moderate noise levels. This approach effectively enhances optimization robustness without requiring modifications to network structures or additional parameters.<br /> <div>
arXiv:2511.07559v1 Announce Type: new 
Abstract: Activation functions are fundamental for enabling nonlinear representations in deep neural networks. However, the standard rectified linear unit (ReLU) often suffers from inactive or "dead" neurons caused by its hard zero cutoff. To address this issue, we introduce N-ReLU (Noise-ReLU), a zero-mean stochastic extension of ReLU that replaces negative activations with Gaussian noise while preserving the same expected output. This expectation-aligned formulation maintains gradient flow in inactive regions and acts as an annealing-style regularizer during training. Experiments on the MNIST dataset using both multilayer perceptron (MLP) and convolutional neural network (CNN) architectures show that N-ReLU achieves accuracy comparable to or slightly exceeding that of ReLU, LeakyReLU, PReLU, GELU, and RReLU at moderate noise levels (sigma = 0.05-0.10), with stable convergence and no dead neurons observed. These results demonstrate that lightweight Gaussian noise injection offers a simple yet effective mechanism to enhance optimization robustness without modifying network structures or introducing additional parameters.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCALAR: Benchmarking SAE Interaction Sparsity in Toy LLMs</title>
<link>https://arxiv.org/abs/2511.07572</link>
<guid>https://arxiv.org/abs/2511.07572</guid>
<content:encoded><![CDATA[
<div> Sparse Autoencoders, Neural Networks, Interpretability, Sparse Connectivity, Benchmark
Summary: 
SCALAR benchmark assesses interaction sparsity in Sparse Autoencoders (SAEs) by comparing TopK SAEs, Jacobian SAEs (JSAEs), and Staircase SAEs. Staircase SAEs, utilizing weight-sharing to limit upstream feature duplication across downstream features, show a significant improvement in interaction sparsity compared to TopK SAEs, with a relative improvement of 59.67% ± 1.83% for feedforward layers and 63.15% ± 1.35% for transformer blocks. JSAEs offer a modest improvement over TopK SAEs for feedforward layers but struggle to train effectively across transformer blocks. Staircase and TopK SAEs perform well throughout the residual stream. Validation on a toy model and GPT-2 Small demonstrates that Staircase SAEs maintain interaction sparsity improvements while preserving feature interpretability. This study underscores the importance of interaction sparsity in SAEs and provides insights into designing more effective and interpretable neural network architectures. 
<br /><br />Summary: <div>
arXiv:2511.07572v1 Announce Type: new 
Abstract: Mechanistic interpretability aims to decompose neural networks into interpretable features and map their connecting circuits. The standard approach trains sparse autoencoders (SAEs) on each layer's activations. However, SAEs trained in isolation don't encourage sparse cross-layer connections, inflating extracted circuits where upstream features needlessly affect multiple downstream features. Current evaluations focus on individual SAE performance, leaving interaction sparsity unexamined. We introduce SCALAR (Sparse Connectivity Assessment of Latent Activation Relationships), a benchmark measuring interaction sparsity between SAE features. We also propose "Staircase SAEs", using weight-sharing to limit upstream feature duplication across downstream features. Using SCALAR, we compare TopK SAEs, Jacobian SAEs (JSAEs), and Staircase SAEs. Staircase SAEs improve relative sparsity over TopK SAEs by $59.67\% \pm 1.83\%$ (feedforward) and $63.15\% \pm 1.35\%$ (transformer blocks). JSAEs provide $8.54\% \pm 0.38\%$ improvement over TopK for feedforward layers but cannot train effectively across transformer blocks, unlike Staircase and TopK SAEs which work anywhere in the residual stream. We validate on a $216$K-parameter toy model and GPT-$2$ Small ($124$M), where Staircase SAEs maintain interaction sparsity improvements while preserving feature interpretability. Our work highlights the importance of interaction sparsity in SAEs through benchmarking and comparing promising architectures.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Output Drift: Cross-Provider Validation &amp; Mitigation for Financial Workflows</title>
<link>https://arxiv.org/abs/2511.07585</link>
<guid>https://arxiv.org/abs/2511.07585</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, financial tasks, output consistency, deterministic testing, regulatory compliance

Summary:
Financial institutions use Large Language Models (LLMs) for tasks like reconciliations and regulatory reporting, but output drift can impact trust and auditability. This study compares five LLM architectures on financial tasks and shows that smaller models achieve better output consistency than larger ones. To address this issue, the researchers developed a finance-calibrated deterministic test harness and task-specific invariant checking methods. They also propose a three-tier model classification system and an audit-ready attestation system for validation. The evaluation across different models and tasks revealed varying levels of output consistency and sensitivity to drift. The study demonstrates the importance of deterministic behavior in AI deployments for financial tasks and provides a framework aligned with regulatory requirements. This research challenges the notion that larger models are always better for production deployment in the financial sector. 

<br /><br />Summary: <div>
arXiv:2511.07585v1 Announce Type: new 
Abstract: Financial institutions deploy Large Language Models (LLMs) for reconciliations, regulatory reporting, and client communications, but nondeterministic outputs (output drift) undermine auditability and trust. We quantify drift across five model architectures (7B-120B parameters) on regulated financial tasks, revealing a stark inverse relationship: smaller models (Granite-3-8B, Qwen2.5-7B) achieve 100% output consistency at T=0.0, while GPT-OSS-120B exhibits only 12.5% consistency (95% CI: 3.5-36.0%) regardless of configuration (p<0.0001, Fisher's exact test). This finding challenges conventional assumptions that larger models are universally superior for production deployment.
  Our contributions include: (i) a finance-calibrated deterministic test harness combining greedy decoding (T=0.0), fixed seeds, and SEC 10-K structure-aware retrieval ordering; (ii) task-specific invariant checking for RAG, JSON, and SQL outputs using finance-calibrated materiality thresholds (plus or minus 5%) and SEC citation validation; (iii) a three-tier model classification system enabling risk-appropriate deployment decisions; and (iv) an audit-ready attestation system with dual-provider validation.
  We evaluated five models (Qwen2.5-7B via Ollama, Granite-3-8B via IBM watsonx.ai, Llama-3.3-70B, Mistral-Medium-2505, and GPT-OSS-120B) across three regulated financial tasks. Across 480 runs (n=16 per condition), structured tasks (SQL) remain stable even at T=0.2, while RAG tasks show drift (25-75%), revealing task-dependent sensitivity. Cross-provider validation confirms deterministic behavior transfers between local and cloud deployments. We map our framework to Financial Stability Board (FSB), Bank for International Settlements (BIS), and Commodity Futures Trading Commission (CFTC) requirements, demonstrating practical pathways for compliance-ready AI deployments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Router to Route Them All: Homogeneous Expert Routing for Heterogeneous Graph Transformers</title>
<link>https://arxiv.org/abs/2511.07603</link>
<guid>https://arxiv.org/abs/2511.07603</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Heterogeneous Graphs, Mixture-of-Experts, Expert Routing, Type-Agnostic Specialization

Summary:
Homogeneous Expert Routing (HER) aims to address the issue of overreliance on surface-level labels in Heterogeneous Graph Neural Networks (HGNNs) by integrating Mixture-of-Experts (MoE) into Heterogeneous Graph Transformers (HGT). The proposed HER layer stochastically masks type embeddings during routing to promote type-agnostic specialization, challenging the necessity of type-specific experts. Evaluation on IMDB, ACM, and DBLP datasets for link prediction shows that HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB reveals that HER experts specialize based on semantic patterns, such as movie genres, rather than node types, indicating that the routing is driven by latent semantics. This work demonstrates that enforcing regularization of type dependence in expert routing results in more generalizable, efficient, and interpretable representations, introducing a new design principle for heterogeneous graph learning. 

<br /><br />Summary: <div>
arXiv:2511.07603v1 Announce Type: new 
Abstract: A common practice in heterogeneous graph neural networks (HGNNs) is to condition parameters on node/edge types, assuming types reflect semantic roles. However, this can cause overreliance on surface-level labels and impede cross-type knowledge transfer. We explore integrating Mixture-of-Experts (MoE) into HGNNs--a direction underexplored despite MoE's success in homogeneous settings. Crucially, we question the need for type-specific experts. We propose Homogeneous Expert Routing (HER), an MoE layer for Heterogeneous Graph Transformers (HGT) that stochastically masks type embeddings during routing to encourage type-agnostic specialization. Evaluated on IMDB, ACM, and DBLP for link prediction, HER consistently outperforms standard HGT and a type-separated MoE baseline. Analysis on IMDB shows HER experts specialize by semantic patterns (e.g., movie genres) rather than node types, confirming routing is driven by latent semantics. Our work demonstrates that regularizing type dependence in expert routing yields more generalizable, efficient, and interpretable representations--a new design principle for heterogeneous graph learning.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial Action Replacement: Tackling Distribution Shift in Offline MARL</title>
<link>https://arxiv.org/abs/2511.07629</link>
<guid>https://arxiv.org/abs/2511.07629</guid>
<content:encoded><![CDATA[
<div> factorized behavior policy, partial action replacement, offline multi-agent reinforcement learning, distribution shift, Soft-Partial Conservative Q-Learning 

Summary:<br />
This study addresses the challenge of evaluating out-of-distribution joint actions in offline multi-agent reinforcement learning (MARL). The researchers propose a strategy called partial action replacement (PAR) that updates only a single or partial agents' actions while keeping others fixed to the behavioral data. This approach reduces distribution shift compared to full joint-action updates. They develop Soft-Partial Conservative Q-Learning (SPaCQL) using PAR to mitigate out-of-distribution issues and dynamically adjust PAR strategies based on value estimation uncertainty. Theoretical analysis shows that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents instead of exponentially with the joint-action space. SPaCQL adapts to distribution shift using uncertainty-informed weights, leading to tighter value error bounds in offline MARL problems. Empirical results demonstrate SPaCQL's effectiveness in policy learning, especially when the offline dataset exhibits independence structure. <div>
arXiv:2511.07629v1 Announce Type: new 
Abstract: Offline multi-agent reinforcement learning (MARL) is severely hampered by the challenge of evaluating out-of-distribution (OOD) joint actions. Our core finding is that when the behavior policy is factorized - a common scenario where agents act fully or partially independently during data collection - a strategy of partial action replacement (PAR) can significantly mitigate this challenge. PAR updates a single or part of agents' actions while the others remain fixed to the behavioral data, reducing distribution shift compared to full joint-action updates. Based on this insight, we develop Soft-Partial Conservative Q-Learning (SPaCQL), using PAR to mitigate OOD issue and dynamically weighting different PAR strategies based on the uncertainty of value estimation. We provide a rigorous theoretical foundation for this approach, proving that under factorized behavior policies, the induced distribution shift scales linearly with the number of deviating agents rather than exponentially with the joint-action space. This yields a provably tighter value error bound for this important class of offline MARL problems. Our theoretical results also indicate that SPaCQL adaptively addresses distribution shift using uncertainty-informed weights. Our empirical results demonstrate SPaCQL enables more effective policy learning, and manifest its remarkable superiority over baseline algorithms when the offline dataset exhibits the independence structure.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowTIE: Flow-based Transport of Intensity Equation for Phase Gradient Estimation from 4D-STEM Data</title>
<link>https://arxiv.org/abs/2511.07633</link>
<guid>https://arxiv.org/abs/2511.07633</guid>
<content:encoded><![CDATA[
<div> FlowTIE, neural network, phase reconstruction, 4D-Scanning Transmission Electron Microscopy, STEM<br />
Summary:<br />
FlowTIE is a new framework for phase reconstruction from 4D-Scanning Transmission Electron Microscopy data, combining the Transport of Intensity Equation (TIE) with a flow-based representation of the phase gradient. This integration of data-driven learning with physics-based priors enhances robustness under dynamical scattering conditions for thick specimens. The model was validated using simulated datasets of crystalline materials, showing improved phase reconstruction accuracy compared to classical TIE and gradient-based optimization methods. FlowTIE is not only fast but also compatible with a thick specimen model, such as the multislice method, making it a versatile tool for phase reconstruction in electron microscopy applications.<br /> 
Summary: <div>
arXiv:2511.07633v1 Announce Type: new 
Abstract: We introduce FlowTIE, a neural-network-based framework for phase reconstruction from 4D-Scanning Transmission Electron Microscopy (STEM) data, which integrates the Transport of Intensity Equation (TIE) with a flow-based representation of the phase gradient. This formulation allows the model to bridge data-driven learning with physics-based priors, improving robustness under dynamical scattering conditions for thick specimen. The validation on simulated datasets of crystalline materials, benchmarking to classical TIE and gradient-based optimization methods are presented. The results demonstrate that FlowTIE improves phase reconstruction accuracy, fast, and can be integrated with a thick specimen model, namely multislice method.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private-RAG: Answering Multiple Queries with LLMs while Keeping Your Data Private</title>
<link>https://arxiv.org/abs/2511.07637</link>
<guid>https://arxiv.org/abs/2511.07637</guid>
<content:encoded><![CDATA[
<div> Retrieval-augmented generation, large language models, sensitive information, differential privacy, multi-query setting <br />
Summary:<br />
This paper introduces two differential privacy (DP) algorithms, MURAG and MURAG-ADA, for retrieval-augmented generation (RAG) systems in multi-query settings. MURAG utilizes an individual privacy filter to limit privacy loss based on document retrieval frequency, while MURAG-ADA enhances utility by privately releasing query-specific thresholds for document selection. Experiments show that these methods maintain meaningful utility while scaling to hundreds of queries within a practical DP budget of around ε≈10. The research addresses the privacy concerns of RAG systems when dealing with sensitive information, providing a practical solution for protecting private data while maintaining system performance. The proposed algorithms improve upon single-query DP guarantees, making them more applicable in real-world scenarios where multiple queries are commonplace. <div>
arXiv:2511.07637v1 Announce Type: new 
Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving documents from an external corpus at inference time. When this corpus contains sensitive information, however, unprotected RAG systems are at risk of leaking private information. Prior work has introduced differential privacy (DP) guarantees for RAG, but only in single-query settings, which fall short of realistic usage. In this paper, we study the more practical multi-query setting and propose two DP-RAG algorithms. The first, MURAG, leverages an individual privacy filter so that the accumulated privacy loss only depends on how frequently each document is retrieved rather than the total number of queries. The second, MURAG-ADA, further improves utility by privately releasing query-specific thresholds, enabling more precise selection of relevant documents. Our experiments across multiple LLMs and datasets demonstrate that the proposed methods scale to hundreds of queries within a practical DP budget ($\varepsilon\approx10$), while preserving meaningful utility.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Graph Learning with Transformer for Multi-Reservoir Inflow Prediction</title>
<link>https://arxiv.org/abs/2511.07649</link>
<guid>https://arxiv.org/abs/2511.07649</guid>
<content:encoded><![CDATA[
<div> Keywords: reservoir inflow prediction, adaptive graph learning, spatial dependencies, time-varying, attention mechanisms 

Summary: 
AdaTrip is a new framework for multi-reservoir inflow forecasting that incorporates adaptive, time-varying graph learning techniques. It considers spatial dependencies among interconnected reservoirs by constructing dynamic graphs with reservoirs as nodes and directed edges representing hydrological connections. Utilizing attention mechanisms, AdaTrip can automatically identify important spatial and temporal dependencies, leading to improved performance compared to existing methods. The framework is particularly effective for reservoirs with limited historical data, thanks to parameter sharing. Furthermore, AdaTrip provides interpretable attention maps at both edge and time-step levels, offering valuable insights for operational decision-making in water resource management. The code for AdaTrip is publicly available for further exploration and implementation. 

<br /><br />Summary: <div>
arXiv:2511.07649v1 Announce Type: new 
Abstract: Reservoir inflow prediction is crucial for water resource management, yet existing approaches mainly focus on single-reservoir models that ignore spatial dependencies among interconnected reservoirs. We introduce AdaTrip as an adaptive, time-varying graph learning framework for multi-reservoir inflow forecasting. AdaTrip constructs dynamic graphs where reservoirs are nodes with directed edges reflecting hydrological connections, employing attention mechanisms to automatically identify crucial spatial and temporal dependencies. Evaluation on thirty reservoirs in the Upper Colorado River Basin demonstrates superiority over existing baselines, with improved performance for reservoirs with limited records through parameter sharing. Additionally, AdaTrip provides interpretable attention maps at edge and time-step levels, offering insights into hydrological controls to support operational decision-making. Our code is available at https://github.com/humphreyhuu/AdaTrip.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Binary Encoded Crime Linkage Analysis Using Siamese Network</title>
<link>https://arxiv.org/abs/2511.07651</link>
<guid>https://arxiv.org/abs/2511.07651</guid>
<content:encoded><![CDATA[
<div> Keywords: crime linkage analysis, Siamese Autoencoder, high-dimensional data, sparse data, heterogeneous data

Summary: 
The article introduces a new approach for crime linkage analysis using a Siamese Autoencoder framework to handle the challenges posed by high-dimensional, sparse, and heterogeneous crime data. By integrating geographic-temporal features at the decoder stage, the framework mitigates signal dilution in sparse feature spaces and enhances the learning of meaningful latent representations. The study uses data from the Violent Crime Linkage Analysis System (ViCLAS) to demonstrate how the approach outperforms traditional methods in crime linkage accuracy. The analysis also explores the impact of domain-informed data reduction strategies on model performance, offering practical recommendations for data preprocessing in crime linkage contexts. The results show that advanced machine learning techniques can significantly improve linkage accuracy, with an increase in AUC by up to 9% over traditional methods, while providing interpretable insights to support investigative decision-making. 

<br /><br />Summary: <div>
arXiv:2511.07651v1 Announce Type: new 
Abstract: Effective crime linkage analysis is crucial for identifying serial offenders and enhancing public safety. To address limitations of traditional crime linkage methods in handling high-dimensional, sparse, and heterogeneous data, we propose a Siamese Autoencoder framework that learns meaningful latent representations and uncovers correlations in complex crime data. Using data from the Violent Crime Linkage Analysis System (ViCLAS), maintained by the Serious Crime Analysis Section of the UK's National Crime Agency, our approach mitigates signal dilution in sparse feature spaces by integrating geographic-temporal features at the decoder stage. This design amplifies behavioral representations rather than allowing them to be overshadowed at the input level, yielding consistent improvements across multiple evaluation metrics. We further analyze how different domain-informed data reduction strategies influence model performance, providing practical guidance for preprocessing in crime linkage contexts. Our results show that advanced machine learning approaches can substantially enhance linkage accuracy, improving AUC by up to 9% over traditional methods while offering interpretable insights to support investigative decision-making.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAE: Character-Level Autoencoder for Non-Semantic Relational Data Grouping</title>
<link>https://arxiv.org/abs/2511.07657</link>
<guid>https://arxiv.org/abs/2511.07657</guid>
<content:encoded><![CDATA[
<div> keywords: Character-Level Autoencoder, non-semantic data, relational databases, data grouping, industrial datasets <br />
Summary: 
The paper introduces a Character-Level Autoencoder (CAE) method for identifying semantically identical columns in non-semantic relational datasets. Unlike traditional NLP models, the CAE operates at the character level with fixed dictionary constraints, enabling scalable processing of large-scale data. By encoding text representations of columns and extracting high-dimensional feature embeddings, the CAE efficiently groups data. The approach significantly reduces memory requirements and training time by maintaining a fixed dictionary size. Experimental results show that the CAE approach outperforms traditional NLP methods, achieving 80.95% accuracy in column matching tasks. This work addresses the challenges of non-semantic data in enterprise relational databases, providing an automated solution for schema understanding and data profiling in industrial datasets at scale.<br /> 
Summary: <div>
arXiv:2511.07657v1 Announce Type: new 
Abstract: Enterprise relational databases increasingly contain vast amounts of non-semantic data - IP addresses, product identifiers, encoded keys, and timestamps - that challenge traditional semantic analysis. This paper introduces a novel Character-Level Autoencoder (CAE) approach that automatically identifies and groups semantically identical columns in non-semantic relational datasets by detecting column similarities based on data patterns and structures. Unlike conventional Natural Language Processing (NLP) models that struggle with limitations in semantic interpretability and out-of-vocabulary tokens, our approach operates at the character level with fixed dictionary constraints, enabling scalable processing of large-scale data lakes and warehouses. The CAE architecture encodes text representations of non-semantic relational table columns and extracts high-dimensional feature embeddings for data grouping. By maintaining a fixed dictionary size, our method significantly reduces both memory requirements and training time, enabling efficient processing of large-scale industrial data environments. Experimental evaluation demonstrates substantial performance gains: our CAE approach achieved 80.95% accuracy in top 5 column matching tasks across relational datasets, substantially outperforming traditional NLP approaches such as Bag of Words (47.62%). These results demonstrate its effectiveness for identifying and clustering identical columns in relational datasets. This work bridges the gap between theoretical advances in character-level neural architectures and practical enterprise data management challenges, providing an automated solution for schema understanding and data profiling of non-semantic industrial datasets at scale.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ZeroSim: Zero-Shot Analog Circuit Evaluation with Unified Transformer Embeddings</title>
<link>https://arxiv.org/abs/2511.07658</link>
<guid>https://arxiv.org/abs/2511.07658</guid>
<content:encoded><![CDATA[
<div> transformer-based performance modeling framework, analog circuit design automation, zero-shot generalization, reinforcement learning, SPICE simulations<br />
<br />
Summary:
The article introduces ZeroSim, a transformer-based framework for efficient performance evaluation in learning-based analog circuit design automation. ZeroSim aims to achieve robust generalization across trained topologies and zero-shot generalization to unseen topologies without fine-tuning. Key strategies include a diverse training corpus covering over 60 amplifier topologies, unified topology embeddings for robust generalization, and a topology-conditioned parameter mapping approach to maintain consistent structural representations. Experimental results show that ZeroSim outperforms baseline models, delivering accurate zero-shot predictions across different amplifier topologies. When integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a significant speedup compared to conventional SPICE simulations, highlighting its practical value for a wide range of analog circuit design automation tasks.<br /><br />Summary: <div>
arXiv:2511.07658v1 Announce Type: new 
Abstract: Although recent advancements in learning-based analog circuit design automation have tackled tasks such as topology generation, device sizing, and layout synthesis, efficient performance evaluation remains a major bottleneck. Traditional SPICE simulations are time-consuming, while existing machine learning methods often require topology-specific retraining or manual substructure segmentation for fine-tuning, hindering scalability and adaptability. In this work, we propose ZeroSim, a transformer-based performance modeling framework designed to achieve robust in-distribution generalization across trained topologies under novel parameter configurations and zero-shot generalization to unseen topologies without any fine-tuning. We apply three key enabling strategies: (1) a diverse training corpus of 3.6 million instances covering over 60 amplifier topologies, (2) unified topology embeddings leveraging global-aware tokens and hierarchical attention to robustly generalize to novel circuits, and (3) a topology-conditioned parameter mapping approach that maintains consistent structural representations independent of parameter variations. Our experimental results demonstrate that ZeroSim significantly outperforms baseline models such as multilayer perceptrons, graph neural networks and transformers, delivering accurate zero-shot predictions across different amplifier topologies. Additionally, when integrated into a reinforcement learning-based parameter optimization pipeline, ZeroSim achieves a remarkable speedup (13x) compared to conventional SPICE simulations, underscoring its practical value for a wide range of analog circuit design automation tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilities Are All You Need: A Probability-Only Approach to Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2511.07694</link>
<guid>https://arxiv.org/abs/2511.07694</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Uncertainty Estimation, Predictive Entropy, Top-K Probabilities, Question-Answering Datasets

Summary:
This paper introduces a new method for uncertainty estimation in Large Language Models (LLMs) to mitigate hallucinations and enhance model trustworthiness. The method approximates predictive entropy using the responses' top-K probabilities, eliminating the need for multiple samples or extra computation. An adaptive mechanism is utilized to determine the optimal K value for filtering out low-confidence probabilities and improving flexibility. Experimental results on free-form question-answering datasets demonstrate the superior performance of the proposed method compared to existing expensive baselines. By efficiently estimating uncertainty, this approach contributes to addressing the issue of factually incorrect outputs in LLMs, advancing the goal of enhancing the reliability and credibility of these models.

<br /><br />Summary: <div>
arXiv:2511.07694v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong performance across various natural language processing (NLP) tasks but remain vulnerable to hallucinations, generating factually incorrect or misleading outputs. Uncertainty estimation, often using predictive entropy estimation, is key to addressing this issue. However, existing methods often require multiple samples or extra computation to assess semantic entropy. This paper proposes an efficient, training-free uncertainty estimation method that approximates predictive entropy using the responses' top-$K$ probabilities. Moreover, we employ an adaptive mechanism to determine $K$ to enhance flexibility and filter out low-confidence probabilities. Experimental results on three free-form question-answering datasets across several LLMs demonstrate that our method outperforms expensive state-of-the-art baselines, contributing to the broader goal of enhancing LLM trustworthiness.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Role of Calibration in Benchmarking Algorithmic Fairness for Skin Cancer Detection</title>
<link>https://arxiv.org/abs/2511.07700</link>
<guid>https://arxiv.org/abs/2511.07700</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence, melanoma detection, calibration, fairness metrics, subgroup biases

Summary:
Artificial Intelligence (AI) models have shown high performance in detecting melanoma but face challenges in adoption due to disparities in performance across demographic subgroups like gender, race, and age. Existing benchmarking efforts often focus on fairness metrics like AUROC, which overlooks calibration as a metric to evaluate models' accuracy. This study evaluates the performance of the top skin cancer detection algorithm of the ISIC 2020 Challenge on different datasets and compares it with other models in terms of sex, race, and age subgroups. The results indicate that while current models improve discriminative accuracy, they tend to overpredict risk and exhibit calibration issues on new datasets. The study emphasizes the need for robust model auditing strategies and comprehensive metadata collection to ensure equitable AI-driven healthcare solutions.<br /><br />Summary: <div>
arXiv:2511.07700v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) models have demonstrated expert-level performance in melanoma detection, yet their clinical adoption is hindered by performance disparities across demographic subgroups such as gender, race, and age. Previous efforts to benchmark the performance of AI models have primarily focused on assessing model performance using group fairness metrics that rely on the Area Under the Receiver Operating Characteristic curve (AUROC), which does not provide insights into a model's ability to provide accurate estimates. In line with clinical assessments, this paper addresses this gap by incorporating calibration as a complementary benchmarking metric to AUROC-based fairness metrics. Calibration evaluates the alignment between predicted probabilities and observed event rates, offering deeper insights into subgroup biases. We assess the performance of the leading skin cancer detection algorithm of the ISIC 2020 Challenge on the ISIC 2020 Challenge dataset and the PROVE-AI dataset, and compare it with the second and third place models, focusing on subgroups defined by sex, race (Fitzpatrick Skin Tone), and age. Our findings reveal that while existing models enhance discriminative accuracy, they often over-diagnose risk and exhibit calibration issues when applied to new datasets. This study underscores the necessity for comprehensive model auditing strategies and extensive metadata collection to achieve equitable AI-driven healthcare solutions. All code is publicly available at https://github.com/bdominique/testing_strong_calibration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Guided Adversarial State Perturbations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07701</link>
<guid>https://arxiv.org/abs/2511.07701</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, adversarial attacks, vision-based environments, state perturbations, defenses<br />
<br />Summary:
The article discusses the vulnerability of reinforcement learning systems to adversarial attacks in vision-based environments. Existing defenses against such attacks rely on $l_p$ norm-constrained methods, which struggle to alter the semantics of image inputs significantly. In response, the authors propose SHIFT, a policy-agnostic diffusion-based attack that generates perturbed states with altered semantics while remaining realistic and difficult to detect. Evaluations show that SHIFT outperforms existing attacks in breaking defenses and maintaining perceptual stealth. The results underscore the need for robust policies in reinforcement learning to address the threat of semantics-aware adversarial perturbations. <div>
arXiv:2511.07701v1 Announce Type: new 
Abstract: Reinforcement learning (RL) systems, while achieving remarkable success across various domains, are vulnerable to adversarial attacks. This is especially a concern in vision-based environments where minor manipulations of high-dimensional image inputs can easily mislead the agent's behavior. To this end, various defenses have been proposed recently, with state-of-the-art approaches achieving robust performance even under large state perturbations. However, after closer investigation, we found that the effectiveness of the current defenses is due to a fundamental weakness of the existing $l_p$ norm-constrained attacks, which can barely alter the semantics of image input even under a relatively large perturbation budget. In this work, we propose SHIFT, a novel policy-agnostic diffusion-based state perturbation attack to go beyond this limitation. Our attack is able to generate perturbed states that are semantically different from the true states while remaining realistic and history-aligned to avoid detection. Evaluations show that our attack effectively breaks existing defenses, including the most sophisticated ones, significantly outperforming existing attacks while being more perceptually stealthy. The results highlight the vulnerability of RL agents to semantics-aware adversarial perturbations, indicating the importance of developing more robust policies.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligent Optimization of Multi-Parameter Micromixers Using a Scientific Machine Learning Framework</title>
<link>https://arxiv.org/abs/2511.07702</link>
<guid>https://arxiv.org/abs/2511.07702</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Optimization, Multidimensional, Micromixer, Reinforcement Learning <br />
Summary: <br />
This paper presents a new framework utilizing Scientific Machine Learning (Sci-ML) to tackle multidimensional optimization challenges in engineering. Traditional simulation-based methods are limited in optimizing one problem at a time and require extensive computational time. The proposed approach overcomes these limitations by employing a Deep Reinforcement Learning (DRL) agent to optimize a micromixer design by exploring key parameters. The agent interacts with a Physics-Informed Neural Network (PINN) environment, resulting in instantaneous solutions to complex optimization problems. Through training, the agent achieved consistently greater efficiency across various Schmidt numbers, with a maximum improvement of 32% at Schmidt number 13.3. A comparison with Genetic Algorithm highlighted the advantages of the proposed method. This framework demonstrates the potential of Sci-ML in revolutionizing multidimensional optimization processes in engineering applications. <br /> <div>
arXiv:2511.07702v1 Announce Type: new 
Abstract: Multidimensional optimization has consistently been a critical challenge in engineering. However, traditional simulation-based optimization methods have long been plagued by significant limitations: they are typically capable of optimizing only a single problem at a time and require substantial computational time for meshing and numerical simulation. This paper introduces a novel framework leveraging cutting-edge Scientific Machine Learning (Sci-ML) methodologies to overcome these inherent drawbacks of conventional approaches. The proposed method provides instantaneous solutions to a spectrum of complex, multidimensional optimization problems. A micromixer case study is employed to demonstrate this methodology. An agent, operating on a Deep Reinforcement Learning (DRL) architecture, serves as the optimizer to explore the relationships between key problem parameters. This optimizer interacts with an environment constituted by a parametric Physics-Informed Neural Network (PINN), which responds to the agent's actions at a significantly higher speed than traditional numerical methods. The agent's objective, conditioned on the Schmidt number is to discover the optimal geometric and physical parameters that maximize the micromixer's efficiency. After training the agent across a wide range of Schmidt numbers, we analyzed the resulting optimal designs. Across this entire spectrum, the achieved efficiency was consistently greater than the baseline, normalized value. The maximum efficiency occurred at a Schmidt number of 13.3, demonstrating an improvement of approximately 32%. Finally, a comparative analysis with a Genetic Algorithm was conducted under equivalent conditions to underscore the advantages of the proposed method.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Ranking-Based Optimization Algorithm for the Vehicle Relocation Problem in Car Sharing Services</title>
<link>https://arxiv.org/abs/2511.07724</link>
<guid>https://arxiv.org/abs/2511.07724</guid>
<content:encoded><![CDATA[
<div> Optimization, Vehicle Relocation, Car-sharing, Scooters, Algorithm

Summary:
The paper presents a solution to the Vehicle Relocation Problem in free-floating car-sharing services by utilizing strategies for repositioning vehicles and transferring personnel with the use of scooters. It divides the service area into zones based on temporal patterns, allowing discrete optimization methods to be applied. The algorithm makes decisions considering the number of cars per zone, demand probability density, and trip durations. Real-world data from a car-sharing service operator in Poland was used for experiments. The proposed algorithm showed an average improvement of 8.44% over the baseline scenario, while the MIP solver had a 19.6% improvement, but it included trip selection decisions not relevant to current business rules. The solution could enhance performance metrics by approximately 3%-10% depending on workforce size. <div>
arXiv:2511.07724v1 Announce Type: new 
Abstract: The paper addresses the Vehicle Relocation Problem in free-floating car-sharing services by presenting a solution focused on strategies for repositioning vehicles and transferring personnel with the use of scooters. Our method begins by dividing the service area into zones that group regions with similar temporal patterns of vehicle presence and service demand, allowing the application of discrete optimization methods. In the next stage, we propose a fast ranking-based algorithm that makes its decisions on the basis of the number of cars available in each zone, the projected probability density of demand, and estimated trip durations. The experiments were carried out on the basis of real-world data originating from a major car-sharing service operator in Poland. The results of this algorithm are evaluated against scenarios without optimization that constitute a baseline and compared with the results of an exact algorithm to solve the Mixed Integer Programming (MIP) model. As performance metrics, the total travel time was used. Under identical conditions (number of vehicles, staff, and demand distribution), the average improvements with respect to the baseline of our algorithm and MIP solver were equal to 8.44\% and 19.6\% correspondingly. However, it should be noted that the MIP model also mimicked decisions on trip selection, which are excluded by current services business rules. The analysis of results suggests that, depending on the size of the workforce, the application of the proposed solution allows for improving performance metrics by roughly 3%-10%.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multistep Quasimetric Learning for Scalable Goal-conditioned Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07730</link>
<guid>https://arxiv.org/abs/2511.07730</guid>
<content:encoded><![CDATA[
<div> key words: AI, GCRL, long-horizon, Monte Carlo, robotic manipulation

Summary:
In the field of artificial intelligence (AI), the challenge of learning how to achieve goals in an environment has persisted. Current methods struggle with reasoning over long time horizons. The article addresses the issue of estimating temporal distances between observations, comparing temporal difference and Monte Carlo methods. By integrating these approaches, the authors propose a GCRL method that utilizes a multistep Monte-Carlo return to fit a quasimetric distance. Their method outperforms existing GCRL methods in long-horizon simulated tasks, including visual observations up to 4000 steps. Additionally, the method is successfully applied in real-world robotic manipulation tasks, specifically in a Bridge setup, enabling multistep stitching from unlabeled offline visual data. This work represents a significant advancement in GCRL methods, particularly in handling long-horizon tasks and real-world applications.<br /><br />Summary: <div>
arXiv:2511.07730v1 Announce Type: new 
Abstract: Learning how to reach goals in an environment is a longstanding challenge in AI, yet reasoning over long horizons remains a challenge for modern methods. The key question is how to estimate the temporal distance between pairs of observations. While temporal difference methods leverage local updates to provide optimality guarantees, they often perform worse than Monte Carlo methods that perform global updates (e.g., with multi-step returns), which lack such guarantees. We show how these approaches can be integrated into a practical GCRL method that fits a quasimetric distance using a multistep Monte-Carlo return. We show our method outperforms existing GCRL methods on long-horizon simulated tasks with up to 4000 steps, even with visual observations. We also demonstrate that our method can enable stitching in the real-world robotic manipulation domain (Bridge setup). Our approach is the first end-to-end GCRL method that enables multistep stitching in this real-world manipulation domain from an unlabeled offline dataset of visual observations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Optimization on Graph-Structured Data via Gaussian Processes with Spectral Representations</title>
<link>https://arxiv.org/abs/2511.07734</link>
<guid>https://arxiv.org/abs/2511.07734</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian optimization, graph-structured domains, Gaussian process surrogates, low-rank spectral representations, efficient global search

Summary: 
This article introduces a scalable framework for global optimization over graph-structured domains using Bayesian optimization. The method utilizes low-rank spectral representations to construct Gaussian process surrogates from sparse structural observations. By jointly inferring graph structure and node representations through learnable embeddings, efficient global search and principled uncertainty estimation can be achieved even with limited data. The theoretical analysis provided establishes conditions for accurately recovering the underlying graph structure under various sampling regimes. Experimental results on synthetic and real-world datasets demonstrate that the proposed approach outperforms existing methods by achieving faster convergence and improved optimization performance. <div>
arXiv:2511.07734v1 Announce Type: new 
Abstract: Bayesian optimization (BO) is a powerful framework for optimizing expensive black-box objectives, yet extending it to graph-structured domains remains challenging due to the discrete and combinatorial nature of graphs. Existing approaches often rely on either full graph topology-impractical for large or partially observed graphs-or incremental exploration, which can lead to slow convergence. We introduce a scalable framework for global optimization over graphs that employs low-rank spectral representations to build Gaussian process (GP) surrogates from sparse structural observations. The method jointly infers graph structure and node representations through learnable embeddings, enabling efficient global search and principled uncertainty estimation even with limited data. We also provide theoretical analysis establishing conditions for accurate recovery of underlying graph structure under different sampling regimes. Experiments on synthetic and real-world datasets demonstrate that our approach achieves faster convergence and improved optimization performance compared to prior methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Exploration to Exploitation: A Two-Stage Entropy RLVR Approach for Noise-Tolerant MLLM Training</title>
<link>https://arxiv.org/abs/2511.07738</link>
<guid>https://arxiv.org/abs/2511.07738</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Verifiable Rewards, Multimodal Large Language Models, Token-level Entropy Optimization, Group-Relative Policy Optimization

Summary:
Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) faces challenges due to limited high-quality labeled data and annotation noise. A new two-stage token-level entropy optimization method is proposed to address these issues. In the exploration phase, token-level entropy maximization promotes diverse output generation, preventing convergence to noisy labels. This ensures reliable reward estimation for Group-Relative Policy Optimization (GRPO). As training progresses, the model transitions to the exploitation phase where entropy minimization enhances prediction accuracy. Empirical tests on MLLM backbones show superior performance over existing methods. The approach unifies external, internal, and entropy-based techniques, offering robust results across various noise settings and tasks.

<br /><br />Summary: Reinforcement Learning with Verifiable Rewards faces challenges due to limited data and annotation noise. A two-stage token-level entropy optimization method is proposed to address these issues, promoting diverse output generation in the exploration phase and enhancing prediction accuracy in the exploitation phase. Empirical tests demonstrate superior performance compared to existing methods, showcasing the approach's robustness across different settings and tasks. <div>
arXiv:2511.07738v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) for Multimodal Large Language Models (MLLMs) is highly dependent on high-quality labeled data, which is often scarce and prone to substantial annotation noise in real-world scenarios. Existing unsupervised RLVR methods, including pure entropy minimization, can overfit to incorrect labels and limit the crucial reward ranking signal for Group-Relative Policy Optimization (GRPO). To address these challenges and enhance noise tolerance, we propose a novel two-stage, token-level entropy optimization method for RLVR. This approach dynamically guides the model from exploration to exploitation during training. In the initial exploration phase, token-level entropy maximization promotes diverse and stochastic output generation, serving as a strong regularizer that prevents premature convergence to noisy labels and ensures sufficient intra-group variation, which enables more reliable reward gradient estimation in GRPO. As training progresses, the method transitions into the exploitation phase, where token-level entropy minimization encourages the model to produce confident and deterministic outputs, thereby consolidating acquired knowledge and refining prediction accuracy. Empirically, across three MLLM backbones - Qwen2-VL-2B, Qwen2-VL-7B, and Qwen2.5-VL-3B - spanning diverse noise settings and multiple tasks, our phased strategy consistently outperforms prior approaches by unifying and enhancing external, internal, and entropy-based methods, delivering robust and superior performance across the board.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Schedulers for Schedule-free: Theoretically inspired hyperparameters</title>
<link>https://arxiv.org/abs/2511.07767</link>
<guid>https://arxiv.org/abs/2511.07767</guid>
<content:encoded><![CDATA[
<div> schedule-free method, hyperparameter tuning, learning rate, convergence theory, deep neural networks

Summary:
The article introduces an extension of the schedule-free method, allowing for any scheduler instead of just a constant learning rate. By updating the averaging parameter as a function of the learning rate, the theory shows predictive power in practical executions on deep neural networks. When applied to the warmup-stable-decay schedule, the optimal convergence rate is shown to be $\mathcal{O}(1/\sqrt{T})$. A new adaptive Polyak learning rate schedule for schedule-free is designed using convexity, with an optimal anytime last-iterate convergence proven. The new Polyak schedule performs well compared to baselines on a black-box model distillation task. Overall, the article advances the understanding and application of schedule-free methods in optimizing deep neural network performance. 

<br /><br />Summary: <div>
arXiv:2511.07767v1 Announce Type: new 
Abstract: The recently proposed schedule-free method has been shown to achieve strong performance when hyperparameter tuning is limited. The current theory for schedule-free only supports a constant learning rate, where-as the implementation used in practice uses a warm-up schedule. We show how to extend the last-iterate convergence theory of schedule-free to allow for any scheduler, and how the averaging parameter has to be updated as a function of the learning rate. We then perform experiments showing how our convergence theory has some predictive power with regards to practical executions on deep neural networks, despite that this theory relies on assuming convexity. When applied to the warmup-stable-decay (wsd) schedule, our theory shows the optimal convergence rate of $\mathcal{O}(1/\sqrt{T})$. We then use convexity to design a new adaptive Polyak learning rate schedule for schedule-free. We prove an optimal anytime last-iterate convergence for our new Polyak schedule, and show that it performs well compared to a number of baselines on a black-box model distillation task.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physical Consistency of Aurora's Encoder: A Quantitative Study</title>
<link>https://arxiv.org/abs/2511.07787</link>
<guid>https://arxiv.org/abs/2511.07787</guid>
<content:encoded><![CDATA[
<div> encoder, weather forecasting, transparency, physical consistency, interpretability methods
Summary:<br />
- The study focuses on the physical consistency of a large-scale weather forecasting model called Aurora, which is known for its high accuracy but lack of transparency. 
- Researchers investigate whether the model's latent representations align with known physical and meteorological concepts. 
- By training linear classifiers on a dataset of embeddings, they identify three concepts: the land-sea boundary, extreme temperature events, and atmospheric instability. 
- The results show that Aurora learns physically consistent features, but struggles with capturing rare events. 
- The study emphasizes the importance of interpretability methods to validate and build trust in AI-driven weather models. 
<br />Summary: <div>
arXiv:2511.07787v1 Announce Type: new 
Abstract: The high accuracy of large-scale weather forecasting models like Aurora is often accompanied by a lack of transparency, as their internal representations remain largely opaque. This "black box" nature hinders their adoption in high-stakes operational settings. In this work, we probe the physical consistency of Aurora's encoder by investigating whether its latent representations align with known physical and meteorological concepts. Using a large-scale dataset of embeddings, we train linear classifiers to identify three distinct concepts: the fundamental land-sea boundary, high-impact extreme temperature events, and atmospheric instability. Our findings provide quantitative evidence that Aurora learns physically consistent features, while also highlighting its limitations in capturing the rarest events. This work underscores the critical need for interpretability methods to validate and build trust in the next generation of Al-driven weather models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analyzing Political Text at Scale with Online Tensor LDA</title>
<link>https://arxiv.org/abs/2511.07809</link>
<guid>https://arxiv.org/abs/2511.07809</guid>
<content:encoded><![CDATA[
<div> Keywords: topic modeling, Tensor Latent Dirichlet Allocation, scalability, large-scale analysis, social media conversations

Summary:
The paper introduces a novel topic modeling method called Tensor Latent Dirichlet Allocation (TLDA), catering to large-scale datasets with billions of documents. This method guarantees identifiable and recoverable parameters, along with sample complexity guarantees for efficient analyses. TLDA outperforms existing parallelized LDA methods in terms of computational and memory efficiency, achieving speeds 3-4 times faster. The open-source, GPU-based implementation further enhances scalability, enabling researchers to study very large corpora. The paper showcases the method's potential by conducting two significant real-world studies. The first study delves into the evolution of the #MeToo movement via Twitter conversations spanning over two years. The second study focuses on social media discussions surrounding election fraud in the 2020 presidential election. These analyses exemplify how TLDA empowers social scientists to examine important issues on a large scale and in near real-time. 

<br /><br />Summary: <div>
arXiv:2511.07809v1 Announce Type: new 
Abstract: This paper proposes a topic modeling method that scales linearly to billions of documents. We make three core contributions: i) we present a topic modeling method, Tensor Latent Dirichlet Allocation (TLDA), that has identifiable and recoverable parameter guarantees and sample complexity guarantees for large data; ii) we show that this method is computationally and memory efficient (achieving speeds over 3-4x those of prior parallelized Latent Dirichlet Allocation (LDA) methods), and that it scales linearly to text datasets with over a billion documents; iii) we provide an open-source, GPU-based implementation, of this method. This scaling enables previously prohibitive analyses, and we perform two real-world, large-scale new studies of interest to political scientists: we provide the first thorough analysis of the evolution of the #MeToo movement through the lens of over two years of Twitter conversation and a detailed study of social media conversations about election fraud in the 2020 presidential election. Thus this method provides social scientists with the ability to study very large corpora at scale and to answer important theoretically-relevant questions about salient issues in near real-time.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Objective Bilevel Learning</title>
<link>https://arxiv.org/abs/2511.07824</link>
<guid>https://arxiv.org/abs/2511.07824</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Multi-Objective Bilevel Learning, Optimization Algorithms, Chebyshev, Pareto Front Exploration

Summary:
The article discusses the need for multi-objective bilevel learning (MOBL) in complex machine learning applications. It aims to address problems with conflicting objectives and decision variables across different layers. The proposed weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) algorithm framework is designed for deterministic and stochastic settings, guaranteeing finite-time Pareto-stationarity convergence rates with low oracle complexity. This framework allows for the identification of preference-guided Pareto-stationary solutions and systematic exploration of the Pareto front. The theoretical foundations and algorithmic efficiency of MOBL optimization algorithms are explored, with empirical experiments validating the theoretical results. The study fills a gap in the understanding of MOBL and provides a systematic approach to tackling complex machine learning problems with multiple conflicting objectives. <br /><br />Summary: <div>
arXiv:2511.07824v1 Announce Type: new 
Abstract: As machine learning (ML) applications grow increasingly complex in recent years, modern ML frameworks often need to address multiple potentially conflicting objectives with coupled decision variables across different layers. This creates a compelling need for multi-objective bilevel learning (MOBL). So far, however, the field of MOBL remains in its infancy and many important problems remain under-explored. This motivates us to fill this gap and systematically investigate the theoretical and algorithmic foundation of MOBL. Specifically, we consider MOBL problems with multiple conflicting objectives guided by preferences at the upper-level subproblem, where part of the inputs depend on the optimal solution of the lower-level subproblem. Our goal is to develop efficient MOBL optimization algorithms to (1) identify a preference-guided Pareto-stationary solution with low oracle complexity; and (2) enable systematic Pareto front exploration. To this end, we propose a unifying algorithmic framework called weighted-Chebyshev multi-hyper-gradient-descent (WC-MHGD) for both deterministic and stochastic settings with finite-time Pareto-stationarity convergence rate guarantees, which not only implies low oracle complexity but also induces systematic Pareto front exploration. We further conduct extensive experiments to confirm our theoretical results.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MURPHY: Multi-Turn GRPO for Self Correcting Code Generation</title>
<link>https://arxiv.org/abs/2511.07833</link>
<guid>https://arxiv.org/abs/2511.07833</guid>
<content:encoded><![CDATA[
<div> Reinforcement Learning, Verifiable Rewards, Large Language Models, Murphy, Reflective Optimization <br />
Summary: The paper introduces Murphy, a framework that enhances the reasoning capabilities of large language models by incorporating iterative self-correction during training. It addresses the limitations of existing approaches like GRPO in agentic tasks requiring iterative decision-making. By utilizing quantitative and qualitative feedback, Murphy enables models to progressively improve their reasoning across multiple turns. Evaluations on code generation benchmarks with model families like Qwen and OLMo demonstrate that Murphy outperforms GRPO, achieving up to an 8% relative gain in pass@1 on similar compute budgets. This advancement in reinforcement learning with verifiable rewards shows promising results in enhancing the reasoning capabilities of language models, particularly in tasks that involve iterative decision-making processes. <br /> <div>
arXiv:2511.07833v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful framework for enhancing the reasoning capabilities of large language models (LLMs). However, existing approaches such as Group Relative Policy Optimization (GRPO) and its variants, while effective on reasoning benchmarks, struggle with agentic tasks that require iterative decision-making. We introduce Murphy, a multi-turn reflective optimization framework that extends GRPO by incorporating iterative self-correction during training. By leveraging both quantitative and qualitative execution feedback, Murphy enables models to progressively refine their reasoning across multiple turns. Evaluations on code generation benchmarks with model families such as Qwen and OLMo show that Murphy consistently improves performance, achieving up to a 8% relative gain in pass@1 over GRPO, on similar compute budgets.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-AdamW: Investigating Decoupled Weight Decay and Bias Correction in Private Deep Learning</title>
<link>https://arxiv.org/abs/2511.07843</link>
<guid>https://arxiv.org/abs/2511.07843</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, differential privacy, AdamW, DP-AdamW, DP-AdamW-BC

Summary:
DP-AdamW and DP-AdamW-BC are introduced as differentially private variants of the AdamW optimizer with DP bias correction for the second moment estimator. The theoretical results provide privacy and convergence guarantees for both optimizers. Empirical analysis across multiple privacy budgets ($\epsilon = 1, 3, 7$) shows that DP-AdamW outperforms existing state-of-the-art differentially private optimizers in text classification, image classification, and graph node classification tasks. DP-AdamW scores over 15% higher in text classification, up to 5% higher in image classification, and consistently 1% higher in graph node classification compared to other optimizers. Interestingly, incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, contrary to the improvement seen with DP-AdamBC over DP-Adam. These findings highlight the importance of optimizing differentially private deep learning models while ensuring privacy protection. 

<br /><br />Summary: DP-AdamW and DP-AdamW-BC, variants of the AdamW optimizer with DP, exhibit strong performance in various classification tasks compared to existing differentially private optimizers. DP-AdamW shows higher accuracy rates, while DP-AdamW-BC, with bias correction, decreases accuracy inconsistently in different scenarios. <div>
arXiv:2511.07843v1 Announce Type: new 
Abstract: As deep learning methods increasingly utilize sensitive data on a widespread scale, differential privacy (DP) offers formal guarantees to protect against information leakage during model training. A significant challenge remains in implementing DP optimizers that retain strong performance while preserving privacy. Recent advances introduced ever more efficient optimizers, with AdamW being a popular choice for training deep learning models because of strong empirical performance. We study \emph{DP-AdamW} and introduce \emph{DP-AdamW-BC}, a differentially private variant of the AdamW optimizer with DP bias correction for the second moment estimator. We start by showing theoretical results for privacy and convergence guarantees of DP-AdamW and DP-AdamW-BC. Then, we empirically analyze the behavior of both optimizers across multiple privacy budgets ($\epsilon = 1, 3, 7$). We find that DP-AdamW outperforms existing state-of-the-art differentially private optimizers like DP-SGD, DP-Adam, and DP-AdamBC, scoring over 15\% higher on text classification, up to 5\% higher on image classification, and consistently 1\% higher on graph node classification. Moreover, we empirically show that incorporating bias correction in DP-AdamW (DP-AdamW-BC) consistently decreases accuracy, in contrast to the improvement of DP-AdamBC improvement over DP-Adam.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Method for Proving Networks Universal Approximation Property</title>
<link>https://arxiv.org/abs/2511.07857</link>
<guid>https://arxiv.org/abs/2511.07857</guid>
<content:encoded><![CDATA[
<div> modular framework, universal approximation, deep learning architectures, progressive refinement, expressive power
<br />
Summary:
This paper introduces a novel modular framework for proving the universal approximation property in deep learning architectures. Instead of relying on separate proofs for each architecture type, the framework defines a Universal Approximation Module (UAM) as a basic building block with the universal approximation property. By composing deep networks using these UAMs, the overall network retains the universal approximation property. The approximation process is viewed as a progressive refinement across modules, offering a unified analysis of various architecture types. This approach not only eliminates the need for separate proofs for different architectures but also allows for a step-by-step understanding of how the expressive power evolves throughout the network. <div>
arXiv:2511.07857v1 Announce Type: new 
Abstract: Deep learning architectures are highly diverse. To prove their universal approximation properties, existing works typically rely on model-specific proofs. Generally, they construct a dedicated mathematical formulation for each architecture (e.g., fully connected networks, CNNs, or Transformers) and then prove their universal approximability. However, this approach suffers from two major limitations: first, every newly proposed architecture often requires a completely new proof from scratch; second, these proofs are largely isolated from one another, lacking a common analytical foundation. This not only incurs significant redundancy but also hinders unified theoretical understanding across different network families. To address these issues, this paper proposes a general and modular framework for proving universal approximation. We define a basic building block (comprising one or multiple layers) that possesses the universal approximation property as a Universal Approximation Module (UAM). Under this condition, we show that any deep network composed of such modules inherently retains the universal approximation property. Moreover, the overall approximation process can be interpreted as a progressive refinement across modules. This perspective not only unifies the analysis of diverse architectures but also enables a step-by-step understanding of how expressive power evolves through the network.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithm-Relative Trajectory Valuation in Policy Gradient Control</title>
<link>https://arxiv.org/abs/2511.07878</link>
<guid>https://arxiv.org/abs/2511.07878</guid>
<content:encoded><![CDATA[
<div> Trajectory Shapley, learning algorithm, policy-gradient control, Persistence of Excitation, REINFORCE <br />
Summary: <br />
The study examines how trajectory value is influenced by the learning algorithm in policy-gradient control, focusing on Trajectory Shapley in an uncertain LQR setting. It reveals a negative correlation between Persistence of Excitation (PE) and marginal value under vanilla REINFORCE, showing a variance-mediated mechanism. Higher PE leads to lower gradient variance, while near saddles, higher variance increases escape probability, impacting marginal contribution. By stabilizing with state whitening or Fisher preconditioning, the variance channel is neutralized, and information content becomes dominant, resulting in a positive correlation. Experiments validate the mechanism and suggest that decision-aligned scores can complement Shapley for pruning toxic subsets. <div>
arXiv:2511.07878v1 Announce Type: new 
Abstract: We study how trajectory value depends on the learning algorithm in policy-gradient control. Using Trajectory Shapley in an uncertain LQR, we find a negative correlation between Persistence of Excitation (PE) and marginal value under vanilla REINFORCE ($r\approx-0.38$). We prove a variance-mediated mechanism: (i) for fixed energy, higher PE yields lower gradient variance; (ii) near saddles, higher variance increases escape probability, raising marginal contribution. When stabilized (state whitening or Fisher preconditioning), this variance channel is neutralized and information content dominates, flipping the correlation positive ($r\approx+0.29$). Hence, trajectory value is algorithm-relative. Experiments validate the mechanism and show decision-aligned scores (Leave-One-Out) complement Shapley for pruning, while Shapley identifies toxic subsets.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-cognitive Multi-scale Hierarchical Reasoning for Motor Imagery Decoding</title>
<link>https://arxiv.org/abs/2511.07884</link>
<guid>https://arxiv.org/abs/2511.07884</guid>
<content:encoded><![CDATA[
<div> Hierarchical signal processing, multi-scale representations, introspective uncertainty estimation, MI-based BCI systems, EEG backbones <br />
<br />
Summary: 
This study introduces a hierarchical and meta-cognitive decoding framework for four-class motor imagery (MI) classification in brain-computer interfaces (BCI). The framework includes a multi-scale signal processing module and an uncertainty estimation module to enhance classification accuracy and reduce inter-subject variability. The framework was tested on three standard EEG backbones and evaluated using the BCI Competition IV-2a dataset in a subject-independent setting. Results show that the proposed components improve classification accuracy and robustness to subject heterogeneity and noisy trials. Combining hierarchical multi-scale processing with introspective confidence estimation shows promising results for enhancing the reliability of MI-based BCI systems. <div>
arXiv:2511.07884v1 Announce Type: new 
Abstract: Brain-computer interface (BCI) aims to decode motor intent from noninvasive neural signals to enable control of external devices, but practical deployment remains limited by noise and variability in motor imagery (MI)-based electroencephalogram (EEG) signals. This work investigates a hierarchical and meta-cognitive decoding framework for four-class MI classification. We introduce a multi-scale hierarchical signal processing module that reorganizes backbone features into temporal multi-scale representations, together with an introspective uncertainty estimation module that assigns per-cycle reliability scores and guides iterative refinement. We instantiate this framework on three standard EEG backbones (EEGNet, ShallowConvNet, and DeepConvNet) and evaluate four-class MI decoding using the BCI Competition IV-2a dataset under a subject-independent setting. Across all backbones, the proposed components improve average classification accuracy and reduce inter-subject variance compared to the corresponding baselines, indicating increased robustness to subject heterogeneity and noisy trials. These results suggest that combining hierarchical multi-scale processing with introspective confidence estimation can enhance the reliability of MI-based BCI systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generalized Spectral Framework to Expain Neural Scaling and Compression Dynamics</title>
<link>https://arxiv.org/abs/2511.07892</link>
<guid>https://arxiv.org/abs/2511.07892</guid>
<content:encoded><![CDATA[
<div> scaling laws, model size, dataset size, compute, spectral framework

Summary: 
The article presents a generalized spectral framework that elucidates the relationship between learning dynamics and model compression in neural networks. It introduces a spectral evolution function characterized by an effective spectral-temporal elasticity, encompassing various scaling behaviors observed in different settings. This framework unifies existing theories on learning and compression, offering a comprehensive understanding of the underlying mechanisms. By generalizing the spectral evolution function to an asymptotically polynomial form, the framework establishes a consistent relation between learning dynamics and compression phenomena. The proposed framework not only recovers previous theories but also provides a more holistic perspective on how different factors such as model size, dataset size, and compute resources impact the performance metrics in neural networks. The framework serves as a valuable tool for analyzing and predicting the behavior of neural networks in various scenarios. 

<br /><br />Summary: <div>
arXiv:2511.07892v1 Announce Type: new 
Abstract: Empirical scaling laws describe how test loss and other performance metrics depend on model size, dataset size, and compute. While such laws are consistent within specific regimes, apparently distinct scaling behaviors have been reported for related settings such as model compression. Motivated by recent progress in spectral analyses of neural representations, this paper develops a \emph{generalized spectral framework} that unifies learning dynamics and compression phenomena under a common functional ansatz. We generalize the spectral evolution function from the linear kernel form $g(\lambda t)=\lambda t$ to an asymptotically polynomial function $g(\lambda,t;\beta)$, characterized by an effective spectral--temporal elasticity $\rho(\beta)$. This framework recovers existing lazy and feature-learning theories as special cases and yields an invariant relation between learning and compression
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistically Assuring Safety of Control Systems using Ensembles of Safety Filters and Conformal Prediction</title>
<link>https://arxiv.org/abs/2511.07899</link>
<guid>https://arxiv.org/abs/2511.07899</guid>
<content:encoded><![CDATA[
<div> reachability analysis, Hamilton-Jacobi, safety assurance, reinforcement learning, conformal prediction

Summary: 
The article introduces a framework using conformal prediction to provide probabilistic safety guarantees when approximating the Hamilton-Jacobi (HJ) value function for safety verification in autonomous systems. The HJ value function is computationally expensive to compute, leading to the use of reinforcement learning to approximate it. However, learned value functions and policies may not always be correct, causing uncertainty in safety guarantees. The conformal prediction framework is utilized to bound this uncertainty and provide safety guarantees when using learned HJ value functions and policies. The framework calibrates the switching between the unsafe nominal controller and the learned safe policy, deriving safety guarantees under this switched policy. Additionally, the article explores using an ensemble of independently trained HJ value functions as a safety filter, comparing its performance to using individual value functions alone. <div>
arXiv:2511.07899v1 Announce Type: new 
Abstract: Safety assurance is a fundamental requirement for deploying learning-enabled autonomous systems. Hamilton-Jacobi (HJ) reachability analysis is a fundamental method for formally verifying safety and generating safe controllers. However, computing the HJ value function that characterizes the backward reachable set (BRS) of a set of user-defined failure states is computationally expensive, especially for high-dimensional systems, motivating the use of reinforcement learning approaches to approximate the value function. Unfortunately, a learned value function and its corresponding safe policy are not guaranteed to be correct. The learned value function evaluated at a given state may not be equal to the actual safety return achieved by following the learned safe policy. To address this challenge, we introduce a conformal prediction-based (CP) framework that bounds such uncertainty. We leverage CP to provide probabilistic safety guarantees when using learned HJ value functions and policies to prevent control systems from reaching failure states. Specifically, we use CP to calibrate the switching between the unsafe nominal controller and the learned HJ-based safe policy and to derive safety guarantees under this switched policy. We also investigate using an ensemble of independently trained HJ value functions as a safety filter and compare this ensemble approach to using individual value functions alone.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-driven Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07904</link>
<guid>https://arxiv.org/abs/2511.07904</guid>
<content:encoded><![CDATA[
<div> Framework, Test-driven Reinforcement Learning, Task definition, Maximum entropy policy optimization, Multi-objective optimization <br />
<br />
Summary: <br />
The article introduces a Test-driven Reinforcement Learning (TdRL) framework that uses multiple test functions to represent task objectives, making it easier to define tasks compared to traditional reward functions in reinforcement learning. The framework includes pass-fail tests for defining the optimal objective and indicative tests for guiding the learning process. It is shown that using a trajectory return function that assigns higher returns to trajectories closer to the optimal trajectory set leads to a policy closer to the optimal policy set. A lexicographic heuristic approach is proposed for learning the trajectory return function and an algorithm implementation of TdRL is developed. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL can match or outperform handcrafted reward methods in policy training, while also supporting multi-objective optimization. TdRL provides a novel perspective for representing task objectives in RL applications, addressing challenges in reward design. <br /> <div>
arXiv:2511.07904v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CellARC: Measuring Intelligence with Cellular Automata</title>
<link>https://arxiv.org/abs/2511.07908</link>
<guid>https://arxiv.org/abs/2511.07908</guid>
<content:encoded><![CDATA[
<div> benchmark, abstraction, reasoning, cellular automata, baseline

Summary:
The study introduces CellARC, a synthetic benchmark for abstraction and reasoning based on multicolor 1D cellular automata (CA). It contains episodes with support pairs and queries in 256 tokens, allowing for quick iteration with small models and offering a task space with adjustable parameters such as alphabet size, radius, rule family, and more. The benchmark provides 95k training episodes and two 1k test splits for evaluation. Various baseline models are tested, with a vanilla transformer achieving high accuracy on both interpolation and extrapolation tasks. A large closed model and an ensemble model combining the transformer with a symbolic baseline also show promising results. CellARC allows for exploration of generalization without human biases, controlled difficulty sampling, and reproducible studies on rule inference by models within constrained budgets. The results highlight the potential of neuro-symbolic approaches in tackling complex reasoning tasks. 

<br /><br />Summary: <div>
arXiv:2511.07908v1 Announce Type: new 
Abstract: We introduce CellARC, a synthetic benchmark for abstraction and reasoning built from multicolor 1D cellular automata (CA). Each episode has five support pairs and one query serialized in 256 tokens, enabling rapid iteration with small models while exposing a controllable task space with explicit knobs for alphabet size k, radius r, rule family, Langton's lambda, query coverage, and cell entropy. We release 95k training episodes plus two 1k test splits (interpolation/extrapolation) and evaluate symbolic, recurrent, convolutional, transformer, recursive, and LLM baselines. CellARC decouples generalization from anthropomorphic priors, supports unlimited difficulty-controlled sampling, and enables reproducible studies of how quickly models infer new rules under tight budgets. Our strongest small-model baseline (a 10M-parameter vanilla transformer) outperforms recent recursive models (TRM, HRM), reaching 58.0%/32.4% per-token accuracy on the interpolation/extrapolation splits, while a large closed model (GPT-5 High) attains 62.3%/48.1% on subsets of 100 test tasks. An ensemble that chooses per episode between the Transformer and the best symbolic baseline reaches 65.4%/35.5%, highlighting neuro-symbolic complementarity. Leaderboard: https://cellarc.mireklzicar.com
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rectified Noise: A Generative Model Using Positive-incentive Noise</title>
<link>https://arxiv.org/abs/2511.07911</link>
<guid>https://arxiv.org/abs/2511.07911</guid>
<content:encoded><![CDATA[
<div> generative model, Rectified Flow, $\pi$-noise, Stochastic Differential Equations, generative performance

Summary:
Rectified Flow (RF) is a popular generative model based on Ordinary Differential Equations (ODE). Injecting noise through reverse-time Stochastic Differential Equations (SDE) has been shown to improve generative performance. A new algorithm called Rectified Noise ($\Delta$RN) enhances RF models by injecting $\pi$-noise into their velocity fields. This innovative approach transforms pre-trained RF models into $\pi$-noise generators, improving their generative capabilities. Experimental results show that RF models using Rectified Noise reduce FID from 10.16 to 9.05 on ImageNet-1k. The $\pi$-noise generators achieve enhanced performance with only 0.39% additional training parameters. Rectified Noise offers a promising strategy to boost the generative abilities of existing RF models. <br /><br />Summary: <div>
arXiv:2511.07911v1 Announce Type: new 
Abstract: Rectified Flow (RF) has been widely used as an effective generative model. Although RF is primarily based on probability flow Ordinary Differential Equations (ODE), recent studies have shown that injecting noise through reverse-time Stochastic Differential Equations (SDE) for sampling can achieve superior generative performance. Inspired by Positive-incentive Noise ($\pi$-noise), we propose an innovative generative algorithm to train $\pi$-noise generators, namely Rectified Noise ($\Delta$RN), which improves the generative performance by injecting $\pi$-noise into the velocity field of pre-trained RF models. After introducing the Rectified Noise pipeline, pre-trained RF models can be efficiently transformed into $\pi$-noise generators. We validate Rectified Noise by conducting extensive experiments across various model architectures on different datasets. Notably, we find that: (1) RF models using Rectified Noise reduce FID from \textbf{10.16 to 9.05} on ImageNet-1k. (2) The models of $\pi$-noise generators achieve improved performance with only \textbf{0.39\%} additional training parameters.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feedback Descent: Open-Ended Text Optimization via Pairwise Comparison</title>
<link>https://arxiv.org/abs/2511.07919</link>
<guid>https://arxiv.org/abs/2511.07919</guid>
<content:encoded><![CDATA[
<div> Keywords: Feedback Descent, text artifacts, structured feedback, preference learning, molecule discovery

Summary: 
Feedback Descent introduces a novel approach to optimizing text artifacts such as prompts, code, and molecules through structured textual feedback. By preserving detailed critiques rather than compressing them to binary preferences, it widens the information bottleneck in preference learning and enables targeted edits in text space. The framework leverages in-context learning to transform structured feedback into directional information, facilitating more effective optimization. Unlike existing methods that simplify judgments into binary decisions, Feedback Descent utilizes high-bandwidth textual feedback as supervision for iterative improvements. This approach does not modify model weights and is task-agnostic. Evaluation across three diverse domains demonstrates that Feedback Descent outperforms state-of-the-art optimization methods and even specialized molecular optimizers in molecule discovery tasks. In the DOCKSTRING benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a large compound database across six protein targets.

<br /><br />Summary: <div>
arXiv:2511.07919v1 Announce Type: new 
Abstract: We introduce \textit{Feedback Descent}, a framework that optimizes text artifacts -- prompts, code, and molecules -- through structured textual feedback, rather than relying solely on scalar rewards. By preserving detailed critiques instead of compressing them to binary preferences, Feedback Descent widens the information bottleneck in preference learning, enabling directed optimization in text space rather than weight space. We show that in-context learning can transform structured feedback into gradient-like directional information, enabling targeted edits. Unlike prior approaches that collapse judgments into single bits, our evaluators pair each comparison with textual feedback, which functions as high-bandwidth supervision. The iteration loop is done purely at inference time, without modifying any model weights, and is task-agnostic. We evaluate Feedback Descent on three diverse domains and find that it outperforms state-of-the-art prompt optimization (GEPA), reinforcement learning methods (GRPO, REINVENT), and even specialized graph-based molecular optimizers. In the DOCKSTRING molecule discovery benchmark, Feedback Descent identifies novel drug-like molecules surpassing the $99.9$th percentile of a database with more than $260{,}000$ compounds across six protein targets.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SERL: Self-Examining Reinforcement Learning on Open-Domain</title>
<link>https://arxiv.org/abs/2511.07922</link>
<guid>https://arxiv.org/abs/2511.07922</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Language Models, Self-Examining, Verifiable Rewards, Human Feedback 

Summary: 
Self-Examining Reinforcement Learning (SERL) is proposed to address the challenges of applying Reinforcement Learning to open-domain tasks. SERL utilizes the large language model as both Actor and Judge, introducing two reward mechanisms without external signals. Rewards are derived from pairwise comparison judgments and self-consistency rewards to improve both actor and judge capabilities. Experimental results show that SERL outperforms existing self-improvement methods, achieving state-of-the-art performance on open-domain tasks. The method shows a significant improvement in the win rate of Qwen3-8B on AlpacaEval 2, demonstrating effectiveness and robustness comparable to larger models like Qwen3-32B. SERL provides a promising approach to enhancing the capabilities of language models in reinforcement learning tasks.  

<br /><br />Summary: <div>
arXiv:2511.07922v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has been shown to improve the capabilities of large language models (LLMs). However, applying RL to open-domain tasks faces two key challenges: (1) the inherent subjectivity of these tasks prevents the verifiable rewards as required by Reinforcement Learning with Verifiable Rewards (RLVR); (2) Reinforcement Learning from Human Feedback (RLHF) relies on external reward mechanisms. To overcome these limitations, we propose Self-Examining Reinforcement Learning (SERL), a novel self-improving framework where the LLM serves as both Actor and Judge. SERL introduces two synergistic reward mechanisms without any external signals. On the one hand, to improve the Actor's capability, we derive rewards from Copeland-style pairwise comparison judgments across a group of generated responses. On the other hand, a self-consistency reward that encourages coherent judgments is proposed to improve the Judge's reliability. This process refines the Judge's capability, which in turn provides a more robust reward for Actor. Experiments show that our method outperforms existing self-improvement training methods. SERL improves the LC win rate of Qwen3-8B on AlpacaEval 2 from 52.37% to 59.90%. To the best of our knowledge, our method achieves state-of-the-art performance among self-improving approaches. Furthermore, it achieves a performance comparable to significantly larger models like Qwen3-32B, demonstrating superior effectiveness and robustness on open-domain tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IBMA: An Imputation-Based Mixup Augmentation Using Self-Supervised Learning for Time Series Data</title>
<link>https://arxiv.org/abs/2511.07930</link>
<guid>https://arxiv.org/abs/2511.07930</guid>
<content:encoded><![CDATA[
<div> Keywords: Data augmentation, time series forecasting, Imputation-Based Mixup Augmentation, DLinear, iTrainformer

Summary:
- Data augmentation is essential in time series forecasting to enhance model performance by introducing variability while maintaining temporal patterns.
- Compared to image or text fields, time series data offers fewer augmentation strategies, with advanced techniques like Mixup seldom used.
- The proposed Imputation-Based Mixup Augmentation (IBMA) method combines Imputation-Augmented data with Mixup augmentation, improving model generalization and forecasting performance.
- Evaluation on various forecasting models shows that IBMA consistently enhances performance, achieving improvements in 22 out of 24 instances.
- Particularly with iTrainformer imputation, IBMA performs significantly better compared to eight other augmentation techniques tested.

<br /><br />Summary: 
Data augmentation is crucial in time series forecasting for enhancing model performance by introducing variability while maintaining temporal patterns. However, time series data has limited augmentation strategies compared to other fields. A novel approach, Imputation-Based Mixup Augmentation (IBMA), combines Imputation-Augmented data with Mixup augmentation to improve model generalization. Experimental results on different forecasting models highlight the effectiveness of IBMA, outperforming eight other techniques in 22 out of 24 instances. Notably, IBMA shows significant performance improvements with iTrainformer imputation. <div>
arXiv:2511.07930v1 Announce Type: new 
Abstract: Data augmentation in time series forecasting plays a crucial role in enhancing model performance by introducing variability while maintaining the underlying temporal patterns. However, time series data offers fewer augmentation strategies compared to fields such as image or text, with advanced techniques like Mixup rarely being used. In this work, we propose a novel approach, Imputation-Based Mixup Augmentation (IBMA), which combines Imputation-Augmented data with Mixup augmentation to bolster model generalization and improve forecasting performance. We evaluate the effectiveness of this method across several forecasting models, including DLinear (MLP), TimesNet (CNN), and iTrainformer (Transformer), these models represent some of the most recent advances in time series forecasting. Our experiments, conducted on four datasets (ETTh1, ETTh2, ETTm1, ETTm2) and compared against eight other augmentation techniques, demonstrate that IBMA consistently enhances performance, achieving 22 improvements out of 24 instances, with 10 of those being the best performances, particularly with iTrainformer imputation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predict-then-Optimize Method for Seaport Power-Logistics Scheduling: Generalization across Varying Tasks Stream</title>
<link>https://arxiv.org/abs/2511.07938</link>
<guid>https://arxiv.org/abs/2511.07938</guid>
<content:encoded><![CDATA[
<div> Framework, Power-logistics scheduling, Seaports, Decision-focused learning, Continual learning<br />
<br />
Summary: 
The article introduces a decision-focused continual learning framework for power-logistics scheduling in modern seaports. Traditional scheduling processes involve predicting and then optimizing tasks, but these approaches struggle to adapt to changing task configurations. The proposed framework addresses this issue by adapting online to a stream of scheduling tasks and using Fisher information-based regularization to enhance cross-task generalization. A differentiable convex surrogate helps stabilize gradient backpropagation during training. By learning a decision-aligned forecasting model for new tasks while retaining generalization on prior tasks, the framework achieves superior decision performance and generalization compared to existing methods. Experiments conducted at Jurong Port confirm the effectiveness of the approach, showcasing improved decision quality and generalization while reducing computational costs. <br /><br /> <div>
arXiv:2511.07938v1 Announce Type: new 
Abstract: Power-logistics scheduling in modern seaports typically follow a predict-then-optimize pipeline. To enhance decision quality, decision-focused learning has been proposed to align forecasting and optimization via end-to-end training. However, most formulations assume a fixed task configuration in downstream optimization, and thus generalize poorly to evolving task structures induced by varying seaport vessel arrivals. We address this gap with a decision-focused continual learning framework that adapts online to a stream of scheduling tasks. Specifically, we introduce Fisher information based regularization to enhance cross-task generalization by preserving parameters critical to prior tasks. A differentiable convex surrogate is also developed to stabilize gradient backpropagation. The proposed approach enables learning a decision-aligned forecasting model for new scheduling tasks while retaining generalization on earlier tasks. Experiments calibrated to the Jurong Port demonstrate superior decision performance and generalization over existing methods with reduced computational cost.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balance Equation-based Distributionally Robust Offline Imitation Learning</title>
<link>https://arxiv.org/abs/2511.07942</link>
<guid>https://arxiv.org/abs/2511.07942</guid>
<content:encoded><![CDATA[
<div> Imitation Learning, Robotic Tasks, Environment Dynamics, Offline Learning, Robustness<br />
<br />
Summary: The article introduces a new framework for offline Imitation Learning (IL) that addresses the challenge of dynamic environment shifts. The Balance Equation-based Distributionally Robust Offline Imitation Learning framework learns robust policies solely from expert demonstrations collected under nominal dynamics. By formulating the problem as a distributionally robust optimization over an uncertainty set of transition models, the framework seeks a policy that minimizes imitation loss under the worst-case transition distribution. Importantly, this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks show that this approach outperforms state-of-the-art offline IL baselines in terms of robustness and generalization, particularly under perturbed or shifted environments. <div>
arXiv:2511.07942v1 Announce Type: new 
Abstract: Imitation Learning (IL) has proven highly effective for robotic and control tasks where manually designing reward functions or explicit controllers is infeasible. However, standard IL methods implicitly assume that the environment dynamics remain fixed between training and deployment. In practice, this assumption rarely holds where modeling inaccuracies, real-world parameter variations, and adversarial perturbations can all induce shifts in transition dynamics, leading to severe performance degradation. We address this challenge through Balance Equation-based Distributionally Robust Offline Imitation Learning, a framework that learns robust policies solely from expert demonstrations collected under nominal dynamics, without requiring further environment interaction. We formulate the problem as a distributionally robust optimization over an uncertainty set of transition models, seeking a policy that minimizes the imitation loss under the worst-case transition distribution. Importantly, we show that this robust objective can be reformulated entirely in terms of the nominal data distribution, enabling tractable offline learning. Empirical evaluations on continuous-control benchmarks demonstrate that our approach achieves superior robustness and generalization compared to state-of-the-art offline IL baselines, particularly under perturbed or shifted environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continual Unlearning for Text-to-Image Diffusion Models: A Regularization Perspective</title>
<link>https://arxiv.org/abs/2511.07970</link>
<guid>https://arxiv.org/abs/2511.07970</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, text-to-image diffusion models, continual unlearning, utility collapse, parameter drift<br />
<br />
Summary:
Continual unlearning in text-to-image diffusion models is challenging due to rapid utility collapse where models forget retained knowledge after a few requests. This is traced to parameter drift from pre-training weights, highlighting the need for regularization. Regularizers are studied to mitigate drift and maintain compatibility with existing unlearning methods. Semantic awareness is crucial for preserving concepts near unlearning targets, proposing a gradient-projection method to constrain parameter drift. This approach significantly improves continual unlearning performance and complements other regularizers. The study establishes continual unlearning as a fundamental challenge in text-to-image generation and offers insights, baselines, and potential directions for advancing safe and accountable generative AI.<br /> <div>
arXiv:2511.07970v1 Announce Type: new 
Abstract: Machine unlearning--the ability to remove designated concepts from a pre-trained model--has advanced rapidly, particularly for text-to-image diffusion models. However, existing methods typically assume that unlearning requests arrive all at once, whereas in practice they often arrive sequentially. We present the first systematic study of continual unlearning in text-to-image diffusion models and show that popular unlearning methods suffer from rapid utility collapse: after only a few requests, models forget retained knowledge and generate degraded images. We trace this failure to cumulative parameter drift from the pre-training weights and argue that regularization is crucial to addressing it. To this end, we study a suite of add-on regularizers that (1) mitigate drift and (2) remain compatible with existing unlearning methods. Beyond generic regularizers, we show that semantic awareness is essential for preserving concepts close to the unlearning target, and propose a gradient-projection method that constrains parameter drift orthogonal to their subspace. This substantially improves continual unlearning performance and is complementary to other regularizers for further gains. Taken together, our study establishes continual unlearning as a fundamental challenge in text-to-image generation and provides insights, baselines, and open directions for advancing safe and accountable generative AI.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Rank Curvature for Zeroth-Order Optimization in LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.07971</link>
<guid>https://arxiv.org/abs/2511.07971</guid>
<content:encoded><![CDATA[
arXiv:2511.07971v1 Announce Type: new 
Abstract: We introduce LOREN, a curvature-aware zeroth-order (ZO) optimization method for fine-tuning large language models (LLMs). Existing ZO methods, which estimate gradients via finite differences using random perturbations, often suffer from high variance and suboptimal search directions. Our approach addresses these challenges by: (i) reformulating the problem of gradient preconditioning as that of adaptively estimating an anisotropic perturbation distribution for gradient estimation, (ii) capturing curvature through a low-rank block diagonal preconditioner using the framework of natural evolution strategies, and (iii) applying a REINFORCE leave-one-out (RLOO) gradient estimator to reduce variance. Experiments on standard LLM benchmarks show that our method outperforms state-of-the-art ZO methods by achieving higher accuracy and faster convergence, while cutting peak memory usage by up to 27.3% compared with MeZO-Adam.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable Insights for Graph Transformers in Theory and Practice</title>
<link>https://arxiv.org/abs/2511.08028</link>
<guid>https://arxiv.org/abs/2511.08028</guid>
<content:encoded><![CDATA[
arXiv:2511.08028v1 Announce Type: new 
Abstract: Graph Transformers (GTs) have shown strong empirical performance, yet current architectures vary widely in their use of attention mechanisms, positional embeddings (PEs), and expressivity. Existing expressivity results are often tied to specific design choices and lack comprehensive empirical validation on large-scale data. This leaves a gap between theory and practice, preventing generalizable insights that exceed particular application domains. Here, we propose the Generalized-Distance Transformer (GDT), a GT architecture using standard attention that incorporates many advancements for GTs from recent years, and develop a fine-grained understanding of the GDT's representation power in terms of attention and PEs. Through extensive experiments, we identify design choices that consistently perform well across various applications, tasks, and model scales, demonstrating strong performance in a few-shot transfer setting without fine-tuning. Our evaluation covers over eight million graphs with roughly 270M tokens across diverse domains, including image-based object detection, molecular property prediction, code summarization, and out-of-distribution algorithmic reasoning. We distill our theoretical and practical findings into several generalizable insights about effective GT design, training, and inference.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Sequential to Recursive: Enhancing Decision-Focused Learning with Bidirectional Feedback</title>
<link>https://arxiv.org/abs/2511.08035</link>
<guid>https://arxiv.org/abs/2511.08035</guid>
<content:encoded><![CDATA[
arXiv:2511.08035v1 Announce Type: new 
Abstract: Decision-focused learning (DFL) has emerged as a powerful end-to-end alternative to conventional predict-then-optimize (PTO) pipelines by directly optimizing predictive models through downstream decision losses. Existing DFL frameworks are limited by their strictly sequential structure, referred to as sequential DFL (S-DFL). However, S-DFL fails to capture the bidirectional feedback between prediction and optimization in complex interaction scenarios. In view of this, we first time propose recursive decision-focused learning (R-DFL), a novel framework that introduces bidirectional feedback between downstream optimization and upstream prediction. We further extend two distinct differentiation methods: explicit unrolling via automatic differentiation and implicit differentiation based on fixed-point methods, to facilitate efficient gradient propagation in R-DFL. We rigorously prove that both methods achieve comparable gradient accuracy, with the implicit method offering superior computational efficiency. Extensive experiments on both synthetic and real-world datasets, including the newsvendor problem and the bipartite matching problem, demonstrate that R-DFL not only substantially enhances the final decision quality over sequential baselines but also exhibits robust adaptability across diverse scenarios in closed-loop decision-making problems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaAct: Large Language Model Reasoning with Dynamic Action Spaces</title>
<link>https://arxiv.org/abs/2511.08043</link>
<guid>https://arxiv.org/abs/2511.08043</guid>
<content:encoded><![CDATA[
arXiv:2511.08043v1 Announce Type: new 
Abstract: In modern sequential decision-making systems, the construction of an optimal candidate action space is critical to efficient inference. However, existing approaches either rely on manually defined action spaces that lack scalability or utilize unstructured spaces that render exhaustive search computationally prohibitive. In this paper, we propose a novel framework named \textsc{DynaAct} for automatically constructing a compact action space to enhance sequential reasoning in complex problem-solving scenarios. Our method first estimates a proxy for the complete action space by extracting general sketches observed in a corpus covering diverse complex reasoning problems using large language models. We then formulate a submodular function that jointly evaluates candidate actions based on their utility to the current state and their diversity, and employ a greedy algorithm to select an optimal candidate set. Extensive experiments on six diverse standard benchmarks demonstrate that our approach significantly improves overall performance, while maintaining efficient inference without introducing substantial latency. The implementation is available at https://github.com/zhaoxlpku/DynaAct.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Linear Regression with Paid Stochastic Features</title>
<link>https://arxiv.org/abs/2511.08073</link>
<guid>https://arxiv.org/abs/2511.08073</guid>
<content:encoded><![CDATA[
arXiv:2511.08073v1 Announce Type: new 
Abstract: We study an online linear regression setting in which the observed feature vectors are corrupted by noise and the learner can pay to reduce the noise level. In practice, this may happen for several reasons: for example, because features can be measured more accurately using more expensive equipment, or because data providers can be incentivized to release less private features. Assuming feature vectors are drawn i.i.d. from a fixed but unknown distribution, we measure the learner's regret against the linear predictor minimizing a notion of loss that combines the prediction error and payment. When the mapping between payments and noise covariance is known, we prove that the rate $\sqrt{T}$ is optimal for regret if logarithmic factors are ignored. When the noise covariance is unknown, we show that the optimal regret rate becomes of order $T^{2/3}$ (ignoring log factors). Our analysis leverages matrix martingale concentration, showing that the empirical loss uniformly converges to the expected one for all payments and linear predictors.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Integrated Fusion Framework for Ensemble Learning Leveraging Gradient Boosting and Fuzzy Rule-Based Models</title>
<link>https://arxiv.org/abs/2511.08077</link>
<guid>https://arxiv.org/abs/2511.08077</guid>
<content:encoded><![CDATA[
arXiv:2511.08077v1 Announce Type: new 
Abstract: The integration of different learning paradigms has long been a focus of machine learning research, aimed at overcoming the inherent limitations of individual methods. Fuzzy rule-based models excel in interpretability and have seen widespread application across diverse fields. However, they face challenges such as complex design specifications and scalability issues with large datasets. The fusion of different techniques and strategies, particularly Gradient Boosting, with Fuzzy Rule-Based Models offers a robust solution to these challenges. This paper proposes an Integrated Fusion Framework that merges the strengths of both paradigms to enhance model performance and interpretability. At each iteration, a Fuzzy Rule-Based Model is constructed and controlled by a dynamic factor to optimize its contribution to the overall ensemble. This control factor serves multiple purposes: it prevents model dominance, encourages diversity, acts as a regularization parameter, and provides a mechanism for dynamic tuning based on model performance, thus mitigating the risk of overfitting. Additionally, the framework incorporates a sample-based correction mechanism that allows for adaptive adjustments based on feedback from a validation set. Experimental results substantiate the efficacy of the presented gradient boosting framework for fuzzy rule-based models, demonstrating performance enhancement, especially in terms of mitigating overfitting and complexity typically associated with many rules. By leveraging an optimal factor to govern the contribution of each model, the framework improves performance, maintains interpretability, and simplifies the maintenance and update of the models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Structure-Property Alignment for Data-Efficient Molecular Generation and Editing</title>
<link>https://arxiv.org/abs/2511.08080</link>
<guid>https://arxiv.org/abs/2511.08080</guid>
<content:encoded><![CDATA[
arXiv:2511.08080v1 Announce Type: new 
Abstract: Property-constrained molecular generation and editing are crucial in AI-driven drug discovery but remain hindered by two factors: (i) capturing the complex relationships between molecular structures and multiple properties remains challenging, and (ii) the narrow coverage and incomplete annotations of molecular properties weaken the effectiveness of property-based models. To tackle these limitations, we propose HSPAG, a data-efficient framework featuring hierarchical structure-property alignment. By treating SMILES and molecular properties as complementary modalities, the model learns their relationships at atom, substructure, and whole-molecule levels. Moreover, we select representative samples through scaffold clustering and hard samples via an auxiliary variational auto-encoder (VAE), substantially reducing the required pre-training data. In addition, we incorporate a property relevance-aware masking mechanism and diversified perturbation strategies to enhance generation quality under sparse annotations. Experiments demonstrate that HSPAG captures fine-grained structure-property relationships and supports controllable generation under multiple property constraints. Two real-world case studies further validate the editing capabilities of HSPAG.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HipKittens: Fast and Furious AMD Kernels</title>
<link>https://arxiv.org/abs/2511.08083</link>
<guid>https://arxiv.org/abs/2511.08083</guid>
<content:encoded><![CDATA[
arXiv:2511.08083v1 Announce Type: new 
Abstract: AMD GPUs offer state-of-the-art compute and memory bandwidth; however, peak performance AMD kernels are written in raw assembly. To address the difficulty of mapping AI algorithms to hardware, recent work proposes C++ embedded and PyTorch-inspired domain-specific languages like ThunderKittens (TK) to simplify high performance AI kernel development on NVIDIA hardware. We explore the extent to which such primitives -- for explicit tile-based programming with optimized memory accesses and fine-grained asynchronous execution across workers -- are NVIDIA-specific or general. We provide the first detailed study of the programming primitives that lead to performant AMD AI kernels, and we encapsulate these insights in the HipKittens (HK) programming framework. We find that tile-based abstractions used in prior DSLs generalize to AMD GPUs, however we need to rethink the algorithms that instantiate these abstractions for AMD. We validate the HK primitives across CDNA3 and CDNA4 AMD platforms. In evaluations, HK kernels compete with AMD's hand-optimized assembly kernels for GEMMs and attention, and consistently outperform compiler baselines. Moreover, assembly is difficult to scale to the breadth of AI workloads; reflecting this, in some settings HK outperforms all available kernel baselines by $1.2-2.4\times$ (e.g., $d=64$ attention, GQA backwards, memory-bound kernels). These findings help pave the way for a single, tile-based software layer for high-performance AI kernels that translates across GPU vendors. HipKittens is released at: https://github.com/HazyResearch/HipKittens.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Sparsity: Challenging Common Sparsity Assumptions for Learning World Models in Robotic Reinforcement Learning Benchmarks</title>
<link>https://arxiv.org/abs/2511.08086</link>
<guid>https://arxiv.org/abs/2511.08086</guid>
<content:encoded><![CDATA[
arXiv:2511.08086v1 Announce Type: new 
Abstract: The use of learned dynamics models, also known as world models, can improve the sample efficiency of reinforcement learning. Recent work suggests that the underlying causal graphs of such dynamics models are sparsely connected, with each of the future state variables depending only on a small subset of the current state variables, and that learning may therefore benefit from sparsity priors. Similarly, temporal sparsity, i.e. sparsely and abruptly changing local dynamics, has also been proposed as a useful inductive bias.
  In this work, we critically examine these assumptions by analyzing ground-truth dynamics from a set of robotic reinforcement learning environments in the MuJoCo Playground benchmark suite, aiming to determine whether the proposed notions of state and temporal sparsity actually tend to hold in typical reinforcement learning tasks.
  We study (i) whether the causal graphs of environment dynamics are sparse, (ii) whether such sparsity is state-dependent, and (iii) whether local system dynamics change sparsely.
  Our results indicate that global sparsity is rare, but instead the tasks show local, state-dependent sparsity in their dynamics and this sparsity exhibits distinct structures, appearing in temporally localized clusters (e.g., during contact events) and affecting specific subsets of state dimensions. These findings challenge common sparsity prior assumptions in dynamics learning, emphasizing the need for grounded inductive biases that reflect the state-dependent sparsity structure of real-world dynamics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stuart-Landau Oscillatory Graph Neural Network</title>
<link>https://arxiv.org/abs/2511.08094</link>
<guid>https://arxiv.org/abs/2511.08094</guid>
<content:encoded><![CDATA[
arXiv:2511.08094v1 Announce Type: new 
Abstract: Oscillatory Graph Neural Networks (OGNNs) are an emerging class of physics-inspired architectures designed to mitigate oversmoothing and vanishing gradient problems in deep GNNs. In this work, we introduce the Complex-Valued Stuart-Landau Graph Neural Network (SLGNN), a novel architecture grounded in Stuart-Landau oscillator dynamics. Stuart-Landau oscillators are canonical models of limit-cycle behavior near Hopf bifurcations, which are fundamental to synchronization theory and are widely used in e.g. neuroscience for mesoscopic brain modeling. Unlike harmonic oscillators and phase-only Kuramoto models, Stuart-Landau oscillators retain both amplitude and phase dynamics, enabling rich phenomena such as amplitude regulation and multistable synchronization. The proposed SLGNN generalizes existing phase-centric Kuramoto-based OGNNs by allowing node feature amplitudes to evolve dynamically according to Stuart-Landau dynamics, with explicit tunable hyperparameters (such as the Hopf-parameter and the coupling strength) providing additional control over the interplay between feature amplitudes and network structure. We conduct extensive experiments across node classification, graph classification, and graph regression tasks, demonstrating that SLGNN outperforms existing OGNNs and establishes a novel, expressive, and theoretically grounded framework for deep oscillatory architectures on graphs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A robust methodology for long-term sustainability evaluation of Machine Learning models</title>
<link>https://arxiv.org/abs/2511.08120</link>
<guid>https://arxiv.org/abs/2511.08120</guid>
<content:encoded><![CDATA[
arXiv:2511.08120v1 Announce Type: new 
Abstract: Sustainability and efficiency have become essential considerations in the development and deployment of Artificial Intelligence systems, yet existing regulatory and reporting practices lack standardized, model-agnostic evaluation protocols. Current assessments often measure only short-term experimental resource usage and disproportionately emphasize batch learning settings, failing to reflect real-world, long-term AI lifecycles. In this work, we propose a comprehensive evaluation protocol for assessing the long-term sustainability of ML models, applicable to both batch and streaming learning scenarios. Through experiments on diverse classification tasks using a range of model types, we demonstrate that traditional static train-test evaluations do not reliably capture sustainability under evolving data and repeated model updates. Our results show that long-term sustainability varies significantly across models, and in many cases, higher environmental cost yields little performance benefit.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SafeMIL: Learning Offline Safe Imitation Policy from Non-Preferred Trajectories</title>
<link>https://arxiv.org/abs/2511.08136</link>
<guid>https://arxiv.org/abs/2511.08136</guid>
<content:encoded><![CDATA[
arXiv:2511.08136v1 Announce Type: new 
Abstract: In this work, we study the problem of offline safe imitation learning (IL). In many real-world settings, online interactions can be risky, and accurately specifying the reward and the safety cost information at each timestep can be difficult. However, it is often feasible to collect trajectories reflecting undesirable or risky behavior, implicitly conveying the behavior the agent should avoid. We refer to these trajectories as non-preferred trajectories. Unlike standard IL, which aims to mimic demonstrations, our agent must also learn to avoid risky behavior using non-preferred trajectories. In this paper, we propose a novel approach, SafeMIL, to learn a parameterized cost that predicts if the state-action pair is risky via \textit{Multiple Instance Learning}. The learned cost is then used to avoid non-preferred behaviors, resulting in a policy that prioritizes safety. We empirically demonstrate that our approach can learn a safer policy that satisfies cost constraints without degrading the reward performance, thereby outperforming several baselines.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BIPPO: Budget-Aware Independent PPO for Energy-Efficient Federated Learning Services</title>
<link>https://arxiv.org/abs/2511.08142</link>
<guid>https://arxiv.org/abs/2511.08142</guid>
<content:encoded><![CDATA[
arXiv:2511.08142v1 Announce Type: new 
Abstract: Federated Learning (FL) is a promising machine learning solution in large-scale IoT systems, guaranteeing load distribution and privacy. However, FL does not natively consider infrastructure efficiency, a critical concern for systems operating in resource-constrained environments. Several Reinforcement Learning (RL) based solutions offer improved client selection for FL; however, they do not consider infrastructure challenges, such as resource limitations and device churn. Furthermore, the training of RL methods is often not designed for practical application, as these approaches frequently do not consider generalizability and are not optimized for energy efficiency. To fill this gap, we propose BIPPO (Budget-aware Independent Proximal Policy Optimization), which is an energy-efficient multi-agent RL solution that improves performance. We evaluate BIPPO on two image classification tasks run in a highly budget-constrained setting, with FL clients training on non-IID data, a challenging context for vanilla FL. The improved sampler of BIPPO enables it to increase the mean accuracy compared to non-RL mechanisms, traditional PPO, and IPPO. In addition, BIPPO only consumes a negligible proportion of the budget, which stays consistent even if the number of clients increases. Overall, BIPPO delivers a performant, stable, scalable, and sustainable solution for client selection in IoT-FL.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep (Predictive) Discounted Counterfactual Regret Minimization</title>
<link>https://arxiv.org/abs/2511.08174</link>
<guid>https://arxiv.org/abs/2511.08174</guid>
<content:encoded><![CDATA[
arXiv:2511.08174v1 Announce Type: new 
Abstract: Counterfactual regret minimization (CFR) is a family of algorithms for effectively solving imperfect-information games. To enhance CFR's applicability in large games, researchers use neural networks to approximate its behavior. However, existing methods are mainly based on vanilla CFR and struggle to effectively integrate more advanced CFR variants. In this work, we propose an efficient model-free neural CFR algorithm, overcoming the limitations of existing methods in approximating advanced CFR variants. At each iteration, it collects variance-reduced sampled advantages based on a value network, fits cumulative advantages by bootstrapping, and applies discounting and clipping operations to simulate the update mechanisms of advanced CFR variants. Experimental results show that, compared with model-free neural algorithms, it exhibits faster convergence in typical imperfect-information games and demonstrates stronger adversarial performance in a large poker game.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Long-Range Interactions in Graph Neural Simulators via Hamiltonian Dynamics</title>
<link>https://arxiv.org/abs/2511.08185</link>
<guid>https://arxiv.org/abs/2511.08185</guid>
<content:encoded><![CDATA[
arXiv:2511.08185v1 Announce Type: new 
Abstract: Learning to simulate complex physical systems from data has emerged as a promising way to overcome the limitations of traditional numerical solvers, which often require prohibitive computational costs for high-fidelity solutions. Recent Graph Neural Simulators (GNSs) accelerate simulations by learning dynamics on graph-structured data, yet often struggle to capture long-range interactions and suffer from error accumulation under autoregressive rollouts. To address these challenges, we propose Information-preserving Graph Neural Simulators (IGNS), a graph-based neural simulator built on the principles of Hamiltonian dynamics. This structure guarantees preservation of information across the graph, while extending to port-Hamiltonian systems allows the model to capture a broader class of dynamics, including non-conservative effects. IGNS further incorporates a warmup phase to initialize global context, geometric encoding to handle irregular meshes, and a multi-step training objective to reduce rollout error. To evaluate these properties systematically, we introduce new benchmarks that target long-range dependencies and challenging external forcing scenarios. Across all tasks, IGNS consistently outperforms state-of-the-art GNSs, achieving higher accuracy and stability under challenging and complex dynamical systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Online Patch Redundancy Eliminator (OPRE): A novel approach to online agnostic continual learning using dataset compression</title>
<link>https://arxiv.org/abs/2511.08226</link>
<guid>https://arxiv.org/abs/2511.08226</guid>
<content:encoded><![CDATA[
arXiv:2511.08226v1 Announce Type: new 
Abstract: In order to achieve Continual Learning (CL), the problem of catastrophic forgetting, one that has plagued neural networks since their inception, must be overcome. The evaluation of continual learning methods relies on splitting a known homogeneous dataset and learning the associated tasks one after the other. We argue that most CL methods introduce a priori information about the data to come and cannot be considered agnostic. We exemplify this point with the case of methods relying on pretrained feature extractors, which are still used in CL. After showing that pretrained feature extractors imply a loss of generality with respect to the data that can be learned by the model, we then discuss other kinds of a priori information introduced in other CL methods. We then present the Online Patch Redundancy Eliminator (OPRE), an online dataset compression algorithm, which, along with the training of a classifier at test time, yields performance on CIFAR-10 and CIFAR-100 superior to a number of other state-of-the-art online continual learning methods. Additionally, OPRE requires only minimal and interpretable hypothesis on the data to come. We suggest that online dataset compression could well be necessary to achieve fully agnostic CL.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Non-Stationary Time Series Forecasting with Temporal Stabilization and Frequency Differencing</title>
<link>https://arxiv.org/abs/2511.08229</link>
<guid>https://arxiv.org/abs/2511.08229</guid>
<content:encoded><![CDATA[
arXiv:2511.08229v1 Announce Type: new 
Abstract: Time series forecasting is critical for decision-making across dynamic domains such as energy, finance, transportation, and cloud computing. However, real-world time series often exhibit non-stationarity, including temporal distribution shifts and spectral variability, which pose significant challenges for long-term time series forecasting. In this paper, we propose DTAF, a dual-branch framework that addresses non-stationarity in both the temporal and frequency domains. For the temporal domain, the Temporal Stabilizing Fusion (TFS) module employs a non-stationary mix of experts (MOE) filter to disentangle and suppress temporal non-stationary patterns while preserving long-term dependencies. For the frequency domain, the Frequency Wave Modeling (FWM) module applies frequency differencing to dynamically highlight components with significant spectral shifts. By fusing the complementary outputs of TFS and FWM, DTAF generates robust forecasts that adapt to both temporal and frequency domain non-stationarity. Extensive experiments on real-world benchmarks demonstrate that DTAF outperforms state-of-the-art baselines, yielding significant improvements in forecasting accuracy under non-stationary conditions. All codes are available at https://github.com/PandaJunk/DTAF.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrefPoE: Advantage-Guided Preference Fusion for Learning Where to Explore</title>
<link>https://arxiv.org/abs/2511.08241</link>
<guid>https://arxiv.org/abs/2511.08241</guid>
<content:encoded><![CDATA[
arXiv:2511.08241v1 Announce Type: new 
Abstract: Exploration in reinforcement learning remains a critical challenge, as naive entropy maximization often results in high variance and inefficient policy updates. We introduce \textbf{PrefPoE}, a novel \textit{Preference-Product-of-Experts} framework that performs intelligent, advantage-guided exploration via the first principled application of product-of-experts (PoE) fusion for single-task exploration-exploitation balancing. By training a preference network to concentrate probability mass on high-advantage actions and fusing it with the main policy through PoE, PrefPoE creates a \textbf{soft trust region} that stabilizes policy updates while maintaining targeted exploration. Across diverse control tasks spanning both continuous and discrete action spaces, PrefPoE demonstrates consistent improvements: +321\% on HalfCheetah-v4 (1276~$\rightarrow$~5375), +69\% on Ant-v4, +276\% on LunarLander-v2, with consistently enhanced training stability and sample efficiency. Unlike standard PPO, which suffers from entropy collapse, PrefPoE sustains adaptive exploration through its unique dynamics, thereby preventing premature convergence and enabling superior performance. Our results establish that learning \textit{where to explore} through advantage-guided preferences is as crucial as learning how to act, offering a general framework for enhancing policy gradient methods across the full spectrum of reinforcement learning domains. Code and pretrained models are available in supplementary materials.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Geometric Field Theory Framework for Transformers: From Manifold Embeddings to Kernel Modulation</title>
<link>https://arxiv.org/abs/2511.08243</link>
<guid>https://arxiv.org/abs/2511.08243</guid>
<content:encoded><![CDATA[
arXiv:2511.08243v1 Announce Type: new 
Abstract: The Transformer architecture has achieved tremendous success in natural language processing, computer vision, and scientific computing through its self-attention mechanism. However, its core components-positional encoding and attention mechanisms-have lacked a unified physical or mathematical interpretation. This paper proposes a structural theoretical framework that integrates positional encoding, kernel integral operators, and attention mechanisms for in-depth theoretical investigation. We map discrete positions (such as text token indices and image pixel coordinates) to spatial functions on continuous manifolds, enabling a field-theoretic interpretation of Transformer layers as kernel-modulated operators acting over embedded manifolds.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Discovery of Feature Groups in Clinical Time Series</title>
<link>https://arxiv.org/abs/2511.08260</link>
<guid>https://arxiv.org/abs/2511.08260</guid>
<content:encoded><![CDATA[
arXiv:2511.08260v1 Announce Type: new 
Abstract: Clinical time series data are critical for patient monitoring and predictive modeling. These time series are typically multivariate and often comprise hundreds of heterogeneous features from different data sources. The grouping of features based on similarity and relevance to the prediction task has been shown to enhance the performance of deep learning architectures. However, defining these groups a priori using only semantic knowledge is challenging, even for domain experts. To address this, we propose a novel method that learns feature groups by clustering weights of feature-wise embedding layers. This approach seamlessly integrates into standard supervised training and discovers the groups that directly improve downstream performance on clinically relevant tasks. We demonstrate that our method outperforms static clustering approaches on synthetic data and achieves performance comparable to expert-defined groups on real-world medical data. Moreover, the learned feature groups are clinically interpretable, enabling data-driven discovery of task-relevant relationships between variables.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Explanation Evaluation under the Retraining Scheme</title>
<link>https://arxiv.org/abs/2511.08281</link>
<guid>https://arxiv.org/abs/2511.08281</guid>
<content:encoded><![CDATA[
arXiv:2511.08281v1 Announce Type: new 
Abstract: Feature attribution has gained prominence as a tool for explaining model decisions, yet evaluating explanation quality remains challenging due to the absence of ground-truth explanations. To circumvent this, explanation-guided input manipulation has emerged as an indirect evaluation strategy, measuring explanation effectiveness through the impact of input modifications on model outcomes during inference. Despite the widespread use, a major concern with inference-based schemes is the distribution shift caused by such manipulations, which undermines the reliability of their assessments. The retraining-based scheme ROAR overcomes this issue by adapting the model to the altered data distribution. However, its evaluation results often contradict the theoretical foundations of widely accepted explainers. This work investigates this misalignment between empirical observations and theoretical expectations. In particular, we identify the sign issue as a key factor responsible for residual information that ultimately distorts retraining-based evaluation. Based on the analysis, we show that a straightforward reframing of the evaluation process can effectively resolve the identified issue. Building on the existing framework, we further propose novel variants that jointly structure a comprehensive perspective on explanation evaluation. These variants largely improve evaluation efficiency over the standard retraining protocol, thereby enhancing practical applicability for explainer selection and benchmarking. Following our proposed schemes, empirical results across various data scales provide deeper insights into the performance of carefully selected explainers, revealing open challenges and future directions in explainability research.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Kernel Graph Community Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.08287</link>
<guid>https://arxiv.org/abs/2511.08287</guid>
<content:encoded><![CDATA[
arXiv:2511.08287v1 Announce Type: new 
Abstract: Graph Contrastive Learning (GCL) has emerged as a powerful paradigm for training Graph Neural Networks (GNNs) in the absence of task-specific labels. However, its scalability on large-scale graphs is hindered by the intensive message passing mechanism of GNN and the quadratic computational complexity of contrastive loss over positive and negative node pairs. To address these issues, we propose an efficient GCL framework that transforms the input graph into a compact network of interconnected node sets while preserving structural information across communities. We firstly introduce a kernelized graph community contrastive loss with linear complexity, enabling effective information transfer among node sets to capture hierarchical structural information of the graph. We then incorporate a knowledge distillation technique into the decoupled GNN architecture to accelerate inference while maintaining strong generalization performance. Extensive experiments on sixteen real-world datasets of varying scales demonstrate that our method outperforms state-of-the-art GCL baselines in both effectiveness and scalability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-time Diverse Reasoning by Riemannian Activation Steering</title>
<link>https://arxiv.org/abs/2511.08305</link>
<guid>https://arxiv.org/abs/2511.08305</guid>
<content:encoded><![CDATA[
arXiv:2511.08305v1 Announce Type: new 
Abstract: Best-of-$N$ reasoning improves the accuracy of language models in solving complex tasks by sampling multiple candidate solutions and then selecting the best one based on some criteria. A critical bottleneck for this strategy is the output diversity limit, which occurs when the model generates similar outputs despite stochastic sampling, and hence recites the same error. To address this lack of variance in reasoning paths, we propose a novel unsupervised activation steering strategy that simultaneously optimizes the steering vectors for multiple reasoning trajectories at test time. At any synchronization anchor along the batch generation process, we find the steering vectors that maximize the total volume spanned by all possible intervened activation subsets. We demonstrate that these steering vectors can be determined by solving a Riemannian optimization problem over the product of spheres with a log-determinant objective function. We then use a Riemannian block-coordinate descent algorithm with a well-tuned learning rate to obtain a stationary point of the problem, and we apply these steering vectors until the generation process reaches the subsequent synchronization anchor. Empirical evaluations on popular mathematical benchmarks demonstrate that our test-time Riemannian activation steering strategy outperforms vanilla sampling techniques in terms of generative diversity and solution accuracy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the accuracy and generalizability of molecular property regression models with a substructure-substitution-rule-informed framework</title>
<link>https://arxiv.org/abs/2511.08314</link>
<guid>https://arxiv.org/abs/2511.08314</guid>
<content:encoded><![CDATA[
arXiv:2511.08314v1 Announce Type: new 
Abstract: Artificial Intelligence (AI)-aided drug discovery is an active research field, yet AI models often exhibit poor accuracy in regression tasks for molecular property prediction, and perform catastrophically poorly for out-of-distribution (OOD) molecules. Here, we present MolRuleLoss, a substructure-substitution-rule-informed framework that improves the accuracy and generalizability of multiple molecular property regression models (MPRMs) such as GEM and UniMol for diverse molecular property prediction tasks. MolRuleLoss incorporates partial derivative constraints for substructure substitution rules (SSRs) into an MPRM's loss function. When using GEM models for predicting lipophilicity, water solubility, and solvation-free energy (using lipophilicity, ESOL, and freeSolv datasets from MoleculeNet), the root mean squared error (RMSE) values with and without MolRuleLoss were 0.587 vs. 0.660, 0.777 vs. 0.798, and 1.252 vs. 1.877, respectively, representing 2.6-33.3% performance improvements. We show that both the number and the quality of SSRs contribute to the magnitude of prediction accuracy gains obtained upon adding MolRuleLoss to an MPRM. MolRuleLoss improved the generalizability of MPRMs for "activity cliff" molecules in a lipophilicity prediction task and improved the generalizability of MPRMs for OOD molecules in a melting point prediction task. In a molecular weight prediction task for OOD molecules, MolRuleLoss reduced the RMSE value of a GEM model from 29.507 to 0.007. We also provide a formal demonstration that the upper bound of the variation for property change of SSRs is positively correlated with an MPRM's error. Together, we show that using the MolRuleLoss framework as a bolt-on boosts the prediction accuracy and generalizability of multiple MPRMs, supporting diverse applications in areas like cheminformatics and AI-aided drug discovery.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Bias: Data Poisoning Attacks on Fairness</title>
<link>https://arxiv.org/abs/2511.08331</link>
<guid>https://arxiv.org/abs/2511.08331</guid>
<content:encoded><![CDATA[
arXiv:2511.08331v1 Announce Type: new 
Abstract: With the growing adoption of AI and machine learning systems in real-world applications, ensuring their fairness has become increasingly critical. The majority of the work in algorithmic fairness focus on assessing and improving the fairness of machine learning systems. There is relatively little research on fairness vulnerability, i.e., how an AI system's fairness can be intentionally compromised. In this work, we first provide a theoretical analysis demonstrating that a simple adversarial poisoning strategy is sufficient to induce maximally unfair behavior in naive Bayes classifiers. Our key idea is to strategically inject a small fraction of carefully crafted adversarial data points into the training set, biasing the model's decision boundary to disproportionately affect a protected group while preserving generalizable performance. To illustrate the practical effectiveness of our method, we conduct experiments across several benchmark datasets and models. We find that our attack significantly outperforms existing methods in degrading fairness metrics across multiple models and datasets, often achieving substantially higher levels of unfairness with a comparable or only slightly worse impact on accuracy. Notably, our method proves effective on a wide range of models, in contrast to prior work, demonstrating a robust and potent approach to compromising the fairness of machine learning systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LPPG-RL: Lexicographically Projected Policy Gradient Reinforcement Learning with Subproblem Exploration</title>
<link>https://arxiv.org/abs/2511.08339</link>
<guid>https://arxiv.org/abs/2511.08339</guid>
<content:encoded><![CDATA[
arXiv:2511.08339v1 Announce Type: new 
Abstract: Lexicographic multi-objective problems, which consist of multiple conflicting subtasks with explicit priorities, are common in real-world applications. Despite the advantages of Reinforcement Learning (RL) in single tasks, extending conventional RL methods to prioritized multiple objectives remains challenging. In particular, traditional Safe RL and Multi-Objective RL (MORL) methods have difficulty enforcing priority orderings efficiently. Therefore, Lexicographic Multi-Objective RL (LMORL) methods have been developed to address these challenges. However, existing LMORL methods either rely on heuristic threshold tuning with prior knowledge or are restricted to discrete domains. To overcome these limitations, we propose Lexicographically Projected Policy Gradient RL (LPPG-RL), a novel LMORL framework which leverages sequential gradient projections to identify feasible policy update directions, thereby enabling LPPG-RL broadly compatible with all policy gradient algorithms in continuous spaces. LPPG-RL reformulates the projection step as an optimization problem, and utilizes Dykstra's projection rather than generic solvers to deliver great speedups, especially for small- to medium-scale instances. In addition, LPPG-RL introduces Subproblem Exploration (SE) to prevent gradient vanishing, accelerate convergence and enhance stability. We provide theoretical guarantees for convergence and establish a lower bound on policy improvement. Finally, through extensive experiments in a 2D navigation environment, we demonstrate the effectiveness of LPPG-RL, showing that it outperforms existing state-of-the-art continuous LMORL methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08340</link>
<guid>https://arxiv.org/abs/2511.08340</guid>
<content:encoded><![CDATA[
arXiv:2511.08340v1 Announce Type: new 
Abstract: Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Confusion to Clarity: ProtoScore - A Framework for Evaluating Prototype-Based XAI</title>
<link>https://arxiv.org/abs/2511.08361</link>
<guid>https://arxiv.org/abs/2511.08361</guid>
<content:encoded><![CDATA[
arXiv:2511.08361v1 Announce Type: new 
Abstract: The complexity and opacity of neural networks (NNs) pose significant challenges, particularly in high-stakes fields such as healthcare, finance, and law, where understanding decision-making processes is crucial. To address these issues, the field of explainable artificial intelligence (XAI) has developed various methods aimed at clarifying AI decision-making, thereby facilitating appropriate trust and validating the fairness of outcomes. Among these methods, prototype-based explanations offer a promising approach that uses representative examples to elucidate model behavior. However, a critical gap exists regarding standardized benchmarks to objectively compare prototype-based XAI methods, especially in the context of time series data. This lack of reliable benchmarks results in subjective evaluations, hindering progress in the field. We aim to establish a robust framework, ProtoScore, for assessing prototype-based XAI methods across different data types with a focus on time series data, facilitating fair and comprehensive evaluations. By integrating the Co-12 properties of Nauta et al., this framework allows for effectively comparing prototype methods against each other and against other XAI methods, ultimately assisting practitioners in selecting appropriate explanation methods while minimizing the costs associated with user studies. All code is publicly available at https://github.com/HelenaM23/ProtoScore .
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-objective Hyperparameter Optimization in the Age of Deep Learning</title>
<link>https://arxiv.org/abs/2511.08371</link>
<guid>https://arxiv.org/abs/2511.08371</guid>
<content:encoded><![CDATA[
arXiv:2511.08371v1 Announce Type: new 
Abstract: While Deep Learning (DL) experts often have prior knowledge about which hyperparameter settings yield strong performance, only few Hyperparameter Optimization (HPO) algorithms can leverage such prior knowledge and none incorporate priors over multiple objectives. As DL practitioners often need to optimize not just one but many objectives, this is a blind spot in the algorithmic landscape of HPO. To address this shortcoming, we introduce PriMO, the first HPO algorithm that can integrate multi-objective user beliefs. We show PriMO achieves state-of-the-art performance across 8 DL benchmarks in the multi-objective and single-objective setting, clearly positioning itself as the new go-to HPO algorithm for DL practitioners.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMAformer: Enhancing Transformer through Embedding Armor for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08396</link>
<guid>https://arxiv.org/abs/2511.08396</guid>
<content:encoded><![CDATA[
arXiv:2511.08396v1 Announce Type: new 
Abstract: Multivariate time series forecasting is crucial across a wide range of domains. While presenting notable progress for the Transformer architecture, iTransformer still lags behind the latest MLP-based models. We attribute this performance gap to unstable inter-channel relationships. To bridge this gap, we propose EMAformer, a simple yet effective model that enhances the Transformer with an auxiliary embedding suite, akin to armor that reinforces its ability. By introducing three key inductive biases, i.e., \textit{global stability}, \textit{phase sensitivity}, and \textit{cross-axis specificity}, EMAformer unlocks the further potential of the Transformer architecture, achieving state-of-the-art performance on 12 real-world benchmarks and reducing forecasting errors by an average of 2.73\% in MSE and 5.15\% in MAE. This significantly advances the practical applicability of Transformer-based approaches for multivariate time series forecasting. The code is available on https://github.com/PlanckChang/EMAformer.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning by Misaligning: Boundary-aware Curriculum Learning for Multimodal Alignment</title>
<link>https://arxiv.org/abs/2511.08399</link>
<guid>https://arxiv.org/abs/2511.08399</guid>
<content:encoded><![CDATA[
arXiv:2511.08399v1 Announce Type: new 
Abstract: Most multimodal models treat every negative pair alike, ignoring the ambiguous negatives that differ from the positive by only a small detail. We propose Boundary-Aware Curriculum with Local Attention (BACL), a lightweight add-on that turns these borderline cases into a curriculum signal. A Boundary-aware Negative Sampler gradually raises difficulty, while a Contrastive Local Attention loss highlights where the mismatch occurs. The two modules are fully differentiable and work with any off-the-shelf dual encoder. Theory predicts a fast O(1/n) error rate; practice shows up to +32% R@1 over CLIP and new SOTA on four large-scale benchmarks, all without extra labels.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARAC: Adaptive Regularized Multi-Agent Soft Actor-Critic in Graph-Structured Adversarial Games</title>
<link>https://arxiv.org/abs/2511.08412</link>
<guid>https://arxiv.org/abs/2511.08412</guid>
<content:encoded><![CDATA[
arXiv:2511.08412v1 Announce Type: new 
Abstract: In graph-structured multi-agent reinforcement learning (MARL) adversarial tasks such as pursuit and confrontation, agents must coordinate under highly dynamic interactions, where sparse rewards hinder efficient policy learning. We propose Adaptive Regularized Multi-Agent Soft Actor-Critic (ARAC), which integrates an attention-based graph neural network (GNN) for modeling agent dependencies with an adaptive divergence regularization mechanism. The GNN enables expressive representation of spatial relations and state features in graph environments. Divergence regularization can serve as policy guidance to alleviate the sparse reward problem, but it may lead to suboptimal convergence when the reference policy itself is imperfect. The adaptive divergence regularization mechanism enables the framework to exploit reference policies for efficient exploration in the early stages, while gradually reducing reliance on them as training progresses to avoid inheriting their limitations. Experiments in pursuit and confrontation scenarios demonstrate that ARAC achieves faster convergence, higher final success rates, and stronger scalability across varying numbers of agents compared with MARL baselines, highlighting its effectiveness in complex graph-structured environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuCLIP: Efficient Large-Scale CLIP Training with Neural Normalizer Optimization</title>
<link>https://arxiv.org/abs/2511.08417</link>
<guid>https://arxiv.org/abs/2511.08417</guid>
<content:encoded><![CDATA[
arXiv:2511.08417v1 Announce Type: new 
Abstract: Accurately estimating the normalization term (also known as the partition function) in the contrastive loss is a central challenge for training Contrastive Language-Image Pre-training (CLIP) models. Conventional methods rely on large batches for approximation, demanding substantial computational resources. To mitigate this issue, prior works introduced per-sample normalizer estimators, which are updated at each epoch in a blockwise coordinate manner to keep track of updated encoders. However, this scheme incurs optimization error that scales with the ratio of dataset size to batch size, limiting effectiveness for large datasets or small batches. To overcome this limitation, we propose NeuCLIP, a novel and elegant optimization framework based on two key ideas: (i) $\textbf{reformulating}$ the contrastive loss for each sample $\textbf{via convex analysis}$ into a minimization problem with an auxiliary variable representing its log-normalizer; and (ii) $\textbf{transforming}$ the resulting minimization over $n$ auxiliary variables (where $n$ is the dataset size) via $\textbf{variational analysis}$ into the minimization over a compact neural network that predicts the log-normalizers. We design an alternating optimization algorithm that jointly trains the CLIP model and the auxiliary network. By employing a tailored architecture and acceleration techniques for the auxiliary network, NeuCLIP achieves more accurate normalizer estimation, leading to improved performance compared with previous methods. Extensive experiments on large-scale CLIP training, spanning datasets from millions to billions of samples, demonstrate that NeuCLIP outperforms previous methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Operators for Cardiac Electrophysiology</title>
<link>https://arxiv.org/abs/2511.08418</link>
<guid>https://arxiv.org/abs/2511.08418</guid>
<content:encoded><![CDATA[
arXiv:2511.08418v1 Announce Type: new 
Abstract: Accurately simulating systems governed by PDEs, such as voltage fields in cardiac electrophysiology (EP) modelling, remains a significant modelling challenge. Traditional numerical solvers are computationally expensive and sensitive to discretisation, while canonical deep learning methods are data-hungry and struggle with chaotic dynamics and long-term predictions. Physics-Informed Neural Networks (PINNs) mitigate some of these issues by incorporating physical constraints in the learning process, yet they remain limited by mesh resolution and long-term predictive stability. In this work, we propose a Physics-Informed Neural Operator (PINO) approach to solve PDE problems in cardiac EP. Unlike PINNs, PINO models learn mappings between function spaces, allowing them to generalise to multiple mesh resolutions and initial conditions. Our results show that PINO models can accurately reproduce cardiac EP dynamics over extended time horizons and across multiple propagation scenarios, including zero-shot evaluations on scenarios unseen during training. Additionally, our PINO models maintain high predictive quality in long roll-outs (where predictions are recursively fed back as inputs), and can scale their predictive resolution by up to 10x the training resolution. These advantages come with a significant reduction in simulation time compared to numerical PDE solvers, highlighting the potential of PINO-based approaches for efficient and scalable cardiac EP simulations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HardFlow: Hard-Constrained Sampling for Flow-Matching Models via Trajectory Optimization</title>
<link>https://arxiv.org/abs/2511.08425</link>
<guid>https://arxiv.org/abs/2511.08425</guid>
<content:encoded><![CDATA[
arXiv:2511.08425v1 Announce Type: new 
Abstract: Diffusion and flow-matching have emerged as powerful methodologies for generative modeling, with remarkable success in capturing complex data distributions and enabling flexible guidance at inference time. Many downstream applications, however, demand enforcing hard constraints on generated samples (for example, robot trajectories must avoid obstacles), a requirement that goes beyond simple guidance. Prevailing projection-based approaches constrain the entire sampling path to the constraint manifold, which is overly restrictive and degrades sample quality. In this paper, we introduce a novel framework that reformulates hard-constrained sampling as a trajectory optimization problem. Our key insight is to leverage numerical optimal control to steer the sampling trajectory so that constraints are satisfied precisely at the terminal time. By exploiting the underlying structure of flow-matching models and adopting techniques from model predictive control, we transform this otherwise complex constrained optimization problem into a tractable surrogate that can be solved efficiently and effectively. Furthermore, this trajectory optimization perspective offers significant flexibility beyond mere constraint satisfaction, allowing for the inclusion of integral costs to minimize distribution shift and terminal objectives to further enhance sample quality, all within a unified framework. We provide a control-theoretic analysis of our method, establishing bounds on the approximation error between our tractable surrogate and the ideal formulation. Extensive experiments across diverse domains, including robotics (planning), partial differential equations (boundary control), and vision (text-guided image editing), demonstrate that our algorithm, which we name $\textit{HardFlow}$, substantially outperforms existing methods in both constraint satisfaction and sample quality.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An update to PYRO-NN: A Python Library for Differentiable CT Operators</title>
<link>https://arxiv.org/abs/2511.08427</link>
<guid>https://arxiv.org/abs/2511.08427</guid>
<content:encoded><![CDATA[
arXiv:2511.08427v1 Announce Type: new 
Abstract: Deep learning has brought significant advancements to X-ray Computed Tomography (CT) reconstruction, offering solutions to challenges arising from modern imaging technologies. These developments benefit from methods that combine classical reconstruction techniques with data-driven approaches. Differentiable operators play a key role in this integration by enabling end-to-end optimization and the incorporation of physical modeling within neural networks.
  In this work, we present an updated version of PYRO-NN, a Python-based library for differentiable CT reconstruction. The updated framework extends compatibility to PyTorch and introduces native CUDA kernel support for efficient projection and back-projection operations across parallel, fan, and cone-beam geometries. Additionally, it includes tools for simulating imaging artifacts, modeling arbitrary acquisition trajectories, and creating flexible, end-to-end trainable pipelines through a high-level Python API. Code is available at: https://github.com/csyben/PYRO-NN
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherence Mechanisms for Provable Self-Improvement</title>
<link>https://arxiv.org/abs/2511.08440</link>
<guid>https://arxiv.org/abs/2511.08440</guid>
<content:encoded><![CDATA[
arXiv:2511.08440v1 Announce Type: new 
Abstract: Self-improvement is a critical capability for large language models and other intelligent systems, enabling them to refine their behavior and internal consistency without external supervision. Despite its importance, prior approaches largely rely on empirical heuristics and lack formal guarantees. In this paper, we propose a principled framework for self-improvement based on the concept of \emph{coherence}, which requires that a model's outputs remain consistent under task-preserving transformations of the input.
  We formalize this concept using projection-based mechanisms that update a baseline model to be coherent while remaining as close as possible to its original behavior. We provide rigorous theoretical guarantees that these mechanisms achieve \emph{monotonic improvement}, measured by a reduction in expected Bregman divergence. Our analysis is comprehensive, covering both \emph{direct} and \emph{two-step} projection methods, and robustly extends these guarantees to non-realizable settings, empirical (finite-sample) distributions, and relaxed coherence constraints.
  Furthermore, we establish a general \emph{characterization theorem}, showing that any mechanism with similar provable improvement guarantees must inherently conform to a coherence-based structure. This culminates in rigidity results under the demand for universal improvement, establishing coherence as a fundamental and, in a formal sense, necessary principle for provable self-improvement.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Model for All: Universal Pre-training for EEG based Emotion Recognition across Heterogeneous Datasets and Paradigms</title>
<link>https://arxiv.org/abs/2511.08444</link>
<guid>https://arxiv.org/abs/2511.08444</guid>
<content:encoded><![CDATA[
arXiv:2511.08444v1 Announce Type: new 
Abstract: EEG-based emotion recognition is hampered by profound dataset heterogeneity (channel/subject variability), hindering generalizable models. Existing approaches struggle to transfer knowledge effectively. We propose 'One Model for All', a universal pre-training framework for EEG analysis across disparate datasets. Our paradigm decouples learning into two stages: (1) Univariate pre-training via self-supervised contrastive learning on individual channels, enabled by a Unified Channel Schema (UCS) that leverages the channel union (e.g., SEED-62ch, DEAP-32ch); (2) Multivariate fine-tuning with a novel 'ART' (Adaptive Resampling Transformer) and 'GAT' (Graph Attention Network) architecture to capture complex spatio-temporal dependencies. Experiments show universal pre-training is an essential stabilizer, preventing collapse on SEED (vs. scratch) and yielding substantial gains on DEAP (+7.65%) and DREAMER (+3.55%). Our framework achieves new SOTA performance on all within-subject benchmarks: SEED (99.27%), DEAP (93.69%), and DREAMER (93.93%). We also show SOTA cross-dataset transfer, achieving 94.08% (intersection) and 93.05% (UCS) on the unseen DREAMER dataset, with the former surpassing the within-domain pre-training benchmark. Ablation studies validate our architecture: the GAT module is critical, yielding a +22.19% gain over GCN on the high-noise DEAP dataset, and its removal causes a catastrophic -16.44% performance drop. This work paves the way for more universal, scalable, and effective pre-trained models for diverse EEG analysis tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Binary Split Categorical feature with Mean Absolute Error Criteria in CART</title>
<link>https://arxiv.org/abs/2511.08470</link>
<guid>https://arxiv.org/abs/2511.08470</guid>
<content:encoded><![CDATA[
arXiv:2511.08470v1 Announce Type: new 
Abstract: In the context of the Classification and Regression Trees (CART) algorithm, the efficient splitting of categorical features using standard criteria like GINI and Entropy is well-established. However, using the Mean Absolute Error (MAE) criterion for categorical features has traditionally relied on various numerical encoding methods. This paper demonstrates that unsupervised numerical encoding methods are not viable for the MAE criteria. Furthermore, we present a novel and efficient splitting algorithm that addresses the challenges of handling categorical features with the MAE criterion. Our findings underscore the limitations of existing approaches and offer a promising solution to enhance the handling of categorical data in CART algorithms.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clustering Guided Residual Neural Networks for Multi-Tx Localization in Molecular Communications</title>
<link>https://arxiv.org/abs/2511.08513</link>
<guid>https://arxiv.org/abs/2511.08513</guid>
<content:encoded><![CDATA[
arXiv:2511.08513v1 Announce Type: new 
Abstract: Transmitter localization in Molecular Communication via Diffusion is a critical topic with many applications. However, accurate localization of multiple transmitters is a challenging problem due to the stochastic nature of diffusion and overlapping molecule distributions at the receiver surface. To address these issues, we introduce clustering-based centroid correction methods that enhance robustness against density variations, and outliers. In addition, we propose two clusteringguided Residual Neural Networks, namely AngleNN for direction refinement and SizeNN for cluster size estimation. Experimental results show that both approaches provide significant improvements with reducing localization error between 69% (2-Tx) and 43% (4-Tx) compared to the K-means.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeJEPA: Provable and Scalable Self-Supervised Learning Without the Heuristics</title>
<link>https://arxiv.org/abs/2511.08544</link>
<guid>https://arxiv.org/abs/2511.08544</guid>
<content:encoded><![CDATA[
arXiv:2511.08544v1 Announce Type: new 
Abstract: Learning manipulable representations of the world and its dynamics is central to AI. Joint-Embedding Predictive Architectures (JEPAs) offer a promising blueprint, but lack of practical guidance and theory has led to ad-hoc R&amp;D. We present a comprehensive theory of JEPAs and instantiate it in {\bf LeJEPA}, a lean, scalable, and theoretically grounded training objective. First, we identify the isotropic Gaussian as the optimal distribution that JEPAs' embeddings should follow to minimize downstream prediction risk. Second, we introduce a novel objective--{\bf Sketched Isotropic Gaussian Regularization} (SIGReg)--to constrain embeddings to reach that ideal distribution. Combining the JEPA predictive loss with SIGReg yields LeJEPA with numerous theoretical and practical benefits: (i) single trade-off hyperparameter, (ii) linear time and memory complexity, (iii) stability across hyper-parameters, architectures (ResNets, ViTs, ConvNets) and domains, (iv) heuristics-free, e.g., no stop-gradient, no teacher-student, no hyper-parameter schedulers, and (v) distributed training-friendly implementation requiring only $\approx$50 lines of code. Our empirical validation covers 10+ datasets, 60+ architectures, all with varying scales and domains. As an example, using imagenet-1k for pretraining and linear evaluation with frozen backbone, LeJEPA reaches 79\% with a ViT-H/14. We hope that the simplicity and theory-friendly ecosystem offered by LeJEPA will reestablish self-supervised pre-training as a core pillar of AI research (\href{git@github.com:rbalestr-lab/lejepa.git}{GitHub repo}).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMMI: Flow Matching Mutual Information Estimation</title>
<link>https://arxiv.org/abs/2511.08552</link>
<guid>https://arxiv.org/abs/2511.08552</guid>
<content:encoded><![CDATA[
arXiv:2511.08552v1 Announce Type: new 
Abstract: We introduce a novel Mutual Information (MI) estimator that fundamentally reframes the discriminative approach. Instead of training a classifier to discriminate between joint and marginal distributions, we learn a normalizing flow that transforms one into the other. This technique produces a computationally efficient and precise MI estimate that scales well to high dimensions and across a wide range of ground-truth MI values.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Path Not Taken: RLVR Provably Learns Off the Principals</title>
<link>https://arxiv.org/abs/2511.08567</link>
<guid>https://arxiv.org/abs/2511.08567</guid>
<content:encoded><![CDATA[
arXiv:2511.08567v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) reliably improves the reasoning performance of large language models, yet it appears to modify only a small fraction of parameters. We revisit this paradox and show that sparsity is a surface artifact of a model-conditioned optimization bias: for a fixed pretrained model, updates consistently localize to preferred parameter regions, highly consistent across runs and largely invariant to datasets and RL recipes. We mechanistically explain these dynamics with a Three-Gate Theory: Gate I (KL Anchor) imposes a KL-constrained update; Gate II (Model Geometry) steers the step off principal directions into low-curvature, spectrum-preserving subspaces; and Gate III (Precision) hides micro-updates in non-preferred regions, making the off-principal bias appear as sparsity. We then validate this theory and, for the first time, provide a parameter-level characterization of RLVR's learning dynamics: RLVR learns off principal directions in weight space, achieving gains via minimal spectral drift, reduced principal-subspace rotation, and off-principal update alignment. In contrast, SFT targets principal weights, distorts the spectrum, and even lags RLVR.
  Together, these results provide the first parameter-space account of RLVR's training dynamics, revealing clear regularities in how parameters evolve. Crucially, we show that RL operates in a distinct optimization regime from SFT, so directly adapting SFT-era parameter-efficient fine-tuning (PEFT) methods can be flawed, as evidenced by our case studies on advanced sparse fine-tuning and LoRA variants. We hope this work charts a path toward a white-box understanding of RLVR and the design of geometry-aware, RLVR-native learning algorithms, rather than repurposed SFT-era heuristics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Grid Updates for Kolmogorov-Arnold Networks using Layer Histograms</title>
<link>https://arxiv.org/abs/2511.08570</link>
<guid>https://arxiv.org/abs/2511.08570</guid>
<content:encoded><![CDATA[
arXiv:2511.08570v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) are a class of neural networks that have received increased attention in recent literature. In contrast to MLPs, KANs leverage parameterized, trainable activation functions and offer several benefits including improved interpretability and higher accuracy on learning symbolic equations. However, the original KAN architecture requires adjustments to the domain discretization of the network (called the "domain grid") during training, creating extra overhead for the user in the training process. Typical KAN layers are not designed with the ability to autonomously update their domains in a data-driven manner informed by the changing output ranges of previous layers. As an added benefit, this histogram algorithm may also be applied towards detecting out-of-distribution (OOD) inputs in a variety of settings. We demonstrate that AdaptKAN exceeds or matches the performance of prior KAN architectures and MLPs on four different tasks: learning scientific equations from the Feynman dataset, image classification from frozen features, learning a control Lyapunov function, and detecting OOD inputs on the OpenOOD v1.5 benchmark.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synera: Synergistic LLM Serving across Device and Cloud at Scale</title>
<link>https://arxiv.org/abs/2511.07423</link>
<guid>https://arxiv.org/abs/2511.07423</guid>
<content:encoded><![CDATA[
arXiv:2511.07423v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are becoming key components in various mobile operating systems, driving smart applications like interactive chatbots and personal assistants. While bringing enhanced intelligence to mobile ends, their deployment suffers from a set of performance challenges, especially the generation quality degradation and prolonged latency. Prior works have mainly relied on solutions of cloud offloading or on-device Small Language Models (SLMs). However, the former is usually limited by the communication bottleneck, and the latter sacrifices generation quality due to resource constraints. To mitigate these limitations, this paper proposes Synera, a device-cloud synergistic LLM serving system that applies an efficient SLM-LLM synergistic mechanism. Through empirical studies on LLM's unique computing characteristics, Synera identifies a set of underexplored optimization opportunities in device-cloud synergistic LLM inference, including offloading decisions, pipeline stalls, and batching bottlenecks. To translate them into enhanced performance, Synera introduces tailored designs of communication-efficient selective offloading, stall-free parallel inference, and scalable cloud batching. Extensive evaluations with real-world testbeds show that Synera enables 1.20-5.47x better generation quality against competitive baselines with on-par latency performance. Compared with existing cloud serving, Synera achieves 8.2-16.5% lower cloud serving cost on various benchmarks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource Allocation in Hybrid Radio-Optical IoT Networks using GNN with Multi-task Learning</title>
<link>https://arxiv.org/abs/2511.07428</link>
<guid>https://arxiv.org/abs/2511.07428</guid>
<content:encoded><![CDATA[
arXiv:2511.07428v1 Announce Type: cross 
Abstract: This paper addresses the problem of dual-technology scheduling in hybrid Internet of Things (IoT) networks that integrate Optical Wireless Communication (OWC) alongside Radio Frequency (RF). We begin by formulating a Mixed-Integer Nonlinear Programming (MINLP) model that jointly considers throughput maximization and delay minimization between access points and IoT nodes under energy and link availability constraints. However, given the intractability of solving such NP-hard problems at scale and the impractical assumption of full channel observability, we propose the Dual-Graph Embedding with Transformer (DGET) framework, a supervised multi-task learning architecture combining a two-stage Graph Neural Networks (GNNs) with a Transformer-based encoder. The first stage employs a transductive GNN that encodes the known graph topology and initial node and link states. The second stage introduces an inductive GNN for temporal refinement, which learns to generalize these embeddings to the evolved states of the same network, capturing changes in energy and queue dynamics over time, by aligning them with ground-truth scheduling decisions through a consistency loss. These enriched embeddings are then processed by a classifier for the communication links with a Transformer encoder that captures cross-link dependencies through multi-head self-attention via classification loss. Simulation results show that hybrid RF-OWC networks outperform standalone RF systems by handling higher traffic loads more efficiently and reducing the Age of Information (AoI) by up to 20%, all while maintaining comparable energy consumption. The proposed DGET framework, compared to traditional optimization-based methods, achieves near-optimal scheduling with over 90% classification accuracy, reduces computational complexity, and demonstrates higher robustness under partial channel observability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-Exec: Impact-Aware Reinforcement Learning for Opportunistic Optimal Liquidation, Outperforms TWAP and a Book-Liquidity VWAP on BTC-USD Replays</title>
<link>https://arxiv.org/abs/2511.07434</link>
<guid>https://arxiv.org/abs/2511.07434</guid>
<content:encoded><![CDATA[
arXiv:2511.07434v1 Announce Type: cross 
Abstract: We study opportunistic optimal liquidation over fixed deadlines on BTC-USD limit-order books (LOB). We present RL-Exec, a PPO agent trained on historical replays augmented with endogenous transient impact (resilience), partial fills, maker/taker fees, and latency. The policy observes depth-20 LOB features plus microstructure indicators and acts under a sell-only inventory constraint to reach a residual target. Evaluation follows a strict time split (train: Jan-2020; test: Feb-2020) and a per-day protocol: for each test day we run ten independent start times and aggregate to a single daily score, avoiding pseudo-replication. We compare the agent to (i) TWAP and (ii) a VWAP-like baseline allocating using opposite-side order-book liquidity (top-20 levels), both executed on identical timestamps and costs. Statistical inference uses one-sided Wilcoxon signed-rank tests on daily RL-baseline differences with Benjamini-Hochberg FDR correction and bootstrap confidence intervals. On the Feb-2020 test set, RL-Exec significantly outperforms both baselines and the gap increases with the execution horizon (+2-3 bps at 30 min, +7-8 bps at 60 min, +23 bps at 120 min).
  Code: github.com/Giafferri/RL-Exec
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REFLEX: Reference-Free Evaluation of Log Summarization via Large Language Model Judgment</title>
<link>https://arxiv.org/abs/2511.07458</link>
<guid>https://arxiv.org/abs/2511.07458</guid>
<content:encoded><![CDATA[
arXiv:2511.07458v1 Announce Type: cross 
Abstract: Evaluating log summarization systems is challenging due to the lack of high-quality reference summaries and the limitations of existing metrics like ROUGE and BLEU, which depend on surface-level lexical overlap. We introduce REFLEX, a reference-free evaluation metric for log summarization based on large language model (LLM) judgment. REFLEX uses LLMs as zero-shot evaluators to assess summary quality along dimensions such as relevance, informativeness, and coherence, without requiring gold-standard references or human annotations. We show that REFLEX produces stable, interpretable, and fine-grained evaluations across multiple log summarization dataset, and more effectively distinguishes model outputs than traditional metrics. REFLEX provides a scalable alternative for evaluating log summaries in real-world settings where reference data is scarce or unavailable.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Hubs to Deserts: Urban Cultural Accessibility Patterns with Explainable AI</title>
<link>https://arxiv.org/abs/2511.07475</link>
<guid>https://arxiv.org/abs/2511.07475</guid>
<content:encoded><![CDATA[
arXiv:2511.07475v1 Announce Type: cross 
Abstract: Cultural infrastructures, such as libraries, museums, theaters, and galleries, support learning, civic life, health, and local economies, yet access is uneven across cities. We present a novel, scalable, and open-data framework to measure spatial equity in cultural access. We map cultural infrastructures and compute a metric called Cultural Infrastructure Accessibility Score (CIAS) using exponential distance decay at fine spatial resolution, then aggregate the score per capita and integrate socio-demographic indicators. Interpretable tree-ensemble models with SHapley Additive exPlanation (SHAP) are used to explain associations between accessibility, income, density, and tract-level racial/ethnic composition. Results show a pronounced core-periphery gradient, where non-library cultural infrastructures cluster near urban cores, while libraries track density and provide broader coverage. Non-library accessibility is modestly higher in higher-income tracts, and library accessibility is slightly higher in denser, lower-income areas.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Correctness: Confidence-Aware Reward Modeling for Enhancing Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2511.07483</link>
<guid>https://arxiv.org/abs/2511.07483</guid>
<content:encoded><![CDATA[
arXiv:2511.07483v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have shifted the post-training paradigm from traditional instruction tuning and human preference alignment toward reinforcement learning (RL) focused on reasoning capabilities. However, numerous technical reports indicate that purely rule-based reward RL frequently results in poor-quality reasoning chains or inconsistencies between reasoning processes and final answers, particularly when the base model is of smaller scale. During the RL exploration process, models might employ low-quality reasoning chains due to the lack of knowledge, occasionally producing correct answers randomly and receiving rewards based on established rule-based judges. This constrains the potential for resource-limited organizations to conduct direct reinforcement learning training on smaller-scale models. We propose a novel confidence-based reward model tailored for enhancing STEM reasoning capabilities. Unlike conventional approaches, our model penalizes not only incorrect answers but also low-confidence correct responses, thereby promoting more robust and logically consistent reasoning. We validate the effectiveness of our approach through static evaluations, Best-of-N inference tests, and PPO-based RL training. Our method outperforms several state-of-the-art open-source reward models across diverse STEM benchmarks. We release our codes and model in https://github.com/qianxiHe147/C2RM.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Laplacian Score Sharpening for Mitigating Hallucination in Diffusion Models</title>
<link>https://arxiv.org/abs/2511.07496</link>
<guid>https://arxiv.org/abs/2511.07496</guid>
<content:encoded><![CDATA[
arXiv:2511.07496v1 Announce Type: cross 
Abstract: Diffusion models, though successful, are known to suffer from hallucinations that create incoherent or unrealistic samples. Recent works have attributed this to the phenomenon of mode interpolation and score smoothening, but they lack a method to prevent their generation during sampling. In this paper, we propose a post-hoc adjustment to the score function during inference that leverages the Laplacian (or sharpness) of the score to reduce mode interpolation hallucination in unconditional diffusion models across 1D, 2D, and high-dimensional image data. We derive an efficient Laplacian approximation for higher dimensions using a finite-difference variant of the Hutchinson trace estimator. We show that this correction significantly reduces the rate of hallucinated samples across toy 1D/2D distributions and a high- dimensional image dataset. Furthermore, our analysis explores the relationship between the Laplacian and uncertainty in the score.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tractable Instances of Bilinear Maximization: Implementing LinUCB on Ellipsoids</title>
<link>https://arxiv.org/abs/2511.07504</link>
<guid>https://arxiv.org/abs/2511.07504</guid>
<content:encoded><![CDATA[
arXiv:2511.07504v1 Announce Type: cross 
Abstract: We consider the maximization of $x^\top \theta$ over $(x,\theta) \in \mathcal{X} \times \Theta$, with $\mathcal{X} \subset \mathbb{R}^d$ convex and $\Theta \subset \mathbb{R}^d$ an ellipsoid. This problem is fundamental in linear bandits, as the learner must solve it at every time step using optimistic algorithms. We first show that for some sets $\mathcal{X}$ e.g. $\ell_p$ balls with $p>2$, no efficient algorithms exist unless $\mathcal{P} = \mathcal{NP}$. We then provide two novel algorithms solving this problem efficiently when $\mathcal{X}$ is a centered ellipsoid. Our findings provide the first known method to implement optimistic algorithms for linear bandits in high dimensions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoPS: Evolutionary Patch Selection for Whole Slide Image Analysis in Computational Pathology</title>
<link>https://arxiv.org/abs/2511.07560</link>
<guid>https://arxiv.org/abs/2511.07560</guid>
<content:encoded><![CDATA[
arXiv:2511.07560v1 Announce Type: cross 
Abstract: In computational pathology, the gigapixel scale of Whole-Slide Images (WSIs) necessitates their division into thousands of smaller patches. Analyzing these high-dimensional patch embeddings is computationally expensive and risks diluting key diagnostic signals with many uninformative patches. Existing patch selection methods often rely on random sampling or simple clustering heuristics and typically fail to explicitly manage the crucial trade-off between the number of selected patches and the accuracy of the resulting slide representation. To address this gap, we propose EvoPS (Evolutionary Patch Selection), a novel framework that formulates patch selection as a multi-objective optimization problem and leverages an evolutionary search to simultaneously minimize the number of selected patch embeddings and maximize the performance of a downstream similarity search task, generating a Pareto front of optimal trade-off solutions. We validated our framework across four major cancer cohorts from The Cancer Genome Atlas (TCGA) using five pretrained deep learning models to generate patch embeddings, including both supervised CNNs and large self-supervised foundation models. The results demonstrate that EvoPS can reduce the required number of training patch embeddings by over 90% while consistently maintaining or even improving the final classification F1-score compared to a baseline that uses all available patches' embeddings selected through a standard extraction pipeline. The EvoPS framework provides a robust and principled method for creating efficient, accurate, and interpretable WSI representations, empowering users to select an optimal balance between computational cost and diagnostic performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shocks Under Control: Taming Transonic Compressible Flow over an RAE2822 Airfoil with Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07564</link>
<guid>https://arxiv.org/abs/2511.07564</guid>
<content:encoded><![CDATA[
arXiv:2511.07564v1 Announce Type: cross 
Abstract: Active flow control of compressible transonic shock-boundary layer interactions over a two-dimensional RAE2822 airfoil at Re = 50,000 is investigated using deep reinforcement learning (DRL). The flow field exhibits highly unsteady dynamics, including complex shock-boundary layer interactions, shock oscillations, and the generation of Kutta waves from the trailing edge. A high-fidelity CFD solver, employing a fifth-order spectral discontinuous Galerkin scheme in space and a strong-stability-preserving Runge-Kutta (5,4) method in time, together with adaptive mesh refinement capability, is used to obtain the accurate flow field. Synthetic jet actuation is employed to manipulate these unsteady flow features, while the DRL agent autonomously discovers effective control strategies through direct interaction with high-fidelity compressible flow simulations. The trained controllers effectively mitigate shock-induced separation, suppress unsteady oscillations, and manipulate aerodynamic forces under transonic conditions. In the first set of experiments, aimed at both drag reduction and lift enhancement, the DRL-based control reduces the average drag coefficient by 13.78% and increases lift by 131.18%, thereby improving the lift-to-drag ratio by 121.52%, which underscores its potential for managing complex flow dynamics. In the second set, targeting drag reduction while maintaining lift, the DRL-based control achieves a 25.62% reduction in drag and a substantial 196.30% increase in lift, accompanied by markedly diminished oscillations. In this case, the lift-to-drag ratio improves by 220.26%.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Infinite-Dimensional Operator/Block Kaczmarz Algorithms: Regret Bounds and $\lambda$-Effectiveness</title>
<link>https://arxiv.org/abs/2511.07604</link>
<guid>https://arxiv.org/abs/2511.07604</guid>
<content:encoded><![CDATA[
arXiv:2511.07604v1 Announce Type: cross 
Abstract: We present a variety of projection-based linear regression algorithms with a focus on modern machine-learning models and their algorithmic performance. We study the role of the relaxation parameter in generalized Kaczmarz algorithms and establish a priori regret bounds with explicit $\lambda$-dependence to quantify how much an algorithm's performance deviates from its optimal performance. A detailed analysis of relaxation parameter is also provided. Applications include: explicit regret bounds for the framework of Kaczmarz algorithm models, non-orthogonal Fourier expansions, and the use of regret estimates in modern machine learning models, including for noisy data, i.e., regret bounds for the noisy Kaczmarz algorithms. Motivated by machine-learning practice, our wider framework treats bounded operators (on infinite-dimensional Hilbert spaces), with updates realized as (block) Kaczmarz algorithms, leading to new and versatile results.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cortex AISQL: A Production SQL Engine for Unstructured Data</title>
<link>https://arxiv.org/abs/2511.07663</link>
<guid>https://arxiv.org/abs/2511.07663</guid>
<content:encoded><![CDATA[
arXiv:2511.07663v1 Announce Type: cross 
Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Experimental Design via Generalised Bayesian Inference</title>
<link>https://arxiv.org/abs/2511.07671</link>
<guid>https://arxiv.org/abs/2511.07671</guid>
<content:encoded><![CDATA[
arXiv:2511.07671v1 Announce Type: cross 
Abstract: Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ResearchRubrics: A Benchmark of Prompts and Rubrics For Evaluating Deep Research Agents</title>
<link>https://arxiv.org/abs/2511.07685</link>
<guid>https://arxiv.org/abs/2511.07685</guid>
<content:encoded><![CDATA[
arXiv:2511.07685v1 Announce Type: cross 
Abstract: Deep Research (DR) is an emerging agent application that leverages large language models (LLMs) to address open-ended queries. It requires the integration of several capabilities, including multi-step reasoning, cross-document synthesis, and the generation of evidence-backed, long-form answers. Evaluating DR remains challenging because responses are lengthy and diverse, admit many valid solutions, and often depend on dynamic information sources. We introduce ResearchRubrics, a standardized benchmark for DR built with over 2,800+ hours of human labor that pairs realistic, domain-diverse prompts with 2,500+ expert-written, fine-grained rubrics to assess factual grounding, reasoning soundness, and clarity. We also propose a new complexity framework for categorizing DR tasks along three axes: conceptual breadth, logical nesting, and exploration. In addition, we develop human and model-based evaluation protocols that measure rubric adherence for DR agents. We evaluate several state-of-the-art DR systems and find that even leading agents like Gemini's DR and OpenAI's DR achieve under 68% average compliance with our rubrics, primarily due to missed implicit context and inadequate reasoning about retrieved information. Our results highlight the need for robust, scalable assessment of deep research capabilities, to which end we release ResearchRubrics(including all prompts, rubrics, and evaluation code) to facilitate progress toward well-justified research assistants.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kolmogorov-Arnold Chemical Reaction Neural Networks for learning pressure-dependent kinetic rate laws</title>
<link>https://arxiv.org/abs/2511.07686</link>
<guid>https://arxiv.org/abs/2511.07686</guid>
<content:encoded><![CDATA[
arXiv:2511.07686v1 Announce Type: cross 
Abstract: Chemical Reaction Neural Networks (CRNNs) have emerged as an interpretable machine learning framework for discovering reaction kinetics directly from data, while strictly adhering to the Arrhenius and mass action laws. However, standard CRNNs cannot represent pressure-dependent rate behavior, which is critical in many combustion and chemical systems and typically requires empirical formulations such as Troe or PLOG. Here, we develop Kolmogorov-Arnold Chemical Reaction Neural Networks (KA-CRNNs) that generalize CRNNs by modeling each kinetic parameter as a learnable function of system pressure using Kolmogorov-Arnold activations. This structure maintains full interpretability and physical consistency while enabling assumption-free inference of pressure effects directly from data. A proof-of-concept study on the CH3 recombination reaction demonstrates that KA-CRNNs accurately reproduce pressure-dependent kinetics across a range of temperatures and pressures, outperforming conventional interpolative models. The framework establishes a foundation for data-driven discovery of extended kinetic behaviors in complex reacting systems, advancing interpretable and physics-consistent approaches for chemical model inference.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stress Testing Factual Consistency Metrics for Long-Document Summarization</title>
<link>https://arxiv.org/abs/2511.07689</link>
<guid>https://arxiv.org/abs/2511.07689</guid>
<content:encoded><![CDATA[
arXiv:2511.07689v1 Announce Type: cross 
Abstract: Evaluating the factual consistency of abstractive text summarization remains a significant challenge, particularly for long documents, where conventional metrics struggle with input length limitations and long-range dependencies. In this work, we systematically evaluate the reliability of six widely used reference-free factuality metrics, originally proposed for short-form summarization, in the long-document setting. We probe metric robustness through seven factuality-preserving perturbations applied to summaries, namely paraphrasing, simplification, synonym replacement, logically equivalent negations, vocabulary reduction, compression, and source text insertion, and further analyze their sensitivity to retrieval context and claim information density. Across three long-form benchmark datasets spanning science fiction, legal, and scientific domains, our results reveal that existing short-form metrics produce inconsistent scores for semantically equivalent summaries and exhibit declining reliability for information-dense claims whose content is semantically similar to many parts of the source document. While expanding the retrieval context improves stability in some domains, no metric consistently maintains factual alignment under long-context conditions. Finally, our results highlight concrete directions for improving factuality evaluation, including multi-span reasoning, context-aware calibration, and training on meaning-preserving variations to enhance robustness in long-form summarization. We release all code, perturbed data, and scripts required to reproduce our results at https://github.com/zainmujahid/metricEval-longSum.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Misaligned by Design: Incentive Failures in Machine Learning</title>
<link>https://arxiv.org/abs/2511.07699</link>
<guid>https://arxiv.org/abs/2511.07699</guid>
<content:encoded><![CDATA[
arXiv:2511.07699v1 Announce Type: cross 
Abstract: The cost of error in many high-stakes settings is asymmetric: misdiagnosing pneumonia when absent is an inconvenience, but failing to detect it when present can be life-threatening. Because of this, artificial intelligence (AI) models used to assist such decisions are frequently trained with asymmetric loss functions that incorporate human decision-makers' trade-offs between false positives and false negatives. In two focal applications, we show that this standard alignment practice can backfire. In both cases, it would be better to train the machine learning model with a loss function that ignores the human's objective and then adjust predictions ex post according to that objective. We rationalize this result using an economic model of incentive design with endogenous information acquisition. The key insight from our theoretical framework is that machine classifiers perform not one but two incentivized tasks: choosing how to classify and learning how to classify. We show that while the adjustments engineers use correctly incentivize choosing, they can simultaneously reduce the incentives to learn. Our formal treatment of the problem reveals that methods embraced for their intuitive appeal can in fact misalign human and machine objectives in predictable ways.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ViPRA: Video Prediction for Robot Actions</title>
<link>https://arxiv.org/abs/2511.07732</link>
<guid>https://arxiv.org/abs/2511.07732</guid>
<content:encoded><![CDATA[
arXiv:2511.07732v1 Announce Type: cross 
Abstract: Can we turn a video prediction model into a robot policy? Videos, including those of humans or teleoperated robots, capture rich physical interactions. However, most of them lack labeled actions, which limits their use in robot learning. We present Video Prediction for Robot Actions (ViPRA), a simple pretraining-finetuning framework that learns continuous robot control from these actionless videos. Instead of directly predicting actions, we train a video-language model to predict both future visual observations and motion-centric latent actions, which serve as intermediate representations of scene dynamics. We train these latent actions using perceptual losses and optical flow consistency to ensure they reflect physically grounded behavior. For downstream control, we introduce a chunked flow matching decoder that maps latent actions to robot-specific continuous action sequences, using only 100 to 200 teleoperated demonstrations. This approach avoids expensive action annotation, supports generalization across embodiments, and enables smooth, high-frequency continuous control upto 22 Hz via chunked action decoding. Unlike prior latent action works that treat pretraining as autoregressive policy learning, explicitly models both what changes and how. Our method outperforms strong baselines, with a 16% gain on the SIMPLER benchmark and a 13% improvement across real world manipulation tasks. We will release models and code at https://vipra-project.github.io
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TurboSAT: Gradient-Guided Boolean Satisfiability Accelerated on GPU-CPU Hybrid System</title>
<link>https://arxiv.org/abs/2511.07737</link>
<guid>https://arxiv.org/abs/2511.07737</guid>
<content:encoded><![CDATA[
arXiv:2511.07737v1 Announce Type: cross 
Abstract: While accelerated computing has transformed many domains of computing, its impact on logical reasoning, specifically Boolean satisfiability (SAT), remains limited. State-of-the-art SAT solvers rely heavily on inherently sequential conflict-driven search algorithms that offer powerful heuristics but limit the amount of parallelism that could otherwise enable significantly more scalable SAT solving. Inspired by neural network training, we formulate the SAT problem as a binarized matrix-matrix multiplication layer that could be optimized using a differentiable objective function. Enabled by this encoding, we combine the strengths of parallel differentiable optimization and sequential search to accelerate SAT on a hybrid GPU-CPU system. In this system, the GPUs leverage parallel differentiable solving to rapidly evaluate SAT clauses and use gradients to stochastically explore the solution space and optimize variable assignments. Promising partial assignments generated by the GPUs are post-processed on many CPU threads which exploit conflict-driven sequential search to further traverse the solution subspaces and identify complete assignments. Prototyping the hybrid solver on an NVIDIA DGX GB200 node, our solver achieves runtime speedups up to over 200x when compared to a state-of-the-art CPU-based solver on public satisfiable benchmark problems from the SAT Competition.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SALT: Steering Activations towards Leakage-free Thinking in Chain of Thought</title>
<link>https://arxiv.org/abs/2511.07772</link>
<guid>https://arxiv.org/abs/2511.07772</guid>
<content:encoded><![CDATA[
arXiv:2511.07772v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) evolve into personal assistants with access to sensitive user data, they face a critical privacy challenge: while prior work has addressed output-level privacy, recent findings reveal that LLMs often leak private information through their internal reasoning processes, violating contextual privacy expectations. These leaky thoughts occur when models inadvertently expose sensitive details in their reasoning traces, even when final outputs appear safe. The challenge lies in preventing such leakage without compromising the model's reasoning capabilities, requiring a delicate balance between privacy and utility. We introduce Steering Activations towards Leakage-free Thinking (SALT), a lightweight test-time intervention that mitigates privacy leakage in model's Chain of Thought (CoT) by injecting targeted steering vectors into hidden state. We identify the high-leakage layers responsible for this behavior. Through experiments across multiple LLMs, we demonstrate that SALT achieves reductions including $18.2\%$ reduction in CPL on QwQ-32B, $17.9\%$ reduction in CPL on Llama-3.1-8B, and $31.2\%$ reduction in CPL on Deepseek in contextual privacy leakage dataset AirGapAgent-R while maintaining comparable task performance and utility. Our work establishes SALT as a practical approach for test-time privacy protection in reasoning-capable language models, offering a path toward safer deployment of LLM-based personal agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Streaming Tensor Program: A streaming abstraction for dynamic parallelism</title>
<link>https://arxiv.org/abs/2511.07776</link>
<guid>https://arxiv.org/abs/2511.07776</guid>
<content:encoded><![CDATA[
arXiv:2511.07776v1 Announce Type: cross 
Abstract: Dynamic behaviors are becoming prevalent in many tensor applications. In machine learning, for example, the input tensors are dynamically shaped or ragged, and data-dependent control flow is widely used in many models. However, the limited expressiveness of prior programming abstractions for spatial dataflow accelerators forces the dynamic behaviors to be implemented statically or lacks the visibility for performance-critical decisions. To address these challenges, we present the Streaming Tensor Program (STeP), a new streaming abstraction that enables dynamic tensor workloads to run efficiently on spatial dataflow accelerators. STeP introduces flexible routing operators, an explicit memory hierarchy, and symbolic shape semantics that expose dynamic data rates and tensor dimensions. These capabilities unlock new optimizations-dynamic tiling, dynamic parallelization, and configuration time-multiplexing-that adapt to dynamic behaviors while preserving dataflow efficiency. Using a cycle-approximate simulator on representative LLM layers with real-world traces, dynamic tiling reduces on-chip memory requirement by 2.18x, dynamic parallelization improves latency by 1.5x, and configuration time-multiplexing improves compute utilization by 2.57x over implementations available in prior abstractions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridGuard: Enhancing Minority-Class Intrusion Detection in Dew-Enabled Edge-of-Things Networks</title>
<link>https://arxiv.org/abs/2511.07793</link>
<guid>https://arxiv.org/abs/2511.07793</guid>
<content:encoded><![CDATA[
arXiv:2511.07793v1 Announce Type: cross 
Abstract: Securing Dew-Enabled Edge-of-Things (EoT) networks against sophisticated intrusions is a critical challenge. This paper presents HybridGuard, a framework that integrates machine learning and deep learning to improve intrusion detection. HybridGuard addresses data imbalance through mutual information based feature selection, ensuring that the most relevant features are used to improve detection performance, especially for minority attack classes. The framework leverages Wasserstein Conditional Generative Adversarial Networks with Gradient Penalty (WCGAN-GP) to further reduce class imbalance and enhance detection precision. It adopts a two-phase architecture called DualNetShield to support advanced traffic analysis and anomaly detection, improving the granular identification of threats in complex EoT environments. HybridGuard is evaluated on the UNSW-NB15, CIC-IDS-2017, and IOTID20 datasets, where it demonstrates strong performance across diverse attack scenarios and outperforms existing solutions in adapting to evolving cybersecurity threats. This approach establishes HybridGuard as an effective tool for protecting EoT networks against modern intrusions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Privacy-preserving Inference System with Homomorphic Encryption and Modular Activation</title>
<link>https://arxiv.org/abs/2511.07807</link>
<guid>https://arxiv.org/abs/2511.07807</guid>
<content:encoded><![CDATA[
arXiv:2511.07807v1 Announce Type: cross 
Abstract: With the rapid advancements in machine learning, models have become increasingly capable of learning and making predictions in various industries. However, deploying these models in critical infrastructures presents a major challenge, as concerns about data privacy prevent unrestricted data sharing. Homomor- phic encryption (HE) offers a solution by enabling computations on encrypted data, but it remains incompatible with machine learning models like convolutional neural networks (CNNs), due to their reliance on non-linear activation functions. To bridge this gap, this work proposes an optimized framework that replaces standard non-linear functions with homomorphically compatible approximations, ensuring secure computations while minimizing computational overhead. The proposed approach restructures the CNN architecture and introduces an efficient activation function approximation method to mitigate the performance trade-offs in- troduced by encryption. Experiments on CIFAR-10 achieve 94.4% accuracy with 2.42 s per single encrypted sample and 24,000 s per 10,000 encrypted samples, using a degree-4 polynomial and Softplus activation under CKKS, balancing accuracy and privacy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Online Markov Game with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2511.07831</link>
<guid>https://arxiv.org/abs/2511.07831</guid>
<content:encoded><![CDATA[
arXiv:2511.07831v1 Announce Type: cross 
Abstract: The sim-to-real gap, where agents trained in a simulator face significant performance degradation during testing, is a fundamental challenge in reinforcement learning. Extansive works adopt the framework of distributionally robust RL, to learn a policy that acts robustly under worst case environment shift. Within this framework, our objective is to devise algorithms that are sample efficient with interactive data collection and large state spaces. By assuming d-rectangularity of environment dynamic shift, we identify a fundamental hardness result for learning in online Markov game, and address it by adopting minimum value assumption. Then, a novel least square value iteration type algorithm, DR-CCE-LSI, with exploration bonus devised specifically for multiple agents, is proposed to find an \episilon-approximate robust Coarse Correlated Equilibrium(CCE). To obtain sample efficient learning, we find that: when the feature mapping function satisfies certain properties, our algorithm, DR-CCE-LSI, is able to achieve \epsilon-approximate CCE with a regret bound of O{dHmin{H,1/min{\sigma_i}}\sqrt{K}}, where K is the number of interacting episodes, H is the horizon length, d is the feature dimension, and \simga_i represents the uncertainty level of player i. Our work introduces the first sample-efficient algorithm for this setting, matches the best result so far in single agent setting, and achieves minimax optimalsample complexity in terms of the feature dimension d. Meanwhile, we also conduct simulation study to validate the efficacy of our algorithm in learning a robust equilibrium.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization</title>
<link>https://arxiv.org/abs/2511.07836</link>
<guid>https://arxiv.org/abs/2511.07836</guid>
<content:encoded><![CDATA[
arXiv:2511.07836v1 Announce Type: cross 
Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p < 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Sampling via Autospeculation</title>
<link>https://arxiv.org/abs/2511.07869</link>
<guid>https://arxiv.org/abs/2511.07869</guid>
<content:encoded><![CDATA[
arXiv:2511.07869v1 Announce Type: cross 
Abstract: We present parallel algorithms to accelerate sampling via counting in two settings: any-order autoregressive models and denoising diffusion models. An any-order autoregressive model accesses a target distribution $\mu$ on $[q]^n$ through an oracle that provides conditional marginals, while a denoising diffusion model accesses a target distribution $\mu$ on $\mathbb{R}^n$ through an oracle that provides conditional means under Gaussian noise. Standard sequential sampling algorithms require $\widetilde{O}(n)$ time to produce a sample from $\mu$ in either setting. We show that, by issuing oracle calls in parallel, the expected sampling time can be reduced to $\widetilde{O}(n^{1/2})$. This improves the previous $\widetilde{O}(n^{2/3})$ bound for any-order autoregressive models and yields the first parallel speedup for diffusion models in the high-accuracy regime, under the relatively mild assumption that the support of $\mu$ is bounded.
  We introduce a novel technique to obtain our results: speculative rejection sampling. This technique leverages an auxiliary ``speculative'' distribution~$\nu$ that approximates~$\mu$ to accelerate sampling. Our technique is inspired by the well-studied ``speculative decoding'' techniques popular in large language models, but differs in key ways. Firstly, we use ``autospeculation,'' namely we build the speculation $\nu$ out of the same oracle that defines~$\mu$. In contrast, speculative decoding typically requires a separate, faster, but potentially less accurate ``draft'' model $\nu$. Secondly, the key differentiating factor in our technique is that we make and accept speculations at a ``sequence'' level rather than at the level of single (or a few) steps. This last fact is key to unlocking our parallel runtime of $\widetilde{O}(n^{1/2})$.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpikCommander: A High-performance Spiking Transformer with Multi-view Learning for Efficient Speech Command Recognition</title>
<link>https://arxiv.org/abs/2511.07883</link>
<guid>https://arxiv.org/abs/2511.07883</guid>
<content:encoded><![CDATA[
arXiv:2511.07883v1 Announce Type: cross 
Abstract: Spiking neural networks (SNNs) offer a promising path toward energy-efficient speech command recognition (SCR) by leveraging their event-driven processing paradigm. However, existing SNN-based SCR methods often struggle to capture rich temporal dependencies and contextual information from speech due to limited temporal modeling and binary spike-based representations. To address these challenges, we first introduce the multi-view spiking temporal-aware self-attention (MSTASA) module, which combines effective spiking temporal-aware attention with a multi-view learning framework to model complementary temporal dependencies in speech commands. Building on MSTASA, we further propose SpikCommander, a fully spike-driven transformer architecture that integrates MSTASA with a spiking contextual refinement channel MLP (SCR-MLP) to jointly enhance temporal context modeling and channel-wise feature integration. We evaluate our method on three benchmark datasets: the Spiking Heidelberg Dataset (SHD), the Spiking Speech Commands (SSC), and the Google Speech Commands V2 (GSC). Extensive experiments demonstrate that SpikCommander consistently outperforms state-of-the-art (SOTA) SNN approaches with fewer parameters under comparable time steps, highlighting its effectiveness and efficiency for robust speech command recognition.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intelligence per Watt: Measuring Intelligence Efficiency of Local AI</title>
<link>https://arxiv.org/abs/2511.07885</link>
<guid>https://arxiv.org/abs/2511.07885</guid>
<content:encoded><![CDATA[
arXiv:2511.07885v1 Announce Type: cross 
Abstract: Large language model (LLM) queries are predominantly processed by frontier models in centralized cloud infrastructure. Rapidly growing demand strains this paradigm, and cloud providers struggle to scale infrastructure at pace. Two advances enable us to rethink this paradigm: small LMs (<=20B active parameters) now achieve competitive performance to frontier models on many tasks, and local accelerators (e.g., Apple M4 Max) run these models at interactive latencies. This raises the question: can local inference viably redistribute demand from centralized infrastructure? Answering this requires measuring whether local LMs can accurately answer real-world queries and whether they can do so efficiently enough to be practical on power-constrained devices (i.e., laptops). We propose intelligence per watt (IPW), task accuracy divided by unit of power, as a metric for assessing capability and efficiency of local inference across model-accelerator pairs. We conduct a large-scale empirical study across 20+ state-of-the-art local LMs, 8 accelerators, and a representative subset of LLM traffic: 1M real-world single-turn chat and reasoning queries. For each query, we measure accuracy, energy, latency, and power. Our analysis reveals $3$ findings. First, local LMs can accurately answer 88.7% of single-turn chat and reasoning queries with accuracy varying by domain. Second, from 2023-2025, IPW improved 5.3x and local query coverage rose from 23.2% to 71.3%. Third, local accelerators achieve at least 1.4x lower IPW than cloud accelerators running identical models, revealing significant headroom for optimization. These findings demonstrate that local inference can meaningfully redistribute demand from centralized infrastructure, with IPW serving as the critical metric for tracking this transition. We release our IPW profiling harness for systematic intelligence-per-watt benchmarking.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Descriptions from Large Language Models with Influence Estimation</title>
<link>https://arxiv.org/abs/2511.07897</link>
<guid>https://arxiv.org/abs/2511.07897</guid>
<content:encoded><![CDATA[
arXiv:2511.07897v1 Announce Type: cross 
Abstract: Deep learning models have been successful in many areas but understanding their behaviors still remains a black-box. Most prior explainable AI (XAI) approaches have focused on interpreting and explaining how models make predictions. In contrast, we would like to understand how data can be explained with deep learning model training and propose a novel approach to understand the data via one of the most common media - language - so that humans can easily understand. Our approach proposes a pipeline to generate textual descriptions that can explain the data with large language models by incorporating external knowledge bases. However, generated data descriptions may still include irrelevant information, so we introduce to exploit influence estimation to choose the most informative textual descriptions, along with the CLIP score. Furthermore, based on the phenomenon of cross-modal transferability, we propose a novel benchmark task named cross-modal transfer classification to examine the effectiveness of our textual descriptions. In the experiment of zero-shot setting, we show that our textual descriptions are more effective than other baseline descriptions, and furthermore, we successfully boost the performance of the model trained only on images across all nine image classification datasets. These results are further supported by evaluation using GPT-4o. Through our approach, we may gain insights into the inherent interpretability of the decision-making process of the model.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Based Automated Parameter Extraction Framework for Modeling Memristive Devices</title>
<link>https://arxiv.org/abs/2511.07926</link>
<guid>https://arxiv.org/abs/2511.07926</guid>
<content:encoded><![CDATA[
arXiv:2511.07926v1 Announce Type: cross 
Abstract: Resistive random access memory (RRAM) is a promising candidate for next-generation nonvolatile memory (NVM) and in-memory computing applications. Compact models are essential for analyzing the circuit and system-level performance of experimental RRAM devices. However, most existing RRAM compact models rely on multiple fitting parameters to reproduce the device I-V characteristics, and in most cases, as the parameters are not directly related to measurable quantities, their extraction requires extensive manual tuning, making the process time-consuming and limiting adaptability across different devices. This work presents an automated framework for extracting the fitting parameters of the widely used Stanford RRAM model directly from the device I-V characteristics. The framework employs a convolutional neural network (CNN) trained on a synthetic dataset to generate initial parameter estimates, which are then refined through three heuristic optimization blocks that minimize errors via adaptive binary search in the parameter space. We evaluated the framework using four key NVM metrics: set voltage, reset voltage, hysteresis loop area, and low resistance state (LRS) slope. Benchmarking against RRAM device characteristics derived from previously reported Stanford model fits, other analytical models, and experimental data shows that the framework achieves low error across diverse device characteristics, offering a fast, reliable, and robust solution for RRAM modeling.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class-feature Watermark: A Resilient Black-box Watermark Against Model Extraction Attacks</title>
<link>https://arxiv.org/abs/2511.07947</link>
<guid>https://arxiv.org/abs/2511.07947</guid>
<content:encoded><![CDATA[
arXiv:2511.07947v1 Announce Type: cross 
Abstract: Machine learning models constitute valuable intellectual property, yet remain vulnerable to model extraction attacks (MEA), where adversaries replicate their functionality through black-box queries. Model watermarking counters MEAs by embedding forensic markers for ownership verification. Current black-box watermarks prioritize MEA survival through representation entanglement, yet inadequately explore resilience against sequential MEAs and removal attacks. Our study reveals that this risk is underestimated because existing removal methods are weakened by entanglement. To address this gap, we propose Watermark Removal attacK (WRK), which circumvents entanglement constraints by exploiting decision boundaries shaped by prevailing sample-level watermark artifacts. WRK effectively reduces watermark success rates by at least 88.79% across existing watermarking benchmarks.
  For robust protection, we propose Class-Feature Watermarks (CFW), which improve resilience by leveraging class-level artifacts. CFW constructs a synthetic class using out-of-domain samples, eliminating vulnerable decision boundaries between original domain samples and their artifact-modified counterparts (watermark samples). CFW concurrently optimizes both MEA transferability and post-MEA stability. Experiments across multiple domains show that CFW consistently outperforms prior methods in resilience, maintaining a watermark success rate of at least 70.15% in extracted models even under the combined MEA and WRK distortion, while preserving the utility of protected models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Speech Emotion Recognition with Phonation Excitation Information and Articulatory Kinematics</title>
<link>https://arxiv.org/abs/2511.07955</link>
<guid>https://arxiv.org/abs/2511.07955</guid>
<content:encoded><![CDATA[
arXiv:2511.07955v1 Announce Type: cross 
Abstract: Speech emotion recognition (SER) has advanced significantly for the sake of deep-learning methods, while textual information further enhances its performance. However, few studies have focused on the physiological information during speech production, which also encompasses speaker traits, including emotional states. To bridge this gap, we conducted a series of experiments to investigate the potential of the phonation excitation information and articulatory kinematics for SER. Due to the scarcity of training data for this purpose, we introduce a portrayed emotional dataset, STEM-E2VA, which includes audio and physiological data such as electroglottography (EGG) and electromagnetic articulography (EMA). EGG and EMA provide information of phonation excitation and articulatory kinematics, respectively. Additionally, we performed emotion recognition using estimated physiological data derived through inversion methods from speech, instead of collected EGG and EMA, to explore the feasibility of applying such physiological information in real-world SER. Experimental results confirm the effectiveness of incorporating physiological information about speech production for SER and demonstrate its potential for practical use in real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrAda-GAN: A Private Adaptive Generative Adversarial Network with Bayes Network Structure</title>
<link>https://arxiv.org/abs/2511.07997</link>
<guid>https://arxiv.org/abs/2511.07997</guid>
<content:encoded><![CDATA[
arXiv:2511.07997v1 Announce Type: cross 
Abstract: We revisit the problem of generating synthetic data under differential privacy. To address the core limitations of marginal-based methods, we propose the Private Adaptive Generative Adversarial Network with Bayes Network Structure (PrAda-GAN), which integrates the strengths of both GAN-based and marginal-based approaches. Our method adopts a sequential generator architecture to capture complex dependencies among variables, while adaptively regularizing the learned structure to promote sparsity in the underlying Bayes network. Theoretically, we establish diminishing bounds on the parameter distance, variable selection error, and Wasserstein distance. Our analysis shows that leveraging dependency sparsity leads to significant improvements in convergence rates. Empirically, experiments on both synthetic and real-world datasets demonstrate that PrAda-GAN outperforms existing tabular data synthesis methods in terms of the privacy-utility trade-off.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prudential Reliability of Large Language Models in Reinsurance: Governance, Assurance, and Capital Efficiency</title>
<link>https://arxiv.org/abs/2511.08082</link>
<guid>https://arxiv.org/abs/2511.08082</guid>
<content:encoded><![CDATA[
arXiv:2511.08082v1 Announce Type: cross 
Abstract: This paper develops a prudential framework for assessing the reliability of large language models (LLMs) in reinsurance. A five-pillar architecture--governance, data lineage, assurance, resilience, and regulatory alignment--translates supervisory expectations from Solvency II, SR 11-7, and guidance from EIOPA (2025), NAIC (2023), and IAIS (2024) into measurable lifecycle controls. The framework is implemented through the Reinsurance AI Reliability and Assurance Benchmark (RAIRAB), which evaluates whether governance-embedded LLMs meet prudential standards for grounding, transparency, and accountability. Across six task families, retrieval-grounded configurations achieved higher grounding accuracy (0.90), reduced hallucination and interpretive drift by roughly 40%, and nearly doubled transparency. These mechanisms lower informational frictions in risk transfer and capital allocation, showing that existing prudential doctrines already accommodate reliable AI when governance is explicit, data are traceable, and assurance is verifiable.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foam Segmentation in Wastewater Treatment Plants: A Federated Learning Approach with Segment Anything Model 2</title>
<link>https://arxiv.org/abs/2511.08130</link>
<guid>https://arxiv.org/abs/2511.08130</guid>
<content:encoded><![CDATA[
arXiv:2511.08130v1 Announce Type: cross 
Abstract: Foam formation in Wastewater Treatment Plants (WTPs) is a major challenge that can reduce treatment efficiency and increase costs. The ability to automatically examine changes in real-time with respect to the percentage of foam can be of great benefit to the plant. However, large amounts of labeled data are required to train standard Machine Learning (ML) models. The development of these systems is slow due to the scarcity and heterogeneity of labeled data. Additionally, the development is often hindered by the fact that different WTPs do not share their data due to privacy concerns. This paper proposes a new framework to address these challenges by combining Federated Learning (FL) with the state-of-the-art base model for image segmentation, Segment Anything Model 2 (SAM2). The FL paradigm enables collaborative model training across multiple WTPs without centralizing sensitive operational data, thereby ensuring privacy. The framework accelerates training convergence and improves segmentation performance even with limited local datasets by leveraging SAM2's strong pre-trained weights for initialization. The methodology involves fine-tuning SAM2 on distributed clients (edge nodes) using the Flower framework, where a central Fog server orchestrates the process by aggregating model weights without accessing private data. The model was trained and validated using various data collections, including real-world images captured at a WTPs in Granada, Spain, a synthetically generated foam dataset, and images from publicly available datasets to improve generalization. This research offers a practical, scalable, and privacy-aware solution for automatic foam tracking in WTPs. The findings highlight the significant potential of integrating large-scale foundational models into FL systems to solve real-world industrial challenges characterized by distributed and sensitive data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Boomda: Balanced Multi-objective Optimization for Multimodal Domain Adaptation</title>
<link>https://arxiv.org/abs/2511.08152</link>
<guid>https://arxiv.org/abs/2511.08152</guid>
<content:encoded><![CDATA[
arXiv:2511.08152v1 Announce Type: cross 
Abstract: Multimodal learning, while contributing to numerous success stories across various fields, faces the challenge of prohibitively expensive manual annotation. To address the scarcity of annotated data, a popular solution is unsupervised domain adaptation, which has been extensively studied in unimodal settings yet remains less explored in multimodal settings. In this paper, we investigate heterogeneous multimodal domain adaptation, where the primary challenge is the varying domain shifts of different modalities from the source to the target domain. We first introduce the information bottleneck method to learn representations for each modality independently, and then match the source and target domains in the representation space with correlation alignment. To balance the domain alignment of all modalities, we formulate the problem as a multi-objective task, aiming for a Pareto optimal solution. By exploiting the properties specific to our model, the problem can be simplified to a quadratic programming problem. Further approximation yields a closed-form solution, leading to an efficient modality-balanced multimodal domain adaptation algorithm. The proposed method features \textbf{B}alanced multi-\textbf{o}bjective \textbf{o}ptimization for \textbf{m}ultimodal \textbf{d}omain \textbf{a}daptation, termed \textbf{Boomda}. Extensive empirical results showcase the effectiveness of the proposed approach and demonstrate that Boomda outperforms the competing schemes. The code is is available at: https://github.com/sunjunaimer/Boomda.git.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Good flavor search in $SU(5)$: a machine learning approach</title>
<link>https://arxiv.org/abs/2511.08154</link>
<guid>https://arxiv.org/abs/2511.08154</guid>
<content:encoded><![CDATA[
arXiv:2511.08154v1 Announce Type: cross 
Abstract: We revisit the fermion mass problem of the $SU(5)$ grand unified theory using machine learning techniques. The original $SU(5)$ model proposed by Georgi and Glashow is incompatible with the observed fermion mass spectrum. Two remedies are known to resolve this discrepancy, one is through introducing a new interaction via a 45-dimensional field, and the other via a 24-dimensional field. We investigate which modification is more natural, defining naturalness as proximity to the original Georgi-Glashow $SU(5)$ model. Our analysis shows that, in both supersymmetric and non-supersymmetric scenarios, the model incorporating the interaction with the 24-dimensional field is more natural under this criterion. We then generalise these models by introducing a continuous parameter $y$, which takes the value 3 for the 45-dimensional field and 1.5 for the 24-dimensional field. Numerical optimisation reveals that $y \approx 0.8$ yields the closest match to the original $SU(5)$ model, indicating that this value corresponds to the most natural model according to our definition.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof Minimization in Neural Network Verification</title>
<link>https://arxiv.org/abs/2511.08198</link>
<guid>https://arxiv.org/abs/2511.08198</guid>
<content:encoded><![CDATA[
arXiv:2511.08198v1 Announce Type: cross 
Abstract: The widespread adoption of deep neural networks (DNNs) requires efficient techniques for verifying their safety. DNN verifiers are complex tools, which might contain bugs that could compromise their soundness and undermine the reliability of the verification process. This concern can be mitigated using proofs: artifacts that are checkable by an external and reliable proof checker, and which attest to the correctness of the verification process. However, such proofs tend to be extremely large, limiting their use in many scenarios. In this work, we address this problem by minimizing proofs of unsatisfiability produced by DNN verifiers. We present algorithms that remove facts which were learned during the verification process, but which are unnecessary for the proof itself. Conceptually, our method analyzes the dependencies among facts used to deduce UNSAT, and removes facts that did not contribute. We then further minimize the proof by eliminating remaining unnecessary dependencies, using two alternative procedures. We implemented our algorithms on top of a proof producing DNN verifier, and evaluated them across several benchmarks. Our results show that our best-performing algorithm reduces proof size by 37%-82% and proof checking time by 30%-88%, while introducing a runtime overhead of 7%-20% to the verification process itself.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Classical to Hybrid: A Practical Framework for Quantum-Enhanced Learning</title>
<link>https://arxiv.org/abs/2511.08205</link>
<guid>https://arxiv.org/abs/2511.08205</guid>
<content:encoded><![CDATA[
arXiv:2511.08205v1 Announce Type: cross 
Abstract: This work addresses the challenge of enabling practitioners without quantum expertise to transition from classical to hybrid quantum-classical machine learning workflows. We propose a three-stage framework: starting with a classical self-training model, then introducing a minimal hybrid quantum variant, and finally applying diagnostic feedback via QMetric to refine the hybrid architecture. In experiments on the Iris dataset, the refined hybrid model improved accuracy from 0.31 in the classical approach to 0.87 in the quantum approach. These results suggest that even modest quantum components, when guided by proper diagnostics, can enhance class separation and representation capacity in hybrid learning, offering a practical pathway for classical machine learning practitioners to leverage quantum-enhanced methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedPoP: Federated Learning Meets Proof of Participation</title>
<link>https://arxiv.org/abs/2511.08207</link>
<guid>https://arxiv.org/abs/2511.08207</guid>
<content:encoded><![CDATA[
arXiv:2511.08207v1 Announce Type: cross 
Abstract: Federated learning (FL) offers privacy preserving, distributed machine learning, allowing clients to contribute to a global model without revealing their local data. As models increasingly serve as monetizable digital assets, the ability to prove participation in their training becomes essential for establishing ownership. In this paper, we address this emerging need by introducing FedPoP, a novel FL framework that allows nonlinkable proof of participation while preserving client anonymity and privacy without requiring either extensive computations or a public ledger. FedPoP is designed to seamlessly integrate with existing secure aggregation protocols to ensure compatibility with real-world FL deployments. We provide a proof of concept implementation and an empirical evaluation under realistic client dropouts. In our prototype, FedPoP introduces 0.97 seconds of per-round overhead atop securely aggregated FL and enables a client to prove its participation/contribution to a model held by a third party in 0.0612 seconds. These results indicate FedPoP is practical for real-world deployments that require auditable participation without sacrificing privacy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Gemini LLM in Food Image-Based Recipe and Nutrition Description with EfficientNet-B4 Visual Backbone</title>
<link>https://arxiv.org/abs/2511.08215</link>
<guid>https://arxiv.org/abs/2511.08215</guid>
<content:encoded><![CDATA[
arXiv:2511.08215v1 Announce Type: cross 
Abstract: The proliferation of digital food applications necessitates robust methods for automated nutritional analysis and culinary guidance. This paper presents a comprehensive comparative evaluation of a decoupled, multimodal pipeline for food recognition. We evaluate a system integrating a specialized visual backbone (EfficientNet-B4) with a powerful generative large language model (Google's Gemini LLM). The core objective is to evaluate the trade-offs between visual classification accuracy, model efficiency, and the quality of generative output (nutritional data and recipes). We benchmark this pipeline against alternative vision backbones (VGG-16, ResNet-50, YOLOv8) and a lightweight LLM (Gemma). We introduce a formalization for "Semantic Error Propagation" (SEP) to analyze how classification inaccuracies from the visual module cascade into the generative output. Our analysis is grounded in a new Custom Chinese Food Dataset (CCFD) developed to address cultural bias in public datasets. Experimental results demonstrate that while EfficientNet-B4 (89.0\% Top-1 Acc.) provides the best balance of accuracy and efficiency, and Gemini (9.2/10 Factual Accuracy) provides superior generative quality, the system's overall utility is fundamentally bottlenecked by the visual front-end's perceptive accuracy. We conduct a detailed per-class analysis, identifying high semantic similarity as the most critical failure mode.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emulating Radiative Transfer in Astrophysical Environments</title>
<link>https://arxiv.org/abs/2511.08219</link>
<guid>https://arxiv.org/abs/2511.08219</guid>
<content:encoded><![CDATA[
arXiv:2511.08219v1 Announce Type: cross 
Abstract: Radiative transfer is a fundamental process in astrophysics, essential for both interpreting observations and modeling thermal and dynamical feedback in simulations via ionizing radiation and photon pressure. However, numerically solving the underlying radiative transfer equation is computationally intensive due to the complex interaction of light with matter and the disparity between the speed of light and the typical gas velocities in astrophysical environments, making it particularly expensive to include the effects of on-the-fly radiation in hydrodynamic simulations. This motivates the development of surrogate models that can significantly accelerate radiative transfer calculations while preserving high accuracy. We present a surrogate model based on a Fourier Neural Operator architecture combined with U-Nets. Our model approximates three-dimensional, monochromatic radiative transfer in time-dependent regimes, in absorption-emission approximation, achieving speedups of more than 2 orders of magnitude while maintaining an average relative error below 3%, demonstrating our approach's potential to be integrated into state-of-the-art hydrodynamic simulations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Accurate Approach for Covariance Matrix Construction</title>
<link>https://arxiv.org/abs/2511.08223</link>
<guid>https://arxiv.org/abs/2511.08223</guid>
<content:encoded><![CDATA[
arXiv:2511.08223v1 Announce Type: cross 
Abstract: Reichel (2025) defined the Bariance as $\mathrm{Bariance}(x)=\frac{1}{n(n-1)}\sum_{i<j}(x_i-x_j)^2$, which admits an $O(n)$ reformulation using scalar sums. We extend this to the covariance matrix by showing that $\mathrm{Cov}(X)=\frac{1}{n-1}\!\left(X^\top X-\frac{1}{n}\,s\,s^\top\right)$ with $s=X^\top \mathbf{1}_n$ is algebraically identical to the pairwise-difference form yet avoids explicit centering. Computation reduces to a single $p\times p$ outer matrix product and one subtraction. Empirical benchmarks in Python show clear runtime gains over numpy.cov in non-BLAS-tuned settings. Faster Gram routines such as RXTX (Rybin et. al) for $XX^\top$ further reduce total cost.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Performance Analysis of Multi-Fidelity Residual Physics-Informed Neural Process-Based State Estimation for Robotic Systems</title>
<link>https://arxiv.org/abs/2511.08231</link>
<guid>https://arxiv.org/abs/2511.08231</guid>
<content:encoded><![CDATA[
arXiv:2511.08231v1 Announce Type: cross 
Abstract: Various neural network architectures are used in many of the state-of-the-art approaches for real-time nonlinear state estimation. With the ever-increasing incorporation of these data-driven models into the estimation domain, model predictions with reliable margins of error are a requirement -- especially for safety-critical applications. This paper discusses the application of a novel real-time, data-driven estimation approach based on the multi-fidelity residual physics-informed neural process (MFR-PINP) toward the real-time state estimation of a robotic system. Specifically, we address the model-mismatch issue of selecting an accurate kinematic model by tasking the MFR-PINP to also learn the residuals between simple, low-fidelity predictions and complex, high-fidelity ground-truth dynamics. To account for model uncertainty present in a physical implementation, robust uncertainty guarantees from the split conformal (SC) prediction framework are modeled in the training and inference paradigms. We provide implementation details of our MFR-PINP-based estimator for a hybrid online learning setting to validate our model's usage in real-time applications. Experimental results of our approach's performance in comparison to the state-of-the-art variants of the Kalman filter (i.e. unscented Kalman filter and deep Kalman filter) in estimation scenarios showed promising results for the MFR-PINP model as a viable option in real-time estimation tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompt Tuning for Natural Language to SQL with Embedding Fine-Tuning and RAG</title>
<link>https://arxiv.org/abs/2511.08245</link>
<guid>https://arxiv.org/abs/2511.08245</guid>
<content:encoded><![CDATA[
arXiv:2511.08245v1 Announce Type: cross 
Abstract: This paper introduces an Error Correction through Prompt Tuning for NL-to-SQL, leveraging the latest advancements in generative pre-training-based LLMs and RAG. Our work addresses the crucial need for efficient and accurate translation of natural language queries into SQL expressions in various settings with the growing use of natural language interfaces. We explore the evolution of NLIDBs from early rule-based systems to advanced neural network-driven approaches. Drawing inspiration from the medical diagnostic process, we propose a novel framework integrating an error correction mechanism that diagnoses error types, identifies their causes, provides fixing instructions, and applies these corrections to SQL queries. This approach is further enriched by embedding fine-tuning and RAG, which harnesses external knowledge bases for improved accuracy and transparency. Through comprehensive experiments, we demonstrate that our framework achieves a significant 12 percent accuracy improvement over existing baselines, highlighting its potential to revolutionize data access and handling in contemporary data-driven environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Calibration of Multi-Label Bird Sound Classifiers</title>
<link>https://arxiv.org/abs/2511.08261</link>
<guid>https://arxiv.org/abs/2511.08261</guid>
<content:encoded><![CDATA[
arXiv:2511.08261v1 Announce Type: cross 
Abstract: Passive acoustic monitoring enables large-scale biodiversity assessment, but reliable classification of bioacoustic sounds requires not only high accuracy but also well-calibrated uncertainty estimates to ground decision-making. In bioacoustics, calibration is challenged by overlapping vocalisations, long-tailed species distributions, and distribution shifts between training and deployment data. The calibration of multi-label deep learning classifiers within the domain of bioacoustics has not yet been assessed. We systematically benchmark the calibration of four state-of-the-art multi-label bird sound classifiers on the BirdSet benchmark, evaluating both global, per-dataset and per-class calibration using threshold-free calibration metrics (ECE, MCS) alongside discrimination metrics (cmAP). Model calibration varies significantly across datasets and classes. While Perch v2 and ConvNeXt$_{BS}$ show better global calibration, results vary between datasets. Both models indicate consistent underconfidence, while AudioProtoPNet and BirdMAE are mostly overconfident. Surprisingly, calibration seems to be better for less frequent classes. Using simple post hoc calibration methods we demonstrate a straightforward way to improve calibration. A small labelled calibration set is sufficient to significantly improve calibration with Platt scaling, while global calibration parameters suffer from dataset variability. Our findings highlight the importance of evaluating and improving uncertainty calibration in bioacoustic classifiers.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>X-IONet: Cross-Platform Inertial Odometry Network with Dual-Stage Attention</title>
<link>https://arxiv.org/abs/2511.08277</link>
<guid>https://arxiv.org/abs/2511.08277</guid>
<content:encoded><![CDATA[
arXiv:2511.08277v1 Announce Type: cross 
Abstract: Learning-based inertial odometry has achieved remarkable progress in pedestrian navigation. However, extending these methods to quadruped robots remains challenging due to their distinct and highly dynamic motion patterns. Models that perform well on pedestrian data often experience severe degradation when deployed on legged platforms. To tackle this challenge, we introduce X-IONet, a cross-platform inertial odometry framework that operates solely using a single Inertial Measurement Unit (IMU). X-IONet incorporates a rule-based expert selection module to classify motion platforms and route IMU sequences to platform-specific expert networks. The displacement prediction network features a dual-stage attention architecture that jointly models long-range temporal dependencies and inter-axis correlations, enabling accurate motion representation. It outputs both displacement and associated uncertainty, which are further fused through an Extended Kalman Filter (EKF) for robust state estimation. Extensive experiments on public pedestrian datasets and a self-collected quadruped robot dataset demonstrate that X-IONet achieves state-of-the-art performance, reducing Absolute Trajectory Error (ATE) by 14.3% and Relative Trajectory Error (RTE) by 11.4% on pedestrian data, and by 52.8% and 41.3% on quadruped robot data. These results highlight the effectiveness of X-IONet in advancing accurate and robust inertial navigation across both human and legged robot platforms.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Treatment Effect Estimation with Unlabeled Covariates via Generalized Riesz Regression</title>
<link>https://arxiv.org/abs/2511.08303</link>
<guid>https://arxiv.org/abs/2511.08303</guid>
<content:encoded><![CDATA[
arXiv:2511.08303v1 Announce Type: cross 
Abstract: This study investigates treatment effect estimation in the semi-supervised setting, where we can use not only the standard triple of covariates, treatment indicator, and outcome, but also unlabeled auxiliary covariates. For this problem, we develop efficiency bounds and efficient estimators whose asymptotic variance aligns with the efficiency bound. In the analysis, we introduce two different data-generating processes: the one-sample setting and the two-sample setting. The one-sample setting considers the case where we can observe treatment indicators and outcomes for a part of the dataset, which is also called the censoring setting. In contrast, the two-sample setting considers two independent datasets with labeled and unlabeled data, which is also called the case-control setting or the stratified setting. In both settings, we find that by incorporating auxiliary covariates, we can lower the efficiency bound and obtain an estimator with an asymptotic variance smaller than that without such auxiliary covariates.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concentration bounds on response-based vector embeddings of black-box generative models</title>
<link>https://arxiv.org/abs/2511.08307</link>
<guid>https://arxiv.org/abs/2511.08307</guid>
<content:encoded><![CDATA[
arXiv:2511.08307v1 Announce Type: cross 
Abstract: Generative models, such as large language models or text-to-image diffusion models, can generate relevant responses to user-given queries. Response-based vector embeddings of generative models facilitate statistical analysis and inference on a given collection of black-box generative models. The Data Kernel Perspective Space embedding is one particular method of obtaining response-based vector embeddings for a given set of generative models, already discussed in the literature. In this paper, under appropriate regularity conditions, we establish high probability concentration bounds on the sample vector embeddings for a given set of generative models, obtained through the method of Data Kernel Perspective Space embedding. Our results tell us the required number of sample responses needed in order to approximate the population-level vector embeddings with a desired level of accuracy. The algebraic tools used to establish our results can be used further for establishing concentration bounds on Classical Multidimensional Scaling embeddings in general, when the dissimilarities are observed with noise.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BDD2Seq: Enabling Scalable Reversible-Circuit Synthesis via Graph-to-Sequence Learning</title>
<link>https://arxiv.org/abs/2511.08315</link>
<guid>https://arxiv.org/abs/2511.08315</guid>
<content:encoded><![CDATA[
arXiv:2511.08315v1 Announce Type: cross 
Abstract: Binary Decision Diagrams (BDDs) are instrumental in many electronic design automation (EDA) tasks thanks to their compact representation of Boolean functions. In BDD-based reversible-circuit synthesis, which is critical for quantum computing, the chosen variable ordering governs the number of BDD nodes and thus the key metrics of resource consumption, such as Quantum Cost. Because finding an optimal variable ordering for BDDs is an NP-complete problem, existing heuristics often degrade as circuit complexity grows. We introduce BDD2Seq, a graph-to-sequence framework that couples a Graph Neural Network encoder with a Pointer-Network decoder and Diverse Beam Search to predict high-quality orderings. By treating the circuit netlist as a graph, BDD2Seq learns structural dependencies that conventional heuristics overlooked, yielding smaller BDDs and faster synthesis. Extensive experiments on three public benchmarks show that BDD2Seq achieves around 1.4 times lower Quantum Cost and 3.7 times faster synthesis than modern heuristic algorithms. To the best of our knowledge, this is the first work to tackle the variable-ordering problem in BDD-based reversible-circuit synthesis with a graph-based generative model and diversity-promoting decoding.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Negative Flips via Margin Preserving Training</title>
<link>https://arxiv.org/abs/2511.08322</link>
<guid>https://arxiv.org/abs/2511.08322</guid>
<content:encoded><![CDATA[
arXiv:2511.08322v1 Announce Type: cross 
Abstract: Minimizing inconsistencies across successive versions of an AI system is as crucial as reducing the overall error. In image classification, such inconsistencies manifest as negative flips, where an updated model misclassifies test samples that were previously classified correctly. This issue becomes increasingly pronounced as the number of training classes grows over time, since adding new categories reduces the margin of each class and may introduce conflicting patterns that undermine their learning process, thereby degrading performance on the original subset. To mitigate negative flips, we propose a novel approach that preserves the margins of the original model while learning an improved one. Our method encourages a larger relative margin between the previously learned and newly introduced classes by introducing an explicit margin-calibration term on the logits. However, overly constraining the logit margin for the new classes can significantly degrade their accuracy compared to a new independently trained model. To address this, we integrate a double-source focal distillation loss with the previous model and a new independently trained model, learning an appropriate decision margin from both old and new data, even under a logit margin calibration. Extensive experiments on image classification benchmarks demonstrate that our approach consistently reduces the negative flip rate with high overall accuracy.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentPRM: Process Reward Models for LLM Agents via Step-Wise Promise and Progress</title>
<link>https://arxiv.org/abs/2511.08325</link>
<guid>https://arxiv.org/abs/2511.08325</guid>
<content:encoded><![CDATA[
arXiv:2511.08325v1 Announce Type: cross 
Abstract: Despite rapid development, large language models (LLMs) still encounter challenges in multi-turn decision-making tasks (i.e., agent tasks) like web shopping and browser navigation, which require making a sequence of intelligent decisions based on environmental feedback. Previous work for LLM agents typically relies on elaborate prompt engineering or fine-tuning with expert trajectories to improve performance. In this work, we take a different perspective: we explore constructing process reward models (PRMs) to evaluate each decision and guide the agent's decision-making process. Unlike LLM reasoning, where each step is scored based on correctness, actions in agent tasks do not have a clear-cut correctness. Instead, they should be evaluated based on their proximity to the goal and the progress they have made. Building on this insight, we propose a re-defined PRM for agent tasks, named AgentPRM, to capture both the interdependence between sequential decisions and their contribution to the final goal. This enables better progress tracking and exploration-exploitation balance. To scalably obtain labeled data for training AgentPRM, we employ a Temporal Difference-based (TD-based) estimation method combined with Generalized Advantage Estimation (GAE), which proves more sample-efficient than prior methods. Extensive experiments across different agentic tasks show that AgentPRM is over $8\times$ more compute-efficient than baselines, and it demonstrates robust improvement when scaling up test-time compute. Moreover, we perform detailed analyses to show how our method works and offer more insights, e.g., applying AgentPRM to the reinforcement learning of LLM agents.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Network Traffic Analysis: Compatible network flows for ML models</title>
<link>https://arxiv.org/abs/2511.08345</link>
<guid>https://arxiv.org/abs/2511.08345</guid>
<content:encoded><![CDATA[
arXiv:2511.08345v1 Announce Type: cross 
Abstract: To ensure that Machine Learning (ML) models can perform a robust detection and classification of cyberattacks, it is essential to train them with high-quality datasets with relevant features. However, it can be difficult to accurately represent the complex traffic patterns of an attack, especially in Internet-of-Things (IoT) networks. This paper studies the impact that seemingly similar features created by different network traffic flow exporters can have on the generalization and robustness of ML models. In addition to the original CSV files of the Bot-IoT, IoT-23, and CICIoT23 datasets, the raw network packets of their PCAP files were analysed with the HERA tool, generating new labelled flows and extracting consistent features for new CSV versions. To assess the usefulness of these new flows for intrusion detection, they were compared with the original versions and were used to fine-tune multiple models. Overall, the results indicate that directly analysing and preprocessing PCAP files, instead of just using the commonly available CSV files, enables the computation of more relevant features to train bagging and gradient boosting decision tree ensembles. It is important to continue improving feature extraction and feature selection processes to make different datasets more compatible and enable a trustworthy evaluation and comparison of the ML models used in cybersecurity solutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revealing the Hidden Third Dimension of Point Defects in Two-Dimensional MXenes</title>
<link>https://arxiv.org/abs/2511.08350</link>
<guid>https://arxiv.org/abs/2511.08350</guid>
<content:encoded><![CDATA[
arXiv:2511.08350v1 Announce Type: cross 
Abstract: Point defects govern many important functional properties of two-dimensional (2D) materials. However, resolving the three-dimensional (3D) arrangement of these defects in multi-layer 2D materials remains a fundamental challenge, hindering rational defect engineering. Here, we overcome this limitation using an artificial intelligence-guided electron microscopy workflow to map the 3D topology and clustering of atomic vacancies in Ti$_3$C$_2$T$_X$ MXene. Our approach reconstructs the 3D coordinates of vacancies across hundreds of thousands of lattice sites, generating robust statistical insight into their distribution that can be correlated with specific synthesis pathways. This large-scale data enables us to classify a hierarchy of defect structures--from isolated vacancies to nanopores--revealing their preferred formation and interaction mechanisms, as corroborated by molecular dynamics simulations. This work provides a generalizable framework for understanding and ultimately controlling point defects across large volumes, paving the way for the rational design of defect-engineered functional 2D materials.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extreme Model Compression with Structured Sparsity at Low Precision</title>
<link>https://arxiv.org/abs/2511.08360</link>
<guid>https://arxiv.org/abs/2511.08360</guid>
<content:encoded><![CDATA[
arXiv:2511.08360v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are used in many applications, but their large size and high computational cost make them hard to run on devices with limited resources. Two widely used techniques to address this challenge are weight quantization, which lowers the precision of all weights, and structured sparsity, which removes unimportant weights while retaining the important ones at full precision. Although both are effective individually, they are typically studied in isolation due to their compounded negative impact on model accuracy when combined. In this work, we introduce SLOPE Structured Sparsity at Low Precision), a unified framework, to effectively combine structured sparsity and low-bit quantization in a principled way. We show that naively combining sparsity and quantization severely harms performance due to the compounded impact of both techniques. To address this, we propose a training-time regularization strategy that minimizes the discrepancy between full-precision weights and their sparse, quantized counterparts by promoting angular alignment rather than direct matching. On ResNet-18, SLOPE achieves $\sim20\times$ model size reduction while retaining $\sim$99% of the original accuracy. It consistently outperforms state-of-the-art quantization and structured sparsity methods across classification, detection, and segmentation tasks on models such as ResNet-18, ViT-Small, and Mask R-CNN.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Information-Minimal Geometry for Qubit-Efficient Optimization</title>
<link>https://arxiv.org/abs/2511.08362</link>
<guid>https://arxiv.org/abs/2511.08362</guid>
<content:encoded><![CDATA[
arXiv:2511.08362v1 Announce Type: cross 
Abstract: Qubit-efficient optimization seeks to represent an $N$-variable combinatorial problem within a Hilbert space smaller than $2^N$, using only as much quantum structure as the objective itself requires. Quadratic unconstrained binary optimization (QUBO) problems, for example, depend only on pairwise information -- expectations and correlations between binary variables -- yet standard quantum circuits explore exponentially large state spaces. We recast qubit-efficient optimization as a geometry problem: the minimal representation should match the $O(N^2)$ structure of quadratic objectives. The key insight is that the local-consistency problem -- ensuring that pairwise marginals correspond to a realizable global distribution -- coincides exactly with the Sherali-Adams level-2 polytope $\mathrm{SA}(2)$, the tightest convex relaxation expressible at the two-body level. Previous qubit-efficient approaches enforced this consistency only implicitly. Here we make it explicit: (a) anchoring learning to the $\mathrm{SA}(2)$ geometry, (b) projecting via a differentiable iterative-proportional-fitting (IPF) step, and (c) decoding through a maximum-entropy Gibbs sampler. This yields a logarithmic-width pipeline ($2\lceil\log_2 N\rceil + 2$ qubits) that is classically simulable yet achieves strong empirical performance. On Gset Max-Cut instances (N=800--2000), depth-2--3 circuits reach near-optimal ratios ($r^* \approx 0.99$), surpassing direct $\mathrm{SA}(2)$ baselines. The framework resolves the local-consistency gap by giving it a concrete convex geometry and a minimal differentiable projection, establishing a clean polyhedral baseline. Extending beyond $\mathrm{SA}(2)$ naturally leads to spectrahedral geometries, where curvature encodes global coherence and genuine quantum structure becomes necessary.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOM Directions are Better than One: Multi-Directional Refusal Suppression in Language Models</title>
<link>https://arxiv.org/abs/2511.08379</link>
<guid>https://arxiv.org/abs/2511.08379</guid>
<content:encoded><![CDATA[
arXiv:2511.08379v1 Announce Type: cross 
Abstract: Refusal refers to the functional behavior enabling safety-aligned language models to reject harmful or unethical prompts. Following the growing scientific interest in mechanistic interpretability, recent work encoded refusal behavior as a single direction in the model's latent space; e.g., computed as the difference between the centroids of harmful and harmless prompt representations. However, emerging evidence suggests that concepts in LLMs often appear to be encoded as a low-dimensional manifold embedded in the high-dimensional latent space. Motivated by these findings, we propose a novel method leveraging Self-Organizing Maps (SOMs) to extract multiple refusal directions. To this end, we first prove that SOMs generalize the prior work's difference-in-means technique. We then train SOMs on harmful prompt representations to identify multiple neurons. By subtracting the centroid of harmless representations from each neuron, we derive a set of multiple directions expressing the refusal concept. We validate our method on an extensive experimental setup, demonstrating that ablating multiple directions from models' internals outperforms not only the single-direction baseline but also specialized jailbreak algorithms, leading to an effective suppression of refusal. Finally, we conclude by analyzing the mechanistic implications of our approach.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction Dynamics as a Reward Signal for LLMs</title>
<link>https://arxiv.org/abs/2511.08394</link>
<guid>https://arxiv.org/abs/2511.08394</guid>
<content:encoded><![CDATA[
arXiv:2511.08394v1 Announce Type: cross 
Abstract: The alignment of Large Language Models (LLMs) for multi-turn conversations typically relies on reward signals derived from the content of the text. This approach, however, overlooks a rich, complementary source of signal: the dynamics of the interaction itself. This paper introduces TRACE (Trajectory-based Reward for Agent Collaboration Estimation), a novel reward signal derived from the geometric properties of a dialogue's embedding trajectory--a concept we term 'conversational geometry'. Our central finding is that a reward model trained only on these structural signals achieves a pairwise accuracy (68.20%) comparable to a powerful LLM baseline that analyzes the full transcript (70.04%). Furthermore, a hybrid model combining interaction dynamics with textual analysis achieves the highest performance (80.17%), demonstrating their complementary nature. This work provides strong evidence that for interactive settings, how an agent communicates is as powerful a predictor of success as what it says, offering a new, privacy-preserving framework that not only aligns agents but also serves as a diagnostic tool for understanding the distinct interaction patterns that drive successful collaboration.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Source-Optimal Training is Transfer-Suboptimal</title>
<link>https://arxiv.org/abs/2511.08401</link>
<guid>https://arxiv.org/abs/2511.08401</guid>
<content:encoded><![CDATA[
arXiv:2511.08401v1 Announce Type: cross 
Abstract: We prove a fundamental misalignment in transfer learning: the source regularization that minimizes source risk almost never coincides with the regularization maximizing transfer benefit. Through sharp phase boundaries for L2-SP ridge regression, we characterize the transfer-optimal source penalty $\tau_0^*$ and show it diverges predictably from task-optimal values, requiring stronger regularization in high-SNR regimes and weaker regularization in low-SNR regimes. Additionally, in isotropic settings the decision to transfer is remarkably independent of target sample size and noise, depending only on task alignment and source characteristics. CIFAR-10 and MNIST experiments confirm this counterintuitive pattern persists in non-linear networks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anatomy-VLM: A Fine-grained Vision-Language Model for Medical Interpretation</title>
<link>https://arxiv.org/abs/2511.08402</link>
<guid>https://arxiv.org/abs/2511.08402</guid>
<content:encoded><![CDATA[
arXiv:2511.08402v1 Announce Type: cross 
Abstract: Accurate disease interpretation from radiology remains challenging due to imaging heterogeneity. Achieving expert-level diagnostic decisions requires integration of subtle image features with clinical knowledge. Yet major vision-language models (VLMs) treat images as holistic entities and overlook fine-grained image details that are vital for disease diagnosis. Clinicians analyze images by utilizing their prior medical knowledge and identify anatomical structures as important region of interests (ROIs). Inspired from this human-centric workflow, we introduce Anatomy-VLM, a fine-grained, vision-language model that incorporates multi-scale information. First, we design a model encoder to localize key anatomical features from entire medical images. Second, these regions are enriched with structured knowledge for contextually-aware interpretation. Finally, the model encoder aligns multi-scale medical information to generate clinically-interpretable disease prediction. Anatomy-VLM achieves outstanding performance on both in- and out-of-distribution datasets. We also validate the performance of Anatomy-VLM on downstream image segmentation tasks, suggesting that its fine-grained alignment captures anatomical and pathology-related knowledge. Furthermore, the Anatomy-VLM's encoder facilitates zero-shot anatomy-wise interpretation, providing its strong expert-level clinical interpretation capabilities.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative AI Meets 6G and Beyond: Diffusion Models for Semantic Communications</title>
<link>https://arxiv.org/abs/2511.08416</link>
<guid>https://arxiv.org/abs/2511.08416</guid>
<content:encoded><![CDATA[
arXiv:2511.08416v1 Announce Type: cross 
Abstract: Semantic communications mark a paradigm shift from bit-accurate transmission toward meaning-centric communication, essential as wireless systems approach theoretical capacity limits. The emergence of generative AI has catalyzed generative semantic communications, where receivers reconstruct content from minimal semantic cues by leveraging learned priors. Among generative approaches, diffusion models stand out for their superior generation quality, stable training dynamics, and rigorous theoretical foundations. However, the field currently lacks systematic guidance connecting diffusion techniques to communication system design, forcing researchers to navigate disparate literatures. This article provides the first comprehensive tutorial on diffusion models for generative semantic communications. We present score-based diffusion foundations and systematically review three technical pillars: conditional diffusion for controllable generation, efficient diffusion for accelerated inference, and generalized diffusion for cross-domain adaptation. In addition, we introduce an inverse problem perspective that reformulates semantic decoding as posterior inference, bridging semantic communications with computational imaging. Through analysis of human-centric, machine-centric, and agent-centric scenarios, we illustrate how diffusion models enable extreme compression while maintaining semantic fidelity and robustness. By bridging generative AI innovations with communication system design, this article aims to establish diffusion models as foundational components of next-generation wireless networks and beyond.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Safety Guarantee for Stochastic Control Systems Using Average Reward MDPs</title>
<link>https://arxiv.org/abs/2511.08419</link>
<guid>https://arxiv.org/abs/2511.08419</guid>
<content:encoded><![CDATA[
arXiv:2511.08419v1 Announce Type: cross 
Abstract: Safety in stochastic control systems, which are subject to random noise with a known probability distribution, aims to compute policies that satisfy predefined operational constraints with high confidence throughout the uncertain evolution of the state variables. The unpredictable evolution of state variables poses a significant challenge for meeting predefined constraints using various control methods. To address this, we present a new algorithm that computes safe policies to determine the safety level across a finite state set. This algorithm reduces the safety objective to the standard average reward Markov Decision Process (MDP) objective. This reduction enables us to use standard techniques, such as linear programs, to compute and analyze safe policies. We validate the proposed method numerically on the Double Integrator and the Inverted Pendulum systems. Results indicate that the average-reward MDPs solution is more comprehensive, converges faster, and offers higher quality compared to the minimum discounted-reward solution.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identification of Empirical Constitutive Models for Age-Hardenable Aluminium Alloy and High-Chromium Martensitic Steel Using Symbolic Regression</title>
<link>https://arxiv.org/abs/2511.08424</link>
<guid>https://arxiv.org/abs/2511.08424</guid>
<content:encoded><![CDATA[
arXiv:2511.08424v1 Announce Type: cross 
Abstract: Process-structure-property relationships are fundamental in materials science and engineering and are key to the development of new and improved materials. Symbolic regression serves as a powerful tool for uncovering mathematical models that describe these relationships. It can automatically generate equations to predict material behaviour under specific manufacturing conditions and optimize performance characteristics such as strength and elasticity.
  The present work illustrates how symbolic regression can derive constitutive models that describe the behaviour of various metallic alloys during plastic deformation. Constitutive modelling is a mathematical framework for understanding the relationship between stress and strain in materials under different loading conditions. In this study, two materials (age-hardenable aluminium alloy and high-chromium martensitic steel) and two different testing methods (compression and tension) are considered to obtain the required stress-strain data. The results highlight the benefits of using symbolic regression while also discussing potential challenges.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Galactification: painting galaxies onto dark matter only simulations using a transformer-based model</title>
<link>https://arxiv.org/abs/2511.08438</link>
<guid>https://arxiv.org/abs/2511.08438</guid>
<content:encoded><![CDATA[
arXiv:2511.08438v1 Announce Type: cross 
Abstract: Connecting the formation and evolution of galaxies to the large-scale structure is crucial for interpreting cosmological observations. While hydrodynamical simulations accurately model the correlated properties of galaxies, they are computationally prohibitive to run over volumes that match modern surveys. We address this by developing a framework to rapidly generate mock galaxy catalogs conditioned on inexpensive dark-matter-only simulations. We present a multi-modal, transformer-based model that takes 3D dark matter density and velocity fields as input, and outputs a corresponding point cloud of galaxies with their physical properties. We demonstrate that our trained model faithfully reproduces a variety of galaxy summary statistics and correctly captures their variation with changes in the underlying cosmological and astrophysical parameters, making it the first accelerated forward model to capture all the relevant galaxy properties, their full spatial distribution, and their conditional dependencies in hydrosimulations.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable Blood Cell Detection via Unified Dataset and Faster R-CNN</title>
<link>https://arxiv.org/abs/2511.08465</link>
<guid>https://arxiv.org/abs/2511.08465</guid>
<content:encoded><![CDATA[
arXiv:2511.08465v1 Announce Type: cross 
Abstract: This paper presents a comprehensive methodology and comparative performance analysis for the automated classification and object detection of peripheral blood cells (PBCs) in microscopic images. Addressing the critical challenge of data scarcity and heterogeneity, robust data pipeline was first developed to standardize and merge four public datasets (PBC, BCCD, Chula, Sickle Cell) into a unified resource. Then employed a state-of-the-art Faster R-CNN object detection framework, leveraging a ResNet-50-FPN backbone. Comparative training rigorously evaluated a randomly initialized baseline model (Regimen 1) against a Transfer Learning Regimen (Regimen 2), initialized with weights pre-trained on the Microsoft COCO dataset. The results demonstrate that the Transfer Learning approach achieved significantly faster convergence and superior stability, culminating in a final validation loss of 0.08666, a substantial improvement over the baseline. This validated methodology establishes a robust foundation for building high-accuracy, deployable systems for automated hematological diagnosis.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Autonomous and Efficient Cybersecurity: A Multi-Objective AutoML-based Intrusion Detection System</title>
<link>https://arxiv.org/abs/2511.08491</link>
<guid>https://arxiv.org/abs/2511.08491</guid>
<content:encoded><![CDATA[
arXiv:2511.08491v1 Announce Type: cross 
Abstract: With increasingly sophisticated cybersecurity threats and rising demand for network automation, autonomous cybersecurity mechanisms are becoming critical for securing modern networks. The rapid expansion of Internet of Things (IoT) systems amplifies these challenges, as resource-constrained IoT devices demand scalable and efficient security solutions. In this work, an innovative Intrusion Detection System (IDS) utilizing Automated Machine Learning (AutoML) and Multi-Objective Optimization (MOO) is proposed for autonomous and optimized cyber-attack detection in modern networking environments. The proposed IDS framework integrates two primary innovative techniques: Optimized Importance and Percentage-based Automated Feature Selection (OIP-AutoFS) and Optimized Performance, Confidence, and Efficiency-based Combined Algorithm Selection and Hyperparameter Optimization (OPCE-CASH). These components optimize feature selection and model learning processes to strike a balance between intrusion detection effectiveness and computational efficiency. This work presents the first IDS framework that integrates all four AutoML stages and employs multi-objective optimization to jointly optimize detection effectiveness, efficiency, and confidence for deployment in resource-constrained systems. Experimental evaluations over two benchmark cybersecurity datasets demonstrate that the proposed MOO-AutoML IDS outperforms state-of-the-art IDSs, establishing a new benchmark for autonomous, efficient, and optimized security for networks. Designed to support IoT and edge environments with resource constraints, the proposed framework is applicable to a variety of autonomous cybersecurity applications across diverse networked environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPEAR-MM: Selective Parameter Evaluation and Restoration via Model Merging for Efficient Financial LLM Adaptation</title>
<link>https://arxiv.org/abs/2511.08500</link>
<guid>https://arxiv.org/abs/2511.08500</guid>
<content:encoded><![CDATA[
arXiv:2511.08500v1 Announce Type: cross 
Abstract: Large language models (LLMs) adapted to financial domains often suffer from catastrophic forgetting of general reasoning capabilities essential for customer interactions and complex financial analysis. We introduce Selective Parameter Evaluation and Restoration via Model Merging (SPEAR-MM), a practical framework that preserves critical capabilities while enabling domain adaptation. Our method approximates layer-wise impact on external benchmarks through post-hoc analysis, then selectively freezes or restores transformer layers via spherical interpolation merging. Applied to LLaMA-3.1-8B for financial tasks, SPEAR-MM achieves 91.2% retention of general capabilities versus 69.7% for standard continual pretraining, while maintaining 94% of domain adaptation gains. The approach provides interpretable trade-off control and reduces computational costs by 90% crucial for resource-constrained financial institutions.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured RAG for Answering Aggregative Questions</title>
<link>https://arxiv.org/abs/2511.08505</link>
<guid>https://arxiv.org/abs/2511.08505</guid>
<content:encoded><![CDATA[
arXiv:2511.08505v1 Announce Type: cross 
Abstract: Retrieval-Augmented Generation (RAG) has become the dominant approach for answering questions over large corpora. However, current datasets and methods are highly focused on cases where only a small part of the corpus (usually a few paragraphs) is relevant per query, and fail to capture the rich world of aggregative queries. These require gathering information from a large set of documents and reasoning over them. To address this gap, we propose S-RAG, an approach specifically designed for such queries. At ingestion time, S-RAG constructs a structured representation of the corpus; at inference time, it translates natural-language queries into formal queries over said representation. To validate our approach and promote further research in this area, we introduce two new datasets of aggregative queries: HOTELS and WORLD CUP. Experiments with S-RAG on the newly introduced datasets, as well as on a public benchmark, demonstrate that it substantially outperforms both common RAG systems and long-context LLMs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CleverBirds: A Multiple-Choice Benchmark for Fine-grained Human Knowledge Tracing</title>
<link>https://arxiv.org/abs/2511.08512</link>
<guid>https://arxiv.org/abs/2511.08512</guid>
<content:encoded><![CDATA[
arXiv:2511.08512v1 Announce Type: cross 
Abstract: Mastering fine-grained visual recognition, essential in many expert domains, can require that specialists undergo years of dedicated training. Modeling the progression of such expertize in humans remains challenging, and accurately inferring a human learner's knowledge state is a key step toward understanding visual learning. We introduce CleverBirds, a large-scale knowledge tracing benchmark for fine-grained bird species recognition. Collected by the citizen-science platform eBird, it offers insight into how individuals acquire expertize in complex fine-grained classification. More than 40,000 participants have engaged in the quiz, answering over 17 million multiple-choice questions spanning over 10,000 bird species, with long-range learning patterns across an average of 400 questions per participant. We release this dataset to support the development and evaluation of new methods for visual knowledge tracing. We show that tracking learners' knowledge is challenging, especially across participant subgroups and question types, with different forms of contextual information offering varying degrees of predictive benefit. CleverBirds is among the largest benchmark of its kind, offering a substantially higher number of learnable concepts. With it, we hope to enable new avenues for studying the development of visual expertize over time and across individuals.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SENCA-st: Integrating Spatial Transcriptomics and Histopathology with Cross Attention Shared Encoder for Region Identification in Cancer Pathology</title>
<link>https://arxiv.org/abs/2511.08573</link>
<guid>https://arxiv.org/abs/2511.08573</guid>
<content:encoded><![CDATA[
arXiv:2511.08573v1 Announce Type: cross 
Abstract: Spatial transcriptomics is an emerging field that enables the identification of functional regions based on the spatial distribution of gene expression. Integrating this functional information present in transcriptomic data with structural data from histopathology images is an active research area with applications in identifying tumor substructures associated with cancer drug resistance. Current histopathology-spatial-transcriptomic region segmentation methods suffer due to either making spatial transcriptomics prominent by using histopathology features just to assist processing spatial transcriptomics data or using vanilla contrastive learning that make histopathology images prominent due to only promoting common features losing functional information. In both extremes, the model gets either lost in the noise of spatial transcriptomics or overly smoothed, losing essential information. Thus, we propose our novel architecture SENCA-st (Shared Encoder with Neighborhood Cross Attention) that preserves the features of both modalities. More importantly, it emphasizes regions that are structurally similar in histopathology but functionally different on spatial transcriptomics using cross-attention. We demonstrate the superior performance of our model that surpasses state-of-the-art methods in detecting tumor heterogeneity and tumor micro-environment regions, a clinically crucial aspect.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think-at-Hard: Selective Latent Iterations to Improve Reasoning Language Models</title>
<link>https://arxiv.org/abs/2511.08577</link>
<guid>https://arxiv.org/abs/2511.08577</guid>
<content:encoded><![CDATA[
arXiv:2511.08577v1 Announce Type: cross 
Abstract: Improving reasoning capabilities of Large Language Models (LLMs), especially under parameter constraints, is crucial for real-world applications. Prior work proposes recurrent transformers, which allocate a fixed number of extra iterations per token to improve generation quality. After the first, standard forward pass, instead of verbalization, last-layer hidden states are fed back as inputs for additional iterations to refine token predictions. Yet we identify a latent overthinking phenomenon: easy token predictions that are already correct after the first pass are sometimes revised into errors in additional iterations. To address this, we propose Think-at-Hard (TaH), a dynamic latent thinking method that iterates deeper only at hard tokens. It employs a lightweight neural decider to trigger latent iterations only at tokens that are likely incorrect after the standard forward pass. During latent iterations, Low-Rank Adaptation (LoRA) modules shift the LLM objective from general next-token prediction to focused hard-token refinement. We further introduce a duo-causal attention mechanism that extends attention from the token sequence dimension to an additional iteration depth dimension. This enables cross-iteration information flow while maintaining full sequential parallelism. Experiments show that TaH boosts LLM reasoning performance across five challenging benchmarks while maintaining the same parameter count. Compared with baselines that iterate twice for all output tokens, TaH delivers 8.1-11.3% accuracy gains while exempting 94% of tokens from the second iteration. Against strong single-iteration Qwen3 models finetuned with the same data, it also delivers 4.0-5.0% accuracy gains. When allowing less than 3% additional parameters from LoRA and the iteration decider, the gains increase to 8.5-12.6% and 5.3-5.4%, respectively. Our code is available at https://github.com/thu-nics/TaH.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Language Models to Explain Their Own Computations</title>
<link>https://arxiv.org/abs/2511.08579</link>
<guid>https://arxiv.org/abs/2511.08579</guid>
<content:encoded><![CDATA[
arXiv:2511.08579v1 Announce Type: cross 
Abstract: Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs' privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs' internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models' privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeFA-Policy: Fast and Accurate Visuomotor Policy Learning with Selective Flow Alignment</title>
<link>https://arxiv.org/abs/2511.08583</link>
<guid>https://arxiv.org/abs/2511.08583</guid>
<content:encoded><![CDATA[
arXiv:2511.08583v1 Announce Type: cross 
Abstract: Developing efficient and accurate visuomotor policies poses a central challenge in robotic imitation learning. While recent rectified flow approaches have advanced visuomotor policy learning, they suffer from a key limitation: After iterative distillation, generated actions may deviate from the ground-truth actions corresponding to the current visual observation, leading to accumulated error as the reflow process repeats and unstable task execution. We present Selective Flow Alignment (SeFA), an efficient and accurate visuomotor policy learning framework. SeFA resolves this challenge by a selective flow alignment strategy, which leverages expert demonstrations to selectively correct generated actions and restore consistency with observations, while preserving multimodality. This design introduces a consistency correction mechanism that ensures generated actions remain observation-aligned without sacrificing the efficiency of one-step flow inference. Extensive experiments across both simulated and real-world manipulation tasks show that SeFA Policy surpasses state-of-the-art diffusion-based and flow-based policies, achieving superior accuracy and robustness while reducing inference latency by over 98%. By unifying rectified flow efficiency with observation-consistent action generation, SeFA provides a scalable and dependable solution for real-time visuomotor policy learning. Code is available on https://github.com/RongXueZoe/SeFA.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multiplicative Reweighting for Robust Neural Network Optimization</title>
<link>https://arxiv.org/abs/2102.12192</link>
<guid>https://arxiv.org/abs/2102.12192</guid>
<content:encoded><![CDATA[
arXiv:2102.12192v5 Announce Type: replace 
Abstract: Neural networks are widespread due to their powerful performance. Yet, they degrade in the presence of noisy labels at training time. Inspired by the setting of learning with expert advice, where multiplicative weights (MW) updates were recently shown to be robust to moderate data corruptions in expert advice, we propose to use MW for reweighting examples during neural networks optimization. We theoretically establish the convergence of our method when used with gradient descent and prove its advantages in 1d cases. We then validate empirically our findings for the general case by showing that MW improves neural networks' accuracy in the presence of label noise on CIFAR-10, CIFAR-100 and Clothing1M. We also show the impact of our approach on adversarial robustness.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Counterfactual Regret Minimization</title>
<link>https://arxiv.org/abs/2305.17327</link>
<guid>https://arxiv.org/abs/2305.17327</guid>
<content:encoded><![CDATA[
arXiv:2305.17327v3 Announce Type: replace 
Abstract: Imperfect Information Games (IIGs) offer robust models for scenarios where decision-makers face uncertainty or lack complete information. Counterfactual Regret Minimization (CFR) has been one of the most successful family of algorithms for tackling IIGs. The integration of skill-based strategy learning with CFR could potentially mirror more human-like decision-making process and enhance the learning performance for complex IIGs. It enables the learning of a hierarchical strategy, wherein low-level components represent skills for solving subgames and the high-level component manages the transition between skills. In this paper, we introduce the first hierarchical version of Deep CFR (HDCFR), an innovative method that boosts learning efficiency in tasks involving extensively large state spaces and deep game trees. A notable advantage of HDCFR over previous works is its ability to facilitate learning with predefined (human) expertise and foster the acquisition of skills that can be transferred to similar tasks. To achieve this, we initially construct our algorithm on a tabular setting, encompassing hierarchical CFR updating rules and a variance-reduced Monte Carlo sampling extension. Notably, we offer the theoretical justifications, including the convergence rate of the proposed updating rule, the unbiasedness of the Monte Carlo regret estimator, and ideal criteria for effective variance reduction. Then, we employ neural networks as function approximators and develop deep learning objectives to adapt our proposed algorithms for large-scale tasks, while maintaining the theoretical support.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pruning at Initialization -- A Sketching Perspective</title>
<link>https://arxiv.org/abs/2305.17559</link>
<guid>https://arxiv.org/abs/2305.17559</guid>
<content:encoded><![CDATA[
arXiv:2305.17559v2 Announce Type: replace 
Abstract: The lottery ticket hypothesis (LTH) has increased attention to pruning neural networks at initialization. We study this problem in the linear setting. We show that finding a sparse mask at initialization is equivalent to the sketching problem introduced for efficient matrix multiplication. This gives us tools to analyze the LTH problem and gain insights into it. Specifically, using the mask found at initialization, we bound the approximation error of the pruned linear model at the end of training. We theoretically justify previous empirical evidence that the search for sparse networks may be data independent. By using the sketching perspective, we suggest a generic improvement to existing algorithms for pruning at initialization, which we show to be beneficial in the data-independent case.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Deep Learning with Decorrelated Backpropagation</title>
<link>https://arxiv.org/abs/2405.02385</link>
<guid>https://arxiv.org/abs/2405.02385</guid>
<content:encoded><![CDATA[
arXiv:2405.02385v5 Announce Type: replace 
Abstract: The backpropagation algorithm remains the dominant and most successful method for training deep neural networks (DNNs). At the same time, training DNNs at scale comes at a significant computational cost and therefore a high carbon footprint. Converging evidence suggests that input decorrelation may speed up deep learning. However, to date, this has not yet translated into substantial improvements in training efficiency in large-scale DNNs. This is mainly caused by the challenge of enforcing fast and stable network-wide decorrelation. Here, we show for the first time that much more efficient training of deep convolutional neural networks is feasible by embracing decorrelated backpropagation as a mechanism for learning. To achieve this goal we made use of a novel algorithm which induces network-wide input decorrelation using minimal computational overhead. By combining this algorithm with careful optimizations, we achieve a more than two-fold speed-up and higher test accuracy compared to backpropagation when training several deep residual networks. This demonstrates that decorrelation provides exciting prospects for efficient deep learning at scale.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ElastoGen: 4D Generative Elastodynamics</title>
<link>https://arxiv.org/abs/2405.15056</link>
<guid>https://arxiv.org/abs/2405.15056</guid>
<content:encoded><![CDATA[
arXiv:2405.15056v3 Announce Type: replace 
Abstract: We present ElastoGen, a knowledge-driven AI model that generates physically accurate 4D elastodynamics. Unlike deep models that learn from video- or image-based observations, ElastoGen leverages the principles of physics and learns from established mathematical and optimization procedures. The core idea of ElastoGen is converting the differential equation, corresponding to the nonlinear force equilibrium, into a series of iterative local convolution-like operations, which naturally fit deep architectures. We carefully build our network module following this overarching design philosophy. ElastoGen is much more lightweight in terms of both training requirements and network scale than deep generative models. Because of its alignment with actual physical procedures, ElastoGen efficiently generates accurate dynamics for a wide range of hyperelastic materials and can be easily integrated with upstream and downstream deep modules to enable end-to-end 4D generation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics</title>
<link>https://arxiv.org/abs/2406.01539</link>
<guid>https://arxiv.org/abs/2406.01539</guid>
<content:encoded><![CDATA[
arXiv:2406.01539v3 Announce Type: replace 
Abstract: On the forefront of scientific computing, Deep Learning (DL), i.e., machine learning with Deep Neural Networks (DNNs), has emerged a powerful new tool for solving Partial Differential Equations (PDEs). It has been observed that DNNs are particularly well suited to weakening the effect of the curse of dimensionality, a term coined by Richard E. Bellman in the late `50s to describe challenges such as the exponential dependence of the sample complexity, i.e., the number of samples required to solve an approximation problem, on the dimension of the ambient space. However, although DNNs have been used to solve PDEs since the `90s, the literature underpinning their mathematical efficiency in terms of numerical analysis (i.e., stability, accuracy, and sample complexity), is only recently beginning to emerge. In this paper, we leverage recent advancements in function approximation using sparsity-based techniques and random sampling to develop and analyze an efficient high-dimensional PDE solver based on DL. We show, both theoretically and numerically, that it can compete with a novel stable and accurate compressive spectral collocation method for the solution of high-dimensional, steady-state diffusion-reaction equations with periodic boundary conditions. In particular, we demonstrate a new practical existence theorem, which establishes the existence of a class of trainable DNNs with suitable bounds on the network architecture and a sufficient condition on the sample complexity, with logarithmic or, at worst, linear scaling in dimension, such that the resulting networks stably and accurately approximate a diffusion-reaction PDE with high probability.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informed Correctors for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2407.21243</link>
<guid>https://arxiv.org/abs/2407.21243</guid>
<content:encoded><![CDATA[
arXiv:2407.21243v5 Announce Type: replace 
Abstract: Discrete diffusion has emerged as a powerful framework for generative modeling in discrete domains, yet efficiently sampling from these models remains challenging. Existing sampling strategies often struggle to balance computation and sample quality when the number of sampling steps is reduced, even when the model has learned the data distribution well. To address these limitations, we propose a predictor-corrector sampling scheme where the corrector is informed by the diffusion model to more reliably counter the accumulating approximation errors. To further enhance the effectiveness of our informed corrector, we introduce complementary architectural modifications based on hollow transformers and a simple tailored training objective that leverages more training signal. We use a synthetic example to illustrate the failure modes of existing samplers and show how informed correctors alleviate these problems. On the text8 and tokenized ImageNet 256x256 datasets, our informed corrector consistently produces superior samples with fewer errors or improved FID scores for discrete diffusion models. These results underscore the potential of informed correctors for fast and high-fidelity generation using discrete diffusion. Our code is available at https://github.com/lindermanlab/informed-correctors.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certified Robust Invariant Polytope Training in Neural Controlled ODEs</title>
<link>https://arxiv.org/abs/2408.01273</link>
<guid>https://arxiv.org/abs/2408.01273</guid>
<content:encoded><![CDATA[
arXiv:2408.01273v2 Announce Type: replace 
Abstract: We consider a nonlinear control system modeled as an ordinary differential equation subject to disturbance, with a state feedback controller parameterized as a feedforward neural network. We propose a framework for training controllers with certified robust forward invariant polytopes, where any trajectory initialized inside the polytope remains within the polytope, regardless of the disturbance. First, we parameterize a family of lifted control systems in a higher dimensional space, where the original neural controlled system evolves on an invariant subspace of each lifted system. We use interval analysis and neural network verifiers to further construct a family of lifted embedding systems, carefully capturing the knowledge of this invariant subspace. If the vector field of any lifted embedding system satisfies a sign constraint at a single point, then a certain convex polytope of the original system is robustly forward invariant. Treating the neural network controller and the lifted system parameters as variables, we propose an algorithm to train controllers with certified forward invariant polytopes in the closed-loop control system. Through two examples, we demonstrate how the simplicity of the sign constraint allows our approach to scale with system dimension to over $50$ states, and outperform state-of-the-art Lyapunov-based sampling approaches in runtime.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes Adaptive Monte Carlo Tree Search for Offline Model-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2410.11234</link>
<guid>https://arxiv.org/abs/2410.11234</guid>
<content:encoded><![CDATA[
arXiv:2410.11234v3 Announce Type: replace 
Abstract: Offline RL is a powerful approach for data-driven decision-making and control. Compared to model-free methods, offline model-based RL (MBRL) explicitly learns world models from a static dataset and uses them as surrogate simulators, improving the data efficiency and enabling the learned policy to potentially generalize beyond the dataset support. However, there could be various MDPs that behave identically on the offline dataset and dealing with the uncertainty about the true MDP can be challenging. In this paper, we propose modeling offline MBRL as a Bayes Adaptive Markov Decision Process (BAMDP), which is a principled framework for addressing model uncertainty. We further propose a novel Bayes Adaptive Monte-Carlo planning algorithm capable of solving BAMDPs in continuous state and action spaces with stochastic transitions. This planning process is based on Monte Carlo Tree Search and can be integrated into offline MBRL as a policy improvement operator in policy iteration. Our ``RL + Search" framework follows in the footsteps of superhuman AIs like AlphaZero, improving on current offline MBRL methods by incorporating more computation input. The proposed algorithm significantly outperforms state-of-the-art offline RL methods on twelve D4RL MuJoCo tasks and three target tracking tasks in a challenging, stochastic tokamak control simulator. The codebase is available at: https://github.com/LucasCJYSDL/Offline-RL-Kit.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash Inference: Near Linear Time Inference for Long Convolution Sequence Models and Beyond</title>
<link>https://arxiv.org/abs/2410.12982</link>
<guid>https://arxiv.org/abs/2410.12982</guid>
<content:encoded><![CDATA[
arXiv:2410.12982v2 Announce Type: replace 
Abstract: While transformers have been at the core of most recent advancements in sequence generative models, their computational cost remains quadratic in sequence length. Several subquadratic architectures have been proposed to address this computational issue. Some of them, including long convolution sequence models (LCSMs), such as Hyena, address this issue at training time but remain quadratic during inference. We propose a method for speeding up LCSMs' exact inference to quasilinear $O(L\log^2L)$ time, identify the key properties that make this possible, and propose a general framework that exploits these. Our approach, inspired by previous work on relaxed polynomial interpolation, is based on a tiling which helps decrease memory movement and share computation. It has the added benefit of allowing for almost complete parallelization across layers of the position-mixing part of the architecture. Empirically, we provide a proof of concept implementation for Hyena, which gets up to $7.8\times$ end-to-end improvement over standard inference by improving $110\times$ within the position-mixing part.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCoTT: Strategic Chain-of-Thought Tasking for Wireless-Aware Robot Navigation in Digital Twins</title>
<link>https://arxiv.org/abs/2411.18212</link>
<guid>https://arxiv.org/abs/2411.18212</guid>
<content:encoded><![CDATA[
arXiv:2411.18212v3 Announce Type: replace 
Abstract: Path planning under wireless performance constraints is a complex challenge in robot navigation. However, naively incorporating such constraints into classical planning algorithms often incurs prohibitive search costs. In this paper, we propose SCoTT, a wireless-aware path planning framework that leverages vision-language models (VLMs) to co-optimize average path gains and trajectory length using wireless heatmap images and ray-tracing data from a digital twin (DT). At the core of our framework is Strategic Chain-of-Thought Tasking (SCoTT), a novel prompting paradigm that decomposes the exhaustive search problem into structured subtasks, each solved via chain-of-thought prompting. To establish strong baselines, we compare classical A* and wireless-aware extensions of it, and derive DP-WA*, an optimal, iterative dynamic programming algorithm that incorporates all path gains and distance metrics from the DT, but at significant computational cost. In extensive experiments, we show that SCoTT achieves path gains within 2% of DP-WA* while consistently generating shorter trajectories. Moreover, SCoTT's intermediate outputs can be used to accelerate DP-WA* by reducing its search space, saving up to 62% in execution time. We validate our framework using four VLMs, demonstrating effectiveness across both large and small models, thus making it applicable to a wide range of compact models at low inference cost. We also show the practical viability of our approach by deploying SCoTT as a ROS node within Gazebo simulations. Finally, we discuss data acquisition pipelines, compute requirements, and deployment considerations for VLMs in 6G-enabled DTs, underscoring the potential of natural language interfaces for wireless-aware navigation in real-world applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPO-VCS: An End-to-End Smart Predict-then-Optimize Framework with Alternating Differentiation Method for Relocation Problems in Large-Scale Vehicle Crowd Sensing</title>
<link>https://arxiv.org/abs/2411.18432</link>
<guid>https://arxiv.org/abs/2411.18432</guid>
<content:encoded><![CDATA[
arXiv:2411.18432v2 Announce Type: replace 
Abstract: Ubiquitous mobile devices have catalyzed the development of vehicle crowd sensing (VCS). In particular, vehicle sensing systems show great potential in the flexible acquisition of spatio-temporal urban data through built-in sensors under diverse sensing scenarios. However, vehicle systems often exhibit biased coverage due to the heterogeneous nature of trip requests and routes. To achieve a high sensing coverage, a critical challenge lies in optimally relocating vehicles to minimize the divergence between vehicle distributions and target sensing distributions. Conventional approaches typically employ a two-stage predict-then-optimize (PTO) process: first predicting real-time vehicle distributions and subsequently generating an optimal relocation strategy based on the predictions. However, this approach can lead to suboptimal decision-making due to the propagation of errors from upstream prediction. To this end, we develop an end-to-end Smart Predict-then-Optimize (SPO) framework by integrating optimization into prediction within the deep learning architecture, and the entire framework is trained by minimizing the task-specific matching divergence rather than the upstream prediction error. Methodologically, we formulate the vehicle relocation problem by quadratic programming (QP) and incorporate a novel unrolling approach based on the Alternating Direction Method of Multipliers (ADMM) within the SPO framework to compute gradients of the QP layer, facilitating backpropagation and gradient-based optimization for end-to-end learning. The effectiveness of the proposed framework is validated by real-world taxi datasets in Hong Kong. Utilizing the alternating differentiation method, the general SPO framework presents a novel concept of addressing decision-making problems with uncertainty, demonstrating significant potential for advancing applications in intelligent transportation systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizing Weisfeiler-Lehman Kernels to Subgraphs</title>
<link>https://arxiv.org/abs/2412.02181</link>
<guid>https://arxiv.org/abs/2412.02181</guid>
<content:encoded><![CDATA[
arXiv:2412.02181v3 Announce Type: replace 
Abstract: Subgraph representation learning has been effective in solving various real-world problems. However, current graph neural networks (GNNs) produce suboptimal results for subgraph-level tasks due to their inability to capture complex interactions within and between subgraphs. To provide a more expressive and efficient alternative, we propose WLKS, a Weisfeiler-Lehman (WL) kernel generalized for subgraphs by applying the WL algorithm on induced $k$-hop neighborhoods. We combine kernels across different $k$-hop levels to capture richer structural information that is not fully encoded in existing models. Our approach can balance expressiveness and efficiency by eliminating the need for neighborhood sampling. In experiments on eight real-world and synthetic benchmarks, WLKS significantly outperforms leading approaches on five datasets while reducing training time, ranging from 0.01x to 0.25x compared to the state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cluster Catch Digraphs with the Nearest Neighbor Distance</title>
<link>https://arxiv.org/abs/2501.06268</link>
<guid>https://arxiv.org/abs/2501.06268</guid>
<content:encoded><![CDATA[
arXiv:2501.06268v2 Announce Type: replace 
Abstract: We introduce a new method for clustering based on Cluster Catch Digraphs (CCDs). The new method addresses the limitations of RK-CCDs by employing a new variant of spatial randomness test that employs the nearest neighbor distance (NND) instead of the Ripley's K function used by RK-CCDs. We conduct a comprehensive Monte Carlo analysis to assess the performance of our method, considering factors such as dimensionality, data set size, number of clusters, cluster volumes, and inter-cluster distance. Our method is particularly effective for high-dimensional data sets, comparable to or outperforming KS-CCDs and RK-CCDs that rely on a KS-type statistic or the Ripley's K function. We also evaluate our methods using real and complex data sets, comparing them to well-known clustering methods. Again, our methods exhibit competitive performance, producing high-quality clusters with desirable properties.
  Keywords: Graph-based clustering, Cluster catch digraphs, High-dimensional data, The nearest neighbor distance, Spatial randomness test
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Human-Centered Evaluation of Explainable AI Methods in Clinical Decision Support Systems</title>
<link>https://arxiv.org/abs/2502.09849</link>
<guid>https://arxiv.org/abs/2502.09849</guid>
<content:encoded><![CDATA[
arXiv:2502.09849v3 Announce Type: replace 
Abstract: Explainable Artificial Intelligence (XAI) is essential for the transparency and clinical adoption of Clinical Decision Support Systems (CDSS). However, the real-world effectiveness of existing XAI methods remains limited and is inconsistently evaluated. This study conducts a systematic PRISMA-guided survey of 31 human-centered evaluations (HCE) of XAI applied to CDSS, classifying them by XAI methodology, evaluation design, and adoption barrier. Our findings reveal that most existing studies employ post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, typically assessed through small-scale clinician studies. The results show that over 80% of the studies adopt post-hoc, model-agnostic approaches such as SHAP and Grad-CAM, and that clinician sample sizes remain below 25 participants. The findings indicate that explanations generally improve clinician trust and diagnostic confidence, but frequently increase cognitive load and exhibit misalignment with domain reasoning processes. To bridge these gaps, we propose a stakeholder-centric evaluation framework that integrates socio-technical principles and human-computer interaction to guide the future development of clinically viable and trustworthy XAI-based CDSS.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Synthesizing High-Dimensional Tabular Data with Limited Samples</title>
<link>https://arxiv.org/abs/2503.06444</link>
<guid>https://arxiv.org/abs/2503.06444</guid>
<content:encoded><![CDATA[
arXiv:2503.06444v2 Announce Type: replace 
Abstract: Diffusion-based tabular data synthesis models have yielded promising results. However, when the data dimensionality increases, existing models tend to degenerate and may perform even worse than simpler, non-diffusion-based models. This is because limited training samples in high-dimensional space often hinder generative models from capturing the distribution accurately. To mitigate the insufficient learning signals and to stabilize training under such conditions, we propose CtrTab, a condition-controlled diffusion model that injects perturbed ground-truth samples as auxiliary inputs during training. This design introduces an implicit L2 regularization on the model's sensitivity to the control signal, improving robustness and stability in high-dimensional, low-data scenarios. Experimental results across multiple datasets show that CtrTab outperforms state-of-the-art models, with a performance gap in accuracy over 90% on average.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COPA: Comparing the incomparable in multi-objective model evaluation</title>
<link>https://arxiv.org/abs/2503.14321</link>
<guid>https://arxiv.org/abs/2503.14321</guid>
<content:encoded><![CDATA[
arXiv:2503.14321v3 Announce Type: replace 
Abstract: In machine learning (ML), we often need to choose one among hundreds of trained ML models at hand, based on various objectives such as accuracy, robustness, fairness or scalability. However, it is often unclear how to compare, aggregate and, ultimately, trade-off these objectives, making it a time-consuming task that requires expert knowledge, as objectives may be measured in different units and scales. In this work, we investigate how objectives can be automatically normalized and aggregated to systematically help the user navigate their Pareto front. To this end, we make incomparable objectives comparable using their cumulative functions, approximated by their relative rankings. As a result, our proposed approach, COPA, can aggregate them while matching user-specific preferences, allowing practitioners to meaningfully navigate and search for models in the Pareto front. We demonstrate the potential impact of COPA in both model selection and benchmarking tasks across diverse ML areas such as fair ML, domain generalization, AutoML and foundation models, where classical ways to normalize and aggregate objectives fall short.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CATransformers: Carbon Aware Transformers Through Joint Model-Hardware Optimization</title>
<link>https://arxiv.org/abs/2505.01386</link>
<guid>https://arxiv.org/abs/2505.01386</guid>
<content:encoded><![CDATA[
arXiv:2505.01386v4 Announce Type: replace 
Abstract: Machine learning solutions are rapidly adopted to enable a variety of key use cases, from conversational AI assistants to scientific discovery. This growing adoption is expected to increase the associated lifecycle carbon footprint, including both \emph{operational carbon} from training and inference and \emph{embodied carbon} from AI hardware manufacturing. We introduce \ourframework -- the first carbon-aware co-optimization framework for Transformer-based models and hardware accelerators. By integrating both operational and embodied carbon into early-stage design space exploration, \ourframework enables sustainability-driven model architecture and hardware accelerator co-design that reveals fundamentally different trade-offs than latency- or energy-centric approaches. Evaluated across a range of Transformer models, \ourframework consistently demonstrates the potential to reduce total carbon emissions -- by up to 30\% -- while maintaining accuracy and latency. We further highlight its extensibility through a focused case study on multi-modal models. Our results emphasize the need for holistic optimization methods that prioritize carbon efficiency without compromising model capability and execution time performance. The source code of \ourframework is available at {\small{\href{https://github.com/facebookresearch/CATransformers}{\texttt{https://github.com/facebookresearch/CATransformers}}}}.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards High Resolution Probabilistic Coastal Inundation Forecasting from Sparse Observations</title>
<link>https://arxiv.org/abs/2505.05381</link>
<guid>https://arxiv.org/abs/2505.05381</guid>
<content:encoded><![CDATA[
arXiv:2505.05381v2 Announce Type: replace 
Abstract: Coastal flooding poses increasing threats to communities worldwide, necessitating accurate and hyper-local inundation forecasting for effective emergency response. However, real-world deployment of forecasting systems is often constrained by sparse sensor networks, where only a limited subset of locations may have sensors due to budget constraints. To approach this challenge, we present DIFF -SPARSE, a masked conditional diffusion model designed for probabilistic coastal inundation forecasting from sparse sensor observations. DIFF -SPARSE primarily utilizes the inundation history of a location and its neighboring locations from a context time window as spatiotemporal context. The fundamental challenge of spatiotemporal prediction based on sparse observations in the context window is addressed by introducing a novel masking strategy during training. Digital elevation data and temporal co-variates are utilized as additional spatial and temporal contexts, respectively. A convolutional neural network and a conditional UNet architecture with cross-attention mechanism are employed to capture the spatiotemporal dynamics in the data. We trained and tested DIFF -SPARSE on coastal inundation data from the Eastern Shore of Virginia and systematically assessed the performance of DIFF -SPARSE across different sparsity levels 0%, 50%, 95% missing observations. Our experiment results show that DIFF -SPARSE achieves upto 62% improvement in terms of two forecasting performance metrics compared to existing methods, at 95% sparsity level. Moreover, our ablation studies reveal that digital elevation data becomes more useful at high sparsity levels compared to temporal co-variates.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Visual-Policy Learning through Parallel Differentiable Simulation</title>
<link>https://arxiv.org/abs/2505.10646</link>
<guid>https://arxiv.org/abs/2505.10646</guid>
<content:encoded><![CDATA[
arXiv:2505.10646v2 Announce Type: replace 
Abstract: In this work, we propose a computationally efficient algorithm for visual policy learning that leverages differentiable simulation and first-order analytical policy gradients. Our approach decouple the rendering process from the computation graph, enabling seamless integration with existing differentiable simulation ecosystems without the need for specialized differentiable rendering software. This decoupling not only reduces computational and memory overhead but also effectively attenuates the policy gradient norm, leading to more stable and smoother optimization. We evaluate our method on standard visual control benchmarks using modern GPU-accelerated simulation. Experiments show that our approach significantly reduces wall-clock training time and consistently outperforms all baseline methods in terms of final returns. Notably, on complex tasks such as humanoid locomotion, our method achieves a $4\times$ improvement in final return, and successfully learns a humanoid running policy within 4 hours on a single GPU.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool-Aided Evolutionary LLM for Generative Policy Toward Efficient Resource Management in Wireless Federated Learning</title>
<link>https://arxiv.org/abs/2505.11570</link>
<guid>https://arxiv.org/abs/2505.11570</guid>
<content:encoded><![CDATA[
arXiv:2505.11570v2 Announce Type: replace 
Abstract: Federated Learning (FL) enables distributed model training across edge devices in a privacy-friendly manner. However, its efficiency heavily depends on effective device selection and high-dimensional resource allocation in dynamic and heterogeneous wireless environments. Conventional methods demand a confluence of domain-specific expertise, extensive hyperparameter tuning, and/or heavy interaction cost. This paper proposes a Tool-aided Evolutionary Large Language Model (T-ELLM) framework to generate a qualified policy for device selection in a wireless FL environment. Unlike conventional optimization methods, T-ELLM leverages natural language-based scenario prompts to enhance generalization across varying network conditions. The framework decouples the joint optimization problem mathematically, enabling tractable learning of device selection policies while delegating resource allocation to convex optimization tools. To improve adaptability, T-ELLM integrates a sample-efficient, model-based virtual learning environment that captures the relationship between device selection and learning performance, facilitating subsequent group relative policy optimization. This concerted approach reduces reliance on real-world interactions, minimizing communication overhead while maintaining high-fidelity decision-making. Theoretical analysis proves that the discrepancy between virtual and real environments is bounded, ensuring the advantage function learned in the virtual environment maintains a provably small deviation from real-world conditions. Experimental results demonstrate that T-ELLM outperforms benchmark methods in energy efficiency and exhibits robust adaptability to environmental changes.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Internal Causal Mechanisms Robustly Predict Language Model Out-of-Distribution Behaviors</title>
<link>https://arxiv.org/abs/2505.11770</link>
<guid>https://arxiv.org/abs/2505.11770</guid>
<content:encoded><![CDATA[
arXiv:2505.11770v2 Announce Type: replace 
Abstract: Interpretability research now offers a variety of techniques for identifying abstract internal mechanisms in neural networks. Can such techniques be used to predict how models will behave on out-of-distribution examples? In this work, we provide a positive answer to this question. Through a diverse set of language modeling tasks--including symbol manipulation, knowledge retrieval, and instruction following--we show that the most robust features for correctness prediction are those that play a distinctive causal role in the model's behavior. Specifically, we propose two methods that leverage causal mechanisms to predict the correctness of model outputs: counterfactual simulation (checking whether key causal variables are realized) and value probing (using the values of those variables to make predictions). Both achieve high AUC-ROC in distribution and outperform methods that rely on causal-agnostic features in out-of-distribution settings, where predicting model behaviors is more crucial. Our work thus highlights a novel and significant application for internal causal analysis of language models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</title>
<link>https://arxiv.org/abs/2505.13697</link>
<guid>https://arxiv.org/abs/2505.13697</guid>
<content:encoded><![CDATA[
arXiv:2505.13697v3 Announce Type: replace 
Abstract: Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy-Driven World Model Adaptation for Robust Offline Model-based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.13709</link>
<guid>https://arxiv.org/abs/2505.13709</guid>
<content:encoded><![CDATA[
arXiv:2505.13709v2 Announce Type: replace 
Abstract: Offline reinforcement learning (RL) offers a powerful paradigm for data-driven control. Compared to model-free approaches, offline model-based RL (MBRL) explicitly learns a world model from a static dataset and uses it as a surrogate simulator, improving data efficiency and enabling potential generalization beyond the dataset support. However, most existing offline MBRL methods follow a two-stage training procedure: first learning a world model by maximizing the likelihood of the observed transitions, then optimizing a policy to maximize its expected return under the learned model. This objective mismatch results in a world model that is not necessarily optimized for effective policy learning. Moreover, we observe that policies learned via offline MBRL often lack robustness during deployment, and small adversarial noise in the environment can lead to significant performance degradation. To address these, we propose a framework that dynamically adapts the world model alongside the policy under a unified learning objective aimed at improving robustness. At the core of our method is a maximin optimization problem, which we solve by innovatively utilizing Stackelberg learning dynamics. We provide theoretical analysis to support our design and introduce computationally efficient implementations. We benchmark our algorithm on twelve noisy D4RL MuJoCo tasks and three stochastic Tokamak Control tasks, demonstrating its state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When fractional quasi p-norms concentrate</title>
<link>https://arxiv.org/abs/2505.19635</link>
<guid>https://arxiv.org/abs/2505.19635</guid>
<content:encoded><![CDATA[
arXiv:2505.19635v2 Announce Type: replace 
Abstract: Concentration of distances in high dimension is an important factor for the development and design of stable and reliable data analysis algorithms. In this paper, we address the fundamental long-standing question about the concentration of distances in high dimension for fractional quasi $p$-norms, $p\in(0,1)$. The topic has been at the centre of various theoretical and empirical controversies. Here we, for the first time, identify conditions when fractional quasi $p$-norms concentrate and when they don't. We show that contrary to some earlier suggestions, for broad classes of distributions, fractional quasi $p$-norms admit exponential and uniform in $p$ concentration bounds. For these distributions, the results effectively rule out previously proposed approaches to alleviate concentration by "optimal" setting the values of $p$ in $(0,1)$. At the same time, we specify conditions and the corresponding families of distributions for which one can still control concentration rates by appropriate choices of $p$. We also show that in an arbitrarily small vicinity of a distribution from a large class of distributions for which uniform concentration occurs, there are uncountably many other distributions featuring anti-concentration properties. Importantly, this behavior enables devising relevant data encoding or representation schemes favouring or discouraging distance concentration. The results shed new light on this long-standing problem and resolve the tension around the topic in both theory and empirical evidence reported in the literature.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals</title>
<link>https://arxiv.org/abs/2505.22422</link>
<guid>https://arxiv.org/abs/2505.22422</guid>
<content:encoded><![CDATA[
arXiv:2505.22422v2 Announce Type: replace 
Abstract: The construction of confidence intervals for the mean of a bounded random variable is a classical problem in statistics with numerous applications in machine learning and virtually all scientific fields. In particular, obtaining the tightest possible confidence intervals is vital every time the sampling of the random variables is expensive. The current state-of-the-art method to construct confidence intervals is by using betting algorithms. This is a very successful approach for deriving optimal confidence sequences, even matching the rate of law of iterated logarithms. However, in the fixed horizon setting, these approaches are either sub-optimal or based on heuristic solutions with strong empirical performance but without a finite-time guarantee. Hence, no betting-based algorithm guaranteeing the optimal $\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the confidence intervals are known. This work bridges this gap. We propose a betting-based algorithm to compute confidence intervals that empirically outperforms the competitors. Our betting strategy uses the optimal strategy in every step (in a certain sense), whereas the standard betting methods choose a constant strategy in advance. Leveraging this fact results in strict improvements even for classical concentration inequalities, such as the ones of Hoeffding or Bernstein. Moreover, we also prove that the width of our confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$. The code is available at https://github.com/vvoracek/STaR-bets-confidence-interval.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Contrastive Learning is Approximately Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.04411</link>
<guid>https://arxiv.org/abs/2506.04411</guid>
<content:encoded><![CDATA[
arXiv:2506.04411v2 Announce Type: replace 
Abstract: Despite its empirical success, the theoretical foundations of self-supervised contrastive learning (CL) are not yet fully established. In this work, we address this gap by showing that standard CL objectives implicitly approximate a supervised variant we call the negatives-only supervised contrastive loss (NSCL), which excludes same-class contrasts. We prove that the gap between the CL and NSCL losses vanishes as the number of semantic classes increases, under a bound that is both label-agnostic and architecture-independent.
  We characterize the geometric structure of the global minimizers of the NSCL loss: the learned representations exhibit augmentation collapse, within-class collapse, and class centers that form a simplex equiangular tight frame. We further introduce a new bound on the few-shot error of linear-probing. This bound depends on two measures of feature variability--within-class dispersion and variation along the line between class centers. We show that directional variation dominates the bound and that the within-class dispersion's effect diminishes as the number of labeled samples increases. These properties enable CL and NSCL-trained representations to support accurate few-shot label recovery using simple linear probes.
  Finally, we empirically validate our theoretical findings: the gap between CL and NSCL losses decays at a rate of $\mathcal{O}(\frac{1}{\#\text{classes}})$; the two losses are highly correlated; minimizing the CL loss implicitly brings the NSCL loss close to the value achieved by direct minimization; and the proposed few-shot error bound provides a tight estimate of probing performance in practice. The code and project page of the paper are available at [\href{https://github.com/DLFundamentals/understanding-ssl}{code}, \href{https://dlfundamentals.github.io/ssl-is-approximately-sl/}{project page}].
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zeroth-Order Optimization Finds Flat Minima</title>
<link>https://arxiv.org/abs/2506.05454</link>
<guid>https://arxiv.org/abs/2506.05454</guid>
<content:encoded><![CDATA[
arXiv:2506.05454v2 Announce Type: replace 
Abstract: Zeroth-order methods are extensively used in machine learning applications where gradients are infeasible or expensive to compute, such as black-box attacks, reinforcement learning, and language model fine-tuning. Existing optimization theory focuses on convergence to an arbitrary stationary point, but less is known on the implicit regularization that provides a fine-grained characterization on which particular solutions are finally reached. We show that zeroth-order optimization with the standard two-point estimator favors solutions with small trace of Hessian, which is widely used in previous work to distinguish between sharp and flat minima. We further provide convergence rates of zeroth-order optimization to approximate flat minima for convex and sufficiently smooth functions, where flat minima are defined as the minimizers that achieve the smallest trace of Hessian among all optimal solutions. Experiments on binary classification tasks with convex losses and language model fine-tuning support our theoretical findings.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do-PFN: In-Context Learning for Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2506.06039</link>
<guid>https://arxiv.org/abs/2506.06039</guid>
<content:encoded><![CDATA[
arXiv:2506.06039v2 Announce Type: replace 
Abstract: Estimation of causal effects is critical to a range of scientific disciplines. Existing methods for this task either require interventional data, knowledge about the ground truth causal graph, or rely on assumptions such as unconfoundedness, restricting their applicability in real-world settings. In the domain of tabular machine learning, Prior-data fitted networks (PFNs) have achieved state-of-the-art predictive performance, having been pre-trained on synthetic data to solve tabular prediction problems via in-context learning. To assess whether this can be transferred to the harder problem of causal effect estimation, we pre-train PFNs on synthetic data drawn from a wide variety of causal structures, including interventions, to predict interventional outcomes given observational data. Through extensive experiments on synthetic case studies, we show that our approach allows for the accurate estimation of causal effects without knowledge of the underlying causal graph. We also perform ablation studies that elucidate Do-PFN's scalability and robustness across datasets with a variety of causal characteristics.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAC: An Efficient Gradient Preconditioning using Mean Activation Approximated Curvature</title>
<link>https://arxiv.org/abs/2506.08464</link>
<guid>https://arxiv.org/abs/2506.08464</guid>
<content:encoded><![CDATA[
arXiv:2506.08464v2 Announce Type: replace 
Abstract: Second-order optimization methods for training neural networks, such as KFAC, exhibit superior convergence by utilizing curvature information of loss landscape. However, it comes at the expense of high computational burden. In this work, we analyze the two components that constitute the layer-wise Fisher information matrix (FIM) used in KFAC: the Kronecker factors related to activations and pre-activation gradients. Based on empirical observations on their eigenspectra, we propose efficient approximations for them, resulting in a computationally efficient optimization method called MAC. To the best of our knowledge, MAC is the first algorithm to apply the Kronecker factorization to the FIM of attention layers used in transformers and explicitly integrate attention scores into the preconditioning. We also study the convergence property of MAC on nonlinear neural networks and provide two conditions under which it converges to global minima. Our extensive evaluations on various network architectures and datasets show that the proposed method outperforms KFAC and other state-of-the-art methods in terms of accuracy, end-to-end training time, and memory usage.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Losses for Diffusion Bridge Samplers</title>
<link>https://arxiv.org/abs/2506.10982</link>
<guid>https://arxiv.org/abs/2506.10982</guid>
<content:encoded><![CDATA[
arXiv:2506.10982v3 Announce Type: replace 
Abstract: Diffusion bridges are a promising class of deep-learning methods for sampling from unnormalized distributions. Recent works show that the Log Variance (LV) loss consistently outperforms the reverse Kullback-Leibler (rKL) loss when using the reparametrization trick to compute rKL-gradients. While the on-policy LV loss yields identical gradients to the rKL loss when combined with the log-derivative trick for diffusion samplers with non-learnable forward processes, this equivalence does not hold for diffusion bridges or when diffusion coefficients are learned. Based on this insight we argue that for diffusion bridges the LV loss does not represent an optimization objective that can be motivated like the rKL loss via the data processing inequality. Our analysis shows that employing the rKL loss with the log-derivative trick (rKL-LD) does not only avoid these conceptual problems but also consistently outperforms the LV loss. Experimental results with different types of diffusion bridges on challenging benchmarks show that samplers trained with the rKL-LD loss achieve better performance. From a practical perspective we find that rKL-LD requires significantly less hyperparameter optimization and yields more stable training behavior.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling 3D Molecular Conformers with Diffusion Transformers</title>
<link>https://arxiv.org/abs/2506.15378</link>
<guid>https://arxiv.org/abs/2506.15378</guid>
<content:encoded><![CDATA[
arXiv:2506.15378v2 Announce Type: replace 
Abstract: Diffusion Transformers (DiTs) have demonstrated strong performance in generative modeling, particularly in image synthesis, making them a compelling choice for molecular conformer generation. However, applying DiTs to molecules introduces novel challenges, such as integrating discrete molecular graph information with continuous 3D geometry, handling Euclidean symmetries, and designing conditioning mechanisms that generalize across molecules of varying sizes and structures. We propose DiTMC, a framework that adapts DiTs to address these challenges through a modular architecture that separates the processing of 3D coordinates from conditioning on atomic connectivity. To this end, we introduce two complementary graph-based conditioning strategies that integrate seamlessly with the DiT architecture. These are combined with different attention mechanisms, including both standard non-equivariant and SO(3)-equivariant formulations, enabling flexible control over the trade-off between between accuracy and computational efficiency. Experiments on standard conformer generation benchmarks (GEOM-QM9, -DRUGS, -XL) demonstrate that DiTMC achieves state-of-the-art precision and physical validity. Our results highlight how architectural choices and symmetry priors affect sample quality and efficiency, suggesting promising directions for large-scale generative modeling of molecular structures. Code is available at https://github.com/ML4MolSim/dit_mc.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORVIT: Near-Optimal Online Distributionally Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2508.03768</link>
<guid>https://arxiv.org/abs/2508.03768</guid>
<content:encoded><![CDATA[
arXiv:2508.03768v2 Announce Type: replace 
Abstract: We investigate reinforcement learning (RL) in the presence of distributional mismatch between training and deployment, where policies trained in simulators often underperform in practice due to mismatches between training and deployment conditions, and thereby reliable guarantees on real-world performance are essential. Distributionally robust RL addresses this issue by optimizing worst-case performance over an uncertainty set of environments and providing an optimized lower bound on deployment performance. However, existing studies typically assume access to either a generative model or offline datasets with broad coverage of the deployment environment-assumptions that limit their practicality in unknown environments without prior knowledge. In this work, we study a more practical and challenging setting: online distributionally robust RL, where the agent interacts only with a single unknown training environment while seeking policies that are robust with respect to an uncertainty set around this nominal model. We consider general $f$-divergence-based ambiguity sets, including $\chi^2$ and KL divergence balls, and design a computationally efficient algorithm that achieves sublinear regret for the robust control objective under minimal assumptions, without requiring generative or offline data access. Moreover, we establish a corresponding minimax lower bound on the regret of any online algorithm, demonstrating the near-optimality of our method. Experiments across diverse environments with model misspecification show that our approach consistently improves worst-case performance and aligns with the theoretical guarantees.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S$^2$M-Former: Spiking Symmetric Mixing Branchformer for Brain Auditory Attention Detection</title>
<link>https://arxiv.org/abs/2508.05164</link>
<guid>https://arxiv.org/abs/2508.05164</guid>
<content:encoded><![CDATA[
arXiv:2508.05164v2 Announce Type: replace 
Abstract: Auditory attention detection (AAD) aims to decode listeners' focus in complex auditory environments from electroencephalography (EEG) recordings, which is crucial for developing neuro-steered hearing devices. Despite recent advancements, EEG-based AAD remains hindered by the absence of synergistic frameworks that can fully leverage complementary EEG features under energy-efficiency constraints. We propose S$^2$M-Former, a novel spiking symmetric mixing framework to address this limitation through two key innovations: i) Presenting a spike-driven symmetric architecture composed of parallel spatial and frequency branches with mirrored modular design, leveraging biologically plausible token-channel mixers to enhance complementary learning across branches; ii) Introducing lightweight 1D token sequences to replace conventional 3D operations, reducing parameters by 14.7$\times$. The brain-inspired spiking architecture further reduces power consumption, achieving a 5.8$\times$ energy reduction compared to recent ANN methods, while also surpassing existing SNN baselines in terms of parameter efficiency and performance. Comprehensive experiments on three AAD benchmarks (KUL, DTU and AV-GC-AAD) across three settings (within-trial, cross-trial and cross-subject) demonstrate that S$^2$M-Former achieves comparable state-of-the-art (SOTA) decoding accuracy, making it a promising low-power, high-performance solution for AAD tasks. Code is available at https://github.com/JackieWang9811/S2M-Former.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Reward Model via Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2508.08746</link>
<guid>https://arxiv.org/abs/2508.08746</guid>
<content:encoded><![CDATA[
arXiv:2508.08746v4 Announce Type: replace 
Abstract: Large language models (LLMs) have been widely deployed across numerous fields. Reinforcement Learning from Human Feedback (RLHF) leverages reward models (RMs) as proxies for human preferences to align LLM behaviors with human values, making the accuracy, reliability, and interpretability of RMs critical for effective alignment. However, traditional RMs lack interpretability, offer limited insight into the reasoning behind reward assignments, and are inflexible toward user preference shifts. While recent multidimensional RMs aim for improved interpretability, they often fail to provide feature-level attribution and require costly annotations. To overcome these limitations, we introduce the Sparse Autoencoder-enhanced Reward Model (SARM), a novel architecture that integrates a pretrained Sparse Autoencoder (SAE) into a reward model. SARM maps the hidden activations of LLM-based RM into an interpretable, sparse, and monosemantic feature space, from which a scalar head aggregates feature activations to produce transparent and conceptually meaningful reward scores. Empirical evaluations demonstrate that SARM facilitates direct feature-level attribution of reward assignments, allows dynamic adjustment to preference shifts, and achieves superior alignment performance compared to conventional reward models. Our code is available at https://github.com/schrieffer-z/sarm.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedShard: Federated Unlearning with Efficiency Fairness and Performance Fairness</title>
<link>https://arxiv.org/abs/2508.09866</link>
<guid>https://arxiv.org/abs/2508.09866</guid>
<content:encoded><![CDATA[
arXiv:2508.09866v2 Announce Type: replace 
Abstract: To protect clients' right to be forgotten in federated learning, federated unlearning aims to remove the data contribution of leaving clients from the global learned model. While current studies mainly focused on enhancing unlearning efficiency and effectiveness, the crucial aspects of efficiency fairness and performance fairness among decentralized clients during unlearning have remained largely unexplored. In this study, we introduce FedShard, the first federated unlearning algorithm designed to concurrently guarantee both efficiency fairness and performance fairness. FedShard adaptively addresses the challenges introduced by dilemmas among convergence, unlearning efficiency, and unlearning fairness. Furthermore, we propose two novel metrics to quantitatively assess the fairness of unlearning algorithms, which we prove to satisfy well-known properties in other existing fairness measurements. Our theoretical analysis and numerical evaluation validate FedShard's fairness in terms of both unlearning performance and efficiency. We demonstrate that FedShard mitigates unfairness risks such as cascaded leaving and poisoning attacks and realizes more balanced unlearning costs among clients. Experimental results indicate that FedShard accelerates the data unlearning process 1.3-6.2 times faster than retraining from scratch and 4.9 times faster than the state-of-the-art exact unlearning methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks</title>
<link>https://arxiv.org/abs/2508.14004</link>
<guid>https://arxiv.org/abs/2508.14004</guid>
<content:encoded><![CDATA[
arXiv:2508.14004v2 Announce Type: replace 
Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where rounding in each layer reduces capacity as bit-width shrinks; the floating-point (FP) checkpoint sets the maximum input rate. We track capacity dynamics as the average bit-width decreases and identify resulting quantization bottlenecks by casting fine-tuning as a smooth, constrained optimization problem. Our approach employs a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds, and enforces a target bit-width via an exterior-point penalty; mild metric smoothing (via distillation) stabilizes training. Despite its simplicity, the method attains competitive accuracy down to the extreme W1A1 setting while retaining the efficiency of STE.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation</title>
<link>https://arxiv.org/abs/2508.14336</link>
<guid>https://arxiv.org/abs/2508.14336</guid>
<content:encoded><![CDATA[
arXiv:2508.14336v2 Announce Type: replace 
Abstract: GNSS localization using everyday mobile devices is challenging in urban environments, as ranging errors caused by the complex propagation of satellite signals and low-quality onboard GNSS hardware are blamed for undermining positioning accuracy. Researchers have pinned their hopes on data-driven methods to regress such ranging errors from raw measurements. However, the grueling annotation of ranging errors impedes their pace. This paper presents a robust end-to-end Neural Ranging Correction (NeRC) framework, where localization-related metrics serve as the task objective for training the neural modules. Instead of seeking impractical ranging error labels, we train the neural network using ground-truth locations that are relatively easy to obtain. This functionality is supported by differentiable moving horizon location estimation (MHE) that handles a horizon of measurements for positioning and backpropagates the gradients for training. Even better, as a blessing of end-to-end learning, we propose a new training paradigm using Euclidean Distance Field (EDF) cost maps, which alleviates the demands on labeled locations. We evaluate the proposed NeRC on public benchmarks and our collected datasets, demonstrating its distinguished improvement in positioning accuracy. We also deploy NeRC on the edge to verify its real-time performance for mobile devices.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HyPINO: Multi-Physics Neural Operators via HyperPINNs and the Method of Manufactured Solutions</title>
<link>https://arxiv.org/abs/2509.05117</link>
<guid>https://arxiv.org/abs/2509.05117</guid>
<content:encoded><![CDATA[
arXiv:2509.05117v4 Announce Type: replace 
Abstract: We present HyPINO, a multi-physics neural operator designed for zero-shot generalization across a broad class of PDEs without requiring task-specific fine-tuning. Our approach combines a Swin Transformer-based hypernetwork with mixed supervision: (i) labeled data from analytical solutions generated via the Method of Manufactured Solutions (MMS), and (ii) unlabeled samples optimized using physics-informed objectives. The model maps PDE parameterizations to target Physics-Informed Neural Networks (PINNs) and can handle linear elliptic, hyperbolic, and parabolic equations in two dimensions with varying source terms, geometries, and mixed Dirichlet/Neumann boundary conditions, including interior boundaries. HyPINO achieves strong zero-shot accuracy on seven benchmark problems from PINN literature, outperforming U-Nets, Poseidon, and Physics-Informed Neural Operators (PINO). Further, we introduce an iterative refinement procedure that treats the residual of the generated PINN as "delta PDE" and performs another forward pass to generate a corrective PINN. Summing their contributions and repeating this process forms an ensemble whose combined solution progressively reduces the error on six benchmarks and achieves a >100x lower $L_2$ loss in the best case, while retaining forward-only inference. Additionally, we evaluate the fine-tuning behavior of PINNs initialized by HyPINO and show that they converge faster and to lower final error than both randomly initialized and Reptile-meta-learned PINNs on five benchmarks, performing on par on the remaining two. Our results highlight the potential of this scalable approach as a foundation for extending neural operators toward solving increasingly complex, nonlinear, and high-dimensional PDE problems. The code and model weights are publicly available at https://github.com/rbischof/hypino.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
<link>https://arxiv.org/abs/2509.05429</link>
<guid>https://arxiv.org/abs/2509.05429</guid>
<content:encoded><![CDATA[
arXiv:2509.05429v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have emerged as powerful models for learning from graph-structured data. However, their widespread adoption has raised serious privacy concerns. While prior research has primarily focused on edge-level privacy, a critical yet underexplored threat lies in topology privacy - the confidentiality of the graph's overall structure. In this work, we present a comprehensive study on topology privacy risks in GNNs, revealing their vulnerability to graph-level inference attacks. To this end, we propose a suite of Topology Inference Attacks (TIAs) that can reconstruct the structure of a target training graph using only black-box access to a GNN model. Our findings show that GNNs are highly susceptible to these attacks, and that existing edge-level differential privacy mechanisms are insufficient as they either fail to mitigate the risk or severely compromise model accuracy. To address this challenge, we introduce Private Graph Reconstruction (PGR), a novel defense framework designed to protect topology privacy while maintaining model accuracy. PGR is formulated as a bi-level optimization problem, where a synthetic training graph is iteratively generated using meta-gradients, and the GNN model is concurrently updated based on the evolving graph. Extensive experiments demonstrate that PGR significantly reduces topology leakage with minimal impact on model accuracy. Our code is available at https://github.com/JeffffffFu/PGR.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRUST-FS: Tensorized Reliable Unsupervised Multi-View Feature Selection for Incomplete Data</title>
<link>https://arxiv.org/abs/2509.13192</link>
<guid>https://arxiv.org/abs/2509.13192</guid>
<content:encoded><![CDATA[
arXiv:2509.13192v2 Announce Type: replace 
Abstract: Multi-view unsupervised feature selection (MUFS), which selects informative features from multi-view unlabeled data, has attracted increasing research interest in recent years. Although great efforts have been devoted to MUFS, several challenges remain: 1) existing methods for incomplete multi-view data are limited to handling missing views and are unable to address the more general scenario of missing variables, where some features have missing values in certain views; 2) most methods address incomplete data by first imputing missing values and then performing feature selection, treating these two processes independently and overlooking their interactions; 3) missing data can result in an inaccurate similarity graph, which reduces the performance of feature selection. To solve this dilemma, we propose a novel MUFS method for incomplete multi-view data with missing variables, termed Tensorized Reliable UnSupervised mulTi-view Feature Selection (TRUST-FS). TRUST-FS introduces a new adaptive-weighted CP decomposition that simultaneously performs feature selection, missing-variable imputation, and view weight learning within a unified tensor factorization framework. By utilizing Subjective Logic to acquire trustworthy cross-view similarity information, TRUST-FS facilitates learning a reliable similarity graph, which subsequently guides feature selection and imputation. Comprehensive experimental results demonstrate the effectiveness and superiority of our method over state-of-the-art methods.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance Generation for Meta-Black-Box Optimization through Latent Space Reverse Engineering</title>
<link>https://arxiv.org/abs/2509.15810</link>
<guid>https://arxiv.org/abs/2509.15810</guid>
<content:encoded><![CDATA[
arXiv:2509.15810v2 Announce Type: replace 
Abstract: To relieve intensive human-expertise required to design optimization algorithms, recent Meta-Black-Box Optimization (MetaBBO) researches leverage generalization strength of meta-learning to train neural network-based algorithm design policies over a predefined training problem set, which automates the adaptability of the low-level optimizers on unseen problem instances. Currently, a common training problem set choice in existing MetaBBOs is well-known benchmark suites CoCo-BBOB. Although such choice facilitates the MetaBBO's development, problem instances in CoCo-BBOB are more or less limited in diversity, raising the risk of overfitting of MetaBBOs, which might further results in poor generalization. In this paper, we propose an instance generation approach, termed as \textbf{LSRE}, which could generate diverse training problem instances for MetaBBOs to learn more generalizable policies. LSRE first trains an autoencoder which maps high-dimensional problem features into a 2-dimensional latent space. Uniform-grid sampling in this latent space leads to hidden representations of problem instances with sufficient diversity. By leveraging a genetic-programming approach to search function formulas with minimal L2-distance to these hidden representations, LSRE reverse engineers a diversified problem set, termed as \textbf{Diverse-BBO}. We validate the effectiveness of LSRE by training various MetaBBOs on Diverse-BBO and observe their generalization performances on either synthetic or realistic scenarios. Extensive experimental results underscore the superiority of Diverse-BBO to existing training set choices in MetaBBOs. Further ablation studies not only demonstrate the effectiveness of design choices in LSRE, but also reveal interesting insights on instance diversity and MetaBBO's generalization.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Realistic Evaluation of Cross-Frequency Transfer Learning and Foundation Forecasting Models</title>
<link>https://arxiv.org/abs/2509.19465</link>
<guid>https://arxiv.org/abs/2509.19465</guid>
<content:encoded><![CDATA[
arXiv:2509.19465v2 Announce Type: replace 
Abstract: Cross-frequency transfer learning (CFTL) has emerged as a popular framework for curating large-scale time series datasets to pre-train foundation forecasting models (FFMs). Although CFTL has shown promise, current benchmarking practices fall short of accurately assessing its performance. This shortcoming stems from many factors: an over-reliance on small-scale evaluation datasets; inadequate treatment of sample size when computing summary statistics; reporting of suboptimal statistical models; and failing to account for non-negligible risks of overlap between pre-training and test datasets. To address these limitations, we introduce a unified reimplementation of widely-adopted neural forecasting networks, adapting them for the CFTL setup; we pre-train only on proprietary and synthetic data, being careful to prevent test leakage; and we evaluate on 15 large, diverse public forecast competition datasets. Our empirical analysis reveals that statistical models' accuracy is frequently underreported. Notably, we confirm that statistical models and their ensembles consistently outperform existing FFMs by more than 8.2% in sCRPS, and by more than 20% MASE, across datasets. However, we also find that synthetic dataset pre-training does improve the accuracy of a FFM by 7% percent.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Foundation Models for Zero-Shot Time Series Anomaly Detection: Leveraging Synthetic Data and Relative Context Discrepancy</title>
<link>https://arxiv.org/abs/2509.21190</link>
<guid>https://arxiv.org/abs/2509.21190</guid>
<content:encoded><![CDATA[
arXiv:2509.21190v3 Announce Type: replace 
Abstract: Time series anomaly detection (TSAD) is a critical task, but developing models that generalize to unseen data in a zero-shot manner remains a major challenge. Prevailing foundation models for TSAD predominantly rely on reconstruction-based objectives, which suffer from a fundamental objective mismatch: they struggle to identify subtle anomalies while often misinterpreting complex normal patterns, leading to high rates of false negatives and positives. To overcome these limitations, we introduce \texttt{TimeRCD}, a novel foundation model for TSAD built upon a new pre-training paradigm: Relative Context Discrepancy (RCD). Instead of learning to reconstruct inputs, \texttt{TimeRCD} is explicitly trained to identify anomalies by detecting significant discrepancies between adjacent time windows. This relational approach, implemented with a standard Transformer architecture, enables the model to capture contextual shifts indicative of anomalies that reconstruction-based methods often miss. To facilitate this paradigm, we develop a large-scale, diverse synthetic corpus with token-level anomaly labels, providing the rich supervisory signal necessary for effective pre-training. Extensive experiments demonstrate that \texttt{TimeRCD} significantly outperforms existing general-purpose and anomaly-specific foundation models in zero-shot TSAD across diverse datasets. Our results validate the superiority of the RCD paradigm and establish a new, effective path toward building robust and generalizable foundation models for time series anomaly detection.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task-Agnostic Federated Continual Learning via Replay-Free Gradient Projection</title>
<link>https://arxiv.org/abs/2509.21606</link>
<guid>https://arxiv.org/abs/2509.21606</guid>
<content:encoded><![CDATA[
arXiv:2509.21606v2 Announce Type: replace 
Abstract: Federated continual learning (FCL) enables distributed client devices to learn from streaming data across diverse and evolving tasks. A major challenge to continual learning, catastrophic forgetting, is exacerbated in decentralized settings by the data heterogeneity, constrained communication and privacy concerns. We propose Federated gradient Projection-based Continual Learning with Task Identity Prediction (FedProTIP), a novel FCL framework that mitigates forgetting by projecting client updates onto the orthogonal complement of the subspace spanned by previously learned representations of the global model. This projection reduces interference with earlier tasks and preserves performance across the task sequence. To further address the challenge of task-agnostic inference, we incorporate a lightweight mechanism that leverages core bases from prior tasks to predict task identity and dynamically adjust the global model's outputs. Extensive experiments across standard FCL benchmarks demonstrate that FedProTIP significantly outperforms state-of-the-art methods in average accuracy, particularly in settings where task identities are a priori unknown.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimism as Risk-Seeking in Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.24047</link>
<guid>https://arxiv.org/abs/2509.24047</guid>
<content:encoded><![CDATA[
arXiv:2509.24047v2 Announce Type: replace 
Abstract: Risk sensitivity has become a central theme in reinforcement learning (RL), where convex risk measures and robust formulations provide principled ways to model preferences beyond expected return. Recent extensions to multi-agent RL (MARL) have largely emphasized the risk-averse setting, prioritizing robustness to uncertainty. In cooperative MARL, however, such conservatism often leads to suboptimal equilibria, and a parallel line of work has shown that optimism can promote cooperation. Existing optimistic methods, though effective in practice, are typically heuristic and lack theoretical grounding. Building on the dual representation for convex risk measures, we propose a principled framework that interprets risk-seeking objectives as optimism. We introduce optimistic value functions, which formalize optimism as divergence-penalized risk-seeking evaluations. Building on this foundation, we derive a policy-gradient theorem for optimistic value functions, including explicit formulas for the entropic risk/KL-penalty setting, and develop decentralized optimistic actor-critic algorithms that implement these updates. Empirical results on cooperative benchmarks demonstrate that risk-seeking optimism consistently improves coordination over both risk-neutral baselines and heuristic optimistic methods. Our framework thus unifies risk-sensitive learning and optimism, offering a theoretically grounded and practically effective approach to cooperation in MARL.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AXIS: Explainable Time Series Anomaly Detection with Large Language Models</title>
<link>https://arxiv.org/abs/2509.24378</link>
<guid>https://arxiv.org/abs/2509.24378</guid>
<content:encoded><![CDATA[
arXiv:2509.24378v2 Announce Type: replace 
Abstract: Time-series anomaly detection (TSAD) increasingly demands explanations that articulate not only if an anomaly occurred, but also what pattern it exhibits and why it is anomalous. Leveraging the impressive explanatory capabilities of Large Language Models (LLMs), recent works have attempted to treat time series as text for explainable TSAD. However, this approach faces a fundamental challenge: LLMs operate on discrete tokens and struggle to directly process long, continuous signals. Consequently, naive time-to-text serialization suffers from a lack of contextual grounding and representation alignment between the two modalities. To address this gap, we introduce AXIS, a framework that conditions a frozen LLM for nuanced time-series understanding. Instead of direct serialization, AXIS enriches the LLM's input with three complementary hints derived from the series: (i) a symbolic numeric hint for numerical grounding, (ii) a context-integrated, step-aligned hint distilled from a pretrained time-series encoder to capture fine-grained dynamics, and (iii) a task-prior hint that encodes global anomaly characteristics. Furthermore, to facilitate robust evaluation of explainability, we introduce a new benchmark featuring multi-format questions and rationales that supervise contextual grounding and pattern-level semantics. Extensive experiments, including both LLM-based and human evaluations, demonstrate that AXIS yields explanations of significantly higher quality and achieves competitive detection accuracy compared to general-purpose LLMs, specialized time-series LLMs, and time-series Vision Language Models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Profiles for Protein Fitness Prediction</title>
<link>https://arxiv.org/abs/2510.07286</link>
<guid>https://arxiv.org/abs/2510.07286</guid>
<content:encoded><![CDATA[
arXiv:2510.07286v2 Announce Type: replace 
Abstract: Predicting the fitness impact of mutations is central to protein engineering but constrained by limited assays relative to the size of sequence space. Protein language models (pLMs) trained with masked language modeling (MLM) exhibit strong zero-shot fitness prediction; we provide a unifying view by interpreting natural evolution as implicit reward maximization and MLM as inverse reinforcement learning (IRL), in which extant sequences act as expert demonstrations and pLM log-odds serve as fitness estimates. Building on this perspective, we introduce EvoIF, a lightweight model that integrates two complementary sources of evolutionary signal: (i) within-family profiles from retrieved homologs and (ii) cross-family structural-evolutionary constraints distilled from inverse folding logits. EvoIF fuses sequence-structure representations with these profiles via a compact transition block, yielding calibrated probabilities for log-odds scoring. On ProteinGym (217 mutational assays; >2.5M mutants), EvoIF and its MSA-enabled variant achieve state-of-the-art or competitive performance while using only 0.15% of the training data and fewer parameters than recent large models. Ablations confirm that within-family and cross-family profiles are complementary, improving robustness across function types, MSA depths, taxa, and mutation depths. The codes will be made publicly available at https://github.com/aim-uofa/EvoIF.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Some theoretical improvements on the tightness of PAC-Bayes risk certificates for neural networks</title>
<link>https://arxiv.org/abs/2510.07935</link>
<guid>https://arxiv.org/abs/2510.07935</guid>
<content:encoded><![CDATA[
arXiv:2510.07935v2 Announce Type: replace 
Abstract: This paper presents four theoretical contributions that improve the usability of risk certificates for neural networks based on PAC-Bayes bounds. First, two bounds on the KL divergence between Bernoulli distributions enable the derivation of the tightest explicit bounds on the true risk of classifiers across different ranges of empirical risk. The paper next focuses on the formalization of an efficient methodology based on implicit differentiation that enables the introduction of the optimization of PAC-Bayesian risk certificates inside the loss/objective function used to fit the network/model. The last contribution is a method to optimize bounds on non-differentiable objectives such as the 0-1 loss. These theoretical contributions are complemented with an empirical evaluation on the MNIST and CIFAR-10 datasets. In fact, this paper presents the first non-vacuous generalization bounds on CIFAR-10 for neural networks. Code to reproduce all experiments is available at github.com/Diegogpcm/pacbayesgradients.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Policy Transfer for Continuous-Time Reinforcement Learning: A (Rough) Differential Equation Approach</title>
<link>https://arxiv.org/abs/2510.15165</link>
<guid>https://arxiv.org/abs/2510.15165</guid>
<content:encoded><![CDATA[
arXiv:2510.15165v2 Announce Type: replace 
Abstract: This paper studies policy transfer, one of the well-known transfer learning techniques adopted in large language models, for two classes of continuous-time reinforcement learning problems. In the first class of continuous-time linear-quadratic systems with Shannon's entropy regularization (a.k.a. LQRs), we fully exploit the Gaussian structure of their optimal policy and the stability of their associated Riccati equations. In the second class where the system has possibly non-linear and bounded dynamics, the key technical component is the stability of diffusion SDEs which is established by invoking the rough path theory. Our work provides the first theoretical proof of policy transfer for continuous-time RL: an optimal policy learned for one RL problem can be used to initialize the search for a near-optimal policy in a closely related RL problem, while maintaining the convergence rate of the original algorithm.
  To illustrate the benefit of policy transfer for RL, we propose a novel policy learning algorithm for continuous-time LQRs, which achieves global linear convergence and local super-linear convergence. As a byproduct of our analysis, we derive the stability of a concrete class of continuous-time score-based diffusion models via their connection with LQRs.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</title>
<link>https://arxiv.org/abs/2510.18713</link>
<guid>https://arxiv.org/abs/2510.18713</guid>
<content:encoded><![CDATA[
arXiv:2510.18713v2 Announce Type: replace 
Abstract: We study online preference-based reinforcement learning (PbRL) with the goal of improving sample efficiency. While a growing body of theoretical work has emerged-motivated by PbRL's recent empirical success, particularly in aligning large language models (LLMs)-most existing studies focus only on pairwise comparisons. A few recent works (Zhu et al., 2023, Mukherjee et al., 2024, Thekumparampil et al., 2024) have explored using multiple comparisons and ranking feedback, but their performance guarantees fail to improve-and can even deteriorate-as the feedback length increases, despite the richer information available. To address this gap, we adopt the Plackett-Luce (PL) model for ranking feedback over action subsets and propose M-AUPO, an algorithm that selects multiple actions by maximizing the average uncertainty within the offered subset. We prove that M-AUPO achieves a suboptimality gap of $\tilde{O}\left( \frac{d}{T} \sqrt{ \sum_{t=1}^T \frac{1}{|S_t|}} \right)$, where $T$ is the total number of rounds, $d$ is the feature dimension, and $|S_t|$ is the size of the subset at round $t$. This result shows that larger subsets directly lead to improved performance and, notably, the bound avoids the exponential dependence on the unknown parameter's norm, which was a fundamental limitation in most previous works. Moreover, we establish a near-matching lower bound of $\Omega \left( \frac{d}{K \sqrt{T}} \right)$, where $K$ is the maximum subset size. To the best of our knowledge, this is the first theoretical result in PbRL with ranking feedback that explicitly shows improved sample efficiency as a function of the subset size.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disentangled Representation Learning via Modular Compositional Bias</title>
<link>https://arxiv.org/abs/2510.21402</link>
<guid>https://arxiv.org/abs/2510.21402</guid>
<content:encoded><![CDATA[
arXiv:2510.21402v2 Announce Type: replace 
Abstract: Recent disentangled representation learning (DRL) methods heavily rely on factor specific strategies-either learning objectives for attributes or model architectures for objects-to embed inductive biases. Such divergent approaches result in significant overhead when novel factors of variation do not align with prior assumptions, such as statistical independence or spatial exclusivity, or when multiple factors coexist, as practitioners must redesign architectures or objectives. To address this, we propose a compositional bias, a modular inductive bias decoupled from both objectives and architectures. Our key insight is that different factors obey distinct recombination rules in the data distribution: global attributes are mutually exclusive, e.g., a face has one nose, while objects share a common support (any subset of objects can co-exist). We therefore randomly remix latents according to factor-specific rules, i.e., a mixing strategy, and force the encoder to discover whichever factor structure the mixing strategy reflects through two complementary objectives: (i) a prior loss that ensures every remix decodes into a realistic image, and (ii) the compositional consistency loss introduced by Wiedemer et al. (arXiv:2310.05327), which aligns each composite image with its corresponding composite latent. Under this general framework, simply adjusting the mixing strategy enables disentanglement of attributes, objects, and even both, without modifying the objectives or architectures. Extensive experiments demonstrate that our method shows competitive performance in both attribute and object disentanglement, and uniquely achieves joint disentanglement of global style and objects. Code is available at https://github.com/whieya/Compositional-DRL.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Normalization in Attention Dynamics</title>
<link>https://arxiv.org/abs/2510.22026</link>
<guid>https://arxiv.org/abs/2510.22026</guid>
<content:encoded><![CDATA[
arXiv:2510.22026v2 Announce Type: replace 
Abstract: We study the effect of normalization schemes on token representations in deep transformers. Modeling their evolution as interacting particles on the sphere, we show that normalization acts as a form of speed regulation. This perspective enables a unified analysis of several schemes -- including Post-LN, Pre-LN, Mix-LN, Peri-LN, nGPT -- revealing how they influence clustering dynamics and representation collapse. Our framework clarifies how different schemes shape token representations across layers and provides a principled basis for comparing them, identifying Peri-LN as a particularly effective choice.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Strategizing against No-regret Learners</title>
<link>https://arxiv.org/abs/1909.13861</link>
<guid>https://arxiv.org/abs/1909.13861</guid>
<content:encoded><![CDATA[
arXiv:1909.13861v2 Announce Type: replace-cross 
Abstract: How should a player who repeatedly plays a game against a no-regret learner strategize to maximize his utility? We study this question and show that under some mild assumptions, the player can always guarantee himself a utility of at least what he would get in a Stackelberg equilibrium of the game. When the no-regret learner has only two actions, we show that the player cannot get any higher utility than the Stackelberg equilibrium utility. But when the no-regret learner has more than two actions and plays a mean-based no-regret strategy, we show that the player can get strictly higher than the Stackelberg equilibrium utility. We provide a characterization of the optimal game-play for the player against a mean-based no-regret learner as a solution to a control problem. When the no-regret learner's strategy also guarantees him a no-swap regret, we show that the player cannot get anything higher than a Stackelberg equilibrium utility.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Guided Machine Learning Methods for Hydrology</title>
<link>https://arxiv.org/abs/2012.02854</link>
<guid>https://arxiv.org/abs/2012.02854</guid>
<content:encoded><![CDATA[
arXiv:2012.02854v2 Announce Type: replace-cross 
Abstract: Streamflow prediction is one of the key challenges in the field of hydrology due to the complex interplay between multiple non-linear physical mechanisms behind streamflow generation. While physics based models are rooted in rich understanding of the physical processes, a significant performance gap still remains which can be potentially addressed by leveraging the recent advances in machine learning. The goal of this work is to incorporate our understanding of hydrological processes and constraints into machine learning algorithms to improve the predictive performance. Traditional ML models for this problem predict streamflow using weather drivers as input. However there are multiple intermediate processes that interact to generate streamflow from weather drivers. The key idea of the approach is to explicitly model these intermediate processes that connect weather drivers to streamflow using a multi-task learning framework. While our proposed approach requires data about intermediate processes during training, only weather drivers will be needed to predict the streamflow during testing phase. We assess the efficacy of the approach on a simulation dataset generated by the SWAT model for a catchment located in the South Branch of the Root River Watershed in southeast Minnesota. While the focus of this paper is on improving the performance given data from a single catchment, methodology presented here is applicable to ML-based approaches that use data from multiple catchments to improve performance of each individual catchment.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large deviations for interacting particle dynamics for finding mixed equilibria in zero-sum games</title>
<link>https://arxiv.org/abs/2206.15177</link>
<guid>https://arxiv.org/abs/2206.15177</guid>
<content:encoded><![CDATA[
arXiv:2206.15177v4 Announce Type: replace-cross 
Abstract: Finding equilibrium points in continuous minmax games has become a key problem within machine learning, in part due to its connection to the training of generative adversarial networks and reinforcement learning. Because of existence and robustness issues, recent developments have shifted from pure equilibria to focusing on mixed equilibrium points. In this work we consider a method for finding mixed equilibria in two-layer zero-sum games based on entropic regularisation, where the two competing strategies are represented by two sets of interacting particles. We show that the sequence of empirical measures of the particle system satisfies a large deviation principle as the number of particles grows to infinity, and how this implies convergence of the empirical measure and the associated Nikaid\^o-Isoda error, complementing existing law of large numbers results.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable data-driven turbulence closure modeling on unstructured grids with differentiable physics</title>
<link>https://arxiv.org/abs/2307.13533</link>
<guid>https://arxiv.org/abs/2307.13533</guid>
<content:encoded><![CDATA[
arXiv:2307.13533v3 Announce Type: replace-cross 
Abstract: Differentiable physical simulators are proving to be valuable tools for developing data-driven models for computational fluid dynamics (CFD). In particular, these simulators enable end-to-end training of machine learning (ML) models embedded within CFD solvers. This paradigm enables novel algorithms which combine the generalization power and low cost of physics-based simulations with the flexibility and automation of deep learning methods. In this study, we introduce a framework for embedding deep learning models within a finite element solver for incompressible Navier-Stokes equations, specifically applying this approach to learn a subgrid-scale (SGS) closure with a graph neural network (GNN). We first demonstrate the feasibility of the approach on flow over a two-dimensional backward-facing step, using it as a proof of concept to show that solver-consistent training produces stable and physically meaningful closures. Then, we extend this to a turbulent flow over a three-dimensional backward-facing step. In this setting, the GNN-based closure not only attains low prediction errors, but also recovers key turbulence statistics and preserves multiscale turbulent structures. We further demonstrate that the closure can be identified in data-limited learning scenarios as well. Overall, the proposed end-to-end learning paradigm offers a viable pathway toward physically consistent and generalizable data-driven SGS modeling on complex and unstructured domains.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Domain Generalization Algorithms in Computational Pathology</title>
<link>https://arxiv.org/abs/2409.17063</link>
<guid>https://arxiv.org/abs/2409.17063</guid>
<content:encoded><![CDATA[
arXiv:2409.17063v2 Announce Type: replace-cross 
Abstract: Deep learning models have shown immense promise in computational pathology (CPath) tasks, but their performance often suffers when applied to unseen data due to domain shifts. Addressing this requires domain generalization (DG) algorithms. However, a systematic evaluation of DG algorithms in the CPath context is lacking. This study aims to benchmark the effectiveness of 30 DG algorithms on 3 CPath tasks of varying difficulty through 7,560 cross-validation runs. We evaluate these algorithms using a unified and robust platform, incorporating modality-specific techniques and recent advances like pretrained foundation models. Our extensive cross-validation experiments provide insights into the relative performance of various DG strategies. We observe that self-supervised learning and stain augmentation consistently outperform other methods, highlighting the potential of pretrained models and data augmentation. Furthermore, we introduce a new pan-cancer tumor detection dataset (HISTOPANTUM) as a benchmark for future research. This study offers valuable guidance to researchers in selecting appropriate DG approaches for CPath tasks.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selection of LLM Fine-Tuning Data based on Orthogonal Rules</title>
<link>https://arxiv.org/abs/2410.04715</link>
<guid>https://arxiv.org/abs/2410.04715</guid>
<content:encoded><![CDATA[
arXiv:2410.04715v3 Announce Type: replace-cross 
Abstract: High-quality training data is critical to the performance of large language models (LLMs). Recent work has explored using LLMs to rate and select data based on a small set of human-designed criteria (rules), but these approaches often rely heavily on heuristics, lack principled metrics for rule evaluation, and generalize poorly to new tasks. We propose a novel rule-based data selection framework that introduces a metric based on the orthogonality of rule score vectors to evaluate and select complementary rules. Our automated pipeline first uses LLMs to generate diverse rules covering multiple aspects of data quality, then rates samples according to these rules and applies the determinantal point process (DPP) to select the most independent rules. These rules are then used to score the full dataset, and high-scoring samples are selected for downstream tasks such as LLM fine-tuning. We evaluate our framework in two experiment setups: (1) alignment with ground-truth ratings and (2) performance of LLMs fine-tuned on the selected data. Experiments across IMDB, Medical, Math, and Code domains demonstrate that our DPP-based rule selection consistently improves both rating accuracy and downstream model performance over strong baselines.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural gradient and parameter estimation for quantum Boltzmann machines</title>
<link>https://arxiv.org/abs/2410.24058</link>
<guid>https://arxiv.org/abs/2410.24058</guid>
<content:encoded><![CDATA[
arXiv:2410.24058v2 Announce Type: replace-cross 
Abstract: Thermal states play a fundamental role in various areas of physics, and they are becoming increasingly important in quantum information science, with applications related to semi-definite programming, quantum Boltzmann machine learning, Hamiltonian learning, and the related task of estimating the parameters of a Hamiltonian. Here we establish formulas underlying the basic geometry of parameterized thermal states, and we delineate quantum algorithms for estimating the values of these formulas. More specifically, we establish formulas for the Fisher--Bures and Kubo--Mori information matrices of parameterized thermal states, and our quantum algorithms for estimating their matrix elements involve a combination of classical sampling, Hamiltonian simulation, and the Hadamard test. These results have applications in developing a natural gradient descent algorithm for quantum Boltzmann machine learning, which takes into account the geometry of thermal states, and in establishing fundamental limitations on the ability to estimate the parameters of a Hamiltonian, when given access to thermal-state samples. For the latter task, and for the special case of estimating a single parameter, we sketch an algorithm that realizes a measurement that is asymptotically optimal for the estimation task. We finally stress that the natural gradient descent algorithm developed here can be used for any machine learning problem that employs the quantum Boltzmann machine ansatz.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Outlyingness Scores with Cluster Catch Digraphs</title>
<link>https://arxiv.org/abs/2501.05530</link>
<guid>https://arxiv.org/abs/2501.05530</guid>
<content:encoded><![CDATA[
arXiv:2501.05530v2 Announce Type: replace-cross 
Abstract: This paper introduces two novel, outlyingness scores (OSs) based on Cluster Catch Digraphs (CCDs): Outbound Outlyingness Score (OOS) and Inbound Outlyingness Score (IOS). These scores enhance the interpretability of outlier detection results. Both OSs employ graph-, density-, and distribution-based techniques, tailored to high-dimensional data with varying cluster shapes and intensities. OOS evaluates the outlyingness of a point relative to its nearest neighbors, while IOS assesses the total ``influence" a point receives from others within its cluster. Both OSs effectively identify global and local outliers, invariant to data collinearity. Moreover, IOS is robust to the masking problems. With extensive Monte Carlo simulations, we compare the performance of both OSs with CCD-based, traditional, and state-of-the-art outlier detection methods. Both OSs exhibit substantial overall improvements over the CCD-based methods in both artificial and real-world data sets, particularly with IOS, which delivers the best overall performance among all the methods, especially in high-dimensional settings.
  Keywords: Outlier detection, Outlyingness score, Graph-based clustering, Cluster catch digraphs, High-dimensional data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in Linear Models</title>
<link>https://arxiv.org/abs/2502.05074</link>
<guid>https://arxiv.org/abs/2502.05074</guid>
<content:encoded><![CDATA[
arXiv:2502.05074v3 Announce Type: replace-cross 
Abstract: We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and linear random feature models. Our results include previously known asymptotics as well as novel ones.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence and Stability of Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning, and Online Decision Transformers</title>
<link>https://arxiv.org/abs/2502.05672</link>
<guid>https://arxiv.org/abs/2502.05672</guid>
<content:encoded><![CDATA[
arXiv:2502.05672v2 Announce Type: replace-cross 
Abstract: This article provides a rigorous analysis of convergence and stability of Episodic Upside-Down Reinforcement Learning, Goal-Conditioned Supervised Learning and Online Decision Transformers. These algorithms performed competitively across various benchmarks, from games to robotic tasks, but their theoretical understanding is limited to specific environmental conditions. This work initiates a theoretical foundation for algorithms that build on the broad paradigm of approaching reinforcement learning through supervised learning or sequence modeling. At the core of this investigation lies the analysis of conditions on the underlying environment, under which the algorithms can identify optimal solutions. We also assess whether emerging solutions remain stable in situations where the environment is subject to tiny levels of noise. Specifically, we study the continuity and asymptotic convergence of command-conditioned policies, values and the goal-reaching objective depending on the transition kernel of the underlying Markov Decision Process. We demonstrate that near-optimal behavior is achieved if the transition kernel is located in a sufficiently small neighborhood of a deterministic kernel. The mentioned quantities are continuous (with respect to a specific topology) at deterministic kernels, both asymptotically and after a finite number of learning cycles. The developed methods allow us to present the first explicit estimates on the convergence and stability of policies and values in terms of the underlying transition kernels. On the theoretical side we introduce a number of new concepts to reinforcement learning, like working in segment spaces, studying continuity in quotient topologies and the application of the fixed-point theory of dynamical systems. The theoretical study is accompanied by a detailed investigation of example environments and numerical experiments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Attention Mechanism Learning to Facilitate Opto-physiological Monitoring during Physical Activity</title>
<link>https://arxiv.org/abs/2502.09291</link>
<guid>https://arxiv.org/abs/2502.09291</guid>
<content:encoded><![CDATA[
arXiv:2502.09291v2 Announce Type: replace-cross 
Abstract: Opto-physiological monitoring including photoplethysmography (PPG) provides non-invasive cardiac and respiratory measurements, yet motion artefacts (MAs) during physical activity degrade its signal quality and downstream estimation concurrently. An attention-mechanism-based generative adversarial network (AM-GAN) was proposed to model motion artefacts and mitigate their impact on raw PPG signals. The AM-GAN learns how to transform motion-affected PPG into artefact-reduced waveforms to align with triaxial acceleration signals corresponding to artefact components gained from a triaxial accelerometer. The AM-GAN has been validated across four experimental protocols with 43 participants performing activities from low to high intensity (6--12km/h). With the public datasets, the AM-GAN achieves mean absolute error (MAE) for heart rate (HR) of 1.81 beats/min on IEEE-SPC and 3.86 beats/min on PPGDalia. On the in-house LU dataset, it shows the MAEs < 1.37 beats/min for HR and 2.49 breaths/min for respiratory rate (RR). A further in-house C2 dataset with three oxygen levels (16%, 18%, and 21%) was applied in the AM-GAN to attain a MAE of 1.65% for SpO2. The outcome demonstrates that the AM-GAN offers a robust and reliable physiological estimation under various intensities of physical activity.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Signature Kernel Computations for Long Time Series via Local Neumann Series Expansions</title>
<link>https://arxiv.org/abs/2502.20392</link>
<guid>https://arxiv.org/abs/2502.20392</guid>
<content:encoded><![CDATA[
arXiv:2502.20392v2 Announce Type: replace-cross 
Abstract: The signature kernel is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via adaptively truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE, our approach employs tilewise Neumann-series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent Goursat PDEs via adaptively truncated local power series expansions and recursive propagation of boundary conditions along a directed graph in a topological ordering. This method strikes an effective balance between computational cost and accuracy, achieving substantial performance improvements over state-of-the-art approaches for computing the signature kernel. It offers (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (one million data points or more) on a single GPU. As demonstrated in our benchmarks, these advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications involving very long and highly volatile sequential data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TeleRAG: Efficient Retrieval-Augmented Generation Inference with Lookahead Retrieval</title>
<link>https://arxiv.org/abs/2502.20969</link>
<guid>https://arxiv.org/abs/2502.20969</guid>
<content:encoded><![CDATA[
arXiv:2502.20969v3 Announce Type: replace-cross 
Abstract: Retrieval-augmented generation (RAG) extends large language models (LLMs) with external data sources to enhance factual correctness and domain coverage. Modern RAG pipelines rely on large datastores, creating a significant system challenge: achieving high throughput and low latency is difficult, especially when GPU memory is limited. To address these challenges, we propose TeleRAG, an efficient inference system that reduces latency and improves throughput with minimal GPU memory requirements. The core innovation of TeleRAG is lookahead retrieval, a prefetching mechanism that predicts required data and transfers them from CPU to GPU in parallel with LLM generation. In addition, TeleRAG adopts a prefetching scheduler and a cache-aware scheduler to support efficient multi-GPU inference with minimal overhead. Evaluations show TeleRAG achieves up to a 1.53x average end-to-end latency reduction (single-query) and 1.83x higher average throughput (batched), as well as good scalability in throughput. This confirms the practical utility of TeleRAG for faster and more memory-efficient deployments of RAG applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IAEmu: Learning Galaxy Intrinsic Alignment Correlations</title>
<link>https://arxiv.org/abs/2504.05235</link>
<guid>https://arxiv.org/abs/2504.05235</guid>
<content:encoded><![CDATA[
arXiv:2504.05235v3 Announce Type: replace-cross 
Abstract: The intrinsic alignments (IA) of galaxies, a key contaminant in weak lensing analyses, arise from correlations in galaxy shapes driven by tidal interactions and galaxy formation processes. Accurate IA modeling is essential for robust cosmological inference, but current approaches rely on perturbative methods that break down on nonlinear scales or on expensive simulations. We introduce IAEmu, a neural network-based emulator that predicts the galaxy position-position ($\xi$), position-orientation ($\omega$), and orientation-orientation ($\eta$) correlation functions and their uncertainties using mock catalogs based on the halo occupation distribution (HOD) framework. Compared to simulations, IAEmu achieves ~3% average error for $\xi$ and ~5% for $\omega$, while capturing the stochasticity of $\eta$ without overfitting. The emulator provides both aleatoric and epistemic uncertainties, helping identify regions where predictions may be less reliable. We also demonstrate generalization to non-HOD alignment signals by fitting to IllustrisTNG hydrodynamical simulation data. As a fully differentiable neural network, IAEmu enables $\sim$10,000$\times$ speed-ups in mapping HOD parameters to correlation functions on GPUs, compared to CPU-based simulations. This acceleration facilitates inverse modeling via gradient-based sampling, making IAEmu a powerful surrogate model for galaxy bias and IA studies with direct applications to Stage IV weak lensing surveys.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTI-LF: A Continuous Learning Framework for Real-Time Malicious Traffic Detection in Multi-Environment Networks</title>
<link>https://arxiv.org/abs/2504.11575</link>
<guid>https://arxiv.org/abs/2504.11575</guid>
<content:encoded><![CDATA[
arXiv:2504.11575v2 Announce Type: replace-cross 
Abstract: Multi-environment (M-En) networks integrate diverse traffic sources, including Internet of Things (IoT) and traditional computing systems, creating complex and evolving conditions for malicious traffic detection. Existing machine learning (ML)-based approaches, typically trained on static single-domain datasets, often fail to generalize across heterogeneous network environments. To address this gap, we develop a realistic Docker-NS3-based testbed that emulates both IoT and traditional traffic conditions, enabling the generation and capture of live, labeled network flows. The resulting M-En Dataset combines this traffic with curated public PCAP traces to provide comprehensive coverage of benign and malicious behaviors. Building on this foundation, we propose Multi-LF, a real-time continuous learning framework that combines a lightweight model (M1) for rapid detection with a deeper model (M2) for high-confidence refinement and adaptation. A confidence-based coordination mechanism enhances efficiency without compromising accuracy, while weight interpolation mitigates catastrophic forgetting during continuous updates. Features extracted at 1-second intervals capture fine-grained temporal patterns, enabling early recognition of evolving attack behaviors. Implemented and evaluated within the Docker-NS3 testbed on live traffic, Multi-LF achieves an accuracy of 0.999 while requiring human intervention for only 0.0026 percent of packets, demonstrating its effectiveness and practicality for real-time malicious traffic detection in heterogeneous network environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the generalization of language models from in-context learning and finetuning: a controlled study</title>
<link>https://arxiv.org/abs/2505.00661</link>
<guid>https://arxiv.org/abs/2505.00661</guid>
<content:encoded><![CDATA[
arXiv:2505.00661v3 Announce Type: replace-cross 
Abstract: Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning. E.g. they can fail to generalize to simple reversals of relations they are trained on, or fail to make simple logical deductions based on trained information. These failures to generalize factual information from fine-tuning can significantly hinder the reasoning capabilities of these models. On the other hand, language models' in-context learning (ICL) shows different inductive biases and deductive reasoning capabilities. Here, we explore these differences in generalization and deductive reasoning between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' abilities to make generalizations over factual information from novel data. These datasets are designed to create clean tests of generalization, by isolating the knowledge in the dataset from that in pretraining. We expose pretrained large models to controlled subsets of the information in these datasets -- either through ICL or fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, ICL can generalize several types of inferences more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context reasoning traces to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the generalization afforded by different modes of learning in language models, and practically improving their performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</title>
<link>https://arxiv.org/abs/2505.06771</link>
<guid>https://arxiv.org/abs/2505.06771</guid>
<content:encoded><![CDATA[
arXiv:2505.06771v3 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot RL (MRRL) policies with realistic robot dynamics and safety constraints, supporting parallelization and hardware acceleration. Our generalizable learning interface integrates easily with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation. Our code is available at https://github.com/GT-STAR-Lab/JaxRobotarium.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein Distributionally Robust Nonparametric Regression</title>
<link>https://arxiv.org/abs/2505.07967</link>
<guid>https://arxiv.org/abs/2505.07967</guid>
<content:encoded><![CDATA[
arXiv:2505.07967v2 Announce Type: replace-cross 
Abstract: Wasserstein distributionally robust optimization (WDRO) strengthens statistical learning under model uncertainty by minimizing the local worst-case risk within a prescribed ambiguity set. Although WDRO has been extensively studied in parametric settings, its theoretical properties in nonparametric frameworks remain underexplored. This paper investigates WDRO for nonparametric regression. We first establish a structural distinction based on the order $k$ of the Wasserstein distance, showing that $k=1$ induces Lipschitz-type regularization, whereas $k > 1$ corresponds to gradient-norm regularization. To address model misspecification, we analyze the excess local worst-case risk, deriving non-asymptotic error bounds for estimators constructed using norm-constrained feedforward neural networks. This analysis is supported by new covering number and approximation bounds that simultaneously control both the function and its gradient. The proposed estimator achieves a convergence rate of $n^{-2\beta/(d+2\beta)}$ up to logarithmic factors, where $\beta$ depends on the target's smoothness and network parameters. This rate is shown to be minimax optimal under conditions commonly satisfied in high-dimensional settings. Moreover, these bounds on the excess local worst-case risk imply guarantees on the excess natural risk, ensuring robustness against any distribution within the ambiguity set. We show the framework's generality across regression and classification problems. Simulation studies and an application to the MNIST dataset further illustrate the estimator's robustness.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HAPO: Training Language Models to Reason Concisely via History-Aware Policy Optimization</title>
<link>https://arxiv.org/abs/2505.11225</link>
<guid>https://arxiv.org/abs/2505.11225</guid>
<content:encoded><![CDATA[
arXiv:2505.11225v2 Announce Type: replace-cross 
Abstract: While scaling the length of responses at test-time has been shown to markedly improve the reasoning abilities and performance of large language models (LLMs), it often results in verbose outputs and increases inference cost. Prior approaches for efficient test-time scaling, typically using universal budget constraints or query-level length optimization, do not leverage historical information from previous encounters with the same problem during training. We hypothesize that this limits their ability to progressively make solutions more concise over time. To address this, we present History-Aware Policy Optimization (HAPO), which keeps track of a history state (e.g., the minimum length over previously generated correct responses) for each problem. HAPO employs a novel length reward function based on this history state to incentivize the discovery of correct solutions that are more concise than those previously found. Crucially, this reward structure avoids overly penalizing shorter incorrect responses with the goal of facilitating exploration towards more efficient solutions. By combining this length reward with a correctness reward, HAPO jointly optimizes for correctness and efficiency. We use HAPO to train DeepSeek-R1-Distill-Qwen-1.5B, DeepScaleR-1.5B-Preview, and Qwen-2.5-1.5B-Instruct, and evaluate HAPO on several math benchmarks that span various difficulty levels. Experiment results demonstrate that HAPO effectively induces LLMs' concise reasoning abilities, producing length reductions of 33-59% with accuracy drops of only 2-5%.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous Subspace Optimization for Continual Learning</title>
<link>https://arxiv.org/abs/2505.11816</link>
<guid>https://arxiv.org/abs/2505.11816</guid>
<content:encoded><![CDATA[
arXiv:2505.11816v2 Announce Type: replace-cross 
Abstract: Continual learning aims to learn multiple tasks sequentially while preserving prior knowledge, but faces the challenge of catastrophic forgetting when adapting to new tasks. Recently, approaches leveraging pre-trained models have gained increasing popularity in mitigating this issue, due to the strong generalization ability of foundation models. To adjust pre-trained models for new tasks, existing methods usually employ low-rank adaptation, which restricts parameter updates to a fixed low-rank subspace. However, constraining the optimization space inherently compromises the model's learning capacity, resulting in inferior performance. To address this limitation, we propose Continuous Subspace Optimization for Continual Learning (CoSO) to fine-tune the model in a series of subspaces rather than a single one. These sequential subspaces are dynamically determined through the singular value decomposition of the gradients. CoSO updates the model by projecting gradients onto these subspaces, ensuring memory-efficient optimization. To mitigate forgetting, the optimization subspace of each task is constrained to be orthogonal to the historical task subspace. During task learning, CoSO maintains a task-specific component that captures the critical update directions for the current task. Upon completing a task, this component is used to update the historical task subspace, laying the groundwork for subsequent learning. Extensive experiments on multiple datasets demonstrate that CoSO significantly outperforms state-of-the-art methods, especially in challenging scenarios with long task sequences.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BroadGen: A Framework for Generating Effective and Efficient Advertiser Broad Match Keyphrase Recommendations</title>
<link>https://arxiv.org/abs/2505.19164</link>
<guid>https://arxiv.org/abs/2505.19164</guid>
<content:encoded><![CDATA[
arXiv:2505.19164v4 Announce Type: replace-cross 
Abstract: In the domain of sponsored search advertising, the focus of {Keyphrase recommendation has largely been on exact match types, which pose issues such as high management expenses, limited targeting scope, and evolving search query patterns. Alternatives like Broad match types can alleviate certain drawbacks of exact matches but present challenges like poor targeting accuracy and minimal supervisory signals owing to limited advertiser usage. This research defines the criteria for an ideal broad match, emphasizing on both efficiency and effectiveness, ensuring that a significant portion of matched queries are relevant. We propose BroadGen, an innovative framework that recommends efficient and effective broad match keyphrases by utilizing historical search query data. Additionally, we demonstrate that BroadGen, through token correspondence modeling, maintains better query stability over time. BroadGen's capabilities allow it to serve daily, millions of sellers at eBay with over 2.5 billion items.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified and Fast-Sampling Diffusion Bridge Framework via Stochastic Optimal Control</title>
<link>https://arxiv.org/abs/2505.21528</link>
<guid>https://arxiv.org/abs/2505.21528</guid>
<content:encoded><![CDATA[
arXiv:2505.21528v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion bridge models leverage Doob's $h$-transform to establish fixed endpoints between distributions, demonstrating promising results in image translation and restoration tasks. However, these approaches often produce blurred or excessively smoothed image details and lack a comprehensive theoretical foundation to explain these shortcomings. To address these limitations, we propose UniDB, a unified and fast-sampling framework for diffusion bridges based on Stochastic Optimal Control (SOC). We reformulate the problem through an SOC-based optimization, proving that existing diffusion bridges employing Doob's $h$-transform constitute a special case, emerging when the terminal penalty coefficient in the SOC cost function tends to infinity. By incorporating a tunable terminal penalty coefficient, UniDB achieves an optimal balance between control costs and terminal penalties, substantially improving detail preservation and output quality. To avoid computationally expensive costs of iterative Euler sampling methods in UniDB, we design a training-free accelerated algorithm by deriving exact closed-form solutions for UniDB's reverse-time SDE. It is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes, effectively reducing error accumulation. Extensive experiments across diverse image restoration tasks validate the superiority and adaptability of the proposed framework, bridging the gap between theoretical generality and practical efficiency. Our code is available online https://github.com/2769433owo/UniDB-plusplus.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images</title>
<link>https://arxiv.org/abs/2505.21872</link>
<guid>https://arxiv.org/abs/2505.21872</guid>
<content:encoded><![CDATA[
arXiv:2505.21872v2 Announce Type: replace-cross 
Abstract: Machine unlearning aims to remove the influence of specific training samples from a trained model without full retraining. While prior work has largely focused on privacy-motivated settings, we recast unlearning as a general-purpose tool for post-deployment model revision. Specifically, we focus on utilizing unlearning in clinical contexts where data shifts, device deprecation, and policy changes are common. To this end, we propose a bilevel optimization formulation of boundary-based unlearning that can be solved using iterative algorithms. We provide convergence guarantees when first-order algorithms are used to unlearn. Our method introduces tunable loss design for controlling the forgetting-retention tradeoff and supports novel model composition strategies that merge the strengths of distinct unlearning runs. Across benchmark and real-world clinical imaging datasets, our approach outperforms baselines on both forgetting and retention metrics, including scenarios involving imaging devices and anatomical outliers. This work establishes machine unlearning as a modular, practical alternative to retraining for real-world model maintenance in clinical applications.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniSite: The First Cross-Structure Dataset and Learning Framework for End-to-End Ligand Binding Site Detection</title>
<link>https://arxiv.org/abs/2506.03237</link>
<guid>https://arxiv.org/abs/2506.03237</guid>
<content:encoded><![CDATA[
arXiv:2506.03237v3 Announce Type: replace-cross 
Abstract: The detection of ligand binding sites for proteins is a fundamental step in Structure-Based Drug Design. Despite notable advances in recent years, existing methods, datasets, and evaluation metrics are confronted with several key challenges: (1) current datasets and methods are centered on individual protein-ligand complexes and neglect that diverse binding sites may exist across multiple complexes of the same protein, introducing significant statistical bias; (2) ligand binding site detection is typically modeled as a discontinuous workflow, employing binary segmentation and subsequent clustering algorithms; (3) traditional evaluation metrics do not adequately reflect the actual performance of different binding site prediction methods. To address these issues, we first introduce UniSite-DS, the first UniProt (Unique Protein)-centric ligand binding site dataset, which contains 4.81 times more multi-site data and 2.08 times more overall data compared to the previously most widely used datasets. We then propose UniSite, the first end-to-end ligand binding site detection framework supervised by set prediction loss with bijective matching. In addition, we introduce Average Precision based on Intersection over Union (IoU) as a more accurate evaluation metric for ligand binding site prediction. Extensive experiments on UniSite-DS and several representative benchmark datasets demonstrate that IoU-based Average Precision provides a more accurate reflection of prediction quality, and that UniSite outperforms current state-of-the-art methods in ligand binding site detection. The dataset and codes will be made publicly available at https://github.com/quanlin-wu/unisite.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Trilemma of Truth in Large Language Models</title>
<link>https://arxiv.org/abs/2506.23921</link>
<guid>https://arxiv.org/abs/2506.23921</guid>
<content:encoded><![CDATA[
arXiv:2506.23921v3 Announce Type: replace-cross 
Abstract: The public often attributes human-like qualities to large language models (LLMs) and assumes they "know" certain things. In reality, LLMs encode information retained during training as internal probabilistic knowledge. This study examines existing methods for probing the veracity of that knowledge and identifies several flawed underlying assumptions. To address these flaws, we introduce sAwMIL (Sparse-Aware Multiple-Instance Learning), a multiclass probing framework that combines multiple-instance learning with conformal prediction. sAwMIL leverages internal activations of LLMs to classify statements as true, false, or neither. We evaluate sAwMIL across 16 open-source LLMs, including default and chat-based variants, on three new curated datasets. Our results show that (1) common probing methods fail to provide a reliable and transferable veracity direction and, in some settings, perform worse than zero-shot prompting; (2) truth and falsehood are not encoded symmetrically; and (3) LLMs encode a third type of signal that is distinct from both true and false.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Malliavin calculus approach to score functions in diffusion generative models</title>
<link>https://arxiv.org/abs/2507.05550</link>
<guid>https://arxiv.org/abs/2507.05550</guid>
<content:encoded><![CDATA[
arXiv:2507.05550v4 Announce Type: replace-cross 
Abstract: Score-based diffusion generative models have recently emerged as a powerful tool for modelling complex data distributions. These models aim at learning the score function, which defines a map from a known probability distribution to the target data distribution via deterministic or stochastic differential equations (SDEs). The score function is typically estimated from data using a variety of approximation techniques, such as denoising or sliced score matching, Hyv\"arien's method, or Schr\"odinger bridges. In this paper, we derive an exact, closed-form, expression for the score function for a broad class of nonlinear diffusion generative models. Our approach combines modern stochastic analysis tools such as Malliavin derivatives and their adjoint operators (Skorokhod integrals or Malliavin Divergence) with a new Bismut-type formula. The resulting expression for the score function can be written entirely in terms of the first and second variation processes, with all Malliavin derivatives systematically eliminated, thereby enhancing its practical applicability. The theoretical framework presented in this work offers a principled foundation for advancing score estimation methods in generative modelling, enabling the design of new sampling algorithms for complex probability distributions. Our results can be extended to broader classes of stochastic differential equations, opening new directions for the development of score-based diffusion generative models.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism</title>
<link>https://arxiv.org/abs/2507.10069</link>
<guid>https://arxiv.org/abs/2507.10069</guid>
<content:encoded><![CDATA[
arXiv:2507.10069v3 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) extend LLMs to handle images, videos, and audio by incorporating feature extractors and projection modules. However, these additional components -- combined with complex inference pipelines and heterogeneous workloads -- introduce significant inference overhead. Therefore, efficiently serving MLLMs remains a major challenge. Current tightly coupled serving architectures struggle to distinguish between mixed request types or adapt parallelism strategies to different inference stages, leading to increased time-to-first-token (TTFT) latency and poor resource utilization. To address this, we introduce Elastic Multimodal Parallelism (EMP), a new serving paradigm that elastically adapts to resource heterogeneity across request types and inference stages. Building upon EMP, we develop ElasticMM, an MLLM serving system that (1) separates requests into independent modality groups with dynamic resource allocation via a modality-aware load balancer; (2) decouples inference stages and enables parallelism adjustment and adaptive scaling via elastic partition scheduling; and (3) improves inference efficiency through unified multimodal prefix caching and non-blocking encoding. Experiments on diverse real-world datasets show that ElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by up to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level objectives (SLOs).
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Embodied Agentic AI: Review and Classification of LLM- and VLM-Driven Robot Autonomy and Interaction</title>
<link>https://arxiv.org/abs/2508.05294</link>
<guid>https://arxiv.org/abs/2508.05294</guid>
<content:encoded><![CDATA[
arXiv:2508.05294v3 Announce Type: replace-cross 
Abstract: Foundation models, including large language models (LLMs) and vision-language models (VLMs), have recently enabled novel approaches to robot autonomy and human-robot interfaces. In parallel, vision-language-action models (VLAs) or large behavior models (LBMs) are increasing the dexterity and capabilities of robotic systems. This survey paper reviews works that advance agentic applications and architectures, including initial efforts with GPT-style interfaces and more complex systems where AI agents function as coordinators, planners, perception actors, or generalist interfaces. Such agentic architectures allow robots to reason over natural language instructions, invoke APIs, plan task sequences, or assist in operations and diagnostics. In addition to peer-reviewed research, due to the fast-evolving nature of the field, we highlight and include community-driven projects, ROS packages, and industrial frameworks that show emerging trends. We propose a taxonomy for classifying model integration approaches and present a comparative analysis of the role that agents play in different solutions in today's literature.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-driven Adaptive Exploration</title>
<link>https://arxiv.org/abs/2509.03219</link>
<guid>https://arxiv.org/abs/2509.03219</guid>
<content:encoded><![CDATA[
arXiv:2509.03219v2 Announce Type: replace-cross 
Abstract: Adaptive exploration methods propose ways to learn complex policies via alternating between exploration and exploitation. An important question for such methods is to determine the appropriate moment to switch between exploration and exploitation and vice versa. This is critical in domains that require the learning of long and complex sequences of actions. In this work, we present a generic adaptive exploration framework that employs uncertainty to address this important issue in a principled manner. Our framework includes previous adaptive exploration approaches as special cases. Moreover, we can incorporate in our framework any uncertainty-measuring mechanism of choice, for instance mechanisms used in intrinsic motivation or epistemic uncertainty-based exploration methods. We experimentally demonstrate that our framework gives rise to adaptive exploration strategies that outperform standard ones across several MuJoCo environments.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How fast can you find a good hypothesis?</title>
<link>https://arxiv.org/abs/2509.03734</link>
<guid>https://arxiv.org/abs/2509.03734</guid>
<content:encoded><![CDATA[
arXiv:2509.03734v2 Announce Type: replace-cross 
Abstract: In the hypothesis selection problem, we are given sample and query access to finite set of candidate distributions (hypotheses), $\mathcal{H} = \{H_1, \ldots, H_n\}$, and samples from an unknown distribution $P$, both over a domain $\mathcal{X}$. The goal is to output a distribution $Q$ whose distance to $P$ is comparable to that of the nearest hypothesis in $\mathcal{H}$. Specifically, if the minimum distance is $\mathsf{OPT}$, we aim to output $Q$ such that, with probability at least $1-\delta$, its total variation distance to $P$ is at most $C \cdot \mathsf{OPT} + \varepsilon$. The optimal approximation for proper algorithms (where $Q \in \mathcal{H}$) is $C=3$ using $\Theta(\log(n/\delta)/\varepsilon^2)$ samples from $P$ and for improper algorithms (where $Q$ is not necessarily in $\mathcal{H}$) is $C=2$ using $\tilde{\Theta}(\log(n/\delta)/\varepsilon^2)$ samples from $P$.
  In the improper setting, the algorithm achieving $C=2$ [Bousquet, Braverman, Kol, Efremenko, Moran, FOCS 2021] runs in time which grows polynomially with $|\mathcal{X}|$ -- it does not run in finite time for real-valued distributions. A promising path towards improved runtime is to consider improper algorithms which output a mixture $Q$ of the hypotheses as such a distribution can be represented in $n$ words of memory. We show (1) a lower bound that no algorithm which outputs a mixture can achieve approximation better than $C = 3-2/n$ unless the number of samples is polynomial in $|\mathcal{X}|$, as well as (2) an algorithm which runs in time $\text{poly}(n)$ and achieves the same approximation guarantee.
  In the proper setting, [Aliakbarpour, Bun, Smith, NeurIPS 2024] provided an algorithm with $C=3$ running in $\tilde{O}(n/(\delta^3\varepsilon^3))$ time. We improve this time complexity to $\tilde{O}(n/(\delta \varepsilon^2))$, significantly reducing the dependence on the confidence and error parameters.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-based Relevance Assessment for Web-Scale Search Evaluation at Pinterest</title>
<link>https://arxiv.org/abs/2509.03764</link>
<guid>https://arxiv.org/abs/2509.03764</guid>
<content:encoded><![CDATA[
arXiv:2509.03764v2 Announce Type: replace-cross 
Abstract: Relevance evaluation plays a crucial role in personalized search systems to ensure that search results align with a user's queries and intent. While human annotation is the traditional method for relevance evaluation, its high cost and long turnaround time limit its scalability. In this work, we present our approach at Pinterest Search to automate relevance evaluation for online experiments using fine-tuned LLMs. We rigorously validate the alignment between LLM-generated judgments and human annotations, demonstrating that LLMs can provide reliable relevance measurement for experiments while greatly improving the evaluation efficiency. Leveraging LLM-based labeling further unlocks the opportunities to expand the query set, optimize sampling design, and efficiently assess a wider range of search experiences at scale. This approach leads to higher-quality relevance metrics and significantly reduces the Minimum Detectable Effect (MDE) in online experiment measurements.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably data-driven projection method for quadratic programming</title>
<link>https://arxiv.org/abs/2509.04524</link>
<guid>https://arxiv.org/abs/2509.04524</guid>
<content:encoded><![CDATA[
arXiv:2509.04524v2 Announce Type: replace-cross 
Abstract: Projection methods aim to reduce the dimensionality of the optimization instance, thereby improving the scalability of high-dimensional problems. Recently, Sakaue and Oki proposed a data-driven approach for linear programs (LPs), where the projection matrix is learned from observed problem instances drawn from an application-specific distribution of problems. We analyze the generalization guarantee for the data-driven projection matrix learning for convex quadratic programs (QPs). Unlike in LPs, the optimal solutions of convex QPs are not confined to the vertices of the feasible polyhedron, and this complicates the analysis of the optimal value function. To overcome this challenge, we demonstrate that the solutions of convex QPs can be localized within a feasible region corresponding to a special active set, utilizing Caratheodory's theorem. Building on such observation, we propose the unrolled active set method, which models the computation of the optimal value as a Goldberg-Jerrum (GJ) algorithm with bounded complexities, thereby establishing learning guarantees. We then further extend our analysis to other settings, including learning to match the optimal solution and input-aware setting, where we learn a mapping from QP problem instances to projection matrices.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable Holographic Reconstruction via Amplitude-Only Diffusion Priors</title>
<link>https://arxiv.org/abs/2509.12728</link>
<guid>https://arxiv.org/abs/2509.12728</guid>
<content:encoded><![CDATA[
arXiv:2509.12728v3 Announce Type: replace-cross 
Abstract: Phase retrieval in inline holography is a fundamental yet ill-posed inverse problem due to the nonlinear coupling between amplitude and phase in coherent imaging. We present a novel off-the-shelf solution that leverages a diffusion model trained solely on object amplitude to recover both amplitude and phase from diffraction intensities. Using a predictor-corrector sampling framework with separate likelihood gradients for amplitude and phase, our method enables complex field reconstruction without requiring ground-truth phase data for training. We validate the proposed approach through extensive simulations and experiments, demonstrating robust generalization across diverse object shapes, imaging system configurations, and modalities, including lensless setups. Notably, a diffusion prior trained on simple amplitude data (e.g., polystyrene beads) successfully reconstructs complex biological tissue structures, highlighting the method's adaptability. This framework provides a cost-effective, generalizable solution for nonlinear inverse problems in computational imaging, and establishes a foundation for broader coherent imaging applications beyond holography.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Core-elements Subsampling for Alternating Least Squares</title>
<link>https://arxiv.org/abs/2509.18024</link>
<guid>https://arxiv.org/abs/2509.18024</guid>
<content:encoded><![CDATA[
arXiv:2509.18024v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel element-wise subset selection method for the alternating least squares (ALS) algorithm, focusing on low-rank matrix factorization involving matrices with missing values, as commonly encountered in recommender systems. While ALS is widely used for providing personalized recommendations based on user-item interaction data, its high computational cost, stemming from repeated regression operations, poses significant challenges for large-scale datasets. To enhance the efficiency of ALS, we propose a core-elements subsampling method that selects a representative subset of data and leverages sparse matrix operations to approximate ALS estimations efficiently. We establish theoretical guarantees for the approximation and convergence of the proposed approach, showing that it achieves similar accuracy with significantly reduced computational time compared to full-data ALS. Extensive simulations and real-world applications demonstrate the effectiveness of our method in various scenarios, emphasizing its potential in large-scale recommendation systems.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Re-uploading quantum data: A universal function approximator for quantum inputs</title>
<link>https://arxiv.org/abs/2509.18530</link>
<guid>https://arxiv.org/abs/2509.18530</guid>
<content:encoded><![CDATA[
arXiv:2509.18530v5 Announce Type: replace-cross 
Abstract: Quantum data re-uploading has proved powerful for classical inputs, where repeatedly encoding features into a small circuit yields universal function approximation. Extending this idea to quantum inputs remains underexplored, as the information contained in a quantum state is not directly accessible in classical form. We propose and analyze a quantum data re-uploading architecture in which a qubit interacts sequentially with fresh copies of an arbitrary input state. The circuit can approximate any bounded continuous function using only one ancilla qubit and single-qubit measurements. By alternating entangling unitaries with mid-circuit resets of the input register, the architecture realizes a discrete cascade of completely positive and trace-preserving maps, analogous to collision models in open quantum system dynamics. Our framework provides a qubit-efficient and expressive approach to designing quantum machine learning models that operate directly on quantum data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Uncertainty Impacts Machine Learning Evaluations</title>
<link>https://arxiv.org/abs/2509.22242</link>
<guid>https://arxiv.org/abs/2509.22242</guid>
<content:encoded><![CDATA[
arXiv:2509.22242v2 Announce Type: replace-cross 
Abstract: Clinical dataset labels are rarely certain as annotators disagree and confidence is not uniform across cases. Typical aggregation procedures, such as majority voting, obscure this variability. In simple experiments on medical imaging benchmarks, accounting for the confidence in binary labels significantly impacts model rankings. We therefore argue that machine-learning evaluations should explicitly account for annotation uncertainty using probabilistic metrics that directly operate on distributions. These metrics can be applied independently of the annotations' generating process, whether modeled by simple counting, subjective confidence ratings, or probabilistic response models. They are also computationally lightweight, as closed-form expressions have linear-time implementations once examples are sorted by model score. We thus urge the community to release raw annotations for datasets and to adopt uncertainty-aware evaluation so that performance estimates may better reflect clinical data.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MENLO: From Preferences to Proficiency -- Evaluating and Modeling Native-like Quality Across 47 Languages</title>
<link>https://arxiv.org/abs/2509.26601</link>
<guid>https://arxiv.org/abs/2509.26601</guid>
<content:encoded><![CDATA[
arXiv:2509.26601v2 Announce Type: replace-cross 
Abstract: Ensuring native-like quality of large language model (LLM) responses across many languages is challenging. To address this, we introduce MENLO, a framework that operationalizes the evaluation of native-like response quality based on audience design-inspired mechanisms. Using MENLO, we create a dataset of 6,423 human-annotated prompt-response preference pairs covering four quality dimensions with high inter-annotator agreement in 47 language varieties. Our evaluation reveals that zero-shot LLM judges benefit significantly from pairwise evaluation and our structured annotation rubrics, yet they still underperform human annotators on our dataset. We demonstrate substantial improvements through fine-tuning with reinforcement learning, reward shaping, and multi-task learning approaches. Additionally, we show that RL-trained judges can serve as generative reward models to enhance LLMs' multilingual proficiency, though discrepancies with human judgment remain. Our findings suggest promising directions for scalable multilingual evaluation and preference alignment. We release our dataset and evaluation framework to support further research in multilingual LLM evaluation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AgentFlux: Decoupled Fine-Tuning &amp; Inference for On-Device Agentic Systems</title>
<link>https://arxiv.org/abs/2510.00229</link>
<guid>https://arxiv.org/abs/2510.00229</guid>
<content:encoded><![CDATA[
arXiv:2510.00229v3 Announce Type: replace-cross 
Abstract: The deployment of Large Language Models (LLMs) as agentic orchestrators has revolutionized task automation, but the need for privacy-preserving, cost-effective solutions demands on-device inference capabilities. However, local LLMs consistently underperform compared to frontier models in tool calling scenarios, struggling with both tool selection from large tool sets and accurate argument generation for complex parameter structures. We introduce a methodology that disaggregates a tool-calling task into two distinct subtasks: tool selection and argument generation. We propose "decoupled fine-tuning", a novel post-training approach that employs LoRA fine-tuning to create dedicated LoRA adapters for tool selection and tool-specific argument generation using separate loss masking for each of the subtasks. Furthermore, we present DualTune, an inference framework that leverages the LoRA adapters created using decoupled fine-tuning to perform efficient agent orchestration with the help of local models on end-user devices. DualTune decomposes the tool-call generation step into tool selection and argument generation, and dynamically loads the corresponding LoRA adapters to generate tool calls. Additionally, DualTune implements hierarchical orchestration to restrict the number of tools required for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool calling accuracy of the base model by 46%, and outperforms other local reasoning, non-reasoning and fine-tuned models of similar size in all cases, and models that are 2x larger, in most cases.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epistemic Diversity and Knowledge Collapse in Large Language Models</title>
<link>https://arxiv.org/abs/2510.04226</link>
<guid>https://arxiv.org/abs/2510.04226</guid>
<content:encoded><![CDATA[
arXiv:2510.04226v5 Announce Type: replace-cross 
Abstract: Large language models (LLMs) tend to generate lexically, semantically, and stylistically homogenous texts. This poses a risk of knowledge collapse, where homogenous LLMs mediate a shrinking in the range of accessible information over time. Existing works on homogenization are limited by a focus on closed-ended multiple-choice setups or fuzzy semantic features, and do not look at trends across time and cultural contexts. To overcome this, we present a new methodology to measure epistemic diversity, i.e., variation in real-world claims in LLM outputs, which we use to perform a broad empirical study of LLM knowledge collapse. We test 27 LLMs, 155 topics covering 12 countries, and 200 prompt variations sourced from real user chats. For the topics in our study, we show that while newer models tend to generate more diverse claims, nearly all models are less epistemically diverse than a basic web search. We find that model size has a negative impact on epistemic diversity, while retrieval-augmented generation (RAG) has a positive impact, though the improvement from RAG varies by the cultural context. Finally, compared to a traditional knowledge source (Wikipedia), we find that country-specific claims reflect the English language more than the local one, highlighting a gap in epistemic representation
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models</title>
<link>https://arxiv.org/abs/2510.16565</link>
<guid>https://arxiv.org/abs/2510.16565</guid>
<content:encoded><![CDATA[
arXiv:2510.16565v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly used across diverse cultural contexts, making accurate cultural understanding essential. Prior evaluations have mostly focused on output-level performance, obscuring the factors that drive differences in responses, while studies using circuit analysis have covered few languages and rarely focused on culture. In this work, we trace LLMs' internal cultural understanding mechanisms by measuring activation path overlaps when answering semantically equivalent questions under two conditions: varying the target country while fixing the question language, and varying the question language while fixing the country. We also use same-language country pairs to disentangle language from cultural aspects. Results show that internal paths overlap more for same-language, cross-country questions than for cross-language, same-country questions, indicating strong language-specific patterns. Notably, the South Korea-North Korea pair exhibits low overlap and high variability, showing that linguistic similarity does not guarantee aligned internal representation.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DynaSolidGeo: A Dynamic Benchmark for Genuine Spatial Mathematical Reasoning of VLMs in Solid Geometry</title>
<link>https://arxiv.org/abs/2510.22340</link>
<guid>https://arxiv.org/abs/2510.22340</guid>
<content:encoded><![CDATA[
arXiv:2510.22340v2 Announce Type: replace-cross 
Abstract: Solid geometry problem solving demands spatial mathematical reasoning that integrates spatial intelligence and symbolic reasoning. However, most existing multimodal mathematical reasoning benchmarks focus primarily on 2D plane geometry, rely on static datasets prone to data contamination and memorization, and evaluate models solely by final answers, overlooking the reasoning process. To address these limitations, we introduce DynaSolidGeo, the first dynamic benchmark for evaluating genuine spatial reasoning in Vision-Language Models (VLMs). Constructed through a semi-automatic annotation pipeline, DynaSolidGeo contains 503 expert-curated seed questions that can, in principle, dynamically generate an unbounded number of diverse multimodal text-visual instances. Beyond answer accuracy, we incorporate process evaluation based on expert-annotated reasoning chains to measure logical validity and causal coherence. Experiments across representative open-source and closed-source VLMs reveal large performance gaps, severe degradation in dynamic settings, and poor performance on tasks requiring high-level spatial intelligence, such as mental rotation and visualization. The code and dataset are available at \href{https://zgca-ai4edu.github.io/DynaSolidGeo/}{DynaSolidGeo}.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heuristic Adaptation of Potentially Misspecified Domain Support for Likelihood-Free Inference in Stochastic Dynamical Systems</title>
<link>https://arxiv.org/abs/2510.26656</link>
<guid>https://arxiv.org/abs/2510.26656</guid>
<content:encoded><![CDATA[
arXiv:2510.26656v2 Announce Type: replace-cross 
Abstract: In robotics, likelihood-free inference (LFI) can provide the domain distribution that adapts a learnt agent in a parametric set of deployment conditions. LFI assumes an arbitrary support for sampling, which remains constant as the initial generic prior is iteratively refined to more descriptive posteriors. However, a potentially misspecified support can lead to suboptimal, yet falsely certain, posteriors. To address this issue, we propose three heuristic LFI variants: EDGE, MODE, and CENTRE. Each interprets the posterior mode shift over inference steps in its own way and, when integrated into an LFI step, adapts the support alongside posterior inference. We first expose the support misspecification issue and evaluate our heuristics using stochastic dynamical benchmarks. We then evaluate the impact of heuristic support adaptation on parameter inference and policy learning for a dynamic deformable linear object (DLO) manipulation task. Inference results in a finer length and stiffness classification for a parametric set of DLOs. When the resulting posteriors are used as domain distributions for sim-based policy learning, they lead to more robust object-centric agent performance.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GUI-AIMA: Aligning Intrinsic Multimodal Attention with a Context Anchor for GUI Grounding</title>
<link>https://arxiv.org/abs/2511.00810</link>
<guid>https://arxiv.org/abs/2511.00810</guid>
<content:encoded><![CDATA[
arXiv:2511.00810v2 Announce Type: replace-cross 
Abstract: Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have some native grounding capability, nested within their attentions, we propose GUI-AIMA, an attention-based and coordinate-free supervised fine-tuning framework for efficient GUI grounding. GUI-AIMA aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. These signals are calculated adaptively for diverse user instructions by multi-head aggregation on simplified query-visual attention matrices. Besides, its coordinate-free manner can easily integrate a plug-and-play zoom-in stage. GUI-AIMA-3B was trained with only 85k screenshots, demonstrating exceptional data efficiency and verifying that light training can trigger the native grounding capability of MLLMs. It achieves state-of-the-art performance among 3B models, attaining an average accuracy of 59.6% on ScreenSpot-Pro, 63.8% on OSWorld-G and 91.5% on ScreenSpot-v2. Project page: https://github.com/sjz5202/GUI-AIMA
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adaptivity Barrier in Batched Nonparametric Bandits: Sharp Characterization of the Price of Unknown Margin</title>
<link>https://arxiv.org/abs/2511.03708</link>
<guid>https://arxiv.org/abs/2511.03708</guid>
<content:encoded><![CDATA[
arXiv:2511.03708v2 Announce Type: replace-cross 
Abstract: We study batched nonparametric contextual bandits under a margin condition when the margin parameter $\alpha$ is unknown. To capture the statistical cost of this ignorance, we introduce the regret inflation criterion, defined as the ratio between the regret of an adaptive algorithm and that of an oracle knowing $\alpha$. We show that the optimal regret inflation grows polynomially with the horizon $T$, with exponent given by the value of a convex optimization problem that depends on the dimension, smoothness, and number of batches $M$. Moreover, the minimizer of this optimization problem directly prescribes the batch allocation and exploration strategy of a rate-optimal algorithm. Building on this principle, we develop RoBIN (RObust batched algorithm with adaptive BINning), which achieves the optimal regret inflation up to polylogarithmic factors. These results reveal a new adaptivity barrier: under batching, adaptation to an unknown margin parameter inevitably incurs a polynomial penalty, sharply characterized by a variational problem. Remarkably, this barrier vanishes once the number of batches exceeds order $\log \log T$; with only a doubly logarithmic number of updates, one can recover the oracle regret rate up to polylogarithmic factors.
]]></content:encoded>
<pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Feedback-Control Framework for Efficient Dataset Collection from In-Vehicle Data Streams</title>
<link>https://arxiv.org/abs/2511.03239</link>
<guid>https://arxiv.org/abs/2511.03239</guid>
<content:encoded><![CDATA[
<div> Controlled data collection, Feedback mechanism, Dataset diversity, Reducing redundancy, Data-centric AI<br />
Summary: 
Modern AI systems often face constraints not in model capacity but in data quality and diversity. The traditional open-loop data collection approach leads to inefficiencies in storage, labeling, and generalization. This paper introduces Feedback Control Data Collection (FCDC), which treats data collection as a closed-loop control problem. FCDC uses an online probabilistic model to approximate data distribution and adjusts sample retention based on feedback signals like likelihood and Mahalanobis distance. This approach balances exploration and exploitation, maintains dataset diversity, and prevents redundancy accumulation. Experimental results show that FCDC produces more balanced datasets by 25.9% and reduces data storage by 39.8%. FCDC demonstrates that data collection can be actively controlled, transforming it from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.<br /><br /> <div>
arXiv:2511.03239v2 Announce Type: replace 
Abstract: Modern AI systems are increasingly constrained not by model capacity but by the quality and diversity of their data. Despite growing emphasis on data-centric AI, most datasets are still gathered in an open-loop manner which accumulates redundant samples without feedback from the current coverage. This results in inefficient storage, costly labeling, and limited generalization. To address this, this paper introduces Feedback Control Data Collection (FCDC), a paradigm that formulates data collection as a closed-loop control problem. FCDC continuously approximates the state of the collected data distribution using an online probabilistic model and adaptively regulates sample retention using based on feedback signals such as likelihood and Mahalanobis distance. Through this feedback mechanism, the system dynamically balances exploration and exploitation, maintains dataset diversity, and prevents redundancy from accumulating over time. In addition to demonstrating the controllability of FCDC on a synthetic dataset that converges toward a uniform distribution under Gaussian input assumption, experiments on real data streams show that FCDC produces more balanced datasets by 25.9% while reducing data storage by 39.8%. These results demonstrate that data collection itself can be actively controlled, transforming collection from a passive pipeline stage into a self-regulating, feedback-driven process at the core of data-centric AI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating and Rotating: A Unified Framework for Weight Conditioning in PEFT</title>
<link>https://arxiv.org/abs/2511.00051</link>
<guid>https://arxiv.org/abs/2511.00051</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-Efficient Fine-Tuning, LoRA, DoRA, Pre-Diag, SORA

Summary:
The study focuses on improving Parameter-Efficient Fine-Tuning (PEFT) methods, particularly examining the DoRA approach. It is revealed that DoRA succeeds by increasing the singular value entropy of weight updates, leading to a more uniform distribution akin to full fine-tuning. This insight allows for the reformulation of DoRA into a more efficient matrix form, identifying it as a learnable weight conditioning method. A unified framework is proposed for designing advanced PEFT methods by exploring conditioning matrix placement and type. Two new methods, Pre-Diag and SORA, are introduced within this framework. Pre-Diag efficiently calibrates pre-trained weights before LoRA updates, enhancing performance and reducing training time. SORA employs parameter-efficient orthogonal rotation for a more powerful transformation of the feature space. Extensive experiments on language tasks demonstrate superior performance and efficiency compared to LoRA and DoRA.<br /><br />Summary: The study delves into enhancing Parameter-Efficient Fine-Tuning methods, dissecting DoRA's success, reformulating it for efficiency, and proposing new methods within a unified framework for superior performance on language tasks. <div>
arXiv:2511.00051v2 Announce Type: replace 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods are crucial for adapting large pre-trained models. Among these, LoRA is considered a foundational approach. Building on this, the influential DoRA method enhances performance by decomposing weight updates into magnitude and direction. However, its underlying mechanism remains unclear, and it introduces significant computational overhead. In this work, we first identify that DoRA's success stems from its capacity to increase the singular value entropy of the weight update matrix, which promotes a more uniform update distribution akin to full fine-tuning. We then reformulate DoRA into a mathematically equivalent and more efficient matrix form, revealing it as a learnable weight conditioning method. Based on this insight, we propose a unified framework for designing advanced PEFT methods by exploring two orthogonal dimensions: the architectural placement and the transformation type of the conditioning matrix. Within this framework, we introduce two novel methods: (1) \textbf{Pre-Diag}, which applies a diagonal conditioning matrix before the LoRA update to efficiently calibrate the pre-trained weights, thereby enhancing performance while reducing training time; and (2) \textbf{S}kewed \textbf{O}rthogonal \textbf{R}otation \textbf{A}daptation (\textbf{SORA}), which employs a parameter-efficient orthogonal rotation to perform a more powerful, norm-preserving transformation of the feature space. Extensive experiments on natural language understanding and generation tasks demonstrate that our proposed methods achieve superior performance and efficiency compared to both LoRA and DoRA. The code is available at https://github.com/MaeChd/SORA.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Brain Signals with Multimodal Speech and Vision Embeddings</title>
<link>https://arxiv.org/abs/2511.00065</link>
<guid>https://arxiv.org/abs/2511.00065</guid>
<content:encoded><![CDATA[
<div> Keywords: house, EEG signals, pre-trained models, language understanding, brain activity

Summary: 
This study explores how the brain processes language by comparing EEG signals with speech embeddings from pre-trained models. Researchers investigate which layers of models like wav2vec2 and CLIP best reflect brain processing during speech perception. Using ridge regression and contrastive decoding, they test different strategies such as individual layers, progressive concatenation, and progressive summation. The results suggest that combining multimodal, layer-aware representations can enhance our understanding of how the brain interprets language as more than just sound, but as a holistic experience. <div>
arXiv:2511.00065v2 Announce Type: replace 
Abstract: When we hear the word "house", we don't just process sound, we imagine walls, doors, memories. The brain builds meaning through layers, moving from raw acoustics to rich, multimodal associations. Inspired by this, we build on recent work from Meta that aligned EEG signals with averaged wav2vec2 speech embeddings, and ask a deeper question: which layers of pre-trained models best reflect this layered processing in the brain? We compare embeddings from two models: wav2vec2, which encodes sound into language, and CLIP, which maps words to images. Using EEG recorded during natural speech perception, we evaluate how these embeddings align with brain activity using ridge regression and contrastive decoding. We test three strategies: individual layers, progressive concatenation, and progressive summation. The findings suggest that combining multimodal, layer-aware representations may bring us closer to decoding how the brain understands language, not just as sound, but as experience.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tool Zero: Training Tool-Augmented LLMs via Pure RL from Scratch</title>
<link>https://arxiv.org/abs/2511.01934</link>
<guid>https://arxiv.org/abs/2511.01934</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, language models, generalization, tool augmentation, dynamic reward design
Summary: 

The article discusses the use of reinforcement learning (RL) to enhance language models' reasoning and generalization abilities, particularly in tool-agnostic scenarios. The authors propose a dynamic reward design for rule-based RL that shifts rewards from exploratory to exploitative tool-use patterns. They introduce the Tool-Zero series models, which aim to enable language models to autonomously utilize general tools without post-training. Experimental results show that these models achieve a performance improvement of over 7% compared to supervised fine-tuning and RL-with-supervised fine-tuning models. The gains are consistent across different dataset evaluations, highlighting the effectiveness and robustness of the proposed methods. By leveraging RL and dynamic reward design, the Tool-Zero series models demonstrate the potential for enhancing language models' capabilities in complex tasks. <div>
arXiv:2511.01934v2 Announce Type: replace 
Abstract: Training tool-augmented LLMs has emerged as a promising approach to enhancing language models' capabilities for complex tasks. The current supervised fine-tuning paradigm relies on constructing extensive domain-specific datasets to train models. However, this approach often struggles to generalize effectively to unfamiliar or intricate tool-use scenarios. Recently, reinforcement learning (RL) paradigm can endow LLMs with superior reasoning and generalization abilities. In this work, we address a key question: Can the pure RL be used to effectively elicit a model's intrinsic reasoning capabilities and enhance the tool-agnostic generalization? We propose a dynamic generalization-guided reward design for rule-based RL, which progressively shifts rewards from exploratory to exploitative tool-use patterns. Based on this design, we introduce the Tool-Zero series models. These models are trained to enable LLMs to autonomously utilize general tools by directly scaling up RL from Zero models (i.e., base models without post-training). Experimental results demonstrate that our models achieve over 7% performance improvement compared to both SFT and RL-with-SFT models under the same experimental settings. These gains are consistently replicated across cross-dataset and intra-dataset evaluations, validating the effectiveness and robustness of our methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustFSM: Submodular Maximization in Federated Setting with Malicious Clients</title>
<link>https://arxiv.org/abs/2511.02029</link>
<guid>https://arxiv.org/abs/2511.02029</guid>
<content:encoded><![CDATA[
<div> Keywords: submodular maximization, federated learning, privacy, client attacks, RobustFSM

Summary:
RobustFSM addresses the problem of submodular maximization in a federated setting, where decentralized clients own their data and have different quality definitions. The goal is to find a representative subset of a large dataset while respecting privacy and autonomy. The federated setting is susceptible to client attacks, such as sharing fake information. RobustFSM is a solution designed to be robust against various practical client attacks, outperforming conventional federated algorithms in severe attack scenarios. Empirical evaluation with real-world datasets demonstrates significant improvement in solution quality, up to 200%, depending on the dataset and attack scenarios. Overall, RobustFSM offers a promising approach to enhancing the security and performance of federated submodular maximization in the face of malicious client behavior.<br /><br />Summary: <div>
arXiv:2511.02029v2 Announce Type: replace 
Abstract: Submodular maximization is an optimization problem benefiting many machine learning applications, where we seek a small subset best representing an extremely large dataset. We focus on the federated setting where the data are locally owned by decentralized clients who have their own definitions for the quality of representability. This setting requires repetitive aggregation of local information computed by the clients. While the main motivation is to respect the privacy and autonomy of the clients, the federated setting is vulnerable to client misbehaviors: malicious clients might share fake information. An analogy is backdoor attack in conventional federated learning, but our challenge differs freshly due to the unique characteristics of submodular maximization. We propose RobustFSM, a federated submodular maximization solution that is robust to various practical client attacks. Its performance is substantiated with an empirical evaluation study using real-world datasets. Numerical results show that the solution quality of RobustFSM substantially exceeds that of the conventional federated algorithm when attacks are severe. The degree of this improvement depends on the dataset and attack scenarios, which can be as high as 200%
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NOWS: Neural Operator Warm Starts for Accelerating Iterative Solvers</title>
<link>https://arxiv.org/abs/2511.02481</link>
<guid>https://arxiv.org/abs/2511.02481</guid>
<content:encoded><![CDATA[
<div> Neural Operator Warm Starts, accelerated solvers, partial differential equations, high-fidelity simulation, iterative methods

Summary:
Neural Operator Warm Starts (NOWS) is a novel approach to speeding up high-fidelity simulations of partial differential equations (PDEs). By utilizing learned solution operators, NOWS enhances classical iterative solvers like conjugate gradient and GMRES by providing high-quality initial guesses. This hybrid strategy significantly reduces iteration counts and end-to-end runtime, cutting computational time by up to 90%. NOWS seamlessly integrates with various numerical methods like finite-difference and finite-element analysis, maintaining stability and convergence guarantees. The combination of neural operators for rapid inference and traditional solvers ensures a reliable and efficient solution for PDE simulations. <div>
arXiv:2511.02481v3 Announce Type: replace 
Abstract: Partial differential equations (PDEs) underpin quantitative descriptions across the physical sciences and engineering, yet high-fidelity simulation remains a major computational bottleneck for many-query, real-time, and design tasks. Data-driven surrogates can be strikingly fast but are often unreliable when applied outside their training distribution. Here we introduce Neural Operator Warm Starts (NOWS), a hybrid strategy that harnesses learned solution operators to accelerate classical iterative solvers by producing high-quality initial guesses for Krylov methods such as conjugate gradient and GMRES. NOWS leaves existing discretizations and solver infrastructures intact, integrating seamlessly with finite-difference, finite-element, isogeometric analysis, finite volume method, etc. Across our benchmarks, the learned initialization consistently reduces iteration counts and end-to-end runtime, resulting in a reduction of the computational time of up to 90 %, while preserving the stability and convergence guarantees of the underlying numerical algorithms. By combining the rapid inference of neural operators with the rigor of traditional solvers, NOWS provides a practical and trustworthy approach to accelerate high-fidelity PDE simulations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realizable Circuit Complexity: Embedding Computation in Space-Time</title>
<link>https://arxiv.org/abs/2509.19161</link>
<guid>https://arxiv.org/abs/2509.19161</guid>
<content:encoded><![CDATA[
<div> realizable circuit classes, physical constraints, geometric, energetic, thermodynamic, circuit complexity<br />
<br />
Summary: 
The article introduces a new family of circuit classes, $\mathbf{RC}_d$, that take into account physical constraints such as geometric limitations, energetic considerations, and thermodynamic realities. These classes model computation embedded in $d$-dimensional space, where each circuit follows conservative realizability laws. The volume and information flux of these circuits are bounded by specific functions of time and dimension, ensuring that growth occurs through local, physically constructible edits. The study shows that algorithms with certain runtimes cannot scale to inputs of maximal entropy in this framework. Furthermore, it establishes that any $d$-dimensional parallel implementation can offer at most a polynomial speed-up of degree $(d-1)$ over its optimal sequential counterpart. As the dimension approaches infinity, the realizable circuit class $\mathbf{RC}_\infty(\mathrm{polylog})$ converges to the classical class $\mathbf{NC}$, highlighting the idealization of classical parallelism. This extension of circuit complexity into the physical domain reveals universal scaling laws for computation. <br /><br />Summary: <div>
arXiv:2509.19161v3 Announce Type: replace-cross 
Abstract: Classical circuit complexity characterizes parallel computation in purely combinatorial terms, ignoring the physical constraints that govern real hardware. The standard classes $\mathbf{NC}$, $\mathbf{AC}$, and $\mathbf{TC}$ treat unlimited fan-in, free interconnection, and polynomial gate counts as feasible -- assumptions that conflict with geometric, energetic, and thermodynamic realities. We introduce the family of realizable circuit classes $\mathbf{RC}_d$, which model computation embedded in physical $d$-dimensional space. Each circuit in $\mathbf{RC}_d$ obeys conservative realizability laws: volume scales as $\mathcal{O}(t^d)$, cross-boundary information flux is bounded by $\mathcal{O}(t^{d-1})$ per unit time, and growth occurs through local, physically constructible edits. These bounds apply to all causal systems, classical or quantum. Within this framework, we show that algorithms with runtime $\omega(n^{d/(d-1)})$ cannot scale to inputs of maximal entropy, and that any $d$-dimensional parallel implementation offers at most a polynomial speed-up of degree $(d-1)$ over its optimal sequential counterpart. In the limit $d\to\infty$, $\mathbf{RC}_\infty(\mathrm{polylog})=\mathbf{NC}$, recovering classical parallelism as a non-physical idealization. By unifying geometry, causality, and information flow, $\mathbf{RC}_d$ extends circuit complexity into the physical domain, revealing universal scaling laws for computation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoAdv: Automated Adversarial Prompting for Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2511.02376</link>
<guid>https://arxiv.org/abs/2511.02376</guid>
<content:encoded><![CDATA[
<div> vulnerable, jailbreaking, multi-turn, AutoAdv, LLMs<br />
Summary:<br />
Large Language Models (LLMs) are still vulnerable to jailbreaking attacks using adversarial prompts, particularly in multi-turn conversations. The AutoAdv framework, which requires no training, achieved a 95% success rate on Llama-3.1-8B within six turns, a significant improvement over single-turn approaches. AutoAdv incorporates a pattern manager, temperature manager, and two-phase rewriting strategy to enhance attacks and disguise harmful requests. Evaluation across various models revealed weaknesses in current safety measures, with multi-turn attacks consistently more successful than single-turn methods. These results emphasize the importance of defenses tailored for multi-turn interactions to ensure robustness in extended conversations. <br /><br />Summary: <div>
arXiv:2511.02376v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) remain vulnerable to jailbreaking attacks where adversarial prompts elicit harmful outputs, yet most evaluations focus on single-turn interactions while real-world attacks unfold through adaptive multi-turn conversations. We present AutoAdv, a training-free framework for automated multi-turn jailbreaking that achieves up to 95% attack success rate on Llama-3.1-8B within six turns a 24 percent improvement over single turn baselines. AutoAdv uniquely combines three adaptive mechanisms: a pattern manager that learns from successful attacks to enhance future prompts, a temperature manager that dynamically adjusts sampling parameters based on failure modes, and a two-phase rewriting strategy that disguises harmful requests then iteratively refines them. Extensive evaluation across commercial and open-source models (GPT-4o-mini, Qwen3-235B, Mistral-7B) reveals persistent vulnerabilities in current safety mechanisms, with multi-turn attacks consistently outperforming single-turn approaches. These findings demonstrate that alignment strategies optimized for single-turn interactions fail to maintain robustness across extended conversations, highlighting an urgent need for multi-turn-aware defenses.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AGRAG: Advanced Graph-based Retrieval-Augmented Generation for LLMs</title>
<link>https://arxiv.org/abs/2511.05549</link>
<guid>https://arxiv.org/abs/2511.05549</guid>
<content:encoded><![CDATA[
<div> Graph-based retrieval-augmented generation, structured knowledge, LLM, reasoning ability, MCMI<br />
Summary:<br />
The article introduces AGRAG, an advanced framework to address challenges in Graph-based RAG. AGRAG tackles issues of Inaccurate Graph Construction by replacing LLM entity extraction with a statistics-based method. It improves Reasoning Ability by formulating reasoning as the MCMI subgraph generation problem, optimizing for high influence nodes with low edge costs. This enhancement allows for explicit reasoning paths that clarify why certain chunks were selected. AGRAG also enhances Answering capability by providing more comprehensive reasoning paths, reducing noise impact, and helping LLM focus on query-related content. By allowing complex graph structures like cycles, AGRAG improves the comprehensiveness of reasoning paths, enhancing overall performance in comparison to NaiveRAG on certain tasks.<br /> <br />Summary: <div>
arXiv:2511.05549v1 Announce Type: new 
Abstract: Graph-based retrieval-augmented generation (Graph-based RAG) has demonstrated significant potential in enhancing Large Language Models (LLMs) with structured knowledge. However, existing methods face three critical challenges: Inaccurate Graph Construction, caused by LLM hallucination; Poor Reasoning Ability, caused by failing to generate explicit reasons telling LLM why certain chunks were selected; and Inadequate Answering, which only partially answers the query due to the inadequate LLM reasoning, making their performance lag behind NaiveRAG on certain tasks. To address these issues, we propose AGRAG, an advanced graph-based retrieval-augmented generation framework. When constructing the graph, AGRAG substitutes the widely used LLM entity extraction method with a statistics-based method, avoiding hallucination and error propagation. When retrieval, AGRAG formulates the graph reasoning procedure as the Minimum Cost Maximum Influence (MCMI) subgraph generation problem, where we try to include more nodes with high influence score, but with less involving edge cost, to make the generated reasoning paths more comprehensive. We prove this problem to be NP-hard, and propose a greedy algorithm to solve it. The MCMI subgraph generated can serve as explicit reasoning paths to tell LLM why certain chunks were retrieved, thereby making the LLM better focus on the query-related part contents of the chunks, reducing the impact of noise, and improving AGRAG's reasoning ability. Furthermore, compared with the simple tree-structured reasoning paths, our MCMI subgraph can allow more complex graph structures, such as cycles, and improve the comprehensiveness of the generated reasoning paths.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep one-gate per layer networks with skip connections are universal classifiers</title>
<link>https://arxiv.org/abs/2511.05552</link>
<guid>https://arxiv.org/abs/2511.05552</guid>
<content:encoded><![CDATA[
<div> Keywords: multilayer perceptron, deep neural network, one-gate layers, skip connections, classification

Summary: 
This paper presents a method to convert a multilayer perceptron with two hidden layers designed for binary classification into a deep neural network with one-gate layers and skip connections. By implementing skip connections and reducing the number of gates in each layer, the network structure becomes more efficient and easier to interpret. This transformation allows for a more streamlined architecture while maintaining or improving classification performance. The use of skip connections helps alleviate the vanishing gradient problem and enables better information flow between layers. Additionally, the introduction of one-gate layers simplifies the network structure and reduces computational complexity. Overall, the proposed approach offers a novel way to enhance the performance and efficiency of neural networks for binary classification tasks. <br /><br />Summary: <div>
arXiv:2511.05552v1 Announce Type: new 
Abstract: This paper shows how a multilayer perceptron with two hidden layers, which has been designed to classify two classes of data points, can easily be transformed into a deep neural network with one-gate layers and skip connections.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Daily Forecasting for Annual Time Series Datasets Using Similarity-Based Machine Learning Methods: A Case Study in the Energy Market</title>
<link>https://arxiv.org/abs/2511.05556</link>
<guid>https://arxiv.org/abs/2511.05556</guid>
<content:encoded><![CDATA[
<div> Keywords: Energy Security Index, daily proxy, time series similarity measures, XGBoost algorithm, high frequency forecasting

Summary:
The study introduces a daily proxy for the Energy Security Index to forecast energy security at a daily frequency, allowing for more timely response to fluctuations in policy environments. By utilizing time series similarity measures, the study identifies Volume Brent as the most suitable proxy and applies the XGBoost algorithm to generate 15-day ahead forecasts with strong performance. The forecasted Brent volume shows short-term fluctuations with prediction intervals, enabling real-time monitoring of energy security dynamics. This innovative framework integrates low-frequency macroeconomic indicators with high-frequency actionable signals, offering policymakers and analysts a scalable tool to respond rapidly to fast-changing policy and market conditions, particularly in data-scarce environments. <div>
arXiv:2511.05556v1 Announce Type: new 
Abstract: The policy environment of countries changes rapidly, influencing macro-level indicators such as the Energy Security Index. However, this index is only reported annually, limiting its responsiveness to short-term fluctuations. To address this gap, the present study introduces a daily proxy for the Energy Security Index and applies it to forecast energy security at a daily frequency.The study employs a two stage approach first, a suitable daily proxy for the annual Energy Security Index is identified by applying six time series similarity measures to key energy related variables. Second, the selected proxy is modeled using the XGBoost algorithm to generate 15 day ahead forecasts, enabling high frequency monitoring of energy security dynamics.As the result of proxy choosing, Volume Brent consistently emerged as the most suitable proxy across the majority of methods. The model demonstrated strong performance, with an R squared of 0.981 on the training set and 0.945 on the test set, and acceptable error metrics . The 15 day forecast of Brent volume indicates short term fluctuations, with a peak around day 4, a decline until day 8, a rise near day 10, and a downward trend toward day 15, accompanied by prediction intervals.By integrating time series similarity measures with machine learning based forecasting, this study provides a novel framework for converting low frequency macroeconomic indicators into high frequency, actionable signals. The approach enables real time monitoring of the Energy Security Index, offering policymakers and analysts a scalable and practical tool to respond more rapidly to fast changing policy and market conditions, especially in data scarce environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diversified Flow Matching with Translation Identifiability</title>
<link>https://arxiv.org/abs/2511.05558</link>
<guid>https://arxiv.org/abs/2511.05558</guid>
<content:encoded><![CDATA[
<div> Keywords: Diversified distribution matching, unified translation function, flow matching, ODE-based framework, translation identifiability

Summary: 
Diversified distribution matching (DDM) aims to map conditional source distributions to their target counterparts, addressing content misalignment in unpaired domain translation and achieving translation identifiability. However, DDM has only been implemented using GANs, which are unstable and lack transport trajectory information. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. DFM adapts flow matching to enforce a unified translation function, overcoming challenges such as learning the translation function's velocity instead of the function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and structural reformulation provide a tangible implementation. DFM is the first ODE-based approach to ensure translation identifiability. Experiments on synthetic and real-world datasets confirm the effectiveness of the proposed method. 

<br /><br />Summary: <div>
arXiv:2511.05558v1 Announce Type: new 
Abstract: Diversified distribution matching (DDM) finds a unified translation function mapping a diverse collection of conditional source distributions to their target counterparts. DDM was proposed to resolve content misalignment issues in unpaired domain translation, achieving translation identifiability. However, DDM has only been implemented using GANs due to its constraints on the translation function. GANs are often unstable to train and do not provide the transport trajectory information -- yet such trajectories are useful in applications such as single-cell evolution analysis and robot route planning. This work introduces diversified flow matching (DFM), an ODE-based framework for DDM. Adapting flow matching (FM) to enforce a unified translation function as in DDM is challenging, as FM learns the translation function's velocity rather than the translation function itself. A custom bilevel optimization-based training loss, a nonlinear interpolant, and a structural reformulation are proposed to address these challenges, offering a tangible implementation. To our knowledge, DFM is the first ODE-based approach guaranteeing translation identifiability. Experiments on synthetic and real-world datasets validate the proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Effective Test-Time Scaling of Discrete Diffusion through Iterative Refinement</title>
<link>https://arxiv.org/abs/2511.05562</link>
<guid>https://arxiv.org/abs/2511.05562</guid>
<content:encoded><![CDATA[
<div> Reward-Guided Generation, Discrete Diffusion Models, Iterative Reward-Guided Refinement, Multiple-Try Metropolis, Test-Time Scaling <br />
Summary: <br />
The article introduces Iterative Reward-Guided Refinement (IterRef) as a test-time scaling method for discrete diffusion models. It leverages reward-guided transitions to refine misaligned states, unlike previous methods that only guide subsequent transitions. IterRef is formalized within a Multiple-Try Metropolis framework, proving convergence to the reward-aligned distribution. It progressively improves intermediate states by steering them towards the optimal distribution. Evaluation on text and image domains shows consistent enhancements in reward-guided generation quality. The method achieves significant improvements under low compute budgets, surpassing prior baselines. <div>
arXiv:2511.05562v1 Announce Type: new 
Abstract: Test-time scaling through reward-guided generation remains largely unexplored for discrete diffusion models despite its potential as a promising alternative. In this work, we introduce Iterative Reward-Guided Refinement (IterRef), a novel test-time scaling method tailored to discrete diffusion that leverages reward- guided noising-denoising transitions to progressively refine misaligned intermediate states. We formalize this process within a Multiple-Try Metropolis (MTM) framework, proving convergence to the reward-aligned distribution. Unlike prior methods that assume the current state is already aligned with the reward distribution and only guide the subsequent transition, our approach explicitly refines each state in situ, progressively steering it toward the optimal intermediate distribution. Across both text and image domains, we evaluate IterRef on diverse discrete diffusion models and observe consistent improvements in reward-guided generation quality. In particular, IterRef achieves striking gains under low compute budgets, far surpassing prior state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lookahead Unmasking Elicits Accurate Decoding in Diffusion Language Models</title>
<link>https://arxiv.org/abs/2511.05563</link>
<guid>https://arxiv.org/abs/2511.05563</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked Diffusion Models, Lookahead Unmasking, path selection, uncertainty, efficiency

Summary: 
Masked Diffusion Models (MDMs) use iteratively unmasking tokens for language models, but their performance is sensitive to the order of unmasking during inference. This study introduces Lookahead Unmasking (LookUM), which optimizes sampling by selecting paths from all possible unmasking orders without needing an external reward model. The framework consists of a path generator proposing paths and a verifier calculating path uncertainty for selection. LookUM leverages uncertainty to avoid error-prone trajectories and enhance performance across various benchmarks. Notably, LookUM achieves peak performance with only two to three paths, outperforming RL-tuned models and improving both base and post-trained versions of LLaDA. The framework's effectiveness in path selection showcases its efficiency and versatility in improving language modeling tasks. The code for LookUM will be made public. 

<br /><br />Summary: <div>
arXiv:2511.05563v1 Announce Type: new 
Abstract: Masked Diffusion Models (MDMs) as language models generate by iteratively unmasking tokens, yet their performance crucially depends on the inference time order of unmasking. Prevailing heuristics, such as confidence based sampling, are myopic: they optimize locally, fail to leverage extra test-time compute, and let early decoding mistakes cascade. We propose Lookahead Unmasking (LookUM), which addresses these concerns by reformulating sampling as path selection over all possible unmasking orders without the need for an external reward model. Our framework couples (i) a path generator that proposes paths by sampling from pools of unmasking sets with (ii) a verifier that computes the uncertainty of the proposed paths and performs importance sampling to subsequently select the final paths. Empirically, erroneous unmasking measurably inflates sequence level uncertainty, and our method exploits this to avoid error-prone trajectories. We validate our framework across six benchmarks, such as mathematics, planning, and coding, and demonstrate consistent performance improvements. LookUM requires only two to three paths to achieve peak performance, demonstrating remarkably efficient path selection. The consistent improvements on both LLaDA and post-trained LLaDA 1.5 are particularly striking: base LLaDA with LookUM rivals the performance of RL-tuned LLaDA 1.5, while LookUM further enhances LLaDA 1.5 itself showing that uncertainty based verification provides orthogonal benefits to reinforcement learning and underscoring the versatility of our framework. Code will be publicly released.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Sample-Level Framework Motivated by Distributionally Robust Optimization with Variance-Based Radius Assignment for Enhanced Neural Network Generalization Under Distribution Shift</title>
<link>https://arxiv.org/abs/2511.05568</link>
<guid>https://arxiv.org/abs/2511.05568</guid>
<content:encoded><![CDATA[
<div> variance-driven, adaptive, sample-level DRO, high-risk training samples, personalized robustness budget, two-sided KL-divergence-style bounds, linear inner maximization problem

Summary:<br /><br />The paper introduces a variance-driven, adaptive, sample-level Distributionally Robust Optimization (Var-DRO) framework to address distribution shifts and minority subpopulations in deep neural network training. This framework automatically identifies high-risk training samples and assigns personalized robustness budgets based on online loss variance. By constraining the ratio between adversarial and empirical weights for each sample, the method optimizes for worst-case risk while avoiding overly conservative models. To stabilize training, a warm-up phase, linear ramp schedule for global budget caps, and label smoothing are introduced. Var-DRO outperforms ERM and KL-DRO on CIFAR-10-C and Waterbirds datasets, demonstrating improved overall performance and competitive results on the original CIFAR-10 dataset. The proposed framework is unsupervised, easy to implement, theoretically grounded, and computationally efficient. <div>
arXiv:2511.05568v1 Announce Type: new 
Abstract: Distribution shifts and minority subpopulations frequently undermine the reliability of deep neural networks trained using Empirical Risk Minimization (ERM). Distributionally Robust Optimization (DRO) addresses this by optimizing for the worst-case risk within a neighborhood of the training distribution. However, conventional methods depend on a single, global robustness budget, which can lead to overly conservative models or a misallocation of robustness. We propose a variance-driven, adaptive, sample-level DRO (Var-DRO) framework that automatically identifies high-risk training samples and assigns a personalized robustness budget to each based on its online loss variance. Our formulation employs two-sided, KL-divergence-style bounds to constrain the ratio between adversarial and empirical weights for every sample. This results in a linear inner maximization problem over a convex polytope, which admits an efficient water-filling solution. To stabilize training, we introduce a warmup phase and a linear ramp schedule for the global cap on per-sample budgets, complemented by label smoothing for numerical robustness. Evaluated on CIFAR-10-C (corruptions), our method achieves the highest overall mean accuracy compared to ERM and KL-DRO. On Waterbirds, Var-DRO improves overall performance while matching or surpassing KL-DRO. On the original CIFAR-10 dataset, Var-DRO remains competitive, exhibiting the modest trade-off anticipated when prioritizing robustness. The proposed framework is unsupervised (requiring no group labels), straightforward to implement, theoretically sound, and computationally efficient.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-driven jet fuel demand forecasting: A case study of Copenhagen Airport</title>
<link>https://arxiv.org/abs/2511.05569</link>
<guid>https://arxiv.org/abs/2511.05569</guid>
<content:encoded><![CDATA[
<div> forecasting, jet fuel demand, machine learning models, data-driven approaches, predictive capabilities 

Summary:
This research addresses the importance of accurate forecasting of jet fuel demand for optimizing supply chain operations in the aviation market. The study evaluates the performance of data-driven approaches, including traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. The research utilizes data from a major aviation fuel distributor in the Danish market and analyzes three different datasets to provide valuable insights to practitioners. A key challenge is the forecasting horizon of 30 days to optimize sourcing strategies. The study aims to demonstrate the advantages of employing data-driven models in jet fuel demand forecasting and emphasizes the impact of incorporating additional variables in the predictive models. <div>
arXiv:2511.05569v1 Announce Type: new 
Abstract: Accurate forecasting of jet fuel demand is crucial for optimizing supply chain operations in the aviation market. Fuel distributors specifically require precise estimates to avoid inventory shortages or excesses. However, there is a lack of studies that analyze the jet fuel demand forecasting problem using machine learning models. Instead, many industry practitioners rely on deterministic or expertise-based models. In this research, we evaluate the performance of data-driven approaches using a substantial amount of data obtained from a major aviation fuel distributor in the Danish market. Our analysis compares the predictive capabilities of traditional time series models, Prophet, LSTM sequence-to-sequence neural networks, and hybrid models. A key challenge in developing these models is the required forecasting horizon, as fuel demand needs to be predicted for the next 30 days to optimize sourcing strategies. To ensure the reliability of the data-driven approaches and provide valuable insights to practitioners, we analyze three different datasets. The primary objective of this study is to present a comprehensive case study on jet fuel demand forecasting, demonstrating the advantages of employing data-driven models and highlighting the impact of incorporating additional variables in the predictive models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning Vision-Language Models for Multimodal Polymer Property Prediction</title>
<link>https://arxiv.org/abs/2511.05577</link>
<guid>https://arxiv.org/abs/2511.05577</guid>
<content:encoded><![CDATA[
<div> dataset, fine-tune, VLM, polymer property prediction, multimodal learning <br />
<br />Summary: 
This study focuses on the development of Vision-Language Models (VLMs) for materials science, specifically in the domain of polymer property prediction. The researchers introduce a new multimodal polymer dataset and propose the use of instruction-tuning pairs to fine-tune VLMs. By leveraging multimodal data, the fine-tuned models, implemented using LoRA, show improved performance compared to unimodal and baseline approaches. The study highlights the benefits of multimodal learning in enhancing prediction accuracy and reducing the need for separate models for different properties. This approach not only enhances prediction performance but also reduces deployment and maintenance costs, making it a promising strategy for scalable and efficient polymer property prediction in materials science.  <br /> <div>
arXiv:2511.05577v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have shown strong performance in tasks like visual question answering and multimodal text generation, but their effectiveness in scientific domains such as materials science remains limited. While some machine learning methods have addressed specific challenges in this field, there is still a lack of foundation models designed for broad tasks like polymer property prediction using multimodal data. In this work, we present a multimodal polymer dataset to fine-tune VLMs through instruction-tuning pairs and assess the impact of multimodality on prediction performance. Our fine-tuned models, using LoRA, outperform unimodal and baseline approaches, demonstrating the benefits of multimodal learning. Additionally, this approach reduces the need to train separate models for different properties, lowering deployment and maintenance costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distillation-Accelerated Uncertainty Modeling for Multi-Objective RTA Interception</title>
<link>https://arxiv.org/abs/2511.05582</link>
<guid>https://arxiv.org/abs/2511.05582</guid>
<content:encoded><![CDATA[
<div> Keywords: Real-Time Auction, Traffic Quality Estimation, Uncertainty Modeling, Multi-Objective Learning, Knowledge Distillation

Summary:
DAUM is proposed as a framework for Real-Time Auction Interception to address challenges in traffic quality estimation and uncertainty modeling. It integrates multi-objective learning with uncertainty modeling to provide accurate traffic quality predictions and reliable confidence estimates. The application of knowledge distillation reduces computational overhead while maintaining predictive accuracy and the benefits of uncertainty estimation. Experiments on the JD advertisement dataset show that DAUM consistently improves predictive performance, with the distilled model achieving a tenfold increase in inference speed. <div>
arXiv:2511.05582v1 Announce Type: new 
Abstract: Real-Time Auction (RTA) Interception aims to filter out invalid or irrelevant traffic to enhance the integrity and reliability of downstream data. However, two key challenges remain: (i) the need for accurate estimation of traffic quality together with sufficiently high confidence in the model's predictions, typically addressed through uncertainty modeling, and (ii) the efficiency bottlenecks that such uncertainty modeling introduces in real-time applications due to repeated inference. To address these challenges, we propose DAUM, a joint modeling framework that integrates multi-objective learning with uncertainty modeling, yielding both traffic quality predictions and reliable confidence estimates. Building on DAUM, we further apply knowledge distillation to reduce the computational overhead of uncertainty modeling, while largely preserving predictive accuracy and retaining the benefits of uncertainty estimation. Experiments on the JD advertisement dataset demonstrate that DAUM consistently improves predictive performance, with the distilled model delivering a tenfold increase in inference speed.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-induced NTK: Bridging Over-parameterized Neural Networks and Deep Neural Kernels</title>
<link>https://arxiv.org/abs/2511.05585</link>
<guid>https://arxiv.org/abs/2511.05585</guid>
<content:encoded><![CDATA[
<div> deep learning, neural networks, kernel methods, representation learning, scaling law

Summary:
The article introduces deep neural kernels as a framework to interpret over-parameterized neural networks. Neural tangent kernels (NTKs) have connected infinitely wide neural networks to nonparametric Bayesian inference. However, the existing NTK paradigm has focused on the infinite-width regime, neglecting network depth. The proposed depth-induced NTK kernel, based on a shortcut-related architecture, converges to a Gaussian process as network depth approaches infinity. The kernel's training invariance and spectrum properties stabilize kernel dynamics and prevent degeneration. Experimental results demonstrate the effectiveness of the proposed method. This research significantly expands neural kernel theory, enhancing understanding of deep learning and the scaling law.<br /><br />Summary: <div>
arXiv:2511.05585v1 Announce Type: new 
Abstract: While deep learning has achieved remarkable success across a wide range of applications, its theoretical understanding of representation learning remains limited. Deep neural kernels provide a principled framework to interpret over-parameterized neural networks by mapping hierarchical feature transformations into kernel spaces, thereby combining the expressive power of deep architectures with the analytical tractability of kernel methods. Recent advances, particularly neural tangent kernels (NTKs) derived by gradient inner products, have established connections between infinitely wide neural networks and nonparametric Bayesian inference. However, the existing NTK paradigm has been predominantly confined to the infinite-width regime, while overlooking the representational role of network depth. To address this gap, we propose a depth-induced NTK kernel based on a shortcut-related architecture, which converges to a Gaussian process as the network depth approaches infinity. We theoretically analyze the training invariance and spectrum properties of the proposed kernel, which stabilizes the kernel dynamics and mitigates degeneration. Experimental results further underscore the effectiveness of our proposed method. Our findings significantly extend the existing landscape of the neural kernel theory and provide an in-depth understanding of deep learning and the scaling law.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prompting Neural-Guided Equation Discovery Based on Residuals</title>
<link>https://arxiv.org/abs/2511.05586</link>
<guid>https://arxiv.org/abs/2511.05586</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural-guided equation discovery, Residuals, Syntax tree, Equation improvement, Mathematical operations 

Summary: 
Residuals for Equation Discovery (RED) is introduced as a post-processing method to enhance given equations by focusing on their residuals. By utilizing node-based calculation rules on the syntax tree of the initial equation, RED computes residuals for each subequation. These residuals are then used as new target variables in the data set to generate a new prompt for equation suggestion. If the resulting equation from the new prompt outperforms the original on a validation set, it replaces the original subequation. RED is compatible with any equation discovery system, efficient in computation, and easily adaptable for additional mathematical operations. Experimental results on 53 equations from the Feynman benchmark demonstrate that RED effectively improves both neural-guided and classical genetic programming equation discovery systems.<br /><br />Summary: <div>
arXiv:2511.05586v1 Announce Type: new 
Abstract: Neural-guided equation discovery systems use a data set as prompt and predict an equation that describes the data set without extensive search. However, if the equation does not meet the user's expectations, there are few options for getting other equation suggestions without intensive work with the system. To fill this gap, we propose Residuals for Equation Discovery (RED), a post-processing method that improves a given equation in a targeted manner, based on its residuals. By parsing the initial equation to a syntax tree, we can use node-based calculation rules to compute the residual for each subequation of the initial equation. It is then possible to use this residual as new target variable in the original data set and generate a new prompt. If, with the new prompt, the equation discovery system suggests a subequation better than the old subequation on a validation set, we replace the latter by the former. RED is usable with any equation discovery system, is fast to calculate, and is easy to extend for new mathematical operations. In experiments on 53 equations from the Feynman benchmark, we show that it not only helps to improve all tested neural-guided systems, but also all tested classical genetic programming systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoPRIS: Efficient and Stable Reinforcement Learning via Concurrency-Controlled Partial Rollout with Importance Sampling</title>
<link>https://arxiv.org/abs/2511.05589</link>
<guid>https://arxiv.org/abs/2511.05589</guid>
<content:encoded><![CDATA[
<div> RL, language models, Concurrency-Controlled Partial Rollout, Importance Sampling, off-policy<br />
<br />
Summary:<br />
The article introduces CoPRIS, a reinforcement learning system for enhancing large language models that addresses inefficiencies in fully synchronous RL systems. CoPRIS maintains a fixed number of concurrent rollouts, terminates early to avoid long-tail inefficiencies, and reuses unfinished trajectories. It also introduces Cross-stage Importance Sampling Correction to handle off-policy trajectories by incorporating buffered log probabilities from the previous policy. Experiments on mathematical reasoning benchmarks demonstrate that CoPRIS achieves up to 1.94x faster training without compromising performance. The code for CoPRIS is available on GitHub for further exploration and implementation. <div>
arXiv:2511.05589v1 Announce Type: new 
Abstract: Reinforcement learning (RL) post-training has become a trending paradigm for enhancing the capabilities of large language models (LLMs). Most existing RL systems for LLMs operate in a fully synchronous manner, where training must wait for the rollout of an entire batch to complete. This design leads to severe inefficiencies, as extremely long trajectories can stall the entire rollout process and leave many GPUs idle. To address this issue, we propose Concurrency- Controlled Partial Rollout with Importance Sampling (CoPRIS), which mitigates long-tail inefficiencies by maintaining a fixed number of concurrent rollouts, early-terminating once sufficient samples are collected, and reusing unfinished trajectories in subsequent rollouts. To mitigate the impact of off-policy trajectories, we introduce Cross-stage Importance Sampling Correction, which concatenates buffered log probabilities from the previous policy with those recomputed under the current policy for importance sampling correction. Experiments on challenging mathematical reasoning benchmarks show that CoPRIS achieves up to 1.94x faster training while maintaining comparable or superior performance to synchronous RL systems. The code of CoPRIS is available at https://github.com/777pomingzi/CoPRIS.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSparQ: Adaptive Sparse Quantization with Error Feedback for Robust &amp; Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2511.05591</link>
<guid>https://arxiv.org/abs/2511.05591</guid>
<content:encoded><![CDATA[
<div> compression, federated learning, communication overhead, model accuracy, convergence robustness

Summary:
FedSparQ introduces a lightweight compression framework for Federated Learning (FL) that dynamically sparsifies gradients, quantizes entries, and integrates error feedback residuals. It reduces communication overhead by 90% compared to FedAvg, while preserving or enhancing model accuracy by up to 6%. The approach adapts seamlessly to data distributions, requires no manual tuning, and is agnostic to model architecture. Empirical evaluations on vision benchmarks show significant improvements in model accuracy and convergence robustness compared to other baselines. FedSparQ is a practical solution for bandwidth-constrained federated deployments, with potential for future extensions in adaptive precision and privacy-preserving protocols. <br /><br />Summary: <div>
arXiv:2511.05591v1 Announce Type: new 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients while preserving data privacy by keeping raw data local. However, FL suffers from significant communication overhead due to the frequent exchange of high-dimensional model updates over constrained networks. In this paper, we present FedSparQ, a lightweight compression framework that dynamically sparsifies the gradient of each client through an adaptive threshold, applies half-precision quanti- zation to retained entries and integrates residuals from error feedback to prevent loss of information. FedSparQ requires no manual tuning of sparsity rates or quantization schedules, adapts seamlessly to both homogeneous and heterogeneous data distributions, and is agnostic to model architecture. Through extensive empirical evaluation on vision benchmarks under independent and identically distributed (IID) and non-IID data, we show that FedSparQ substantially reduces communication overhead (reducing by 90% of bytes sent compared to FedAvg) while preserving or improving model accuracy (improving by 6% compared to FedAvg non-compressed solution or to state-of-the- art compression models) and enhancing convergence robustness (by 50%, compared to the other baselines). Our approach provides a practical, easy-to-deploy solution for bandwidth- constrained federated deployments and lays the groundwork for future extensions in adaptive precision and privacy-preserving protocols.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAVER: Generative Graph Vocabularies for Robust Graph Foundation Models Fine-tuning</title>
<link>https://arxiv.org/abs/2511.05592</link>
<guid>https://arxiv.org/abs/2511.05592</guid>
<content:encoded><![CDATA[
<div> Generative augmentations, ego-graph disentanglement, graphon-based generative experts, MoE-CoE network, few-shot node and graph classification <br />
Summary:<br />
1. The paper introduces GRAVER, a framework for robust and efficient fine-tuning of Graph Foundation Models (GFMs) through generative augmentations.
2. GRAVER identifies transferable subgraph patterns through ego-graph disentanglement and constructs graph vocabularies based on ego-graph similarity.
3. It utilizes a universal task template and graphon-based generative experts for effective pre-training across diverse domains.
4. The framework graces support samples with in-context vocabularies to facilitate prompt fine-tuning, with knowledge routing through a MoE-CoE network.
5. Extensive experiments demonstrate GRAVER's superiority in effectiveness, robustness, and efficiency on few-shot node and graph classification tasks compared to 15 state-of-the-art baselines. <br /> <div>
arXiv:2511.05592v1 Announce Type: new 
Abstract: Inspired by the remarkable success of foundation models in language and vision, Graph Foundation Models (GFMs) hold significant promise for broad applicability across diverse graph tasks and domains. However, existing GFMs struggle with unstable few-shot fine-tuning, where both performance and adaptation efficiency exhibit significant fluctuations caused by the randomness in the support sample selection and structural discrepancies between the pre-trained and target graphs. How to fine-tune GFMs robustly and efficiently to enable trustworthy knowledge transfer across domains and tasks is the major challenge. In this paper, we propose GRAVER, a novel Generative gRAph VocabulariEs for Robust GFM fine-tuning framework that tackles the aforementioned instability via generative augmentations. Specifically, to identify transferable units, we analyze and extract key class-specific subgraph patterns by ego-graph disentanglement and validate their transferability both theoretically and empirically. To enable effective pre-training across diverse domains, we leverage a universal task template based on ego-graph similarity and construct graph vocabularies via graphon-based generative experts. To facilitate robust and efficient prompt fine-tuning, we grave the support samples with in-context vocabularies, where the lightweight MoE-CoE network attentively routes knowledge from source domains. Extensive experiments demonstrate the superiority of GRAVER over effectiveness, robustness, and efficiency on downstream few-shot node and graph classification tasks compared with 15 state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Projection onto Historical Descent Directions for Communication-Efficient Federated Learning</title>
<link>https://arxiv.org/abs/2511.05593</link>
<guid>https://arxiv.org/abs/2511.05593</guid>
<content:encoded><![CDATA[
<div> compressors, federated learning, ProjFL, ProjFL+EF, communication efficiency

Summary:
ProjFL and ProjFL+EF are algorithms introduced in this work for federated learning to improve communication efficiency. ProjFL works with unbiased compressors, while ProjFL+EF is tailored for biased compressors with an Error Feedback mechanism. Both algorithms project local gradients onto a shared client-server subspace using historical descent directions to exchange information efficiently with minimal communication overhead. Convergence guarantees are established for strongly convex, convex, and non-convex settings. Empirical evaluations on FL classification benchmarks with deep neural networks show that both methods achieve comparable accuracy to existing baselines while significantly reducing communication costs. <div>
arXiv:2511.05593v1 Announce Type: new 
Abstract: Federated Learning (FL) enables decentralized model training across multiple clients while optionally preserving data privacy. However, communication efficiency remains a critical bottleneck, particularly for large-scale models. In this work, we introduce two complementary algorithms: ProjFL, designed for unbiased compressors, and ProjFL+EF, tailored for biased compressors through an Error Feedback mechanism. Both methods rely on projecting local gradients onto a shared client-server subspace spanned by historical descent directions, enabling efficient information exchange with minimal communication overhead. We establish convergence guarantees for both algorithms under strongly convex, convex, and non-convex settings. Empirical evaluations on standard FL classification benchmarks with deep neural networks show that ProjFL and ProjFL+EF achieve accuracy comparable to existing baselines while substantially reducing communication costs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Predictive Maintenance in Intelligent Manufacturing: An Integrated FNO-DAE-GNN-PPO MDP Framework</title>
<link>https://arxiv.org/abs/2511.05594</link>
<guid>https://arxiv.org/abs/2511.05594</guid>
<content:encoded><![CDATA[
<div> Keywords: smart manufacturing, predictive maintenance, Markov Decision Process, deep learning, industrial potential

Summary: 
The paper introduces a novel framework for predictive maintenance in smart manufacturing systems. It combines advanced soft computing techniques including Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) to address the challenges of maintenance in complex systems. The framework uses FNOs for capturing temporal patterns, DAEs for noise-resistant state embedding, and GNNs for modeling inter-device dependencies. PPO is employed for stable optimization of long-term maintenance strategies. Experimental results show the framework outperforms deep learning baseline models with cost reductions of up to 13% and demonstrates strong convergence and inter-module synergy. The approach has significant potential in the industry to reduce downtime and operating expenses through data-driven maintenance strategies.

<br /><br />Summary: <div>
arXiv:2511.05594v1 Announce Type: new 
Abstract: In the era of smart manufacturing, predictive maintenance (PdM) plays a pivotal role in improving equipment reliability and reducing operating costs. In this paper, we propose a novel Markov Decision Process (MDP) framework that integrates advanced soft computing techniques - Fourier Neural Operator (FNO), Denoising Autoencoder (DAE), Graph Neural Network (GNN), and Proximal Policy Optimisation (PPO) - to address the multidimensional challenges of predictive maintenance in complex manufacturing systems. Specifically, the proposed framework innovatively combines the powerful frequency-domain representation capability of FNOs to capture high-dimensional temporal patterns; DAEs to achieve robust, noise-resistant latent state embedding from complex non-Gaussian sensor data; and GNNs to accurately represent inter-device dependencies for coordinated system-wide maintenance decisions. Furthermore, by exploiting PPO, the framework ensures stable and efficient optimisation of long-term maintenance strategies to effectively handle uncertainty and non-stationary dynamics. Experimental validation demonstrates that the approach significantly outperforms multiple deep learning baseline models with up to 13% cost reduction, as well as strong convergence and inter-module synergy. The framework has considerable industrial potential to effectively reduce downtime and operating expenses through data-driven strategies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowNet: Modeling Dynamic Spatio-Temporal Systems via Flow Propagation</title>
<link>https://arxiv.org/abs/2511.05595</link>
<guid>https://arxiv.org/abs/2511.05595</guid>
<content:encoded><![CDATA[
<div> FlowNet, dynamic node couplings, flow transfers, conservation principles, context-aware propagation
Summary:
FlowNet is a novel architecture based on the Spatio-Temporal Flow paradigm that models dynamic node couplings through flow transfers governed by conservation principles. It uses flow tokens to simulate source-to-destination transfers, ensuring state redistribution aligns with conservation laws. The architecture includes Flow Allocation Modules for efficient information transfer and an Adaptive Spatial Masking module to adjust the interaction radius. The cascaded architecture enhances scalability and nonlinear representation capacity. In experiments on real-world systems, FlowNet outperforms existing approaches on seven metrics, demonstrating its efficiency and physical interpretability. This methodology provides a principled approach for modeling complex systems through spatio-temporal flow interactions. 
Summary:  <div>
arXiv:2511.05595v1 Announce Type: new 
Abstract: Accurately modeling complex dynamic spatio-temporal systems requires capturing flow-mediated interdependencies and context-sensitive interaction dynamics. Existing methods, predominantly graph-based or attention-driven, rely on similarity-driven connectivity assumptions, neglecting asymmetric flow exchanges that govern system evolution. We propose Spatio-Temporal Flow, a physics-inspired paradigm that explicitly models dynamic node couplings through quantifiable flow transfers governed by conservation principles. Building on this, we design FlowNet, a novel architecture leveraging flow tokens as information carriers to simulate source-to-destination transfers via Flow Allocation Modules, ensuring state redistribution aligns with conservation laws. FlowNet dynamically adjusts the interaction radius through an Adaptive Spatial Masking module, suppressing irrelevant noise while enabling context-aware propagation. A cascaded architecture enhances scalability and nonlinear representation capacity. Experiments demonstrate that FlowNet significantly outperforms existing state-of-the-art approaches on seven metrics in the modeling of three real-world systems, validating its efficiency and physical interpretability. We establish a principled methodology for modeling complex systems through spatio-temporal flow interactions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoHood3D: A Multi-Modal Benchmark for Automotive Hood Design and Fluid-Structure Interaction</title>
<link>https://arxiv.org/abs/2511.05596</link>
<guid>https://arxiv.org/abs/2511.05596</guid>
<content:encoded><![CDATA[
<div> Dataset, automotive hoods, machine learning, multiphysics, fluid-solid coupling 
Summary: 
AutoHood3D presents a new high-fidelity multi-modal dataset consisting of 16000+ geometric variants of automotive hoods for machine learning applications in engineering component design and process optimization. The dataset focuses on hood deformation due to fluid entrapment and inertial loading during rotary-dip painting, modeled using a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA). It includes time-resolved physical fields, STL meshes, and natural language prompts for text-to-geometry synthesis. The dataset addresses limitations of existing datasets by offering 3D cases, diverse geometric variations, and multi-modal annotations. The study validates the numerical methodology, establishes neural architecture baselines, and highlights systematic errors in displacement and force predictions. New approaches and multiphysics loss functions are recommended to improve model training with fluid-solid coupling. AutoHood3D facilitates physics-aware ML development, accelerates generative-design iteration, and supports the creation of new fluid-structure interaction benchmarks. URLs for dataset and code access are provided in the appendix. 
Summary: <div>
arXiv:2511.05596v1 Announce Type: new 
Abstract: This study presents a new high-fidelity multi-modal dataset containing 16000+ geometric variants of automotive hoods useful for machine learning (ML) applications such as engineering component design and process optimization, and multiphysics system surrogates. The dataset is centered on a practical multiphysics problem-hood deformation from fluid entrapment and inertial loading during rotary-dip painting. Each hood is numerically modeled with a coupled Large-Eddy Simulation (LES)-Finite Element Analysis (FEA), using 1.2M cells in total to ensure spatial and temporal accuracy. The dataset provides time-resolved physical fields, along with STL meshes and structured natural language prompts for text-to-geometry synthesis. Existing datasets are either confined to 2D cases, exhibit limited geometric variations, or lack the multi-modal annotations and data structures - shortcomings we address with AutoHood3D. We validate our numerical methodology, establish quantitative baselines across five neural architectures, and demonstrate systematic surrogate errors in displacement and force predictions. These findings motivate the design of novel approaches and multiphysics loss functions that enforce fluid-solid coupling during model training. By providing fully reproducible workflows, AutoHood3D enables physics-aware ML development, accelerates generative-design iteration, and facilitates the creation of new FSI benchmarks. Dataset and code URLs in Appendix.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FiCABU: A Fisher-Based, Context-Adaptive Machine Unlearning Processor for Edge AI</title>
<link>https://arxiv.org/abs/2511.05605</link>
<guid>https://arxiv.org/abs/2511.05605</guid>
<content:encoded><![CDATA[
<div> Machine unlearning, edge AI processors, FiCABU, privacy regulations, right to be forgotten  
Summary:  
FiCABU, a software-hardware co-design, enables efficient machine unlearning at the edge for privacy compliance. It combines Context-Adaptive Unlearning and Balanced Dampening to achieve random-guess forget accuracy while maintaining retain accuracy. The approach starts edits from back-end layers and adjusts dampening strength by depth, reducing computation by up to 87.52% for ResNet-18 and 71.03% for ViT models. Validated on FPGA and synthesized in 45 nm, FiCABU surpasses the retraining-free SSD baseline in retain preservation and energy efficiency on an INT8 hardware prototype. This innovative method demonstrates that depth-aware unlearning can be practical and efficient for resource-constrained edge AI devices.  
<br /><br />Summary: <div>
arXiv:2511.05605v1 Announce Type: new 
Abstract: Machine unlearning, driven by privacy regulations and the "right to be forgotten", is increasingly needed at the edge, yet server-centric or retraining-heavy methods are impractical under tight computation and energy budgets. We present FiCABU (Fisher-based Context-Adaptive Balanced Unlearning), a software-hardware co-design that brings unlearning to edge AI processors. FiCABU combines (i) Context-Adaptive Unlearning, which begins edits from back-end layers and halts once the target forgetting is reached, with (ii) Balanced Dampening, which scales dampening strength by depth to preserve retain accuracy. These methods are realized in a full RTL design of a RISC-V edge AI processor that integrates two lightweight IPs for Fisher estimation and dampening into a GEMM-centric streaming pipeline, validated on an FPGA prototype and synthesized in 45 nm for power analysis. Across CIFAR-20 and PinsFaceRecognition with ResNet-18 and ViT, FiCABU achieves random-guess forget accuracy while matching the retraining-free Selective Synaptic Dampening (SSD) baseline on retain accuracy, reducing computation by up to 87.52 percent (ResNet-18) and 71.03 percent (ViT). On the INT8 hardware prototype, FiCABU further improves retain preservation and reduces energy to 6.48 percent (CIFAR-20) and 0.13 percent (PinsFaceRecognition) of the SSD baseline. In sum, FiCABU demonstrates that back-end-first, depth-aware unlearning can be made both practical and efficient for resource-constrained edge AI devices.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Prediction-Driven Adaptive Sampling for Digital Twins of Water Distribution Networks</title>
<link>https://arxiv.org/abs/2511.05610</link>
<guid>https://arxiv.org/abs/2511.05610</guid>
<content:encoded><![CDATA[
<div> Adaptive Framework, LSTM Forecasting, Conformal Prediction, Water Distribution Networks, Digital Twins
Summary:
An adaptive framework combining LSTM forecasting and Conformal Prediction is proposed for accurate state estimation in Water Distribution Networks (WDNs) using Digital Twins. This framework aims to optimize sensor placement by focusing on nodes with the highest uncertainty, rather than using uniform sampling which can be wasteful. The use of Marginal Conformal Prediction allows for low computational cost, making it suitable for real-time implementation. Experimental results on three different network datasets demonstrate a significant reduction in demand error compared to uniform sampling, achieving 33-34% lower error rates at 40% coverage. The adaptive framework also maintains high empirical coverage (89.4-90.2%) with only a small increase in computational cost (5-10%). Overall, the proposed approach shows promising results in improving state estimation accuracy in WDNs while optimizing resource utilization through targeted sensor placement. 
<br /><br />Summary: <div>
arXiv:2511.05610v1 Announce Type: new 
Abstract: Digital Twins (DTs) for Water Distribution Networks (WDNs) require accurate state estimation with limited sensors. Uniform sampling often wastes resources across nodes with different uncertainty. We propose an adaptive framework combining LSTM forecasting and Conformal Prediction (CP) to estimate node-wise uncertainty and focus sensing on the most uncertain points. Marginal CP is used for its low computational cost, suitable for real-time DTs. Experiments on Hanoi, Net3, and CTOWN show 33-34% lower demand error than uniform sampling at 40% coverage and maintain 89.4-90.2% empirical coverage with only 5-10% extra computation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An MLCommons Scientific Benchmarks Ontology</title>
<link>https://arxiv.org/abs/2511.05614</link>
<guid>https://arxiv.org/abs/2511.05614</guid>
<content:encoded><![CDATA[
<div> physics, chemistry, materials science, biology, climate science<br />
Summary:<br />
This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort within the MLCommons ecosystem. The ontology covers diverse domains such as physics, chemistry, materials science, biology, and climate science. It consolidates various benchmarks and frameworks into a single taxonomy, allowing for standardized and cross-domain benchmarking in scientific machine learning. New benchmarks can be added through an open submission workflow and evaluated against a six-category rating rubric. The architecture is extensible, supporting future scientific and AI/ML motifs. The effort aims to provide a scalable foundation for reproducible, high-quality benchmarking in scientific machine learning. A companion webpage has been developed to track the evolution of the project: https://mlcommons-science.github.io/benchmark/ <div>
arXiv:2511.05614v1 Announce Type: new 
Abstract: Scientific machine learning research spans diverse domains and data modalities, yet existing benchmark efforts remain siloed and lack standardization. This makes novel and transformative applications of machine learning to critical scientific use-cases more fragmented and less clear in pathways to impact. This paper introduces an ontology for scientific benchmarking developed through a unified, community-driven effort that extends the MLCommons ecosystem to cover physics, chemistry, materials science, biology, climate science, and more. Building on prior initiatives such as XAI-BENCH, FastML Science Benchmarks, PDEBench, and the SciMLBench framework, our effort consolidates a large set of disparate benchmarks and frameworks into a single taxonomy of scientific, application, and system-level benchmarks. New benchmarks can be added through an open submission workflow coordinated by the MLCommons Science Working Group and evaluated against a six-category rating rubric that promotes and identifies high-quality benchmarks, enabling stakeholders to select benchmarks that meet their specific needs. The architecture is extensible, supporting future scientific and AI/ML motifs, and we discuss methods for identifying emerging computing patterns for unique scientific workloads. The MLCommons Science Benchmarks Ontology provides a standardized, scalable foundation for reproducible, cross-domain benchmarking in scientific machine learning. A companion webpage for this work has also been developed as the effort evolves: https://mlcommons-science.github.io/benchmark/
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>wa-hls4ml: A Benchmark and Surrogate Models for hls4ml Resource and Latency Estimation</title>
<link>https://arxiv.org/abs/2511.05615</link>
<guid>https://arxiv.org/abs/2511.05615</guid>
<content:encoded><![CDATA[
<div> benchmark, ML accelerator, resource estimation, latency estimation, surrogate model 

Summary:
The article discusses the increasing integration of machine learning (ML) into hardware for real-time scientific applications and the challenges it faces. Advanced toolchains have reduced design iteration time but have also highlighted new obstacles like hardware synthesis bottlenecks. To address these issues, efforts are being made to develop ML-based surrogate models for estimating resource usage of ML accelerator architectures. The article introduces wa-hls4ml, a benchmark tool for ML accelerator resource and latency estimation, along with a dataset of over 680,000 neural networks synthesized using hls4ml on Xilinx FPGAs. It evaluates resource and latency predictors on various ML model architectures and introduces GNN- and transformer-based surrogate models for predicting accelerator performance. The models show promise, accurately predicting latency and resources close to the synthesized values on a test dataset. <div>
arXiv:2511.05615v1 Announce Type: new 
Abstract: As machine learning (ML) is increasingly implemented in hardware to address real-time challenges in scientific applications, the development of advanced toolchains has significantly reduced the time required to iterate on various designs. These advancements have solved major obstacles, but also exposed new challenges. For example, processes that were not previously considered bottlenecks, such as hardware synthesis, are becoming limiting factors in the rapid iteration of designs. To mitigate these emerging constraints, multiple efforts have been undertaken to develop an ML-based surrogate model that estimates resource usage of ML accelerator architectures. We introduce wa-hls4ml, a benchmark for ML accelerator resource and latency estimation, and its corresponding initial dataset of over 680,000 fully connected and convolutional neural networks, all synthesized using hls4ml and targeting Xilinx FPGAs. The benchmark evaluates the performance of resource and latency predictors against several common ML model architectures, primarily originating from scientific domains, as exemplar models, and the average performance across a subset of the dataset. Additionally, we introduce GNN- and transformer-based surrogate models that predict latency and resources for ML accelerators. We present the architecture and performance of the models and find that the models generally predict latency and resources for the 75% percentile within several percent of the synthesized resources on the synthetic test dataset.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Frequency Matters: When Time Series Foundation Models Fail Under Spectral Shift</title>
<link>https://arxiv.org/abs/2511.05619</link>
<guid>https://arxiv.org/abs/2511.05619</guid>
<content:encoded><![CDATA[
<div> TSFMs, industrial settings, spectral shift, player engagement prediction, frequency awareness <br />
<br />
Time series foundation models (TSFMs) have shown strong performance on standard benchmarks, leading to comparisons with the "BERT moment" for time series analysis. However, their efficacy in real-world industrial applications is uncertain. Research indicates that TSFMs struggle to generalize due to spectral shift, where frequency components in downstream tasks differ from those during pretraining. An investigation in mobile gaming highlighted TSFMs' underperformance compared to domain-adapted models. Controlled synthetic experiments revealed systematic performance degradation under spectral mismatch, emphasizing the importance of frequency awareness in robust TSFM deployment. These findings suggest the need for new pretraining and evaluation approaches that consider spectral diversity explicitly. <br /><br />Summary: <div>
arXiv:2511.05619v1 Announce Type: new 
Abstract: Time series foundation models (TSFMs) have shown strong results on public benchmarks, prompting comparisons to a "BERT moment" for time series. Their effectiveness in industrial settings, however, remains uncertain. We examine why TSFMs often struggle to generalize and highlight spectral shift (a mismatch between the dominant frequency components in downstream tasks and those represented during pretraining) as a key factor. We present evidence from an industrial-scale player engagement prediction task in mobile gaming, where TSFMs underperform domain-adapted baselines. To isolate the mechanism, we design controlled synthetic experiments contrasting signals with seen versus unseen frequency bands, observing systematic degradation under spectral mismatch. These findings position frequency awareness as critical for robust TSFM deployment and motivate new pretraining and evaluation protocols that explicitly account for spectral diversity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fooling Algorithms in Non-Stationary Bandits using Belief Inertia</title>
<link>https://arxiv.org/abs/2511.05620</link>
<guid>https://arxiv.org/abs/2511.05620</guid>
<content:encoded><![CDATA[
<div> Belief Inertia, Piecewise Stationary Bandits, Regret, Lower Bounds, Non-Stationarity <br />
Summary:
The study examines worst-case regret in piecewise stationary multi-armed bandits. Traditional lower bounds in time-varying settings rely on infrequent sampling, where lack of exploration leads to high regret. The paper introduces a novel approach based on belief inertia, demonstrating how algorithms' historical reward averages create momentum that resists new evidence post-change. This inertia can mislead classical algorithms like Explore Then Commit, epsilon-greedy, and UCB, causing substantial regret growth despite parameter tuning. Even with periodic restarts to handle non-stationarity, worst-case regret remains linear. The findings suggest that leveraging belief inertia is an effective method for deriving precise lower bounds in non-stationary bandit settings. <br /> <div>
arXiv:2511.05620v1 Announce Type: new 
Abstract: We study the problem of worst case regret in piecewise stationary multi armed bandits. While the minimax theory for stationary bandits is well established, understanding analogous limits in time-varying settings is challenging. Existing lower bounds rely on what we refer to as infrequent sampling arguments, where long intervals without exploration allow adversarial reward changes that induce large regret.
  In this paper, we introduce a fundamentally different approach based on a belief inertia argument. Our analysis captures how an algorithm's empirical beliefs, encoded through historical reward averages, create momentum that resists new evidence after a change. We show how this inertia can be exploited to construct adversarial instances that mislead classical algorithms such as Explore Then Commit, epsilon greedy, and UCB, causing them to suffer regret that grows linearly with T and with a substantial constant factor, regardless of how their parameters are tuned, even with a single change point.
  We extend the analysis to algorithms that periodically restart to handle non stationarity and prove that, even then, the worst case regret remains linear in T. Our results indicate that utilizing belief inertia can be a powerful method for deriving sharp lower bounds in non stationary bandits.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling the Training Dynamics of ReLU Networks through a Linear Lens</title>
<link>https://arxiv.org/abs/2511.05628</link>
<guid>https://arxiv.org/abs/2511.05628</guid>
<content:encoded><![CDATA[
<div> learning mechanisms, ReLU networks, effective weights, representation learning, decision boundaries
<br />
<br />
Summary: 
This study introduces a novel analytical framework that simplifies multi-layer ReLU networks into a single-layer linear model with input-dependent effective weights. These effective weights, denoted as $W_{\text{eff}}(x)$, adjust based on the activation pattern of ReLU units for each input sample. As training progresses, the effective weights for samples of the same class converge, while those from different classes diverge. Tracking the trajectories of these sample-specific effective weights offers insights into the formation of class-specific decision boundaries and the development of semantic representations within the network. This approach provides a clearer understanding of the internal workings of deep neural networks and sheds light on the fundamental principles of representation learning in complex, high-dimensional systems. <div>
arXiv:2511.05628v1 Announce Type: new 
Abstract: Deep neural networks, particularly those employing Rectified Linear Units (ReLU), are often perceived as complex, high-dimensional, non-linear systems. This complexity poses a significant challenge to understanding their internal learning mechanisms. In this work, we propose a novel analytical framework that recasts a multi-layer ReLU network into an equivalent single-layer linear model with input-dependent "effective weights". For any given input sample, the activation pattern of ReLU units creates a unique computational path, effectively zeroing out a subset of weights in the network. By composing the active weights across all layers, we can derive an effective weight matrix, $W_{\text{eff}}(x)$, that maps the input directly to the output for that specific sample. We posit that the evolution of these effective weights reveals fundamental principles of representation learning. Our work demonstrates that as training progresses, the effective weights corresponding to samples from the same class converge, while those from different classes diverge. By tracking the trajectories of these sample-wise effective weights, we provide a new lens through which to interpret the formation of class-specific decision boundaries and the emergence of semantic representations within the network.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSTODE: Ocean-Atmosphere Physics-Informed Neural ODEs for Sea Surface Temperature Prediction</title>
<link>https://arxiv.org/abs/2511.05629</link>
<guid>https://arxiv.org/abs/2511.05629</guid>
<content:encoded><![CDATA[
<div> data-driven models, SST prediction, physics-informed neural networks, ocean-atmosphere dynamics, Neural Ordinary Differential Equations (Neural ODEs)<br />
Summary:<br />
Sea Surface Temperature (SST) prediction is crucial for understanding ocean dynamics and interactions. Existing data-driven models lack interpretability and overlook key physical processes. SSTODE, a physics-informed Neural ODE framework, addresses these challenges by incorporating fluid transport principles to model spatiotemporal dynamics. The model derives ODEs for ocean dynamics and integrates external SST drivers, providing insights into SST evolution. SSTODE outperforms existing models in global and regional SST forecasting benchmarks, highlighting its interpretability and physical consistency. The model visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. SSTODE's innovative approach has the potential to enhance understanding of SST dynamics and improve ocean-atmosphere predictions.
 <div>
arXiv:2511.05629v1 Announce Type: new 
Abstract: Sea Surface Temperature (SST) is crucial for understanding upper-ocean thermal dynamics and ocean-atmosphere interactions, which have profound economic and social impacts. While data-driven models show promise in SST prediction, their black-box nature often limits interpretability and overlooks key physical processes. Recently, physics-informed neural networks have been gaining momentum but struggle with complex ocean-atmosphere dynamics due to 1) inadequate characterization of seawater movement (e.g., coastal upwelling) and 2) insufficient integration of external SST drivers (e.g., turbulent heat fluxes). To address these challenges, we propose SSTODE, a physics-informed Neural Ordinary Differential Equations (Neural ODEs) framework for SST prediction. First, we derive ODEs from fluid transport principles, incorporating both advection and diffusion to model ocean spatiotemporal dynamics. Through variational optimization, we recover a latent velocity field that explicitly governs the temporal dynamics of SST. Building upon ODE, we introduce an Energy Exchanges Integrator (EEI)-inspired by ocean heat budget equations-to account for external forcing factors. Thus, the variations in the components of these factors provide deeper insights into SST dynamics. Extensive experiments demonstrate that SSTODE achieves state-of-the-art performances in global and regional SST forecasting benchmarks. Furthermore, SSTODE visually reveals the impact of advection dynamics, thermal diffusion patterns, and diurnal heating-cooling cycles on SST evolution. These findings demonstrate the model's interpretability and physical consistency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Machine Learning for Uncertainty Quantification in Turbulence Models</title>
<link>https://arxiv.org/abs/2511.05633</link>
<guid>https://arxiv.org/abs/2511.05633</guid>
<content:encoded><![CDATA[
<div> Keywords: turbulent flows, turbulence models, Eigenspace Perturbation Method, convolutional neural network, calibration

Summary:
Predicting the evolution of turbulent flows is crucial in various fields, but the use of turbulence models introduces uncertainty. The Eigenspace Perturbation Method (EPM) is commonly used to assess model-form uncertainty, but it may overestimate uncertainty bounds due to its purely physics-based nature. In this study, a hybrid approach combining a convolutional neural network (CNN) with EPM is proposed to enhance the calibration of uncertainty estimates while maintaining physical consistency. By modulating EPM perturbation magnitudes with CNN, the hybrid ML-EPM framework demonstrates significant improvements in providing tighter and better-calibrated uncertainty estimates compared to using EPM alone. This innovative approach holds promise for more accurate predictions in turbulent flow simulations, bridging the gap between physics-based methods and machine learning techniques. 

<br /><br />Summary: <div>
arXiv:2511.05633v1 Announce Type: new 
Abstract: Predicting the evolution of turbulent flows is central across science and engineering. Most studies rely on simulations with turbulence models, whose empirical simplifications introduce epistemic uncertainty. The Eigenspace Perturbation Method (EPM) is a widely used physics-based approach to quantify model-form uncertainty, but being purely physics-based it can overpredict uncertainty bounds. We propose a convolutional neural network (CNN)-based modulation of EPM perturbation magnitudes to improve calibration while preserving physical consistency. Across canonical cases, the hybrid ML-EPM framework yields substantially tighter, better-calibrated uncertainty estimates than baseline EPM alone.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blind Inverse Game Theory: Jointly Decoding Rewards and Rationality in Entropy-Regularized Competitive Games</title>
<link>https://arxiv.org/abs/2511.05640</link>
<guid>https://arxiv.org/abs/2511.05640</guid>
<content:encoded><![CDATA[
<div> entropy-regularized, Quantal Response Equilibrium, Inverse Game Theory, Blind-IGT, joint parameter recovery <br />
<br />
Summary: 
The article introduces Blind-IGT, a statistical framework for recovering both the rationality parameter and reward parameters in competitive settings where the rationality parameter is unknown. By imposing a normalization constraint to resolve scale ambiguity, the framework can uniquely identify these parameters. An efficient Normalized Least Squares estimator is proposed, achieving optimal convergence rates. In cases where unique identification is not possible, confidence sets are constructed for partial identification. The framework is extended to Markov games, demonstrating optimal convergence rates and strong empirical performance even when transition dynamics are unknown. <div>
arXiv:2511.05640v1 Announce Type: new 
Abstract: Inverse Game Theory (IGT) methods based on the entropy-regularized Quantal Response Equilibrium (QRE) offer a tractable approach for competitive settings, but critically assume the agents' rationality parameter (temperature $\tau$) is known a priori. When $\tau$ is unknown, a fundamental scale ambiguity emerges that couples $\tau$ with the reward parameters ($\theta$), making them statistically unidentifiable. We introduce Blind-IGT, the first statistical framework to jointly recover both $\theta$ and $\tau$ from observed behavior. We analyze this bilinear inverse problem and establish necessary and sufficient conditions for unique identification by introducing a normalization constraint that resolves the scale ambiguity. We propose an efficient Normalized Least Squares (NLS) estimator and prove it achieves the optimal $\mathcal{O}(N^{-1/2})$ convergence rate for joint parameter recovery. When strong identifiability conditions fail, we provide partial identification guarantees through confidence set construction. We extend our framework to Markov games and demonstrate optimal convergence rates with strong empirical performance even when transition dynamics are unknown.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KLASS: KL-Guided Fast Inference in Masked Diffusion Models</title>
<link>https://arxiv.org/abs/2511.05664</link>
<guid>https://arxiv.org/abs/2511.05664</guid>
<content:encoded><![CDATA[
<div> Keywords: Masked diffusion models, KL-Adaptive Stability Sampling, iteration refinement process, fast sampling method, diverse domains

Summary:
Masked diffusion models have shown promise in various tasks, including language generation. However, their iterative refinement process often leads to slow inference due to static sampling speed. To address this issue, the authors propose KL-Adaptive Stability Sampling (KLASS), a fast sampling method that leverages token-level KL divergence to identify stable predictions with high confidence. By unmasking multiple tokens in each iteration without additional training, KLASS significantly accelerates generation speed while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to 2.78 times faster wall-clock speedups compared to standard greedy decoding, outperforming other diffusion-based samplers. The effectiveness of KLASS is demonstrated across different domains such as text, image, and molecular generation, highlighting its versatility as a widely applicable sampler for diverse models.<br /><br />Summary: <div>
arXiv:2511.05664v1 Announce Type: new 
Abstract: Masked diffusion models have demonstrated competitive results on various tasks including language generation. However, due to its iterative refinement process, the inference is often bottlenecked by slow and static sampling speed. To overcome this problem, we introduce `KL-Adaptive Stability Sampling' (KLASS), a fast yet effective sampling method that exploits token-level KL divergence to identify stable, high-confidence predictions. By unmasking multiple tokens in each iteration without any additional model training, our approach speeds up generation significantly while maintaining sample quality. On reasoning benchmarks, KLASS achieves up to $2.78\times$ wall-clock speedups while improving performance over standard greedy decoding, attaining state-of-the-art results among diffusion-based samplers. We further validate KLASS across diverse domains, including text, image, and molecular generation, showing its effectiveness as a broadly applicable sampler across different models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Self Paced Curriculum Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.05694</link>
<guid>https://arxiv.org/abs/2511.05694</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Distributionally Robust, Self-Paced Curriculum, Robustness, Performance

Summary: 
Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL) addresses the challenge in reinforcement learning of policies failing under distribution shifts in real-world environments. It treats the robustness budget as a continuous curriculum, adapting it based on the agent's progress. This adaptive scheduling allows for a balance between nominal and robust performance. Empirical results show that DR-SPCRL stabilizes training and achieves a superior trade-off between robustness and performance. Compared to fixed or heuristic scheduling strategies, DR-SPCRL demonstrates an average 11.8% increase in episodic return under varying perturbations and achieves approximately 1.9 times the performance of corresponding nominal RL algorithms. This method offers a promising solution to the trade-off challenge in reinforcement learning, providing improved robustness and performance in complex environments.<br /><br />Summary: <div>
arXiv:2511.05694v1 Announce Type: new 
Abstract: A central challenge in reinforcement learning is that policies trained in controlled environments often fail under distribution shifts at deployment into real-world environments. Distributionally Robust Reinforcement Learning (DRRL) addresses this by optimizing for worst-case performance within an uncertainty set defined by a robustness budget $\epsilon$. However, fixing $\epsilon$ results in a tradeoff between performance and robustness: small values yield high nominal performance but weak robustness, while large values can result in instability and overly conservative policies. We propose Distributionally Robust Self-Paced Curriculum Reinforcement Learning (DR-SPCRL), a method that overcomes this limitation by treating $\epsilon$ as a continuous curriculum. DR-SPCRL adaptively schedules the robustness budget according to the agent's progress, enabling a balance between nominal and robust performance. Empirical results across multiple environments demonstrate that DR-SPCRL not only stabilizes training but also achieves a superior robustness-performance trade-off, yielding an average 11.8\% increase in episodic return under varying perturbations compared to fixed or heuristic scheduling strategies, and achieving approximately 1.9$\times$ the performance of the corresponding nominal RL algorithms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-assisted workflow enables rapid, high-fidelity breast cancer clinical trial eligibility prescreening</title>
<link>https://arxiv.org/abs/2511.05696</link>
<guid>https://arxiv.org/abs/2511.05696</guid>
<content:encoded><![CDATA[
<div> AI, clinical trials, cancer care, eligibility screening, oncology

Summary: 
MSK-MATCH is an AI system developed for automated eligibility screening in cancer clinical trials. It integrates a large language model with an oncology trial knowledge base and retrieval-augmented architecture, providing explanations for all AI predictions based on source text. In a retrospective dataset of 88,518 clinical documents, MSK-MATCH achieved 98.6% accuracy in patient-level eligibility classification, resolving 61.9% of cases automatically and triaging the rest for human review. The AI-assisted workflow outperformed human-only and AI-only comparisons in sensitivity and specificity. For cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time significantly and cost only $0.96 per patient-trial pair. This innovative approach could improve participation rates in cancer clinical trials by streamlining eligibility screening processes. 

<br /><br />Summary: <div>
arXiv:2511.05696v1 Announce Type: new 
Abstract: Clinical trials play an important role in cancer care and research, yet participation rates remain low. We developed MSK-MATCH (Memorial Sloan Kettering Multi-Agent Trial Coordination Hub), an AI system for automated eligibility screening from clinical text. MSK-MATCH integrates a large language model with a curated oncology trial knowledge base and retrieval-augmented architecture providing explanations for all AI predictions grounded in source text. In a retrospective dataset of 88,518 clinical documents from 731 patients across six breast cancer trials, MSK-MATCH automatically resolved 61.9% of cases and triaged 38.1% for human review. This AI-assisted workflow achieved 98.6% accuracy, 98.4% sensitivity, and 98.7% specificity for patient-level eligibility classification, matching or exceeding performance of the human-only and AI-only comparisons. For the triaged cases requiring manual review, prepopulating eligibility screens with AI-generated explanations reduced screening time from 20 minutes to 43 seconds at an average cost of $0.96 per patient-trial pair.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabDistill: Distilling Transformers into Neural Nets for Few-Shot Tabular Classification</title>
<link>https://arxiv.org/abs/2511.05704</link>
<guid>https://arxiv.org/abs/2511.05704</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer-based models, tabular data, few-shot regime, knowledge distillation, neural networks

Summary:
TabDistill introduces a strategy to distill the pre-trained knowledge from complex transformer-based models into simpler neural networks for classifying tabular data. This approach aims to achieve high performance with limited training data while remaining parameter-efficient. The distilled neural networks outperform classical baselines such as regular neural networks, XGBoost, and logistic regression with equal training data. In some cases, the distilled models even outperform the original transformer-based models they were derived from. This strategy bridges the gap between performance and complexity, providing a middle ground for effectively handling tabular data classification tasks. <div>
arXiv:2511.05704v1 Announce Type: new 
Abstract: Transformer-based models have shown promising performance on tabular data compared to their classical counterparts such as neural networks and Gradient Boosted Decision Trees (GBDTs) in scenarios with limited training data. They utilize their pre-trained knowledge to adapt to new domains, achieving commendable performance with only a few training examples, also called the few-shot regime. However, the performance gain in the few-shot regime comes at the expense of significantly increased complexity and number of parameters. To circumvent this trade-off, we introduce TabDistill, a new strategy to distill the pre-trained knowledge in complex transformer-based models into simpler neural networks for effectively classifying tabular data. Our framework yields the best of both worlds: being parameter-efficient while performing well with limited training data. The distilled neural networks surpass classical baselines such as regular neural networks, XGBoost and logistic regression under equal training data, and in some cases, even the original transformer-based models that they were distilled from.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributionally Robust Multimodal Machine Learning</title>
<link>https://arxiv.org/abs/2511.05716</link>
<guid>https://arxiv.org/abs/2511.05716</guid>
<content:encoded><![CDATA[
<div> Distributionally robust optimization, multimodal machine learning, uncertainty modeling, complexity analysis, performance guarantees
Summary:
1. The article addresses the problem of distributionally robust multimodal machine learning, proposing a novel framework that considers both theoretical and practical insights.
2. The framework justifies the setup and demonstrates the significance of the problem through complexity analysis.
3. Generalization upper bounds and minimax lower bounds are established to provide performance guarantees, which are further extended to settings involving encoder-specific error propogations.
4. Empirical results show that the proposed approach improves robustness in simulation settings and real-world datasets.
5. The findings offer a principled foundation for utilizing multimodal machine learning models in high-stakes applications where uncertainty is inevitable.
<br /><br />Summary: <div>
arXiv:2511.05716v1 Announce Type: new 
Abstract: We consider the problem of distributionally robust multimodal machine learning. Existing approaches often rely on merging modalities on the feature level (early fusion) or heuristic uncertainty modeling, which downplays modality-aware ef- fects and provide limited insights. We propose a novel distributionally robust optimization (DRO) framework that aims to study both the theoretical and practical insights of multimodal machine learning. We first justify this setup and show the significance of this problem through complexity analysis. We then establish both generalization upper bounds and minimax lower bounds which provide perfor- mance guarantees. These results are further extended in settings where we consider encoder-specific error propogations. Empirically, we demonstrate that our approach improves robustness in both simulation settings and real-world datasets. Together, these findings provide a principled foundation for employing multimodal machine learning models in high-stakes applications where uncertainty is unavoidable.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GastroDL-Fusion: A Dual-Modal Deep Learning Framework Integrating Protein-Ligand Complexes and Gene Sequences for Gastrointestinal Disease Drug Discovery</title>
<link>https://arxiv.org/abs/2511.05726</link>
<guid>https://arxiv.org/abs/2511.05726</guid>
<content:encoded><![CDATA[
<div> Prediction, protein-ligand binding affinity, deep learning, gastrointestinal diseases, drug development

Summary:
- The article introduces GastroDL-Fusion, a dual-modal deep learning framework that combines protein-ligand complex data with disease-associated gene sequence information for drug discovery in gastrointestinal diseases.
- GastroDL-Fusion integrates molecular graph representations of protein-ligand complexes with gene sequence embeddings generated by a pre-trained Transformer model.
- The model utilizes a Graph Isomorphism Network (GIN) for the protein-ligand complex data and a multi-layer perceptron for cross-modal interaction learning.
- Evaluation on GI disease-related targets shows that GastroDL-Fusion outperforms traditional methods, achieving a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75.
- Incorporating both structural and genetic features leads to more accurate predictions of binding affinities, making GastroDL-Fusion a valuable tool for designing targeted therapies and vaccines in the context of gastrointestinal diseases.

<br /><br />Summary: <div>
arXiv:2511.05726v1 Announce Type: new 
Abstract: Accurate prediction of protein-ligand binding affinity plays a pivotal role in accelerating the discovery of novel drugs and vaccines, particularly for gastrointestinal (GI) diseases such as gastric ulcers, Crohn's disease, and ulcerative colitis. Traditional computational models often rely on structural information alone and thus fail to capture the genetic determinants that influence disease mechanisms and therapeutic responses. To address this gap, we propose GastroDL-Fusion, a dual-modal deep learning framework that integrates protein-ligand complex data with disease-associated gene sequence information for drug and vaccine development. In our approach, protein-ligand complexes are represented as molecular graphs and modeled using a Graph Isomorphism Network (GIN), while gene sequences are encoded into biologically meaningful embeddings via a pre-trained Transformer (ProtBERT/ESM). These complementary modalities are fused through a multi-layer perceptron to enable robust cross-modal interaction learning. We evaluate the model on benchmark datasets of GI disease-related targets, demonstrating that GastroDL-Fusion significantly improves predictive performance over conventional methods. Specifically, the model achieves a mean absolute error (MAE) of 1.12 and a root mean square error (RMSE) of 1.75, outperforming CNN, BiLSTM, GIN, and Transformer-only baselines. These results confirm that incorporating both structural and genetic features yields more accurate predictions of binding affinities, providing a reliable computational tool for accelerating the design of targeted therapies and vaccines in the context of gastrointestinal diseases.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Compressing Chemistry Reveals Functional Groups</title>
<link>https://arxiv.org/abs/2511.05728</link>
<guid>https://arxiv.org/abs/2511.05728</guid>
<content:encoded><![CDATA[
<div> Keywords: chemical functional groups, unsupervised learning, Minimum Message Length, bioactivity prediction, fingerprint representations

Summary:
In this study, the utility of traditional chemical functional groups in chemical explanations was formally assessed using an unsupervised learning algorithm based on the Minimum Message Length principle. The algorithm searched for substructures that compress around three million biologically relevant molecules, discovering both human-curated functional groups and novel larger patterns with specific functions. The algorithm was then applied to 24 bioactivity prediction datasets to find dataset-specific functional groups. Fingerprints constructed from these dataset-specific functional groups outperformed other fingerprint representations in bioactivity regression tasks. This research showcases the effectiveness of using unsupervised learning algorithms and the Minimum Message Length principle in identifying functional groups and improving bioactivity prediction models. <div>
arXiv:2511.05728v1 Announce Type: new 
Abstract: We introduce the first formal large-scale assessment of the utility of traditional chemical functional groups as used in chemical explanations. Our assessment employs a fundamental principle from computational learning theory: a good explanation of data should also compress the data. We introduce an unsupervised learning algorithm based on the Minimum Message Length (MML) principle that searches for substructures that compress around three million biologically relevant molecules. We demonstrate that the discovered substructures contain most human-curated functional groups as well as novel larger patterns with more specific functions. We also run our algorithm on 24 specific bioactivity prediction datasets to discover dataset-specific functional groups. Fingerprints constructed from dataset-specific functional groups are shown to significantly outperform other fingerprint representations, including the MACCS and Morgan fingerprint, when training ridge regression models on bioactivity regression tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiVC-Net: Quantum-Inspired Variational Convolutional Network, with Application to Biosignal Classification</title>
<link>https://arxiv.org/abs/2511.05730</link>
<guid>https://arxiv.org/abs/2511.05730</guid>
<content:encoded><![CDATA[
<div> Keywords: Quantum-inspired variational convolution, probabilistic inference, ensemble rotation, uncertainty modeling, biosignal classification 

Summary:<br />
The Quantum-inspired Variational Convolution (QiVC) framework combines probabilistic inference and quantum-inspired transformations within convolutional architectures. The QiVC framework introduces the Quantum-inspired Rotated Ensemble (QiRE) mechanism, which performs differentiable subspace rotations of convolutional weights akin to quantum state evolution. This allows for structured uncertainty modeling while maintaining parameter space geometry, resulting in more expressive and stable representations. The proposed QiVC-Net, with a QiVC layer that does not add parameters, achieves state-of-the-art performance in biosignal classification tasks, particularly in challenging domains like phonocardiogram (PCG) recordings. The QiVC-Net demonstrates high accuracy on benchmark datasets, showcasing its promise in uncertainty-aware modeling for real-world biomedical signal analysis. Implementation of the QiVC layer is publicly available on GitHub. 

<br /><br />Summary: <div>
arXiv:2511.05730v1 Announce Type: new 
Abstract: This work introduces the quantum-inspired variational convolution (QiVC) framework, a novel learning paradigm that integrates principles of probabilistic inference, variational optimization, and quantum-inspired transformations within convolutional architectures. The central innovation of QiVC lies in its quantum-inspired rotated ensemble (QiRE) mechanism. QiRE performs differentiable low-dimensional subspace rotations of convolutional weights, analogously to quantum state evolution. This approach enables structured uncertainty modeling while preserving the intrinsic geometry of the parameter space, resulting in more expressive, stable, and uncertainty-aware representations. To demonstrate its practical potential, the concept is instantiated in a QiVC-based convolutional network (QiVC-Net) and evaluated in the context of biosignal classification, focusing on phonocardiogram (PCG) recordings, a challenging domain characterized by high noise, inter-subject variability, and often imbalanced data. The proposed QiVC-Net integrates an architecture in which the QiVC layer does not introduce additional parameters, instead performing an ensemble rotation of the convolutional weights through a structured mechanism ensuring robustness without added highly computational burden. Experiments on two benchmark datasets, PhysioNet CinC 2016 and PhysioNet CirCor DigiScope 2022, show that QiVC-Net achieves state-of-the-art performance, reaching accuracies of 97.84% and 97.89%, respectively. These findings highlight the versatility of the QiVC framework and its promise for advancing uncertainty-aware modeling in real-world biomedical signal analysis. The implementation of the QiVConv layer is openly available in GitHub.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Exponential Savings for Mean Estimation with Active Learning</title>
<link>https://arxiv.org/abs/2511.05736</link>
<guid>https://arxiv.org/abs/2511.05736</guid>
<content:encoded><![CDATA[
<div> Algorithm, Active Learning, Estimation, Covariates, PartiBandits <br />
<br />
The study presents an active learning algorithm called PartiBandits for efficient estimation of the mean of a k-class random variable with limited labels and access to informative covariates. The algorithm provides an estimate of the mean with an error that decreases with the number of labels N. It consists of two stages - learning a partition of unlabeled data to reduce conditional variance and using a UCB-style subroutine for label requests. The convergence rates of both the algorithm and subroutine are optimal. PartiBandits combines UCB and disagreement-based approaches in active learning, demonstrating versatility. Simulation using electronic health records showcases the algorithm's effectiveness. The PartiBandits package in R enables easy implementation of the proposed method. <br /><br />Summary: <div>
arXiv:2511.05736v1 Announce Type: new 
Abstract: We study the problem of efficiently estimating the mean of a $k$-class random variable, $Y$, using a limited number of labels, $N$, in settings where the analyst has access to auxiliary information (i.e.: covariates) $X$ that may be informative about $Y$. We propose an active learning algorithm ("PartiBandits") to estimate $\mathbb{E}[Y]$. The algorithm yields an estimate, $\widehat{\mu}_{\text{PB}}$, such that $\left( \widehat{\mu}_{\text{PB}} - \mathbb{E}[Y]\right)^2$ is $\tilde{\mathcal{O}}\left( \frac{\nu + \exp(c \cdot (-N/\log(N))) }{N} \right)$, where $c > 0$ is a constant and $\nu$ is the risk of the Bayes-optimal classifier. PartiBandits is essentially a two-stage algorithm. In the first stage, it learns a partition of the unlabeled data that shrinks the average conditional variance of $Y$. In the second stage it uses a UCB-style subroutine ("WarmStart-UCB") to request labels from each stratum round-by-round. Both the main algorithm's and the subroutine's convergence rates are minimax optimal in classical settings. PartiBandits bridges the UCB and disagreement-based approaches to active learning despite these two approaches being designed to tackle very different tasks. We illustrate our methods through simulation using nationwide electronic health records. Our methods can be implemented using the PartiBandits package in R.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Redundancy: Diverse and Specialized Multi-Expert Sparse Autoencoder</title>
<link>https://arxiv.org/abs/2511.05745</link>
<guid>https://arxiv.org/abs/2511.05745</guid>
<content:encoded><![CDATA[
<div> Keywords: Sparse autoencoders, large language models, interpretability, Mixture of Experts, feature redundancy

Summary: 
Sparse autoencoders (SAEs) are important for interpreting large language models (LLMs) by breaking down token activations into understandable features. However, the high dimensionality of SAEs for better interpretability leads to costly training and inference. Mixture of Experts (MoE) techniques aim to address this by splitting SAEs into narrower expert networks with gated activation to reduce computation. Nonetheless, MoE-SAE faces a limitation where experts often do not specialize and learn overlapping features. To tackle this issue, the authors introduce Multiple Expert Activation to engage semantically weighted expert subsets simultaneously and Feature Scaling for diverse feature learning via adaptive scaling. Experimental results show improved performance with a 24% lower reconstruction error and 99% less feature redundancy compared to existing MoE-SAE methods. This work bridges the gap between interpretability and efficiency in LLM analysis by enabling transparent model investigation while maintaining computational feasibility.<br /><br />Summary: <div>
arXiv:2511.05745v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting large language models (LLMs) by decomposing token activations into combinations of human-understandable features. While SAEs provide crucial insights into LLM explanations, their practical adoption faces a fundamental challenge: better interpretability demands that SAEs' hidden layers have high dimensionality to satisfy sparsity constraints, resulting in prohibitive training and inference costs. Recent Mixture of Experts (MoE) approaches attempt to address this by partitioning SAEs into narrower expert networks with gated activation, thereby reducing computation. In a well-designed MoE, each expert should focus on learning a distinct set of features. However, we identify a \textit{critical limitation} in MoE-SAE: Experts often fail to specialize, which means they frequently learn overlapping or identical features. To deal with it, we propose two key innovations: (1) Multiple Expert Activation that simultaneously engages semantically weighted expert subsets to encourage specialization, and (2) Feature Scaling that enhances diversity through adaptive high-frequency scaling. Experiments demonstrate a 24\% lower reconstruction error and a 99\% reduction in feature redundancy compared to existing MoE-SAE methods. This work bridges the interpretability-efficiency gap in LLM analysis, allowing transparent model inspection without compromising computational feasibility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Primal-Only Actor Critic Algorithm for Robust Constrained Average Cost MDPs</title>
<link>https://arxiv.org/abs/2511.05758</link>
<guid>https://arxiv.org/abs/2511.05758</guid>
<content:encoded><![CDATA[
<div> Average-Cost RCMDPs, Robust policies, Actor-critic algorithm, Feasibility, Optimality <br />
<br />
Summary: 
In the study of Robust Constrained Average-Cost Markov Decision Processes (RCMDPs), the lack of strong duality and non-contractive properties of the Robust Bellman operator pose challenges for traditional methods. To tackle these obstacles, an actor-critic algorithm for Average-Cost RCMDPs is proposed in this work. The algorithm is shown to achieve both feasibility and optimality within certain bounds, with sample complexities comparable to the discounted setting. The method addresses the difficulties of finding robust and safe policies in a complex environment, showcasing promising results in overcoming the limitations of standard primal-dual approaches. This research provides a valuable contribution to the field of constrained reinforcement learning, offering a new perspective on tackling challenging problems in Markov decision processes. <br /><br /> <div>
arXiv:2511.05758v1 Announce Type: new 
Abstract: In this work, we study the problem of finding robust and safe policies in Robust Constrained Average-Cost Markov Decision Processes (RCMDPs). A key challenge in this setting is the lack of strong duality, which prevents the direct use of standard primal-dual methods for constrained RL. Additional difficulties arise from the average-cost setting, where the Robust Bellman operator is not a contraction under any norm. To address these challenges, we propose an actor-critic algorithm for Average-Cost RCMDPs. We show that our method achieves both \(\epsilon\)-feasibility and \(\epsilon\)-optimality, and we establish a sample complexities of \(\tilde{O}\left(\epsilon^{-4}\right)\) and \(\tilde{O}\left(\epsilon^{-6}\right)\) with and without slackness assumption, which is comparable to the discounted setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Efficient Gradient-Aware Error-Bounded Lossy Compressor for Federated Learning</title>
<link>https://arxiv.org/abs/2511.05770</link>
<guid>https://arxiv.org/abs/2511.05770</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Error-Bounded Lossy Compression, Gradient Data, Communication Efficiency, Model Accuracy

Summary: 
An innovative error-bounded lossy compression framework tailored for Federated Learning (FL) gradient data is proposed to address communication efficiency in collaborative model training. The framework leverages temporal correlations across FL training rounds and structural regularities within convolutional kernels for high compression ratios. It includes a magnitude predictor based on a normalized exponential moving average and a sign predictor leveraging gradient oscillation and kernel-level consistency. Experimental results show up to 1.53x higher compression ratios compared to existing methods with lower accuracy loss. Integrated into a real-world FL framework, APPFL, the new framework reduces end-to-end communication time by 76.1%-96.2% under constrained-bandwidth scenarios, demonstrating scalability for practical FL deployments. <div>
arXiv:2511.05770v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative model training without exposing clients' private data, but its deployment is often constrained by the communication cost of transmitting gradients between clients and the central server, especially under system heterogeneity where low-bandwidth clients bottleneck overall performance. Lossy compression of gradient data can mitigate this overhead, and error-bounded lossy compression (EBLC) is particularly appealing for its fine-grained utility-compression tradeoff. However, existing EBLC methods (e.g., SZ), originally designed for smooth scientific data with strong spatial locality, rely on generic predictors such as Lorenzo and interpolation for entropy reduction to improve compression ratio. Gradient tensors, in contrast, exhibit low smoothness and weak spatial correlation, rendering these predictors ineffective and leading to poor compression ratios. To address this limitation, we propose an EBLC framework tailored for FL gradient data to achieve high compression ratios while preserving model accuracy. The core of it is an innovative prediction mechanism that exploits temporal correlations across FL training rounds and structural regularities within convolutional kernels to reduce residual entropy. The predictor is compatible with standard quantizers and entropy coders and comprises (1) a cross-round magnitude predictor based on a normalized exponential moving average, and (2) a sign predictor that leverages gradient oscillation and kernel-level sign consistency. Experiments show that this new EBLC yields up to 1.53x higher compression ratios than SZ3 with lower accuracy loss. Integrated into a real-world FL framework, APPFL, it reduces end-to-end communication time by 76.1%-96.2% under various constrained-bandwidth scenarios, demonstrating strong scalability for real-world FL deployments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories</title>
<link>https://arxiv.org/abs/2511.05773</link>
<guid>https://arxiv.org/abs/2511.05773</guid>
<content:encoded><![CDATA[
<div> framework, real-time, activity recognition, sensor streams, deep learning 
Summary:
The paper introduces MARAuder's Map, a framework for real-time human activity recognition in smart homes using raw sensor data. The method projects sensor activations onto a physical floorplan to create trajectory-aware image sequences, capturing the spatial flow of human movement. A hybrid deep learning model is employed to process these representations, incorporating spatial structure and temporal dependencies. A learnable time embedding module encodes contextual cues like hour-of-day and day-of-week to enhance temporal awareness. An attention-based encoder focuses on informative segments within observation windows, enabling accurate recognition even during cross-activity transitions and temporal ambiguity. Extensive experiments on real-world smart home datasets demonstrate the superiority of the proposed method over existing baselines, offering a practical solution for real-time human activity recognition in ambient sensor environments. 
<br /><br />Summary: <div>
arXiv:2511.05773v1 Announce Type: new 
Abstract: Ambient sensor-based human activity recognition (HAR) in smart homes remains challenging due to the need for real-time inference, spatially grounded reasoning, and context-aware temporal modeling. Existing approaches often rely on pre-segmented, within-activity data and overlook the physical layout of the environment, limiting their robustness in continuous, real-world deployments. In this paper, we propose MARAuder's Map, a novel framework for real-time activity recognition from raw, unsegmented sensor streams. Our method projects sensor activations onto the physical floorplan to generate trajectory-aware, image-like sequences that capture the spatial flow of human movement. These representations are processed by a hybrid deep learning model that jointly captures spatial structure and temporal dependencies. To enhance temporal awareness, we introduce a learnable time embedding module that encodes contextual cues such as hour-of-day and day-of-week. Additionally, an attention-based encoder selectively focuses on informative segments within each observation window, enabling accurate recognition even under cross-activity transitions and temporal ambiguity. Extensive experiments on multiple real-world smart home datasets demonstrate that our method outperforms strong baselines, offering a practical solution for real-time HAR in ambient sensor environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control</title>
<link>https://arxiv.org/abs/2511.05790</link>
<guid>https://arxiv.org/abs/2511.05790</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Reinforcement Learning, Traffic Signal Control, Monte Carlo Tree Search, Interpretable Policies, Symbolic Priority Functions

Summary:
SymLight introduces a framework based on Monte Carlo Tree Search (MCTS) to discover interpretable symbolic priority functions for traffic signal control policies. These priority functions take traffic features as input and output priorities for each traffic signal phase, allowing for effective phase transition. The framework includes a concise representation for the priority function to handle the complexity of the action space in MCTS. Additionally, a probabilistic structural rollout strategy uses patterns from high-quality priority functions to guide the search process. Experimental results show SymLight outperforms baselines on real-world datasets, producing interpretable policies that can be deployed effectively while maintaining high performance. Overall, SymLight offers a promising approach for generating efficient and understandable traffic signal control policies using deep reinforcement learning. 

<br /><br />Summary: <div>
arXiv:2511.05790v1 Announce Type: new 
Abstract: Deep Reinforcement Learning have achieved significant success in automatically devising effective traffic signal control (TSC) policies. Neural policies, however, tend to be over-parameterized and non-transparent, hindering their interpretability and deployability on resource-limited edge devices. This work presents SymLight, a priority function search framework based on Monte Carlo Tree Search (MCTS) for discovering inherently interpretable and deployable symbolic priority functions to serve as the TSC policies. The priority function, in particular, accepts traffic features as input and then outputs a priority for each traffic signal phase, which subsequently directs the phase transition. For effective search, we propose a concise yet expressive priority function representation. This helps mitigate the combinatorial explosion of the action space in MCTS. Additionally, a probabilistic structural rollout strategy is introduced to leverage structural patterns from previously discovered high-quality priority functions, guiding the rollout process. Our experiments on real-world datasets demonstrate SymLight's superior performance across a range of baselines. A key advantage is SymLight's ability to produce interpretable and deployable TSC policies while maintaining excellent performance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits</title>
<link>https://arxiv.org/abs/2511.05802</link>
<guid>https://arxiv.org/abs/2511.05802</guid>
<content:encoded><![CDATA[
<div> bandit algorithms, multi-objective decision-making, lexicographic preferences, regret minimization, best arm identification

Summary: In the context of multi-objective decision-making with hierarchical preferences, this study introduces two elimination-based algorithms that balance regret minimization and best arm identification under lexicographic preferences. The first algorithm sequentially eliminates suboptimal arms based on priority objectives, achieving competitive sample complexity and regret bounds. The second algorithm leverages cross-objective dependencies to outperform the lower bound for single-objective bandit problems. Empirical results confirm the superior performance of these algorithms over baseline methods, highlighting the effectiveness of incorporating cross-objective information sharing in the multi-objective setting. <div>
arXiv:2511.05802v1 Announce Type: new 
Abstract: In multi-objective decision-making with hierarchical preferences, lexicographic bandits provide a natural framework for optimizing multiple objectives in a prioritized order. In this setting, a learner repeatedly selects arms and observes reward vectors, aiming to maximize the reward for the highest-priority objective, then the next, and so on. While previous studies have primarily focused on regret minimization, this work bridges the gap between \textit{regret minimization} and \textit{best arm identification} under lexicographic preferences. We propose two elimination-based algorithms to address this joint objective. The first algorithm eliminates suboptimal arms sequentially, layer by layer, in accordance with the objective priorities, and achieves sample complexity and regret bounds comparable to those of the best single-objective algorithms. The second algorithm simultaneously leverages reward information from all objectives in each round, effectively exploiting cross-objective dependencies. Remarkably, it outperforms the known lower bound for the single-objective bandit problem, highlighting the benefit of cross-objective information sharing in the multi-objective setting. Empirical results further validate their superior performance over baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Catching Contamination Before Generation: Spectral Kill Switches for Agents</title>
<link>https://arxiv.org/abs/2511.05804</link>
<guid>https://arxiv.org/abs/2511.05804</guid>
<content:encoded><![CDATA[
<div> detect, agentic language models, attention, spectral statistics, context verification  
Summary:  
The article introduces a diagnostic method for agentic language models that can detect errors in reasoning chains caused by inconsistent context, retrieval errors, or adversarial inputs. The method analyzes token graphs induced by attention in early layers and computes spectral statistics, specifically the high frequency energy ratio and spectral entropy. Using a two-regime mixture assumption, the method shows that a single threshold on the high frequency energy ratio is optimal for detecting context inconsistency. Empirical results demonstrate the method's robustness in context verification across different model families, with low processing overhead. The approach can be integrated into retrieval augmented agent pipelines and used as an inline safety monitor to detect contamination in the text while the model is still processing it, preventing errors from propagating in the reasoning chain.<br /><br />Summary: <div>
arXiv:2511.05804v1 Announce Type: new 
Abstract: Agentic language models compose multi step reasoning chains, yet intermediate steps can be corrupted by inconsistent context, retrieval errors, or adversarial inputs, which makes post hoc evaluation too late because errors propagate before detection. We introduce a diagnostic that requires no additional training and uses only the forward pass to emit a binary accept or reject signal during agent execution. The method analyzes token graphs induced by attention and computes two spectral statistics in early layers, namely the high frequency energy ratio and spectral entropy. We formalize these signals, establish invariances, and provide finite sample estimators with uncertainty quantification. Under a two regime mixture assumption with a monotone likelihood ratio property, we show that a single threshold on the high frequency energy ratio is optimal in the Bayes sense for detecting context inconsistency. Empirically, the high frequency energy ratio exhibits robust bimodality during context verification across multiple model families, which enables gating decisions with overhead below one millisecond on our hardware and configurations. We demonstrate integration into retrieval augmented agent pipelines and discuss deployment as an inline safety monitor. The approach detects contamination while the model is still processing the text, before errors commit to the reasoning chain.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Model Performance in the Presence of an Intervention</title>
<link>https://arxiv.org/abs/2511.05805</link>
<guid>https://arxiv.org/abs/2511.05805</guid>
<content:encoded><![CDATA[
<div> Randomized controlled trials, model evaluation, bias, intervention, nuisance parameter weighting

Summary:
- AI models are often evaluated based on their predictive ability, but interventions in social impact applications can bias evaluations.
- RCTs randomize interventions, but traditional approaches ignore data from the treatment group, leading to inefficiencies.
- The study quantifies bias from aggregating performance estimates from treatment and control groups and proposes nuisance parameter weighting (NPW) for unbiased model evaluation.
- NPW reweights treatment group data to mimic distributions under no intervention, improving model selection compared to standard approaches.
- Synthetic and real-world datasets demonstrate the effectiveness of NPW in various intervention effect and sample size scenarios. 

<br /><br />Summary: <div>
arXiv:2511.05805v1 Announce Type: new 
Abstract: AI models are often evaluated based on their ability to predict the outcome of interest. However, in many AI for social impact applications, the presence of an intervention that affects the outcome can bias the evaluation. Randomized controlled trials (RCTs) randomly assign interventions, allowing data from the control group to be used for unbiased model evaluation. However, this approach is inefficient because it ignores data from the treatment group. Given the complexity and cost often associated with RCTs, making the most use of the data is essential. Thus, we investigate model evaluation strategies that leverage all data from an RCT. First, we theoretically quantify the estimation bias that arises from na\"ively aggregating performance estimates from treatment and control groups, and derive the condition under which this bias leads to incorrect model selection. Leveraging these theoretical insights, we propose nuisance parameter weighting (NPW), an unbiased model evaluation approach that reweights data from the treatment group to mimic the distributions of samples that would or would not experience the outcome under no intervention. Using synthetic and real-world datasets, we demonstrate that our proposed evaluation approach consistently yields better model selection than the standard approach, which ignores data from the treatment group, across various intervention effect and sample size settings. Our contribution represents a meaningful step towards more efficient model evaluation in real-world contexts.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling</title>
<link>https://arxiv.org/abs/2511.05811</link>
<guid>https://arxiv.org/abs/2511.05811</guid>
<content:encoded><![CDATA[
<div> Keywords: FP8 training, numerical stability, two-level microscaling, automatic scaling, training efficiency

Summary: 
MOSS is a new FP8 training framework designed to address challenges in efficiency and numerical stability. It introduces a two-level microscaling strategy for quantizing sensitive activations, combining global and local scales to balance precision and dequantization cost effectively. Additionally, MOSS implements automatic scaling for weights in linear layers, eliminating costly max-reduction operations by predicting and adjusting scaling factors during training. These innovations allow for efficient FP8 training of large language models, achieving performance comparable to BF16 baseline models while achieving up to 34% higher training throughput.MOSS improves efficiency and stability in FP8 training by optimizing quantization strategies for activations and weights, leading to significant performance gains in training large language models. 

<br /><br />Summary: <div>
arXiv:2511.05811v1 Announce Type: new 
Abstract: Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading</title>
<link>https://arxiv.org/abs/2511.05814</link>
<guid>https://arxiv.org/abs/2511.05814</guid>
<content:encoded><![CDATA[
<div> Memory Optimization, Mixture of Experts, Offloading, Caching, Pre-fetching
Summary:
This study delves into the challenges of deploying Mixture of Experts (MoE) models due to their high memory requirements. The authors analyze expert activation and caching behavior, proposing a Least Frequently Used (LFU) caching optimization that outperforms the traditional Least Recently Used (LRU) approach. Furthermore, they explore the implementation and potential benefits of speculative expert pre-fetching. The study provides insights into the behavior of the MoE architecture, shedding light on the characteristics of the gating network and experts. These findings not only enhance our understanding of MoE models but also lay the groundwork for future research on interpreting MoE models and developing pruning techniques with minimal performance impact. 
<br /><br />Summary: <div>
arXiv:2511.05814v1 Announce Type: new 
Abstract: In today's landscape, Mixture of Experts (MoE) is a crucial architecture that has been used by many of the most advanced models. One of the major challenges of MoE models is that they usually require much more memory than their dense counterparts due to their unique architecture, and hence are harder to deploy in environments with limited GPU memory, such as edge devices. MoE offloading is a promising technique proposed to overcome this challenge, especially if it is enhanced with caching and pre-fetching, but prior work stopped at suboptimal caching algorithm and offered limited insights. In this work, we study MoE offloading in depth and make the following contributions: 1. We analyze the expert activation and LRU caching behavior in detail and provide traces. 2. We propose LFU caching optimization based on our analysis and obtain strong improvements from LRU. 3. We implement and experiment speculative expert pre-fetching, providing detailed trace showing its huge potential . 4. In addition, our study extensively covers the behavior of the MoE architecture itself, offering information on the characteristic of the gating network and experts. This can inspire future work on the interpretation of MoE models and the development of pruning techniques for MoE architecture with minimal performance loss.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector</title>
<link>https://arxiv.org/abs/2511.05823</link>
<guid>https://arxiv.org/abs/2511.05823</guid>
<content:encoded><![CDATA[
<div> Keywords: artificial intelligence, electronic design automation, chip design, unified library, data pipeline

Summary:
AiEDA is a new open-source library for electronic design automation (EDA) that addresses the fragmented nature of current AI-EDA infrastructures. It integrates various design-to-vector data representation techniques to create universal multi-level vector representations, optimizing the AI-aided design (AAD) workflow. The library provides complete physical design flows with standardized Python interfaces for seamless integration of EDA datasets and AI frameworks. AiEDA has been used to generate a large dataset called iDATA, derived from 50 real chip designs at 28nm, and has been validated through various AAD tasks such as prediction, generation, optimization, and analysis. The code for AiEDA is available on GitHub, and the iDATA dataset is in the process of being released publicly. This development lays the foundation for future research in AI-EDA. 

<br /><br />Summary: AiEDA is a unified open-source library for electronic design automation that addresses current infrastructure challenges, providing multi-level vector representations and standardized Python interfaces. It has been used to generate and validate the iDATA dataset for real chip designs, facilitating various AAD tasks and laying the groundwork for future AI-EDA research. <div>
arXiv:2511.05823v1 Announce Type: new 
Abstract: Recent research has demonstrated that artificial intelligence (AI) can assist electronic design automation (EDA) in improving both the quality and efficiency of chip design. But current AI for EDA (AI-EDA) infrastructures remain fragmented, lacking comprehensive solutions for the entire data pipeline from design execution to AI integration. Key challenges include fragmented flow engines that generate raw data, heterogeneous file formats for data exchange, non-standardized data extraction methods, and poorly organized data storage. This work introduces a unified open-source library for EDA (AiEDA) that addresses these issues. AiEDA integrates multiple design-to-vector data representation techniques that transform diverse chip design data into universal multi-level vector representations, establishing an AI-aided design (AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical design flows with programmatic data extraction and standardized Python interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA library, we generate iDATA, a 600GB dataset of structured data derived from 50 real chip designs (28nm), and validate its effectiveness through seven representative AAD tasks spanning prediction, generation, optimization and analysis. The code is publicly available at https://github.com/OSCC-Project/AiEDA, while the full iDATA dataset is being prepared for public release, providing a foundation for future AI-EDA research.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering</title>
<link>https://arxiv.org/abs/2511.05826</link>
<guid>https://arxiv.org/abs/2511.05826</guid>
<content:encoded><![CDATA[
<div> distance metric, categorical data clustering, cluster-customized, mixed data, experiments

Summary:
The article introduces a new approach for clustering categorical data by proposing a cluster-customized distance metric. Traditional distance metrics do not consider the varying distances between attribute values in different clusters, leading to inaccurate measurements. The proposed method updates distances based on the distributions of attributes within each cluster, resulting in more precise clustering. Additionally, the distance metric is extended to handle mixed data with numerical and categorical attributes. Experimental results demonstrate the effectiveness of the proposed approach, consistently achieving top rankings across multiple datasets. The source code for the method is also made available for further research and implementation. <div>
arXiv:2511.05826v1 Announce Type: new 
Abstract: An appropriate distance metric is crucial for categorical data clustering, as the distance between categorical data cannot be directly calculated. However, the distances between attribute values usually vary in different clusters induced by their different distributions, which has not been taken into account, thus leading to unreasonable distance measurement. Therefore, we propose a cluster-customized distance metric for categorical data clustering, which can competitively update distances based on different distributions of attributes in each cluster. In addition, we extend the proposed distance metric to the mixed data that contains both numerical and categorical attributes. Experiments demonstrate the efficacy of the proposed method, i.e., achieving an average ranking of around first in fourteen datasets. The source code is available at https://anonymous.4open.science/r/CADM-47D8
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Future by Retrieving the Past</title>
<link>https://arxiv.org/abs/2511.05859</link>
<guid>https://arxiv.org/abs/2511.05859</guid>
<content:encoded><![CDATA[
arXiv:2511.05859v1 Announce Type: new 
Abstract: Deep learning models such as MLP, Transformer, and TCN have achieved remarkable success in univariate time series forecasting, typically relying on sliding window samples from historical data for training. However, while these models implicitly compress historical information into their parameters during training, they are unable to explicitly and dynamically access this global knowledge during inference, relying only on the local context within the lookback window. This results in an underutilization of rich patterns from the global history. To bridge this gap, we propose Predicting the Future by Retrieving the Past (PFRP), a novel approach that explicitly integrates global historical data to enhance forecasting accuracy. Specifically, we construct a Global Memory Bank (GMB) to effectively store and manage global historical patterns. A retrieval mechanism is then employed to extract similar patterns from the GMB, enabling the generation of global predictions. By adaptively combining these global predictions with the outputs of any local prediction model, PFRP produces more accurate and interpretable forecasts. Extensive experiments conducted on seven real-world datasets demonstrate that PFRP significantly enhances the average performance of advanced univariate forecasting models by 8.4\%. Codes can be found in https://github.com/ddz16/PFRP.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning</title>
<link>https://arxiv.org/abs/2511.05863</link>
<guid>https://arxiv.org/abs/2511.05863</guid>
<content:encoded><![CDATA[
arXiv:2511.05863v1 Announce Type: new 
Abstract: Emotion recognition from EEG signals is essential for affective computing and has been widely explored using deep learning. While recent deep learning approaches have achieved strong performance on single EEG emotion datasets, their generalization across datasets remains limited due to the heterogeneity in annotation schemes and data formats. Existing models typically require dataset-specific architectures tailored to input structure and lack semantic alignment across diverse emotion labels. To address these challenges, we propose EMOD: A Unified EEG Emotion Representation Framework Leveraging Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and emotion-aware representations from heterogeneous datasets by bridging both semantic and structural gaps. Specifically, we project discrete and continuous emotion labels into a unified V-A space and formulate a soft-weighted supervised contrastive loss that encourages emotionally similar samples to cluster in the latent space. To accommodate variable EEG formats, EMOD employs a flexible backbone comprising a Triple-Domain Encoder followed by a Spatial-Temporal Transformer, enabling robust extraction and integration of temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG datasets and evaluate its performance on three benchmark datasets. Experimental results show that EMOD achieves state-of-the-art performance, demonstrating strong adaptability and generalization across diverse EEG-based emotion recognition scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem</title>
<link>https://arxiv.org/abs/2511.05872</link>
<guid>https://arxiv.org/abs/2511.05872</guid>
<content:encoded><![CDATA[
arXiv:2511.05872v1 Announce Type: new 
Abstract: Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for small to medium-sized tabular data, which has attracted much attention recently. This paper investigates the application of TabPFN in Combinatorial Optimization (CO) problems. The aim is to lessen challenges in time and data-intensive training requirements often observed in using traditional methods including exact and heuristic algorithms, Machine Learning (ML)-based models, to solve CO problems. Proposing possibly the first ever application of TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the Travelling Salesman Problem (TSP), one of the most well-known CO problems. Specifically, we adopt the node-based approach and the node-predicting adaptation strategy to construct the entire TSP route. Our evaluation with varying instance sizes confirms that TabPFN requires minimal training, adapts to TSP using a single sample, performs better generalization across varying TSP instance sizes, and reduces performance degradation. Furthermore, the training process with adaptation and fine-tuning is completed within minutes. The methodology leads to strong solution quality even without post-processing and achieves performance comparable to other models with post-processing refinement. Our findings suggest that the TabPFN model is a promising approach to solve structured and CO problems efficiently under training resource constraints and rapid deployment requirements.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge</title>
<link>https://arxiv.org/abs/2511.05878</link>
<guid>https://arxiv.org/abs/2511.05878</guid>
<content:encoded><![CDATA[
arXiv:2511.05878v1 Announce Type: new 
Abstract: Log-based anomaly detection is critical for ensuring the stability and reliability of web systems. One of the key problems in this task is the lack of sufficient labeled logs, which limits the rapid deployment in new systems. Existing works usually leverage large-scale labeled logs from a mature web system and a small amount of labeled logs from a new system, using transfer learning to extract and generalize general knowledge across both domains. However, these methods focus solely on the transfer of general knowledge and neglect the disparity and potential mismatch between such knowledge and the proprietary knowledge of target system, thus constraining performance. To address this limitation, we propose FusionLog, a novel zero-label cross-system log-based anomaly detection method that effectively achieves the fusion of general and proprietary knowledge, enabling cross-system generalization without any labeled target logs. Specifically, we first design a training-free router based on semantic similarity that dynamically partitions unlabeled target logs into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs a small model based on system-agnostic representation meta-learning for direct training and inference, inheriting the general anomaly patterns shared between the source and target systems. For proprietary logs, we iteratively generate pseudo-labels and fine-tune the small model using multi-round collaborative knowledge distillation and fusion based on large language model (LLM) and small model (SM) to enhance its capability to recognize anomaly patterns specific to the target system. Experimental results on three public log datasets from different systems show that FusionLog achieves over 90% F1-score under a fully zero-label setting, significantly outperforming state-of-the-art cross-system log-based anomaly detection methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation</title>
<link>https://arxiv.org/abs/2511.05879</link>
<guid>https://arxiv.org/abs/2511.05879</guid>
<content:encoded><![CDATA[
arXiv:2511.05879v1 Announce Type: new 
Abstract: Green hydrogen production via polymer electrolyte membrane (PEM) water electrolysis is pivotal for energy transition, yet hydrogen crossover through membranes threatens safety and economic viability-approaching explosive limits (4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current physics-based models require extensive calibration and computational resources that preclude real-time implementation, while purely data-driven approaches fail to extrapolate beyond training conditions-critical for dynamic electrolyzer operation. Here we present the first application of physics-informed neural networks (PINNs) for hydrogen crossover prediction, integrating mass conservation, Fick's diffusion law, and Henry's solubility law within a compact architecture (17,793 parameters). Validated across six membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar, 25-85{\deg}C), our PINN achieves exceptional accuracy (R$^2$ = 99.84%, RMSE = 0.0348%) with sub-millisecond inference times suitable for real-time control. Remarkably, the model maintains R$^2$ > 86% when predicting crossover at pressures 2.5x beyond training range-substantially outperforming pure neural networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs to edge devices (Raspberry Pi 4), enables distributed safety monitoring essential for gigawatt-scale installations. By bridging physical rigor and computational efficiency, this work establishes a new paradigm for real-time electrolyzer monitoring, accelerating deployment of safe, efficient green hydrogen infrastructure crucial for net-zero emissions targets.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Kernels to Attention: A Transformer Framework for Density and Score Estimation</title>
<link>https://arxiv.org/abs/2511.05924</link>
<guid>https://arxiv.org/abs/2511.05924</guid>
<content:encoded><![CDATA[
arXiv:2511.05924v1 Announce Type: new 
Abstract: We introduce a unified attention-based framework for joint score and density estimation. Framing the problem as a sequence-to-sequence task, we develop a permutation- and affine-equivariant transformer that estimates both the probability density $f(x)$ and its score $\nabla_x \log f(x)$ directly from i.i.d. samples. Unlike traditional score-matching methods that require training a separate model for each distribution, our approach learns a single distribution-agnostic operator that generalizes across densities and sample sizes. The architecture employs cross-attention to connect observed samples with arbitrary query points, enabling generalization beyond the training data, while built-in symmetry constraints ensure equivariance to permutation and affine transformations. Analytically, we show that the attention weights can recover classical kernel density estimation (KDE), and verify it empirically, establishing a principled link between classical KDE and the transformer architecture. Empirically, the model achieves substantially lower error and better scaling than KDE and score-debiased KDE (SD-KDE), while exhibiting better runtime scaling. Together, these results establish transformers as general-purpose, data-adaptive operators for nonparametric density and score estimation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients</title>
<link>https://arxiv.org/abs/2511.05960</link>
<guid>https://arxiv.org/abs/2511.05960</guid>
<content:encoded><![CDATA[
arXiv:2511.05960v1 Announce Type: new 
Abstract: Patients with chronic obstructive pulmonary disease (COPD) have an increased risk of hospitalizations, strongly associated with decreased survival, yet predicting the timing of these events remains challenging and has received limited attention in the literature. In this study, we performed survival analysis to predict hospitalization and death in COPD patients using longitudinal electronic health records (EHRs), comparing statistical models, machine learning (ML), and deep learning (DL) approaches. We analyzed data from more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013 to 2017, modeling hospitalization as a first event and death as a semi-competing terminal event. Multiple models were evaluated, including Cox proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit, and Deep Recurrent Survival Machine. Results showed that DL models utilizing recurrent architectures outperformed both ML and linear approaches in concordance and time-dependent AUC, especially for hospitalization, which proved to be the harder event to predict. This study is, to our knowledge, the first to apply deep survival analysis on longitudinal EHR data to jointly predict multiple time-to-event outcomes in COPD patients, highlighting the potential of DL approaches to capture temporal patterns and improve risk stratification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Latent Prediction Transformers Learn Compact World Models</title>
<link>https://arxiv.org/abs/2511.05963</link>
<guid>https://arxiv.org/abs/2511.05963</guid>
<content:encoded><![CDATA[
arXiv:2511.05963v1 Announce Type: new 
Abstract: Transformers replace recurrence with a memory that grows with sequence length and self-attention that enables ad-hoc look ups over past tokens. Consequently, they lack an inherent incentive to compress history into compact latent states with consistent transition rules. This often leads to learning solutions that generalize poorly. We introduce Next-Latent Prediction (NextLat), which extends standard next-token training with self-supervised predictions in the latent space. Specifically, NextLat trains a transformer to learn latent representations that are predictive of its next latent state given the next output token. Theoretically, we show that these latents provably converge to belief states, compressed information of the history necessary to predict the future. This simple auxiliary objective also injects a recurrent inductive bias into transformers, while leaving their architecture, parallel training, and inference unchanged. NextLat effectively encourages the transformer to form compact internal world models with its own belief states and transition dynamics -- a crucial property absent in standard next-token prediction transformers. Empirically, across benchmarks targeting core sequence modeling competencies -- world modeling, reasoning, planning, and language modeling -- NextLat demonstrates significant gains over standard next-token training in downstream accuracy, representation compression, and lookahead planning. NextLat stands as a simple and efficient paradigm for shaping transformer representations toward stronger generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals</title>
<link>https://arxiv.org/abs/2511.05973</link>
<guid>https://arxiv.org/abs/2511.05973</guid>
<content:encoded><![CDATA[
arXiv:2511.05973v1 Announce Type: new 
Abstract: Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP) disorder caused by the presence of an accessory pathway (AP) that bypasses the atrioventricular node, faster ventricular activation rate, and provides a substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate localization of the AP is critical for planning and guiding catheter ablation procedures. While traditional diagnostic tree (DT) methods and more recent machine learning (ML) approaches have been proposed to predict AP location from surface electrocardiogram (ECG), they are often constrained by limited anatomical localization resolution, poor interpretability, and the use of small clinical datasets. In this study, we present a Deep Learning (DL) model for the localization of single manifest APs across 24 cardiac regions, trained on a large, physiologically realistic database of synthetic ECGs generated using a personalized virtual heart model. We also integrate eXplainable Artificial Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided Grad-CAM, into the pipeline. This enables interpretation of DL decision-making and addresses one of the main barriers to clinical adoption: lack of transparency in ML predictions. Our model achieves localization accuracy above 95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are physiologically validated against known depolarization patterns, and a novel index is introduced to identify the most informative ECG leads for AP localization. Results highlight lead V2 as the most critical, followed by aVF, V1, and aVL. This work demonstrates the potential of combining cardiac digital twins with explainable DL to enable accurate, transparent, and non-invasive AP localization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference</title>
<link>https://arxiv.org/abs/2511.05978</link>
<guid>https://arxiv.org/abs/2511.05978</guid>
<content:encoded><![CDATA[
arXiv:2511.05978v1 Announce Type: new 
Abstract: Anomaly troubleshooting for large model distributed inference (LMDI) remains a critical challenge. Resolving anomalies such as inference performance degradation or latency jitter in distributed system demands significant manual efforts from domain experts, resulting in extremely time-consuming diagnosis processes with relatively low accuracy. In this paper, we introduce Kunlun Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework tailored for LMDI. KAT addresses this problem through two core innovations. First, KAT exploits the synchronicity and consistency of GPU workers, innovatively leverages function trace data to precisely detect kernel-level anomalies and associated hardware components at nanosecond resolution. Second, KAT integrates these detection results into a domain-adapted LLM, delivering systematic causal reasoning and natural language interpretation of complex anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production environment indicate that KAT achieves over 0.884 precision and 0.936 recall in anomaly detection, providing detail anomaly insights that significantly narrow down the diagnostic scope and improve both the efficiency and success rate of troubleshooting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Time-Indexed Foundation Models the Future of Time Series Imputation?</title>
<link>https://arxiv.org/abs/2511.05980</link>
<guid>https://arxiv.org/abs/2511.05980</guid>
<content:encoded><![CDATA[
arXiv:2511.05980v1 Announce Type: new 
Abstract: Foundation models for time series imputation remain largely unexplored. Recently, two such models, TabPFN-TS and MoTM, have emerged. These models share a common philosophy that places them within the family of time-indexed foundation models. This paper presents the first large-scale empirical study of these models for zero-shot imputation, which enables missing value recovery without retraining across a wide range of scenarios. We conduct extensive univariate experiments across 33 out-of-domain datasets (approximately 1.3M imputation windows) and evaluate their ability to integrate covariates at inference time to improve accuracy without fine-tuning. Our results demonstrate that time-indexed foundation models are a powerful and practical step toward achieving general-purpose, zero-shot imputation for real-world time series.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bespoke Co-processor for Energy-Efficient Health Monitoring on RISC-V-based Flexible Wearables</title>
<link>https://arxiv.org/abs/2511.05985</link>
<guid>https://arxiv.org/abs/2511.05985</guid>
<content:encoded><![CDATA[
arXiv:2511.05985v1 Announce Type: new 
Abstract: Flexible electronics offer unique advantages for conformable, lightweight, and disposable healthcare wearables. However, their limited gate count, large feature sizes, and high static power consumption make on-body machine learning classification highly challenging. While existing bendable RISC-V systems provide compact solutions, they lack the energy efficiency required. We present a mechanically flexible RISC-V that integrates a bespoke multiply-accumulate co-processor with fixed coefficients to maximize energy efficiency and minimize latency. Our approach formulates a constrained programming problem to jointly determine co-processor constants and optimally map Multi-Layer Perceptron (MLP) inference operations, enabling compact, model-specific hardware by leveraging the low fabrication and non-recurring engineering costs of flexible technologies. Post-layout results demonstrate near-real-time performance across several healthcare datasets, with our circuits operating within the power budget of existing flexible batteries and occupying only 2.42 mm^2, offering a promising path toward accessible, sustainable, and conformable healthcare wearables. Our microprocessors achieve an average 2.35x speedup and 2.15x lower energy consumption compared to the state of the art.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference</title>
<link>https://arxiv.org/abs/2511.06010</link>
<guid>https://arxiv.org/abs/2511.06010</guid>
<content:encoded><![CDATA[
arXiv:2511.06010v1 Announce Type: new 
Abstract: The escalating context length in Large Language Models (LLMs) creates a severe performance bottleneck around the Key-Value (KV) cache, whose memory-bound nature leads to significant GPU under-utilization. This paper introduces Mixture of Shared KV Attention (MoSKA), an architecture that addresses this challenge by exploiting the heterogeneity of context data. It differentiates between per-request unique and massively reused shared sequences. The core of MoSKA is a novel Shared KV Attention mechanism that transforms the attention on shared data from a series of memory-bound GEMV operations into a single, compute-bound GEMM by batching concurrent requests. This is supported by an MoE-inspired sparse attention strategy that prunes the search space and a tailored Disaggregated Infrastructure that specializes hardware for unique and shared data. This comprehensive approach demonstrates a throughput increase of up to 538.7x over baselines in workloads with high context sharing, offering a clear architectural path toward scalable LLM inference.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving</title>
<link>https://arxiv.org/abs/2511.06029</link>
<guid>https://arxiv.org/abs/2511.06029</guid>
<content:encoded><![CDATA[
arXiv:2511.06029v1 Announce Type: new 
Abstract: Generative reasoning with large language models (LLMs) often involves long decoding sequences, leading to substantial memory and latency overheads from accumulating key-value (KV) caches. While existing KV compression methods primarily focus on reducing prefill memory from long input sequences, they fall short in addressing the dynamic and layer-sensitive nature of long-form generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV cache management framework that introduces adaptivity along both the spatial and temporal dimensions of decoding. Along the spatial dimension, Lethe performs layerwise sparsity-aware allocation, assigning token pruning budgets to each transformer layer based on estimated attention redundancy. Along the temporal dimension, Lethe conducts multi-round token pruning during generation, driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends traditional recency-based heuristics by also considering token relevance derived from evolving attention patterns, enabling informed decisions about which tokens to retain or evict. Empirical results demonstrate that Lethe achieves a favorable balance between efficiency and generation quality across diverse models and tasks, increases throughput by up to 2.56x.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes</title>
<link>https://arxiv.org/abs/2511.06032</link>
<guid>https://arxiv.org/abs/2511.06032</guid>
<content:encoded><![CDATA[
arXiv:2511.06032v1 Announce Type: new 
Abstract: Marked Temporal Point Processes (MTPPs) provide a principled framework for modeling asynchronous event sequences by conditioning on the history of past events. However, most existing MTPP models rely on channel-mixing strategies that encode information from different event types into a single, fixed-size latent representation. This entanglement can obscure type-specific dynamics, leading to performance degradation and increased risk of overfitting. In this work, we introduce ITPP, a novel channel-independent architecture for MTPP modeling that decouples event type information using an encoder-decoder framework with an ODE-based backbone. Central to ITPP is a type-aware inverted self-attention mechanism, designed to explicitly model inter-channel correlations among heterogeneous event types. This architecture enhances effectiveness and robustness while reducing overfitting. Comprehensive experiments on multiple real-world and synthetic datasets demonstrate that ITPP consistently outperforms state-of-the-art MTPP models in both predictive accuracy and generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Ocean State Estimation with efficient and scalable AI</title>
<link>https://arxiv.org/abs/2511.06041</link>
<guid>https://arxiv.org/abs/2511.06041</guid>
<content:encoded><![CDATA[
arXiv:2511.06041v1 Announce Type: new 
Abstract: Accurate and efficient global ocean state estimation remains a grand challenge for Earth system science, hindered by the dual bottlenecks of computational scalability and degraded data fidelity in traditional data assimilation (DA) and deep learning (DL) approaches. Here we present an AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly assimilates multi-source and multi-scale observations, ranging from sparse in-situ measurements to 4 km satellite swaths, without any interpolation or data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous mapping from heterogeneous inputs to ocean states, preserving native data fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\circ$ mesoscale dynamics from coarse 1$^\circ$ fields, which ensures both efficiency and scalability, with just 3.7\% more parameters than the 1$^\circ$ configuration. When coupled with a DL forecasting system, ADAF-Ocean extends global forecast skill by up to 20 days compared to baselines without assimilation. This framework establishes a computationally viable and scientifically rigorous pathway toward real-time, high-resolution Earth system monitoring.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Design of Input Convex Neural Networks for Consistency Optimal Transport Flow Matching</title>
<link>https://arxiv.org/abs/2511.06042</link>
<guid>https://arxiv.org/abs/2511.06042</guid>
<content:encoded><![CDATA[
arXiv:2511.06042v1 Announce Type: new 
Abstract: We propose a consistency model based on the optimal-transport flow. A physics-informed design of partially input-convex neural networks (PICNN) plays a central role in constructing the flow field that emulates the displacement interpolation. During the training stage, we couple the Hamilton-Jacobi (HJ) residual in the OT formulation with the original flow matching loss function. Our approach avoids inner optimization subproblems that are present in previous one-step OFM approaches. During the prediction stage, our approach supports both one-step (Brenier-map) and multi-step ODE sampling from the same learned potential, leveraging the straightness of the OT flow. We validate scalability and performance on standard OT benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy</title>
<link>https://arxiv.org/abs/2511.06044</link>
<guid>https://arxiv.org/abs/2511.06044</guid>
<content:encoded><![CDATA[
arXiv:2511.06044v1 Announce Type: new 
Abstract: Attention mechanism is a significant part of Transformer models. It helps extract features from embedded vectors by adding global information and its expressivity has been proved to be powerful. Nevertheless, the quadratic complexity restricts its practicability. Although several researches have provided attention mechanism in sparse form, they are lack of theoretical analysis about the expressivity of their mechanism while reducing complexity. In this paper, we put forward Random Batch Attention (RBA), a linear self-attention mechanism, which has theoretical support of the ability to maintain its expressivity. Random Batch Attention has several significant strengths as follows: (1) Random Batch Attention has linear time complexity. Other than this, it can be implemented in parallel on a new dimension, which contributes to much memory saving. (2) Random Batch Attention mechanism can improve most of the existing models by replacing their attention mechanisms, even many previously improved attention mechanisms. (3) Random Batch Attention mechanism has theoretical explanation in convergence, as it comes from Random Batch Methods on computation mathematics. Experiments on large graphs have proved advantages mentioned above. Also, the theoretical modeling of self-attention mechanism is a new tool for future research on attention-mechanism analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection</title>
<link>https://arxiv.org/abs/2511.06054</link>
<guid>https://arxiv.org/abs/2511.06054</guid>
<content:encoded><![CDATA[
arXiv:2511.06054v1 Announce Type: new 
Abstract: Anomaly Detection (AD) is evolving through algorithms capable of identifying outliers in complex datasets. The Isolation Forest (IF), a pivotal AD technique, exhibits adaptability limitations and biases. This paper introduces the Function-based Isolation Forest (FuBIF), a generalization of IF that enables the use of real-valued functions for dataset branching, significantly enhancing the flexibility of evaluation tree construction. Complementing this, the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability in IF-based approaches by providing feature importance scores across possible FuBIF models. This paper details the operational framework of FuBIF, evaluates its performance against established methods, and explores its theoretical contributions. An open-source implementation is provided to encourage further research and ensure reproducibility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding</title>
<link>https://arxiv.org/abs/2511.06072</link>
<guid>https://arxiv.org/abs/2511.06072</guid>
<content:encoded><![CDATA[
arXiv:2511.06072v1 Announce Type: new 
Abstract: Backdoor attacks in machine learning have drawn significant attention for their potential to compromise models stealthily, yet most research has focused on homogeneous data such as images. In this work, we propose a novel backdoor attack on tabular data, which is particularly challenging due to the presence of both numerical and categorical features. Our key idea is a novel technique to convert categorical values into floating-point representations. This approach preserves enough information to maintain clean-model accuracy compared to traditional methods like one-hot or ordinal encoding. By doing this, we create a gradient-based universal perturbation that applies to all features, including categorical ones.
  We evaluate our method on five datasets and four popular models. Our results show up to a 100% attack success rate in both white-box and black-box settings (including real-world applications like Vertex AI), revealing a severe vulnerability for tabular data. Our method is shown to surpass the previous works like Tabdoor in terms of performance, while remaining stealthy against state-of-the-art defense mechanisms. We evaluate our attack against Spectral Signatures, Neural Cleanse, Beatrix, and Fine-Pruning, all of which fail to defend successfully against it. We also verify that our attack successfully bypasses popular outlier detection mechanisms.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin</title>
<link>https://arxiv.org/abs/2511.06077</link>
<guid>https://arxiv.org/abs/2511.06077</guid>
<content:encoded><![CDATA[
arXiv:2511.06077v1 Announce Type: new 
Abstract: Short-video recommenders such as Douyin must exploit extremely long user histories without breaking latency or cost budgets. We present an end-to-end system that scales long-sequence modeling to 10k-length histories in production. First, we introduce Stacked Target-to-History Cross Attention (STCA), which replaces history self-attention with stacked cross-attention from the target to the history, reducing complexity from quadratic to linear in sequence length and enabling efficient end-to-end training. Second, we propose Request Level Batching (RLB), a user-centric batching scheme that aggregates multiple targets for the same user/request to share the user-side encoding, substantially lowering sequence-related storage, communication, and compute without changing the learning objective. Third, we design a length-extrapolative training strategy -- train on shorter windows, infer on much longer ones -- so the model generalizes to 10k histories without additional training cost. Across offline and online experiments, we observe predictable, monotonic gains as we scale history length and model capacity, mirroring the scaling law behavior observed in large language models. Deployed at full traffic on Douyin, our system delivers significant improvements on key engagement metrics while meeting production latency, demonstrating a practical path to scaling end-to-end long-sequence recommendation to the 10k regime.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Event-driven physics-informed operator learning for reliability analysis</title>
<link>https://arxiv.org/abs/2511.06083</link>
<guid>https://arxiv.org/abs/2511.06083</guid>
<content:encoded><![CDATA[
arXiv:2511.06083v1 Announce Type: new 
Abstract: Reliability analysis of engineering systems under uncertainty poses significant computational challenges, particularly for problems involving high-dimensional stochastic inputs, nonlinear system responses, and multiphysics couplings. Traditional surrogate modeling approaches often incur high energy consumption, which severely limits their scalability and deployability in resource-constrained environments. We introduce NeuroPOL, \textit{the first neuroscience-inspired physics-informed operator learning framework} for reliability analysis. NeuroPOL incorporates Variable Spiking Neurons into a physics-informed operator architecture, replacing continuous activations with event-driven spiking dynamics. This innovation promotes sparse communication, significantly reduces computational load, and enables an energy-efficient surrogate model. The proposed framework lowers both computational and power demands, supporting real-time reliability assessment and deployment on edge devices and digital twins. By embedding governing physical laws into operator learning, NeuroPOL builds physics-consistent surrogates capable of accurate uncertainty propagation and efficient failure probability estimation, even for high-dimensional problems. We evaluate NeuroPOL on five canonical benchmarks, the Burgers equation, Nagumo equation, two-dimensional Poisson equation, two-dimensional Darcy equation, and incompressible Navier-Stokes equation with energy coupling. Results show that NeuroPOL achieves reliability measures comparable to standard physics-informed operators, while introducing significant communication sparsity, enabling scalable, distributed, and energy-efficient deployment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximating Shapley Explanations in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06094</link>
<guid>https://arxiv.org/abs/2511.06094</guid>
<content:encoded><![CDATA[
arXiv:2511.06094v1 Announce Type: new 
Abstract: Reinforcement learning has achieved remarkable success in complex decision-making environments, yet its lack of transparency limits its deployment in practice, especially in safety-critical settings. Shapley values from cooperative game theory provide a principled framework for explaining reinforcement learning; however, the computational cost of Shapley explanations is an obstacle to their use. We introduce FastSVERL, a scalable method for explaining reinforcement learning by approximating Shapley values. FastSVERL is designed to handle the unique challenges of reinforcement learning, including temporal dependencies across multi-step trajectories, learning from off-policy data, and adapting to evolving agent behaviours in real time. FastSVERL introduces a practical, scalable approach for principled and rigorous interpretability in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Web Agents with Synthetic Supervision</title>
<link>https://arxiv.org/abs/2511.06101</link>
<guid>https://arxiv.org/abs/2511.06101</guid>
<content:encoded><![CDATA[
arXiv:2511.06101v1 Announce Type: new 
Abstract: Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, we refine tasks when conflicts with actual observations are detected, mitigating hallucinations while maintaining task consistency. After collection, we conduct trajectory refinement with a global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code will be publicly available at https://github.com/aiming-lab/SynthAgent.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices</title>
<link>https://arxiv.org/abs/2511.06111</link>
<guid>https://arxiv.org/abs/2511.06111</guid>
<content:encoded><![CDATA[
arXiv:2511.06111v1 Announce Type: new 
Abstract: We study the sequential decision-making problem for automated weaning of mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS devices are percutaneous micro-axial flow pumps that provide left ventricular unloading and forward blood flow, but current weaning strategies vary significantly across care teams and lack data-driven approaches. Offline reinforcement learning (RL) has proven to be successful in sequential decision-making tasks, but our setting presents challenges for training and evaluating traditional offline RL methods: prohibition of online patient interaction, highly uncertain circulatory dynamics due to concurrent treatments, and limited data availability. We developed an end-to-end machine learning framework with two key contributions (1) Clinically-aware OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized offline RL algorithm for out-of-distribution suppression that also incorporates clinically-informed reward shaping and (2) a Transformer-based probabilistic digital twin that models MCS circulatory dynamics for policy evaluation with rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves theoretical performance guarantees under mild assumptions. CORMPO attains a higher reward than the offline RL baselines by 28% and higher scores in clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a principled framework for safe offline policy learning in high-stakes medical applications where domain expertise and safety constraints are essential.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Convergence and Stability of Distributed Sub-model Training</title>
<link>https://arxiv.org/abs/2511.06132</link>
<guid>https://arxiv.org/abs/2511.06132</guid>
<content:encoded><![CDATA[
arXiv:2511.06132v1 Announce Type: new 
Abstract: As learning models continue to grow in size, enabling on-device local training of these models has emerged as a critical challenge in federated learning. A popular solution is sub-model training, where the server only distributes randomly sampled sub-models to the edge clients, and clients only update these small models. However, those random sampling of sub-models may not give satisfying convergence performance. In this paper, observing the success of SGD with shuffling, we propose a distributed shuffled sub-model training, where the full model is partitioned into several sub-models in advance, and the server shuffles those sub-models, sends each of them to clients at each round, and by the end of local updating period, clients send back the updated sub-models, and server averages them. We establish the convergence rate of this algorithm. We also study the generalization of distributed sub-model training via stability analysis, and find that the sub-model training can improve the generalization via amplifying the stability of training process. The extensive experiments also validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Robustness of Graph Neural Networks through p-Laplacian</title>
<link>https://arxiv.org/abs/2511.06143</link>
<guid>https://arxiv.org/abs/2511.06143</guid>
<content:encoded><![CDATA[
arXiv:2511.06143v1 Announce Type: new 
Abstract: With the increase of data in day-to-day life, businesses and different stakeholders need to analyze the data for better pre- dictions. Traditionally, relational data has been a source of various insights, but with the increase in computational power and the need to understand deeper relationships between en- tities, the need to design new techniques has arisen. For this graph data analysis has become an extraordinary tool for un- derstanding the data, which reveals more realistic and flexible modelling of complex relationships. Recently, Graph Neural Networks (GNNs) have shown great promise in various ap- plications, such as social network analysis, recommendation systems, drug discovery, and more. However, many adversar- ial attacks can happen over the data, whether during training (poisoning attack) or during testing (evasion attack), which can adversely manipulate the desired outcome from the GNN model. Therefore, it is crucial to make the GNNs robust to such attacks. The existing robustness methods are computa- tionally demanding and perform poorly when the intensity of attack increases. This paper presents a computationally ef- ficient framework, namely, pLAPGNN, based on weighted p-Laplacian for making GNNs robust. Empirical evaluation on real datasets establishes the efficacy and efficiency of the proposed method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Models Got Talent: Identifying High Performing Wearable Human Activity Recognition Models Without Training</title>
<link>https://arxiv.org/abs/2511.06157</link>
<guid>https://arxiv.org/abs/2511.06157</guid>
<content:encoded><![CDATA[
arXiv:2511.06157v1 Announce Type: new 
Abstract: A promising alternative to the computationally expensive Neural Architecture Search (NAS) involves the development of \textit{Zero Cost Proxies (ZCPs)}, which correlate well to trained performance, but can be computed through a single forward/backward pass on a randomly sampled batch of data. In this paper, we investigate the effectiveness of ZCPs for HAR on six benchmark datasets, and demonstrate that they discover network architectures that obtain within 5\% of performance attained by full scale training involving 1500 randomly sampled architectures. This results in substantial computational savings as high performing architectures can be discovered with minimal training. Our experiments not only introduce ZCPs to sensor-based HAR, but also demonstrate that they are robust to data noise, further showcasing their suitability for practical scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains</title>
<link>https://arxiv.org/abs/2511.06161</link>
<guid>https://arxiv.org/abs/2511.06161</guid>
<content:encoded><![CDATA[
arXiv:2511.06161v1 Announce Type: new 
Abstract: Transfer learning of tabular data is non-trivial due to heterogeneity in the feature space across disparate domains. The limited success of traditional deep learning in tabular knowledge transfer can be advanced by leveraging large language models (LLMs). However, the efficacy of LLMs often stagnates for mixed data types structured in tables due to the limitations of text prompts and in-context learning. We propose a lightweight transfer learning framework that fine-tunes an LLM using source tabular data and transplants the LLM's selective $key$ and $value$ projection weights into a gated feature tokenized transformer (gFTT) built for tabular data. The gFTT model with cross-domain attention is fine-tuned using target tabular data for transfer learning, eliminating the need for shared features, LLM prompt engineering, and large-scale pretrained models. Our experiments using ten pairs of source-target data sets and 12 baselines demonstrate the superiority of the proposed LLM-attention transplant for transfer learning (LATTLE) method over traditional ML models, state-of-the-art deep tabular architectures, and transfer learning models trained on thousands to billions of tabular samples. The proposed attention transfer demonstrates an effective solution to learning relationships between data tables using an LLM in a low-resource learning environment. The source code for the proposed method is publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Gaussian DAG Models without Condition Number Bounds</title>
<link>https://arxiv.org/abs/2511.06164</link>
<guid>https://arxiv.org/abs/2511.06164</guid>
<content:encoded><![CDATA[
arXiv:2511.06164v1 Announce Type: new 
Abstract: We study the problem of learning the topology of a directed Gaussian Graphical Model under the equal-variance assumption, where the graph has $n$ nodes and maximum in-degree $d$. Prior work has established that $O(d \log n)$ samples are sufficient for this task. However, an important factor that is often overlooked in these analyses is the dependence on the condition number of the covariance matrix of the model. Indeed, all algorithms from prior work require a number of samples that grows polynomially with this condition number. In many cases this is unsatisfactory, since the condition number could grow polynomially with $n$, rendering these prior approaches impractical in high-dimensional settings. In this work, we provide an algorithm that recovers the underlying graph and prove that the number of samples required is independent of the condition number. Furthermore, we establish lower bounds that nearly match the upper bound up to a $d$-factor, thus providing an almost tight characterization of the true sample complexity of the problem. Moreover, under a further assumption that all the variances of the variables are bounded, we design a polynomial-time algorithm that recovers the underlying graph, at the cost of an additional polynomial dependence of the sample complexity on $d$. We complement our theoretical findings with simulations on synthetic datasets that confirm our predictions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local K-Similarity Constraint for Federated Learning with Label Noise</title>
<link>https://arxiv.org/abs/2511.06169</link>
<guid>https://arxiv.org/abs/2511.06169</guid>
<content:encoded><![CDATA[
arXiv:2511.06169v1 Announce Type: new 
Abstract: Federated learning on clients with noisy labels is a challenging problem, as such clients can infiltrate the global model, impacting the overall generalizability of the system. Existing methods proposed to handle noisy clients assume that a sufficient number of clients with clean labels are available, which can be leveraged to learn a robust global model while dampening the impact of noisy clients. This assumption fails when a high number of heterogeneous clients contain noisy labels, making the existing approaches ineffective. In such scenarios, it is important to locally regularize the clients before communication with the global model, to ensure the global model isn't corrupted by noisy clients. While pre-trained self-supervised models can be effective for local regularization, existing centralized approaches relying on pretrained initialization are impractical in a federated setting due to the potentially large size of these models, which increases communication costs. In that line, we propose a regularization objective for client models that decouples the pre-trained and classification models by enforcing similarity between close data points within the client. We leverage the representation space of a self-supervised pretrained model to evaluate the closeness among examples. This regularization, when applied with the standard objective function for the downstream task in standard noisy federated settings, significantly improves performance, outperforming existing state-of-the-art federated methods in multiple computer vision and medical image classification benchmarks. Unlike other techniques that rely on self-supervised pretrained initialization, our method does not require the pretrained model and classifier backbone to share the same architecture, making it architecture-agnostic.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilience Inference for Supply Chains with Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2511.06208</link>
<guid>https://arxiv.org/abs/2511.06208</guid>
<content:encoded><![CDATA[
arXiv:2511.06208v1 Announce Type: new 
Abstract: Supply chains are integral to global economic stability, yet disruptions can swiftly propagate through interconnected networks, resulting in substantial economic impacts. Accurate and timely inference of supply chain resilience the capability to maintain core functions during disruptions is crucial for proactive risk mitigation and robust network design. However, existing approaches lack effective mechanisms to infer supply chain resilience without explicit system dynamics and struggle to represent the higher-order, multi-entity dependencies inherent in supply chain networks. These limitations motivate the definition of a novel problem and the development of targeted modeling solutions. To address these challenges, we formalize a novel problem: Supply Chain Resilience Inference (SCRI), defined as predicting supply chain resilience using hypergraph topology and observed inventory trajectories without explicit dynamic equations. To solve this problem, we propose the Supply Chain Resilience Inference Hypergraph Network (SC-RIHN), a novel hypergraph-based model leveraging set-based encoding and hypergraph message passing to capture multi-party firm-product interactions. Comprehensive experiments demonstrate that SC-RIHN significantly outperforms traditional MLP, representative graph neural network variants, and ResInf baselines across synthetic benchmarks, underscoring its potential for practical, early-warning risk assessment in complex supply chain systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Linear Regression is Easy on Random Supports</title>
<link>https://arxiv.org/abs/2511.06211</link>
<guid>https://arxiv.org/abs/2511.06211</guid>
<content:encoded><![CDATA[
arXiv:2511.06211v1 Announce Type: new 
Abstract: Sparse linear regression is one of the most basic questions in machine learning and statistics. Here, we are given as input a design matrix $X \in \mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$ where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements. Importantly, we have the additional constraint that the unknown signal vector ${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than the ambient dimension. Our goal is to output a prediction vector $\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^* - {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of measurements: under most natural noise distributions, we can get prediction error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples. Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately, with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap (in the dependence on $d$) between the two and we do not know if it is possible to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices ${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d, 1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for random design matrices from well-behaved families, matrices that have a very low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed sensing), or those with special structural properties.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks</title>
<link>https://arxiv.org/abs/2511.06216</link>
<guid>https://arxiv.org/abs/2511.06216</guid>
<content:encoded><![CDATA[
arXiv:2511.06216v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) learns node and graph representations by contrasting multiple views of the same graph. Existing methods typically rely on fixed, handcrafted views-usually a local and a global perspective, which limits their ability to capture multi-scale structural patterns. We present an augmentation-free, multi-view GCL framework grounded in fractional-order continuous dynamics. By varying the fractional derivative order $\alpha \in (0,1]$, our encoders produce a continuous spectrum of views: small $\alpha$ yields localized features, while large $\alpha$ induces broader, global aggregation. We treat $\alpha$ as a learnable parameter so the model can adapt diffusion scales to the data and automatically discover informative views. This principled approach generates diverse, complementary representations without manual augmentations. Extensive experiments on standard benchmarks demonstrate that our method produces more robust and expressive embeddings and outperforms state-of-the-art GCL baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Dynamic Origin-Destination Matrix Estimation in Microscopic Traffic Simulations Considering Credit Assignment</title>
<link>https://arxiv.org/abs/2511.06229</link>
<guid>https://arxiv.org/abs/2511.06229</guid>
<content:encoded><![CDATA[
arXiv:2511.06229v1 Announce Type: new 
Abstract: This paper focuses on dynamic origin-destination matrix estimation (DODE), a crucial calibration process necessary for the effective application of microscopic traffic simulations. The fundamental challenge of the DODE problem in microscopic simulations stems from the complex temporal dynamics and inherent uncertainty of individual vehicle dynamics. This makes it highly challenging to precisely determine which vehicle traverses which link at any given moment, resulting in intricate and often ambiguous relationships between origin-destination (OD) matrices and their contributions to resultant link flows. This phenomenon constitutes the credit assignment problem, a central challenge addressed in this study. We formulate the DODE problem as a Markov Decision Process (MDP) and propose a novel framework that applies model-free deep reinforcement learning (DRL). Within our proposed framework, the agent learns an optimal policy to sequentially generate OD matrices, refining its strategy through direct interaction with the simulation environment. The proposed method is validated on the Nguyen-Dupuis network using SUMO, where its performance is evaluated against ground-truth link flows aggregated at 5-minute intervals over a 30-minute horizon. Experimental results demonstrate that our approach achieves a 43.2% reduction in mean squared error (MSE) compared to the best-performing conventional baseline. By reframing DODE as a sequential decision-making problem, our approach addresses the credit assignment challenge through its learned policy, thereby overcoming the limitations of conventional methods and proposing a novel framework for calibration of microscopic traffic simulations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals</title>
<link>https://arxiv.org/abs/2511.06231</link>
<guid>https://arxiv.org/abs/2511.06231</guid>
<content:encoded><![CDATA[
arXiv:2511.06231v1 Announce Type: new 
Abstract: Human-computer interaction increasingly demands systems that recognize not only explicit user inputs but also implicit emotional states. While substantial progress has been made in affective computing, most emotion recognition systems rely on cloud-based inference, introducing privacy vulnerabilities and latency constraints unsuitable for real-time applications. This work presents a comprehensive evaluation of machine learning architectures for on-device emotion recognition from wrist-based photoplethysmography (PPG), systematically comparing different models spanning classical ensemble methods, deep neural networks, and transformers on the WESAD stress detection dataset. Results demonstrate that classical ensemble methods substantially outperform deep learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826 on combined features and F1 = 0.623 on wrist-only features, compared to transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint, 0.05 ms inference latency, and 152x speedup over the original implementation. Furthermore, ONNX optimization yields a 30.5% average storage reduction and 40.1x inference speedup, highlighting the feasibility of privacy-preserving on-device emotion recognition for real-world wearables.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Laws and In-Context Learning: A Unified Theoretical Framework</title>
<link>https://arxiv.org/abs/2511.06232</link>
<guid>https://arxiv.org/abs/2511.06232</guid>
<content:encoded><![CDATA[
arXiv:2511.06232v1 Announce Type: new 
Abstract: In-context learning (ICL) enables large language models to adapt to new tasks from demonstrations without parameter updates. Despite extensive empirical studies, a principled understanding of ICL emergence at scale remains more elusive. We present a unified theoretical framework connecting scaling laws to ICL emergence in transformers. Our analysis establishes that ICL performance follows power-law relationships with model depth $L$, width $d$, context length $k$, and training data $D$, with exponents determined by task structure. We show that under specific conditions, transformers implement gradient-based metalearning in their forward pass, with an effective learning rate $\eta_{\text{eff}} = \Theta(1/\sqrt{Ld})$. We demonstrate sharp phase transitions at critical scales and derive optimal depth-width allocations favoring $L^* \propto N^{2/3}$, $d^* \propto N^{1/3}$ for the fixed parameter budget $N = Ld$. Systematic experiments on synthetic tasks validate our predictions, with measured scaling exponents closely matching theory. This work provides both necessary and sufficient conditions for the emergence of ICLs and establishes fundamental computational limits on what transformers can learn in-context.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixtures of SubExperts for Large Language Continual Learning</title>
<link>https://arxiv.org/abs/2511.06237</link>
<guid>https://arxiv.org/abs/2511.06237</guid>
<content:encoded><![CDATA[
arXiv:2511.06237v1 Announce Type: new 
Abstract: Adapting Large Language Models (LLMs) to a continuous stream of tasks is a critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT) methods have become a standard for this, they face a fundamental dilemma in continual learning. Reusing a single set of PEFT parameters for new tasks often leads to catastrophic forgetting of prior knowledge. Conversely, allocating distinct parameters for each task prevents forgetting but results in a linear growth of the model's size and fails to facilitate knowledge transfer between related tasks. To overcome these limitations, we propose a novel adaptive PEFT method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel continual learning framework designed for minimal forgetting and efficient scalability. MoSEs integrate a sparse Mixture of SubExperts into the transformer layers, governed by a task-specific routing mechanism. This architecture allows the model to isolate and protect knowledge within dedicated SubExperts, thereby minimizing parameter interference and catastrophic forgetting. Crucially, the router can adaptively select and combine previously learned sparse parameters for new tasks, enabling effective knowledge transfer while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that MoSEs significantly outperform conventional continual learning approaches in both knowledge retention and scalability to new tasks, achieving state-of-the-art performance with substantial memory and computational savings.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constraint-Informed Active Learning for End-to-End ACOPF Optimization Proxies</title>
<link>https://arxiv.org/abs/2511.06248</link>
<guid>https://arxiv.org/abs/2511.06248</guid>
<content:encoded><![CDATA[
arXiv:2511.06248v1 Announce Type: new 
Abstract: This paper studies optimization proxies, machine learning (ML) models trained to efficiently predict optimal solutions for AC Optimal Power Flow (ACOPF) problems. While promising, optimization proxy performance heavily depends on training data quality. To address this limitation, this paper introduces a novel active sampling framework for ACOPF optimization proxies designed to generate realistic and diverse training data. The framework actively explores varied, flexible problem specifications reflecting plausible operational realities. More importantly, the approach uses optimization-specific quantities (active constraint sets) that better capture the salient features of an ACOPF that lead to the optimal solution. Numerical results show superior generalization over existing sampling methods with an equivalent training budget, significantly advancing the state-of-practice for trustworthy ACOPF optimization proxies.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Iterative Error Correction for Efficient Diffusion Models</title>
<link>https://arxiv.org/abs/2511.06250</link>
<guid>https://arxiv.org/abs/2511.06250</guid>
<content:encoded><![CDATA[
arXiv:2511.06250v1 Announce Type: new 
Abstract: With the growing demand for high-quality image generation on resource-constrained devices, efficient diffusion models have received increasing attention. However, such models suffer from approximation errors introduced by efficiency techniques, which significantly degrade generation quality. Once deployed, these errors are difficult to correct, as modifying the model is typically infeasible in deployment environments. Through an analysis of error propagation across diffusion timesteps, we reveal that these approximation errors can accumulate exponentially, severely impairing output quality. Motivated by this insight, we propose Iterative Error Correction (IEC), a novel test-time method that mitigates inference-time errors by iteratively refining the model's output. IEC is theoretically proven to reduce error propagation from exponential to linear growth, without requiring any retraining or architectural changes. IEC can seamlessly integrate into the inference process of existing diffusion models, enabling a flexible trade-off between performance and efficiency. Extensive experiments show that IEC consistently improves generation quality across various datasets, efficiency techniques, and model architectures, establishing it as a practical and generalizable solution for test-time enhancement of efficient diffusion models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios</title>
<link>https://arxiv.org/abs/2511.06252</link>
<guid>https://arxiv.org/abs/2511.06252</guid>
<content:encoded><![CDATA[
arXiv:2511.06252v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) is a crucial approach to enhance the generalization capabilities and improve the sample efficiency of RL algorithms. However, current MBRL methods focus primarily on building world models for single tasks and rarely address generalization across different scenarios. Building on the insight that dynamics within the same simulation engine share inherent properties, we attempt to construct a unified world model capable of generalizing across different scenarios, named Meta-Regularized Contextual World-Model (MrCoM). This method first decomposes the latent state space into various components based on the dynamic characteristics, thereby enhancing the accuracy of world-model prediction. Further, MrCoM adopts meta-state regularization to extract unified representation of scenario-relevant information, and meta-value regularization to align world-model optimization with policy learning across diverse scenario objectives. We theoretically analyze the generalization error upper bound of MrCoM in multi-scenario settings. We systematically evaluate our algorithm's generalization ability across diverse scenarios, demonstrating significantly better performance than previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra</title>
<link>https://arxiv.org/abs/2511.06259</link>
<guid>https://arxiv.org/abs/2511.06259</guid>
<content:encoded><![CDATA[
arXiv:2511.06259v1 Announce Type: new 
Abstract: Retrieving molecular structures from tandem mass spectra is a crucial step in rapid compound identification. Existing retrieval methods, such as traditional mass spectral library matching, suffer from limited spectral library coverage, while recent cross-modal representation learning frameworks often encounter modality misalignment, resulting in suboptimal retrieval accuracy and generalization. To address these limitations, we propose GLMR, a Generative Language Model-based Retrieval framework that mitigates the cross-modal misalignment through a two-stage process. In the pre-retrieval stage, a contrastive learning-based model identifies top candidate molecules as contextual priors for the input mass spectrum. In the generative retrieval stage, these candidate molecules are integrated with the input mass spectrum to guide a generative model in producing refined molecular structures, which are then used to re-rank the candidates based on molecular similarity. Experiments on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR significantly outperforms existing methods, achieving over 40% improvement in top-1 accuracy and exhibiting strong generalizability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMP-HiVe: Cyclic Pair Merging based Efficient DNN Pruning with Hessian-Vector Approximation for Resource-Constrained Systems</title>
<link>https://arxiv.org/abs/2511.06265</link>
<guid>https://arxiv.org/abs/2511.06265</guid>
<content:encoded><![CDATA[
arXiv:2511.06265v1 Announce Type: new 
Abstract: Deep learning algorithms are becoming an essential component of many artificial intelligence (AI) driven applications, many of which run on resource-constrained and energy-constrained systems. For efficient deployment of these algorithms, although different techniques for the compression of neural network models are proposed, neural pruning is one of the fastest and effective methods, which can provide a high compression gain with minimal cost. To harness enhanced performance gain with respect to model complexity, we propose a novel neural network pruning approach utilizing Hessian-vector products that approximate crucial curvature information in the loss function, which significantly reduces the computation demands. By employing a power iteration method, our algorithm effectively identifies and preserves the essential information, ensuring a balanced trade-off between model accuracy and computational efficiency. Herein, we introduce CAMP-HiVe, a cyclic pair merging-based pruning with Hessian Vector approximation by iteratively consolidating weight pairs, combining significant and less significant weights, thus effectively streamlining the model while preserving its performance. This dynamic, adaptive framework allows for real-time adjustment of weight significance, ensuring that only the most critical parameters are retained. Our experimental results demonstrate that our proposed method achieves significant reductions in computational requirements while maintaining high performance across different neural network architectures, e.g., ResNet18, ResNet56, and MobileNetv2, on standard benchmark datasets, e.g., CIFAR10, CIFAR-100, and ImageNet, and it outperforms the existing state-of-the-art neural pruning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction</title>
<link>https://arxiv.org/abs/2511.06269</link>
<guid>https://arxiv.org/abs/2511.06269</guid>
<content:encoded><![CDATA[
arXiv:2511.06269v1 Announce Type: new 
Abstract: Drug-target interaction (DTI) prediction is of great significance for drug discovery and drug repurposing. With the accumulation of a large volume of valuable data, data-driven methods have been increasingly harnessed to predict DTIs, reducing costs across various dimensions. Therefore, this paper proposes a $\textbf{L}$arge $\textbf{L}$anguage $\textbf{M}$odel and $\textbf{M}$ulti-$\textbf{M}$odel data co-powered $\textbf{D}$rug $\textbf{T}$arget $\textbf{I}$nteraction prediction framework, named LLM$^3$-DTI. LLM$^3$-DTI constructs multi-modal data embedding to enhance DTI prediction performance. In this framework, the text semantic embeddings of drugs and targets are encoded by a domain-specific LLM. To effectively align and fuse multi-modal embedding. We propose the dual cross-attention mechanism and the TSFusion module. Finally, these multi-modal data are utilized for the DTI task through an output network. The experimental results indicate that LLM$^3$-DTI can proficiently identify validated DTIs, surpassing the performance of the models employed for comparison across diverse scenarios. Consequently, LLM$^3$-DTI is adept at fulfilling the task of DTI prediction with excellence. The data and code are available at https://github.com/chaser-gua/LLM3DTI.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions</title>
<link>https://arxiv.org/abs/2511.06273</link>
<guid>https://arxiv.org/abs/2511.06273</guid>
<content:encoded><![CDATA[
arXiv:2511.06273v1 Announce Type: new 
Abstract: Accurate prediction of financial and electricity markets, especially under extreme conditions, remains a significant challenge due to their intrinsic nonlinearity, rapid fluctuations, and chaotic patterns. To address these limitations, we propose the Chaotic Oscillatory Transformer Network (COTN). COTN innovatively combines a Transformer architecture with a novel Lee Oscillator activation function, processed through Max-over-Time pooling and a lambda-gating mechanism. This design is specifically tailored to effectively capture chaotic dynamics and improve responsiveness during periods of heightened volatility, where conventional activation functions (e.g., ReLU, GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder Self-Regressive (ASR) module to detect and isolate abnormal market patterns, such as sudden price spikes or crashes, thereby preventing corruption of the core prediction process and enhancing robustness. Extensive experiments across electricity spot markets and financial markets demonstrate the practical applicability and resilience of COTN. Our approach outperforms state-of-the-art deep learning models like Informer by up to 17% and traditional statistical methods like GARCH by as much as 40%. These results underscore COTN's effectiveness in navigating real-world market uncertainty and complexity, offering a powerful tool for forecasting highly volatile systems under duress.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Achieving Fairness Without Harm via Selective Demographic Experts</title>
<link>https://arxiv.org/abs/2511.06293</link>
<guid>https://arxiv.org/abs/2511.06293</guid>
<content:encoded><![CDATA[
arXiv:2511.06293v1 Announce Type: new 
Abstract: As machine learning systems become increasingly integrated into human-centered domains such as healthcare, ensuring fairness while maintaining high predictive performance is critical. Existing bias mitigation techniques often impose a trade-off between fairness and accuracy, inadvertently degrading performance for certain demographic groups. In high-stakes domains like clinical diagnosis, such trade-offs are ethically and practically unacceptable. In this study, we propose a fairness-without-harm approach by learning distinct representations for different demographic groups and selectively applying demographic experts consisting of group-specific representations and personalized classifiers through a no-harm constrained selection. We evaluate our approach on three real-world medical datasets -- covering eye disease, skin cancer, and X-ray diagnosis -- as well as two face datasets. Extensive empirical results demonstrate the effectiveness of our approach in achieving fairness without harm.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transolver is a Linear Transformer: Revisiting Physics-Attention through the Lens of Linear Attention</title>
<link>https://arxiv.org/abs/2511.06294</link>
<guid>https://arxiv.org/abs/2511.06294</guid>
<content:encoded><![CDATA[
arXiv:2511.06294v1 Announce Type: new 
Abstract: Recent advances in Transformer-based Neural Operators have enabled significant progress in data-driven solvers for Partial Differential Equations (PDEs). Most current research has focused on reducing the quadratic complexity of attention to address the resulting low training and inference efficiency. Among these works, Transolver stands out as a representative method that introduces Physics-Attention to reduce computational costs. Physics-Attention projects grid points into slices for slice attention, then maps them back through deslicing. However, we observe that Physics-Attention can be reformulated as a special case of linear attention, and that the slice attention may even hurt the model performance. Based on these observations, we argue that its effectiveness primarily arises from the slice and deslice operations rather than interactions between slices. Building on this insight, we propose a two-step transformation to redesign Physics-Attention into a canonical linear attention, which we call Linear Attention Neural Operator (LinearNO). Our method achieves state-of-the-art performance on six standard PDE benchmarks, while reducing the number of parameters by an average of 40.0% and computational cost by 36.2%. Additionally, it delivers superior performance on two challenging, industrial-level datasets: AirfRANS and Shape-Net Car.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>3dSAGER: Geospatial Entity Resolution over 3D Objects (Technical Report)</title>
<link>https://arxiv.org/abs/2511.06300</link>
<guid>https://arxiv.org/abs/2511.06300</guid>
<content:encoded><![CDATA[
arXiv:2511.06300v1 Announce Type: new 
Abstract: Urban environments are continuously mapped and modeled by various data collection platforms, including satellites, unmanned aerial vehicles and street cameras. The growing availability of 3D geospatial data from multiple modalities has introduced new opportunities and challenges for integrating spatial knowledge at scale, particularly in high-impact domains such as urban planning and rapid disaster management. Geospatial entity resolution is the task of identifying matching spatial objects across different datasets, often collected independently under varying conditions. Existing approaches typically rely on spatial proximity, textual metadata, or external identifiers to determine correspondence. While useful, these signals are often unavailable, unreliable, or misaligned, especially in cross-source scenarios. To address these limitations, we shift the focus to the intrinsic geometry of 3D spatial objects and present 3dSAGER (3D Spatial-Aware Geospatial Entity Resolution), an end-to-end pipeline for geospatial entity resolution over 3D objects. 3dSAGER introduces a novel, spatial-reference-independent featurization mechanism that captures intricate geometric characteristics of matching pairs, enabling robust comparison even across datasets with incompatible coordinate systems where traditional spatial methods fail. As a key component of 3dSAGER, we also propose a new lightweight and interpretable blocking method, BKAFI, that leverages a trained model to efficiently generate high-recall candidate sets. We validate 3dSAGER through extensive experiments on real-world urban datasets, demonstrating significant gains in both accuracy and efficiency over strong baselines. Our empirical study further dissects the contributions of each component, providing insights into their impact and the overall design choices.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation</title>
<link>https://arxiv.org/abs/2511.06304</link>
<guid>https://arxiv.org/abs/2511.06304</guid>
<content:encoded><![CDATA[
arXiv:2511.06304v1 Announce Type: new 
Abstract: Since 2010, Kaggle has been a platform where data scientists from around the world come together to compete, collaborate, and push the boundaries of Data Science. Over these 15 years, it has grown from a purely competition-focused site into a broader ecosystem with forums, notebooks, models, datasets, and more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now have a unique opportunity to explore these competitions, technologies, and real-world applications of Machine Learning and AI. And so in this study, we take a closer look at 15 years of data science on Kaggle - through metadata, shared code, community discussions, and the competitions themselves. We explore Kaggle's growth, its impact on the data science community, uncover hidden technological trends, analyze competition winners, how Kagglers approach problems in general, and more. We do this by analyzing millions of kernels and discussion threads to perform both longitudinal trend analysis and standard exploratory data analysis. Our findings show that Kaggle is a steadily growing platform with increasingly diverse use cases, and that Kagglers are quick to adapt to new trends and apply them to real-world challenges, while producing - on average - models with solid generalization capabilities. We also offer a snapshot of the platform as a whole, highlighting its history and technological evolution. Finally, this study is accompanied by a video (https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up (https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi) for your convenience.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation</title>
<link>https://arxiv.org/abs/2511.06307</link>
<guid>https://arxiv.org/abs/2511.06307</guid>
<content:encoded><![CDATA[
arXiv:2511.06307v1 Announce Type: new 
Abstract: Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a resurgence of interest in RLVR. Nevertheless, advances are dominated by mathematics (e.g., AIME), with competitive-programming code generation underexplored and data curation receiving less attention than RL algorithm design. We investigate how to construct RLVR datasets (i.e., RL prompts) and present practical training techniques that yield strong performance on competitive-programming code generation. Our pipeline begins with supervised fine-tuning (SFT) distilled from strong open-source models, augmented with general-purpose and reasoning-intensive data. RL then follows a two-stage process with executable, testcase-driven rewards: first, training on a large, uniformly distributed set of competitive-programming problems using Group Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively short response-generation window (e.g., 32k during SFT and 24k in this stage) to expand entropy and mitigate repetition and truncation; second, we perform \textbf{Pre-GRPO}: updating on a small, high-quality set of challenging problems with a large rollout budget (64 rollouts per prompt) under a hard-focus curriculum that continuously retains the most difficult instances throughout training. We implement our method on Qwen2.5-32B and evaluate on LeetCode and Codeforces weekly contests to avoid data leakage. The resulting model achieves state-of-the-art performance among models of similar scale and is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking. We also examine scaling trends and observe strong RL scaling on an internal large-scale MoE model. Our study distills concise best practices for data curation, entropy expansion, and curriculum design in RLVR for competitive-programming code generation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation</title>
<link>https://arxiv.org/abs/2511.06341</link>
<guid>https://arxiv.org/abs/2511.06341</guid>
<content:encoded><![CDATA[
arXiv:2511.06341v1 Announce Type: new 
Abstract: Control barrier functions (CBFs) are a popular tool for safety certification of nonlinear dynamical control systems. Recently, CBFs represented as neural networks have shown great promise due to their expressiveness and applicability to a broad class of dynamics and safety constraints. However, verifying that a trained neural network is indeed a valid CBF is a computational bottleneck that limits the size of the networks that can be used. To overcome this limitation, we present a novel framework for verifying neural CBFs based on piecewise linear upper and lower bounds on the conditions required for a neural network to be a CBF. Our approach is rooted in linear bound propagation (LBP) for neural networks, which we extend to compute bounds on the gradients of the network. Combined with McCormick relaxation, we derive linear upper and lower bounds on the CBF conditions, thereby eliminating the need for computationally expensive verification procedures. Our approach applies to arbitrary control-affine systems and a broad range of nonlinear activation functions. To reduce conservatism, we develop a parallelizable refinement strategy that adaptively refines the regions over which these bounds are computed. Our approach scales to larger neural networks than state-of-the-art verification procedures for CBFs, as demonstrated by our numerical experiments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets</title>
<link>https://arxiv.org/abs/2511.06356</link>
<guid>https://arxiv.org/abs/2511.06356</guid>
<content:encoded><![CDATA[
arXiv:2511.06356v1 Announce Type: new 
Abstract: Chemical reaction prediction remains a fundamental challenge in organic chemistry, where existing machine learning models face two critical limitations: sensitivity to input permutations (molecule/atom orderings) and inadequate modeling of substructural interactions governing reactivity. These shortcomings lead to inconsistent predictions and poor generalization to real-world scenarios. To address these challenges, we propose ReaDISH, a novel reaction prediction model that learns permutation-invariant representations while incorporating interaction-aware features. It introduces two innovations: (1) symmetric difference shingle encoding, which computes molecular shingle differences to capture reaction-specific structural changes while eliminating order sensitivity; and (2) geometry-structure interaction attention, a mechanism that models intra- and inter-molecular interactions at the shingle level. Extensive experiments demonstrate that ReaDISH improves reaction prediction performance across diverse benchmarks. It shows enhanced robustness with an average improvement of 8.76% on R$^2$ under permutation perturbations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization</title>
<link>https://arxiv.org/abs/2511.06363</link>
<guid>https://arxiv.org/abs/2511.06363</guid>
<content:encoded><![CDATA[
arXiv:2511.06363v1 Announce Type: new 
Abstract: The optimization of urban traffic is threatened by the complexity of achieving a balance between transport efficiency and the maintenance of privacy, as well as the equitable distribution of traffic based on socioeconomically diverse neighborhoods. Current centralized traffic management schemes invade user location privacy and further entrench traffic disparity by offering disadvantaged route suggestions, whereas current federated learning frameworks do not consider fairness constraints in multi-objective traffic settings. This study presents a privacy-preserving federated learning framework, termed FedFair-Traffic, that jointly and simultaneously optimizes travel efficiency, traffic fairness, and differential privacy protection. This is the first attempt to integrate three conflicting objectives to improve urban transportation systems. The proposed methodology enables collaborative learning between related vehicles with data locality by integrating Graph Neural Networks with differential privacy mechanisms ($\epsilon$-privacy guarantees) and Gini coefficient-based fair constraints using multi-objective optimization. The framework uses federated aggregation methods of gradient clipping and noise injection to provide differential privacy and optimize Pareto-efficient solutions for the efficiency-fairness tradeoff. Real-world comprehensive experiments on the METR-LA traffic dataset showed that FedFair-Traffic can reduce the average travel time by 7\% (14.2 minutes) compared with their centralized baselines, promote traffic fairness by 73\% (Gini coefficient, 0.78), and offer high privacy protection (privacy score, 0.8) with an 89\% reduction in communication overhead. These outcomes demonstrate that FedFair-Traffic is a scalable privacy-aware smart city infrastructure with possible use-cases in metropolitan traffic flow control and federated transportation networks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Regularization for Large-Scale Sparse Feature Embedding Models</title>
<link>https://arxiv.org/abs/2511.06374</link>
<guid>https://arxiv.org/abs/2511.06374</guid>
<content:encoded><![CDATA[
arXiv:2511.06374v1 Announce Type: new 
Abstract: The one-epoch overfitting problem has drawn widespread attention, especially in CTR and CVR estimation models in search, advertising, and recommendation domains. These models which rely heavily on large-scale sparse categorical features, often suffer a significant decline in performance when trained for multiple epochs. Although recent studies have proposed heuristic solutions, they have not clearly identified the fundamental cause of this phenomenon. In this work, we provide a theoretical analysis that explains why overfitting occurs in models that use large-scale sparse categorical features. Based on this analysis, we propose an adaptive regularization method to address it. Our approach not only prevents the severe performance degradation observed during multi-epoch training, but also improves model performance within a single epoch. This method has already been deployed in online production systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding</title>
<link>https://arxiv.org/abs/2511.06376</link>
<guid>https://arxiv.org/abs/2511.06376</guid>
<content:encoded><![CDATA[
arXiv:2511.06376v1 Announce Type: new 
Abstract: Numerous studies have demonstrated that the Transformer architecture possesses the capability for in-context learning (ICL). In scenarios involving function approximation, context can serve as a control parameter for the model, endowing it with the universal approximation property (UAP). In practice, context is represented by tokens from a finite set, referred to as a vocabulary, which is the case considered in this paper, \emph{i.e.}, vocabulary in-context learning (VICL). We demonstrate that VICL in single-layer Transformers, without positional encoding, does not possess the UAP; however, it is possible to achieve the UAP when positional encoding is included. Several sufficient conditions for the positional encoding are provided. Our findings reveal the benefits of positional encoding from an approximation theory perspective in the context of ICL.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models</title>
<link>https://arxiv.org/abs/2511.06430</link>
<guid>https://arxiv.org/abs/2511.06430</guid>
<content:encoded><![CDATA[
arXiv:2511.06430v1 Announce Type: new 
Abstract: Test-time Reinforcement Learning (TTRL) has shown promise in adapting foundation models for complex tasks at test-time, resulting in large performance improvements. TTRL leverages an elegant two-phase sampling strategy: first, multi-sampling derives a pseudo-label via majority voting, while subsequent downsampling and reward-based fine-tuning encourages the model to explore and learn diverse valid solutions, with the pseudo-label modulating the reward signal. Meanwhile, in-context learning has been widely explored at inference time and demonstrated the ability to enhance model performance without weight updates. However, TTRL's two-phase sampling strategy under-utilizes contextual guidance, which can potentially improve pseudo-label accuracy in the initial exploitation phase while regulating exploration in the second. To address this, we propose context-guided TTRL (CG-TTRL), integrating context dynamically into both sampling phases and propose a method for efficient context selection for on-device applications. Our evaluations on mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g. additional 7% relative accuracy improvement over TTRL), while boosting efficiency by obtaining strong performance after only a few steps of test-time training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation</title>
<link>https://arxiv.org/abs/2511.06443</link>
<guid>https://arxiv.org/abs/2511.06443</guid>
<content:encoded><![CDATA[
arXiv:2511.06443v1 Announce Type: new 
Abstract: Existing graph neural networks typically rely on heuristic choices for hidden dimensions and propagation depths, which often lead to severe information loss during propagation, known as over-squashing. To address this issue, we propose Channel Capacity Constrained Estimation (C3E), a novel framework that formulates the selection of hidden dimensions and depth as a nonlinear programming problem grounded in information theory. Through modeling spectral graph neural networks as communication channels, our approach directly connects channel capacity to hidden dimensions, propagation depth, propagation mechanism, and graph structure. Extensive experiments on nine public datasets demonstrate that hidden dimensions and depths estimated by C3E can mitigate over-squashing and consistently improve representation learning. Experimental results show that over-squashing occurs due to the cumulative compression of information in representation matrices. Furthermore, our findings show that increasing hidden dimensions indeed mitigate information compression, while the role of propagation depth is more nuanced, uncovering a fundamental balance between information compression and representation complexity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEX: Continuous Agent Evolution via Forward Learning from Experience</title>
<link>https://arxiv.org/abs/2511.06449</link>
<guid>https://arxiv.org/abs/2511.06449</guid>
<content:encoded><![CDATA[
arXiv:2511.06449v1 Announce Type: new 
Abstract: Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures</title>
<link>https://arxiv.org/abs/2511.06451</link>
<guid>https://arxiv.org/abs/2511.06451</guid>
<content:encoded><![CDATA[
arXiv:2511.06451v1 Announce Type: new 
Abstract: We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX term structures under no-arbitrage constraints. ARBITER maps market states to an operator that outputs implied volatility and variance curves while enforcing static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and monotonicity. The model couples operator learning with constrained decoders and is trained with extragradient-style updates plus projection. We introduce evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap, Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and state-space sequence models on historical SPX and VIX data. Ablation studies indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI, Lipschitz projection stabilizes calibration, and selective state updates improve long-horizon generalization. We provide identifiability and approximation results and describe practical recipes for arbitrage-free interpolation and extrapolation across maturities and strikes.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains</title>
<link>https://arxiv.org/abs/2511.06452</link>
<guid>https://arxiv.org/abs/2511.06452</guid>
<content:encoded><![CDATA[
arXiv:2511.06452v1 Announce Type: new 
Abstract: Although multimodal fusion has made significant progress, its advancement is severely hindered by the lack of adequate evaluation benchmarks. Current fusion methods are typically evaluated on a small selection of public datasets, a limited scope that inadequately represents the complexity and diversity of real-world scenarios, potentially leading to biased evaluations. This issue presents a twofold challenge. On one hand, models may overfit to the biases of specific datasets, hindering their generalization to broader practical applications. On the other hand, the absence of a unified evaluation standard makes fair and objective comparisons between different fusion methods difficult. Consequently, a truly universal and high-performance fusion model has yet to emerge. To address these challenges, we have developed a large-scale, domain-adaptive benchmark for multimodal evaluation. This benchmark integrates over 30 datasets, encompassing 15 modalities and 20 predictive tasks across key application domains. To complement this, we have also developed an open-source, unified, and automated evaluation pipeline that includes standardized implementations of state-of-the-art models and diverse fusion paradigms. Leveraging this platform, we have conducted large-scale experiments, successfully establishing new performance baselines across multiple tasks. This work provides the academic community with a crucial platform for rigorous and reproducible assessment of multimodal models, aiming to propel the field of multimodal artificial intelligence to new heights.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction and Secrecy under Approximate Distance Queries</title>
<link>https://arxiv.org/abs/2511.06461</link>
<guid>https://arxiv.org/abs/2511.06461</guid>
<content:encoded><![CDATA[
arXiv:2511.06461v1 Announce Type: new 
Abstract: Consider the task of locating an unknown target point using approximate distance queries: in each round, a reconstructor selects a query point and receives a noisy version of its distance to the target. This problem arises naturally in various contexts ranging from localization in GPS and sensor networks to privacy-aware data access, and spans a wide variety of metric spaces. It is relevant from the perspective of both the reconstructor (seeking accurate recovery) and the responder (aiming to limit information disclosure, e.g., for privacy or security reasons). We study this reconstruction game through a learning-theoretic lens, focusing on the rate and limits of the best possible reconstruction error. Our first result provides a tight geometric characterization of the optimal error in terms of the Chebyshev radius, a classical concept from geometry. This characterization applies to all compact metric spaces (in fact, even to all totally bounded spaces) and yields explicit formulas for natural metric spaces. Our second result addresses the asymptotic behavior of reconstruction, distinguishing between pseudo-finite spaces -- where the optimal error is attained after finitely many queries -- and spaces where the approximation curve exhibits nontrivial decay. We characterize pseudo-finiteness for convex Euclidean spaces.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Error Estimate and Convergence Analysis for Data Valuation</title>
<link>https://arxiv.org/abs/2511.06463</link>
<guid>https://arxiv.org/abs/2511.06463</guid>
<content:encoded><![CDATA[
arXiv:2511.06463v1 Announce Type: new 
Abstract: Data valuation quantifies data importance, but existing methods cannot ensure validity in a single training process. The neural dynamic data valuation (NDDV) method [3] addresses this limitation. Based on NDDV, we are the first to explore error estimation and convergence analysis in data valuation. Under Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss differences that scale inversely with time steps and quadratically with control variations, ensuring stability. We also prove that the expected squared gradient norm for the training loss vanishes asymptotically, and that the meta loss converges sublinearly over iterations. In particular, NDDV achieves sublinear convergence.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning</title>
<link>https://arxiv.org/abs/2511.06477</link>
<guid>https://arxiv.org/abs/2511.06477</guid>
<content:encoded><![CDATA[
arXiv:2511.06477v1 Announce Type: new 
Abstract: Recently, optimizers that explicitly treat weights as matrices, rather than flattened vectors, have demonstrated their effectiveness. This perspective naturally leads to structured approximations of the Fisher matrix as preconditioners, where the matrix view induces a Kronecker-factorized form that enables memory-efficient representation. However, constructing such approximations both efficiently and accurately remains an open challenge, since obtaining the optimal factorization is resource-intensive and practical methods therefore rely on heuristic design choices. In this work, we introduce a novel approach that leverages projector-splitting integrators to construct effective preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the Fisher Matrix), consistently improves the Fisher matrix approximation quality. Experiments on large language model pre-training and fine-tuning demonstrate that DyKAF outperforms existing optimizers across a range of evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable AI For Early Detection Of Sepsis</title>
<link>https://arxiv.org/abs/2511.06492</link>
<guid>https://arxiv.org/abs/2511.06492</guid>
<content:encoded><![CDATA[
arXiv:2511.06492v1 Announce Type: new 
Abstract: Sepsis is a life-threatening condition that requires rapid detection and treatment to prevent progression to severe sepsis, septic shock, or multi-organ failure. Despite advances in medical technology, it remains a major challenge for clinicians. While recent machine learning models have shown promise in predicting sepsis onset, their black-box nature limits interpretability and clinical trust. In this study, we present an interpretable AI approach for sepsis analysis that integrates machine learning with clinical knowledge. Our method not only delivers accurate predictions of sepsis onset but also enables clinicians to understand, validate, and align model outputs with established medical expertise.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Time-Varying Graph Signals via Koopman</title>
<link>https://arxiv.org/abs/2511.06493</link>
<guid>https://arxiv.org/abs/2511.06493</guid>
<content:encoded><![CDATA[
arXiv:2511.06493v1 Announce Type: new 
Abstract: A wide variety of real-world data, such as sea measurements, e.g., temperatures collected by distributed sensors and multiple unmanned aerial vehicles (UAV) trajectories, can be naturally represented as graphs, often exhibiting non-Euclidean structures. These graph representations may evolve over time, forming time-varying graphs. Effectively modeling and analyzing such dynamic graph data is critical for tasks like predicting graph evolution and reconstructing missing graph data. In this paper, we propose a framework based on the Koopman autoencoder (KAE) to handle time-varying graph data. Specifically, we assume the existence of a hidden non-linear dynamical system, where the state vector corresponds to the graph embedding of the time-varying graph signals. To capture the evolving graph structures, the graph data is first converted into a vector time series through graph embedding, representing the structural information in a finite-dimensional latent space. In this latent space, the KAE is applied to learn the underlying non-linear dynamics governing the temporal evolution of graph features, enabling both prediction and reconstruction tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Route Experts by Sequence, not by Token</title>
<link>https://arxiv.org/abs/2511.06494</link>
<guid>https://arxiv.org/abs/2511.06494</guid>
<content:encoded><![CDATA[
arXiv:2511.06494v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by activating only a subset of experts per token, but the standard TopK routing assigns the same fixed number of experts to all tokens, ignoring their varying complexity. Prior adaptive routing methods introduce additional modules and hyperparameters, often requiring costly retraining from scratch. We propose Sequence-level TopK (SeqTopK), a minimal modification that shifts the expert budget from the token level to the sequence level. By selecting the top $T \cdot K$ experts across all $T$ tokens, SeqTopK enables end-to-end learned dynamic allocation -- assigning more experts to difficult tokens and fewer to easy ones -- while preserving the same overall budget. SeqTopK requires only a few lines of code, adds less than 1% overhead, and remains fully compatible with pretrained MoE models. Experiments across math, coding, law, and writing show consistent improvements over TopK and prior parameter-free adaptive methods, with gains that become substantially larger under higher sparsity (up to 16.9%). These results highlight SeqTopK as a simple, efficient, and scalable routing strategy, particularly well-suited for the extreme sparsity regimes of next-generation LLMs. Code is available at https://github.com/Y-Research-SBU/SeqTopK.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probably Approximately Global Robustness Certification</title>
<link>https://arxiv.org/abs/2511.06495</link>
<guid>https://arxiv.org/abs/2511.06495</guid>
<content:encoded><![CDATA[
arXiv:2511.06495v1 Announce Type: new 
Abstract: We propose and investigate probabilistic guarantees for the adversarial robustness of classification algorithms. While traditional formal verification approaches for robustness are intractable and sampling-based approaches do not provide formal guarantees, our approach is able to efficiently certify a probabilistic relaxation of robustness. The key idea is to sample an $\epsilon$-net and invoke a local robustness oracle on the sample. Remarkably, the size of the sample needed to achieve probably approximately global robustness guarantees is independent of the input dimensionality, the number of classes, and the learning algorithm itself. Our approach can, therefore, be applied even to large neural networks that are beyond the scope of traditional formal verification. Experiments empirically confirm that it characterizes robustness better than state-of-the-art sampling-based approaches and scales better than formal methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Approximation of Volterra Series for High-Dimensional Systems</title>
<link>https://arxiv.org/abs/2511.06527</link>
<guid>https://arxiv.org/abs/2511.06527</guid>
<content:encoded><![CDATA[
arXiv:2511.06527v1 Announce Type: new 
Abstract: The identification of high-dimensional nonlinear dynamical systems via the Volterra series has significant potential, but has been severely hindered by the curse of dimensionality. Tensor Network (TN) methods such as the Modified Alternating Linear Scheme (MVMALS) have been a breakthrough for the field, offering a tractable approach by exploiting the low-rank structure in Volterra kernels. However, these techniques still encounter prohibitive computational and memory bottlenecks due to high-order polynomial scaling with respect to input dimension. To overcome this barrier, we introduce the Tensor Head Averaging (THA) algorithm, which significantly reduces complexity by constructing an ensemble of localized MVMALS models trained on small subsets of the input space. In this paper, we present a theoretical foundation for the THA algorithm. We establish observable, finite-sample bounds on the error between the THA ensemble and a full MVMALS model, and we derive an exact decomposition of the squared error. This decomposition is used to analyze the manner in which subset models implicitly compensate for omitted dynamics. We quantify this effect, and prove that correlation between the included and omitted dynamics creates an optimization incentive which drives THA's performance toward accuracy superior to a simple truncation of a full MVMALS model. THA thus offers a scalable and theoretically grounded approach for identifying previously intractable high-dimensional systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TriShGAN: Enhancing Sparsity and Robustness in Multivariate Time Series Counterfactuals Explanation</title>
<link>https://arxiv.org/abs/2511.06529</link>
<guid>https://arxiv.org/abs/2511.06529</guid>
<content:encoded><![CDATA[
arXiv:2511.06529v1 Announce Type: new 
Abstract: In decision-making processes, stakeholders often rely on counterfactual explanations, which provide suggestions about what should be changed in the queried instance to alter the outcome of an AI system. However, generating these explanations for multivariate time series presents challenges due to their complex, multi-dimensional nature. Traditional Nearest Unlike Neighbor-based methods typically substitute subsequences in a queried time series with influential subsequences from an NUN, which is not always realistic in real-world scenarios due to the rigid direct substitution. Counterfactual with Residual Generative Adversarial Networks-based methods aim to address this by learning from the distribution of observed data to generate synthetic counterfactual explanations. However, these methods primarily focus on minimizing the cost from the queried time series to the counterfactual explanations and often neglect the importance of distancing the counterfactual explanation from the decision boundary. This oversight can result in explanations that no longer qualify as counterfactual if minor changes occur within the model. To generate a more robust counterfactual explanation, we introduce TriShGAN, under the CounteRGAN framework enhanced by the incorporation of triplet loss. This unsupervised learning approach uses distance metric learning to encourage the counterfactual explanations not only to remain close to the queried time series but also to capture the feature distribution of the instance with the desired outcome, thereby achieving a better balance between minimal cost and robustness. Additionally, we integrate a Shapelet Extractor that strategically selects the most discriminative parts of the high-dimensional queried time series to enhance the sparsity of counterfactual explanation and efficiency of the training process.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction</title>
<link>https://arxiv.org/abs/2511.06538</link>
<guid>https://arxiv.org/abs/2511.06538</guid>
<content:encoded><![CDATA[
arXiv:2511.06538v1 Announce Type: new 
Abstract: Accurate EV power estimation underpins range prediction and energy management, yet practitioners need both point accuracy and trustworthy uncertainty. We propose an anchored-ensemble Long Short-Term Memory (LSTM) with a Student-t likelihood that jointly captures epistemic (model) and aleatoric (data) uncertainty. Anchoring imposes a Gaussian weight prior (MAP training), yielding posterior-like diversity without test-time sampling, while the t-head provides heavy-tailed robustness and closed-form prediction intervals. Using vehicle-kinematic time series (e.g., speed, motor RPM), our model attains strong accuracy: RMSE 3.36 +/- 1.10, MAE 2.21 +/- 0.89, R-squared = 0.93 +/- 0.02, explained variance 0.93 +/- 0.02, and delivers well-calibrated uncertainty bands with near-nominal coverage. Against competitive baselines (Student-t MC dropout; quantile regression with/without anchoring), our method matches or improves log-scores while producing sharper intervals at the same coverage. Crucially for real-time deployment, inference is a single deterministic pass per ensemble member (or a weight-averaged collapse), eliminating Monte Carlo latency. The result is a compact, theoretically grounded estimator that couples accuracy, calibration, and systems efficiency, enabling reliable range estimation and decision-making for production EV energy management.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Policy Distillation for Reinforcement Learning in Radio Access Networks</title>
<link>https://arxiv.org/abs/2511.06563</link>
<guid>https://arxiv.org/abs/2511.06563</guid>
<content:encoded><![CDATA[
arXiv:2511.06563v1 Announce Type: new 
Abstract: Adopting artificial intelligence (AI) in radio access networks (RANs) presents several challenges, including limited availability of link-level measurements (e.g., CQI reports), stringent real-time processing constraints (e.g., sub-1 ms per TTI), and network heterogeneity (different spectrum bands, cell types, and vendor equipment). A critical yet often overlooked barrier lies in the computational and memory limitations of RAN baseband hardware, particularly in legacy 4th Generation (4G) systems, which typically lack on-chip neural accelerators. As a result, only lightweight AI models (under 1 Mb and sub-100~\mu s inference time) can be effectively deployed, limiting both their performance and applicability. However, achieving strong generalization across diverse network conditions often requires large-scale models with substantial resource demands. To address this trade-off, this paper investigates policy distillation in the context of a reinforcement learning-based link adaptation task. We explore two strategies: single-policy distillation, where a scenario-agnostic teacher model is compressed into one generalized student model; and multi-policy distillation, where multiple scenario-specific teachers are consolidated into a single generalist student. Experimental evaluations in a high-fidelity, 5th Generation (5G)-compliant simulator demonstrate that both strategies produce compact student models that preserve the teachers' generalization capabilities while complying with the computational and memory limitations of existing RAN hardware.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity</title>
<link>https://arxiv.org/abs/2511.06568</link>
<guid>https://arxiv.org/abs/2511.06568</guid>
<content:encoded><![CDATA[
arXiv:2511.06568v1 Announce Type: new 
Abstract: Link prediction is a fundamental task in graph machine learning with applications, ranging from social recommendation to knowledge graph completion. Fairness in this setting is critical, as biased predictions can exacerbate societal inequalities. Prior work adopts a dyadic definition of fairness, enforcing fairness through demographic parity between intra-group and inter-group link predictions. However, we show that this dyadic framing can obscure underlying disparities across subgroups, allowing systemic biases to go undetected. Moreover, we argue that demographic parity does not meet desired properties for fairness assessment in ranking-based tasks such as link prediction. We formalize the limitations of existing fairness evaluations and propose a framework that enables a more expressive assessment. Additionally, we propose a lightweight post-processing method combined with decoupled link predictors that effectively mitigates bias and achieves state-of-the-art fairness-utility trade-offs.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimistic Online-to-Batch Conversions for Accelerated Convergence and Universality</title>
<link>https://arxiv.org/abs/2511.06597</link>
<guid>https://arxiv.org/abs/2511.06597</guid>
<content:encoded><![CDATA[
arXiv:2511.06597v1 Announce Type: new 
Abstract: In this work, we study offline convex optimization with smooth objectives, where the classical Nesterov's Accelerated Gradient (NAG) method achieves the optimal accelerated convergence. Extensive research has aimed to understand NAG from various perspectives, and a recent line of work approaches this from the viewpoint of online learning and online-to-batch conversion, emphasizing the role of optimistic online algorithms for acceleration. In this work, we contribute to this perspective by proposing novel optimistic online-to-batch conversions that incorporate optimism theoretically into the analysis, thereby significantly simplifying the online algorithm design while preserving the optimal convergence rates. Specifically, we demonstrate the effectiveness of our conversions through the following results: (i) when combined with simple online gradient descent, our optimistic conversion achieves the optimal accelerated convergence; (ii) our conversion also applies to strongly convex objectives, and by leveraging both optimistic online-to-batch conversion and optimistic online algorithms, we achieve the optimal accelerated convergence rate for strongly convex and smooth objectives, for the first time through the lens of online-to-batch conversion; (iii) our optimistic conversion can achieve universality to smoothness -- applicable to both smooth and non-smooth objectives without requiring knowledge of the smoothness coefficient -- and remains efficient as non-universal methods by using only one gradient query in each iteration. Finally, we highlight the effectiveness of our optimistic online-to-batch conversions by a precise correspondence with NAG.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Initial Residual Connections for GNNs with Theoretical Guarantees</title>
<link>https://arxiv.org/abs/2511.06598</link>
<guid>https://arxiv.org/abs/2511.06598</guid>
<content:encoded><![CDATA[
arXiv:2511.06598v1 Announce Type: new 
Abstract: Message passing is the core operation in graph neural networks, where each node updates its embeddings by aggregating information from its neighbors. However, in deep architectures, this process often leads to diminished expressiveness. A popular solution is to use residual connections, where the input from the current (or initial) layer is added to aggregated neighbor information to preserve embeddings across layers. Following a recent line of research, we investigate an adaptive residual scheme in which different nodes have varying residual strengths. We prove that this approach prevents oversmoothing; particularly, we show that the Dirichlet energy of the embeddings remains bounded away from zero. This is the first theoretical guarantee not only for the adaptive setting, but also for static residual connections (where residual strengths are shared across nodes) with activation functions. Furthermore, extensive experiments show that this adaptive approach outperforms standard and state-of-the-art message passing mechanisms, especially on heterophilic graphs. To improve the time complexity of our approach, we introduce a variant in which residual strengths are not learned but instead set heuristically, a choice that performs as well as the learnable version.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Probabilistic Machine Learning for Predicting Drilling Fluid Loss of Circulation in Marun Oil Field</title>
<link>https://arxiv.org/abs/2511.06607</link>
<guid>https://arxiv.org/abs/2511.06607</guid>
<content:encoded><![CDATA[
arXiv:2511.06607v1 Announce Type: new 
Abstract: Lost circulation remains a major and costly challenge in drilling operations, often resulting in wellbore instability, stuck pipe, and extended non-productive time. Accurate prediction of fluid loss is therefore essential for improving drilling safety and efficiency. This study presents a probabilistic machine learning framework based on Gaussian Process Regression (GPR) for predicting drilling fluid loss in complex formations. The GPR model captures nonlinear dependencies among drilling parameters while quantifying predictive uncertainty, offering enhanced reliability for high-risk decision-making. Model hyperparameters are optimized using the Limited memory Broyden Fletcher Goldfarb Shanno (LBFGS) algorithm to ensure numerical stability and robust generalization. To improve interpretability, Local Interpretable Model agnostic Explanations (LIME) are employed to elucidate how individual features influence model predictions. The results highlight the potential of explainable probabilistic learning for proactive identification of lost-circulation risks, optimized design of lost circulation materials (LCM), and reduction of operational uncertainties in drilling applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Fixed Depth: Adaptive Graph Neural Networks for Node Classification Under Varying Homophily</title>
<link>https://arxiv.org/abs/2511.06608</link>
<guid>https://arxiv.org/abs/2511.06608</guid>
<content:encoded><![CDATA[
arXiv:2511.06608v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success in addressing node classification tasks. However, the effectiveness of traditional GNNs degrades on heterophilic graphs, where connected nodes often belong to different labels or properties. While recent work has introduced mechanisms to improve GNN performance under heterophily, certain key limitations still exist. Most existing models apply a fixed aggregation depth across all nodes, overlooking the fact that nodes may require different propagation depths based on their local homophily levels and neighborhood structures. Moreover, many methods are tailored to either homophilic or heterophilic settings, lacking the flexibility to generalize across both regimes. To address these challenges, we develop a theoretical framework that links local structural and label characteristics to information propagation dynamics at the node level. Our analysis shows that optimal aggregation depth varies across nodes and is critical for preserving class-discriminative information. Guided by this insight, we propose a novel adaptive-depth GNN architecture that dynamically selects node-specific aggregation depths using theoretically grounded metrics. Our method seamlessly adapts to both homophilic and heterophilic patterns within a unified model. Extensive experiments demonstrate that our approach consistently enhances the performance of standard GNN backbones across diverse benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Weak Penalty Neural ODE for Learning Chaotic Dynamics from Noisy Time Series</title>
<link>https://arxiv.org/abs/2511.06609</link>
<guid>https://arxiv.org/abs/2511.06609</guid>
<content:encoded><![CDATA[
arXiv:2511.06609v1 Announce Type: new 
Abstract: Accurate forecasting of complex high-dimensional dynamical systems from observational data is essential for several applications across science and engineering. A key challenge, however, is that real-world measurements are often corrupted by noise, which severely degrades the performance of data-driven models. Particularly, in chaotic dynamical systems, where small errors amplify rapidly, it is challenging to identify a data-driven model from noisy data that achieves short-term accuracy while preserving long-term invariant properties. In this paper, we propose the use of the weak formulation as a complementary approach to the classical strong formulation of data-driven time-series forecasting models. Specifically, we focus on the neural ordinary differential equation (NODE) architecture. Unlike the standard strong formulation, which relies on the discretization of the NODE followed by optimization, the weak formulation constrains the model using a set of integrated residuals over temporal subdomains. While such a formulation yields an effective NODE model, we discover that the performance of a NODE can be further enhanced by employing this weak formulation as a penalty alongside the classical strong formulation-based learning. Through numerical demonstrations, we illustrate that our proposed training strategy, which we coined as the Weak-Penalty NODE (WP-NODE), achieves state-of-the-art forecasting accuracy and exceptional robustness across benchmark chaotic dynamical systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Rival Data as Rival Products: An Encapsulation-Forging Approach for Data Synthesis</title>
<link>https://arxiv.org/abs/2511.06610</link>
<guid>https://arxiv.org/abs/2511.06610</guid>
<content:encoded><![CDATA[
arXiv:2511.06610v1 Announce Type: new 
Abstract: The non-rival nature of data creates a dilemma for firms: sharing data unlocks value but risks eroding competitive advantage. Existing data synthesis methods often exacerbate this problem by creating data with symmetric utility, allowing any party to extract its value. This paper introduces the Encapsulation-Forging (EnFo) framework, a novel approach to generate rival synthetic data with asymmetric utility. EnFo operates in two stages: it first encapsulates predictive knowledge from the original data into a designated ``key'' model, and then forges a synthetic dataset by optimizing the data to intentionally overfit this key model. This process transforms non-rival data into a rival product, ensuring its value is accessible only to the intended model, thereby preventing unauthorized use and preserving the data owner's competitive edge. Our framework demonstrates remarkable sample efficiency, matching the original data's performance with a fraction of its size, while providing robust privacy protection and resistance to misuse. EnFo offers a practical solution for firms to collaborate strategically without compromising their core analytical advantage.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-branch Spatial-Temporal Self-supervised Representation for Enhanced Road Network Learning</title>
<link>https://arxiv.org/abs/2511.06633</link>
<guid>https://arxiv.org/abs/2511.06633</guid>
<content:encoded><![CDATA[
arXiv:2511.06633v1 Announce Type: new 
Abstract: Road network representation learning (RNRL) has attracted increasing attention from both researchers and practitioners as various spatiotemporal tasks are emerging. Recent advanced methods leverage Graph Neural Networks (GNNs) and contrastive learning to characterize the spatial structure of road segments in a self-supervised paradigm. However, spatial heterogeneity and temporal dynamics of road networks raise severe challenges to the neighborhood smoothing mechanism of self-supervised GNNs. To address these issues, we propose a $\textbf{D}$ual-branch $\textbf{S}$patial-$\textbf{T}$emporal self-supervised representation framework for enhanced road representations, termed as DST. On one hand, DST designs a mix-hop transition matrix for graph convolution to incorporate dynamic relations of roads from trajectories. Besides, DST contrasts road representations of the vanilla road network against that of the hypergraph in a spatial self-supervised way. The hypergraph is newly built based on three types of hyperedges to capture long-range relations. On the other hand, DST performs next token prediction as the temporal self-supervised task on the sequences of traffic dynamics based on a causal Transformer, which is further regularized by differentiating traffic modes of weekdays from those of weekends. Extensive experiments against state-of-the-art methods verify the superiority of our proposed framework. Moreover, the comprehensive spatiotemporal modeling facilitates DST to excel in zero-shot learning scenarios.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CaberNet: Causal Representation Learning for Cross-Domain HVAC Energy Prediction</title>
<link>https://arxiv.org/abs/2511.06634</link>
<guid>https://arxiv.org/abs/2511.06634</guid>
<content:encoded><![CDATA[
arXiv:2511.06634v1 Announce Type: new 
Abstract: Cross-domain HVAC energy prediction is essential for scalable building energy management, particularly because collecting extensive labeled data for every new building is both costly and impractical. Yet, this task remains highly challenging due to the scarcity and heterogeneity of data across different buildings, climate zones, and seasonal patterns. In particular, buildings situated in distinct climatic regions introduce variability that often leads existing methods to overfit to spurious correlations, rely heavily on expert intervention, or compromise on data diversity. To address these limitations, we propose CaberNet, a causal and interpretable deep sequence model that learns invariant (Markov blanket) representations for robust cross-domain prediction. In a purely data-driven fashion and without requiring any prior knowledge, CaberNet integrates i) a global feature gate trained with a self-supervised Bernoulli regularization to distinguish superior causal features from inferior ones, and ii) a domain-wise training scheme that balances domain contributions, minimizes cross-domain loss variance, and promotes latent factor independence. We evaluate CaberNet on real-world datasets collected from three buildings located in three climatically diverse cities, and it consistently outperforms all baselines, achieving a 22.9\% reduction in normalized mean squared error (NMSE) compared to the best benchmark. Our code is available at https://github.com/rickzky1001/CaberNet-CRL.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neyman-Pearson Classification under Both Null and Alternative Distributions Shift</title>
<link>https://arxiv.org/abs/2511.06641</link>
<guid>https://arxiv.org/abs/2511.06641</guid>
<content:encoded><![CDATA[
arXiv:2511.06641v1 Announce Type: new 
Abstract: We consider the problem of transfer learning in Neyman-Pearson classification, where the objective is to minimize the error w.r.t. a distribution $\mu_1$, subject to the constraint that the error w.r.t. a distribution $\mu_0$ remains below a prescribed threshold. While transfer learning has been extensively studied in traditional classification, transfer learning in imbalanced classification such as Neyman-Pearson classification has received much less attention. This setting poses unique challenges, as both types of errors must be simultaneously controlled. Existing works address only the case of distribution shift in $\mu_1$, whereas in many practical scenarios shifts may occur in both $\mu_0$ and $\mu_1$. We derive an adaptive procedure that not only guarantees improved Type-I and Type-II errors when the source is informative, but also automatically adapt to situations where the source is uninformative, thereby avoiding negative transfer. In addition to such statistical guarantees, the procedures is efficient, as shown via complementary computational guarantees.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Asset Allocation in a Fast Moving Consumer Goods B2B Company: An Interpretable Machine Learning Framework for Commercial Cooler Assignment Based on Multi-Tier Growth Targets</title>
<link>https://arxiv.org/abs/2511.06642</link>
<guid>https://arxiv.org/abs/2511.06642</guid>
<content:encoded><![CDATA[
arXiv:2511.06642v1 Announce Type: new 
Abstract: In the fast-moving consumer goods (FMCG) industry, deciding where to place physical assets, such as commercial beverage coolers, can directly impact revenue growth and execution efficiency. Although churn prediction and demand forecasting have been widely studied in B2B contexts, the use of machine learning to guide asset allocation remains relatively unexplored. This paper presents a framework focused on predicting which beverage clients are most likely to deliver strong returns in volume after receiving a cooler. Using a private dataset from a well-known Central American brewing and beverage company of 3,119 B2B traditional trade channel clients that received a cooler from 2022-01 to 2024-07, and tracking 12 months of sales transactions before and after cooler installation, three growth thresholds were defined: 10%, 30% and 50% growth in sales volume year over year. The analysis compares results of machine learning models such as XGBoost, LightGBM, and CatBoost combined with SHAP for interpretable feature analysis in order to have insights into improving business operations related to cooler allocation; the results show that the best model has AUC scores of 0.857, 0.877, and 0.898 across the thresholds on the validation set. Simulations suggest that this approach can improve ROI because it better selects potential clients to grow at the expected level and increases cost savings by not assigning clients that will not grow, compared to traditional volume-based approaches with substantial business management recommendations
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Pathway Fusion of EHRs and Knowledge Graphs for Predicting Unseen Drug-Drug Interactions</title>
<link>https://arxiv.org/abs/2511.06662</link>
<guid>https://arxiv.org/abs/2511.06662</guid>
<content:encoded><![CDATA[
arXiv:2511.06662v1 Announce Type: new 
Abstract: Drug-drug interactions (DDIs) remain a major source of preventable harm, and many clinically important mechanisms are still unknown. Existing models either rely on pharmacologic knowledge graphs (KGs), which fail on unseen drugs, or on electronic health records (EHRs), which are noisy, temporal, and site-dependent. We introduce, to our knowledge, the first system that conditions KG relation scoring on patient-level EHR context and distills that reasoning into an EHR-only model for zero-shot inference. A fusion "Teacher" learns mechanism-specific relations for drug pairs represented in both sources, while a distilled "Student" generalizes to new or rarely used drugs without KG access at inference. Both operate under a shared ontology (set) of pharmacologic mechanisms (drug relations) to produce interpretable, auditable alerts rather than opaque risk scores. Trained on a multi-institution EHR corpus paired with a curated DrugBank DDI graph, and evaluated using a clinically aligned, decision-focused protocol with leakage-safe negatives that avoid artificially easy pairs, the system maintains precision across multi-institutuion test data, produces mechanism-specific, clinically consistent predictions, reduces false alerts (higher precision) at comparable overall detection performance (F1), and misses fewer true interactions compared to prior methods. Case studies further show zero-shot identification of clinically recognized CYP-mediated and pharmacodynamic mechanisms for drugs absent from the KG, supporting real-world use in clinical decision support and pharmacovigilance.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Machine Learning Triage Framework for Predicting Alzheimer's Disease Progression</title>
<link>https://arxiv.org/abs/2511.06681</link>
<guid>https://arxiv.org/abs/2511.06681</guid>
<content:encoded><![CDATA[
arXiv:2511.06681v1 Announce Type: new 
Abstract: Accurate predictions of conversion from mild cognitive impairment (MCI) to Alzheimer's disease (AD) can enable effective personalized therapy. While cognitive tests and clinical data are routinely collected, they lack the predictive power of PET scans and CSF biomarker analysis, which are prohibitively expensive to obtain for every patient. To address this cost-accuracy dilemma, we design a two-stage machine learning framework that selectively obtains advanced, costly features based on their predicted "value of information". We apply our framework to predict AD progression for MCI patients using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). Our framework reduces the need for advanced testing by 20% while achieving a test AUROC of 0.929, comparable to the model that uses both basic and advanced features (AUROC=0.915, p=0.1010). We also provide an example interpretability analysis showing how one may explain the triage decision. Our work presents an interpretable, data-driven framework that optimizes AD diagnostic pathways and balances accuracy with cost, representing a step towards making early, reliable AD prediction more accessible in real-world practice. Future work should consider multiple categories of advanced features and larger-scale validation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Modality Imbalance in Multi-modal Learning via Multi-objective Optimization</title>
<link>https://arxiv.org/abs/2511.06686</link>
<guid>https://arxiv.org/abs/2511.06686</guid>
<content:encoded><![CDATA[
arXiv:2511.06686v1 Announce Type: new 
Abstract: Multi-modal learning (MML) aims to integrate information from multiple modalities, which is expected to lead to superior performance over single-modality learning. However, recent studies have shown that MML can underperform, even compared to single-modality approaches, due to imbalanced learning across modalities. Methods have been proposed to alleviate this imbalance issue using different heuristics, which often lead to computationally intensive subroutines. In this paper, we reformulate the MML problem as a multi-objective optimization (MOO) problem that overcomes the imbalanced learning issue among modalities and propose a gradient-based algorithm to solve the modified MML problem. We provide convergence guarantees for the proposed method, and empirical evaluations on popular MML benchmarks showcasing the improved performance of the proposed method over existing balanced MML and MOO baselines, with up to ~20x reduction in subroutine computation time. Our code is available at https://github.com/heshandevaka/MIMO.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peeling Context from Cause for Multimodal Molecular Property Prediction</title>
<link>https://arxiv.org/abs/2511.06692</link>
<guid>https://arxiv.org/abs/2511.06692</guid>
<content:encoded><![CDATA[
arXiv:2511.06692v1 Announce Type: new 
Abstract: Deep models are used for molecular property prediction, yet they are often difficult to interpret and may rely on spurious context rather than causal structure, which reduces reliability under distribution shift and harms predictive performance. We introduce CLaP (Causal Layerwise Peeling), a framework that separates causal signal from context in a layerwise manner and integrates diverse graph representations of molecules. At each layer, a causal block performs a soft split into causal and non-causal branches, fuses causal evidence across modalities, and progressively removes batch-coupled context to focus on label-relevant structure, thereby limiting shortcut signals and stabilizing layerwise refinement. Across four molecular benchmarks, CLaP consistently improves MAE, MSE, and $R^2$ over competitive baselines. The model also produces atom-level causal saliency maps that highlight substructures responsible for predictions, providing actionable guidance for targeted molecular edits. Case studies confirm the accuracy of these maps and their alignment with chemical intuition. By peeling context from cause at every layer, the model yields predictors that are both accurate and interpretable for molecular design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ML-EcoLyzer: Quantifying the Environmental Cost of Machine Learning Inference Across Frameworks and Hardware</title>
<link>https://arxiv.org/abs/2511.06694</link>
<guid>https://arxiv.org/abs/2511.06694</guid>
<content:encoded><![CDATA[
arXiv:2511.06694v1 Announce Type: new 
Abstract: Machine learning inference occurs at a massive scale, yet its environmental impact remains poorly quantified, especially on low-resource hardware. We present ML-EcoLyzer, a cross-framework tool for measuring the carbon, energy, thermal, and water costs of inference across CPUs, consumer GPUs, and datacenter accelerators. The tool supports both classical and modern models, applying adaptive monitoring and hardware-aware evaluation.
  We introduce the Environmental Sustainability Score (ESS), which quantifies the number of effective parameters served per gram of CO$_2$ emitted. Our evaluation covers over 1,900 inference configurations, spanning diverse model architectures, task modalities (text, vision, audio, tabular), hardware types, and precision levels. These rigorous and reliable measurements demonstrate that quantization enhances ESS, huge accelerators can be inefficient for lightweight applications, and even small models may incur significant costs when implemented suboptimally. ML-EcoLyzer sets a standard for sustainability-conscious model selection and offers an extensive empirical evaluation of environmental costs during inference.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnitude-Modulated Equivariant Adapter for Parameter-Efficient Fine-Tuning of Equivariant Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.06696</link>
<guid>https://arxiv.org/abs/2511.06696</guid>
<content:encoded><![CDATA[
arXiv:2511.06696v1 Announce Type: new 
Abstract: Pretrained equivariant graph neural networks based on spherical harmonics offer efficient and accurate alternatives to computationally expensive ab-initio methods, yet adapting them to new tasks and chemical environments still requires fine-tuning. Conventional parameter-efficient fine-tuning (PEFT) techniques, such as Adapters and LoRA, typically break symmetry, making them incompatible with those equivariant architectures. ELoRA, recently proposed, is the first equivariant PEFT method. It achieves improved parameter efficiency and performance on many benchmarks. However, the relatively high degrees of freedom it retains within each tensor order can still perturb pretrained feature distributions and ultimately degrade performance. To address this, we present Magnitude-Modulated Equivariant Adapter (MMEA), a novel equivariant fine-tuning method which employs lightweight scalar gating to modulate feature magnitudes on a per-order and per-multiplicity basis. We demonstrate that MMEA preserves strict equivariance and, across multiple benchmarks, consistently improves energy and force predictions to state-of-the-art levels while training fewer parameters than competing approaches. These results suggest that, in many practical scenarios, modulating channel magnitudes is sufficient to adapt equivariant models to new chemical environments without breaking symmetry, pointing toward a new paradigm for equivariant PEFT design.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sensor Calibration Model Balancing Accuracy, Real-time, and Efficiency</title>
<link>https://arxiv.org/abs/2511.06715</link>
<guid>https://arxiv.org/abs/2511.06715</guid>
<content:encoded><![CDATA[
arXiv:2511.06715v1 Announce Type: new 
Abstract: Most on-device sensor calibration studies benchmark models only against three macroscopic requirements (i.e., accuracy, real-time, and resource efficiency), thereby hiding deployment bottlenecks such as instantaneous error and worst-case latency. We therefore decompose this triad into eight microscopic requirements and introduce Scare (Sensor Calibration model balancing Accuracy, Real-time, and Efficiency), an ultra-compressed transformer that fulfills them all. SCARE comprises three core components: (1) Sequence Lens Projector (SLP) that logarithmically compresses time-series data while preserving boundary information across bins, (2) Efficient Bitwise Attention (EBA) module that replaces costly multiplications with bitwise operations via binary hash codes, and (3) Hash optimization strategy that ensures stable training without auxiliary loss terms. Together, these components minimize computational overhead while maintaining high accuracy and compatibility with microcontroller units (MCUs). Extensive experiments on large-scale air-quality datasets and real microcontroller deployments demonstrate that Scare outperforms existing linear, hybrid, and deep-learning baselines, making Scare, to the best of our knowledge, the first model to meet all eight microscopic requirements simultaneously.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileLLM-Pro Technical Report</title>
<link>https://arxiv.org/abs/2511.06719</link>
<guid>https://arxiv.org/abs/2511.06719</guid>
<content:encoded><![CDATA[
arXiv:2511.06719v1 Announce Type: new 
Abstract: Efficient on-device language models around 1 billion parameters are essential for powering low-latency AI applications on mobile and wearable devices. However, achieving strong performance in this model class, while supporting long context windows and practical deployment remains a significant challenge. We introduce MobileLLM-Pro, a 1-billion-parameter language model optimized for on-device deployment. MobileLLM-Pro achieves state-of-the-art results across 11 standard benchmarks, significantly outperforming both Gemma 3-1B and Llama 3.2-1B, while supporting context windows of up to 128,000 tokens and showing only minor performance regressions at 4-bit quantization. These improvements are enabled by four core innovations: (1) implicit positional distillation, a novel technique that effectively instills long-context capabilities through knowledge distillation; (2) a specialist model merging framework that fuses multiple domain experts into a compact model without parameter growth; (3) simulation-driven data mixing using utility estimation; and (4) 4-bit quantization-aware training with self-distillation. We release our model weights and code to support future research in efficient on-device language models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Continual Learning via Cross-Modality Adapters and Representation Alignment with Knowledge Preservation</title>
<link>https://arxiv.org/abs/2511.06723</link>
<guid>https://arxiv.org/abs/2511.06723</guid>
<content:encoded><![CDATA[
arXiv:2511.06723v1 Announce Type: new 
Abstract: Continual learning is essential for adapting models to new tasks while retaining previously acquired knowledge. While existing approaches predominantly focus on uni-modal data, multi-modal learning offers substantial benefits by utilizing diverse sensory inputs, akin to human perception. However, multi-modal continual learning presents additional challenges, as the model must effectively integrate new information from various modalities while preventing catastrophic forgetting. In this work, we propose a pre-trained model-based framework for multi-modal continual learning. Our framework includes a novel cross-modality adapter with a mixture-of-experts structure to facilitate effective integration of multi-modal information across tasks. We also introduce a representation alignment loss that fosters learning of robust multi-modal representations, and regularize relationships between learned representations to preserve knowledge from previous tasks. Experiments on several multi-modal datasets demonstrate that our approach consistently outperforms baselines in both class-incremental and domain-incremental learning, achieving higher accuracy and reduced forgetting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rank-1 LoRAs Encode Interpretable Reasoning Signals</title>
<link>https://arxiv.org/abs/2511.06739</link>
<guid>https://arxiv.org/abs/2511.06739</guid>
<content:encoded><![CDATA[
arXiv:2511.06739v1 Announce Type: new 
Abstract: Reasoning models leverage inference-time compute to significantly enhance the performance of language models on difficult logical tasks, and have become a dominating paradigm in frontier LLMs. Despite their wide adoption, the mechanisms underpinning the enhanced performance of these reasoning models are not well understood. In this work, we show that the majority of new capabilities in reasoning models can be elicited by small, single-rank changes to base model parameters, with many of these changes being interpretable. Specifically, we use a rank-1 LoRA to create a minimal parameter adapter for Qwen-2.5-32B-Instruct which recovers 73-90% of reasoning-benchmark performance compared to a full parameter finetune. We find that the activations of this LoRA are as interpretable as MLP neurons, and fire for reasoning-specific behaviors. Finally, we train a sparse autoencoder on the entire activation state of this LoRA and identify fine-grained and monosemantic features. Our findings highlight that reasoning performance can arise largely from minimal changes to base model parameters, and explore what these changes affect. More broadly, our work shows that parameter-efficient training methods can be used as a targeted lens for uncovering fundamental insights about language model behavior and dynamics.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Mamba for Node-Specific Representation Learning: Tackling Over-Smoothing with Selective State Space Modeling</title>
<link>https://arxiv.org/abs/2511.06756</link>
<guid>https://arxiv.org/abs/2511.06756</guid>
<content:encoded><![CDATA[
arXiv:2511.06756v1 Announce Type: new 
Abstract: Over-smoothing remains a fundamental challenge in deep Graph Neural Networks (GNNs), where repeated message passing causes node representations to become indistinguishable. While existing solutions, such as residual connections and skip layers, alleviate this issue to some extent, they fail to explicitly model how node representations evolve in a node-specific and progressive manner across layers. Moreover, these methods do not take global information into account, which is also crucial for mitigating the over-smoothing problem. To address the aforementioned issues, in this work, we propose a Dual Mamba-enhanced Graph Convolutional Network (DMbaGCN), which is a novel framework that integrates Mamba into GNNs to address over-smoothing from both local and global perspectives. DMbaGCN consists of two modules: the Local State-Evolution Mamba (LSEMba) for local neighborhood aggregation and utilizing Mamba's selective state space modeling to capture node-specific representation dynamics across layers, and the Global Context-Aware Mamba (GCAMba) that leverages Mamba's global attention capabilities to incorporate global context for each node. By combining these components, DMbaGCN enhances node discriminability in deep GNNs, thereby mitigating over-smoothing. Extensive experiments on multiple benchmarks demonstrate the effectiveness and efficiency of our method.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Federated In-context Learning For Task-Specific LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.06757</link>
<guid>https://arxiv.org/abs/2511.06757</guid>
<content:encoded><![CDATA[
arXiv:2511.06757v1 Announce Type: new 
Abstract: As large language models continue to develop and expand, the extensive public data they rely on faces the risk of depletion. Consequently, leveraging private data within organizations to enhance the performance of large models has emerged as a key challenge. The federated learning paradigm, combined with model fine-tuning techniques, effectively reduces the number of trainable parameters. However,the necessity to process high-dimensional feature spaces results in substantial overall computational overhead. To address this issue, we propose the Implicit Federated In-Context Learning (IFed-ICL) framework. IFed-ICL draws inspiration from federated learning to establish a novel distributed collaborative paradigm, by converting client local context examples into implicit vector representations, it enables distributed collaborative computation during the inference phase and injects model residual streams to enhance model performance. Experiments demonstrate that our proposed method achieves outstanding performance across multiple text classification tasks. Compared to traditional methods, IFed-ICL avoids the extensive parameter updates required by conventional fine-tuning methods while reducing data transmission and local computation at the client level in federated learning. This enables efficient distributed context learning using local private-domain data, significantly improving model performance on specific tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QUARK: Quantization-Enabled Circuit Sharing for Transformer Acceleration by Exploiting Common Patterns in Nonlinear Operations</title>
<link>https://arxiv.org/abs/2511.06767</link>
<guid>https://arxiv.org/abs/2511.06767</guid>
<content:encoded><![CDATA[
arXiv:2511.06767v1 Announce Type: new 
Abstract: Transformer-based models have revolutionized computer vision (CV) and natural language processing (NLP) by achieving state-of-the-art performance across a range of benchmarks. However, nonlinear operations in models significantly contribute to inference latency, presenting unique challenges for efficient hardware acceleration. To this end, we propose QUARK, a quantization-enabled FPGA acceleration framework that leverages common patterns in nonlinear operations to enable efficient circuit sharing, thereby reducing hardware resource requirements. QUARK targets all nonlinear operations within Transformer-based models, achieving high-performance approximation through a novel circuit-sharing design tailored to accelerate these operations. Our evaluation demonstrates that QUARK significantly reduces the computational overhead of nonlinear operators in mainstream Transformer architectures, achieving up to a 1.96 times end-to-end speedup over GPU implementations. Moreover, QUARK lowers the hardware overhead of nonlinear modules by more than 50% compared to prior approaches, all while maintaining high model accuracy -- and even substantially boosting accuracy under ultra-low-bit quantization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Trajectory Alignment for LLM Domain Adaptation: A Two-Phase Synthesis Framework for Telecommunications Mathematics</title>
<link>https://arxiv.org/abs/2511.06776</link>
<guid>https://arxiv.org/abs/2511.06776</guid>
<content:encoded><![CDATA[
arXiv:2511.06776v1 Announce Type: new 
Abstract: General-purpose large language models (LLMs) are increasingly deployed in verticals such as telecommunications, where adaptation is hindered by scarce, low-information-density corpora and tight mobile/edge constraints. We propose Data Trajectory Alignment (DTA), a two-phase, model-agnostic data curation framework that treats solution processes - not only final answers - as first-class supervision. Phase I (Initializing) synthesizes diverse, high-coverage candidates using an ensemble of strong teachers. Phase II (DTA) rewrites teacher solutions to align intermediate steps and presentation style with the target student's inductive biases and then performs signal-aware exemplar selection via agreement checks and reflection-based judging. Instantiated on telecommunications mathematics (e.g., link budgets, SNR/AMC selection, and power-control feasibility), DTA yields state-of-the-art (SOTA) accuracy on TELEMATH without enabling explicit "thinking" modes: 72.45% pass@1, surpassing distilled-only training by +17.65 points and outperforming a strong baseline (Qwen3-32B with thinking enabled) by +2.94 points. Token-shift analyses indicate that DTA concentrates gains on logical-structural discourse markers rather than merely amplifying domain nouns, indicating improved reasoning scaffolding. Under edge-like inference settings, DTA improves efficiency by reducing reliance on multi-sample voting and disabling expensive reasoning heuristics, cutting energy per output token by ~42% versus Qwen3-32B (thinking mode enabled) and end-to-end latency by ~60% versus Qwen3-32B (thinking mode disabled). These results demonstrate that aligning how solutions are produced enables compact, high-yield supervision that is effective for both accuracy and efficiency, offering a practical recipe for domain adaptation in low-resource verticals beyond telecom.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Mechanisms of Collaborative Learning in VAE Recommenders</title>
<link>https://arxiv.org/abs/2511.06781</link>
<guid>https://arxiv.org/abs/2511.06781</guid>
<content:encoded><![CDATA[
arXiv:2511.06781v1 Announce Type: new 
Abstract: Variational Autoencoders (VAEs) are a powerful alternative to matrix factorization for recommendation. A common technique in VAE-based collaborative filtering (CF) consists in applying binary input masking to user interaction vectors, which improves performance but remains underexplored theoretically. In this work, we analyze how collaboration arises in VAE-based CF and show it is governed by latent proximity: we derive a latent sharing radius that informs when an SGD update on one user strictly reduces the loss on another user, with influence decaying as the latent Wasserstein distance increases. We further study the induced geometry: with clean inputs, VAE-based CF primarily exploits \emph{local} collaboration between input-similar users and under-utilizes global collaboration between far-but-related users. We compare two mechanisms that encourage \emph{global} mixing and characterize their trade-offs: (1) $\beta$-KL regularization directly tightens the information bottleneck, promoting posterior overlap but risking representational collapse if too large; (2) input masking induces stochastic geometric contractions and expansions, which can bring distant users onto the same latent neighborhood but also introduce neighborhood drift. To preserve user identity while enabling global consistency, we propose an anchor regularizer that aligns user posteriors with item embeddings, stabilizing users under masking and facilitating signal sharing across related items. Our analyses are validated on the Netflix, MovieLens-20M, and Million Song datasets. We also successfully deployed our proposed algorithm on an Amazon streaming platform following a successful online experiment.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resource Efficient Sleep Staging via Multi-Level Masking and Prompt Learning</title>
<link>https://arxiv.org/abs/2511.06785</link>
<guid>https://arxiv.org/abs/2511.06785</guid>
<content:encoded><![CDATA[
arXiv:2511.06785v1 Announce Type: new 
Abstract: Automatic sleep staging plays a vital role in assessing sleep quality and diagnosing sleep disorders. Most existing methods rely heavily on long and continuous EEG recordings, which poses significant challenges for data acquisition in resource-constrained systems, such as wearable or home-based monitoring systems. In this paper, we propose the task of resource-efficient sleep staging, which aims to reduce the amount of signal collected per sleep epoch while maintaining reliable classification performance. To solve this task, we adopt the masking and prompt learning strategy and propose a novel framework called Mask-Aware Sleep Staging (MASS). Specifically, we design a multi-level masking strategy to promote effective feature modeling under partial and irregular observations. To mitigate the loss of contextual information introduced by masking, we further propose a hierarchical prompt learning mechanism that aggregates unmasked data into a global prompt, serving as a semantic anchor for guiding both patch-level and epoch-level feature modeling. MASS is evaluated on four datasets, demonstrating state-of-the-art performance, especially when the amount of data is very limited. This result highlights its potential for efficient and scalable deployment in real-world low-resource sleep monitoring environments.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Parameter Sharing as Graph Coloring for Structured Compression</title>
<link>https://arxiv.org/abs/2511.06786</link>
<guid>https://arxiv.org/abs/2511.06786</guid>
<content:encoded><![CDATA[
arXiv:2511.06786v1 Announce Type: new 
Abstract: Modern deep models have massive parameter sizes, leading to high inference-time memory usage that limits practical deployment. Parameter sharing, a form of structured compression, effectively reduces redundancy, but existing approaches remain heuristic-restricted to adjacent layers and lacking a systematic analysis for cross-layer sharing. However, extending sharing across multiple layers leads to an exponentially expanding configuration space, making exhaustive search computationally infeasible and forming a critical bottleneck for parameter sharing. We recast parameter sharing from a group-theoretic perspective as introducing structural symmetries in the model's parameter space. A sharing configuration can be described by a coloring function $\alpha:L\rightarrow C$ (L: layer indices and C: sharing classes), which determines inter-layer sharing groups while preserving structural symmetry. To determine the coloring function, we propose a second-order geometric criterion based on Taylor expansion and the Hessian spectrum. By projecting perturbations onto the Hessian's low-curvature eigensubspace, the criterion provides an analytic rule for selecting sharing groups that minimize performance impact, yielding a principled and scalable configuration procedure. Across diverse architectures and tasks, Geo-Sharing consistently outperforms state-of-the-art heuristic sharing strategies, achieving higher compression ratios with smaller accuracy degradation.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Causal Discovery under Imperfect Structural Constraints</title>
<link>https://arxiv.org/abs/2511.06790</link>
<guid>https://arxiv.org/abs/2511.06790</guid>
<content:encoded><![CDATA[
arXiv:2511.06790v1 Announce Type: new 
Abstract: Robust causal discovery from observational data under imperfect prior knowledge remains a significant and largely unresolved challenge. Existing methods typically presuppose perfect priors or can only handle specific, pre-identified error types. And their performance degrades substantially when confronted with flawed constraints of unknown location and type. This decline arises because most of them rely on inflexible and biased thresholding strategies that may conflict with the data distribution. To overcome these limitations, we propose to harmonizes knowledge and data through prior alignment and conflict resolution. First, we assess the credibility of imperfect structural constraints through a surrogate model, which then guides a sparse penalization term measuring the loss between the learned and constrained adjacency matrices. We theoretically prove that, under ideal assumption, the knowledge-driven objective aligns with the data-driven objective. Furthermore, to resolve conflicts when this assumption is violated, we introduce a multi-task learning framework optimized via multi-gradient descent, jointly minimizing both objectives. Our proposed method is robust to both linear and nonlinear settings. Extensive experiments, conducted under diverse noise conditions and structural equation model types, demonstrate the effectiveness and efficiency of our method under imperfect structural constraints.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coupling Agent-based Modeling and Life Cycle Assessment to Analyze Trade-offs in Resilient Energy Transitions</title>
<link>https://arxiv.org/abs/2511.06791</link>
<guid>https://arxiv.org/abs/2511.06791</guid>
<content:encoded><![CDATA[
arXiv:2511.06791v1 Announce Type: new 
Abstract: Transitioning to sustainable and resilient energy systems requires navigating complex and interdependent trade-offs across environmental, social, and resource dimensions. Neglecting these trade-offs can lead to unintended consequences across sectors. However, existing assessments often evaluate emerging energy pathways and their impacts in silos, overlooking critical interactions such as regional resource competition and cumulative impacts. We present an integrated modeling framework that couples agent-based modeling and Life Cycle Assessment (LCA) to simulate how energy transition pathways interact with regional resource competition, ecological constraints, and community-level burdens. We apply the model to a case study in Southern California. The results demonstrate how integrated and multiscale decision making can shape energy pathway deployment and reveal spatially explicit trade-offs under scenario-driven constraints. This modeling framework can further support more adaptive and resilient energy transition planning on spatial and institutional scales.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Modal Unlearning via Influential Neuron Path Editing in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2511.06793</link>
<guid>https://arxiv.org/abs/2511.06793</guid>
<content:encoded><![CDATA[
arXiv:2511.06793v1 Announce Type: new 
Abstract: Multimodal Large Language Models (MLLMs) extend foundation models to real-world applications by integrating inputs such as text and vision. However, their broad knowledge capacity raises growing concerns about privacy leakage, toxicity mitigation, and intellectual property violations. Machine Unlearning (MU) offers a practical solution by selectively forgetting targeted knowledge while preserving overall model utility. When applied to MLLMs, existing neuron-editing-based MU approaches face two fundamental challenges: (1) forgetting becomes inconsistent across modalities because existing point-wise attribution methods fail to capture the structured, layer-by-layer information flow that connects different modalities; and (2) general knowledge performance declines when sensitive neurons that also support important reasoning paths are pruned, as this disrupts the model's ability to generalize. To alleviate these limitations, we propose a multimodal influential neuron path editor (MIP-Editor) for MU. Our approach introduces modality-specific attribution scores to identify influential neuron paths responsible for encoding forget-set knowledge and applies influential-path-aware neuron-editing via representation misdirection. This strategy also enables effective and coordinated forgetting across modalities while preserving the model's general capabilities. Experimental results demonstrate that MIP-Editor achieves a superior unlearning performance on multimodal tasks, with a maximum forgetting rate of 87.75% and up to 54.26% improvement in general knowledge retention. On textual tasks, MIP-Editor achieves up to 80.65% forgetting and preserves 77.9% of general performance. Codes are available at https://github.com/PreckLi/MIP-Editor.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Uniform Deletion: A Data Value-Weighted Framework for Certified Machine Unlearning</title>
<link>https://arxiv.org/abs/2511.06794</link>
<guid>https://arxiv.org/abs/2511.06794</guid>
<content:encoded><![CDATA[
arXiv:2511.06794v1 Announce Type: new 
Abstract: As the right to be forgotten becomes legislated worldwide, machine unlearning mechanisms have emerged to efficiently update models for data deletion and enhance user privacy protection. However, existing machine unlearning algorithms frequently neglect the fact that different data points may contribute unequally to model performance (i.e., heterogeneous data values). Treat them equally in machine unlearning procedure can potentially degrading the performance of updated models. To address this limitation, we propose Data Value-Weighted Unlearning (DVWU), a general unlearning framework that accounts for data value heterogeneity into the unlearning process. Specifically, we design a weighting strategy based on data values, which are then integrated into the unlearning procedure to enable differentiated unlearning for data points with varying utility to the model. The DVWU framework can be broadly adapted to various existing machine unlearning methods. We use the one-step Newton update as an example for implementation, developing both output and objective perturbation algorithms to achieve certified unlearning. Experiments on both synthetic and real-world datasets demonstrate that our methods achieve superior predictive performance and robustness compared to conventional unlearning approaches. We further show the extensibility of our framework on gradient ascent method by incorporating the proposed weighting strategy into the gradient terms, highlighting the adaptability of DVWU for broader gradient-based deep unlearning methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedNET: Federated Learning for Proactive Traffic Management and Network Capacity Planning</title>
<link>https://arxiv.org/abs/2511.06797</link>
<guid>https://arxiv.org/abs/2511.06797</guid>
<content:encoded><![CDATA[
arXiv:2511.06797v1 Announce Type: new 
Abstract: We propose FedNET, a proactive and privacy-preserving framework for early identification of high-risk links in large-scale communication networks, that leverages a distributed multi-step traffic forecasting method. FedNET employs Federated Learning (FL) to model the temporal evolution of node-level traffic in a distributed manner, enabling accurate multi-step-ahead predictions (e.g., several hours to days) without exposing sensitive network data. Using these node-level forecasts and known routing information, FedNET estimates the future link-level utilization by aggregating traffic contributions across all source-destination pairs. The links are then ranked according to the predicted load intensity and temporal variability, providing an early warning signal for potential high-risk links. We compare the federated traffic prediction of FedNET against a centralized multi-step learning baseline and then systematically analyze the impact of history and prediction window sizes on forecast accuracy using the $R^2$ score. Results indicate that FL achieves accuracy close to centralized training, with shorter prediction horizons consistently yielding the highest accuracy ($R^2 >0.92$), while longer horizons providing meaningful forecasts ($R^2 \approx 0.45\text{--}0.55$). We further validate the efficacy of the FedNET framework in predicting network utilization on a realistic network topology and demonstrate that it consistently identifies high-risk links well in advance (i.e., three days ahead) of the critical stress states emerging, making it a practical tool for anticipatory traffic engineering and capacity planning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recursive Dynamics in Fast-Weights Homeostatic Reentry Networks: Toward Reflective Intelligence</title>
<link>https://arxiv.org/abs/2511.06798</link>
<guid>https://arxiv.org/abs/2511.06798</guid>
<content:encoded><![CDATA[
arXiv:2511.06798v1 Announce Type: new 
Abstract: This study introduces the Fast-Weights Homeostatic Reentry Layer (FH-RL), a neural mechanism that integrates fast-weight associative memory, homeostatic regularization, and learned reentrant feedback to approximate self-referential computation in neural networks. Unlike standard transformer architectures that operate in a purely feedforward manner during inference, FH-RL enables internal recurrence without external looping, allowing prior latent states to be dynamically re-entered into the ongoing computation stream. We conduct controlled experiments sweeping the reentry gain $\gamma$ and evaluate emergent internal dynamics using three novel metrics: the Information Reentry Ratio (IRR), Eigen-Spectrum Recursion Index (ESRI), and Representational Drift Periodicity (RDP). Results show that reentry quantity increases proportionally with~$\gamma$, while the learned feedback matrix $W_r$ remains bounded and becomes more structured at moderate gains. Critically, a stable reflective band emerges around $\gamma \approx 0.10-0.20$, where internal feedback is maximally expressive yet spectrally stable: IRR rises smoothly, ESRI remains near zero, and RDP exhibits consistent low-frequency cycles. These findings provide quantitative evidence that reflective, thought-like internal processing can arise from a principled balance between feedback amplification and homeostatic regulation, linking modern fast-weight architectures to theories of cortical reentry and recursive cognition.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural-Initialized Newton: Accelerating Nonlinear Finite Elements via Operator Learning</title>
<link>https://arxiv.org/abs/2511.06802</link>
<guid>https://arxiv.org/abs/2511.06802</guid>
<content:encoded><![CDATA[
arXiv:2511.06802v1 Announce Type: new 
Abstract: We propose a Newton-based scheme, initialized by neural operator predictions, to accelerate the parametric solution of nonlinear problems in computational solid mechanics. First, a physics informed conditional neural field is trained to approximate the nonlinear parametric solutionof the governing equations. This establishes a continuous mapping between the parameter and solution spaces, which can then be evaluated for a given parameter at any spatial resolution. Second, since the neural approximation may not be exact, it is subsequently refined using a Newton-based correction initialized by the neural output. To evaluate the effectiveness of this hybrid approach, we compare three solution strategies: (i) the standard Newton-Raphson solver used in NFEM, which is robust and accurate but computationally demanding; (ii) physics-informed neural operators, which provide rapid inference but may lose accuracy outside the training distribution and resolution; and (iii) the neural-initialized Newton (NiN) strategy, which combines the efficiency of neural operators with the robustness of NFEM. The results demonstrate that the proposed hybrid approach reduces computational cost while preserving accuracy, highlighting its potential to accelerate large-scale nonlinear simulations.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable Flow Matching for Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06816</link>
<guid>https://arxiv.org/abs/2511.06816</guid>
<content:encoded><![CDATA[
arXiv:2511.06816v1 Announce Type: new 
Abstract: Model-based reinforcement learning (MBRL) typically relies on modeling environment dynamics for data efficiency. However, due to the accumulation of model errors over long-horizon rollouts, such methods often face challenges in maintaining modeling stability. To address this, we propose CtrlFlow, a trajectory-level synthetic method using conditional flow matching (CFM), which directly modeling the distribution of trajectories from initial states to high-return terminal states without explicitly modeling the environment transition function. Our method ensures optimal trajectory sampling by minimizing the control energy governed by the non-linear Controllability Gramian Matrix, while the generated diverse trajectory data significantly enhances the robustness and cross-task generalization of policy learning. In online settings, CtrlFlow demonstrates the better performance on common MuJoCo benchmark tasks than dynamics models and achieves superior sample efficiency compared to standard MBRL methods.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepRWCap: Neural-Guided Random-Walk Capacitance Solver for IC Design</title>
<link>https://arxiv.org/abs/2511.06831</link>
<guid>https://arxiv.org/abs/2511.06831</guid>
<content:encoded><![CDATA[
arXiv:2511.06831v1 Announce Type: new 
Abstract: Monte Carlo random walk methods are widely used in capacitance extraction for their mesh-free formulation and inherent parallelism. However, modern semiconductor technologies with densely packed structures present significant challenges in unbiasedly sampling transition domains in walk steps with multiple high-contrast dielectric materials. We present DeepRWCap, a machine learning-guided random walk solver that predicts the transition quantities required to guide each step of the walk. These include Poisson kernels, gradient kernels, signs and magnitudes of weights. DeepRWCap employs a two-stage neural architecture that decomposes structured outputs into face-wise distributions and spatial kernels on cube faces. It uses 3D convolutional networks to capture volumetric dielectric interactions and 2D depthwise separable convolutions to model localized kernel behavior. The design incorporates grid-based positional encodings and structural design choices informed by cube symmetries to reduce learning redundancy and improve generalization. Trained on 100,000 procedurally generated dielectric configurations, DeepRWCap achieves a mean relative error of $1.24\pm0.53$\% when benchmarked against the commercial Raphael solver on the self-capacitance estimation of 10 industrial designs spanning 12 to 55 nm nodes. Compared to the state-of-the-art stochastic difference method Microwalk, DeepRWCap achieves an average 23\% speedup. On complex designs with runtimes over 10 s, it reaches an average 49\% acceleration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Width of Deep Narrow Networks for Universal Approximation</title>
<link>https://arxiv.org/abs/2511.06837</link>
<guid>https://arxiv.org/abs/2511.06837</guid>
<content:encoded><![CDATA[
arXiv:2511.06837v1 Announce Type: new 
Abstract: Determining the minimum width of fully connected neural networks has become a fundamental problem in recent theoretical studies of deep neural networks. In this paper, we study the lower bounds and upper bounds of the minimum width required for fully connected neural networks in order to have universal approximation capability, which is important in network design and training. We show that $w_{min}\leq\max(2d_x+1, d_y)$ for networks with ELU, SELU, and the upper bound of this inequality is attained when $d_y=2d_x$, where $d_x$, $d_y$ denote the input and output dimensions, respectively. Besides, we show that $d_x+1\leq w_{min}\leq d_x+d_y$ for networks with LeakyReLU, ELU, CELU, SELU, Softplus, by proving that ReLU can be approximated by these activation functions. In addition, in the case that the activation function is injective or can be uniformly approximated by a sequence of injective functions (e.g., ReLU), we present a new proof of the inequality $w_{min}\ge d_y+\mathbf{1}_{d_x<d_y\leq2d_x}$ by constructing a more intuitive example via a new geometric approach based on Poincar$\acute{\text{e}}$-Miranda Theorem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MI-to-Mid Distilled Compression (M2M-DC): An Hybrid-Information-Guided-Block Pruning with Progressive Inner Slicing Approach to Model Compression</title>
<link>https://arxiv.org/abs/2511.06842</link>
<guid>https://arxiv.org/abs/2511.06842</guid>
<content:encoded><![CDATA[
arXiv:2511.06842v1 Announce Type: new 
Abstract: We introduce MI-to-Mid Distilled Compression (M2M-DC), a two-scale, shape-safe compression framework that interleaves information-guided block pruning with progressive inner slicing and staged knowledge distillation (KD). First, M2M-DC ranks residual (or inverted-residual) blocks by a label-aware mutual information (MI) signal and removes the least informative units (structured prune-after-training). It then alternates short KD phases with stage-coherent, residual-safe channel slicing: (i) stage "planes" (co-slicing conv2 out-channels with the downsample path and next-stage inputs), and (ii) an optional mid-channel trim (conv1 out / bn1 / conv2 in). This targets complementary redundancy, whole computational motifs and within-stage width while preserving residual shape invariants. On CIFAR-100, M2M-DC yields a clean accuracy-compute frontier. For ResNet-18, we obtain 85.46% Top-1 with 3.09M parameters and 0.0139 GMacs (72% params, 63% GMacs vs. teacher; mean final 85.29% over three seeds). For ResNet-34, we reach 85.02% Top-1 with 5.46M params and 0.0195 GMacs (74% / 74% vs. teacher; mean final 84.62%). Extending to inverted-residuals, MobileNetV2 achieves a mean final 68.54% Top-1 at 1.71M params (27%) and 0.0186 conv GMacs (24%), improving over the teacher's 66.03% by +2.5 points across three seeds. Because M2M-DC exposes only a thin, architecture-aware interface (blocks, stages, and down sample/skip wiring), it generalizes across residual CNNs and extends to inverted-residual families with minor legalization rules. The result is a compact, practical recipe for deployment-ready models that match or surpass teacher accuracy at a fraction of the compute.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Observations: Reconstruction Error-Guided Irregularly Sampled Time Series Representation Learning</title>
<link>https://arxiv.org/abs/2511.06854</link>
<guid>https://arxiv.org/abs/2511.06854</guid>
<content:encoded><![CDATA[
arXiv:2511.06854v1 Announce Type: new 
Abstract: Irregularly sampled time series (ISTS), characterized by non-uniform time intervals with natural missingness, are prevalent in real-world applications. Existing approaches for ISTS modeling primarily rely on observed values to impute unobserved ones or infer latent dynamics. However, these methods overlook a critical source of learning signal: the reconstruction error inherently produced during model training. Such error implicitly reflects how well a model captures the underlying data structure and can serve as an informative proxy for unobserved values. To exploit this insight, we propose iTimER, a simple yet effective self-supervised pre-training framework for ISTS representation learning. iTimER models the distribution of reconstruction errors over observed values and generates pseudo-observations for unobserved timestamps through a mixup strategy between sampled errors and the last available observations. This transforms unobserved timestamps into noise-aware training targets, enabling meaningful reconstruction signals. A Wasserstein metric aligns reconstruction error distributions between observed and pseudo-observed regions, while a contrastive learning objective enhances the discriminability of learned representations. Extensive experiments on classification, interpolation, and forecasting tasks demonstrate that iTimER consistently outperforms state-of-the-art methods under the ISTS setting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contact Wasserstein Geodesics for Non-Conservative Schrodinger Bridges</title>
<link>https://arxiv.org/abs/2511.06856</link>
<guid>https://arxiv.org/abs/2511.06856</guid>
<content:encoded><![CDATA[
arXiv:2511.06856v1 Announce Type: new 
Abstract: The Schr\"odinger Bridge provides a principled framework for modeling stochastic processes between distributions; however, existing methods are limited by energy-conservation assumptions, which constrains the bridge's shape preventing it from model varying-energy phenomena. To overcome this, we introduce the non-conservative generalized Schr\"odinger bridge (NCGSB), a novel, energy-varying reformulation based on contact Hamiltonian mechanics. By allowing energy to change over time, the NCGSB provides a broader class of real-world stochastic processes, capturing richer and more faithful intermediate dynamics. By parameterizing the Wasserstein manifold, we lift the bridge problem to a tractable geodesic computation in a finite-dimensional space. Unlike computationally expensive iterative solutions, our contact Wasserstein geodesic (CWG) is naturally implemented via a ResNet architecture and relies on a non-iterative solver with near-linear complexity. Furthermore, CWG supports guided generation by modulating a task-specific distance metric. We validate our framework on tasks including manifold navigation, molecular dynamics predictions, and image generation, demonstrating its practical benefits and versatility.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TuckA: Hierarchical Compact Tensor Experts for Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.06859</link>
<guid>https://arxiv.org/abs/2511.06859</guid>
<content:encoded><![CDATA[
arXiv:2511.06859v1 Announce Type: new 
Abstract: Efficiently fine-tuning pre-trained models for downstream tasks is a key challenge in the era of foundation models. Parameter-efficient fine-tuning (PEFT) presents a promising solution, achieving performance comparable to full fine-tuning by updating only a small number of adaptation weights per layer. Traditional PEFT methods typically rely on a single expert, where the adaptation weight is a low-rank matrix. However, for complex tasks, the data's inherent diversity poses a significant challenge for such models, as a single adaptation weight cannot adequately capture the features of all samples. To address this limitation, we explore how to integrate multiple small adaptation experts into a compact structure to defeat a large adapter. Specifically, we propose Tucker Adaptation (TuckA), a method with four key properties: (i) We use Tucker decomposition to create a compact 3D tensor where each slice naturally serves as an expert. The low-rank nature of this decomposition ensures that the number of parameters scales efficiently as more experts are added. (ii) We introduce a hierarchical strategy that organizes these experts into groups at different granularities, allowing the model to capture both local and global data patterns. (iii) We develop an efficient batch-level routing mechanism, which reduces the router's parameter size by a factor of $L$ compared to routing at every adapted layer (where $L$ is the number of adapted layers) (iv) We propose data-aware initialization to achieve loss-free expert load balancing based on theoretical analysis. Extensive experiments on benchmarks in natural language understanding, image classification, and mathematical reasoning speak to the efficacy of TuckA, offering a new and effective solution to the PEFT problem.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepBooTS: Dual-Stream Residual Boosting for Drift-Resilient Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2511.06893</link>
<guid>https://arxiv.org/abs/2511.06893</guid>
<content:encoded><![CDATA[
arXiv:2511.06893v1 Announce Type: new 
Abstract: Time-Series (TS) exhibits pronounced non-stationarity. Consequently, most forecasting methods display compromised robustness to concept drift, despite the prevalent application of instance normalization. We tackle this challenge by first analysing concept drift through a bias-variance lens and proving that weighted ensemble reduces variance without increasing bias. These insights motivate DeepBooTS, a novel end-to-end dual-stream residual-decreasing boosting method that progressively reconstructs the intrinsic signal. In our design, each block of a deep model becomes an ensemble of learners with an auxiliary output branch forming a highway to the final prediction. The block-wise outputs correct the residuals of previous blocks, leading to a learning-driven decomposition of both inputs and targets. This method enhances versatility and interpretability while substantially improving robustness to concept drift. Extensive experiments, including those on large-scale datasets, show that the proposed method outperforms existing methods by a large margin, yielding an average performance improvement of 15.8% across various datasets, establishing a new benchmark for TS forecasting.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>COGNOS: Universal Enhancement for Time Series Anomaly Detection via Constrained Gaussian-Noise Optimization and Smoothing</title>
<link>https://arxiv.org/abs/2511.06894</link>
<guid>https://arxiv.org/abs/2511.06894</guid>
<content:encoded><![CDATA[
arXiv:2511.06894v1 Announce Type: new 
Abstract: Reconstruction-based methods are a dominant paradigm in time series anomaly detection (TSAD), however, their near-universal reliance on Mean Squared Error (MSE) loss results in statistically flawed reconstruction residuals. This fundamental weakness leads to noisy, unstable anomaly scores with a poor signal-to-noise ratio, hindering reliable detection. To address this, we propose Constrained Gaussian-Noise Optimization and Smoothing (COGNOS), a universal, model-agnostic enhancement framework that tackles this issue at its source. COGNOS introduces a novel Gaussian-White Noise Regularization strategy during training, which directly constrains the model's output residuals to conform to a Gaussian white noise distribution. This engineered statistical property creates the ideal precondition for our second contribution: a Kalman Smoothing Post-processor that provably operates as a statistically optimal estimator to denoise the raw anomaly scores. The synergy between these two components allows COGNOS to robustly separate the true anomaly signal from random fluctuations. Extensive experiments demonstrate that COGNOS is highly effective, delivering an average F-score uplift of 57.9% when applied to 12 diverse backbone models across multiple real-world benchmark datasets. Our work reveals that directly regularizing output statistics is a powerful and generalizable strategy for significantly improving anomaly detection systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Presence of Double-Descent in Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06895</link>
<guid>https://arxiv.org/abs/2511.06895</guid>
<content:encoded><![CDATA[
arXiv:2511.06895v1 Announce Type: new 
Abstract: The double descent (DD) paradox, where over-parameterized models see generalization improve past the interpolation point, remains largely unexplored in the non-stationary domain of Deep Reinforcement Learning (DRL). We present preliminary evidence that DD exists in model-free DRL, investigating it systematically across varying model capacity using the Actor-Critic framework. We rely on an information-theoretic metric, Policy Entropy, to measure policy uncertainty throughout training. Preliminary results show a clear epoch-wise DD curve; the policy's entrance into the second descent region correlates with a sustained, significant reduction in Policy Entropy. This entropic decay suggests that over-parameterization acts as an implicit regularizer, guiding the policy towards robust, flatter minima in the loss landscape. These findings establish DD as a factor in DRL and provide an information-based mechanism for designing agents that are more general, transferable, and robust.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Autoencoder-Transformer Model for Robust Day-Ahead Electricity Price Forecasting under Extreme Conditions</title>
<link>https://arxiv.org/abs/2511.06898</link>
<guid>https://arxiv.org/abs/2511.06898</guid>
<content:encoded><![CDATA[
arXiv:2511.06898v1 Announce Type: new 
Abstract: Accurate day-ahead electricity price forecasting (DAEPF) is critical for the efficient operation of power systems, but extreme condition and market anomalies pose significant challenges to existing forecasting methods. To overcome these challenges, this paper proposes a novel hybrid deep learning framework that integrates a Distilled Attention Transformer (DAT) model and an Autoencoder Self-regression Model (ASM). The DAT leverages a self-attention mechanism to dynamically assign higher weights to critical segments of historical data, effectively capturing both long-term trends and short-term fluctuations. Concurrently, the ASM employs unsupervised learning to detect and isolate anomalous patterns induced by extreme conditions, such as heavy rain, heat waves, or human festivals. Experiments on datasets sampled from California and Shandong Province demonstrate that our framework significantly outperforms state-of-the-art methods in prediction accuracy, robustness, and computational efficiency. Our framework thus holds promise for enhancing grid resilience and optimizing market operations in future power systems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Closer Look at Knowledge Distillation in Spiking Neural Network Training</title>
<link>https://arxiv.org/abs/2511.06902</link>
<guid>https://arxiv.org/abs/2511.06902</guid>
<content:encoded><![CDATA[
arXiv:2511.06902v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) become popular due to excellent energy efficiency, yet facing challenges for effective model training. Recent works improve this by introducing knowledge distillation (KD) techniques, with the pre-trained artificial neural networks (ANNs) used as teachers and the target SNNs as students. This is commonly accomplished through a straightforward element-wise alignment of intermediate features and prediction logits from ANNs and SNNs, often neglecting the intrinsic differences between their architectures. Specifically, ANN's outputs exhibit a continuous distribution, whereas SNN's outputs are characterized by sparsity and discreteness. To mitigate this issue, we introduce two innovative KD strategies. Firstly, we propose the Saliency-scaled Activation Map Distillation (SAMD), which aligns the spike activation map of the student SNN with the class-aware activation map of the teacher ANN. Rather than performing KD directly on the raw %and distinct features of ANN and SNN, our SAMD directs the student to learn from saliency activation maps that exhibit greater semantic and distribution consistency. Additionally, we propose a Noise-smoothed Logits Distillation (NLD), which utilizes Gaussian noise to smooth the sparse logits of student SNN, facilitating the alignment with continuous logits from teacher ANN. Extensive experiments on multiple datasets demonstrate the effectiveness of our methods. Code is available~\footnote{https://github.com/SinoLeu/CKDSNN.git}.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Explanation for Multivariate Time Series Forecasting with Exogenous Variables</title>
<link>https://arxiv.org/abs/2511.06906</link>
<guid>https://arxiv.org/abs/2511.06906</guid>
<content:encoded><![CDATA[
arXiv:2511.06906v1 Announce Type: new 
Abstract: Currently, machine learning is widely used across various domains, including time series data analysis. However, some machine learning models function as black boxes, making interpretability a critical concern. One approach to address this issue is counterfactual explanation (CE), which aims to provide insights into model predictions. This study focuses on the relatively underexplored problem of generating counterfactual explanations for time series forecasting. We propose a method for extracting CEs in time series forecasting using exogenous variables, which are frequently encountered in fields such as business and marketing. In addition, we present methods for analyzing the influence of each variable over an entire time series, generating CEs by altering only specific variables, and evaluating the quality of the resulting CEs. We validate the proposed method through theoretical analysis and empirical experiments, showcasing its accuracy and practical applicability. These contributions are expected to support real-world decision-making based on time series data analysis.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling and Loss Weights in Multi-Domain Training</title>
<link>https://arxiv.org/abs/2511.06913</link>
<guid>https://arxiv.org/abs/2511.06913</guid>
<content:encoded><![CDATA[
arXiv:2511.06913v1 Announce Type: new 
Abstract: In the training of large deep neural networks, there is a need for vast amounts of training data. To meet this need, data is collected from multiple domains, such as Wikipedia and GitHub. These domains are heterogeneous in both data quality and the diversity of information they provide. This raises the question of how much we should rely on each domain. Several methods have attempted to address this issue by assigning sampling weights to each data domain using heuristics or approximations. As a first step toward a deeper understanding of the role of data mixing, this work revisits the problem by studying two kinds of weights: sampling weights, which control how much each domain contributes in a batch, and loss weights, which scale the loss from each domain during training. Through a rigorous study of linear regression, we show that these two weights play complementary roles. First, they can reduce the variance of gradient estimates in iterative methods such as stochastic gradient descent (SGD). Second, they can improve generalization performance by reducing the generalization gap. We provide both theoretical and empirical support for these claims. We further study the joint dynamics of sampling weights and loss weights, examining how they can be combined to capture both contributions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Focus: Prioritizing Informative Histories with Structured Attention Mechanisms in Partially Observable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.06946</link>
<guid>https://arxiv.org/abs/2511.06946</guid>
<content:encoded><![CDATA[
arXiv:2511.06946v1 Announce Type: new 
Abstract: Transformers have shown strong ability to model long-term dependencies and are increasingly adopted as world models in model-based reinforcement learning (RL) under partial observability. However, unlike natural language corpora, RL trajectories are sparse and reward-driven, making standard self-attention inefficient because it distributes weight uniformly across all past tokens rather than emphasizing the few transitions critical for control. To address this, we introduce structured inductive priors into the self-attention mechanism of the dynamics head: (i) per-head memory-length priors that constrain attention to task-specific windows, and (ii) distributional priors that learn smooth Gaussian weightings over past state-action pairs. We integrate these mechanisms into UniZero, a model-based RL agent with a Transformer-based world model that supports planning under partial observability. Experiments on the Atari 100k benchmark show that most efficiency gains arise from the Gaussian prior, which smoothly allocates attention to informative transitions, while memory-length priors often truncate useful signals with overly restrictive cut-offs. In particular, Gaussian Attention achieves a 77% relative improvement in mean human-normalized scores over UniZero. These findings suggest that in partially observable RL domains with non-stationary temporal dependencies, discrete memory windows are difficult to learn reliably, whereas smooth distributional priors flexibly adapt across horizons and yield more robust data efficiency. Overall, our results demonstrate that encoding structured temporal priors directly into self-attention improves the prioritization of informative histories for dynamics modeling under partial observability.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Autoencoders for Tabular Data: Leveraging Model-Based Augmentation in Low-Label Settings</title>
<link>https://arxiv.org/abs/2511.06961</link>
<guid>https://arxiv.org/abs/2511.06961</guid>
<content:encoded><![CDATA[
arXiv:2511.06961v1 Announce Type: new 
Abstract: Deep neural networks often under-perform on tabular data due to their sensitivity to irrelevant features and a spectral bias toward smooth, low-frequency functions. These limitations hinder their ability to capture the sharp, high-frequency signals that often define tabular structure, especially under limited labeled samples. While self-supervised learning (SSL) offers promise in such settings, it remains challenging in tabular domains due to the lack of effective data augmentations. We propose a hybrid autoencoder that combines a neural encoder with an oblivious soft decision tree (OSDT) encoder, each guided by its own stochastic gating network that performs sample-specific feature selection. Together, these structurally different encoders and model-specific gating networks implement model-based augmentation, producing complementary input views tailored to each architecture. The two encoders, trained with a shared decoder and cross-reconstruction loss, learn distinct yet aligned representations that reflect their respective inductive biases. During training, the OSDT encoder (robust to noise and effective at modeling localized, high-frequency structure) guides the neural encoder toward representations more aligned with tabular data. At inference, only the neural encoder is used, preserving flexibility and SSL compatibility. Spectral analysis highlights the distinct inductive biases of each encoder. Our method achieves consistent gains in low-label classification and regression across diverse tabular datasets, outperforming deep and tree-based supervised baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oh That Looks Familiar: A Novel Similarity Measure for Spreadsheet Template Discovery</title>
<link>https://arxiv.org/abs/2511.06973</link>
<guid>https://arxiv.org/abs/2511.06973</guid>
<content:encoded><![CDATA[
arXiv:2511.06973v1 Announce Type: new 
Abstract: Traditional methods for identifying structurally similar spreadsheets fail to capture the spatial layouts and type patterns defining templates. To quantify spreadsheet similarity, we introduce a hybrid distance metric that combines semantic embeddings, data type information, and spatial positioning. In order to calculate spreadsheet similarity, our method converts spreadsheets into cell-level embeddings and then uses aggregation techniques like Chamfer and Hausdorff distances. Experiments across template families demonstrate superior unsupervised clustering performance compared to the graph-based Mondrian baseline, achieving perfect template reconstruction (Adjusted Rand Index of 1.00 versus 0.90) on the FUSTE dataset. Our approach facilitates large-scale automated template discovery, which in turn enables downstream applications such as retrieval-augmented generation over tabular collections, model training, and bulk data cleaning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Crystal Symmetry Prediction: A Decoupled Perspective</title>
<link>https://arxiv.org/abs/2511.06976</link>
<guid>https://arxiv.org/abs/2511.06976</guid>
<content:encoded><![CDATA[
arXiv:2511.06976v1 Announce Type: new 
Abstract: Efficiently and accurately determining the symmetry is a crucial step in the structural analysis of crystalline materials. Existing methods usually mindlessly apply deep learning models while ignoring the underlying chemical rules. More importantly, experiments show that they face a serious sub-property confusion SPC problem. To address the above challenges, from a decoupled perspective, we introduce the XRDecoupler framework, a problem-solving arsenal specifically designed to tackle the SPC problem. Imitating the thinking process of chemists, we innovatively incorporate multidimensional crystal symmetry information as superclass guidance to ensure that the model's prediction process aligns with chemical intuition. We further design a hierarchical PXRD pattern learning model and a multi-objective optimization approach to achieve high-quality representation and balanced optimization. Comprehensive evaluations on three mainstream databases (e.g., CCDC, CoREMOF, and InorganicData) demonstrate that XRDecoupler excels in performance, interpretability, and generalization.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Bayesian Updates via Harmonic Representations</title>
<link>https://arxiv.org/abs/2511.06978</link>
<guid>https://arxiv.org/abs/2511.06978</guid>
<content:encoded><![CDATA[
arXiv:2511.06978v1 Announce Type: new 
Abstract: Bayesian inference, while foundational to probabilistic reasoning, is often hampered by the computational intractability of posterior distributions, particularly through the challenging evidence integral. Conventional approaches like Markov Chain Monte Carlo (MCMC) and Variational Inference (VI) face significant scalability and efficiency limitations. This paper introduces a novel, unifying framework for fast Bayesian updates by leveraging harmonic analysis. We demonstrate that representing the prior and likelihood in a suitable orthogonal basis transforms the Bayesian update rule into a spectral convolution. Specifically, the Fourier coefficients of the posterior are shown to be the normalized convolution of the prior and likelihood coefficients. To achieve computational feasibility, we introduce a spectral truncation scheme, which, for smooth functions, yields an exceptionally accurate finite-dimensional approximation and reduces the update to a circular convolution. This formulation allows us to exploit the Fast Fourier Transform (FFT), resulting in a deterministic algorithm with O(N log N) complexity -- a substantial improvement over the O(N^2) cost of naive methods. We establish rigorous mathematical criteria for the applicability of our method, linking its efficiency to the smoothness and spectral decay of the involved distributions. The presented work offers a paradigm shift, connecting Bayesian computation to signal processing and opening avenues for real-time, sequential inference in a wide class of problems.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking the Gradient Barrier: Unveiling Large Language Models for Strategic Classification</title>
<link>https://arxiv.org/abs/2511.06979</link>
<guid>https://arxiv.org/abs/2511.06979</guid>
<content:encoded><![CDATA[
arXiv:2511.06979v1 Announce Type: new 
Abstract: Strategic classification~(SC) explores how individuals or entities modify their features strategically to achieve favorable classification outcomes. However, existing SC methods, which are largely based on linear models or shallow neural networks, face significant limitations in terms of scalability and capacity when applied to real-world datasets with significantly increasing scale, especially in financial services and the internet sector. In this paper, we investigate how to leverage large language models to design a more scalable and efficient SC framework, especially in the case of growing individuals engaged with decision-making processes. Specifically, we introduce GLIM, a gradient-free SC method grounded in in-context learning. During the feed-forward process of self-attention, GLIM implicitly simulates the typical bi-level optimization process of SC, including both the feature manipulation and decision rule optimization. Without fine-tuning the LLMs, our proposed GLIM enjoys the advantage of cost-effective adaptation in dynamic strategic environments. Theoretically, we prove GLIM can support pre-trained LLMs to adapt to a broad range of strategic manipulations. We validate our approach through experiments with a collection of pre-trained LLMs on real-world and synthetic datasets in financial and internet domains, demonstrating that our GLIM exhibits both robustness and efficiency, and offering an effective solution for large-scale SC tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HCFSLN: Adaptive Hyperbolic Few-Shot Learning for Multimodal Anxiety Detection</title>
<link>https://arxiv.org/abs/2511.06988</link>
<guid>https://arxiv.org/abs/2511.06988</guid>
<content:encoded><![CDATA[
arXiv:2511.06988v1 Announce Type: new 
Abstract: Anxiety disorders impact millions globally, yet traditional diagnosis relies on clinical interviews, while machine learning models struggle with overfitting due to limited data. Large-scale data collection remains costly and time-consuming, restricting accessibility. To address this, we introduce the Hyperbolic Curvature Few-Shot Learning Network (HCFSLN), a novel Few-Shot Learning (FSL) framework for multimodal anxiety detection, integrating speech, physiological signals, and video data. HCFSLN enhances feature separability through hyperbolic embeddings, cross-modal attention, and an adaptive gating network, enabling robust classification with minimal data. We collected a multimodal anxiety dataset from 108 participants and benchmarked HCFSLN against six FSL baselines, achieving 88% accuracy, outperforming the best baseline by 14%. These results highlight the effectiveness of hyperbolic space for modeling anxiety-related speech patterns and demonstrate FSL's potential for anxiety classification.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoLM: Collaborative Large Models via A Client-Server Paradigm</title>
<link>https://arxiv.org/abs/2511.06991</link>
<guid>https://arxiv.org/abs/2511.06991</guid>
<content:encoded><![CDATA[
arXiv:2511.06991v1 Announce Type: new 
Abstract: Large models have achieved remarkable performance across a range of reasoning and understanding tasks. Prior work often utilizes model ensembles or multi-agent systems to collaboratively generate responses, effectively operating in a server-to-server paradigm. However, such approaches do not align well with practical deployment settings, where a limited number of server-side models are shared by many clients under modern internet architectures. In this paper, we introduce \textbf{CoLM} (\textbf{Co}llaboration in \textbf{L}arge-\textbf{M}odels), a novel framework for collaborative reasoning that redefines cooperation among large models from a client-server perspective. Unlike traditional ensemble methods that rely on simultaneous inference from multiple models to produce a single output, CoLM allows the outputs of multiple models to be aggregated or shared, enabling each client model to independently refine and update its own generation based on these high-quality outputs. This design enables collaborative benefits by fully leveraging both client-side and shared server-side models. We further extend CoLM to vision-language models (VLMs), demonstrating its applicability beyond language tasks. Experimental results across multiple benchmarks show that CoLM consistently improves model performance on previously failed queries, highlighting the effectiveness of collaborative guidance in enhancing single-model capabilities.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S$^2$Drug: Bridging Protein Sequence and 3D Structure in Contrastive Representation Learning for Virtual Screening</title>
<link>https://arxiv.org/abs/2511.07006</link>
<guid>https://arxiv.org/abs/2511.07006</guid>
<content:encoded><![CDATA[
arXiv:2511.07006v1 Announce Type: new 
Abstract: Virtual screening (VS) is an essential task in drug discovery, focusing on the identification of small-molecule ligands that bind to specific protein pockets. Existing deep learning methods, from early regression models to recent contrastive learning approaches, primarily rely on structural data while overlooking protein sequences, which are more accessible and can enhance generalizability. However, directly integrating protein sequences poses challenges due to the redundancy and noise in large-scale protein-ligand datasets. To address these limitations, we propose \textbf{S$^2$Drug}, a two-stage framework that explicitly incorporates protein \textbf{S}equence information and 3D \textbf{S}tructure context in protein-ligand contrastive representation learning. In the first stage, we perform protein sequence pretraining on ChemBL using an ESM2-based backbone, combined with a tailored data sampling strategy to reduce redundancy and noise on both protein and ligand sides. In the second stage, we fine-tune on PDBBind by fusing sequence and structure information through a residue-level gating module, while introducing an auxiliary binding site prediction task. This auxiliary task guides the model to accurately localize binding residues within the protein sequence and capture their 3D spatial arrangement, thereby refining protein-ligand matching. Across multiple benchmarks, S$^2$Drug consistently improves virtual screening performance and achieves strong results on binding site prediction, demonstrating the value of bridging sequence and structure in contrastive learning.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correcting False Alarms from Unseen: Adapting Graph Anomaly Detectors at Test Time</title>
<link>https://arxiv.org/abs/2511.07023</link>
<guid>https://arxiv.org/abs/2511.07023</guid>
<content:encoded><![CDATA[
arXiv:2511.07023v1 Announce Type: new 
Abstract: Graph anomaly detection (GAD), which aims to detect outliers in graph-structured data, has received increasing research attention recently. However, existing GAD methods assume identical training and testing distributions, which is rarely valid in practice. In real-world scenarios, unseen but normal samples may emerge during deployment, leading to a normality shift that degrades the performance of GAD models trained on the original data. Through empirical analysis, we reveal that the degradation arises from (1) semantic confusion, where unseen normal samples are misinterpreted as anomalies due to their novel patterns, and (2) aggregation contamination, where the representations of seen normal nodes are distorted by unseen normals through message aggregation. While retraining or fine-tuning GAD models could be a potential solution to the above challenges, the high cost of model retraining and the difficulty of obtaining labeled data often render this approach impractical in real-world applications. To bridge the gap, we proposed a lightweight and plug-and-play Test-time adaptation framework for correcting Unseen Normal pattErns (TUNE) in GAD. To address semantic confusion, a graph aligner is employed to align the shifted data to the original one at the graph attribute level. Moreover, we utilize the minimization of representation-level shift as a supervision signal to train the aligner, which leverages the estimated aggregation contamination as a key indicator of normality shift. Extensive experiments on 10 real-world datasets demonstrate that TUNE significantly enhances the generalizability of pre-trained GAD models to both synthetic and real unseen normal patterns.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Bayesian Data Selection via Generalized Discrepancy Measures</title>
<link>https://arxiv.org/abs/2511.07032</link>
<guid>https://arxiv.org/abs/2511.07032</guid>
<content:encoded><![CDATA[
arXiv:2511.07032v1 Announce Type: new 
Abstract: Fairness concerns are increasingly critical as machine learning models are deployed in high-stakes applications. While existing fairness-aware methods typically intervene at the model level, they often suffer from high computational costs, limited scalability, and poor generalization. To address these challenges, we propose a Bayesian data selection framework that ensures fairness by aligning group-specific posterior distributions of model parameters and sample weights with a shared central distribution. Our framework supports flexible alignment via various distributional discrepancy measures, including Wasserstein distance, maximum mean discrepancy, and $f$-divergence, allowing geometry-aware control without imposing explicit fairness constraints. This data-centric approach mitigates group-specific biases in training data and improves fairness in downstream tasks, with theoretical guarantees. Experiments on benchmark datasets show that our method consistently outperforms existing data selection and model-based fairness methods in both fairness and accuracy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Quantized Continuous Controllers for Integer Hardware</title>
<link>https://arxiv.org/abs/2511.07046</link>
<guid>https://arxiv.org/abs/2511.07046</guid>
<content:encoded><![CDATA[
arXiv:2511.07046v1 Announce Type: new 
Abstract: Deploying continuous-control reinforcement learning policies on embedded hardware requires meeting tight latency and power budgets. Small FPGAs can deliver these, but only if costly floating point pipelines are avoided. We study quantization-aware training (QAT) of policies for integer inference and we present a learning-to-hardware pipeline that automatically selects low-bit policies and synthesizes them to an Artix-7 FPGA. Across five MuJoCo tasks, we obtain policy networks that are competitive with full precision (FP32) policies but require as few as 3 or even only 2 bits per weight, and per internal activation value, as long as input precision is chosen carefully. On the target hardware, the selected policies achieve inference latencies on the order of microseconds and consume microjoules per action, favorably comparing to a quantized reference. Last, we observe that the quantized policies exhibit increased input noise robustness compared to the floating-point baseline.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Privacy in Federated Clustering: Perfect Input Reconstruction via Temporal Correlations</title>
<link>https://arxiv.org/abs/2511.07073</link>
<guid>https://arxiv.org/abs/2511.07073</guid>
<content:encoded><![CDATA[
arXiv:2511.07073v1 Announce Type: new 
Abstract: Federated clustering allows multiple parties to discover patterns in distributed data without sharing raw samples. To reduce overhead, many protocols disclose intermediate centroids during training. While often treated as harmless for efficiency, whether such disclosure compromises privacy remains an open question. Prior analyses modeled the problem as a so-called Hidden Subset Sum Problem (HSSP) and argued that centroid release may be safe, since classical HSSP attacks fail to recover inputs.
  We revisit this question and uncover a new leakage mechanism: temporal regularities in $k$-means iterations create exploitable structure that enables perfect input reconstruction. Building on this insight, we propose Trajectory-Aware Reconstruction (TAR), an attack that combines temporal assignment information with algebraic analysis to recover exact original inputs. Our findings provide the first rigorous evidence, supported by a practical attack, that centroid disclosure in federated clustering significantly compromises privacy, exposing a fundamental tension between privacy and efficiency.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct Molecular Polarizability Prediction with SO(3) Equivariant Local Frame GNNs</title>
<link>https://arxiv.org/abs/2511.07087</link>
<guid>https://arxiv.org/abs/2511.07087</guid>
<content:encoded><![CDATA[
arXiv:2511.07087v1 Announce Type: new 
Abstract: We introduce a novel equivariant graph neural network (GNN) architecture designed to predict the tensorial response properties of molecules. Unlike traditional frameworks that focus on regressing scalar quantities and derive tensorial properties from their derivatives, our approach maintains $SO(3)$-equivariance through the use of local coordinate frames. Our GNN effectively captures geometric information by integrating scalar, vector, and tensor channels within a local message-passing framework. To assess the accuracy of our model, we apply it to predict the polarizabilities of molecules in the QM7-X dataset and show that tensorial message passing outperforms scalar message passing models. This work marks an advancement towards developing structured, geometry-aware neural models for molecular property prediction.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Joint Minimization of Regularization Loss Functions in Deep Variational Bayesian Methods for Attribute-Controlled Symbolic Music Generation</title>
<link>https://arxiv.org/abs/2511.07118</link>
<guid>https://arxiv.org/abs/2511.07118</guid>
<content:encoded><![CDATA[
arXiv:2511.07118v1 Announce Type: new 
Abstract: Explicit latent variable models provide a flexible yet powerful framework for data synthesis, enabling controlled manipulation of generative factors. With latent variables drawn from a tractable probability density function that can be further constrained, these models enable continuous and semantically rich exploration of the output space by navigating their latent spaces. Structured latent representations are typically obtained through the joint minimization of regularization loss functions. In variational information bottleneck models, reconstruction loss and Kullback-Leibler Divergence (KLD) are often linearly combined with an auxiliary Attribute-Regularization (AR) loss. However, balancing KLD and AR turns out to be a very delicate matter. When KLD dominates over AR, generative models tend to lack controllability; when AR dominates over KLD, the stochastic encoder is encouraged to violate the standard normal prior. We explore this trade-off in the context of symbolic music generation with explicit control over continuous musical attributes. We show that existing approaches struggle to jointly minimize both regularization objectives, whereas suitable attribute transformations can help achieve both controllability and regularization of the target latent dimensions.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REACT-LLM: A Benchmark for Evaluating LLM Integration with Causal Features in Clinical Prognostic Tasks</title>
<link>https://arxiv.org/abs/2511.07127</link>
<guid>https://arxiv.org/abs/2511.07127</guid>
<content:encoded><![CDATA[
arXiv:2511.07127v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and causal learning each hold strong potential for clinical decision making (CDM). However, their synergy remains poorly understood, largely due to the lack of systematic benchmarks evaluating their integration in clinical risk prediction. In real-world healthcare, identifying features with causal influence on outcomes is crucial for actionable and trustworthy predictions. While recent work highlights LLMs' emerging causal reasoning abilities, there lacks comprehensive benchmarks to assess their causal learning and performance informed by causal features in clinical risk prediction. To address this, we introduce REACT-LLM, a benchmark designed to evaluate whether combining LLMs with causal features can enhance clinical prognostic performance and potentially outperform traditional machine learning (ML) methods. Unlike existing LLM-clinical benchmarks that often focus on a limited set of outcomes, REACT-LLM evaluates 7 clinical outcomes across 2 real-world datasets, comparing 15 prominent LLMs, 6 traditional ML models, and 3 causal discovery (CD) algorithms. Our findings indicate that while LLMs perform reasonably in clinical prognostics, they have not yet outperformed traditional ML models. Integrating causal features derived from CD algorithms into LLMs offers limited performance gains, primarily due to the strict assumptions of many CD methods, which are often violated in complex clinical data. While the direct integration yields limited improvement, our benchmark reveals a more promising synergy.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Diffusion as Latent Constraints for Controllable Symbolic Music Generation</title>
<link>https://arxiv.org/abs/2511.07156</link>
<guid>https://arxiv.org/abs/2511.07156</guid>
<content:encoded><![CDATA[
arXiv:2511.07156v1 Announce Type: new 
Abstract: Recent advances in latent diffusion models have demonstrated state-of-the-art performance in high-dimensional time-series data synthesis while providing flexible control through conditioning and guidance. However, existing methodologies primarily rely on musical context or natural language as the main modality of interacting with the generative process, which may not be ideal for expert users who seek precise fader-like control over specific musical attributes. In this work, we explore the application of denoising diffusion processes as plug-and-play latent constraints for unconditional symbolic music generation models. We focus on a framework that leverages a library of small conditional diffusion models operating as implicit probabilistic priors on the latents of a frozen unconditional backbone. While previous studies have explored domain-specific use cases, this work, to the best of our knowledge, is the first to demonstrate the versatility of such an approach across a diverse array of musical attributes, such as note density, pitch range, contour, and rhythm complexity. Our experiments show that diffusion-driven constraints outperform traditional attribute regularization and other latent constraints architectures, achieving significantly stronger correlations between target and generated attributes while maintaining high perceptual quality and diversity.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guiding Generative Models to Uncover Diverse and Novel Crystals via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.07158</link>
<guid>https://arxiv.org/abs/2511.07158</guid>
<content:encoded><![CDATA[
arXiv:2511.07158v1 Announce Type: new 
Abstract: Discovering functional crystalline materials entails navigating an immense combinatorial design space. While recent advances in generative artificial intelligence have enabled the sampling of chemically plausible compositions and structures, a fundamental challenge remains: the objective misalignment between likelihood-based sampling in generative modelling and targeted focus on underexplored regions where novel compounds reside. Here, we introduce a reinforcement learning framework that guides latent denoising diffusion models toward diverse and novel, yet thermodynamically viable crystalline compounds. Our approach integrates group relative policy optimisation with verifiable, multi-objective rewards that jointly balance creativity, stability, and diversity. Beyond de novo generation, we demonstrate enhanced property-guided design that preserves chemical validity, while targeting desired functional properties. This approach establishes a modular foundation for controllable AI-driven inverse design that addresses the novelty-validity trade-off across scientific discovery applications of generative models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMscape</title>
<link>https://arxiv.org/abs/2511.07161</link>
<guid>https://arxiv.org/abs/2511.07161</guid>
<content:encoded><![CDATA[
arXiv:2511.07161v1 Announce Type: new 
Abstract: LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Combining digital data streams and epidemic networks for real time outbreak detection</title>
<link>https://arxiv.org/abs/2511.07163</link>
<guid>https://arxiv.org/abs/2511.07163</guid>
<content:encoded><![CDATA[
arXiv:2511.07163v1 Announce Type: new 
Abstract: Responding to disease outbreaks requires close surveillance of their trajectories, but outbreak detection is hindered by the high noise in epidemic time series. Aggregating information across data sources has shown great denoising ability in other fields, but remains underexplored in epidemiology. Here, we present LRTrend, an interpretable machine learning framework to identify outbreaks in real time. LRTrend effectively aggregates diverse health and behavioral data streams within one region and learns disease-specific epidemic networks to aggregate information across regions. We reveal diverse epidemic clusters and connections across the United States that are not well explained by commonly used human mobility networks and may be informative for future public health coordination. We apply LRTrend to 2 years of COVID-19 data in 305 hospital referral regions and frequently detect regional Delta and Omicron waves within 2 weeks of the outbreak's start, when case counts are a small fraction of the wave's resulting peak.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fuzzy Label: From Concept to Its Application in Label Learning</title>
<link>https://arxiv.org/abs/2511.07165</link>
<guid>https://arxiv.org/abs/2511.07165</guid>
<content:encoded><![CDATA[
arXiv:2511.07165v1 Announce Type: new 
Abstract: Label learning is a fundamental task in machine learning that aims to construct intelligent models using labeled data, encompassing traditional single-label and multi-label classification models. Traditional methods typically rely on logical labels, such as binary indicators (e.g., "yes/no") that specify whether an instance belongs to a given category. However, in practical applications, label annotations often involve significant uncertainty due to factors such as data noise, inherent ambiguity in the observed entities, and the subjectivity of human annotators. Therefore, representing labels using simplistic binary logic can obscure valuable information and limit the expressiveness of label learning models. To overcome this limitation, this paper introduces the concept of fuzzy labels, grounded in fuzzy set theory, to better capture and represent label uncertainty. We further propose an efficient fuzzy labeling method that mines and generates fuzzy labels from the original data, thereby enriching the label space with more informative and nuanced representations. Based on this foundation, we present fuzzy-label-enhanced algorithms for both single-label and multi-label learning, using the classical K-Nearest Neighbors (KNN) and multi-label KNN algorithms as illustrative examples. Experimental results indicate that fuzzy labels can more effectively characterize the real-world labeling information and significantly enhance the performance of label learning models.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Stealing Graph Neural Network Models</title>
<link>https://arxiv.org/abs/2511.07170</link>
<guid>https://arxiv.org/abs/2511.07170</guid>
<content:encoded><![CDATA[
arXiv:2511.07170v1 Announce Type: new 
Abstract: Current graph neural network (GNN) model-stealing methods rely heavily on queries to the victim model, assuming no hard query limits. However, in reality, the number of allowed queries can be severely limited. In this paper, we demonstrate how an adversary can extract the GNN with very limited interactions with the model. Our approach first enables the adversary to obtain the model backbone without making direct queries to the victim model and then to strategically utilize a fixed query limit to extract the most informative data. The experiments on eight real-world datasets demonstrate the effectiveness of the attack, even under a very restricted query limit and under defense against model extraction in place. Our findings underscore the need for robust defenses against GNN model extraction threats.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synergy over Discrepancy: A Partition-Based Approach to Multi-Domain LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.07198</link>
<guid>https://arxiv.org/abs/2511.07198</guid>
<content:encoded><![CDATA[
arXiv:2511.07198v1 Announce Type: new 
Abstract: Large language models (LLMs) demonstrate impressive generalization abilities, yet adapting them effectively across multiple heterogeneous domains remains challenging due to inter-domain interference. To overcome this challenge, we propose a partition-based multi-stage fine-tuning framework designed to exploit inter-domain synergies while minimizing negative transfer. Our approach strategically partitions domains into subsets (stages) by balancing domain discrepancy, synergy, and model capacity constraints. We theoretically analyze the proposed framework and derive novel generalization bounds that justify our partitioning strategy. Extensive empirical evaluations on various language understanding tasks show that our method consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SMiLE: Provably Enforcing Global Relational Properties in Neural Networks</title>
<link>https://arxiv.org/abs/2511.07208</link>
<guid>https://arxiv.org/abs/2511.07208</guid>
<content:encoded><![CDATA[
arXiv:2511.07208v1 Announce Type: new 
Abstract: Artificial Intelligence systems are increasingly deployed in settings where ensuring robustness, fairness, or domain-specific properties is essential for regulation compliance and alignment with human values. However, especially on Neural Networks, property enforcement is very challenging, and existing methods are limited to specific constraints or local properties (defined around datapoints), or fail to provide full guarantees. We tackle these limitations by extending SMiLE, a recently proposed enforcement framework for NNs, to support global relational properties (defined over the entire input space). The proposed approach scales well with model complexity, accommodates general properties and backbones, and provides full satisfaction guarantees. We evaluate SMiLE on monotonicity, global robustness, and individual fairness, on synthetic and real data, for regression and classification tasks. Our approach is competitive with property-specific baselines in terms of accuracy and runtime, and strictly superior in terms of generality and level of guarantees. Overall, our results emphasize the potential of the SMiLE framework as a platform for future research and applications.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DETECT: Data-Driven Evaluation of Treatments Enabled by Classification Transformers</title>
<link>https://arxiv.org/abs/2511.07213</link>
<guid>https://arxiv.org/abs/2511.07213</guid>
<content:encoded><![CDATA[
arXiv:2511.07213v1 Announce Type: new 
Abstract: Chronic pain is a global health challenge affecting millions of individuals, making it essential for physicians to have reliable and objective methods to measure the functional impact of clinical treatments. Traditionally used methods, like the numeric rating scale, while personalized and easy to use, are subjective due to their self-reported nature. Thus, this paper proposes DETECT (Data-Driven Evaluation of Treatments Enabled by Classification Transformers), a data-driven framework that assesses treatment success by comparing patient activities of daily life before and after treatment. We use DETECT on public benchmark datasets and simulated patient data from smartphone sensors. Our results demonstrate that DETECT is objective yet lightweight, making it a significant and novel contribution to clinical decision-making. By using DETECT, independently or together with other self-reported metrics, physicians can improve their understanding of their treatment impacts, ultimately leading to more personalized and responsive patient care.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Neural Operator Learning for Probabilistic Models</title>
<link>https://arxiv.org/abs/2511.07235</link>
<guid>https://arxiv.org/abs/2511.07235</guid>
<content:encoded><![CDATA[
arXiv:2511.07235v1 Announce Type: new 
Abstract: We propose a deep neural-operator framework for a general class of probability models. Under global Lipschitz conditions on the operator over the entire Euclidean space-and for a broad class of probabilistic models-we establish a universal approximation theorem with explicit network-size bounds for the proposed architecture. The underlying stochastic processes are required only to satisfy integrability and general tail-probability conditions. We verify these assumptions for both European and American option-pricing problems within the forward-backward SDE (FBSDE) framework, which in turn covers a broad class of operators arising from parabolic PDEs, with or without free boundaries. Finally, we present a numerical example for a basket of American options, demonstrating that the learned model produces optimal stopping boundaries for new strike prices without retraining.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does TabPFN Understand Causal Structures?</title>
<link>https://arxiv.org/abs/2511.07236</link>
<guid>https://arxiv.org/abs/2511.07236</guid>
<content:encoded><![CDATA[
arXiv:2511.07236v1 Announce Type: new 
Abstract: Causal discovery is fundamental for multiple scientific domains, yet extracting causal information from real world data remains a significant challenge. Given the recent success on real data, we investigate whether TabPFN, a transformer-based tabular foundation model pre-trained on synthetic datasets generated from structural causal models, encodes causal information in its internal representations. We develop an adapter framework using a learnable decoder and causal tokens that extract causal signals from TabPFN's frozen embeddings and decode them into adjacency matrices for causal discovery. Our evaluations demonstrate that TabPFN's embeddings contain causal information, outperforming several traditional causal discovery algorithms, with such causal information being concentrated in mid-range layers. These findings establish a new direction for interpretable and adaptable foundation models and demonstrate the potential for leveraging pre-trained tabular models for causal discovery.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Few Govern the Many:Unveiling Few-Layer Dominance for Time Series Models</title>
<link>https://arxiv.org/abs/2511.07237</link>
<guid>https://arxiv.org/abs/2511.07237</guid>
<content:encoded><![CDATA[
arXiv:2511.07237v1 Announce Type: new 
Abstract: Large-scale models are at the forefront of time series (TS) forecasting, dominated by two paradigms: fine-tuning text-based Large Language Models (LLM4TS) and training Time Series Foundation Models (TSFMs) from scratch. Both approaches share a foundational assumption that scaling up model capacity and data volume leads to improved performance. However, we observe a \textit{\textbf{scaling paradox}} in TS models, revealing a puzzling phenomenon that larger models do \emph{NOT} achieve better performance. Through extensive experiments on two model families across four scales (100M to 1.7B parameters) and diverse data (up to 6B observations), we rigorously confirm that the scaling paradox is a pervasive issue. We then diagnose its root cause by analyzing internal representations, identifying a phenomenon we call \textit{few-layer dominance}: only a small subset of layers are functionally important, while the majority are redundant, under-utilized, and can even distract training. Based on this discovery, we propose a practical method to automatically identify and retain only these dominant layers. In our models, retaining only 21\% of the parameters achieves up to a 12\% accuracy improvement and a 2.7$\times$ inference speedup. We validate the universality of our method on 8 prominent SOTA models (LLM4TS and TSFMs, 90M to 6B), showing that retaining less than 30\% of layers achieves comparable or superior accuracy in over 95\% of tasks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the role of depth in the neural tangent kernel for overparameterized neural networks</title>
<link>https://arxiv.org/abs/2511.07272</link>
<guid>https://arxiv.org/abs/2511.07272</guid>
<content:encoded><![CDATA[
arXiv:2511.07272v1 Announce Type: new 
Abstract: Overparameterized fully-connected neural networks have been shown to behave like kernel models when trained with gradient descent, under mild conditions on the width, the learning rate, and the parameter initialization. In the limit of infinitely large widths and small learning rate, the kernel that is obtained allows to represent the output of the learned model with a closed-form solution. This closed-form solution hinges on the invertibility of the limiting kernel, a property that often holds on real-world datasets. In this work, we analyze the sensitivity of large ReLU networks to increasing depths by characterizing the corresponding limiting kernel. Our theoretical results demonstrate that the normalized limiting kernel approaches the matrix of ones. In contrast, they show the corresponding closed-form solution approaches a fixed limit on the sphere. We empirically evaluate the order of magnitude in network depth required to observe this convergent behavior, and we describe the essential properties that enable the generalization of our results to other kernels.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-modal Dynamic Proxy Learning for Personalized Multiple Clustering</title>
<link>https://arxiv.org/abs/2511.07274</link>
<guid>https://arxiv.org/abs/2511.07274</guid>
<content:encoded><![CDATA[
arXiv:2511.07274v1 Announce Type: new 
Abstract: Multiple clustering aims to discover diverse latent structures from different perspectives, yet existing methods generate exhaustive clusterings without discerning user interest, necessitating laborious manual screening. Current multi-modal solutions suffer from static semantic rigidity: predefined candidate words fail to adapt to dataset-specific concepts, and fixed fusion strategies ignore evolving feature interactions. To overcome these limitations, we propose Multi-DProxy, a novel multi-modal dynamic proxy learning framework that leverages cross-modal alignment through learnable textual proxies. Multi-DProxy introduces 1) gated cross-modal fusion that synthesizes discriminative joint representations by adaptively modeling feature interactions. 2) dual-constraint proxy optimization where user interest constraints enforce semantic consistency with domain concepts while concept constraints employ hard example mining to enhance cluster discrimination. 3) dynamic candidate management that refines textual proxies through iterative clustering feedback. Therefore, Multi-DProxy not only effectively captures a user's interest through proxies but also enables the identification of relevant clusterings with greater precision. Extensive experiments demonstrate state-of-the-art performance with significant improvements over existing methods across a broad set of multi-clustering benchmarks.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RobustA: Robust Anomaly Detection in Multimodal Data</title>
<link>https://arxiv.org/abs/2511.07276</link>
<guid>https://arxiv.org/abs/2511.07276</guid>
<content:encoded><![CDATA[
arXiv:2511.07276v1 Announce Type: new 
Abstract: In recent years, multimodal anomaly detection methods have demonstrated remarkable performance improvements over video-only models. However, real-world multimodal data is often corrupted due to unforeseen environmental distortions. In this paper, we present the first-of-its-kind work that comprehensively investigates the adverse effects of corrupted modalities on multimodal anomaly detection task. To streamline this work, we propose RobustA, a carefully curated evaluation dataset to systematically observe the impacts of audio and visual corruptions on the overall effectiveness of anomaly detection systems. Furthermore, we propose a multimodal anomaly detection method, which shows notable resilience against corrupted modalities. The proposed method learns a shared representation space for different modalities and employs a dynamic weighting scheme during inference based on the estimated level of corruption. Our work represents a significant step forward in enabling the real-world application of multimodal anomaly detection, addressing situations where the likely events of modality corruptions occur. The proposed evaluation dataset with corrupted modalities and respective extracted features will be made publicly available.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MG-HGNN: A Heterogeneous GNN Framework for Indoor Wi-Fi Fingerprint-Based Localization</title>
<link>https://arxiv.org/abs/2511.07282</link>
<guid>https://arxiv.org/abs/2511.07282</guid>
<content:encoded><![CDATA[
arXiv:2511.07282v1 Announce Type: new 
Abstract: Received signal strength indicator (RSSI) is the primary representation of Wi-Fi fingerprints and serves as a crucial tool for indoor localization. However, existing RSSI-based positioning methods often suffer from reduced accuracy due to environmental complexity and challenges in processing multi-source information. To address these issues, we propose a novel multi-graph heterogeneous GNN framework (MG-HGNN) to enhance spatial awareness and improve positioning performance. In this framework, two graph construction branches perform node and edge embedding, respectively, to generate informative graphs. Subsequently, a heterogeneous graph neural network is employed for graph representation learning, enabling accurate positioning. The MG-HGNN framework introduces the following key innovations: 1) multi-type task-directed graph construction that combines label estimation and feature encoding for richer graph information; 2) a heterogeneous GNN structure that enhances the performance of conventional GNN models. Evaluations on the UJIIndoorLoc and UTSIndoorLoc public datasets demonstrate that MG-HGNN not only achieves superior performance compared to several state-of-the-art methods, but also provides a novel perspective for enhancing GNN-based localization methods. Ablation studies further confirm the rationality and effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Off-Policy Imitation Learning with Deep Actor Critic Stabilization</title>
<link>https://arxiv.org/abs/2511.07288</link>
<guid>https://arxiv.org/abs/2511.07288</guid>
<content:encoded><![CDATA[
arXiv:2511.07288v1 Announce Type: new 
Abstract: Learning complex policies with Reinforcement Learning (RL) is often hindered by instability and slow convergence, a problem exacerbated by the difficulty of reward engineering. Imitation Learning (IL) from expert demonstrations bypasses this reliance on rewards. However, state-of-the-art IL methods, exemplified by Generative Adversarial Imitation Learning (GAIL)Ho et. al, suffer from severe sample inefficiency. This is a direct consequence of their foundational on-policy algorithms, such as TRPO Schulman et.al. In this work, we introduce an adversarial imitation learning algorithm that incorporates off-policy learning to improve sample efficiency. By combining an off-policy framework with auxiliary techniques specifically, double Q network based stabilization and value learning without reward function inference we demonstrate a reduction in the samples required to robustly match expert behavior.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Training Dynamics of Scale-Invariant Neural Networks Be Explained by the Thermodynamics of an Ideal Gas?</title>
<link>https://arxiv.org/abs/2511.07308</link>
<guid>https://arxiv.org/abs/2511.07308</guid>
<content:encoded><![CDATA[
arXiv:2511.07308v1 Announce Type: new 
Abstract: Understanding the training dynamics of deep neural networks remains a major open problem, with physics-inspired approaches offering promising insights. Building on this perspective, we develop a thermodynamic framework to describe the stationary distributions of stochastic gradient descent (SGD) with weight decay for scale-invariant neural networks, a setting that both reflects practical architectures with normalization layers and permits theoretical analysis. We establish analogies between training hyperparameters (e.g., learning rate, weight decay) and thermodynamic variables such as temperature, pressure, and volume. Starting with a simplified isotropic noise model, we uncover a close correspondence between SGD dynamics and ideal gas behavior, validated through theory and simulation. Extending to training of neural networks, we show that key predictions of the framework, including the behavior of stationary entropy, align closely with experimental observations. This framework provides a principled foundation for interpreting training dynamics and may guide future work on hyperparameter tuning and the design of learning rate schedulers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Superhuman AI for Stratego Using Self-Play Reinforcement Learning and Test-Time Search</title>
<link>https://arxiv.org/abs/2511.07312</link>
<guid>https://arxiv.org/abs/2511.07312</guid>
<content:encoded><![CDATA[
arXiv:2511.07312v1 Announce Type: new 
Abstract: Few classical games have been regarded as such significant benchmarks of artificial intelligence as to have justified training costs in the millions of dollars. Among these, Stratego -- a board wargame exemplifying the challenge of strategic decision making under massive amounts of hidden information -- stands apart as a case where such efforts failed to produce performance at the level of top humans. This work establishes a step change in both performance and cost for Stratego, showing that it is now possible not only to reach the level of top humans, but to achieve vastly superhuman level -- and that doing so requires not an industrial budget, but merely a few thousand dollars. We achieved this result by developing general approaches for self-play reinforcement learning and test-time search under imperfect information.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-RAG: Long Context Multi-step Retrieval via Value-based Embedder Training</title>
<link>https://arxiv.org/abs/2511.07328</link>
<guid>https://arxiv.org/abs/2511.07328</guid>
<content:encoded><![CDATA[
arXiv:2511.07328v1 Announce Type: new 
Abstract: Retrieval-Augmented Generation (RAG) methods enhance LLM performance by efficiently filtering relevant context for LLMs, reducing hallucinations and inference cost. However, most existing RAG methods focus on single-step retrieval, which is often insufficient for answering complex questions that require multi-step search. Recently, multi-step retrieval approaches have emerged, typically involving the fine-tuning of small LLMs to perform multi-step retrieval. This type of fine-tuning is highly resource-intensive and does not enable the use of larger LLMs. In this work, we propose Q-RAG, a novel approach that fine-tunes the Embedder model for multi-step retrieval using reinforcement learning (RL). Q-RAG offers a competitive, resource-efficient alternative to existing multi-step retrieval methods for open-domain question answering and achieves state-of-the-art results on the popular long-context benchmarks Babilong and RULER for contexts up to 10M tokens.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preparation of Fractal-Inspired Computational Architectures for Advanced Large Language Model Analysis</title>
<link>https://arxiv.org/abs/2511.07329</link>
<guid>https://arxiv.org/abs/2511.07329</guid>
<content:encoded><![CDATA[
arXiv:2511.07329v1 Announce Type: new 
Abstract: It introduces FractalNet, a fractal-inspired computational architectures for advanced large language model analysis that mainly challenges model diversity on a large scale in an efficient manner. The new set-up involves a template-driven generator, runner, and evaluation framework that, through systematic permutations of convolutional, normalization, activation, and dropout layers, can create more than 1,200 variants of neural networks. Fractal templates allow for structural recursion and multi-column pathways, thus, models become deeper and wider in a balanced way. Training utilizes PyTorch, Automatic Mixed Precision (AMP), and gradient checkpointing and is carried out on the CIFAR-10 dataset for five epochs. The outcomes show that fractal-based architectures are capable of strong performance and are computationally efficient. The paper positions fractal design as a feasible and resource-efficient method of automated architecture exploration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding Computer Use Agents on Human Demonstrations</title>
<link>https://arxiv.org/abs/2511.07332</link>
<guid>https://arxiv.org/abs/2511.07332</guid>
<content:encoded><![CDATA[
arXiv:2511.07332v1 Announce Type: new 
Abstract: Building reliable computer-use agents requires grounding: accurately connecting natural language instructions to the correct on-screen elements. While large datasets exist for web and mobile interactions, high-quality resources for desktop environments are limited. To address this gap, we introduce GroundCUA, a large-scale desktop grounding dataset built from expert human demonstrations. It covers 87 applications across 12 categories and includes 56K screenshots, with every on-screen element carefully annotated for a total of over 3.56M human-verified annotations. From these demonstrations, we generate diverse instructions that capture a wide range of real-world tasks, providing high-quality data for model training. Using GroundCUA, we develop the GroundNext family of models that map instructions to their target UI elements. At both 3B and 7B scales, GroundNext achieves state-of-the-art results across five benchmarks using supervised fine-tuning, while requiring less than one-tenth the training data of prior work. Reinforcement learning post-training further improves performance, and when evaluated in an agentic setting on the OSWorld benchmark using o3 as planner, GroundNext attains comparable or superior results to models trained with substantially more data,. These results demonstrate the critical role of high-quality, expert-driven datasets in advancing general-purpose computer-use agents.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TNT: Improving Chunkwise Training for Test-Time Memorization</title>
<link>https://arxiv.org/abs/2511.07343</link>
<guid>https://arxiv.org/abs/2511.07343</guid>
<content:encoded><![CDATA[
arXiv:2511.07343v1 Announce Type: new 
Abstract: Recurrent neural networks (RNNs) with deep test-time memorization modules, such as Titans and TTT, represent a promising, linearly-scaling paradigm distinct from Transformers. While these expressive models do not yet match the peak performance of state-of-the-art Transformers, their potential has been largely untapped due to prohibitively slow training and low hardware utilization. Existing parallelization methods force a fundamental conflict governed by the chunksize hyperparameter: large chunks boost speed but degrade performance, necessitating a fixed, suboptimal compromise. To solve this challenge, we introduce TNT, a novel training paradigm that decouples training efficiency from inference performance through a two-stage process. Stage one is an efficiency-focused pre-training phase utilizing a hierarchical memory. A global module processes large, hardware-friendly chunks for long-range context, while multiple parallel local modules handle fine-grained details. Crucially, by periodically resetting local memory states, we break sequential dependencies to enable massive context parallelization. Stage two is a brief fine-tuning phase where only the local memory modules are adapted to a smaller, high-resolution chunksize, maximizing accuracy with minimal overhead. Evaluated on Titans and TTT models, TNT achieves a substantial acceleration in training speed-up to 17 times faster than the most accurate baseline configuration - while simultaneously improving model accuracy. This improvement removes a critical scalability barrier, establishing a practical foundation for developing expressive RNNs and facilitating future work to close the performance gap with Transformers.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Evaluating LLMs for Multi-Step Tasks: Stepwise Confidence Estimation for Failure Detection</title>
<link>https://arxiv.org/abs/2511.07364</link>
<guid>https://arxiv.org/abs/2511.07364</guid>
<content:encoded><![CDATA[
arXiv:2511.07364v1 Announce Type: new 
Abstract: Reliability and failure detection of large language models (LLMs) is critical for their deployment in high-stakes, multi-step reasoning tasks. Prior work explores confidence estimation for self-evaluating LLM-scorer systems, with confidence scorers estimating the likelihood of errors in LLM responses. However, most methods focus on single-step outputs and overlook the challenges of multi-step reasoning. In this work, we extend self-evaluation techniques to multi-step tasks, testing two intuitive approaches: holistic scoring and step-by-step scoring. Using two multi-step benchmark datasets, we show that stepwise evaluation generally outperforms holistic scoring in detecting potential errors, with up to 15% relative increase in AUC-ROC. Our findings demonstrate that self-evaluating LLM systems provide meaningful confidence estimates in complex reasoning, improving their trustworthiness and providing a practical framework for failure detection.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Private Sketches for Linear Regression</title>
<link>https://arxiv.org/abs/2511.07365</link>
<guid>https://arxiv.org/abs/2511.07365</guid>
<content:encoded><![CDATA[
arXiv:2511.07365v1 Announce Type: new 
Abstract: Linear regression is frequently applied in a variety of domains. In order to improve the efficiency of these methods, various methods have been developed that compute summaries or \emph{sketches} of the datasets. Certain domains, however, contain sensitive data which necessitates that the application of these statistical methods does not reveal private information. Differentially private (DP) linear regression methods have been developed for mitigating this problem. These techniques typically involve estimating a noisy version of the parameter vector. Instead, we propose releasing private sketches of the datasets. We present differentially private sketches for the problems of least squares regression, as well as least absolute deviations regression. The availability of these private sketches facilitates the application of commonly available solvers for regression, without the risk of privacy leakage.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency Is Not Always Correct: Towards Understanding the Role of Exploration in Post-Training Reasoning</title>
<link>https://arxiv.org/abs/2511.07368</link>
<guid>https://arxiv.org/abs/2511.07368</guid>
<content:encoded><![CDATA[
arXiv:2511.07368v1 Announce Type: new 
Abstract: Foundation models exhibit broad knowledge but limited task-specific reasoning, motivating post-training strategies such as RLVR and inference scaling with outcome or process reward models (ORM/PRM). While recent work highlights the role of exploration and entropy stability in improving pass@K, empirical evidence points to a paradox: RLVR and ORM/PRM typically reinforce existing tree-like reasoning paths rather than expanding the reasoning scope, raising the question of why exploration helps at all if no new patterns emerge.
  To reconcile this paradox, we adopt the perspective of Kim et al. (2025), viewing easy (e.g., simplifying a fraction) versus hard (e.g., discovering a symmetry) reasoning steps as low- versus high-probability Markov transitions, and formalize post-training dynamics through Multi-task Tree-structured Markov Chains (TMC). In this tractable model, pretraining corresponds to tree expansion, while post-training corresponds to chain-of-thought reweighting. We show that several phenomena recently observed in empirical studies arise naturally in this setting: (1) RLVR induces a squeezing effect, reducing reasoning entropy and forgetting some correct paths; (2) population rewards of ORM/PRM encourage consistency rather than accuracy, thereby favoring common patterns; and (3) certain rare, high-uncertainty reasoning paths by the base model are responsible for solving hard problem instances.
  Together, these explain why exploration -- even when confined to the base model's reasoning scope -- remains essential: it preserves access to rare but crucial reasoning traces needed for difficult cases, which are squeezed out by RLVR or unfavored by inference scaling. Building on this, we further show that exploration strategies such as rejecting easy instances and KL regularization help preserve rare reasoning traces. Empirical simulations corroborate our theoretical results.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Benefit of Curriculum in Transformer Tree-Reasoning Post-Training</title>
<link>https://arxiv.org/abs/2511.07372</link>
<guid>https://arxiv.org/abs/2511.07372</guid>
<content:encoded><![CDATA[
arXiv:2511.07372v1 Announce Type: new 
Abstract: Recent curriculum techniques in the post-training stage of LLMs have been widely observed to outperform non-curriculum approaches in enhancing reasoning performance, yet a principled understanding of why and to what extent they work remains elusive. To address this gap, we develop a theoretical framework grounded in the intuition that progressively learning through manageable steps is more efficient than directly tackling a hard reasoning task, provided each stage stays within the model's effective competence. Under mild complexity conditions linking consecutive curriculum stages, we show that curriculum post-training avoids the exponential complexity bottleneck.
  To substantiate this result, drawing insights from the Chain-of-Thoughts (CoTs) solving mathematical problems such as Countdown and parity, we model CoT generation as a states-conditioned autoregressive reasoning tree, define a uniform-branching base model to capture pretrained behavior, and formalize curriculum stages as either depth-increasing (longer reasoning chains) or hint-decreasing (shorter prefixes) subtasks. Our analysis shows that, under outcome-only reward signals, reinforcement learning finetuning achieves high accuracy with polynomial sample complexity, whereas direct learning suffers from an exponential bottleneck. We further establish analogous guarantees for test-time scaling, where curriculum-aware querying reduces both reward oracle calls and sampling cost from exponential to polynomial order.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers Provably Learn Chain-of-Thought Reasoning with Length Generalization</title>
<link>https://arxiv.org/abs/2511.07378</link>
<guid>https://arxiv.org/abs/2511.07378</guid>
<content:encoded><![CDATA[
arXiv:2511.07378v1 Announce Type: new 
Abstract: The ability to reason lies at the core of artificial intelligence (AI), and challenging problems usually call for deeper and longer reasoning to tackle. A crucial question about AI reasoning is whether models can extrapolate learned reasoning patterns to solve harder tasks with longer chain-of-thought (CoT). In this work, we present a theoretical analysis of transformers learning on synthetic state-tracking tasks with gradient descent. We mathematically prove how the algebraic structure of state-tracking problems governs the degree of extrapolation of the learned CoT. Specifically, our theory characterizes the length generalization of transformers through the mechanism of attention concentration, linking the retrieval robustness of the attention layer to the state-tracking task structure of long-context reasoning. Moreover, for transformers with limited reasoning length, we prove that a recursive self-training scheme can progressively extend the range of solvable problem lengths. To our knowledge, we provide the first optimization guarantee that constant-depth transformers provably learn $\mathsf{NC}^1$-complete problems with CoT, significantly going beyond prior art confined in $\mathsf{TC}^0$, unless the widely held conjecture $\mathsf{TC}^0 \neq \mathsf{NC}^1$ fails. Finally, we present a broad set of experiments supporting our theoretical results, confirming the length generalization behaviors and the mechanism of attention concentration.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LoReTTA: A Low Resource Framework To Poison Continuous Time Dynamic Graphs</title>
<link>https://arxiv.org/abs/2511.07379</link>
<guid>https://arxiv.org/abs/2511.07379</guid>
<content:encoded><![CDATA[
arXiv:2511.07379v1 Announce Type: new 
Abstract: Temporal Graph Neural Networks (TGNNs) are increasingly used in high-stakes domains, such as financial forecasting, recommendation systems, and fraud detection. However, their susceptibility to poisoning attacks poses a critical security risk. We introduce LoReTTA (Low Resource Two-phase Temporal Attack), a novel adversarial framework on Continuous-Time Dynamic Graphs, which degrades TGNN performance by an average of 29.47% across 4 widely benchmark datasets and 4 State-of-the-Art (SotA) models. LoReTTA operates through a two-stage approach: (1) sparsify the graph by removing high-impact edges using any of the 16 tested temporal importance metrics, (2) strategically replace removed edges with adversarial negatives via LoReTTA's novel degree-preserving negative sampling algorithm. Our plug-and-play design eliminates the need for expensive surrogate models while adhering to realistic unnoticeability constraints. LoReTTA degrades performance by upto 42.0% on MOOC, 31.5% on Wikipedia, 28.8% on UCI, and 15.6% on Enron. LoReTTA outperforms 11 attack baselines, remains undetectable to 4 leading anomaly detection systems, and is robust to 4 SotA adversarial defense training methods, establishing its effectiveness, unnoticeability, and robustness.
]]></content:encoded>
<pubDate>Tue, 11 Nov 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>