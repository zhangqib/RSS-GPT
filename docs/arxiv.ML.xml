<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>


<item>
<title>LLMscape</title>
<link>https://arxiv.org/abs/2511.07161</link>
<guid>https://arxiv.org/abs/2511.07161</guid>
<content:encoded><![CDATA[
<div> LLMscape, interactive installation, AI agents, shared uncertainty, meaning-making<br /><br />Summary:<br /><br />LLMscape is an interactive art installation that explores how humans and artificial intelligence (AI) collaboratively construct meaning in conditions marked by uncertainty. The installation features a mutable, projection-mapped landscape that participants can actively reshape, creating a dynamic environment. Multiple AI agents interact within this landscape, each generating partial and provisional interpretations of their surroundings rather than definitive conclusions. The project has been exhibited in Shanghai and continues to evolve, emphasizing the ongoing nature of meaning-making processes. Instead of portraying AI as mere deterministic tools, LLMscape reimagines these systems as embodied co-witnesses that share an unstable and ever-changing world with humans. By positioning AI and humans as parallel agents in interpreting reality, the work invites viewers to reflect on the inherent limits of knowledge—both human and artificial. The installation thus fosters deeper consideration of epistemic uncertainties and redefines the relationship between humans and AI in knowledge creation. <div>
arXiv:2511.07161v3 Announce Type: replace 
Abstract: LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Algorithms for Mobile Health Interventions with Active Querying Optimization</title>
<link>https://arxiv.org/abs/2512.08950</link>
<guid>https://arxiv.org/abs/2512.08950</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, mobile health, Act-Then-Measure, Bayesian update, Q-learning<br /><br />Summary:<br /><br />1. The paper addresses reinforcement learning challenges in mobile health (mHealth) interventions, where balancing intervention effectiveness and minimizing user burden is critical due to costly state measurements like surveys or feedback. <br />2. The Act-Then-Measure (ATM) heuristic tackles this by separating control actions from measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework, but the traditional ATM uses Q-learning, which can be unstable in sparse and noisy settings. <br />3. The authors propose a Bayesian extension to ATM that replaces Q-learning with a Kalman filter-style Bayesian update, allowing uncertainty-aware Q-value estimates, leading to more stable and sample-efficient learning. <br />4. Experimental results in small tabular environments show that Bayesian ATM achieves comparable or better scalarized returns with lower variance and more stable policies compared to standard ATM. <br />5. However, in larger, more complex mHealth domains, both ATM variants struggle due to mismatches between ATM assumptions and real-world problem structure, emphasizing the need for new RL algorithms that incorporate causal reasoning, continuous states, delayed feedback, and observation cost considerations. <div>
arXiv:2512.08950v1 Announce Type: new 
Abstract: Reinforcement learning in mobile health (mHealth) interventions requires balancing intervention efficacy with user burden, particularly when state measurements (for example, user surveys or feedback) are costly yet essential. The Act-Then-Measure (ATM) heuristic addresses this challenge by decoupling control and measurement actions within the Action-Contingent Noiselessly Observable Markov Decision Process (ACNO-MDP) framework. However, the standard ATM algorithm relies on a temporal-difference-inspired Q-learning method, which is prone to instability in sparse and noisy environments. In this work, we propose a Bayesian extension to ATM that replaces standard Q-learning with a Kalman filter-style Bayesian update, maintaining uncertainty-aware estimates of Q-values and enabling more stable and sample-efficient learning. We evaluate our method in both toy environments and clinically motivated testbeds. In small, tabular environments, Bayesian ATM achieves comparable or improved scalarized returns with substantially lower variance and more stable policy behavior. In contrast, in larger and more complex mHealth settings, both the standard and Bayesian ATM variants perform poorly, suggesting a mismatch between ATM's modeling assumptions and the structural challenges of real-world mHealth domains. These findings highlight the value of uncertainty-aware methods in low-data settings while underscoring the need for new RL algorithms that explicitly model causal structure, continuous states, and delayed feedback under observation cost constraints.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Ask: Simulation-Trained Humanoids for Mental-Health Diagnosis</title>
<link>https://arxiv.org/abs/2512.08952</link>
<guid>https://arxiv.org/abs/2512.08952</guid>
<content:encoded><![CDATA[
<div> Keywords: humanoid robots, conversational agent, nonverbal dynamics, TD3 controller, simulation training  

<br /><br />Summary:  
This work addresses the challenges of testing humanoid robots for mental health screening, highlighting the difficulties posed by slow user testing, hardware wear, and limited iteration diversity. It introduces a virtualized conversational agent that simulates humanoid behavior using 276 Unreal Engine MetaHuman patient models with synchronized speech, gaze, facial expressions, and body poses, integrated with PHQ-8 and PCL-C diagnostic flows. The system employs a perception-fusion-policy loop that dynamically manages conversational timing, prosody, backchannels, and interruption avoidance within a safety framework. Training leverages counterfactual replay—bounded nonverbal perturbations—and an uncertainty-aware turn manager to reduce diagnostic ambiguity. Performance comparisons of three reinforcement learning controllers (TD3, PPO, and CEM) demonstrate that the custom TD3 algorithm outperforms others by achieving near-ceiling coverage, maintaining steadier pacing, and enhancing decision quality. Key metrics include minimal turn overlaps, aligned cut timing, fewer clarification requests, and reduced wait times. Importantly, TD3’s performance remains stable despite modality dropouts and renderer changes, with consistent rankings observed on unseen patient data. The contributions include (1) an agent-centred simulator with interactive counterfactual patients, (2) a safe learning framework prioritizing timing and rapport, (3) a comparative RL study showcasing TD3’s advantages, and (4) robustness analyses that support clinician-supervised humanoid deployments. <div>
arXiv:2512.08952v1 Announce Type: new 
Abstract: Testing humanoid robots with users is slow, causes wear, and limits iteration and diversity. Yet screening agents must master conversational timing, prosody, backchannels, and what to attend to in faces and speech for Depression and PTSD. Most simulators omit policy learning with nonverbal dynamics; many controllers chase task accuracy while underweighting trust, pacing, and rapport. We virtualise the humanoid as a conversational agent to train without hardware burden. Our agent-centred, simulation-first pipeline turns interview data into 276 Unreal Engine MetaHuman patients with synchronised speech, gaze/face, and head-torso poses, plus PHQ-8 and PCL-C flows. A perception-fusion-policy loop decides what and when to speak, when to backchannel, and how to avoid interruptions, under a safety shield. Training uses counterfactual replay (bounded nonverbal perturbations) and an uncertainty-aware turn manager that probes to reduce diagnostic ambiguity. Results are simulation-only; the humanoid is the transfer target. In comparing three controllers, a custom TD3 (Twin Delayed DDPG) outperformed PPO and CEM, achieving near-ceiling coverage with steadier pace at comparable rewards. Decision-quality analyses show negligible turn overlap, aligned cut timing, fewer clarification prompts, and shorter waits. Performance stays stable under modality dropout and a renderer swap, and rankings hold on a held-out patient split. Contributions: (1) an agent-centred simulator that turns interviews into 276 interactive patients with bounded nonverbal counterfactuals; (2) a safe learning loop that treats timing and rapport as first-class control variables; (3) a comparative study (TD3 vs PPO/CEM) with clear gains in completeness and social timing; and (4) ablations and robustness analyses explaining the gains and enabling clinician-supervised humanoid pilots.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Electrocardiogram Multi-task Benchmark with Comprehensive Evaluations and Insightful Findings</title>
<link>https://arxiv.org/abs/2512.08954</link>
<guid>https://arxiv.org/abs/2512.08954</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG analysis, foundation models, self-supervised learning, time-series models, AI in healthcare<br /><br />Summary: This study investigates the utility of foundation models in analyzing Electrocardiogram (ECG) data, which is crucial for non-invasive cardiac diagnosis. Traditional ECG analysis requires expert knowledge, posing challenges for AI application in healthcare. Leveraging advances in self-supervised learning, the research compares the performance of language-based, general time-series, and specialized ECG foundation models against conventional time-series deep learning models. Experimental results show that general time-series and ECG foundation models achieve up to 80% top performance, suggesting notable effectiveness in ECG interpretation. The paper provides in-depth analyses highlighting both the strengths and limitations of foundation models when applied to physiological waveform data, offering insights into potential improvements. Additionally, the study contributes to the research community by sharing a comprehensive benchmark dataset and code via a public GitHub repository, facilitating further development in this domain. Overall, the research underscores the promising role of foundation models in advancing AI-driven ECG analysis and the broader field of physiological signal processing. <div>
arXiv:2512.08954v1 Announce Type: new 
Abstract: In the process of patient diagnosis, non-invasive measurements are widely used due to their low risks and quick results. Electrocardiogram (ECG), as a non-invasive method to collect heart activities, is used to diagnose cardiac conditions. Analyzing the ECG typically requires domain expertise, which is a roadblock to applying artificial intelligence (AI) for healthcare. Through advances in self-supervised learning and foundation models, AI systems can now acquire and leverage domain knowledge without relying solely on human expertise. However, there is a lack of comprehensive analyses over the foundation models' performance on ECG. This study aims to answer the research question: "Are Foundation Models Useful for ECG Analysis?" To address it, we evaluate language/general time-series/ECG foundation models in comparison with time-series deep learning models. The experimental results show that general time-series/ECG foundation models achieve a top performance rate of 80%, indicating their effectiveness in ECG analysis. In-depth analyses and insights are provided along with comprehensive experimental results. This study highlights the limitations and potential of foundation models in advancing physiological waveform analysis. The data and code for this benchmark are publicly available at https://github.com/yuhaoxu99/ECGMultitasks-Benchmark.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM4XCE: Large Language Models for Extremely Large-Scale Massive MIMO Channel Estimation</title>
<link>https://arxiv.org/abs/2512.08955</link>
<guid>https://arxiv.org/abs/2512.08955</guid>
<content:encoded><![CDATA[
<div> Keywords: XL-MIMO, channel estimation, large language models, hybrid-field channels, semantic communication<br /><br />Summary: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) technology is vital for 6G networks due to its ability to provide vast spatial degrees of freedom. However, the coexistence of near-field and far-field effects in hybrid-field channels complicates channel estimation, where conventional approaches often fail to generalize well. This paper introduces LLM4XCE, a novel channel estimation framework that leverages large language models (LLMs) to capture essential spatial-channel representations, aligning with the recent trend toward semantic communication which prioritizes task-oriented understanding. The proposed system features a specialized embedding module combined with Parallel Feature-Spatial Attention to deeply fuse pilot signal features with spatial structural information, generating a semantically enriched representation suitable as input for LLMs. By fine-tuning only the top two layers of the Transformer architecture, LLM4XCE efficiently learns latent dependencies in the pilot data while maintaining training efficiency. Extensive simulation results demonstrate that this approach markedly surpasses state-of-the-art methods in hybrid-field scenarios by offering higher estimation accuracy and enhanced generalization capabilities, highlighting the efficacy of integrating language model techniques into wireless channel estimation tasks. <div>
arXiv:2512.08955v1 Announce Type: new 
Abstract: Extremely large-scale massive multiple-input multiple-output (XL-MIMO) is a key enabler for sixth-generation (6G) networks, offering massive spatial degrees of freedom. Despite these advantages, the coexistence of near-field and far-field effects in hybrid-field channels presents significant challenges for accurate estimation, where traditional methods often struggle to generalize effectively. In recent years, large language models (LLMs) have achieved impressive performance on downstream tasks via fine-tuning, aligning with the semantic communication shift toward task-oriented understanding over bit-level accuracy.
  Motivated by this, we propose Large Language Models for XL-MIMO Channel Estimation (LLM4XCE), a novel channel estimation framework that leverages the semantic modeling capabilities of large language models to recover essential spatial-channel representations for downstream tasks. The model integrates a carefully designed embedding module with Parallel Feature-Spatial Attention, enabling deep fusion of pilot features and spatial structures to construct a semantically rich representation for LLM input. By fine-tuning only the top two Transformer layers, our method effectively captures latent dependencies in the pilot data while ensuring high training efficiency. Extensive simulations demonstrate that LLM4XCE significantly outperforms existing state-of-the-art methods under hybrid-field conditions, achieving superior estimation accuracy and generalization performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability</title>
<link>https://arxiv.org/abs/2512.08956</link>
<guid>https://arxiv.org/abs/2512.08956</guid>
<content:encoded><![CDATA[
<div> Keywords: K-Nearest Neighbors, DW-KNN, distance weighting, neighbor validity, interpretability

<br /><br />Summary:  
The article addresses a key limitation in standard distance-weighted K-Nearest Neighbors (KNN) classifiers where all neighbors are assumed equally reliable, which reduces effectiveness in heterogeneous feature spaces. To overcome this, the authors introduce DW-KNN (Double Weighted KNN), a new variant that combines exponential distance weighting with neighbor validity to enhance prediction reliability. DW-KNN provides instance-level interpretability, allowing users to better understand the influence of specific neighbors on classifications. It also improves robustness by suppressing the impact of noisy or mislabeled samples, and reduces sensitivity to hyperparameter choices, making it simpler to tune. The authors evaluated DW-KNN on nine datasets, achieving an average accuracy of 0.8988, ranking it second among six tested methods and just 0.2% below the highest-performing Ensemble KNN method. It also demonstrated the lowest cross-validation variance (0.0156), indicating stronger stability and consistency in predictions. Statistical tests confirmed significant performance improvements over compactness weighted KNN (+4.09%) and kernel weighted KNN (+1.13%) with p-values less than 0.001. Overall, DW-KNN offers a straightforward yet powerful alternative to more complex adaptive KNN schemes, especially valuable for applications demanding explainable and reliable machine learning models. <div>
arXiv:2512.08956v1 Announce Type: new 
Abstract: K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUMOS: Large User MOdels for User Behavior Prediction</title>
<link>https://arxiv.org/abs/2512.08957</link>
<guid>https://arxiv.org/abs/2512.08957</guid>
<content:encoded><![CDATA[
<div> Keywords: user behavior prediction, transformer, multi-task learning, cross-attention, multi-modal tokenization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of predicting user behavior at scale for online B2C platforms, highlighting limitations of traditional task-specific models and manual feature engineering.  
2. The authors introduce LUMOS (Large User MOdel Series), a transformer-based architecture that jointly learns multiple tasks using raw user activity data, removing the need for task-specific models and domain expertise.  
3. A novel cross-attention mechanism is presented in LUMOS, which conditions predictions on future known events such as holidays or sales, allowing the model to anticipate complex behavior patterns influenced by these events.  
4. The architecture integrates multi-modal tokenization, combining user transactions, event context, and static user demographic data into comprehensive embeddings processed through specialized pathways.  
5. Extensive experiments on a massive production dataset containing 275 billion tokens from 250 million users demonstrate that LUMOS outperforms traditional models, achieving an average ROC-AUC improvement of 0.025 for classification tasks and a 4.6% reduction in MAPE for regression tasks. Moreover, online A/B testing confirms these improvements result in a 3.15% increase in Daily Active Users, validating real business impact. <div>
arXiv:2512.08957v1 Announce Type: new 
Abstract: User behavior prediction at scale remains a critical challenge for online B2C platforms. Traditional approaches rely heavily on task-specific models and domain-specific feature engineering. This is time-consuming, computationally expensive, and requires domain expertise and therefore not scalable. We present LUMOS (Large User MOdel Series), a transformer-based architecture that eliminates task-specific models and manual feature engineering by learning multiple tasks jointly using only raw user activity data. LUMOS introduces a novel cross-attention mechanism that conditions predictions on future known events (e.g., holidays, sales, etc.), enabling the model to predict complex behaviour patterns like "how will upcoming holidays affect user engagement?" The architecture also employs multi-modal tokenization, combining user transactions, event context, and static user demographic attributes into rich representations processed through specialized embedding pathways.
  Through extensive experiments on a production dataset spanning 275 billion user activity tokens from 250 million users, we demonstrate that LUMOS achieves superior performance compared to traditional task-specific models. Across 5 tasks with established baselines, we achieve an average improvement of 0.025 in ROC-AUC for binary classification tasks and 4.6\% reduction in MAPE for regression tasks. Online A/B testing validates these improvements translate to measurable business impact with a 3.15\% increase in Daily Active Users.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EEG-Bench: A Benchmark for EEG Foundation Models in Clinical Applications</title>
<link>https://arxiv.org/abs/2512.08959</link>
<guid>https://arxiv.org/abs/2512.08959</guid>
<content:encoded><![CDATA[
<div> EEG, foundation models, clinical applications, benchmarking, diagnostic tasks<br /><br />Summary:<br /><br />1. This article introduces a unified benchmarking framework designed to evaluate EEG-based foundation models specifically in clinical applications.<br /><br />2. The benchmark covers 11 well-defined diagnostic tasks derived from 14 publicly available EEG datasets, encompassing a range of conditions such as epilepsy, schizophrenia, Parkinson's disease, obsessive-compulsive disorder (OCD), and mild traumatic brain injury.<br /><br />3. Key features of the framework include minimal preprocessing requirements, standardized evaluation protocols, and the capability to enable direct comparisons between classical baseline models and modern foundation models.<br /><br />4. The study’s results reveal that although foundation models demonstrate strong performance in certain scenarios, simpler models often remain competitive, especially when accounting for distribution shifts commonly observed in clinical settings.<br /><br />5. To support reproducibility and encourage wider adoption, the authors release all prepared datasets and code in an accessible and extensible format, facilitating future research and benchmarking efforts in this domain. <div>
arXiv:2512.08959v1 Announce Type: new 
Abstract: We introduce a unified benchmarking framework focused on evaluating EEG-based foundation models in clinical applications. The benchmark spans 11 well-defined diagnostic tasks across 14 publicly available EEG datasets, including epilepsy, schizophrenia, Parkinson's disease, OCD, and mild traumatic brain injury. It features minimal preprocessing, standardized evaluation protocols, and enables side-by-side comparisons of classical baselines and modern foundation models. Our results show that while foundation models achieve strong performance in certain settings, simpler models often remain competitive, particularly under clinical distribution shifts. To facilitate reproducibility and adoption, we release all prepared data and code in an accessible and extensible format.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resolving Conflicts in Lifelong Learning via Aligning Updates in Subspaces</title>
<link>https://arxiv.org/abs/2512.08960</link>
<guid>https://arxiv.org/abs/2512.08960</guid>
<content:encoded><![CDATA[
<div> Low-Rank Adaptation, Continual Learning, Catastrophic Forgetting, Parameter Stability, Gradient Alignment<br /><br />Summary:<br /><br />Low-Rank Adaptation (LoRA) is a technique that facilitates efficient continual learning but tends to suffer from catastrophic forgetting, primarily due to destructive interference between tasks. The core issue arises from antagonistic directional updates, where gradients from new tasks conflict directly with the historical weight trajectory of the model, leading to degradation in performance. To address this challenge, the authors propose PS-LoRA (Parameter Stability LoRA), a novel framework that resolves update conflicts by aligning gradients within the optimization subspace. PS-LoRA introduces a dual-regularization objective designed to penalize conflicting directional updates while constraining magnitude deviations, ensuring updates remain consistent with previously acquired knowledge. In addition to this regularization strategy, the framework features a magnitude-based merging technique that consolidates sequential adapters into a single robust representation without requiring retraining. Experimental validations on both Natural Language Processing (NLP) and Vision benchmarks demonstrate that PS-LoRA outperforms existing state-of-the-art methods. This improvement is achieved by effectively preserving the stability of learned representations, thereby enabling the model to efficiently adapt to new domains while mitigating catastrophic forgetting in continual learning settings. <div>
arXiv:2512.08960v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) enables efficient Continual Learning but often suffers from catastrophic forgetting due to destructive interference between tasks. Our analysis reveals that this degradation is primarily driven by antagonistic directional updates where new task gradients directly oppose the historical weight trajectory. To address this, we propose PS-LoRA (Parameter Stability LoRA), a framework designed to resolve conflicts by aligning updates within the optimization subspace. Our approach employs a dual-regularization objective that penalizes conflicting directions and constrains magnitude deviations to ensure consistency with prior knowledge. Additionally, we implement a magnitude-based merging strategy to consolidate sequential adapters into a robust representation without retraining. Experiments on NLP and Vision benchmarks show that PS-LoRA outperforms state-of-the-art methods by preserving the stability of learned representations while efficiently adapting to new domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEA: Spectral Edge Attacks on Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.08964</link>
<guid>https://arxiv.org/abs/2512.08964</guid>
<content:encoded><![CDATA[
<div> Spectral Edge Attacks, Graph Neural Networks, adversarial attacks, spectral embedding, robustness evaluation  

<br /><br />Summary:  
Graph Neural Networks (GNNs) perform well on graph-structured data but are vulnerable to small, deliberate changes in the graph structure. Most existing attacks rely on gradients or local connectivity and treat all edges equally when deciding which to manipulate. This paper introduces Spectral Edge Attacks (SEA), a novel set of adversarial attacks using spectral robustness evaluation to guide structural changes more effectively. The core idea is to compute a spectral embedding that identifies fragile directions in the input graph manifold, allowing assignment of a robustness score to each edge or non-edge. SEA proposes two attack types: (i) Spade-guided deletion, which removes the most robust edges negatively impacting spectral stability, and (ii) Spade-guided addition, which inserts edges between nodes whose spectral embeddings indicate maximum incompatibility, thus disrupting the graph structure. Both attacks are graph-level, model-aware but simple in design, not requiring gradients and easily integrated into existing GNNs. The paper details the spectral theory behind the method, the attack algorithms, and evaluates performance across standard benchmark datasets, demonstrating improved effectiveness and a new perspective on adversarial robustness in graph learning. <div>
arXiv:2512.08964v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) achieve strong performance on graph-structured data, but are notoriously vulnerable to small, carefully crafted perturbations of the graph structure. Most existing structure-based attacks rely on gradient-based heuristics or local connectivity patterns, and treat edges as equally important candidates for manipulation. In this paper, we propose Spectral Edge Attacks (SEA), a new family of adversarial attacks that explicitly leverage spectral robustness evaluation to guide structural perturbations. Our key idea is to compute a spectral embedding that captures the most fragile directions of the input manifold and to use it to assign a robustness score to each edge or non-edge. Based on these scores, we introduce two complementary attack variants: (i) a Spade-guided deletion attack that removes the most spectrally robust edges, and (ii) a Spade-guided addition attack that inserts edges between nodes that are maximally incompatible in the fragile spectral space. Both attacks operate at the graph level, are model-aware but conceptually simple, and can be plugged into existing GNN architectures without requiring gradients. We describe the spectral formulation, the attack algorithms, and experiments on benchmarks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Instruction Following Evaluation (FIFE)</title>
<link>https://arxiv.org/abs/2512.08965</link>
<guid>https://arxiv.org/abs/2512.08965</guid>
<content:encoded><![CDATA[
<div> Keywords: Language Models, Financial Analysis, Instruction Following, Benchmark, Reinforcement Learning<br /><br />Summary: This paper introduces FIFE, a challenging benchmark specifically designed to evaluate language models' abilities to follow complex, interdependent instructions in financial analysis tasks where accuracy is paramount. FIFE includes 88 human-authored prompts and a verification system utilizing chainable and verifiable constraints, providing fine-grained reward signals to assess model performance more precisely. The authors evaluate 53 language models, including proprietary, open-weight, and open-source variants, under zero-shot conditions. Results reveal a clear performance hierarchy: the top open-weight model achieves a strict accuracy of 76.1% and a loose accuracy of 79.5%, outperforming the best proprietary model at 65.9% strict and 70.5% loose accuracy. Open-source models trail behind significantly, with 45.5% strict and 48.9% loose scores. Despite these differences, even the best-performing models do not meet perfect compliance with FIFE's demanding requirements, highlighting ongoing challenges. The research contributes to advancing reinforcement learning applications in finance by releasing the dataset and code openly, encouraging further community-driven improvement in instruction-following capabilities for high-stakes financial tasks. <div>
arXiv:2512.08965v1 Announce Type: new 
Abstract: Language Models (LMs) struggle with complex, interdependent instructions, particularly in high-stakes domains like finance where precision is critical. We introduce FIFE, a novel, high-difficulty benchmark designed to assess LM instruction-following capabilities for financial analysis tasks. FIFE comprises 88 human-authored prompts and employs a verification system with chainable, verifiable constraints for fine-grained reward signals. We evaluate 53 models (proprietary, open-weight, open-source) in a zero-shot setting. Our key findings reveal a clear performance hierarchy: the top open-weight model (76.1 strict / 79.5 loose) surpasses the leading proprietary system (65.9 strict / 70.5 loose), while the best open-source models lag significantly (45.5 strict / 48.9 loose). However, even top-performing models struggle with FIFE's complex requirements, failing to achieve perfect compliance. We release our dataset and code as an open-source resource to promote research in Reinforcement Learning for the financial domain.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CluCERT: Certifying LLM Robustness via Clustering-Guided Denoising Smoothing</title>
<link>https://arxiv.org/abs/2512.08967</link>
<guid>https://arxiv.org/abs/2512.08967</guid>
<content:encoded><![CDATA[
<div> adversarial robustness, Large Language Models, semantic clustering, denoising smoothing, computational efficiency<br /><br />Summary:<br /><br />Recent advances in Large Language Models (LLMs) have increased their use in various applications, but these models remain vulnerable to adversarial attacks where small, meaning-preserving changes like synonym substitutions can cause incorrect outputs. Robustness certification against such adversarial prompts is crucial. Existing methods typically rely on word deletion or simple denoising to achieve robustness certification but have two main drawbacks: they produce loose robustness bounds because they lack semantic validation of perturbed outputs, and they are computationally expensive due to repetitive sampling. To overcome these issues, the paper introduces CluCERT, a new framework leveraging clustering-guided denoising smoothing. CluCERT improves robustness certification by applying a semantic clustering filter that removes noisy perturbations while preserving meaningful changes, supported by theoretical analysis to ensure tighter robustness bounds. Additionally, the method increases computational efficiency through a refine module to extract core semantic information and a fast synonym substitution strategy that speeds up the denoising process. Extensive experimental evaluation across various downstream tasks and jailbreak defense scenarios shows that CluCERT outperforms existing certified approaches in terms of both tighter robustness guarantees and reduced computational costs. <div>
arXiv:2512.08967v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have led to their widespread adoption in daily applications. Despite their impressive capabilities, they remain vulnerable to adversarial attacks, as even minor meaning-preserving changes such as synonym substitutions can lead to incorrect predictions. As a result, certifying the robustness of LLMs against such adversarial prompts is of vital importance. Existing approaches focused on word deletion or simple denoising strategies to achieve robustness certification. However, these methods face two critical limitations: (1) they yield loose robustness bounds due to the lack of semantic validation for perturbed outputs and (2) they suffer from high computational costs due to repeated sampling. To address these limitations, we propose CluCERT, a novel framework for certifying LLM robustness via clustering-guided denoising smoothing. Specifically, to achieve tighter certified bounds, we introduce a semantic clustering filter that reduces noisy samples and retains meaningful perturbations, supported by theoretical analysis. Furthermore, we enhance computational efficiency through two mechanisms: a refine module that extracts core semantics, and a fast synonym substitution strategy that accelerates the denoising process. Finally, we conduct extensive experiments on various downstream tasks and jailbreak defense scenarios. Experimental results demonstrate that our method outperforms existing certified approaches in both robustness bounds and computational efficiency.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StructuredDNA: A Bio-Physical Framework for Energy-Aware Transformer Routing</title>
<link>https://arxiv.org/abs/2512.08968</link>
<guid>https://arxiv.org/abs/2512.08968</guid>
<content:encoded><![CDATA[
<div> Keywords: StructuredDNA, energy-aware routing, sparse architecture, Transformer, semantic energy minimization<br /><br />Summary:  
The paper introduces StructuredDNA, a novel sparse architecture framework designed to optimize Transformer routing through energy-awareness and modularity. Inspired by biological systems, StructuredDNA replaces conventional dense Mixture-of-Experts (MoE) routing with a bio-physical routing layer that minimizes semantic energy, effectively grouping inputs into semantic codons and selecting experts by minimizing a global energy functional combining cohesion, uncertainty, and computational cost. The approach is validated on both specialized (BioASQ) and open-domain (WikiText-103) benchmarks, achieving a remarkable 97.7% reduction in Energy Utilization Density (EUD) on BioASQ with a Semantic Stability Index (SSI) of 0.998, indicating highly stable semantic routing. Furthermore, when scaling expert granularity to K=2048 on WikiText-103, the architecture maintains over 99% energy efficiency, demonstrating its generalization and scalability. StructuredDNA establishes a domain-agnostic framework linking bio-physical principles to sparse expert routing, promising future developments in energy-efficient, modular, and scalable computational models. The authors acknowledge limitations as a proof-of-concept and outline future work for scaling to larger models, datasets, and hardware platforms. The implementation is openly available at the provided GitHub repository, encouraging further exploration and adoption. <div>
arXiv:2512.08968v1 Announce Type: new 
Abstract: The rapid scaling of large computational models has led to a critical increase in energy and compute costs. Inspired by biological systems where structure and function emerge from low-energy configurations, we introduce StructuredDNA, a sparse architecture framework for modular, energy-aware Transformer routing. StructuredDNA replaces dense Mixture-of-Experts routing with a bio-physical, energy-guided routing layer based on semantic energy minimization. Inputs are dynamically grouped into semantic codons, and routing selects a single expert by minimizing a global energy functional that combines cohesion, uncertainty, and computational cost.
  We validate StructuredDNA on both specialized (BioASQ) and open-domain benchmarks (WikiText-103). On BioASQ (K = 50), we achieve a 97.7% reduction in Energy Utilization Density (EUD) and a Semantic Stability Index (SSI) of 0.998. We further demonstrate a Semantic Scaling Law on WikiText-103, showing that the architecture generalizes to open domains by scaling expert granularity (K = 2048) while maintaining more than 99% energy efficiency. StructuredDNA thus establishes a robust, domain-agnostic paradigm for future sparse computational frameworks.
  StructuredDNA provides an explicit link between bio-physical principles and sparse expert routing in Transformer architectures, and points toward future energy-aware, modular, and scalable computational systems. We discuss limitations of this proof-of-concept study and outline directions for scaling the approach to larger models, datasets, and hardware platforms. The StructuredDNA implementation is available at https://github.com/InnoDeep-repos/StructuredDNA .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Robust Representations for Malicious Content Detection via Contrastive Sampling and Uncertainty Estimation</title>
<link>https://arxiv.org/abs/2512.08969</link>
<guid>https://arxiv.org/abs/2512.08969</guid>
<content:encoded><![CDATA[
<div> Uncertainty Contrastive Framework, Positive-Unlabeled Learning, Adaptive Temperature Scaling, Self-Attention LSTM Encoder, Malicious Content Classification<br /><br />Summary:  
This paper introduces the Uncertainty Contrastive Framework (UCF), a novel approach designed for Positive-Unlabeled (PU) representation learning that enhances classification performance in noisy and imbalanced datasets. The framework incorporates an uncertainty-aware contrastive loss, which dynamically adjusts contrastive weighting based on the confidence of each sample, ensuring that more reliable data influences learning. UCF also utilizes adaptive temperature scaling that modifies temperature parameters in response to batch-level variability, helping to stabilize training. A self-attention-guided LSTM encoder is implemented to better capture contextual information within sequences and improve embedding quality. UCF employs positive anchors during training to further enhance stability and robustness. When applied to malicious content classification tasks, embeddings generated by UCF enable traditional classifiers to achieve exceptional results, with accuracy exceeding 93.38%, precision over 0.93, near-perfect recall, very few false negatives, and competitive ROC-AUC values. Visual analysis of the embeddings confirms a clear separation between positive and unlabeled instances, demonstrating UCF’s capacity for producing discriminative and well-calibrated embeddings. The framework shows promise as a scalable and robust solution for PU learning in critical applications such as cybersecurity and biomedical text mining. <div>
arXiv:2512.08969v1 Announce Type: new 
Abstract: We propose the Uncertainty Contrastive Framework (UCF), a Positive-Unlabeled (PU) representation learning framework that integrates uncertainty-aware contrastive loss, adaptive temperature scaling, and a self-attention-guided LSTM encoder to improve classification under noisy and imbalanced conditions. UCF dynamically adjusts contrastive weighting based on sample confidence, stabilizes training using positive anchors, and adapts temperature parameters to batch-level variability. Applied to malicious content classification, UCF-generated embeddings enable multiple traditional classifiers to achieve more than 93.38% accuracy, precision above 0.93, and near-perfect recall, with minimal false negatives and competitive ROC-AUC scores. Visual analyses confirm clear separation between positive and unlabeled instances, highlighting the framework's ability to produce calibrated, discriminative embeddings. These results position UCF as a robust and scalable solution for PU learning in high-stakes domains such as cybersecurity and biomedical text mining.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Peek-a-Boo Reasoning: Contrastive Region Masking in MLLMs</title>
<link>https://arxiv.org/abs/2512.08976</link>
<guid>https://arxiv.org/abs/2512.08976</guid>
<content:encoded><![CDATA[
<div> Contrastive Region Masking, multimodal large language models, chain-of-thought reasoning, visual benchmarks, robustness<br /><br />Summary:  
This paper introduces Contrastive Region Masking (CRM), a novel, training-free diagnostic method designed to analyze how multimodal large language models (MLLMs) rely on specific visual regions during each step of chain-of-thought (CoT) reasoning. Unlike traditional methods that focus only on final answers or attention visualization, CRM causally attributes model behavior on a step-by-step basis by masking annotated visual regions and comparing the resulting reasoning paths against unmasked cases. When applied to datasets such as VisArgs, CRM uncovers varied failure modes in MLLMs: some models maintain reasoning coherence but hallucinate information when visual evidence is absent, while others strongly ground themselves in visual cues yet become unstable when those cues are perturbed. By shifting evaluation emphasis from the accuracy of final answers to the faithfulness and robustness of intermediate reasoning processes, CRM encourages reconsideration of visual benchmarks not just as performance metrics but as diagnostic tools for reasoning fidelity. This approach highlights the importance of developing multimodal evaluation frameworks that better capture reasoning transparency, robustness, and reliability beyond surface-level correctness. <div>
arXiv:2512.08976v1 Announce Type: new 
Abstract: We introduce Contrastive Region Masking (CRM), a training free diagnostic that reveals how multimodal large language models (MLLMs) depend on specific visual regions at each step of chain-of-thought (CoT) reasoning. Unlike prior approaches limited to final answers or attention maps, CRM provides causal, step-level attri- bution by systematically masking annotated regions and contrasting the resulting reasoning traces with unmasked baselines. Applied to datasets such as VisArgs, CRM reveals distinct failure modes: some models preserve reasoning structure, but hallucinate when evidence is missing, while others ground tightly to visual cues yet collapse under perturbations. By shifting the evaluation from correctness of an- swers to faithfulness of reasoning, CRM reframes visual benchmarks as diagnostic tools, highlighting the need for multimodal evaluation frameworks that measure not just performance, but also robustness and fidelity of reasoning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Deep Learning for Intracranial Aneurysm Blood Flow Simulation and Risk Assessment</title>
<link>https://arxiv.org/abs/2512.09013</link>
<guid>https://arxiv.org/abs/2512.09013</guid>
<content:encoded><![CDATA[
<div> Intracranial aneurysms, hemodynamics, graph neural network, wall shear stress, real-time simulation<br /><br />Summary:  
This article addresses the challenge of accurately and quickly assessing rupture risk in intracranial aneurysms by focusing on local hemodynamics, specifically wall shear stress and oscillatory shear index, which are critical factors influencing aneurysm stability. Traditional computational fluid dynamics (CFD) simulations provide high-fidelity insights but are computationally expensive and require specialized expertise, limiting their clinical use. Alternative imaging methods like 4D Flow MRI offer direct in vivo measurements but lack the spatial resolution needed to capture fine-scale shear patterns and are costly and impractical for routine use. To bridge this gap, the authors propose a novel graph neural network (GNN) surrogate model capable of predicting full-field hemodynamics—including blood flow, wall shear stress, and oscillatory shear index—from vascular geometries within less than one minute per cardiac cycle. The model is trained on a large dataset of high-fidelity patient-specific aneurysm simulations and effectively generalizes to new geometries and inflow conditions without requiring mesh-specific calibration. This approach facilitates near real-time inference, can integrate with existing clinical imaging pipelines, and provides physically grounded, high-resolution flow predictions. Ultimately, this work transforms CFD from a niche research tool into a practical, data-driven decision support system for bedside aneurysm analysis, enabling faster and more accessible clinical assessment. <div>
arXiv:2512.09013v1 Announce Type: new 
Abstract: Intracranial aneurysms remain a major cause of neurological morbidity and mortality worldwide, where rupture risk is tightly coupled to local hemodynamics particularly wall shear stress and oscillatory shear index. Conventional computational fluid dynamics simulations provide accurate insights but are prohibitively slow and require specialized expertise. Clinical imaging alternatives such as 4D Flow MRI offer direct in-vivo measurements, yet their spatial resolution remains insufficient to capture the fine-scale shear patterns that drive endothelial remodeling and rupture risk while being extremely impractical and expensive.
  We present a graph neural network surrogate model that bridges this gap by reproducing full-field hemodynamics directly from vascular geometries in less than one minute per cardiac cycle. Trained on a comprehensive dataset of high-fidelity simulations of patient-specific aneurysms, our architecture combines graph transformers with autoregressive predictions to accurately simulate blood flow, wall shear stress, and oscillatory shear index. The model generalizes across unseen patient geometries and inflow conditions without mesh-specific calibration. Beyond accelerating simulation, our framework establishes the foundation for clinically interpretable hemodynamic prediction. By enabling near real-time inference integrated with existing imaging pipelines, it allows direct comparison with hospital phase-diagram assessments and extends them with physically grounded, high-resolution flow fields.
  This work transforms high-fidelity simulations from an expert-only research tool into a deployable, data-driven decision support system. Our full pipeline delivers high-resolution hemodynamic predictions within minutes of patient imaging, without requiring computational specialists, marking a step-change toward real-time, bedside aneurysm analysis.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Multi-Class Calibration through Normalization-Aware Isotonic Techniques</title>
<link>https://arxiv.org/abs/2512.09054</link>
<guid>https://arxiv.org/abs/2512.09054</guid>
<content:encoded><![CDATA[
<div> isotonic regression, multi-class calibration, probability normalization, NA-FIR, SCIR<br /><br />Summary:<br /><br />1. The paper addresses the challenge of producing accurate and reliable probability predictions in multi-class supervised learning, emphasizing the importance of well-calibrated models for informed decision-making. <br /><br />2. Traditional isotonic regression, effective in binary calibration, has limitations when extended to multi-class problems via one-vs-rest approaches, often yielding suboptimal calibration compared to parametric methods. <br /><br />3. The authors introduce novel isotonic normalization-aware calibration techniques developed with natural assumptions aligned with practitioner expectations, aiming to improve multi-class probability calibration. <br /><br />4. Two key methods are proposed: NA-FIR, which integrates probability normalization directly into the isotonic regression optimization process, and SCIR, which models the calibration problem as a cumulative bivariate isotonic regression to inherently account for normalization constraints. <br /><br />5. Extensive empirical evaluations across diverse text and image classification datasets and different model architectures demonstrate that the proposed methods consistently enhance calibration performance, reflected by improvements in negative log-likelihood (NLL) and expected calibration error (ECE) metrics compared to existing approaches. <div>
arXiv:2512.09054v1 Announce Type: new 
Abstract: Accurate and reliable probability predictions are essential for multi-class supervised learning tasks, where well-calibrated models enable rational decision-making. While isotonic regression has proven effective for binary calibration, its extension to multi-class problems via one-vs-rest calibration produced suboptimal results when compared to parametric methods, limiting its practical adoption. In this work, we propose novel isotonic normalization-aware techniques for multiclass calibration, grounded in natural and intuitive assumptions expected by practitioners. Unlike prior approaches, our methods inherently account for probability normalization by either incorporating normalization directly into the optimization process (NA-FIR) or modeling the problem as a cumulative bivariate isotonic regression (SCIR). Empirical evaluation on a variety of text and image classification datasets across different model architectures reveals that our approach consistently improves negative log-likelihood (NLL) and expected calibration error (ECE) metrics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Diffusion-Based Framework for High-Resolution Precipitation Forecasting over CONUS</title>
<link>https://arxiv.org/abs/2512.09059</link>
<guid>https://arxiv.org/abs/2512.09059</guid>
<content:encoded><![CDATA[
<div> Keywords: precipitation forecasting, deep learning, residual prediction, HRRR, MRMS  

<br /><br />Summary:  
This study introduces a diffusion-based deep learning (DL) framework aimed at improving accurate precipitation forecasting, which is crucial for managing hydrometeorological risks like flash flooding and infrastructure damage. It compares three residual prediction strategies based solely on their input data: (1) a fully data-driven model using past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model that leverages only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model that integrates both MRMS observations and selected HRRR forecast variables. The models are evaluated on a consistent setup across the Continental United States (CONUS), producing forecasts at 1-km resolution initially for 1 hour and extending to 12 hours through autoregressive rollouts. Performance metrics include both overall and extreme rainfall-specific assessments, at both regional and CONUS-wide scales. The DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics across all lead times. The hybrid model shows superior performance for short lead times, while the HRRR-corrective model excels at longer lead times, maintaining skill up to 12 hours. Additionally, uncertainty quantification calibrated for residual learning is incorporated to ensure reliability. These improvements, especially at extended lead times, are critical for emergency preparedness by enabling better decision-making and advancing the applicability of DL in precipitation forecasting across regions. <div>
arXiv:2512.09059v1 Announce Type: new 
Abstract: Accurate precipitation forecasting is essential for hydrometeorological risk management, especially for anticipating extreme rainfall that can lead to flash flooding and infrastructure damage. This study introduces a diffusion-based deep learning (DL) framework that systematically compares three residual prediction strategies differing only in their input sources: (1) a fully data-driven model using only past observations from the Multi-Radar Multi-Sensor (MRMS) system, (2) a corrective model using only forecasts from the High-Resolution Rapid Refresh (HRRR) numerical weather prediction system, and (3) a hybrid model integrating both MRMS and selected HRRR forecast variables. By evaluating these approaches under a unified setup, we provide a clearer understanding of how each data source contributes to predictive skill over the Continental United States (CONUS). Forecasts are produced at 1-km spatial resolution, beginning with direct 1-hour predictions and extending to 12 hours using autoregressive rollouts. Performance is evaluated using both CONUS-wide and region-specific metrics that assess overall performance and skill at extreme rainfall thresholds. Across all lead times, our DL framework consistently outperforms the HRRR baseline in pixel-wise and spatiostatistical metrics. The hybrid model performs best at the shortest lead time, while the HRRR-corrective model outperforms others at longer lead times, maintaining high skill through 12 hours. To assess reliability, we incorporate calibrated uncertainty quantification tailored to the residual learning setup. These gains, particularly at longer lead times, are critical for emergency preparedness, where modest increases in forecast horizon can improve decision-making. This work advances DL-based precipitation forecasting by enhancing predictive skill, reliability, and applicability across regions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrast transfer functions help quantify neural network out-of-distribution generalization in HRTEM</title>
<link>https://arxiv.org/abs/2512.09067</link>
<guid>https://arxiv.org/abs/2512.09067</guid>
<content:encoded><![CDATA[
<div> arXiv, neural networks, out-of-distribution generalization, HRTEM imaging, simulation-based data curation<br /><br />Summary:<br /><br />1. This article addresses the challenge of out-of-distribution (OOD) generalization in neural networks, focusing on segmentation models applied to high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles. 2. Due to the variability in experimental imaging conditions and the difficulty in establishing ground truth, the authors use simulation-based data curation that allows fine-grained control over underlying distributions and access to ground-truth information, enabling precise study of OOD behavior. 3. They trained and evaluated over 12,000 neural networks using synthetic datasets generated by random structure sampling combined with multislice simulation techniques that mimic HRTEM imaging. 4. By employing the HRTEM contrast transfer function, the paper introduces a framework to compare the information content across different datasets and quantitatively measure OOD domain shifts. 5. The findings indicate that neural network segmentation models maintain stable performance but degrade smoothly and predictably as imaging conditions deviate from those in the training set. Lastly, the authors discuss limitations in explaining OOD shifts relating to atomic structural differences and highlight complementary strategies for better understanding neural network generalization under various experimental shifts. <div>
arXiv:2512.09067v1 Announce Type: new 
Abstract: Neural networks, while effective for tackling many challenging scientific tasks, are not known to perform well out-of-distribution (OOD), i.e., within domains which differ from their training data. Understanding neural network OOD generalization is paramount to their successful deployment in experimental workflows, especially when ground-truth knowledge about the experiment is hard to establish or experimental conditions significantly vary. With inherent access to ground-truth information and fine-grained control of underlying distributions, simulation-based data curation facilitates precise investigation of OOD generalization behavior. Here, we probe generalization with respect to imaging conditions of neural network segmentation models for high-resolution transmission electron microscopy (HRTEM) imaging of nanoparticles, training and measuring the OOD generalization of over 12,000 neural networks using synthetic data generated via random structure sampling and multislice simulation. Using the HRTEM contrast transfer function, we further develop a framework to compare information content of HRTEM datasets and quantify OOD domain shifts. We demonstrate that neural network segmentation models enjoy significant performance stability, but will smoothly and predictably worsen as imaging conditions shift from the training distribution. Lastly, we consider limitations of our approach in explaining other OOD shifts, such as of the atomic structures, and discuss complementary techniques for understanding generalization in such settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Deep-Learning-Based Early Warning System for Deadly Heatwave Prediction</title>
<link>https://arxiv.org/abs/2512.09074</link>
<guid>https://arxiv.org/abs/2512.09074</guid>
<content:encoded><![CDATA[
<div> Keywords: heatwaves, early warning system, deep learning, mortality prediction, urban health<br /><br />Summary:<br /><br />Severe heatwaves in urban areas pose significant risks to public health, necessitating the development of early warning systems to mitigate their impact. Predicting deadly heatwaves is challenging due to difficulties in defining and estimating heat-related mortality and the need for systems that handle data availability, spatial and temporal robustness, and decision-making costs. To tackle these issues, the study introduces DeepTherm, a modular early warning system that predicts deadly heatwaves without relying on historical heat-related mortality data. DeepTherm employs a flexible deep learning-based dual-prediction pipeline that separates baseline mortality—occurring without heatwaves or other irregular events—from all-cause mortality. The system was evaluated using real-world data from various regions in Spain, demonstrating consistent, robust, and accurate performance across different locations, time periods, and demographic groups. Additionally, DeepTherm allows users to balance the trade-off between missed alarms and false alarms, offering practical flexibility for public health decision-making. These results highlight DeepTherm's potential as a valuable tool in urban heatwave preparedness and public health protection. <div>
arXiv:2512.09074v1 Announce Type: new 
Abstract: Severe heatwaves in urban areas significantly threaten public health, calling for establishing early warning strategies. Despite predicting occurrence of heatwaves and attributing historical mortality, predicting an incoming deadly heatwave remains a challenge due to the difficulty in defining and estimating heat-related mortality. Furthermore, establishing an early warning system imposes additional requirements, including data availability, spatial and temporal robustness, and decision costs. To address these challenges, we propose DeepTherm, a modular early warning system for deadly heatwave prediction without requiring heat-related mortality history. By highlighting the flexibility of deep learning, DeepTherm employs a dual-prediction pipeline, disentangling baseline mortality in the absence of heatwaves and other irregular events from all-cause mortality. We evaluated DeepTherm on real-world data across Spain. Results demonstrate consistent, robust, and accurate performance across diverse regions, time periods, and population groups while allowing trade-off between missed alarms and false alarms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Hype: Comparing Lightweight and Deep Learning Models for Air Quality Forecasting</title>
<link>https://arxiv.org/abs/2512.09076</link>
<guid>https://arxiv.org/abs/2512.09076</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban air pollution, Forecasting, Additive models, Facebook Prophet, NeuralProphet<br /><br />Summary:<br /><br />This study focuses on accurately forecasting urban air pollution, specifically particulate matter (PM$_{2.5}$ and PM$_{10}$) in Beijing, which is crucial for public health protection and policy guidance. The research compares lightweight additive models, Facebook Prophet (FBP) and NeuralProphet (NP), against deep learning and traditional statistical methods, aiming to address the complexity and interpretability issues of recent models. Using multi-year pollutant and meteorological data, systematic feature selection methods—correlation, mutual information, and minimum redundancy maximum relevance (mRMR)—were applied alongside leakage-safe scaling and chronological splits to ensure robustness. Both FBP and NP were trained using pollutant and precursor regressors, with NP additionally incorporating lagged dependencies to capture temporal effects. For benchmarking, two machine learning baselines (LSTM and LightGBM) and a traditional SARIMAX model were also implemented. Model performance was assessed over a 7-day holdout period using MAE, RMSE, and $R^2$ metrics. The results revealed that Facebook Prophet consistently outperformed NeuralProphet, SARIMAX, and the learning-based baselines, achieving test $R^2$ values above 0.94 for both particulate matter types. These findings highlight that interpretable additive models can match or exceed the accuracy of more complex approaches while offering transparency and ease of deployment, making them practical for operational air quality forecasting. <div>
arXiv:2512.09076v1 Announce Type: new 
Abstract: Accurate forecasting of urban air pollution is essential for protecting public health and guiding mitigation policies. While Deep Learning (DL) and hybrid pipelines dominate recent research, their complexity and limited interpretability hinder operational use. This study investigates whether lightweight additive models -- Facebook Prophet (FBP) and NeuralProphet (NP) -- can deliver competitive forecasts for particulate matter (PM$_{2.5}$, PM$_{10}$) in Beijing, China. Using multi-year pollutant and meteorological data, we applied systematic feature selection (correlation, mutual information, mRMR), leakage-safe scaling, and chronological data splits. Both models were trained with pollutant and precursor regressors, with NP additionally leveraging lagged dependencies. For context, two machine learning baselines (LSTM, LightGBM) and one traditional statistical model (SARIMAX) were also implemented. Performance was evaluated on a 7-day holdout using MAE, RMSE, and $R^2$. Results show that FBP consistently outperformed NP, SARIMAX, and the learning-based baselines, achieving test $R^2$ above 0.94 for both pollutants. These findings demonstrate that interpretable additive models remain competitive with both traditional and complex approaches, offering a practical balance of accuracy, transparency, and ease of deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GS-KAN: Parameter-Efficient Kolmogorov-Arnold Networks via Sprecher-Type Shared Basis Functions</title>
<link>https://arxiv.org/abs/2512.09084</link>
<guid>https://arxiv.org/abs/2512.09084</guid>
<content:encoded><![CDATA[
<div> Keywords: Kolmogorov-Arnold theorem, GS-KAN, parameter efficiency, function approximation, high-dimensional tasks<br /><br />Summary:<br /><br />1. The paper addresses inefficiencies in Kolmogorov-Arnold Networks (KANs), which leverage the Kolmogorov-Arnold representation theorem by placing learnable univariate functions on edges rather than nodes but suffer from substantial parameter overhead due to unique parameterization of each edge.<br /><br />2. The authors propose GS-KAN (Generalized Sprecher-KAN), a novel, lightweight architecture inspired by David Sprecher’s refinement of the superposition theorem, which generates unique edge functions through learnable linear transformations of a single shared parent function per layer.<br /><br />3. GS-KAN is evaluated against traditional KANs and Multi-Layer Perceptrons (MLPs) on multiple tasks including synthetic continuous function approximation, tabular data regression, and image classification.<br /><br />4. Results show GS-KAN outperforms MLPs and standard KANs in continuous function approximation with superior parameter efficiency, achieves competitive performance on tabular regression, and surpasses MLPs in high-dimensional classification tasks.<br /><br />5. The architecture is particularly valuable in high-dimensional scenarios with strict parameter limits, enabling feasible deployment of KAN-based models where parameter explosion would otherwise inhibit use, with code publicly available. <div>
arXiv:2512.09084v1 Announce Type: new 
Abstract: The Kolmogorov-Arnold representation theorem offers a theoretical alternative to Multi-Layer Perceptrons (MLPs) by placing learnable univariate functions on edges rather than nodes. While recent implementations such as Kolmogorov-Arnold Networks (KANs) demonstrate high approximation capabilities, they suffer from significant parameter inefficiency due to the requirement of maintaining unique parameterizations for every network edge. In this work, we propose GS-KAN (Generalized Sprecher-KAN), a lightweight architecture inspired by David Sprecher's refinement of the superposition theorem. GS-KAN constructs unique edge functions by applying learnable linear transformations to a single learnable, shared parent function per layer. We evaluate GS-KAN against existing KAN architectures and MLPs across synthetic function approximation, tabular data regression and image classification tasks. Our results demonstrate that GS-KAN outperforms both MLPs and standard KAN baselines on continuous function approximation tasks while maintaining superior parameter efficiency. Additionally, GS-KAN achieves competitive performance with existing KAN architectures on tabular regression and outperforms MLPs on high-dimensional classification tasks. Crucially, the proposed architecture enables the deployment of KAN-based architectures in high-dimensional regimes under strict parameter constraints, a setting where standard implementations are typically infeasible due to parameter explosion. The source code is available at https://github.com/rambamn48/gs-impl.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Geometry of Robust Data Attribution: From Convex Models to Deep Networks</title>
<link>https://arxiv.org/abs/2512.09103</link>
<guid>https://arxiv.org/abs/2512.09103</guid>
<content:encoded><![CDATA[
<div> Keywords: data attribution, certified robustness, Wasserstein metric, deep networks, anomaly detection<br /><br />Summary:<br /><br />1. The paper addresses the challenge of data attribution methods, which link training examples to model predictions but lack robustness under distributional perturbations, limiting practical use.<br /><br />2. It introduces a unified framework for certified robust attribution, applicable to both convex models and deep neural networks.<br /><br />3. For convex models, the authors derive Wasserstein-Robust Influence Functions (W-RIF) that come with provable coverage guarantees to ensure reliable attribution.<br /><br />4. They expose that Euclidean certification techniques fail for deep networks due to spectral amplification, a phenomenon where deep representations inflate Lipschitz constant estimates by over 10,000 times, making standard robustness certification vacuous.<br /><br />5. To overcome this, the authors propose the Natural Wasserstein metric which leverages the model's own feature covariance geometry to eliminate spectral amplification effects, improving robustness by 76 times.<br /><br />6. Experiments on CIFAR-10 with ResNet-18 show that the Natural W-TRAK method certifies 68.7% of ranking pairs for attribution stability, compared to 0% for Euclidean baselines, providing the first non-vacuous certified bounds for neural network attribution.<br /><br />7. The study proves that the Self-Influence term equals the Lipschitz constant that governs attribution stability and demonstrates its practical utility in leverage-based anomaly detection.<br /><br />8. Empirical results highlight that Self-Influence achieves a high AUROC of 0.970 for detecting label noise, successfully identifying 94.1% of corrupted labels by analyzing only the top 20% of the training data.<br /><br />9. Overall, the work advances the theoretical and empirical understanding of robust data attribution, making it more reliable and interpretable for complex models. <div>
arXiv:2512.09103v1 Announce Type: new 
Abstract: Data attribution methods identify which training examples are responsible for a model's predictions, but their sensitivity to distributional perturbations undermines practical reliability. We present a unified framework for certified robust attribution that extends from convex models to deep networks. For convex settings, we derive Wasserstein-Robust Influence Functions (W-RIF) with provable coverage guarantees. For deep networks, we demonstrate that Euclidean certification is rendered vacuous by spectral amplification -- a mechanism where the inherent ill-conditioning of deep representations inflates Lipschitz bounds by over $10{,}000\times$. This explains why standard TRAK scores, while accurate point estimates, are geometrically fragile: naive Euclidean robustness analysis yields 0\% certification. Our key contribution is the Natural Wasserstein metric, which measures perturbations in the geometry induced by the model's own feature covariance. This eliminates spectral amplification, reducing worst-case sensitivity by $76\times$ and stabilizing attribution estimates. On CIFAR-10 with ResNet-18, Natural W-TRAK certifies 68.7\% of ranking pairs compared to 0\% for Euclidean baselines -- to our knowledge, the first non-vacuous certified bounds for neural network attribution. Furthermore, we prove that the Self-Influence term arising from our analysis equals the Lipschitz constant governing attribution stability, providing theoretical grounding for leverage-based anomaly detection. Empirically, Self-Influence achieves 0.970 AUROC for label noise detection, identifying 94.1\% of corrupted labels by examining just the top 20\% of training data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Unmasking Policies for Diffusion Language Models</title>
<link>https://arxiv.org/abs/2512.09106</link>
<guid>https://arxiv.org/abs/2512.09106</guid>
<content:encoded><![CDATA[
<div> Diffusion models, masked discrete diffusion, reinforcement learning, sampling procedure, semi-autoregressive generation  

<br /><br />Summary:  
This work focuses on improving sampling procedures in masked discrete diffusion-based large language models (dLLMs), which are competitive with autoregressive models in downstream tasks and potentially more efficient during inference. A key challenge addressed is selecting which tokens to unmask at each diffusion step since unmasking too many tokens simultaneously can reduce quality. Prior heuristics, like confidence thresholding, improve quality and throughput over random unmasking but suffer from manual tuning requirements and degraded performance with larger buffer sizes. The authors propose framing the sampling process as a Markov decision process and using reinforcement learning to train sampling policies. They introduce a lightweight policy network—a single-layer transformer—that uses token confidence scores to decide token unmasking. Experiments demonstrate that these trained policies rival state-of-the-art heuristics when combined with semi-autoregressive generation and outperform them in full diffusion setups. Additionally, the learned policies exhibit transferability across different dLLM models and longer sequences. However, limitations include decreased performance on out-of-domain data and difficulties in fine-tuning the accuracy-efficiency balance. This approach offers a more automated and potentially adaptable strategy for sampling in diffusion language models compared to heuristic methods. <div>
arXiv:2512.09106v1 Announce Type: new 
Abstract: Diffusion (Large) Language Models (dLLMs) now match the downstream performance of their autoregressive counterparts on many tasks, while holding the promise of being more efficient during inference. One particularly successful variant is masked discrete diffusion, in which a buffer filled with special mask tokens is progressively replaced with tokens sampled from the model's vocabulary. Efficiency can be gained by unmasking several tokens in parallel, but doing too many at once risks degrading the generation quality. Thus, one critical design aspect of dLLMs is the sampling procedure that selects, at each step of the diffusion process, which tokens to replace. Indeed, recent work has found that heuristic strategies such as confidence thresholding lead to both higher quality and token throughput compared to random unmasking. However, such heuristics have downsides: they require manual tuning, and we observe that their performance degrades with larger buffer sizes. In this work, we instead propose to train sampling procedures using reinforcement learning. Specifically, we formalize masked diffusion sampling as a Markov decision process in which the dLLM serves as the environment, and propose a lightweight policy architecture based on a single-layer transformer that maps dLLM token confidences to unmasking decisions. Our experiments show that these trained policies match the performance of state-of-the-art heuristics when combined with semi-autoregressive generation, while outperforming them in the full diffusion setting. We also examine the transferability of these policies, finding that they can generalize to new underlying dLLMs and longer sequence lengths. However, we also observe that their performance degrades when applied to out-of-domain data, and that fine-grained tuning of the accuracy-efficiency trade-off can be challenging with our approach.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Embedding via Chebyshev Bases for Robust DeepONet Approximation</title>
<link>https://arxiv.org/abs/2512.09165</link>
<guid>https://arxiv.org/abs/2512.09165</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Operator Networks, Spectral-Embedded DeepONet, Chebyshev spectral dictionary, PDE surrogate modeling, non-periodic boundary conditions  

<br /><br />Summary:  
Deep Operator Networks (DeepONets) are pivotal in learning nonlinear operators from PDE data but face challenges in representing sharp gradients, boundary layers, and non-periodic structures, especially for PDEs on bounded domains with Dirichlet or Neumann boundary conditions. To overcome these issues, the authors propose the Spectral-Embedded DeepONet (SEDONet), which replaces the standard trunk input of raw spatial or spatiotemporal coordinates with a fixed Chebyshev spectral dictionary. This spectral embedding introduces a non-periodic inductive bias that is well-suited for bounded domains, improving the network’s ability to capture fine-scale and boundary-localized features that conventional Fourier or fully connected trunk designs struggle to represent. SEDONet is benchmarked on a diverse set of PDE problems including 2D Poisson, 1D Burgers, 1D advection-diffusion, Allen-Cahn dynamics, and the Lorenz-96 system, representing elliptic, parabolic, advective, and multiscale temporal phenomena. Compared to baseline DeepONet and Fourier-embedded variants (FEDONet), SEDONet achieves the lowest relative L2 errors, typically improving accuracy by 30-40%, especially on non-periodic geometries. Spectral analysis confirms that SEDONet better preserves high-frequency components and boundary effects. The new architecture is a simple, parameter-neutral modification offering a robust and efficient spectral learning framework for surrogate modeling of PDE operators on bounded domains. <div>
arXiv:2512.09165v1 Announce Type: new 
Abstract: Deep Operator Networks (DeepONets) have become a central tool in data-driven operator learning, providing flexible surrogates for nonlinear mappings arising in partial differential equations (PDEs). However, the standard trunk design based on fully connected layers acting on raw spatial or spatiotemporal coordinates struggles to represent sharp gradients, boundary layers, and non-periodic structures commonly found in PDEs posed on bounded domains with Dirichlet or Neumann boundary conditions. To address these limitations, we introduce the Spectral-Embedded DeepONet (SEDONet), a new DeepONet variant in which the trunk is driven by a fixed Chebyshev spectral dictionary rather than coordinate inputs. This non-periodic spectral embedding provides a principled inductive bias tailored to bounded domains, enabling the learned operator to capture fine-scale non-periodic features that are difficult for Fourier or MLP trunks to represent. SEDONet is evaluated on a suite of PDE benchmarks including 2D Poisson, 1D Burgers, 1D advection-diffusion, Allen-Cahn dynamics, and the Lorenz-96 chaotic system, covering elliptic, parabolic, advective, and multiscale temporal phenomena, all of which can be viewed as canonical problems in computational mechanics. Across all datasets, SEDONet consistently achieves the lowest relative L2 errors among DeepONet, FEDONet, and SEDONet, with average improvements of about 30-40% over the baseline DeepONet and meaningful gains over Fourier-embedded variants on non-periodic geometries. Spectral analyses further show that SEDONet more accurately preserves high-frequency and boundary-localized features, demonstrating the value of Chebyshev embeddings in non-periodic operator learning. The proposed architecture offers a simple, parameter-neutral modification to DeepONets, delivering a robust and efficient spectral framework for surrogate modeling of PDEs on bounded domains.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Failure Modes of Transformers through the Lens of Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.09182</link>
<guid>https://arxiv.org/abs/2512.09182</guid>
<content:encoded><![CDATA[
<div> Transformers, decoder-only, failure modes, graph neural networks, information propagation<br /><br />Summary:<br /><br />1. The article investigates various failure modes of transformer models, especially decoder-only transformers, which are prevalent in large language models (LLMs).<br />2. It frames deep learning models, including transformers, as systems that perform learnable information mixing and propagation, making model failures interpretable as bottlenecks in this information flow.<br />3. The authors draw parallels between transformers and graph neural networks (GNNs), leveraging the extensive theoretical understanding of GNNs to explain transformer vulnerabilities.<br />4. The causal structure of decoder-only transformers induces unique geometric characteristics in how information propagates, leading to predictable and sometimes severe failure modes.<br />5. Existing remedies to transformer issues are often heuristic and lack rigorous theoretical grounding; this work aims to unify these solutions under a theoretical framework to better understand their effectiveness and guide future improvements targeted at specific failure modes.<br /><br />This article thus bridges the gap between practical observations of transformer failures and underlying theoretical principles, advancing the understanding needed to develop more robust transformer architectures. <div>
arXiv:2512.09182v1 Announce Type: new 
Abstract: Transformers and more specifically decoder-only transformers dominate modern LLM architectures. While they have shown to work exceptionally well, they are not without issues, resulting in surprising failure modes and predictably asymmetric performance degradation. This article is a study of many of these observed failure modes of transformers through the lens of graph neural network (GNN) theory. We first make the case that much of deep learning, including transformers, is about learnable information mixing and propagation. This makes the study of model failure modes a study of bottlenecks in information propagation. This naturally leads to GNN theory, where there is already a rich literature on information propagation bottlenecks and theoretical failure modes of models. We then make the case that many issues faced by GNNs are also experienced by transformers. In addition, we analyze how the causal nature of decoder-only transformers create interesting geometric properties in information propagation, resulting in predictable and potentially devastating failure modes. Finally, we observe that existing solutions in transformer research tend to be ad-hoc and driven by intuition rather than grounded theoretical motivation. As such, we unify many such solutions under a more theoretical perspective, providing insight into why they work, what problem they are actually solving, and how they can be further improved to target specific failure modes of transformers. Overall, this article is an attempt to bridge the gap between observed failure modes in transformers and a general lack of theoretical understanding of them in this space.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Optimal Valve Prescription for Transcatheter Aortic Valve Replacement (TAVR) Surgery: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2512.09198</link>
<guid>https://arxiv.org/abs/2512.09198</guid>
<content:encoded><![CDATA[
<div> Transcatheter Aortic Valve Replacement, Permanent Pacemaker Implantation, Data-driven clinical support, Valve type prescription, Population heterogeneity  

<br /><br />Summary:  
This paper addresses the challenge of optimizing valve type selection in Transcatheter Aortic Valve Replacement (TAVR) to reduce the risk of permanent pacemaker implantation (PPI), a common postoperative complication. A novel, integrated dataset is created that combines patient information from the U.S. and Greece, including demographics, computed tomography scans, and echocardiograms, with harmonization across differing healthcare records. The authors propose a data-driven clinical decision support tool that utilizes a leaf-level analysis approach, which accounts for population heterogeneity and avoids reliance on uncertain counterfactual risk estimates. The resulting prescriptive model is demonstrated to significantly reduce PPI rates by 26% in the internal U.S. cohort and by 16% in an external Greek validation cohort compared to existing standard care. This study is notable for being the first to offer a personalized, unified strategy for selecting transcatheter heart valves (THV) during TAVR procedures, potentially improving patient outcomes by tailoring valve choice to individual patient characteristics. The integration of multi-national data and advanced analytical methods highlights its contribution to precision medicine in cardiovascular interventions. <div>
arXiv:2512.09198v1 Announce Type: new 
Abstract: Transcatheter Aortic Valve Replacement (TAVR) has emerged as a minimally invasive treatment option for patients with severe aortic stenosis, a life-threatening cardiovascular condition. Multiple transcatheter heart valves (THV) have been approved for use in TAVR, but current guidelines regarding valve type prescription remain an active topic of debate. We propose a data-driven clinical support tool to identify the optimal valve type with the objective of minimizing the risk of permanent pacemaker implantation (PPI), a predominant postoperative complication. We synthesize a novel dataset that combines U.S. and Greek patient populations and integrates three distinct data sources (patient demographics, computed tomography scans, echocardiograms) while harmonizing differences in each country's record system. We introduce a leaf-level analysis to leverage population heterogeneity and avoid benchmarking against uncertain counterfactual risk estimates. The final prescriptive model shows a reduction in PPI rates of 26% and 16% compared with the current standard of care in our internal U.S. population and external Greek validation cohort, respectively. To the best of our knowledge, this work represents the first unified, personalized prescription strategy for THV selection in TAVR.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs for Analog Circuit Design Continuum (ACDC)</title>
<link>https://arxiv.org/abs/2512.09199</link>
<guid>https://arxiv.org/abs/2512.09199</guid>
<content:encoded><![CDATA[
<div> Large Language Models, analog circuit design, reliability, data representations, model generalization  

<br /><br />Summary:  
This paper explores the use of Large Language Models (LLMs) and transformer architectures for analog circuit design, a domain requiring specialized reasoning, physical constraints adherence, and structured data handling. The study assesses how different data representations affect model behavior, shedding light on format sensitivity. It compares the performance and reliability of smaller models like T5 and GPT-2 against larger foundation models such as Mistral-7B and GPT-oss-20B under various training conditions. Key challenges identified include instability in the designs generated by LLMs and their limited ability to generalize to previously unseen circuit configurations. The research emphasizes that despite impressive reasoning capabilities, current LLMs face significant reliability and robustness issues in real-world engineering workflows where human collaboration is vital. These findings underline the need for more dependable and consistent model designs to facilitate practical deployment of LLMs in structured, domain-specific applications. Ultimately, the work offers valuable insights into how LLMs might be developed and integrated as effective tools that augment rather than replace human expertise in engineering tasks. <div>
arXiv:2512.09199v1 Announce Type: new 
Abstract: Large Language Models (LLMs) and transformer architectures have shown impressive reasoning and generation capabilities across diverse natural language tasks. However, their reliability and robustness in real-world engineering domains remain largely unexplored, limiting their practical utility in human-centric workflows. In this work, we investigate the applicability and consistency of LLMs for analog circuit design -- a task requiring domain-specific reasoning, adherence to physical constraints, and structured representations -- focusing on AI-assisted design where humans remain in the loop. We study how different data representations influence model behavior and compare smaller models (e.g., T5, GPT-2) with larger foundation models (e.g., Mistral-7B, GPT-oss-20B) under varying training conditions. Our results highlight key reliability challenges, including sensitivity to data format, instability in generated designs, and limited generalization to unseen circuit configurations. These findings provide early evidence on the limits and potential of LLMs as tools to enhance human capabilities in complex engineering tasks, offering insights into designing reliable, deployable foundation models for structured, real-world applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor-Compressed and Fully-Quantized Training of Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2512.09202</link>
<guid>https://arxiv.org/abs/2512.09202</guid>
<content:encoded><![CDATA[
<div> Physics-Informed Neural Networks, Quantized Training, Stein's Estimator, Tensor-Train Decomposition, Edge Computing<br /><br />Summary:<br /><br />This paper addresses the challenge of deploying Physics-Informed Neural Networks (PINNs) on resource-limited edge devices by proposing a scalable and energy-efficient training framework. The framework integrates fully quantized training techniques to reduce computational cost and memory usage. It leverages Stein's estimator (SE) for residual loss computation and applies tensor-train (TT) decomposition to compress network weights effectively. Three key innovations are introduced: (1) a mixed-precision training approach using a square-block MX (SMX) format that removes data duplication during backpropagation, enhancing efficiency; (2) a difference-based quantization scheme tailored for Stein's estimator to avoid numerical underflow issues; and (3) a partial-reconstruction scheme (PRS) in TT layers to minimize the accumulation of quantization errors. Additionally, the authors design PINTA, a precision-scalable hardware accelerator that maximizes the performance benefits of their framework. Experimental validation is conducted on complex PDEs including 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations. The results demonstrate accuracy on par with or exceeding traditional full-precision baselines, while achieving significant speedups ranging from 5.5x to 83.5x and energy savings between 159.6x and 2324.1x. This research enables real-time PDE solving on edge devices and advances energy-efficient scientific computing. <div>
arXiv:2512.09202v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a promising paradigm for solving partial differential equations (PDEs) by embedding physical laws into neural network training objectives. However, their deployment on resource-constrained platforms is hindered by substantial computational and memory overhead, primarily stemming from higher-order automatic differentiation, intensive tensor operations, and reliance on full-precision arithmetic. To address these challenges, we present a framework that enables scalable and energy-efficient PINN training on edge devices. This framework integrates fully quantized training, Stein's estimator (SE)-based residual loss computation, and tensor-train (TT) decomposition for weight compression. It contributes three key innovations: (1) a mixed-precision training method that use a square-block MX (SMX) format to eliminate data duplication during backpropagation; (2) a difference-based quantization scheme for the Stein's estimator that mitigates underflow; and (3) a partial-reconstruction scheme (PRS) for TT-Layers that reduces quantization-error accumulation. We further design PINTA, a precision-scalable hardware accelerator, to fully exploit the performance of the framework. Experiments on the 2-D Poisson, 20-D Hamilton-Jacobi-Bellman (HJB), and 100-D Heat equations demonstrate that the proposed framework achieves accuracy comparable to or better than full-precision, uncompressed baselines while delivering 5.5x to 83.5x speedups and 159.6x to 2324.1x energy savings. This work enables real-time PDE solving on edge devices and paves the way for energy-efficient scientific computing at scale.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Learning for Semi-Supervised Deep Regression with Generalized Ordinal Rankings from Spectral Seriation</title>
<link>https://arxiv.org/abs/2512.09267</link>
<guid>https://arxiv.org/abs/2512.09267</guid>
<content:encoded><![CDATA[
<div> Contrastive Learning, Semi-supervised Regression, Ordinal Ranking, Spectral Seriation, Dynamic Programming<br /><br />Summary:<br /><br />This paper addresses the limitation of contrastive learning methods in regression tasks that rely heavily on labeled data to enforce ordinal relationships in feature space. The authors propose an extension to semi-supervised settings by incorporating unlabeled data into the feature similarity matrix construction within mini-batches, enabling recovery of ordinal rankings for unlabeled samples through spectral seriation algorithms, provided the error remains within acceptable bounds. Labeled data provide regularization and improve the reliability of these ordinal rankings. To enhance robustness, dynamic programming is employed to select stable features used for matrix construction, reducing feature perturbations. The recovered ordinal relationships are leveraged both for contrastive learning on unlabeled samples and as supervisory signals for prediction, increasing effective training data and improving representation learning. The method is supported by theoretical guarantees and extensive experiments on diverse datasets, demonstrating superiority over existing state-of-the-art semi-supervised deep regression techniques. The authors provide practical implementation by releasing their code publicly at the provided GitHub repository, facilitating future research and application. <div>
arXiv:2512.09267v1 Announce Type: new 
Abstract: Contrastive learning methods enforce label distance relationships in feature space to improve representation capability for regression models. However, these methods highly depend on label information to correctly recover ordinal relationships of features, limiting their applications to semi-supervised regression. In this work, we extend contrastive regression methods to allow unlabeled data to be used in the semi-supervised setting, thereby reducing the dependence on costly annotations. Particularly we construct the feature similarity matrix with both labeled and unlabeled samples in a mini-batch to reflect inter-sample relationships, and an accurate ordinal ranking of involved unlabeled samples can be recovered through spectral seriation algorithms if the level of error is within certain bounds. The introduction of labeled samples above provides regularization of the ordinal ranking with guidance from the ground-truth label information, making the ranking more reliable. To reduce feature perturbations, we further utilize the dynamic programming algorithm to select robust features for the matrix construction. The recovered ordinal relationship is then used for contrastive learning on unlabeled samples, and we thus allow more data to be used for feature representation learning, thereby achieving more robust results. The ordinal rankings can also be used to supervise predictions on unlabeled samples, serving as an additional training signal. We provide theoretical guarantees and empirical verification through experiments on various datasets, demonstrating that our method can surpass existing state-of-the-art semi-supervised deep regression methods. Our code have been released on https://github.com/xmed-lab/CLSS.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal inference with Rao-Blackwellized Particle Filters</title>
<link>https://arxiv.org/abs/2512.09269</link>
<guid>https://arxiv.org/abs/2512.09269</guid>
<content:encoded><![CDATA[
<div> Keywords: intent inference, Rao-Blackwellized Particle Filter, closed-loop behavior, Gaussian mixture model, information-theoretic leakage<br /><br />Summary:<br /><br />This paper addresses the problem of inferring the ultimate goal or intent of a mobile agent based on noisy observations of its trajectory, a key challenge in estimation theory. The authors introduce a novel approach using a variant of the Rao-Blackwellized Particle Filter (RBPF), which leverages the assumption that the agent’s intent is reflected through closed-loop behavior possessing a practical stability property. By exploiting the closed-form dynamics of the agent, the RBPF analytically marginalizes the linear-Gaussian components and updates only the particle weights, significantly enhancing sample efficiency compared to standard particle filters. Two estimators are proposed: one based on a Gaussian mixture model constructed using RBPF weights, and another reduced estimator that restricts the mixture to the effective sample, simplifying computations. The study provides information-theoretic metrics to quantify how accurately an adversary can recover the true intent, including computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates by using Gaussian-mixture KL bounds. Additionally, the paper derives a performance bound showing that the reduced estimator performs nearly as well as the full mixture estimator. Experimental results demonstrate rapid and accurate intent recovery for compliant agents, encouraging future research on controllers designed to obfuscate intent. <div>
arXiv:2512.09269v1 Announce Type: new 
Abstract: Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hetero-SplitEE: Split Learning of Neural Networks with Early Exits for Heterogeneous IoT Devices</title>
<link>https://arxiv.org/abs/2512.09313</link>
<guid>https://arxiv.org/abs/2512.09313</guid>
<content:encoded><![CDATA[
<div> Keywords: Split Learning, Heterogeneous IoT, Early Exits, Collaborative Training, Deep Neural Networks<br /><br />Summary:<br /><br />The paper addresses the challenge of training deep neural networks in heterogeneous IoT environments where devices have varying computational capacities. Existing Split Learning methods assume uniform client capabilities and fixed split points, limiting their practicality for diverse IoT systems. To overcome this, the authors propose Hetero-SplitEE, a method that enables multiple heterogeneous clients to collaboratively train a shared deep neural network by selecting different split points corresponding to their resources. Hetero-SplitEE introduces heterogeneous early exits within a hierarchical training framework, allowing clients to operate at varied model depths and computational loads. To coordinate training across clients with different split points, the paper presents two cooperative strategies: a Sequential strategy that processes clients one after another, reducing computational overhead on the server, and an Averaging strategy that supports parallel training with periodic model aggregation across layers. The effectiveness of Hetero-SplitEE is validated through experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18. Results show the approach sustains comparable model accuracy while efficiently accommodating clients with different hardware constraints, enabling scalable and practical deployment of collaborative deep learning in heterogeneous IoT settings. <div>
arXiv:2512.09313v1 Announce Type: new 
Abstract: The continuous scaling of deep neural networks has fundamentally transformed machine learning, with larger models demonstrating improved performance across diverse tasks. This growth in model size has dramatically increased the computational resources required for the training process. Consequently, distributed approaches, such as Federated Learning and Split Learning, have become essential paradigms for scalable deployment. However, existing Split Learning approaches assume client homogeneity and uniform split points across all participants. This critically limits their applicability to real-world IoT systems where devices exhibit heterogeneity in computational resources. To address this limitation, this paper proposes Hetero-SplitEE, a novel method that enables heterogeneous IoT devices to train a shared deep neural network in parallel collaboratively. By integrating heterogeneous early exits into hierarchical training, our approach allows each client to select distinct split points (cut layers) tailored to its computational capacity. In addition, we propose two cooperative training strategies, the Sequential strategy and the Averaging strategy, to facilitate this collaboration among clients with different split points. The Sequential strategy trains clients sequentially with a shared server model to reduce computational overhead. The Averaging strategy enables parallel client training with periodic cross-layer aggregation. Extensive experiments on CIFAR-10, CIFAR-100, and STL-10 datasets using ResNet-18 demonstrate that our method maintains competitive accuracy while efficiently supporting diverse computational constraints, enabling practical deployment of collaborative deep learning in heterogeneous IoT ecosystems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning with Gaussian Processes</title>
<link>https://arxiv.org/abs/2512.09322</link>
<guid>https://arxiv.org/abs/2512.09322</guid>
<content:encoded><![CDATA[
<div> Self-supervised learning, Gaussian processes, uncertainty quantification, representation learning, kernel methods<br /><br />Summary: Self-supervised learning (SSL) enables models to learn data representations without labeled supervision, improving performance on downstream tasks such as clustering and classification. Traditional SSL methods rely on generating positive pairs of similar observations, which can be difficult for certain data types and often neglect uncertainty quantification, leading to poorer out-of-sample predictions. To overcome these challenges, the authors propose Gaussian Process Self Supervised Learning (GPSSL), which integrates Gaussian process (GP) priors into the representation learning framework. GPSSL formulates a generalized Bayesian posterior that minimizes a loss function designed to yield informative representations. The covariance functions in GPs naturally cluster similar data points in the representation space, eliminating the need for explicitly generated positive pairs. GPSSL shows strong theoretical connections to kernel PCA and the VICReg method but uniquely enables posterior uncertainty estimates that can be leveraged in subsequent tasks. Experimental results across multiple datasets and downstream tasks, including classification and regression, demonstrate that GPSSL achieves superior accuracy, better uncertainty quantification, and improved error control compared to traditional SSL approaches. This method offers a promising direction toward reliable and interpretable self-supervised learning frameworks. <div>
arXiv:2512.09322v1 Announce Type: new 
Abstract: Self supervised learning (SSL) is a machine learning paradigm where models learn to understand the underlying structure of data without explicit supervision from labeled samples. The acquired representations from SSL have demonstrated useful for many downstream tasks including clustering, and linear classification, etc. To ensure smoothness of the representation space, most SSL methods rely on the ability to generate pairs of observations that are similar to a given instance. However, generating these pairs may be challenging for many types of data. Moreover, these methods lack consideration of uncertainty quantification and can perform poorly in out-of-sample prediction settings. To address these limitations, we propose Gaussian process self supervised learning (GPSSL), a novel approach that utilizes Gaussian processes (GP) models on representation learning. GP priors are imposed on the representations, and we obtain a generalized Bayesian posterior minimizing a loss function that encourages informative representations. The covariance function inherent in GPs naturally pulls representations of similar units together, serving as an alternative to using explicitly defined positive samples. We show that GPSSL is closely related to both kernel PCA and VICReg, a popular neural network-based SSL method, but unlike both allows for posterior uncertainties that can be propagated to downstream tasks. Experiments on various datasets, considering classification and regression tasks, demonstrate that GPSSL outperforms traditional methods in terms of accuracy, uncertainty quantification, and error control.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self Distillation Fine-Tuning of Protein Language Models Improves Versatility in Protein Design</title>
<link>https://arxiv.org/abs/2512.09329</link>
<guid>https://arxiv.org/abs/2512.09329</guid>
<content:encoded><![CDATA[
<div> Protein language models, supervised fine-tuning, protein sequence modeling, enzyme design, novel protein generation<br /><br />Summary: This article proposes a streamlined supervised fine-tuning (SFT) method tailored for protein language models (PLMs), which enhances their ability to generate biologically relevant protein sequences. Unlike conventional approaches relying on costly, precompiled experimental datasets, the method uses the PLM itself with a lightweight curation pipeline and domain-specific filters to create high-quality training data for SFT. These domain-specific filters not only refine the PLM outputs but also help select candidates suitable for experimental validation. By combining the filters with SFT, the method produces protein sequences that are more stable, functional, and novel, expanding exploration into previously uncharted protein sequence spaces beyond natural variants. The approach is model-agnostic and system-agnostic, demonstrated effectively using the genome-scale PLM GenSLM focused on the tryptophan synthase enzyme family. The fine-tuned models show improved performance in meeting targeted design constraints and exhibit enhanced emergent protein properties compared to baseline outputs. Overall, this framework offers a generalizable strategy for rapid, cost-effective protein design, enabling the generation of enzymes with superior characteristics for synthetic biology and related fields. <div>
arXiv:2512.09329v1 Announce Type: new 
Abstract: Supervised fine-tuning (SFT) is a standard approach for adapting large language models to specialized domains, yet its application to protein sequence modeling and protein language models (PLMs) remains ad hoc. This is in part because high-quality annotated data are far more difficult to obtain for proteins than for natural language. We present a simple and general recipe for fast SFT of PLMs, designed to improve the fidelity, reliability, and novelty of generated protein sequences. Unlike existing approaches that require costly precompiled experimental datasets for SFT, our method leverages the PLM itself, integrating a lightweight curation pipeline with domain-specific filters to construct high-quality training data. These filters can independently refine a PLM's output and identify candidates for in vitro evaluation; when combined with SFT, they enable PLMs to generate more stable and functional enzymes, while expanding exploration into protein sequence space beyond natural variants. Although our approach is agnostic to both the choice of protein language model (PLM) and the protein system, we demonstrate its effectiveness with a genome-scale PLM (GenSLM) applied to the tryptophan synthase enzyme family. The supervised fine-tuned model generates sequences that are not only more novel but also display improved characteristics across both targeted design constraints and emergent protein property measures.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Physics-Driven Neural Network to Solve Inverse Scattering Problems</title>
<link>https://arxiv.org/abs/2512.09333</link>
<guid>https://arxiv.org/abs/2512.09333</guid>
<content:encoded><![CDATA[
<div> Keywords: electromagnetic inverse scattering, physics-driven neural network, GLOW activation function, dynamic scatter subregion, transfer learning<br /><br />Summary: This paper introduces an improved physics-driven neural network (IPDNN) framework designed to solve electromagnetic inverse scattering problems (ISPs) more effectively. First, a novel Gaussian-localized oscillation-suppressing window (GLOW) activation function is proposed to enhance network stability and enable a more lightweight yet accurate neural network architecture. Second, the authors develop a dynamic scatter subregion identification strategy, which adaptively refines the computational domain to prevent missed detections while simultaneously reducing computational cost. Third, the framework incorporates transfer learning to broaden the solver’s applicability in practical, real-world scenarios by combining the interpretability of traditional iterative algorithms with the fast inference capabilities of neural networks. Fourth, extensive numerical simulations alongside experimental results validate the IPDNN’s superior performance, demonstrating improved reconstruction accuracy, robustness against noise and other disturbances, and higher computational efficiency when compared to current state-of-the-art methods. Overall, this work represents a significant advancement in electromagnetic inverse scattering by effectively marrying physics-based modeling with modern machine learning techniques to deliver a reliable and efficient reconstruction solver. <div>
arXiv:2512.09333v1 Announce Type: new 
Abstract: This paper presents an improved physics-driven neural network (IPDNN) framework for solving electromagnetic inverse scattering problems (ISPs). A new Gaussian-localized oscillation-suppressing window (GLOW) activation function is introduced to stabilize convergence and enable a lightweight yet accurate network architecture. A dynamic scatter subregion identification strategy is further developed to adaptively refine the computational domain, preventing missed detections and reducing computational cost. Moreover, transfer learning is incorporated to extend the solver's applicability to practical scenarios, integrating the physical interpretability of iterative algorithms with the real-time inference capability of neural networks. Numerical simulations and experimental results demonstrate that the proposed solver achieves superior reconstruction accuracy, robustness, and efficiency compared with existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Branching Strategies Based on Subgraph GNNs: A Study on Theoretical Promise versus Practical Reality</title>
<link>https://arxiv.org/abs/2512.09355</link>
<guid>https://arxiv.org/abs/2512.09355</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Mixed-Integer Linear Programming, Subgraph GNNs, Strong Branching, Expressivity-efficiency tradeoff

<br /><br />Summary:  
This paper explores the use of Graph Neural Networks (GNNs) for improving the "learning to branch" process in Mixed-Integer Linear Programming (MILP). Standard Message-Passing GNNs (MPNNs) are computationally efficient but lack sufficient expressivity to capture the complexities of MILP structures fully. Higher-order models such as 2-FGNNs provide greater expressivity but are computationally expensive. The authors investigate node-anchored Subgraph GNNs as a theoretically favorable compromise, which have expressivity strictly lower than 3-WL but are shown to be sufficient to approximate Strong Branching scores accurately. Empirical evaluation on four benchmark datasets, however, reveals a practical challenge: the O(n) complexity overhead of node-anchored Subgraph GNNs leads to significant memory bottlenecks and slower MILP solving times compared to MPNNs and heuristic methods. Thus, despite theoretical advantages in branching decision quality, these expressive GNNs currently incur prohibitive computational costs. The study highlights a critical tradeoff in MILP branching between expressivity and efficiency, emphasizing the need for future research to develop GNN architectures that maintain or improve expressivity while preserving practical computational efficiency. <div>
arXiv:2512.09355v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as a promising approach for ``learning to branch'' in Mixed-Integer Linear Programming (MILP). While standard Message-Passing GNNs (MPNNs) are efficient, they theoretically lack the expressive power to fully represent MILP structures. Conversely, higher-order GNNs (like 2-FGNNs) are expressive but computationally prohibitive. In this work, we investigate Subgraph GNNs as a theoretical middle ground. Crucially, while previous work [Chen et al., 2025] demonstrated that GNNs with 3-WL expressive power can approximate Strong Branching, we prove a sharper result: node-anchored Subgraph GNNs whose expressive power is strictly lower than 3-WL [Zhang et al., 2023] are sufficient to approximate Strong Branching scores. However, our extensive empirical evaluation on four benchmark datasets reveals a stark contrast between theory and practice. While node-anchored Subgraph GNNs theoretically offer superior branching decisions, their $O(n)$ complexity overhead results in significant memory bottlenecks and slower solving times than MPNNs and heuristics. Our results indicate that for MILP branching, the computational cost of expressive GNNs currently outweighs their gains in decision quality, suggesting that future research must focus on efficiency-preserving expressivity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Granular Framework for Construction Material Price Forecasting: Econometric and Machine-Learning Approaches</title>
<link>https://arxiv.org/abs/2512.09360</link>
<guid>https://arxiv.org/abs/2512.09360</guid>
<content:encoded><![CDATA[
<div> Construction material prices, forecasting, LSTM, ARIMA, cost estimation  

<br /><br />Summary:  
This study addresses the challenge of volatile construction material prices by developing a granular and scalable forecasting framework based on the Construction Specifications Institute (CSI) MasterFormat at the six-digit section level. The framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators to improve forecast accuracy. Four time-series models—Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt—were tested under baseline (CSI data only) and extended (with explanatory variables) configurations. Results show that models incorporating explanatory variables significantly outperform those relying on CSI data alone. Among all models, the LSTM consistently delivered the highest accuracy, achieving root mean square error (RMSE) values as low as 1.390 and mean absolute percentage error (MAPE) values of 0.957, reflecting up to 59% improvement over ARIMA. The framework’s scalability was validated across multiple CSI divisions, with Division 06 (Wood, Plastics, and Composites) provided as a detailed case study. Ultimately, this methodology enables construction owners and contractors to enhance budgeting practices and produce more reliable and definitive-level cost estimations, mitigating risks associated with price volatility in construction materials. <div>
arXiv:2512.09360v1 Announce Type: new 
Abstract: The persistent volatility of construction material prices poses significant risks to cost estimation, budgeting, and project delivery, underscoring the urgent need for granular and scalable forecasting methods. This study develops a forecasting framework that leverages the Construction Specifications Institute (CSI) MasterFormat as the target data structure, enabling predictions at the six-digit section level and supporting detailed cost projections across a wide spectrum of building materials. To enhance predictive accuracy, the framework integrates explanatory variables such as raw material prices, commodity indexes, and macroeconomic indicators. Four time-series models, Long Short-Term Memory (LSTM), Autoregressive Integrated Moving Average (ARIMA), Vector Error Correction Model (VECM), and Chronos-Bolt, were evaluated under both baseline configurations (using CSI data only) and extended versions with explanatory variables. Results demonstrate that incorporating explanatory variables significantly improves predictive performance across all models. Among the tested approaches, the LSTM model consistently achieved the highest accuracy, with RMSE values as low as 1.390 and MAPE values of 0.957, representing improvements of up to 59\% over the traditional statistical time-series model, ARIMA. Validation across multiple CSI divisions confirmed the framework's scalability, while Division 06 (Wood, Plastics, and Composites) is presented in detail as a demonstration case. This research offers a robust methodology that enables owners and contractors to improve budgeting practices and achieve more reliable cost estimation at the Definitive level.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KGOT: Unified Knowledge Graph and Optimal Transport Pseudo-Labeling for Molecule-Protein Interaction Prediction</title>
<link>https://arxiv.org/abs/2512.09365</link>
<guid>https://arxiv.org/abs/2512.09365</guid>
<content:encoded><![CDATA[
<div> Keywords: molecule-protein interactions, pseudo-labeling, optimal transport, heterogeneous biological data, drug discovery  

<br /><br />Summary:  
Predicting molecule-protein interactions (MPIs) is crucial for computational biology tasks like drug discovery and molecular function annotation. Existing MPI models suffer from two main challenges: limited labeled data and underutilization of broader biological context such as gene and pathway information. To overcome these, the proposed framework aggregates diverse biological datasets encompassing molecules, proteins, genes, and pathways. It introduces an optimal transport-based method to generate high-quality pseudo-labels for unlabeled molecule-protein pairs by leveraging the distribution patterns of known interactions. This pseudo-labeling serves as a bridge to integrate multiple biological modalities effectively, allowing the use of heterogeneous data to improve MPI prediction. The framework was tested on various MPI benchmarks, including virtual screening and protein retrieval tasks, where it significantly outperformed state-of-the-art methods in accuracy and zero-shot prediction of unseen interactions. Beyond improving MPI prediction, this approach establishes a new paradigm for combining diverse biological data sources, addressing limitations of single- or bi-modal learning systems. It holds promise for advancing computational biology and accelerating drug discovery efforts through more comprehensive and integrative modeling approaches. <div>
arXiv:2512.09365v1 Announce Type: new 
Abstract: Predicting molecule-protein interactions (MPIs) is a fundamental task in computational biology, with crucial applications in drug discovery and molecular function annotation. However, existing MPI models face two major challenges. First, the scarcity of labeled molecule-protein pairs significantly limits model performance, as available datasets capture only a small fraction of biological relevant interactions. Second, most methods rely solely on molecular and protein features, ignoring broader biological context such as genes, metabolic pathways, and functional annotations that could provide essential complementary information. To address these limitations, our framework first aggregates diverse biological datasets, including molecular, protein, genes and pathway-level interactions, and then develop an optimal transport-based approach to generate high-quality pseudo-labels for unlabeled molecule-protein pairs, leveraging the underlying distribution of known interactions to guide label assignment. By treating pseudo-labeling as a mechanism for bridging disparate biological modalities, our approach enables the effective use of heterogeneous data to enhance MPI prediction. We evaluate our framework on multiple MPI datasets including virtual screening tasks and protein retrieval tasks, demonstrating substantial improvements over state-of-the-art methods in prediction accuracies and zero shot ability across unseen interactions. Beyond MPI prediction, our approach provides a new paradigm for leveraging diverse biological data sources to tackle problems traditionally constrained by single- or bi-modal learning, paving the way for future advances in computational biology and drug discovery.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFLight: Enhancing Safety with Traffic Signal Control through Counterfactual Learning</title>
<link>https://arxiv.org/abs/2512.09368</link>
<guid>https://arxiv.org/abs/2512.09368</guid>
<content:encoded><![CDATA[
<div> Keywords: Traffic Signal Control, Reinforcement Learning, Counterfactual Learning, Safety, Traffic Accidents<br /><br />Summary:<br /><br />This paper addresses the critical issue of traffic safety at intersections where accidents frequently occur, proposing improvements to Traffic Signal Control (TSC) using Reinforcement Learning (RL). Traditional RL approaches often focus on optimizing driving efficiency but neglect safety concerns and lack interpretability. The authors introduce a novel framework that applies Counterfactual (CF) learning to RL for TSC, asking the question: what would happen if alternative actions were taken after an unsafe event? To analyze this, a new structural causal model is designed to predict outcomes following different actions, alongside a CF module integrated with additional "X" modules to encourage safer RL behavior. The resulting algorithm, CFLight, demonstrates a near-zero collision control strategy, effectively reducing safety incidents at intersections. Extensive experiments on both real-world and synthetic datasets validate that CFLight outperforms conventional RL and recent safe RL models in reducing collisions and enhancing overall traffic performance. Beyond traffic management, the proposed framework offers a generalized and interpretable approach to safe RL that could be adapted in other applications requiring a balance between performance and safety. The authors provide open access to their code and datasets for further research and development. <div>
arXiv:2512.09368v1 Announce Type: new 
Abstract: Traffic accidents result in millions of injuries and fatalities globally, with a significant number occurring at intersections each year. Traffic Signal Control (TSC) is an effective strategy for enhancing safety at these urban junctures. Despite the growing popularity of Reinforcement Learning (RL) methods in optimizing TSC, these methods often prioritize driving efficiency over safety, thus failing to address the critical balance between these two aspects. Additionally, these methods usually need more interpretability. CounterFactual (CF) learning is a promising approach for various causal analysis fields. In this study, we introduce a novel framework to improve RL for safety aspects in TSC. This framework introduces a novel method based on CF learning to address the question: ``What if, when an unsafe event occurs, we backtrack to perform alternative actions, and will this unsafe event still occur in the subsequent period?'' To answer this question, we propose a new structure causal model to predict the result after executing different actions, and we propose a new CF module that integrates with additional ``X'' modules to promote safe RL practices. Our new algorithm, CFLight, which is derived from this framework, effectively tackles challenging safety events and significantly improves safety at intersections through a near-zero collision control strategy. Through extensive numerical experiments on both real-world and synthetic datasets, we demonstrate that CFLight reduces collisions and improves overall traffic performance compared to conventional RL methods and the recent safe RL model. Moreover, our method represents a generalized and safe framework for RL methods, opening possibilities for applications in other domains. The data and code are available in the github https://github.com/MJLee00/CFLight-Enhancing-Safety-with-Traffic-Signal-Control-through-Counterfactual-Learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.09369</link>
<guid>https://arxiv.org/abs/2512.09369</guid>
<content:encoded><![CDATA[
<div> Keywords: PathHD, hyperdimensional computing, knowledge graphs, large language models, efficient reasoning

<br /><br />Summary:  
This paper introduces PathHD, a novel lightweight and encoder-free framework for knowledge graph (KG) reasoning combined with large language models (LLMs). Existing KG-LLM methods suffer from high latency, heavy GPU usage, and lack interpretability due to extensive neural encoding or multiple LLM calls. PathHD addresses these issues by replacing neural path scoring with hyperdimensional computing (HDC), requiring only a single LLM call per query. The framework encodes relation paths into block-diagonal GHRR hypervectors, enabling efficient ranking using blockwise cosine similarity and Top-K pruning. It then applies a one-shot LLM adjudication to generate the final answer along with supporting KG paths for transparent reasoning. Key technical contributions include an order-aware, non-commutative binding operator for precise path composition, a calibrated similarity measure for robust retrieval in hypervector space, and a one-shot adjudication step that balances interpretability with minimal LLM invocation. Evaluations on datasets like WebQSP, CWQ, and GrailQA demonstrate that PathHD achieves comparable or superior Hits@1 accuracy to strong neural baselines, cuts end-to-end latency by 40–60%, and reduces GPU memory consumption by 3–5 times. Moreover, PathHD provides faithful, path-grounded explanations that enhance error diagnosis and system controllability. Overall, this work shows that carefully designed HDC representations offer a practical and scalable substrate for efficient KG-LLM reasoning with favorable accuracy, efficiency, and interpretability trade-offs. <div>
arXiv:2512.09369v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rates and architectures for learning geometrically non-trivial operators</title>
<link>https://arxiv.org/abs/2512.09376</link>
<guid>https://arxiv.org/abs/2512.09376</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, operator learning, double fibration transforms, integral geometry, scientific machine learning<br /><br />Summary:<br /><br />1. The article addresses deep learning methods capable of recovering operators between high-dimensional spaces, such as solution maps of partial differential equations (PDEs), from limited training data. 2. Previous theoretical results established data-efficiency for certain elliptic operators on simple geometries that do not propagate singularities; however, many scientific problems involve propagation of singularities in complex or unknown ways (e.g., waves, advection, fluid dynamics). 3. To bridge this gap, the authors extend learning theory to a broader class of geometric integral operators known as double fibration transforms, which generalize Radon and geodesic ray transforms. 4. They prove that this class of operators avoids the curse of dimensionality, with approximation error decreasing superalgebraically relative to the number of training samples. 5. Additionally, the study proposes neural network architectures inspired by cross-attention mechanisms and level set methods that explicitly encode the geometry of these transforms, providing a universal, stable parameterization that efficiently learns double fibration transforms from very few examples. This work advances theoretical understanding in scientific machine learning, particularly operator learning for problems involving complex singularity propagation. <div>
arXiv:2512.09376v1 Announce Type: new 
Abstract: Deep learning methods have proven capable of recovering operators between high-dimensional spaces, such as solution maps of PDEs and similar objects in mathematical physics, from very few training samples. This phenomenon of data-efficiency has been proven for certain classes of elliptic operators with simple geometry, i.e., operators that do not change the domain of the function or propagate singularities. However, scientific machine learning is commonly used for problems that do involve the propagation of singularities in a priori unknown ways, such as waves, advection, and fluid dynamics. In light of this, we expand the learning theory to include double fibration transforms--geometric integral operators that include generalized Radon and geodesic ray transforms. We prove that this class of operators does not suffer from the curse of dimensionality: the error decays superalgebraically, that is, faster than any fixed power of the reciprocal of the number of training samples. Furthermore, we investigate architectures that explicitly encode the geometry of these transforms, demonstrating that an architecture reminiscent of cross-attention based on levelset methods yields a parameterization that is universal, stable, and learns double fibration transforms from very few training examples. Our results contribute to a rapidly-growing line of theoretical work on learning operators for scientific machine learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Distillation Assisted Vehicle Edge Caching Scheme Based on Lightweight DDPM</title>
<link>https://arxiv.org/abs/2512.09378</link>
<guid>https://arxiv.org/abs/2512.09378</guid>
<content:encoded><![CDATA[
<div> Vehicle edge caching, federated learning, denoising diffusion probabilistic model, communication overhead, cache hit percentage<br /><br />Summary:<br /><br />The article addresses the challenge of reducing latency for vehicle users (VUs) accessing content by leveraging vehicle edge caching, which preloads user-interested content at edge nodes. Preserving user privacy during content prediction is essential, and while traditional federated learning (FL) provides privacy protection by sharing model updates instead of raw data, it suffers from high communication overhead due to frequent model transmissions. Furthermore, vehicle mobility leads to vehicles often leaving the roadside unit (RSU) coverage area before training completes, causing training failures. To overcome these problems, the article proposes a novel vehicle edge caching scheme that incorporates federated distillation assisted by a lightweight denoising diffusion probabilistic model (LDPM). This approach reduces the size and frequency of transmitted information, thereby lowering communication overhead. Simulation results confirm that the proposed scheme enhances robustness against varying vehicle speeds, ensuring more reliable training and caching performance. Additionally, it significantly improves cache hit percentage, meaning more requested content is served directly from the edge cache, reducing access latency and improving the user experience for VUs. Overall, the proposed method effectively balances privacy, communication efficiency, and caching accuracy in vehicle edge networks. <div>
arXiv:2512.09378v1 Announce Type: new 
Abstract: Vehicle edge caching is a promising technology that can significantly reduce the latency for vehicle users (VUs) to access content by pre-caching user-interested content at edge nodes. It is crucial to accurately predict the content that VUs are interested in without exposing their privacy. Traditional federated learning (FL) can protect user privacy by sharing models rather than raw data. However, the training of FL requires frequent model transmission, which can result in significant communication overhead. Additionally, vehicles may leave the road side unit (RSU) coverage area before training is completed, leading to training failures. To address these issues, in this letter, we propose a federated distillation-assisted vehicle edge caching scheme based on lightweight denoising diffusion probabilistic model (LDPM). The simulation results demonstrate that the proposed vehicle edge caching scheme has good robustness to variations in vehicle speed, significantly reducing communication overhead and improving cache hit percentage.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Transportation: A Conditional Transformer for Accident-Informed Traffic Forecasting</title>
<link>https://arxiv.org/abs/2512.09398</link>
<guid>https://arxiv.org/abs/2512.09398</guid>
<content:encoded><![CDATA[
<div> Traffic prediction, spatio-temporal data mining, ConFormer, traffic accidents, guided normalization<br /><br />Summary:  
Traffic prediction remains challenging due to the complex influence of external factors like traffic accidents and regulations, which many existing models fail to fully integrate. To overcome this, the authors introduce two enriched traffic datasets from Tokyo and California that incorporate accident and regulation data. Building on these datasets, they propose ConFormer (Conditional Transformer), a novel framework that combines graph propagation with a guided normalization layer. This innovative design allows dynamic adjustments of spatial and temporal node relationships based on historical data patterns, improving forecasting accuracy. The model demonstrates superior predictive performance and efficiency compared to the state-of-the-art STAEFormer, with lower computational costs and fewer parameters. Extensive experiments validate that ConFormer consistently outperforms mainstream spatio-temporal prediction methods across multiple evaluation metrics. The results suggest that the integration of external traffic factors and the novel architectural components of ConFormer have significant potential to advance traffic prediction research and applications. <div>
arXiv:2512.09398v1 Announce Type: new 
Abstract: Traffic prediction remains a key challenge in spatio-temporal data mining, despite progress in deep learning. Accurate forecasting is hindered by the complex influence of external factors such as traffic accidents and regulations, often overlooked by existing models due to limited data integration. To address these limitations, we present two enriched traffic datasets from Tokyo and California, incorporating traffic accident and regulation data. Leveraging these datasets, we propose ConFormer (Conditional Transformer), a novel framework that integrates graph propagation with guided normalization layer. This design dynamically adjusts spatial and temporal node relationships based on historical patterns, enhancing predictive accuracy. Our model surpasses the state-of-the-art STAEFormer in both predictive performance and efficiency, achieving lower computational costs and reduced parameter demands. Extensive evaluations demonstrate that ConFormer consistently outperforms mainstream spatio-temporal baselines across multiple metrics, underscoring its potential to advance traffic prediction research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Black-Box Behavioral Distillation Breaks Safety Alignment in Medical LLMs</title>
<link>https://arxiv.org/abs/2512.09403</link>
<guid>https://arxiv.org/abs/2512.09403</guid>
<content:encoded><![CDATA[
<div> Keywords: medical LLMs, black-box distillation, alignment collapse, adversarial prompts, safety mechanisms<br /><br />Summary:<br /><br />1. The paper investigates the vulnerability of safety-aligned medical large language models (LLMs) to black-box model extraction attacks, a topic underexplored compared to classification models and memorization leakage.  
2. The authors develop a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access, without needing access to model weights, safety filters, or training data.  
3. By issuing 48,000 instruction queries to the Meditron-7B model and collecting 25,000 benign instruction-response pairs, they fine-tune a LLaMA3 8B surrogate via parameter-efficient LoRA under zero-alignment supervision conditions, at a minimal cost of $12.  
4. The surrogate achieves high fidelity on benign input tasks while significantly degrading alignment by producing unsafe outputs on 86% of adversarial prompts, a rate much higher than Meditron-7B (66%) and the untuned base model (46%), revealing a critical functional-ethical gap.  
5. To understand this alignment collapse, a dynamic adversarial evaluation framework is proposed, leveraging generative query-based harmful prompt generation, filtering, failure analysis, and adaptive jailbreak attacks. A layered defense system for real-time alignment drift detection in black-box deployments is also introduced.  
6. The study highlights a practical threat whereby adversaries can cheaply replicate medical LLM capabilities but remove safety mechanisms, emphasizing the urgent need for extraction-aware monitoring to maintain safe deployment of medical LLMs. <div>
arXiv:2512.09403v1 Announce Type: new 
Abstract: As medical large language models (LLMs) become increasingly integrated into clinical workflows, concerns around alignment robustness, and safety are escalating. Prior work on model extraction has focused on classification models or memorization leakage, leaving the vulnerability of safety-aligned generative medical LLMs underexplored.
  We present a black-box distillation attack that replicates the domain-specific reasoning of safety-aligned medical LLMs using only output-level access. By issuing 48,000 instruction queries to Meditron-7B and collecting 25,000 benign instruction response pairs, we fine-tune a LLaMA3 8B surrogate via parameter efficient LoRA under a zero-alignment supervision setting, requiring no access to model weights, safety filters, or training data. With a cost of $12, the surrogate achieves strong fidelity on benign inputs while producing unsafe completions for 86% of adversarial prompts, far exceeding both Meditron-7B (66%) and the untuned base model (46%). This reveals a pronounced functional-ethical gap, task utility transfers, while alignment collapses. To analyze this collapse, we develop a dynamic adversarial evaluation framework combining Generative Query (GQ)-based harmful prompt generation, verifier filtering, category-wise failure analysis, and adaptive Random Search (RS) jailbreak attacks. We also propose a layered defense system, as a prototype detector for real-time alignment drift in black-box deployments.
  Our findings show that benign-only black-box distillation exposes a practical and under-recognized threat: adversaries can cheaply replicate medical LLM capabilities while stripping safety mechanisms, underscoring the need for extraction-aware safety monitoring.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cauchy-Schwarz Fairness Regularizer</title>
<link>https://arxiv.org/abs/2512.09467</link>
<guid>https://arxiv.org/abs/2512.09467</guid>
<content:encoded><![CDATA[
<div> Keywords: group fairness, fairness regularizer, Cauchy-Schwarz divergence, demographic parity, utility-fairness trade-off<br /><br />Summary:  
This paper addresses the challenge of designing effective fairness regularizers in machine learning that reduce dependence between model predictions and sensitive attributes. The authors categorize existing in-process fairness methods into three families: matching prediction statistics across sensitive groups, aligning latent representations, and directly minimizing dependence between predictions and sensitive attributes. They identify key desirable properties for a fairness regularizer's underlying distance measure, such as tight generalization bounds, robustness to scale differences, and the capability to handle arbitrary prediction distributions. Motivated by these criteria, the paper proposes a novel Cauchy-Schwarz (CS) fairness regularizer, which penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Theoretical analysis under Gaussian assumptions demonstrates that CS divergence provides tighter bounds compared to Kullback-Leibler divergence, Maximum Mean Discrepancy, and mean disparity used in Demographic Parity. The proposed approach extends naturally to multiple sensitive attributes via a distribution-free, kernel-based estimator. Experimental evaluation on four tabular datasets and one image dataset shows that the CS regularizer consistently improves fairness metrics such as Demographic Parity and Equal Opportunity while maintaining competitive model accuracy. Additionally, the CS regularizer exhibits more stable and favorable utility-fairness trade-offs across different hyperparameter settings compared to existing regularizers. <div>
arXiv:2512.09467v1 Announce Type: new 
Abstract: Group fairness in machine learning is often enforced by adding a regularizer that reduces the dependence between model predictions and sensitive attributes. However, existing regularizers are built on heterogeneous distance measures and design choices, which makes their behavior hard to reason about and their performance inconsistent across tasks. This raises a basic question: what properties make a good fairness regularizer? We address this question by first organizing existing in-process methods into three families: (i) matching prediction statistics across sensitive groups, (ii) aligning latent representations, and (iii) directly minimizing dependence between predictions and sensitive attributes. Through this lens, we identify desirable properties of the underlying distance measure, including tight generalization bounds, robustness to scale differences, and the ability to handle arbitrary prediction distributions. Motivated by these properties, we propose a Cauchy-Schwarz (CS) fairness regularizer that penalizes the empirical CS divergence between prediction distributions conditioned on sensitive groups. Under a Gaussian comparison, we show that CS divergence yields a tighter bound than Kullback-Leibler divergence, Maximum Mean Discrepancy, and the mean disparity used in Demographic Parity, and we discuss how these advantages translate to a distribution-free, kernel-based estimator that naturally extends to multiple sensitive attributes. Extensive experiments on four tabular benchmarks and one image dataset demonstrate that the proposed CS regularizer consistently improves Demographic Parity and Equal Opportunity metrics while maintaining competitive accuracy, and achieves a more stable utility-fairness trade-off across hyperparameter settings compared to prior regularizers.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Invariance and Allocation: When Subgroup Balance Matters</title>
<link>https://arxiv.org/abs/2512.09496</link>
<guid>https://arxiv.org/abs/2512.09496</guid>
<content:encoded><![CDATA[
<div> Demographic representation, data imbalance, subgroup performance, latent separation hypothesis, foundation model fine-tuning<br /><br />Summary:  
1. The paper addresses the challenges posed by unequal demographic subgroup representation in training data, which affects the generalisation performance of machine learning models across diverse populations.  
2. Contrary to standard beliefs that balancing subgroup representation improves overall performance, the study highlights empirical findings where imbalanced data can either enhance subgroup performance or where missing a complete subgroup in training does not negatively impact that subgroup’s performance.  
3. The authors conduct a comprehensive analysis using four vision and language models, systematically varying the composition of training data to evaluate how subgroup performance responds to data balance changes.  
4. They propose the latent separation hypothesis suggesting that the model’s reliance on subgroup representation during partial fine-tuning depends on how distinctly subgroups are separated in the latent space of the pre-trained model. The hypothesis is both formalised theoretically and supported by empirical validation.  
5. Practical implications are demonstrated by applying the hypothesis to foundation model fine-tuning, where measuring latent subgroup separation offers actionable insights for guiding data collection and balancing strategies, ultimately improving model fairness and robustness. <div>
arXiv:2512.09496v1 Announce Type: new 
Abstract: Unequal representation of demographic groups in training data poses challenges to model generalisation across populations. Standard practice assumes that balancing subgroup representation optimises performance. However, recent empirical results contradict this assumption: in some cases, imbalanced data distributions actually improve subgroup performance, while in others, subgroup performance remains unaffected by the absence of an entire subgroup during training. We conduct a systematic study of subgroup allocation across four vision and language models, varying training data composition to characterise the sensitivity of subgroup performance to data balance. We propose the latent separation hypothesis, which states that a partially fine-tuned model's dependence on subgroup representation is determined by the degree of separation between subgroups in the latent space of the pre-trained model. We formalise this hypothesis, provide theoretical analysis, and validate it empirically. Finally, we present a practical application to foundation model fine-tuning, demonstrating that quantitative analysis of latent subgroup separation can inform data collection and balancing decisions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Dynamic Pricing with Heterogeneous Buyers</title>
<link>https://arxiv.org/abs/2512.09513</link>
<guid>https://arxiv.org/abs/2512.09513</guid>
<content:encoded><![CDATA[
<div> contextual dynamic pricing, heterogeneous buyers, regret bounds, optimistic posterior sampling, variance-aware zooming  

<br /><br />Summary:  
1. This paper addresses the problem of contextual dynamic pricing where a seller sets prices over multiple rounds based on observable context and receives only binary purchase feedback.  
2. Unlike previous works assuming homogeneous buyer valuations, this work considers heterogeneous buyer populations with valuations drawn from an unknown finite-support distribution of size \( K_{\star} \).  
3. The authors introduce a new pricing algorithm using optimistic posterior sampling that achieves a regret bound of order \(\widetilde{O}(K_{\star} \sqrt{d T})\), where \(d\) is the context dimension and \(T\) the number of rounds.  
4. They prove that this regret bound is tight in terms of \(d\) and \(T\) up to logarithmic factors, establishing near-optimal performance of the algorithm in these parameters.  
5. For the simpler non-contextual pricing scenario, the authors propose a variance-aware zooming algorithm that improves dependence on \(K_{\star}\), achieving the optimal regret rate with respect to the number of buyer types. <div>
arXiv:2512.09513v1 Announce Type: new 
Abstract: We initiate the study of contextual dynamic pricing with a heterogeneous population of buyers, where a seller repeatedly posts prices (over $T$ rounds) that depend on the observable $d$-dimensional context and receives binary purchase feedback. Unlike prior work assuming homogeneous buyer types, in our setting the buyer's valuation type is drawn from an unknown distribution with finite support size $K_{\star}$. We develop a contextual pricing algorithm based on optimistic posterior sampling with regret $\widetilde{O}(K_{\star}\sqrt{dT})$, which we prove to be tight in $d$ and $T$ up to logarithmic terms. Finally, we refine our analysis for the non-contextual pricing case, proposing a variance-aware zooming algorithm that achieves the optimal dependence on $K_{\star}$.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QuanvNeXt: An end-to-end quanvolutional neural network for EEG-based detection of major depressive disorder</title>
<link>https://arxiv.org/abs/2512.09517</link>
<guid>https://arxiv.org/abs/2512.09517</guid>
<content:encoded><![CDATA[
<div> QuanvNeXt, EEG, Depression Diagnosis, Cross Residual Block, Explainable AI  

<br /><br />Summary:  
This study introduces QuanvNeXt, an end-to-end fully quanvolutional neural network designed for EEG-based depression diagnosis. A key innovation is the novel Cross Residual block, which enhances the model by reducing feature homogeneity and strengthening cross-feature relationships while keeping parameter efficiency intact. QuanvNeXt was evaluated on two publicly available EEG datasets, achieving an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming leading baseline models such as InceptionTime, which scored 91.7% accuracy and 95.9% AUC-ROC. The authors conducted an uncertainty analysis under Gaussian noise perturbations, finding that QuanvNeXt maintains well-calibrated predictions, with Expected Calibration Error (ECE) scores ranging from low (0.0436) to moderate (0.1159) even at the highest noise level tested (epsilon = 0.1). Furthermore, a post-hoc explainable AI analysis validated that QuanvNeXt effectively captures relevant spectrotemporal EEG patterns that discriminate between healthy individuals and those with major depressive disorder. Overall, QuanvNeXt demonstrates an efficient, reliable, and interpretable approach that advances the state-of-the-art in automated EEG-based depression diagnosis. <div>
arXiv:2512.09517v1 Announce Type: new 
Abstract: This study presents QuanvNeXt, an end-to-end fully quanvolutional model for EEG-based depression diagnosis. QuanvNeXt incorporates a novel Cross Residual block, which reduces feature homogeneity and strengthens cross-feature relationships while retaining parameter efficiency. We evaluated QuanvNeXt on two open-source datasets, where it achieved an average accuracy of 93.1% and an average AUC-ROC of 97.2%, outperforming state-of-the-art baselines such as InceptionTime (91.7% accuracy, 95.9% AUC-ROC). An uncertainty analysis across Gaussian noise levels demonstrated well-calibrated predictions, with ECE scores remaining low (0.0436, Dataset 1) to moderate (0.1159, Dataset 2) even at the highest perturbation ({\epsilon} = 0.1). Additionally, a post-hoc explainable AI analysis confirmed that QuanvNeXt effectively identifies and learns spectrotemporal patterns that distinguish between healthy controls and major depressive disorder. Overall, QuanvNeXt establishes an efficient and reliable approach for EEG-based depression diagnosis.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent-Autoregressive GP-VAE Language Model</title>
<link>https://arxiv.org/abs/2512.09535</link>
<guid>https://arxiv.org/abs/2512.09535</guid>
<content:encoded><![CDATA[
<div> Keywords: Latent AutoRegressive, Gaussian Process, Variational Autoencoder, non-autoregressive decoder, temporal structure<br /><br />Summary:  
This study proposes a novel Latent AutoRegressive model that integrates a Gaussian Process (GP) into a Variational Autoencoder (VAE) framework. The model shifts sequential dynamics from the traditional observation space into a continuous latent space, enabling a distinct approach to temporal modeling. Unlike typical autoregressive language models, linguistic generation here is handled in parallel through a non-autoregressive decoder, which is designed to enhance efficiency. The authors present a comprehensive methodological formulation including a causal GP prior that enforces temporal causality, a structured amortized posterior to approximate the latent distribution effectively, and a training protocol that relies on a regularized Evidence Lower Bound Objective (ELBO). Experiments were conducted within a constrained proof-of-concept (POC) setting to evaluate model stability and consistency across different sampling methods. Results demonstrate that the model can be trained stably with consistent performance between sequential and parallel sampling variants. Crucially, the findings indicate that temporal dependencies in language can be partly captured through the probabilistic geometry of the latent space itself, rather than relying solely on explicit neural autoregressive mechanisms. This approach suggests promising directions for efficient and theoretically grounded temporal modeling in generative language tasks. <div>
arXiv:2512.09535v1 Announce Type: new 
Abstract: We investigate a fully Latent AutoRegressive scheme based on a Gaussian Process (GP) integrated into a Variational Autoencoder (VAE). In this setting, sequential dynamics are transferred from the observation space to a continuous latent space, while linguistic generation remains parallel through a non-autoregressive decoder. We present a complete methodological formulation, including a causal GP prior, a structured amortized posterior, and a training protocol based on a regularized ELBO. Empirical evaluation, conducted within a deliberately constrained proof-of-concept (POC) framework, shows that the model can be trained stably and that the sequential and parallel sampling variants exhibit consistent behavior. Overall, the results suggest that part of the temporal structure in a language model can be supported by the probabilistic geometry of the latent space rather than by explicit neural operations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stanford Sleep Bench: Evaluating Polysomnography Pre-training Methods for Sleep Foundation Models</title>
<link>https://arxiv.org/abs/2512.09591</link>
<guid>https://arxiv.org/abs/2512.09591</guid>
<content:encoded><![CDATA[
<div> Polysomnography, self-supervised learning, sleep staging, disease prediction, contrastive learning<br /><br />Summary:<br /><br />1. Polysomnography (PSG) is the gold standard for sleep analysis, producing large amounts of multimodal clinical data suitable for self-supervised representation learning (SSRL) to pre-train foundational models.<br />2. Current progress in developing sleep foundation models faces two major challenges: the lack of a shared, diverse dataset and benchmark for training and evaluation, and the absence of a comprehensive evaluation of SSRL methods across sleep-related tasks.<br />3. To overcome these challenges, the authors introduce Stanford Sleep Bench, a large-scale dataset comprising 17,467 PSG recordings totaling over 163,000 hours, collected from a major sleep clinic. It includes 13 clinical disease prediction tasks as well as key sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation.<br />4. The study systematically evaluates various SSRL pre-training approaches on the Stanford Sleep Bench dataset across four downstream tasks: sleep staging, apnea diagnosis, age estimation, and disease & mortality prediction.<br />5. Results demonstrate that multiple pretraining methods perform similarly on sleep staging, apnea diagnosis, and age estimation tasks; however, for mortality and disease prediction, contrastive learning significantly outperforms other methods and achieves faster convergence during pretraining.<br />6. To promote reproducibility and further research, the Stanford Sleep Bench dataset, along with pretrained model weights, training pipelines, and evaluation code, will be publicly released. <div>
arXiv:2512.09591v1 Announce Type: new 
Abstract: Polysomnography (PSG), the gold standard test for sleep analysis, generates vast amounts of multimodal clinical data, presenting an opportunity to leverage self-supervised representation learning (SSRL) for pre-training foundation models to enhance sleep analysis. However, progress in sleep foundation models is hindered by two key limitations: (1) the lack of a shared dataset and benchmark with diverse tasks for training and evaluation, and (2) the absence of a systematic evaluation of SSRL approaches across sleep-related tasks. To address these gaps, we introduce Stanford Sleep Bench, a large-scale PSG dataset comprising 17,467 recordings totaling over 163,000 hours from a major sleep clinic, including 13 clinical disease prediction tasks alongside canonical sleep-related tasks such as sleep staging, apnea diagnosis, and age estimation. We systematically evaluate SSRL pre-training methods on Stanford Sleep Bench, assessing downstream performance across four tasks: sleep staging, apnea diagnosis, age estimation, and disease and mortality prediction. Our results show that multiple pretraining methods achieve comparable performance for sleep staging, apnea diagnosis, and age estimation. However, for mortality and disease prediction, contrastive learning significantly outperforms other approaches while also converging faster during pretraining. To facilitate reproducibility and advance sleep research, we will release Stanford Sleep Bench along with pretrained model weights, training pipelines, and evaluation code.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic-Aware Cooperative Communication and Computation Framework in Vehicular Networks</title>
<link>https://arxiv.org/abs/2512.09621</link>
<guid>https://arxiv.org/abs/2512.09621</guid>
<content:encoded><![CDATA[
<div> Semantic Communication, Vehicular Edge Computing, Internet of Vehicles, Multi-agent Proximal Policy Optimization, Task Offloading  

<br /><br />Summary:  
This paper addresses the integration of Semantic Communication (SC) with Vehicular Edge Computing (VEC) to enhance task processing efficiency in Internet of Vehicles (IoV), particularly focused on highway scenarios. It proposes a novel Tripartite Cooperative Semantic Communication (TCSC) framework that allows Vehicle Users (VUs) to offload semantic tasks through both Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communication modes. The framework aims to optimize task latency and minimize the number of semantic symbols needed for communication. To tackle this, the problem is formulated as a Mixed-Integer Nonlinear Programming (MINLP) model, which is then decomposed into two manageable subproblems. The first subproblem involves optimizing the number of semantic symbols and is innovatively solved by a multi-agent proximal policy optimization algorithm enhanced with parametric distribution noise (MAPPO-PDN). The second subproblem, concerning the offloading ratio, is approached through linear programming techniques. Simulation results demonstrate that this combined optimization strategy outperforms existing baseline algorithms in efficiency and effectiveness, highlighting the promise of the TCSC framework for next-generation vehicular communication systems. <div>
arXiv:2512.09621v1 Announce Type: new 
Abstract: Semantic Communication (SC) combined with Vehicular edge computing (VEC) provides an efficient edge task processing paradigm for Internet of Vehicles (IoV). Focusing on highway scenarios, this paper proposes a Tripartite Cooperative Semantic Communication (TCSC) framework, which enables Vehicle Users (VUs) to perform semantic task offloading via Vehicle-to-Infrastructure (V2I) and Vehicle-to-Vehicle (V2V) communications. Considering task latency and the number of semantic symbols, the framework constructs a Mixed-Integer Nonlinear Programming (MINLP) problem, which is transformed into two subproblems. First, we innovatively propose a multi-agent proximal policy optimization task offloading optimization method based on parametric distribution noise (MAPPO-PDN) to solve the optimization problem of the number of semantic symbols; second, linear programming (LP) is used to solve offloading ratio. Simulations show that performance of this scheme is superior to that of other algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Membership and Dataset Inference Attacks on Large Audio Generative Models</title>
<link>https://arxiv.org/abs/2512.09654</link>
<guid>https://arxiv.org/abs/2512.09654</guid>
<content:encoded><![CDATA[
arXiv:2512.09654v1 Announce Type: new 
Abstract: Generative audio models, based on diffusion and autoregressive architectures, have advanced rapidly in both quality and expressiveness. This progress, however, raises pressing copyright concerns, as such models are often trained on vast corpora of artistic and commercial works. A central question is whether one can reliably verify if an artist's material was included in training, thereby providing a means for copyright holders to protect their content. In this work, we investigate the feasibility of such verification through membership inference attacks (MIA) on open-source generative audio models, which attempt to determine whether a specific audio sample was part of the training set. Our empirical results show that membership inference alone is of limited effectiveness at scale, as the per-sample membership signal is weak for models trained on large and diverse datasets. However, artists and media owners typically hold collections of works rather than isolated samples. Building on prior work in text and vision domains, in this work we focus on dataset inference (DI), which aggregates diverse membership evidence across multiple samples. We find that DI is successful in the audio domain, offering a more practical mechanism for assessing whether an artist's works contributed to model training. Our results suggest DI as a promising direction for copyright protection and dataset accountability in the era of large audio generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drawback of Enforcing Equivariance and its Compensation via the Lens of Expressive Power</title>
<link>https://arxiv.org/abs/2512.09673</link>
<guid>https://arxiv.org/abs/2512.09673</guid>
<content:encoded><![CDATA[
arXiv:2512.09673v1 Announce Type: new 
Abstract: Equivariant neural networks encode symmetry as an inductive bias and have achieved strong empirical performance in wide domains. However, their expressive power remains not well understood. Focusing on 2-layer ReLU networks, this paper investigates the impact of equivariance constraints on the expressivity of equivariant and layer-wise equivariant networks. By examining the boundary hyperplanes and the channel vectors of ReLU networks, we construct an example showing that equivariance constraints could strictly limit expressive power. However, we demonstrate that this drawback can be compensated via enlarging the model size. Furthermore, we show that despite a larger model size, the resulting architecture could still correspond to a hypothesis space with lower complexity, implying superior generalizability for equivariant networks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A data-driven approach to linking design features with manufacturing process data for sustainable product development</title>
<link>https://arxiv.org/abs/2512.09690</link>
<guid>https://arxiv.org/abs/2512.09690</guid>
<content:encoded><![CDATA[
arXiv:2512.09690v1 Announce Type: new 
Abstract: The growing adoption of Industrial Internet of Things (IIoT) technologies enables automated, real-time collection of manufacturing process data, unlocking new opportunities for data-driven product development. Current data-driven methods are generally applied within specific domains, such as design or manufacturing, with limited exploration of integrating design features and manufacturing process data. Since design decisions significantly affect manufacturing outcomes, such as error rates, energy consumption, and processing times, the lack of such integration restricts the potential for data-driven product design improvements. This paper presents a data-driven approach to mapping and analyzing the relationship between design features and manufacturing process data. A comprehensive system architecture is developed to ensure continuous data collection and integration. The linkage between design features and manufacturing process data serves as the basis for developing a machine learning model that enables automated design improvement suggestions. By integrating manufacturing process data with sustainability metrics, this approach opens new possibilities for sustainable product development.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training One Model to Master Cross-Level Agentic Actions via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09706</link>
<guid>https://arxiv.org/abs/2512.09706</guid>
<content:encoded><![CDATA[
arXiv:2512.09706v1 Announce Type: new 
Abstract: The paradigm of agentic AI is shifting from engineered complex workflows to post-training native models. However, existing agents are typically confined to static, predefined action spaces--such as exclusively using APIs, GUI events, or robotic commands. This rigidity limits their adaptability in dynamic environments where the optimal granularity of interaction varies contextually. To bridge this gap, we propose CrossAgent, a unified agentic model that masters heterogeneous action spaces and autonomously selects the most effective interface for each step of a trajectory. We introduce a comprehensive training pipeline that integrates cold-start supervised fine-tuning with a Multi-Turn Group Relative Policy Optimization (GRPO) algorithm. This approach enables the agent to learn adaptive action switching--balancing high-level efficiency with low-level precision--without human-specified rules. Extensive experiments on over 800 tasks in the open-world Minecraft environment demonstrate that CrossAgent achieves state-of-the-art performance. By dynamically leveraging the strengths of diverse action spaces, our model significantly outperforms fixed-action baselines, exhibiting superior generalization and efficiency in long-horizon reasoning. All code and models are available at https://github.com/CraftJarvis/OpenHA
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Lookup Key-Value Experts</title>
<link>https://arxiv.org/abs/2512.09723</link>
<guid>https://arxiv.org/abs/2512.09723</guid>
<content:encoded><![CDATA[
arXiv:2512.09723v1 Announce Type: new 
Abstract: Recent research has developed several LLM architectures suitable for inference on end-user devices, such as the Mixture of Lookup Experts (MoLE)~\parencite{jie_mixture_2025}. A key feature of MoLE is that each token id is associated with a dedicated group of experts. For a given input, only the experts corresponding to the input token id will be activated. Since the communication overhead of loading this small number of activated experts into RAM during inference is negligible, expert parameters can be offloaded to storage, making MoLE suitable for resource-constrained devices. However, MoLE's context-independent expert selection mechanism, based solely on input ids, may limit model performance. To address this, we propose the \textbf{M}ixture \textbf{o}f \textbf{L}ookup \textbf{K}ey-\textbf{V}alue Experts (\textbf{MoLKV}) model. In MoLKV, each expert is structured as a key-value pair. For a given input, the input-derived query interacts with the cached key-value experts from the current sequence, generating a context-aware expert output. This context-aware mechanism alleviates the limitation of MoLE, and experimental results demonstrate that MoLKV achieves significantly lower validation loss in small-scale evaluations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Circuits, Features, and Heuristics in Molecular Transformers</title>
<link>https://arxiv.org/abs/2512.09757</link>
<guid>https://arxiv.org/abs/2512.09757</guid>
<content:encoded><![CDATA[
arXiv:2512.09757v1 Announce Type: new 
Abstract: Transformers generate valid and diverse chemical structures, but little is known about the mechanisms that enable these models to capture the rules of molecular representation. We present a mechanistic analysis of autoregressive transformers trained on drug-like small molecules to reveal the computational structure underlying their capabilities across multiple levels of abstraction. We identify computational patterns consistent with low-level syntactic parsing and more abstract chemical validity constraints. Using sparse autoencoders (SAEs), we extract feature dictionaries associated with chemically relevant activation patterns. We validate our findings on downstream tasks and find that mechanistic insights can translate to predictive performance in various practical settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Aware Heterogeneous GNN Architecture for Real-Time BESS Optimization in Unbalanced Distribution Systems</title>
<link>https://arxiv.org/abs/2512.09780</link>
<guid>https://arxiv.org/abs/2512.09780</guid>
<content:encoded><![CDATA[
arXiv:2512.09780v1 Announce Type: new 
Abstract: Battery energy storage systems (BESS) have become increasingly vital in three-phase unbalanced distribution grids for maintaining voltage stability and enabling optimal dispatch. However, existing deep learning approaches often lack explicit three-phase representation, making it difficult to accurately model phase-specific dynamics and enforce operational constraints--leading to infeasible dispatch solutions. This paper demonstrates that by embedding detailed three-phase grid information--including phase voltages, unbalanced loads, and BESS states--into heterogeneous graph nodes, diverse GNN architectures (GCN, GAT, GraphSAGE, GPS) can jointly predict network state variables with high accuracy. Moreover, a physics-informed loss function incorporates critical battery constraints--SoC and C-rate limits--via soft penalties during training. Experimental validation on the CIGRE 18-bus distribution system shows that this embedding-loss approach achieves low prediction errors, with bus voltage MSEs of 6.92e-07 (GCN), 1.21e-06 (GAT), 3.29e-05 (GPS), and 9.04e-07 (SAGE). Importantly, the physics-informed method ensures nearly zero SoC and C-rate constraint violations, confirming its effectiveness for reliable, constraint-compliant dispatch.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Polymer Solubility in Solvents Using SMILES Strings</title>
<link>https://arxiv.org/abs/2512.09784</link>
<guid>https://arxiv.org/abs/2512.09784</guid>
<content:encoded><![CDATA[
arXiv:2512.09784v1 Announce Type: new 
Abstract: Understanding and predicting polymer solubility in various solvents is critical for applications ranging from recycling to pharmaceutical formulation. This work presents a deep learning framework that predicts polymer solubility, expressed as weight percent (wt%), directly from SMILES representations of both polymers and solvents. A dataset of 8,049 polymer solvent pairs at 25 deg C was constructed from calibrated molecular dynamics simulations (Zhou et al., 2023), and molecular descriptors and fingerprints were combined into a 2,394 feature representation per sample. A fully connected neural network with six hidden layers was trained using the Adam optimizer and evaluated using mean squared error loss, achieving strong agreement between predicted and actual solubility values. Generalizability was demonstrated using experimentally measured data from the Materials Genome Project, where the model maintained high accuracy on 25 unseen polymer solvent combinations. These findings highlight the viability of SMILES based machine learning models for scalable solubility prediction and high-throughput solvent screening, supporting applications in green chemistry, polymer processing, and materials design.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TinyD\'ej\`aVu: Smaller Memory Footprint &amp; Faster Inference on Sensor Data Streams with Always-On Microcontrollers</title>
<link>https://arxiv.org/abs/2512.09786</link>
<guid>https://arxiv.org/abs/2512.09786</guid>
<content:encoded><![CDATA[
arXiv:2512.09786v1 Announce Type: new 
Abstract: Always-on sensors are increasingly expected to embark a variety of tiny neural networks and to continuously perform inference on time-series of the data they sense. In order to fit lifetime and energy consumption requirements when operating on battery, such hardware uses microcontrollers (MCUs) with tiny memory budget e.g., 128kB of RAM. In this context, optimizing data flows across neural network layers becomes crucial. In this paper, we introduce TinyD\'ej\`aVu, a new framework and novel algorithms we designed to drastically reduce the RAM footprint required by inference using various tiny ML models for sensor data time-series on typical microcontroller hardware. We publish the implementation of TinyD\'ej\`aVu as open source, and we perform reproducible benchmarks on hardware. We show that TinyD\'ej\`aVu can save more than 60% of RAM usage and eliminate up to 90% of redundant compute on overlapping sliding window inputs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Diversion for Efficient Morphology Control and Policy Transfer</title>
<link>https://arxiv.org/abs/2512.09796</link>
<guid>https://arxiv.org/abs/2512.09796</guid>
<content:encoded><![CDATA[
arXiv:2512.09796v1 Announce Type: new 
Abstract: Universal morphology control aims to learn a universal policy that generalizes across heterogeneous agent morphologies, with Transformer-based controllers emerging as a popular choice. However, such architectures incur substantial computational costs, resulting in high deployment overhead, and existing methods exhibit limited cross-task generalization, necessitating training from scratch for each new task. To this end, we propose \textbf{DivMorph}, a modular training paradigm that leverages knowledge diversion to learn decomposable controllers. DivMorph factorizes randomly initialized Transformer weights into factor units via SVD prior to training and employs dynamic soft gating to modulate these units based on task and morphology embeddings, separating them into shared \textit{learngenes} and morphology- and task-specific \textit{tailors}, thereby achieving knowledge disentanglement. By selectively activating relevant components, DivMorph enables scalable and efficient policy deployment while supporting effective policy transfer to novel tasks. Extensive experiments demonstrate that DivMorph achieves state-of-the-art performance, achieving a 3$\times$ improvement in sample efficiency over direct finetuning for cross-task transfer and a 17$\times$ reduction in model size for single-agent deployment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ariel-ML: Computing Parallelization with Embedded Rust for Neural Networks on Heterogeneous Multi-core Microcontrollers</title>
<link>https://arxiv.org/abs/2512.09800</link>
<guid>https://arxiv.org/abs/2512.09800</guid>
<content:encoded><![CDATA[
arXiv:2512.09800v1 Announce Type: new 
Abstract: Low-power microcontroller (MCU) hardware is currently evolving from single-core architectures to predominantly multi-core architectures. In parallel, new embedded software building blocks are more and more written in Rust, while C/C++ dominance fades in this domain. On the other hand, small artificial neural networks (ANN) of various kinds are increasingly deployed in edge AI use cases, thus deployed and executed directly on low-power MCUs. In this context, both incremental improvements and novel innovative services will have to be continuously retrofitted using ANNs execution in software embedded on sensing/actuating systems already deployed in the field. However, there was so far no Rust embedded software platform automating parallelization for inference computation on multi-core MCUs executing arbitrary TinyML models. This paper thus fills this gap by introducing Ariel-ML, a novel toolkit we designed combining a generic TinyML pipeline and an embedded Rust software platform which can take full advantage of multi-core capabilities of various 32bit microcontroller families (Arm Cortex-M, RISC-V, ESP-32). We published the full open source code of its implementation, which we used to benchmark its capabilities using a zoo of various TinyML models. We show that Ariel-ML outperforms prior art in terms of inference latency as expected, and we show that, compared to pre-existing toolkits using embedded C/C++, Ariel-ML achieves comparable memory footprints. Ariel-ML thus provides a useful basis for TinyML practitioners and resource-constrained embedded Rust developers.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incorporating Fairness in Neighborhood Graphs for Fair Spectral Clustering</title>
<link>https://arxiv.org/abs/2512.09810</link>
<guid>https://arxiv.org/abs/2512.09810</guid>
<content:encoded><![CDATA[
arXiv:2512.09810v1 Announce Type: new 
Abstract: Graph clustering plays a pivotal role in unsupervised learning methods like spectral clustering, yet traditional methods for graph clustering often perpetuate bias through unfair graph constructions that may underrepresent some groups. The current research introduces novel approaches for constructing fair k-nearest neighbor (kNN) and fair epsilon-neighborhood graphs that proactively enforce demographic parity during graph formation. By incorporating fairness constraints at the earliest stage of neighborhood selection steps, our approaches incorporate proportional representation of sensitive features into the local graph structure while maintaining geometric consistency.Our work addresses a critical gap in pre-processing for fair spectral clustering, demonstrating that topological fairness in graph construction is essential for achieving equitable clustering outcomes. Widely used graph construction methods like kNN and epsilon-neighborhood graphs propagate edge based disparate impact on sensitive groups, leading to biased clustering results. Providing representation of each sensitive group in the neighborhood of every node leads to fairer spectral clustering results because the topological features of the graph naturally reflect equitable group ratios. This research fills an essential shortcoming in fair unsupervised learning, by illustrating how topological fairness in graph construction inherently facilitates fairer spectral clustering results without the need for changes to the clustering algorithm itself. Thorough experiments on three synthetic datasets, seven real-world tabular datasets, and three real-world image datasets prove that our fair graph construction methods surpass the current baselines in graph clustering tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting the Containment Time of California Wildfires Using Machine Learning</title>
<link>https://arxiv.org/abs/2512.09835</link>
<guid>https://arxiv.org/abs/2512.09835</guid>
<content:encoded><![CDATA[
arXiv:2512.09835v1 Announce Type: new 
Abstract: California's wildfire season keeps getting worse over the years, overwhelming the emergency response teams. These fires cause massive destruction to both property and human life. Because of these reasons, there's a growing need for accurate and practical predictions that can help assist with resources allocation for the Wildfire managers or the response teams. In this research, we built machine learning models to predict the number of days it will require to fully contain a wildfire in California. Here, we addressed an important gap in the current literature. Most prior research has concentrated on wildfire risk or how fires spread, and the few that examine the duration typically predict it in broader categories rather than a continuous measure. This research treats the wildfire duration prediction as a regression task, which allows for more detailed and precise forecasts rather than just the broader categorical predictions used in prior work. We built the models by combining three publicly available datasets from California Department of Forestry and Fire Protection's Fire and Resource Assessment Program (FRAP). This study compared the performance of baseline ensemble regressor, Random Forest and XGBoost, with a Long Short-Term Memory (LSTM) neural network. The results show that the XGBoost model slightly outperforms the Random Forest model, likely due to its superior handling of static features in the dataset. The LSTM model, on the other hand, performed worse than the ensemble models because the dataset lacked temporal features. Overall, this study shows that, depending on the feature availability, Wildfire managers or Fire management authorities can select the most appropriate model to accurately predict wildfire containment duration and allocate resources effectively.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Bandits: Bringing statistical validity and reward efficiency to the small-gap regime</title>
<link>https://arxiv.org/abs/2512.09850</link>
<guid>https://arxiv.org/abs/2512.09850</guid>
<content:encoded><![CDATA[
arXiv:2512.09850v1 Announce Type: new 
Abstract: We introduce Conformal Bandits, a novel framework integrating Conformal Prediction (CP) into bandit problems, a classic paradigm for sequential decision-making under uncertainty. Traditional regret-minimisation bandit strategies like Thompson Sampling and Upper Confidence Bound (UCB) typically rely on distributional assumptions or asymptotic guarantees; further, they remain largely focused on regret, neglecting their statistical properties. We address this gap. Through the adoption of CP, we bridge the regret-minimising potential of a decision-making bandit policy with statistical guarantees in the form of finite-time prediction coverage.
  We demonstrate the potential of it Conformal Bandits through simulation studies and an application to portfolio allocation, a typical small-gap regime, where differences in arm rewards are far too small for classical policies to achieve optimal regret bounds in finite sample. Motivated by this, we showcase our framework's practical advantage in terms of regret in small-gap settings, as well as its added value in achieving nominal coverage guarantees where classical UCB policies fail. Focusing on our application of interest, we further illustrate how integrating hidden Markov models to capture the regime-switching behaviour of financial markets, enhances the exploration-exploitation trade-off, and translates into higher risk-adjusted regret efficiency returns, while preserving coverage guarantees.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPM-KD: Hierarchical Progressive Multi-Teacher Framework for Knowledge Distillation and Efficient Model Compression</title>
<link>https://arxiv.org/abs/2512.09886</link>
<guid>https://arxiv.org/abs/2512.09886</guid>
<content:encoded><![CDATA[
arXiv:2512.09886v1 Announce Type: new 
Abstract: Knowledge Distillation (KD) has emerged as a promising technique for model compression but faces critical limitations: (1) sensitivity to hyperparameters requiring extensive manual tuning, (2) capacity gap when distilling from very large teachers to small students, (3) suboptimal coordination in multi-teacher scenarios, and (4) inefficient use of computational resources. We present \textbf{HPM-KD}, a framework that integrates six synergistic components: (i) Adaptive Configuration Manager via meta-learning that eliminates manual hyperparameter tuning, (ii) Progressive Distillation Chain with automatically determined intermediate models, (iii) Attention-Weighted Multi-Teacher Ensemble that learns dynamic per-sample weights, (iv) Meta-Learned Temperature Scheduler that adapts temperature throughout training, (v) Parallel Processing Pipeline with intelligent load balancing, and (vi) Shared Optimization Memory for cross-experiment reuse. Experiments on CIFAR-10, CIFAR-100, and tabular datasets demonstrate that HPM-KD: achieves 10x-15x compression while maintaining 85% accuracy retention, eliminates the need for manual tuning, and reduces training time by 30-40% via parallelization. Ablation studies confirm independent contribution of each component (0.10-0.98 pp). HPM-KD is available as part of the open-source DeepBridge library.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Analysis of Dirichlet Energies as Over-smoothing Measures</title>
<link>https://arxiv.org/abs/2512.09890</link>
<guid>https://arxiv.org/abs/2512.09890</guid>
<content:encoded><![CDATA[
arXiv:2512.09890v1 Announce Type: new 
Abstract: We analyze the distinctions between two functionals often used as over-smoothing measures: the Dirichlet energies induced by the unnormalized graph Laplacian and the normalized graph Laplacian. We demonstrate that the latter fails to satisfy the axiomatic definition of a node-similarity measure proposed by Rusch \textit{et al.} By formalizing fundamental spectral properties of these two definitions, we highlight critical distinctions necessary to select the metric that is spectrally compatible with the GNN architecture, thereby resolving ambiguities in monitoring the dynamics.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Learning from Modern Language Models via Low Logit Rank</title>
<link>https://arxiv.org/abs/2512.09892</link>
<guid>https://arxiv.org/abs/2512.09892</guid>
<content:encoded><![CDATA[
arXiv:2512.09892v1 Announce Type: new 
Abstract: While modern language models and their inner workings are incredibly complex, recent work (Golowich, Liu & Shetty; 2025) has proposed a simple and potentially tractable abstraction for them through the observation that empirically, these language models all seem to have approximately low logit rank. Roughly, this means that a matrix formed by the model's log probabilities of various tokens conditioned on certain sequences of tokens is well approximated by a low rank matrix.
  In this paper, our focus is on understanding how this structure can be exploited algorithmically for obtaining provable learning guarantees. Since low logit rank models can encode hard-to-learn distributions such as noisy parities, we study a query learning model with logit queries that reflects the access model for common APIs. Our main result is an efficient algorithm for learning any approximately low logit rank model from queries. We emphasize that our structural assumption closely reflects the behavior that is empirically observed in modern language models. Thus, our result gives what we believe is the first end-to-end learning guarantee for a generative model that plausibly captures modern language models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Protein Language Model Architecture-Induced Biases for Antibody Comprehension</title>
<link>https://arxiv.org/abs/2512.09894</link>
<guid>https://arxiv.org/abs/2512.09894</guid>
<content:encoded><![CDATA[
arXiv:2512.09894v1 Announce Type: new 
Abstract: Recent advances in protein language models (PLMs) have demonstrated remarkable capabilities in understanding protein sequences. However, the extent to which different model architectures capture antibody-specific biological properties remains unexplored. In this work, we systematically investigate how architectural choices in PLMs influence their ability to comprehend antibody sequence characteristics and functions. We evaluate three state-of-the-art PLMs-AntiBERTa, BioBERT, and ESM2--against a general-purpose language model (GPT-2) baseline on antibody target specificity prediction tasks. Our results demonstrate that while all PLMs achieve high classification accuracy, they exhibit distinct biases in capturing biological features such as V gene usage, somatic hypermutation patterns, and isotype information. Through attention attribution analysis, we show that antibody-specific models like AntiBERTa naturally learn to focus on complementarity-determining regions (CDRs), while general protein models benefit significantly from explicit CDR-focused training strategies. These findings provide insights into the relationship between model architecture and biological feature extraction, offering valuable guidance for future PLM development in computational antibody design.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STACHE: Local Black-Box Explanations for Reinforcement Learning Policies</title>
<link>https://arxiv.org/abs/2512.09909</link>
<guid>https://arxiv.org/abs/2512.09909</guid>
<content:encoded><![CDATA[
arXiv:2512.09909v1 Announce Type: new 
Abstract: Reinforcement learning agents often behave unexpectedly in sparse-reward or safety-critical environments, creating a strong need for reliable debugging and verification tools. In this paper, we propose STACHE, a comprehensive framework for generating local, black-box explanations for an agent's specific action within discrete Markov games. Our method produces a Composite Explanation consisting of two complementary components: (1) a Robustness Region, the connected neighborhood of states where the agent's action remains invariant, and (2) Minimal Counterfactuals, the smallest state perturbations required to alter that decision. By exploiting the structure of factored state spaces, we introduce an exact, search-based algorithm that circumvents the fidelity gaps of surrogate models. Empirical validation on Gymnasium environments demonstrates that our framework not only explains policy actions, but also effectively captures the evolution of policy logic during training - from erratic, unstable behavior to optimized, robust strategies - providing actionable insights into agent sensitivity and decision boundaries.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FALCON: Few-step Accurate Likelihoods for Continuous Flows</title>
<link>https://arxiv.org/abs/2512.09914</link>
<guid>https://arxiv.org/abs/2512.09914</guid>
<content:encoded><![CDATA[
arXiv:2512.09914v1 Announce Type: new 
Abstract: Scalable sampling of molecular states in thermodynamic equilibrium is a long-standing challenge in statistical physics. Boltzmann Generators tackle this problem by pairing a generative model, capable of exact likelihood computation, with importance sampling to obtain consistent samples under the target distribution. Current Boltzmann Generators primarily use continuous normalizing flows (CNFs) trained with flow matching for efficient training of powerful models. However, likelihood calculation for these models is extremely costly, requiring thousands of function evaluations per sample, severely limiting their adoption. In this work, we propose Few-step Accurate Likelihoods for Continuous Flows (FALCON), a method which allows for few-step sampling with a likelihood accurate enough for importance sampling applications by introducing a hybrid training objective that encourages invertibility. We show FALCON outperforms state-of-the-art normalizing flow models for molecular Boltzmann sampling and is two orders of magnitude faster than the equivalently performing CNF model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the Train-Test Gap in World Models for Gradient-Based Planning</title>
<link>https://arxiv.org/abs/2512.09929</link>
<guid>https://arxiv.org/abs/2512.09929</guid>
<content:encoded><![CDATA[
arXiv:2512.09929v1 Announce Type: new 
Abstract: World models paired with model predictive control (MPC) can be trained offline on large-scale datasets of expert trajectories and enable generalization to a wide range of planning tasks at inference time. Compared to traditional MPC procedures, which rely on slow search algorithms or on iteratively solving optimization problems exactly, gradient-based planning offers a computationally efficient alternative. However, the performance of gradient-based planning has thus far lagged behind that of other approaches. In this paper, we propose improved methods for training world models that enable efficient gradient-based planning. We begin with the observation that although a world model is trained on a next-state prediction objective, it is used at test-time to instead estimate a sequence of actions. The goal of our work is to close this train-test gap. To that end, we propose train-time data synthesis techniques that enable significantly improved gradient-based planning with existing world models. At test time, our approach outperforms or matches the classical gradient-free cross-entropy method (CEM) across a variety of object manipulation and navigation tasks in 10% of the time budget.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controlling Steering Angle for Cooperative Self-driving Vehicles utilizing CNN and LSTM-based Deep Networks</title>
<link>https://arxiv.org/abs/1904.04375</link>
<guid>https://arxiv.org/abs/1904.04375</guid>
<content:encoded><![CDATA[
arXiv:1904.04375v3 Announce Type: cross 
Abstract: A fundamental challenge in autonomous vehicles is adjusting the steering angle at different road conditions. Recent state-of-the-art solutions addressing this challenge include deep learning techniques as they provide end-to-end solution to predict steering angles directly from the raw input images with higher accuracy. Most of these works ignore the temporal dependencies between the image frames. In this paper, we tackle the problem of utilizing multiple sets of images shared between two autonomous vehicles to improve the accuracy of controlling the steering angle by considering the temporal dependencies between the image frames. This problem has not been studied in the literature widely. We present and study a new deep architecture to predict the steering angle automatically by using Long-Short-Term-Memory (LSTM) in our deep architecture. Our deep architecture is an end-to-end network that utilizes CNN, LSTM and fully connected (FC) layers and it uses both present and futures images (shared by a vehicle ahead via Vehicle-to-Vehicle (V2V) communication) as input to control the steering angle. Our model demonstrates the lowest error when compared to the other existing approaches in the literature.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness and Adaptability of Reinforcement Learning based Cooperative Autonomous Driving in Mixed-autonomy Traffic</title>
<link>https://arxiv.org/abs/2202.00881</link>
<guid>https://arxiv.org/abs/2202.00881</guid>
<content:encoded><![CDATA[
arXiv:2202.00881v1 Announce Type: cross 
Abstract: Building autonomous vehicles (AVs) is a complex problem, but enabling them to operate in the real world where they will be surrounded by human-driven vehicles (HVs) is extremely challenging. Prior works have shown the possibilities of creating inter-agent cooperation between a group of AVs that follow a social utility. Such altruistic AVs can form alliances and affect the behavior of HVs to achieve socially desirable outcomes. We identify two major challenges in the co-existence of AVs and HVs. First, social preferences and individual traits of a given human driver, e.g., selflessness and aggressiveness are unknown to an AV, and it is almost impossible to infer them in real-time during a short AV-HV interaction. Second, contrary to AVs that are expected to follow a policy, HVs do not necessarily follow a stationary policy and therefore are extremely hard to predict. To alleviate the above-mentioned challenges, we formulate the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose a decentralized framework and reward function for training cooperative AVs. Our approach enables AVs to learn the decision-making of HVs implicitly from experience, optimizes for a social utility while prioritizing safety and allowing adaptability; robustifying altruistic AVs to different human behaviors and constraining them to a safe action space. Finally, we investigate the robustness, safety and sensitivity of AVs to various HVs behavioral traits and present the settings in which the AVs can learn cooperative policies that are adaptable to different situations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-based social coordination to improve safety and robustness of cooperative autonomous vehicles in mixed traffic</title>
<link>https://arxiv.org/abs/2211.11963</link>
<guid>https://arxiv.org/abs/2211.11963</guid>
<content:encoded><![CDATA[
arXiv:2211.11963v1 Announce Type: cross 
Abstract: It is expected that autonomous vehicles(AVs) and heterogeneous human-driven vehicles(HVs) will coexist on the same road. The safety and reliability of AVs will depend on their social awareness and their ability to engage in complex social interactions in a socially accepted manner. However, AVs are still inefficient in terms of cooperating with HVs and struggle to understand and adapt to human behavior, which is particularly challenging in mixed autonomy. In a road shared by AVs and HVs, the social preferences or individual traits of HVs are unknown to the AVs and different from AVs, which are expected to follow a policy, HVs are particularly difficult to forecast since they do not necessarily follow a stationary policy. To address these challenges, we frame the mixed-autonomy problem as a multi-agent reinforcement learning (MARL) problem and propose an approach that allows AVs to learn the decision-making of HVs implicitly from experience, account for all vehicles' interests, and safely adapt to other traffic situations. In contrast with existing works, we quantify AVs' social preferences and propose a distributed reward structure that introduces altruism into their decision-making process, allowing the altruistic AVs to learn to establish coalitions and influence the behavior of HVs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Inference of Constrained Optimization: Primal-Dual Optimality and Sequential Quadratic Programming</title>
<link>https://arxiv.org/abs/2512.08948</link>
<guid>https://arxiv.org/abs/2512.08948</guid>
<content:encoded><![CDATA[
arXiv:2512.08948v1 Announce Type: cross 
Abstract: We study online statistical inference for the solutions of stochastic optimization problems with equality and inequality constraints. Such problems are prevalent in statistics and machine learning, encompassing constrained $M$-estimation, physics-informed models, safe reinforcement learning, and algorithmic fairness. We develop a stochastic sequential quadratic programming (SSQP) method to solve these problems, where the step direction is computed by sequentially performing a quadratic approximation of the objective and a linear approximation of the constraints. Despite having access to unbiased estimates of population gradients, a key challenge in constrained stochastic problems lies in dealing with the bias in the step direction. As such, we apply a momentum-style gradient moving-average technique within SSQP to debias the step. We show that our method achieves global almost-sure convergence and exhibits local asymptotic normality with an optimal primal-dual limiting covariance matrix in the sense of H\'ajek and Le Cam. In addition, we provide a plug-in covariance matrix estimator for practical inference. To our knowledge, the proposed SSQP method is the first fully online method that attains primal-dual asymptotic minimax optimality without relying on projection operators onto the constraint set, which are generally intractable for nonlinear problems. Through extensive experiments on benchmark nonlinear problems, as well as on constrained generalized linear models and portfolio allocation problems using both synthetic and real data, we demonstrate superior performance of our method, showing that the method and its asymptotic behavior not only solve constrained stochastic problems efficiently but also provide valid and practical online inference in real-world applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multivariate time series prediction using clustered echo state network</title>
<link>https://arxiv.org/abs/2512.08963</link>
<guid>https://arxiv.org/abs/2512.08963</guid>
<content:encoded><![CDATA[
arXiv:2512.08963v1 Announce Type: cross 
Abstract: Many natural and physical processes can be understood by analyzing multiple system variables evolving, forming a multivariate time series. Predicting such time series is challenging due to the inherent noise and interdependencies among variables. Echo state networks (ESNs), a class of Reservoir Computing (RC) models, offer an efficient alternative to conventional recurrent neural networks by training only the output weights while keeping the reservoir dynamics fixed, reducing computational complexity. We propose a clustered ESNs (CESNs) that enhances the ability to model and predict multivariate time series by organizing the reservoir nodes into clusters, each corresponding to a distinct input variable. Input signals are directly mapped to their associated clusters, and intra-cluster connections remain dense while inter-cluster connections are sparse, mimicking the modular architecture of biological neural networks. This architecture improves information processing by limiting cross-variable interference and enhances computational efficiency through independent cluster-wise training via ridge regression. We further explore different reservoir topologies, including ring, Erd\H{o}s-R\'enyi (ER), and scale-free (SF) networks, to evaluate their impact predictive performance. Our algorithm works well across diverse real-world datasets such as the stock market, solar wind, and chaotic R\"ossler system, demonstrating that CESNs consistently outperform conventional ESNs in terms of predictive accuracy and robustness to noise, particularly when using ER and SF topologies. These findings highlight the adaptability of CESNs for complex, multivariate time series forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Automatic Speech Recognition Through Integrated Noise Detection Architecture</title>
<link>https://arxiv.org/abs/2512.08973</link>
<guid>https://arxiv.org/abs/2512.08973</guid>
<content:encoded><![CDATA[
arXiv:2512.08973v1 Announce Type: cross 
Abstract: This research presents a novel approach to enhancing automatic speech recognition systems by integrating noise detection capabilities directly into the recognition architecture. Building upon the wav2vec2 framework, the proposed method incorporates a dedicated noise identification module that operates concurrently with speech transcription. Experimental validation using publicly available speech and environmental audio datasets demonstrates substantial improvements in transcription quality and noise discrimination. The enhanced system achieves superior performance in word error rate, character error rate, and noise detection accuracy compared to conventional architectures. Results indicate that joint optimization of transcription and noise classification objectives yields more reliable speech recognition in challenging acoustic conditions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FuXi-Nowcast: Meet the longstanding challenge of convective initiation in nowcasting</title>
<link>https://arxiv.org/abs/2512.08974</link>
<guid>https://arxiv.org/abs/2512.08974</guid>
<content:encoded><![CDATA[
arXiv:2512.08974v1 Announce Type: cross 
Abstract: Accurate nowcasting of convective storms remains a major challenge for operational forecasting, particularly for convective initiation and the evolution of high-impact rainfall and strong winds. Here we present FuXi-Nowcast, a deep-learning system that jointly predicts composite radar reflectivity, surface precipitation, near-surface temperature, wind speed and wind gusts at 1-km resolution over eastern China. FuXi-Nowcast integrates multi-source observations, such as radar, surface stations and the High-Resolution Land Data Assimilation System (HRLDAS), with three-dimensional atmospheric fields from the machine-learning weather model FuXi-2.0 within a multi-task Swin-Transformer architecture. A convective signal enhancement module and distribution-aware hybrid loss functions are designed to preserve intense convective structures and mitigate the rapid intensity decay common in deep-learning nowcasts. FuXi-Nowcast surpasses the operational CMA-MESO 3-km numerical model in Critical Success Index for reflectivity, precipitation and wind gusts across thresholds and lead times up to 12 h, with the largest gains for heavy rainfall. Case studies further show that FuXi-Nowcast more accurately captures the timing, location and structure of convective initiation and subsequent evolution of convection. These results demonstrate that coupling three-dimensional machine-learning forecasts with high-resolution observations can provide multi-hazard, long-lead nowcasts that outperforms current operational systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deterministic World Models for Verification of Closed-loop Vision-based Systems</title>
<link>https://arxiv.org/abs/2512.08991</link>
<guid>https://arxiv.org/abs/2512.08991</guid>
<content:encoded><![CDATA[
arXiv:2512.08991v1 Announce Type: cross 
Abstract: Verifying closed-loop vision-based control systems remains a fundamental challenge due to the high dimensionality of images and the difficulty of modeling visual environments. While generative models are increasingly used as camera surrogates in verification, their reliance on stochastic latent variables introduces unnecessary overapproximation error. To address this bottleneck, we propose a Deterministic World Model (DWM) that maps system states directly to generative images, effectively eliminating uninterpretable latent variables to ensure precise input bounds. The DWM is trained with a dual-objective loss function that combines pixel-level reconstruction accuracy with a control difference loss to maintain behavioral consistency with the real system. We integrate DWM into a verification pipeline utilizing Star-based reachability analysis (StarV) and employ conformal prediction to derive rigorous statistical bounds on the trajectory deviation between the world model and the actual vision-based system. Experiments on standard benchmarks show that our approach yields significantly tighter reachable sets and better verification performance than a latent-variable baseline.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Chest Disease Classification Using an Improved CheXNet Framework with EfficientNetV2-M and Optimization-Driven Learning</title>
<link>https://arxiv.org/abs/2512.08992</link>
<guid>https://arxiv.org/abs/2512.08992</guid>
<content:encoded><![CDATA[
arXiv:2512.08992v1 Announce Type: cross 
Abstract: The interpretation of Chest X-ray is an important diagnostic issue in clinical practice and especially in the resource-limited setting where the shortage of radiologists plays a role in delayed diagnosis and poor patient outcomes. Although the original CheXNet architecture has shown potential in automated analysis of chest radiographs, DenseNet-121 backbone is computationally inefficient and poorly single-label classifier. To eliminate such shortcomings, we suggest a better classification framework of chest disease that relies on EfficientNetV2-M and incorporates superior training approaches such as Automatic Mixed Precision training, AdamW, Cosine Annealing learning rate scheduling, and Exponential Moving Average regularization. We prepared a dataset of 18,080 chest X-ray images of three source materials of high authority and representing five key clinically significant disease categories which included Cardiomegaly, COVID-19, Normal, Pneumonia, and Tuberculosis. To achieve statistical reliability and reproducibility, nine independent experimental runs were run. The suggested architecture showed significant gains with mean test accuracy of 96.45 percent compared to 95.30 percent at baseline (p less than 0.001) and macro-averaged F1-score increased to 91.08 percent (p less than 0.001). Critical infectious diseases showed near-perfect classification performance with COVID-19 detection having 99.95 percent accuracy and Tuberculosis detection having 99.97 percent accuracy. Although 6.8 times more parameters are included, the training time was reduced by 11.4 percent and performance stability was increased by 22.7 percent. This framework presents itself as a decision-support tool that can be used to respond to a pandemic, screen tuberculosis, and assess thoracic disease regularly in various healthcare facilities.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demo: Generative AI helps Radiotherapy Planning with User Preference</title>
<link>https://arxiv.org/abs/2512.08996</link>
<guid>https://arxiv.org/abs/2512.08996</guid>
<content:encoded><![CDATA[
arXiv:2512.08996v1 Announce Type: cross 
Abstract: Radiotherapy planning is a highly complex process that often varies significantly across institutions and individual planners. Most existing deep learning approaches for 3D dose prediction rely on reference plans as ground truth during training, which can inadvertently bias models toward specific planning styles or institutional preferences. In this study, we introduce a novel generative model that predicts 3D dose distributions based solely on user-defined preference flavors. These customizable preferences enable planners to prioritize specific trade-offs between organs-at-risk (OARs) and planning target volumes (PTVs), offering greater flexibility and personalization. Designed for seamless integration with clinical treatment planning systems, our approach assists users in generating high-quality plans efficiently. Comparative evaluations demonstrate that our method can surpasses the Varian RapidPlan model in both adaptability and plan quality in some scenarios.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Luxical: High-Speed Lexical-Dense Text Embeddings</title>
<link>https://arxiv.org/abs/2512.09015</link>
<guid>https://arxiv.org/abs/2512.09015</guid>
<content:encoded><![CDATA[
arXiv:2512.09015v1 Announce Type: cross 
Abstract: Frontier language model quality increasingly hinges on our ability to organize web-scale text corpora for training. Today's dominant tools trade off speed and flexibility: lexical classifiers (e.g., FastText) are fast but limited to producing classification output scores, while the vector-valued outputs of transformer text embedding models flexibly support numerous workflows (e.g., clustering, classification, and retrieval) but are computationally expensive to produce. We introduce Luxical, a library for high-speed "lexical-dense" text embeddings that aims to recover the best properties of both approaches for web-scale text organization. Luxical combines sparse TF--IDF features, a small ReLU network, and a knowledge distillation training regimen to approximate large transformer embedding models at a fraction of their operational cost. In this technical report, we describe the Luxical architecture and training objective and evaluate a concrete Luxical model in two disparate applications: a targeted webcrawl document retrieval test and an end-to-end language model data curation task grounded in text classification. In these tasks we demonstrate speedups ranging from 3x to 100x over varying-sized neural baselines, and comparable to FastText model inference during the data curation task. On these evaluations, the tested Luxical model illustrates favorable compute/quality trade-offs for large-scale text organization, matching the quality of neural baselines. Luxical is available as open-source software at https://github.com/datologyai/luxical.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable machine learning of halo gas density profiles: a sensitivity analysis of cosmological hydrodynamical simulations</title>
<link>https://arxiv.org/abs/2512.09021</link>
<guid>https://arxiv.org/abs/2512.09021</guid>
<content:encoded><![CDATA[
arXiv:2512.09021v1 Announce Type: cross 
Abstract: Stellar and AGN-driven feedback processes affect the distribution of gas on a wide range of scales, from within galaxies well into the intergalactic medium. Yet, it remains unclear how feedback, through its connection to key galaxy properties, shapes the radial gas density profile in the host halo. We tackle this question using suites of the EAGLE, IllustrisTNG, and Simba cosmological hydrodynamical simulations, which span a variety of feedback models. We develop a random forest algorithm that predicts the radial gas density profile within haloes from the total halo mass and five global properties of the central galaxy: gas and stellar mass; star formation rate; mass and accretion rate of the central black hole (BH). The algorithm reproduces the simulated gas density profiles with an average accuracy of $\sim$80-90% over the halo mass range $10^{9.5} \, \mathrm{M}_{\odot} < M_{\rm 200c} < 10^{15} \, \mathrm{M}_{\odot}$ and redshift interval $0<4$. For the first time, we apply Sobol statistical sensitivity analysis to full cosmological hydrodynamical simulations, quantifying how each feature affects the gas density as a function of distance from the halo centre. Across all simulations and redshifts, the total halo mass and the gas mass of the central galaxy are the most strongly tied to the halo gas distribution, while stellar and BH properties are generally less informative. The exact relative importance of the different features depends on the feedback scenario and redshift. Our framework can be readily embedded in semi-analytic models of galaxy formation to incorporate halo gas density profiles consistent with different hydrodynamical simulations. Our work also provides a proof of concept for constraining feedback models with future observations of galaxy properties and of the surrounding gas distribution.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIP: Site in Pieces- A Dataset of Disaggregated Construction-Phase 3D Scans for Semantic Segmentation and Scene Understanding</title>
<link>https://arxiv.org/abs/2512.09062</link>
<guid>https://arxiv.org/abs/2512.09062</guid>
<content:encoded><![CDATA[
arXiv:2512.09062v1 Announce Type: cross 
Abstract: Accurate 3D scene interpretation in active construction sites is essential for progress monitoring, safety assessment, and digital twin development. LiDAR is widely used in construction because it offers advantages over camera-based systems, performing reliably in cluttered and dynamically changing conditions. Yet most public datasets for 3D perception are derived from densely fused scans with uniform sampling and complete visibility, conditions that do not reflect real construction sites. Field data are often collected as isolated single-station LiDAR views, constrained by safety requirements, limited access, and ongoing operations. These factors lead to radial density decay, fragmented geometry, and view-dependent visibility-characteristics that remain underrepresented in existing datasets. This paper presents SIP, Site in Pieces, a dataset created to reflect the practical constraints of LiDAR acquisition during construction. SIP provides indoor and outdoor scenes captured with a terrestrial LiDAR scanner and annotated at the point level using a taxonomy tailored to construction environments: A. Built Environment, B. Construction Operations, and C. Site Surroundings. The dataset includes both structural components and slender temporary objects such as scaffolding, MEP piping, and scissor lifts, where sparsity caused by occlusion and fragmented geometry make segmentation particularly challenging. The scanning protocol, annotation workflow, and quality control procedures establish a consistent foundation for the dataset. SIP is openly available with a supporting Git repository, offering adaptable class configurations that streamline adoption within modern 3D deep learning frameworks. By providing field data that retain real-world sensing characteristics, SIP enables robust benchmarking and contributes to advancing construction-oriented 3D vision tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KD-OCT: Efficient Knowledge Distillation for Clinical-Grade Retinal OCT Classification</title>
<link>https://arxiv.org/abs/2512.09069</link>
<guid>https://arxiv.org/abs/2512.09069</guid>
<content:encoded><![CDATA[
arXiv:2512.09069v1 Announce Type: cross 
Abstract: Age-related macular degeneration (AMD) and choroidal neovascularization (CNV)-related conditions are leading causes of vision loss worldwide, with optical coherence tomography (OCT) serving as a cornerstone for early detection and management. However, deploying state-of-the-art deep learning models like ConvNeXtV2-Large in clinical settings is hindered by their computational demands. Therefore, it is desirable to develop efficient models that maintain high diagnostic performance while enabling real-time deployment. In this study, a novel knowledge distillation framework, termed KD-OCT, is proposed to compress a high-performance ConvNeXtV2-Large teacher model, enhanced with advanced augmentations, stochastic weight averaging, and focal loss, into a lightweight EfficientNet-B2 student for classifying normal, drusen, and CNV cases. KD-OCT employs real-time distillation with a combined loss balancing soft teacher knowledge transfer and hard ground-truth supervision. The effectiveness of the proposed method is evaluated on the Noor Eye Hospital (NEH) dataset using patient-level cross-validation. Experimental results demonstrate that KD-OCT outperforms comparable multi-scale or feature-fusion OCT classifiers in efficiency- accuracy balance, achieving near-teacher performance with substantial reductions in model size and inference time. Despite the compression, the student model exceeds most existing frameworks, facilitating edge deployment for AMD screening. Code is available at https://github.com/erfan-nourbakhsh/KD- OCT.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Banach neural operator for Navier-Stokes equations</title>
<link>https://arxiv.org/abs/2512.09070</link>
<guid>https://arxiv.org/abs/2512.09070</guid>
<content:encoded><![CDATA[
arXiv:2512.09070v1 Announce Type: cross 
Abstract: Classical neural networks are known for their ability to approximate mappings between finite-dimensional spaces, but they fall short in capturing complex operator dynamics across infinite-dimensional function spaces. Neural operators, in contrast, have emerged as powerful tools in scientific machine learning for learning such mappings. However, standard neural operators typically lack mechanisms for mixing or attending to input information across space and time. In this work, we introduce the Banach neural operator (BNO) -- a novel framework that integrates Koopman operator theory with deep neural networks to predict nonlinear, spatiotemporal dynamics from partial observations. The BNO approximates a nonlinear operator between Banach spaces by combining spectral linearization (via Koopman theory) with deep feature learning (via convolutional neural networks and nonlinear activations). This sequence-to-sequence model captures dominant dynamic modes and allows for mesh-independent prediction. Numerical experiments on the Navier-Stokes equations demonstrate the method's accuracy and generalization capabilities. In particular, BNO achieves robust zero-shot super-resolution in unsteady flow prediction and consistently outperforms conventional Koopman-based methods and deep learning models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Attribution of Model Performance Gaps in Medical Imaging Under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.09094</link>
<guid>https://arxiv.org/abs/2512.09094</guid>
<content:encoded><![CDATA[
arXiv:2512.09094v1 Announce Type: cross 
Abstract: Deep learning models for medical image segmentation suffer significant performance drops due to distribution shifts, but the causal mechanisms behind these drops remain poorly understood. We extend causal attribution frameworks to high-dimensional segmentation tasks, quantifying how acquisition protocols and annotation variability independently contribute to performance degradation. We model the data-generating process through a causal graph and employ Shapley values to fairly attribute performance changes to individual mechanisms. Our framework addresses unique challenges in medical imaging: high-dimensional outputs, limited samples, and complex mechanism interactions. Validation on multiple sclerosis (MS) lesion segmentation across 4 centers and 7 annotators reveals context-dependent failure modes: annotation protocol shifts dominate when crossing annotators (7.4% $\pm$ 8.9% DSC attribution), while acquisition shifts dominate when crossing imaging centers (6.5% $\pm$ 9.1%). This mechanism-specific quantification enables practitioners to prioritize targeted interventions based on deployment context.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding temperature tuning in energy-based models</title>
<link>https://arxiv.org/abs/2512.09152</link>
<guid>https://arxiv.org/abs/2512.09152</guid>
<content:encoded><![CDATA[
arXiv:2512.09152v1 Announce Type: cross 
Abstract: Generative models of complex systems often require post-hoc parameter adjustments to produce useful outputs. For example, energy-based models for protein design are sampled at an artificially low ''temperature'' to generate novel, functional sequences. This temperature tuning is a common yet poorly understood heuristic used across machine learning contexts to control the trade-off between generative fidelity and diversity. Here, we develop an interpretable, physically motivated framework to explain this phenomenon. We demonstrate that in systems with a large ''energy gap'' - separating a small fraction of meaningful states from a vast space of unrealistic states - learning from sparse data causes models to systematically overestimate high-energy state probabilities, a bias that lowering the sampling temperature corrects. More generally, we characterize how the optimal sampling temperature depends on the interplay between data size and the system's underlying energy landscape. Crucially, our results show that lowering the sampling temperature is not always desirable; we identify the conditions where \emph{raising} it results in better generative performance. Our framework thus casts post-hoc temperature tuning as a diagnostic tool that reveals properties of the true data distribution and the limits of the learned model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WTNN: Weibull-Tailored Neural Networks for survival analysis</title>
<link>https://arxiv.org/abs/2512.09163</link>
<guid>https://arxiv.org/abs/2512.09163</guid>
<content:encoded><![CDATA[
arXiv:2512.09163v1 Announce Type: cross 
Abstract: The Weibull distribution is a commonly adopted choice for modeling the survival of systems subject to maintenance over time. When only proxy indicators and censored observations are available, it becomes necessary to express the distribution's parameters as functions of time-dependent covariates. Deep neural networks provide the flexibility needed to learn complex relationships between these covariates and operational lifetime, thereby extending the capabilities of traditional regression-based models. Motivated by the analysis of a fleet of military vehicles operating in highly variable and demanding environments, as well as by the limitations observed in existing methodologies, this paper introduces WTNN, a new neural network-based modeling framework specifically designed for Weibull survival studies. The proposed architecture is specifically designed to incorporate qualitative prior knowledge regarding the most influential covariates, in a manner consistent with the shape and structure of the Weibull distribution. Through numerical experiments, we show that this approach can be reliably trained on proxy and right-censored data, and is capable of producing robust and interpretable survival predictions that can improve existing approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust and Sparse Estimation of Unbounded Density Ratio under Heavy Contamination</title>
<link>https://arxiv.org/abs/2512.09266</link>
<guid>https://arxiv.org/abs/2512.09266</guid>
<content:encoded><![CDATA[
arXiv:2512.09266v1 Announce Type: cross 
Abstract: We examine the non-asymptotic properties of robust density ratio estimation (DRE) in contaminated settings. Weighted DRE is the most promising among existing methods, exhibiting doubly strong robustness from an asymptotic perspective. This study demonstrates that Weighted DRE achieves sparse consistency even under heavy contamination within a non-asymptotic framework. This method addresses two significant challenges in density ratio estimation and robust estimation. For density ratio estimation, we provide the non-asymptotic properties of estimating unbounded density ratios under the assumption that the weighted density ratio function is bounded. For robust estimation, we introduce a non-asymptotic framework for doubly strong robustness under heavy contamination, assuming that at least one of the following conditions holds: (i) contamination ratios are small, and (ii) outliers have small weighted values. This work provides the first non-asymptotic analysis of strong robustness under heavy contamination.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impact of Positional Encoding: Clean and Adversarial Rademacher Complexity for Transformers under In-Context Regression</title>
<link>https://arxiv.org/abs/2512.09275</link>
<guid>https://arxiv.org/abs/2512.09275</guid>
<content:encoded><![CDATA[
arXiv:2512.09275v1 Announce Type: cross 
Abstract: Positional encoding (PE) is a core architectural component of Transformers, yet its impact on the Transformer's generalization and robustness remains unclear. In this work, we provide the first generalization analysis for a single-layer Transformer under in-context regression that explicitly accounts for a completely trainable PE module. Our result shows that PE systematically enlarges the generalization gap. Extending to the adversarial setting, we derive the adversarial Rademacher generalization bound. We find that the gap between models with and without PE is magnified under attack, demonstrating that PE amplifies the vulnerability of models. Our bounds are empirically validated by a simulation study. Together, this work establishes a new framework for understanding the clean and adversarial generalization in ICL with PE.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distributional Shrinkage II: Optimal Transport Denoisers with Higher-Order Scores</title>
<link>https://arxiv.org/abs/2512.09295</link>
<guid>https://arxiv.org/abs/2512.09295</guid>
<content:encoded><![CDATA[
arXiv:2512.09295v1 Announce Type: cross 
Abstract: We revisit the signal denoising problem through the lens of optimal transport: the goal is to recover an unknown scalar signal distribution $X \sim P$ from noisy observations $Y = X + \sigma Z$, with $Z$ being standard Gaussian independent of $X$ and $\sigma>0$ a known noise level. Let $Q$ denote the distribution of $Y$. We introduce a hierarchy of denoisers $T_0, T_1, \ldots, T_\infty : \mathbb{R} \to \mathbb{R}$ that are agnostic to the signal distribution $P$, depending only on higher-order score functions of $Q$. Each denoiser $T_K$ is progressively refined using the $(2K-1)$-th order score function of $Q$ at noise resolution $\sigma^{2K}$, achieving better denoising quality measured by the Wasserstein metric $W(T_K \sharp Q, P)$. The limiting denoiser $T_\infty$ identifies the optimal transport map with $T_\infty \sharp Q = P$.
  We provide a complete characterization of the combinatorial structure underlying this hierarchy through Bell polynomial recursions, revealing how higher-order score functions encode the optimal transport map for signal denoising. We study two estimation strategies with convergence rates for higher-order scores from i.i.d. samples drawn from $Q$: (i) plug-in estimation via Gaussian kernel smoothing, and (ii) direct estimation via higher-order score matching. This hierarchy of agnostic denoisers opens new perspectives in signal denoising and empirical Bayes.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Visual Categorization Across Minds and Models: Cognitive Analysis of Human Labeling and Neuro-Symbolic Integration</title>
<link>https://arxiv.org/abs/2512.09340</link>
<guid>https://arxiv.org/abs/2512.09340</guid>
<content:encoded><![CDATA[
arXiv:2512.09340v1 Announce Type: cross 
Abstract: Understanding how humans and AI systems interpret ambiguous visual stimuli offers critical insight into the nature of perception, reasoning, and decision-making. This paper examines image labeling performance across human participants and deep neural networks, focusing on low-resolution, perceptually degraded stimuli. Drawing from computational cognitive science, cognitive architectures, and connectionist-symbolic hybrid models, we contrast human strategies such as analogical reasoning, shape-based recognition, and confidence modulation with AI's feature-based processing. Grounded in Marr's tri-level hypothesis, Simon's bounded rationality, and Thagard's frameworks of representation and emotion, we analyze participant responses in relation to Grad-CAM visualizations of model attention. Human behavior is further interpreted through cognitive principles modeled in ACT-R and Soar, revealing layered and heuristic decision strategies under uncertainty. Our findings highlight key parallels and divergences between biological and artificial systems in representation, inference, and confidence calibration. The analysis motivates future neuro-symbolic architectures that unify structured symbolic reasoning with connectionist representations. Such architectures, informed by principles of embodiment, explainability, and cognitive alignment, offer a path toward AI systems that are not only performant but also interpretable and cognitively grounded.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-learning three-factor plasticity rules for structured credit assignment with sparse feedback</title>
<link>https://arxiv.org/abs/2512.09366</link>
<guid>https://arxiv.org/abs/2512.09366</guid>
<content:encoded><![CDATA[
arXiv:2512.09366v1 Announce Type: cross 
Abstract: Biological neural networks learn complex behaviors from sparse, delayed feedback using local synaptic plasticity, yet the mechanisms enabling structured credit assignment remain elusive. In contrast, artificial recurrent networks solving similar tasks typically rely on biologically implausible global learning rules or hand-crafted local updates. The space of local plasticity rules capable of supporting learning from delayed reinforcement remains largely unexplored. Here, we present a meta-learning framework that discovers local learning rules for structured credit assignment in recurrent networks trained with sparse feedback. Our approach interleaves local neo-Hebbian-like updates during task execution with an outer loop that optimizes plasticity parameters via \textbf{tangent-propagation through learning}. The resulting three-factor learning rules enable long-timescale credit assignment using only local information and delayed rewards, offering new insights into biologically grounded mechanisms for learning in recurrent circuits.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.09385</link>
<guid>https://arxiv.org/abs/2512.09385</guid>
<content:encoded><![CDATA[
arXiv:2512.09385v1 Announce Type: cross 
Abstract: The rapid growth of Ethereum has made it more important to quickly and accurately detect smart contract vulnerabilities. While machine-learning-based methods have shown some promise, many still rely on rule-based preprocessing designed by domain experts. Rule-based preprocessing methods often discard crucial context from the source code, potentially causing certain vulnerabilities to be overlooked and limiting adaptability to newly emerging threats. We introduce BugSweeper, an end-to-end deep learning framework that detects vulnerabilities directly from the source code without manual engineering. BugSweeper represents each Solidity function as a Function-Level Abstract Syntax Graph (FLAG), a novel graph that combines its Abstract Syntax Tree (AST) with enriched control-flow and data-flow semantics. Then, our two-stage Graph Neural Network (GNN) analyzes these graphs. The first-stage GNN filters noise from the syntax graphs, while the second-stage GNN conducts high-level reasoning to detect diverse vulnerabilities. Extensive experiments on real-world contracts show that BugSweeper significantly outperforms all state-of-the-art detection methods. By removing the need for handcrafted rules, our approach offers a robust, automated, and scalable solution for securing smart contracts without any dependence on security experts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CONCUR: A Framework for Continual Constrained and Unconstrained Routing</title>
<link>https://arxiv.org/abs/2512.09386</link>
<guid>https://arxiv.org/abs/2512.09386</guid>
<content:encoded><![CDATA[
arXiv:2512.09386v1 Announce Type: cross 
Abstract: AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection and Localization of Subdural Hematoma Using Deep Learning on Computed Tomography</title>
<link>https://arxiv.org/abs/2512.09393</link>
<guid>https://arxiv.org/abs/2512.09393</guid>
<content:encoded><![CDATA[
arXiv:2512.09393v1 Announce Type: cross 
Abstract: Background. Subdural hematoma (SDH) is a common neurosurgical emergency, with increasing incidence in aging populations. Rapid and accurate identification is essential to guide timely intervention, yet existing automated tools focus primarily on detection and provide limited interpretability or spatial localization. There remains a need for transparent, high-performing systems that integrate multimodal clinical and imaging information to support real-time decision-making.
  Methods. We developed a multimodal deep-learning framework that integrates structured clinical variables, a 3D convolutional neural network trained on CT volumes, and a transformer-enhanced 2D segmentation model for SDH detection and localization. Using 25,315 head CT studies from Hartford HealthCare (2015--2024), of which 3,774 (14.9\%) contained clinician-confirmed SDH, tabular models were trained on demographics, comorbidities, medications, and laboratory results. Imaging models were trained to detect SDH and generate voxel-level probability maps. A greedy ensemble strategy combined complementary predictors.
  Findings. Clinical variables alone provided modest discriminatory power (AUC 0.75). Convolutional models trained on CT volumes and segmentation-derived maps achieved substantially higher accuracy (AUCs 0.922 and 0.926). The multimodal ensemble integrating all components achieved the best overall performance (AUC 0.9407; 95\% CI, 0.930--0.951) and produced anatomically meaningful localization maps consistent with known SDH patterns.
  Interpretation. This multimodal, interpretable framework provides rapid and accurate SDH detection and localization, achieving high detection performance and offering transparent, anatomically grounded outputs. Integration into radiology workflows could streamline triage, reduce time to intervention, and improve consistency in SDH management.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizable Collaborative Search-and-Capture in Cluttered Environments via Path-Guided MAPPO and Directional Frontier Allocation</title>
<link>https://arxiv.org/abs/2512.09410</link>
<guid>https://arxiv.org/abs/2512.09410</guid>
<content:encoded><![CDATA[
arXiv:2512.09410v1 Announce Type: cross 
Abstract: Collaborative pursuit-evasion in cluttered environments presents significant challenges due to sparse rewards and constrained Fields of View (FOV). Standard Multi-Agent Reinforcement Learning (MARL) often suffers from inefficient exploration and fails to scale to large scenarios. We propose PGF-MAPPO (Path-Guided Frontier MAPPO), a hierarchical framework bridging topological planning with reactive control. To resolve local minima and sparse rewards, we integrate an A*-based potential field for dense reward shaping. Furthermore, we introduce Directional Frontier Allocation, combining Farthest Point Sampling (FPS) with geometric angle suppression to enforce spatial dispersion and accelerate coverage. The architecture employs a parameter-shared decentralized critic, maintaining O(1) model complexity suitable for robotic swarms. Experiments demonstrate that PGF-MAPPO achieves superior capture efficiency against faster evaders. Policies trained on 10x10 maps exhibit robust zero-shot generalization to unseen 20x20 environments, significantly outperforming rule-based and learning-based baselines.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architectures for Building Agentic AI</title>
<link>https://arxiv.org/abs/2512.09458</link>
<guid>https://arxiv.org/abs/2512.09458</guid>
<content:encoded><![CDATA[
arXiv:2512.09458v1 Announce Type: cross 
Abstract: This chapter argues that the reliability of agentic and generative AI is chiefly an architectural property. We define agentic systems as goal-directed, tool-using decision makers operating in closed loops, and show how reliability emerges from principled componentisation (goal manager, planner, tool-router, executor, memory, verifiers, safety monitor, telemetry), disciplined interfaces (schema-constrained, validated, least-privilege tool calls), and explicit control and assurance loops. Building on classical foundations, we propose a practical taxonomy-tool-using agents, memory-augmented agents, planning and self-improvement agents, multi-agent systems, and embodied or web agents - and analyse how each pattern reshapes the reliability envelope and failure modes. We distil design guidance on typed schemas, idempotency, permissioning, transactional semantics, memory provenance and hygiene, runtime governance (budgets, termination conditions), and simulate-before-actuate safeguards.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WarmServe: Enabling One-for-Many GPU Prewarming for Multi-LLM Serving</title>
<link>https://arxiv.org/abs/2512.09472</link>
<guid>https://arxiv.org/abs/2512.09472</guid>
<content:encoded><![CDATA[
arXiv:2512.09472v1 Announce Type: cross 
Abstract: Deploying multiple models within shared GPU clusters is promising for improving resource efficiency in large language model (LLM) serving. Existing multi-LLM serving systems optimize GPU utilization at the cost of worse inference performance, especially time-to-first-token (TTFT). We identify the root cause of such compromise as their unawareness of future workload characteristics. In contrast, recent analysis on real-world traces has shown the high periodicity and long-term predictability of LLM serving workloads.
  We propose universal GPU workers to enable one-for-many GPU prewarming that loads models with knowledge of future workloads. Based on universal GPU workers, we design and build WarmServe, a multi-LLM serving system that (1) mitigates cluster-wide prewarming interference by adopting an evict-aware model placement strategy, (2) prepares universal GPU workers in advance by proactive prewarming, and (3) manages GPU memory with a zero-overhead memory switching mechanism. Evaluation under real-world datasets shows that WarmServe improves TTFT by up to 50.8$\times$ compared to the state-of-the-art autoscaling-based system, while being capable of serving up to 2.5$\times$ more requests compared to the GPU-sharing system.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimation of Stochastic Optimal Transport Maps</title>
<link>https://arxiv.org/abs/2512.09499</link>
<guid>https://arxiv.org/abs/2512.09499</guid>
<content:encoded><![CDATA[
arXiv:2512.09499v1 Announce Type: cross 
Abstract: The optimal transport (OT) map is a geometry-driven transformation between high-dimensional probability distributions which underpins a wide range of tasks in statistics, applied probability, and machine learning. However, existing statistical theory for OT map estimation is quite restricted, hinging on Brenier's theorem (quadratic cost, absolutely continuous source) to guarantee existence and uniqueness of a deterministic OT map, on which various additional regularity assumptions are imposed to obtain quantitative error bounds. In many real-world problems these conditions fail or cannot be certified, in which case optimal transportation is possible only via stochastic maps that can split mass. To broaden the scope of map estimation theory to such settings, this work introduces a novel metric for evaluating the transportation quality of stochastic maps. Under this metric, we develop computationally efficient map estimators with near-optimal finite-sample risk bounds, subject to easy-to-verify minimal assumptions. Our analysis further accommodates common forms of adversarial sample contamination, yielding estimators with robust estimation guarantees. Empirical experiments are provided which validate our theory and demonstrate the utility of the proposed framework in settings where existing theory fails. These contributions constitute the first general-purpose theory for map estimation, compatible with a wide spectrum of real-world applications where optimal transport may be intrinsically stochastic.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transport Novelty Distance: A Distributional Metric for Evaluating Material Generative Models</title>
<link>https://arxiv.org/abs/2512.09514</link>
<guid>https://arxiv.org/abs/2512.09514</guid>
<content:encoded><![CDATA[
arXiv:2512.09514v1 Announce Type: cross 
Abstract: Recent advances in generative machine learning have opened new possibilities for the discovery and design of novel materials. However, as these models become more sophisticated, the need for rigorous and meaningful evaluation metrics has grown. Existing evaluation approaches often fail to capture both the quality and novelty of generated structures, limiting our ability to assess true generative performance. In this paper, we introduce the Transport Novelty Distance (TNovD) to judge generative models used for materials discovery jointly by the quality and novelty of the generated materials. Based on ideas from Optimal Transport theory, TNovD uses a coupling between the features of the training and generated sets, which is refined into a quality and memorization regime by a threshold. The features are generated from crystal structures using a graph neural network that is trained to distinguish between materials, their augmented counterparts, and differently sized supercells using contrastive learning. We evaluate our proposed metric on typical toy experiments relevant for crystal structure prediction, including memorization, noise injection and lattice deformations. Additionally, we validate the TNovD on the MP20 validation set and the WBM substitution dataset, demonstrating that it is capable of detecting both memorized and low-quality material data. We also benchmark the performance of several popular material generative models. While introduced for materials, our TNovD framework is domain-agnostic and can be adapted for other areas, such as images and molecules.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroSketch: An Effective Framework for Neural Decoding via Systematic Architectural Optimization</title>
<link>https://arxiv.org/abs/2512.09524</link>
<guid>https://arxiv.org/abs/2512.09524</guid>
<content:encoded><![CDATA[
arXiv:2512.09524v1 Announce Type: cross 
Abstract: Neural decoding, a critical component of Brain-Computer Interface (BCI), has recently attracted increasing research interest. Previous research has focused on leveraging signal processing and deep learning methods to enhance neural decoding performance. However, the in-depth exploration of model architectures remains underexplored, despite its proven effectiveness in other tasks such as energy forecasting and image classification. In this study, we propose NeuroSketch, an effective framework for neural decoding via systematic architecture optimization. Starting with the basic architecture study, we find that CNN-2D outperforms other architectures in neural decoding tasks and explore its effectiveness from temporal and spatial perspectives. Building on this, we optimize the architecture from macro- to micro-level, achieving improvements in performance at each step. The exploration process and model validations take over 5,000 experiments spanning three distinct modalities (visual, auditory, and speech), three types of brain signals (EEG, SEEG, and ECoG), and eight diverse decoding tasks. Experimental results indicate that NeuroSketch achieves state-of-the-art (SOTA) performance across all evaluated datasets, positioning it as a powerful tool for neural decoding. Our code and scripts are available at https://github.com/Galaxy-Dawn/NeuroSketch.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers for Tabular Data: A Training Perspective of Self-Attention via Optimal Transport</title>
<link>https://arxiv.org/abs/2512.09530</link>
<guid>https://arxiv.org/abs/2512.09530</guid>
<content:encoded><![CDATA[
arXiv:2512.09530v1 Announce Type: cross 
Abstract: This thesis examines self-attention training through the lens of Optimal Transport (OT) and develops an OT-based alternative for tabular classification. The study tracks intermediate projections of the self-attention layer during training and evaluates their evolution using discrete OT metrics, including Wasserstein distance, Monge gap, optimality, and efficiency. Experiments are conducted on classification tasks with two and three classes, as well as on a biomedical dataset.
  Results indicate that the final self-attention mapping often approximates the OT optimal coupling, yet the training trajectory remains inefficient. Pretraining the MLP section on synthetic data partially improves convergence but is sensitive to their initialization. To address these limitations, an OT-based algorithm is introduced: it generates class-specific dummy Gaussian distributions, computes an OT alignment with the data, and trains an MLP to generalize this mapping. The method achieves accuracy comparable to Transformers while reducing computational cost and scaling more efficiently under standardized inputs, though its performance depends on careful dummy-geometry design. All experiments and implementations are conducted in R.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search</title>
<link>https://arxiv.org/abs/2512.09538</link>
<guid>https://arxiv.org/abs/2512.09538</guid>
<content:encoded><![CDATA[
arXiv:2512.09538v1 Announce Type: cross 
Abstract: Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis of Hash-based Malware Clustering via K-Means</title>
<link>https://arxiv.org/abs/2512.09539</link>
<guid>https://arxiv.org/abs/2512.09539</guid>
<content:encoded><![CDATA[
arXiv:2512.09539v1 Announce Type: cross 
Abstract: With the adoption of multiple digital devices in everyday life, the cyber-attack surface has increased. Adversaries are continuously exploring new avenues to exploit them and deploy malware. On the other hand, detection approaches typically employ hashing-based algorithms such as SSDeep, TLSH, and IMPHash to capture structural and behavioural similarities among binaries. This work focuses on the analysis and evaluation of these techniques for clustering malware samples using the K-means algorithm. More specifically, we experimented with established malware families and traits and found that TLSH and IMPHash produce more distinct, semantically meaningful clusters, whereas SSDeep is more efficient for broader classification tasks. The findings of this work can guide the development of more robust threat-detection mechanisms and adaptive security mechanisms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Closed-loop Molecular Discovery via Language Model, Property Alignment and Strategic Search</title>
<link>https://arxiv.org/abs/2512.09566</link>
<guid>https://arxiv.org/abs/2512.09566</guid>
<content:encoded><![CDATA[
arXiv:2512.09566v1 Announce Type: cross 
Abstract: Drug discovery is a time-consuming and expensive process, with traditional high-throughput and docking-based virtual screening hampered by low success rates and limited scalability. Recent advances in generative modelling, including autoregressive, diffusion, and flow-based approaches, have enabled de novo ligand design beyond the limits of enumerative screening. Yet these models often suffer from inadequate generalization, limited interpretability, and an overemphasis on binding affinity at the expense of key pharmacological properties, thereby restricting their translational utility. Here we present Trio, a molecular generation framework integrating fragment-based molecular language modeling, reinforcement learning, and Monte Carlo tree search, for effective and interpretable closed-loop targeted molecular design. Through the three key components, Trio enables context-aware fragment assembly, enforces physicochemical and synthetic feasibility, and guides a balanced search between the exploration of novel chemotypes and the exploitation of promising intermediates within protein binding pockets. Experimental results show that Trio reliably achieves chemically valid and pharmacologically enhanced ligands, outperforming state-of-the-art approaches with improved binding affinity (+7.85%), drug-likeness (+11.10%) and synthetic accessibility (+12.05%), while expanding molecular diversity more than fourfold.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Based Bayesian Optimization for Quantum Circuit Architecture Search with Uncertainty Calibrated Surrogates</title>
<link>https://arxiv.org/abs/2512.09586</link>
<guid>https://arxiv.org/abs/2512.09586</guid>
<content:encoded><![CDATA[
arXiv:2512.09586v1 Announce Type: cross 
Abstract: Quantum circuit design is a key bottleneck for practical quantum machine learning on complex, real-world data. We present an automated framework that discovers and refines variational quantum circuits (VQCs) using graph-based Bayesian optimization with a graph neural network (GNN) surrogate. Circuits are represented as graphs and mutated and selected via an expected improvement acquisition function informed by surrogate uncertainty with Monte Carlo dropout. Candidate circuits are evaluated with a hybrid quantum-classical variational classifier on the next generation firewall telemetry and network internet of things (NF-ToN-IoT-V2) cybersecurity dataset, after feature selection and scaling for quantum embedding. We benchmark our pipeline against an MLP-based surrogate, random search, and greedy GNN selection. The GNN-guided optimizer consistently finds circuits with lower complexity and competitive or superior classification accuracy compared to all baselines. Robustness is assessed via a noise study across standard quantum noise channels, including amplitude damping, phase damping, thermal relaxation, depolarizing, and readout bit flip noise. The implementation is fully reproducible, with time benchmarking and export of best found circuits, providing a scalable and interpretable route to automated quantum circuit discovery.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Chain-of-Thought Reasoning for Videos</title>
<link>https://arxiv.org/abs/2512.09616</link>
<guid>https://arxiv.org/abs/2512.09616</guid>
<content:encoded><![CDATA[
arXiv:2512.09616v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) reasoning has been highly successful in solving complex tasks in natural language processing, and recent multimodal large language models (MLLMs) have extended this paradigm to video reasoning. However, these models typically build on lengthy reasoning chains and large numbers of input visual tokens. Motivated by empirical observations from our benchmark study, we hypothesize that concise reasoning combined with a reduced set of visual tokens can be sufficient for effective video reasoning. To evaluate this hypothesis, we design and validate an efficient post-training and inference framework that enhances a video MLLM's reasoning capability. Our framework enables models to operate on compressed visual tokens and generate brief reasoning traces prior to answering. The resulting models achieve substantially improved inference efficiency, deliver competitive performance across diverse benchmarks, and avoid reliance on manual CoT annotations or supervised fine-tuning. Collectively, our results suggest that long, human-like CoT reasoning may not be necessary for general video reasoning, and that concise reasoning can be both effective and efficient. Our code will be released at https://github.com/LaVi-Lab/Rethink_CoT_Video.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An End-to-end Planning Framework with Agentic LLMs and PDDL</title>
<link>https://arxiv.org/abs/2512.09629</link>
<guid>https://arxiv.org/abs/2512.09629</guid>
<content:encoded><![CDATA[
arXiv:2512.09629v1 Announce Type: cross 
Abstract: We present an end-to-end framework for planning supported by verifiers. An orchestrator receives a human specification written in natural language and converts it into a PDDL (Planning Domain Definition Language) model, where the domain and problem are iteratively refined by sub-modules (agents) to address common planning requirements, such as time constraints and optimality, as well as ambiguities and contradictions that may exist in the human specification. The validated domain and problem are then passed to an external planning engine to generate a plan. The orchestrator and agents are powered by Large Language Models (LLMs) and require no human intervention at any stage of the process. Finally, a module translates the final plan back into natural language to improve human readability while maintaining the correctness of each step. We demonstrate the flexibility and effectiveness of our framework across various domains and tasks, including the Google NaturalPlan benchmark and PlanBench, as well as planning problems like Blocksworld and the Tower of Hanoi (where LLMs are known to struggle even with small instances). Our framework can be integrated with any PDDL planning engine and validator (such as Fast Downward, LPG, POPF, VAL, and uVAL, which we have tested) and represents a significant step toward end-to-end planning aided by LLMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynthPix: A lightspeed PIV images generator</title>
<link>https://arxiv.org/abs/2512.09664</link>
<guid>https://arxiv.org/abs/2512.09664</guid>
<content:encoded><![CDATA[
arXiv:2512.09664v1 Announce Type: cross 
Abstract: We describe SynthPix, a synthetic image generator for Particle Image Velocimetry (PIV) with a focus on performance and parallelism on accelerators, implemented in JAX. SynthPix supports the same configuration parameters as existing tools but achieves a throughput several orders of magnitude higher in image-pair generation per second. SynthPix was developed to enable the training of data-hungry reinforcement learning methods for flow estimation and for reducing the iteration times during the development of fast flow estimation methods used in recent active fluids control studies with real-time PIV feedback. We believe SynthPix to be useful for the fluid dynamics community, and in this paper we describe the main ideas behind this software package.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OxEnsemble: Fair Ensembles for Low-Data Classification</title>
<link>https://arxiv.org/abs/2512.09665</link>
<guid>https://arxiv.org/abs/2512.09665</guid>
<content:encoded><![CDATA[
arXiv:2512.09665v1 Announce Type: cross 
Abstract: We address the problem of fair classification in settings where data is scarce and unbalanced across demographic groups. Such low-data regimes are common in domains like medical imaging, where false negatives can have fatal consequences.
  We propose a novel approach \emph{OxEnsemble} for efficiently training ensembles and enforcing fairness in these low-data regimes. Unlike other approaches, we aggregate predictions across ensemble members, each trained to satisfy fairness constraints. By construction, \emph{OxEnsemble} is both data-efficient, carefully reusing held-out data to enforce fairness reliably, and compute-efficient, requiring little more compute than used to fine-tune or evaluate an existing model. We validate this approach with new theoretical guarantees. Experimentally, our approach yields more consistent outcomes and stronger fairness-accuracy trade-offs than existing methods across multiple challenging medical imaging classification datasets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Ky Fan Norms and Beyond: Dual Norms and Combinations for Matrix Optimization</title>
<link>https://arxiv.org/abs/2512.09678</link>
<guid>https://arxiv.org/abs/2512.09678</guid>
<content:encoded><![CDATA[
arXiv:2512.09678v1 Announce Type: cross 
Abstract: In this article, we explore the use of various matrix norms for optimizing functions of weight matrices, a crucial problem in training large language models. Moving beyond the spectral norm underlying the Muon update, we leverage duals of the Ky Fan $k$-norms to introduce a family of Muon-like algorithms we name Fanions, which are closely related to Dion. By working with duals of convex combinations of the Ky Fan $k$-norms with either the Frobenius norm or the $l_\infty$ norm, we construct the families of F-Fanions and S-Fanions, respectively. Their most prominent members are F-Muon and S-Muon. We complement our theoretical analysis with an extensive empirical study of these algorithms across a wide range of tasks and settings, demonstrating that F-Muon and S-Muon consistently match Muon's performance, while outperforming vanilla Muon on a synthetic linear least squares problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreto: An Explainability Library for Transformers</title>
<link>https://arxiv.org/abs/2512.09730</link>
<guid>https://arxiv.org/abs/2512.09730</guid>
<content:encoded><![CDATA[
arXiv:2512.09730v1 Announce Type: cross 
Abstract: Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs</title>
<link>https://arxiv.org/abs/2512.09742</link>
<guid>https://arxiv.org/abs/2512.09742</guid>
<content:encoded><![CDATA[
arXiv:2512.09742v1 Announce Type: cross 
Abstract: LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal certification of constant-local Hamiltonians</title>
<link>https://arxiv.org/abs/2512.09778</link>
<guid>https://arxiv.org/abs/2512.09778</guid>
<content:encoded><![CDATA[
arXiv:2512.09778v1 Announce Type: cross 
Abstract: We study the problem of certifying local Hamiltonians from real-time access to their dynamics. Given oracle access to $e^{-itH}$ for an unknown $k$-local Hamiltonian $H$ and a fully specified target Hamiltonian $H_0$, the goal is to decide whether $H$ is exactly equal to $H_0$ or differs from $H_0$ by at least $\varepsilon$ in normalized Frobenius norm, while minimizing the total evolution time. We introduce the first intolerant Hamiltonian certification protocol that achieves optimal performance for all constant-locality Hamiltonians. For general $n$-qubit, $k$-local, traceless Hamiltonians, our procedure uses $O(c^k/\varepsilon)$ total evolution time for a universal constant $c$, and succeeds with high probability. In particular, for $O(1)$-local Hamiltonians, the total evolution time becomes $\Theta(1/\varepsilon)$, matching the known $\Omega(1/\varepsilon)$ lower bounds and achieving the gold-standard Heisenberg-limit scaling. Prior certification methods either relied on implementing inverse evolution of $H$, required controlled access to $e^{-itH}$, or achieved near-optimal guarantees only in restricted settings such as the Ising case ($k=2$). In contrast, our algorithm requires neither inverse evolution nor controlled operations: it uses only forward real-time dynamics and achieves optimal intolerant certification for all constant-locality Hamiltonians.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathCo-LatticE: Pathology-Constrained Lattice-Of Experts Framework for Fully-supervised Few-Shot Cardiac MRI Segmentation</title>
<link>https://arxiv.org/abs/2512.09779</link>
<guid>https://arxiv.org/abs/2512.09779</guid>
<content:encoded><![CDATA[
arXiv:2512.09779v1 Announce Type: cross 
Abstract: Few-shot learning (FSL) mitigates data scarcity in cardiac MRI segmentation but typically relies on semi-supervised techniques sensitive to domain shifts and validation bias, restricting zero-shot generalizability. We propose PathCo-LatticE, a fully supervised FSL framework that replaces unlabeled data with pathology-guided synthetic supervision. First, our Virtual Patient Engine models continuous latent disease trajectories from sparse clinical anchors, using generative modeling to synthesize physiologically plausible, fully labeled 3D cohorts. Second, Self-Reinforcing Interleaved Validation (SIV) provides a leakage-free protocol that evaluates models online with progressively challenging synthetic samples, eliminating the need for real validation data. Finally, a dynamic Lattice-of-Experts (LoE) organizes specialized networks within a pathology-aware topology and activates the most relevant experts per input, enabling robust zero-shot generalization to unseen data without target-domain fine-tuning. We evaluated PathCo-LatticE in a strict out-of-distribution (OOD) setting, deriving all anchors and severity statistics from a single-source domain (ACDC) and performing zero-shot testing on the multi-center, multi-vendor M&amp;Ms dataset. PathCo-LatticE outperforms four state-of-the-art FSL methods by 4.2-11% Dice starting from only 7 labeled anchors, and approaches fully supervised performance (within 1% Dice) with only 19 labeled anchors. The method shows superior harmonization across four vendors and generalization to unseen pathologies. [Code will be made publicly available].
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M3Net: A Multi-Metric Mixture of Experts Network Digital Twin with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.09797</link>
<guid>https://arxiv.org/abs/2512.09797</guid>
<content:encoded><![CDATA[
arXiv:2512.09797v1 Announce Type: cross 
Abstract: The rise of 5G/6G network technologies promises to enable applications like autonomous vehicles and virtual reality, resulting in a significant increase in connected devices and necessarily complicating network management. Even worse, these applications often have strict, yet heterogeneous, performance requirements across metrics like latency and reliability. Much recent work has thus focused on developing the ability to predict network performance. However, traditional methods for network modeling, like discrete event simulators and emulation, often fail to balance accuracy and scalability. Network Digital Twins (NDTs), augmented by machine learning, present a viable solution by creating virtual replicas of physical networks for real- time simulation and analysis. State-of-the-art models, however, fall short of full-fledged NDTs, as they often focus only on a single performance metric or simulated network data. We introduce M3Net, a Multi-Metric Mixture-of-experts (MoE) NDT that uses a graph neural network architecture to estimate multiple performance metrics from an expanded set of network state data in a range of scenarios. We show that M3Net significantly enhances the accuracy of flow delay predictions by reducing the MAPE (Mean Absolute Percentage Error) from 20.06% to 17.39%, while also achieving 66.47% and 78.7% accuracy on jitter and packets dropped for each flow
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations</title>
<link>https://arxiv.org/abs/2512.09804</link>
<guid>https://arxiv.org/abs/2512.09804</guid>
<content:encoded><![CDATA[
arXiv:2512.09804v1 Announce Type: cross 
Abstract: This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A roadmap of geospatial soil quality analysis systems</title>
<link>https://arxiv.org/abs/2512.09817</link>
<guid>https://arxiv.org/abs/2512.09817</guid>
<content:encoded><![CDATA[
arXiv:2512.09817v1 Announce Type: cross 
Abstract: Soil quality (SQ) plays a crucial role in sustainable agriculture, environmental conservation, and land-use planning. Traditional SQ assessment techniques rely on costly, labor-intensive sampling and laboratory analysis, limiting their spatial and temporal coverage. Advances in Geographic Information Systems (GIS), remote sensing, and machine learning (ML) enabled efficient SQ evaluation. This paper presents a comprehensive roadmap distinguishing it from previous reviews by proposing a unified and modular pipeline that integrates multi-source soil data, GIS and remote sensing tools, and machine learning techniques to support transparent and scalable soil quality assessment. It also includes practical applications. Contrary to existing studies that predominantly target isolated soil parameters or specific modeling methodologies, this approach consolidates recent advancements in Geographic Information Systems (GIS), remote sensing technologies, and machine learning algorithms within the entire soil quality assessment pipeline. It also addresses existing challenges and limitations while exploring future developments and emerging trends in the field that can deliver the next generation of soil quality systems making them more transparent, adaptive, and aligned with sustainable land management.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RIFT: A Scalable Methodology for LLM Accelerator Fault Assessment using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.09829</link>
<guid>https://arxiv.org/abs/2512.09829</guid>
<content:encoded><![CDATA[
arXiv:2512.09829v1 Announce Type: cross 
Abstract: The massive scale of modern AI accelerators presents critical challenges to traditional fault assessment methodologies, which face prohibitive computational costs and provide poor coverage of critical failure modes. This paper introduces RIFT (Reinforcement Learning-guided Intelligent Fault Targeting), a scalable framework that automates the discovery of minimal, high-impact fault scenarios for efficient design-time fault assessment. RIFT transforms the complex search for worst-case faults into a sequential decision-making problem, combining hybrid sensitivity analysis for search space pruning with reinforcement learning to intelligently generate minimal, high-impact test suites. Evaluated on billion-parameter Large Language Model (LLM) workloads using NVIDIA A100 GPUs, RIFT achieves a \textbf{2.2$\times$} fault assessment speedup over evolutionary methods and reduces the required test vector volume by over \textbf{99\%} compared to random fault injection, all while achieving \textbf{superior fault coverage}. The proposed framework also provides actionable data to enable intelligent hardware protection strategies, demonstrating that RIFT-guided selective error correction code provides a \textbf{12.8$\times$} improvement in \textbf{cost-effectiveness} (coverage per unit area) compared to uniform triple modular redundancy protection. RIFT automatically generates UVM-compliant verification artifacts, ensuring its findings are directly actionable and integrable into commercial RTL verification workflows.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretation as Linear Transformation: A Cognitive-Geometric Model of Belief and Meaning</title>
<link>https://arxiv.org/abs/2512.09831</link>
<guid>https://arxiv.org/abs/2512.09831</guid>
<content:encoded><![CDATA[
arXiv:2512.09831v1 Announce Type: cross 
Abstract: This paper develops a geometric framework for modeling belief, motivation, and influence across cognitively heterogeneous agents. Each agent is represented by a personalized value space, a vector space encoding the internal dimensions through which the agent interprets and evaluates meaning. Beliefs are formalized as structured vectors-abstract beings-whose transmission is mediated by linear interpretation maps. A belief survives communication only if it avoids the null spaces of these maps, yielding a structural criterion for intelligibility, miscommunication, and belief death.
  Within this framework, I show how belief distortion, motivational drift, counterfactual evaluation, and the limits of mutual understanding arise from purely algebraic constraints. A central result-"the No-Null-Space Leadership Condition"-characterizes leadership as a property of representational reachability rather than persuasion or authority. More broadly, the model explains how abstract beings can propagate, mutate, or disappear as they traverse diverse cognitive geometries.
  The account unifies insights from conceptual spaces, social epistemology, and AI value alignment by grounding meaning preservation in structural compatibility rather than shared information or rationality. I argue that this cognitive-geometric perspective clarifies the epistemic boundaries of influence in both human and artificial systems, and offers a general foundation for analyzing belief dynamics across heterogeneous agents.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Factorized Learning: Powered by In-Memory Database Systems</title>
<link>https://arxiv.org/abs/2512.09836</link>
<guid>https://arxiv.org/abs/2512.09836</guid>
<content:encoded><![CDATA[
arXiv:2512.09836v1 Announce Type: cross 
Abstract: Learning models over factorized joins avoids redundant computations by identifying and pre-computing shared cofactors. Previous work has investigated the performance gain when computing cofactors on traditional disk-based database systems. Due to the absence of published code, the experiments could not be reproduced on in-memory database systems. This work describes the implementation when using cofactors for in-database factorized learning. We benchmark our open-source implementation for learning linear regression on factorized joins with PostgreSQL -- as a disk-based database system -- and HyPer -- as an in-memory engine. The evaluation shows a performance gain of factorized learning on in-memory database systems by 70\% to non-factorized learning and by a factor of 100 compared to disk-based database systems. Thus, modern database engines can contribute to the machine learning pipeline by pre-computing aggregates prior to data extraction to accelerate training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Supervised learning pays attention</title>
<link>https://arxiv.org/abs/2512.09912</link>
<guid>https://arxiv.org/abs/2512.09912</guid>
<content:encoded><![CDATA[
arXiv:2512.09912v1 Announce Type: cross 
Abstract: In-context learning with attention enables large neural networks to make context-specific predictions by selectively focusing on relevant examples. Here, we adapt this idea to supervised learning procedures such as lasso regression and gradient boosting, for tabular data. Our goals are to (1) flexibly fit personalized models for each prediction point and (2) retain model simplicity and interpretability.
  Our method fits a local model for each test observation by weighting the training data according to attention, a supervised similarity measure that emphasizes features and interactions that are predictive of the outcome. Attention weighting allows the method to adapt to heterogeneous data in a data-driven way, without requiring cluster or similarity pre-specification. Further, our approach is uniquely interpretable: for each test observation, we identify which features are most predictive and which training observations are most relevant. We then show how to use attention weighting for time series and spatial data, and we present a method for adapting pretrained tree-based models to distributional shift using attention-weighted residual corrections. Across real and simulated datasets, attention weighting improves predictive performance while preserving interpretability, and theory shows that attention-weighting linear models attain lower mean squared error than the standard linear model under mixture-of-models data-generating processes with known subgroup structure.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TCNN: Triple Convolutional Neural Network Models for Retrieval-based Question Answering System in E-commerce</title>
<link>https://arxiv.org/abs/2004.10919</link>
<guid>https://arxiv.org/abs/2004.10919</guid>
<content:encoded><![CDATA[
arXiv:2004.10919v2 Announce Type: replace 
Abstract: Automatic question-answering (QA) systems have boomed during last few years, and commonly used techniques can be roughly categorized into Information Retrieval (IR)-based and generation-based. A key solution to the IR based models is to retrieve the most similar knowledge entries of a given query from a QA knowledge base, and then rerank those knowledge entries with semantic matching models. In this paper, we aim to improve an IR based e-commerce QA system-AliMe with proposed text matching models, including a basic Triple Convolutional Neural Network (TCNN) model and two Attention-based TCNN (ATCNN) models. Experimental results show their effect.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Information-Theoretic Active Correlation Clustering</title>
<link>https://arxiv.org/abs/2402.03587</link>
<guid>https://arxiv.org/abs/2402.03587</guid>
<content:encoded><![CDATA[
arXiv:2402.03587v3 Announce Type: replace 
Abstract: Correlation clustering is a flexible framework for partitioning data based solely on pairwise similarity or dissimilarity information, without requiring the number of clusters as input. However, in many practical scenarios, these pairwise similarities are not available a priori and must be obtained through costly measurements or human feedback. This motivates the use of active learning to query only the most informative pairwise comparisons, enabling effective clustering under budget constraints. In this work, we develop a principled active learning approach for correlation clustering by introducing several information-theoretic acquisition functions that prioritize queries based on entropy and expected information gain. These strategies aim to reduce uncertainty about the clustering structure as efficiently as possible. We evaluate our methods across a range of synthetic and real-world settings and show that they significantly outperform existing baselines in terms of clustering accuracy and query efficiency. Our results highlight the benefits of combining active learning with correlation clustering in settings where similarity information is costly or limited.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hard Work Does Not Always Pay Off: Poisoning Attacks on Neural Architecture Search</title>
<link>https://arxiv.org/abs/2405.06073</link>
<guid>https://arxiv.org/abs/2405.06073</guid>
<content:encoded><![CDATA[
arXiv:2405.06073v2 Announce Type: replace 
Abstract: We study the robustness of data-centric methods to find neural network architectures, known as neural architecture search (NAS), against data poisoning. To audit this robustness, we design a poisoning framework that enables the systematic evaluation of the ability of NAS to produce architectures under data corruption. Our framework examines four off-the-shelf NAS algorithms, representing different approaches to architecture discovery, against four data poisoning attacks, including one we tailor specifically for NAS. In our evaluation with the CIFAR-10 and CIFAR-100 benchmarks, we show that NAS is \emph{seemingly} robust to data poisoning, showing marginal accuracy drops even under large poisoning budgets. However, we demonstrate that when considering NAS algorithms designed to achieve a few percentage points of accuracy gain, this expected improvement can be substantially diminished under data poisoning. We also show that the reduction varies across NAS algorithms and analyze the factors contributing to their robustness. Our findings are: (1) Training-based NAS algorithms are the least robust due to their reliance on data. (2) Training-free NAS approaches are the most robust but produce architectures that perform similarly to random selections from the search space. (3) NAS algorithms can produce architectures with improved accuracy, even when using out-of-distribution data like MNIST. We lastly discuss potential countermeasures. Our code is available at: https://github.com/ztcoalson/NAS-Robustness-to-Data-Poisoning
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Informed Weighting Channel Normalizing Flow for Deep Generative Models</title>
<link>https://arxiv.org/abs/2407.04958</link>
<guid>https://arxiv.org/abs/2407.04958</guid>
<content:encoded><![CDATA[
arXiv:2407.04958v2 Announce Type: replace 
Abstract: Normalizing Flows (NFs) are widely used in deep generative models for their exact likelihood estimation and efficient sampling.
  However, they require substantial memory since the latent space matches the input dimension.
  Multi-scale architectures address this by progressively reducing latent dimensions while preserving reversibility.
  Existing multi-scale architectures use simple, static channel-wise splitting, limiting expressiveness. To improve this, we introduce a regularized, feature-dependent $\mathtt{Shuffle}$ operation and integrate it into vanilla multi-scale architecture.
  This operation adaptively generates channel-wise weights and shuffles latent variables before splitting them.
  We observe that such operation guides the variables to evolve in the direction of entropy increase, hence we refer to NFs with the $\mathtt{Shuffle}$ operation as \emph{Entropy-Informed Weighting Channel Normalizing Flow} (EIW-Flow).
  Extensive experiments on CIFAR-10, CelebA, ImageNet, and LSUN demonstrate that EIW-Flow achieves state-of-the-art density estimation and competitive sample quality for deep generative modeling, with minimal computational overhead.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Point Neuron Learning: A New Physics-Informed Neural Network Architecture</title>
<link>https://arxiv.org/abs/2408.16969</link>
<guid>https://arxiv.org/abs/2408.16969</guid>
<content:encoded><![CDATA[
arXiv:2408.16969v2 Announce Type: replace 
Abstract: Machine learning and neural networks have advanced numerous research domains, but challenges such as large training data requirements and inconsistent model performance hinder their application in certain scientific problems. To overcome these challenges, researchers have investigated integrating physics principles into machine learning models, mainly through: (i) physics-guided loss functions, generally termed as physics-informed neural networks, and (ii) physics-guided architectural design. While both approaches have demonstrated success across multiple scientific disciplines, they have limitations including being trapped to a local minimum, poor interpretability, and restricted generalizability. This paper proposes a new physics-informed neural network (PINN) architecture that combines the strengths of both approaches by embedding the fundamental solution of the wave equation into the network architecture, enabling the learned model to strictly satisfy the wave equation. The proposed point neuron learning method can model an arbitrary sound field based on microphone observations without any dataset. Compared to other PINN methods, our approach directly processes complex numbers and offers better interpretability and generalizability. We evaluate the versatility of the proposed architecture by a sound field reconstruction problem in a reverberant environment. Results indicate that the point neuron method outperforms two competing methods and can efficiently handle noisy environments with sparse microphone observations.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning and Opportunistic Inference for Continuous Monitoring of Freezing of Gait in Parkinson's Disease</title>
<link>https://arxiv.org/abs/2410.21326</link>
<guid>https://arxiv.org/abs/2410.21326</guid>
<content:encoded><![CDATA[
arXiv:2410.21326v2 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a progressive neurological disorder that impacts the quality of life significantly, making in-home monitoring of motor symptoms such as Freezing of Gait (FoG) critical. However, existing symptom monitoring technologies are power-hungry, rely on extensive amounts of labeled data, and operate in controlled settings. These shortcomings limit real-world deployment of the technology. This work presents LIFT-PD, a computationally-efficient self-supervised learning framework for real-time FoG detection. Our method combines self-supervised pre-training on unlabeled data with a novel differential hopping windowing technique to learn from limited labeled instances. An opportunistic model activation module further minimizes power consumption by selectively activating the deep learning module only during active periods. Extensive experimental results show that LIFT-PD achieves a 7.25% increase in precision and 4.4% improvement in accuracy compared to supervised models while using as low as 40% of the labeled training data used for supervised learning. Additionally, the model activation module reduces inference time by up to 67% compared to continuous inference. LIFT-PD paves the way for practical, energy-efficient, and unobtrusive in-home monitoring of PD patients with minimal labeling requirements.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Analysis of Diffusion Models with Application to Schedule Design</title>
<link>https://arxiv.org/abs/2502.00180</link>
<guid>https://arxiv.org/abs/2502.00180</guid>
<content:encoded><![CDATA[
arXiv:2502.00180v3 Announce Type: replace 
Abstract: Diffusion models (DMs) have emerged as powerful tools for modeling complex data distributions and generating realistic new samples. Over the years, advanced architectures and sampling methods have been developed to make these models practically usable. However, certain synthesis process decisions still rely on heuristics without a solid theoretical foundation. In our work, we offer a novel analysis of the DM's inference process, introducing a comprehensive frequency response perspective. Specifically, by relying on Gaussianity assumption, we present the inference process as a closed-form spectral transfer function, capturing how the generated signal evolves in response to the initial noise. We demonstrate how the proposed analysis can be leveraged to design a noise schedule that aligns effectively with the characteristics of the data. The spectral perspective also provides insights into the underlying dynamics and sheds light on the relationship between spectral properties and noise schedule structure. Our results lead to scheduling curves that are dependent on the spectral content of the data, offering a theoretical justification for some of the heuristics taken by practitioners.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Intermediate-Layer Matching in Knowledge Distillation: Layer-Selection Strategy Doesn't Matter (Much)</title>
<link>https://arxiv.org/abs/2502.04499</link>
<guid>https://arxiv.org/abs/2502.04499</guid>
<content:encoded><![CDATA[
arXiv:2502.04499v2 Announce Type: replace 
Abstract: Knowledge distillation (KD) is a popular method of transferring knowledge from a large "teacher" model to a small "student" model. Previous work has explored various layer-selection strategies (e.g., forward matching and in-order random matching) for intermediate-layer matching in KD, where a student layer is forced to resemble a certain teacher layer. In this work, we revisit such layer-selection strategies and observe an intriguing phenomenon that layer-selection strategy does not matter (much) in intermediate-layer matching -- even seemingly nonsensical matching strategies such as reverse matching still result in surprisingly good student performance. We provide an interpretation for this phenomenon by examining the angles between teacher layers viewed from the student's perspective. Our work sheds light on KD practice, as layer-selection strategies may not be the main focus of KD system design, and vanilla forward matching works well in most setups.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory Injection Attacks on LLM Agents via Query-Only Interaction</title>
<link>https://arxiv.org/abs/2503.03704</link>
<guid>https://arxiv.org/abs/2503.03704</guid>
<content:encoded><![CDATA[
arXiv:2503.03704v4 Announce Type: replace 
Abstract: Agents powered by large language models (LLMs) have demonstrated strong capabilities in a wide range of complex, real-world applications. However, LLM agents with a compromised memory bank may easily produce harmful outputs when the past records retrieved for demonstration are malicious. In this paper, we propose a novel Memory INJection Attack, MINJA, without assuming that the attacker can directly modify the memory bank of the agent. The attacker injects malicious records into the memory bank by only interacting with the agent via queries and output observations. These malicious records are designed to elicit a sequence of malicious reasoning steps corresponding to a different target query during the agent's execution of the victim user's query. Specifically, we introduce a sequence of bridging steps to link victim queries to the malicious reasoning steps. During the memory injection, we propose an indication prompt that guides the agent to autonomously generate similar bridging steps, with a progressive shortening strategy that gradually removes the indication prompt, such that the malicious record will be easily retrieved when processing later victim queries. Our extensive experiments across diverse agents demonstrate the effectiveness of MINJA in compromising agent memory. With minimal requirements for execution, MINJA enables any user to influence agent memory, highlighting the risk.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sinusoidal Initialization, Time for a New Start</title>
<link>https://arxiv.org/abs/2505.12909</link>
<guid>https://arxiv.org/abs/2505.12909</guid>
<content:encoded><![CDATA[
arXiv:2505.12909v3 Announce Type: replace 
Abstract: Initialization plays a critical role in Deep Neural Network training, directly influencing convergence, stability, and generalization. Common approaches such as Glorot and He initializations rely on randomness, which can produce uneven weight distributions across layer connections. In this paper, we introduce the Sinusoidal initialization, a novel deterministic method that employs sinusoidal functions to construct structured weight matrices expressly to improve the spread and balance of weights throughout the network while simultaneously fostering a more uniform, well-conditioned distribution of neuron activation states from the very first forward pass. Because Sinusoidal initialization begins with weights and activations that are already evenly and efficiently utilized, it delivers consistently faster convergence, greater training stability, and higher final accuracy across a wide range of models, including convolutional neural networks, vision transformers, and large language models. On average, our experiments show an increase of 4.9% in final validation accuracy and 20.9% in convergence speed. By replacing randomness with structure, this initialization provides a stronger and more reliable foundation for Deep Learning systems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarially Pretrained Transformers May Be Universally Robust In-Context Learners</title>
<link>https://arxiv.org/abs/2505.14042</link>
<guid>https://arxiv.org/abs/2505.14042</guid>
<content:encoded><![CDATA[
arXiv:2505.14042v2 Announce Type: replace 
Abstract: Adversarial training is one of the most effective adversarial defenses, but it incurs a high computational cost. In this study, we present the first theoretical analysis suggesting that adversarially pretrained transformers can serve as universally robust foundation models -- models that can robustly adapt to diverse downstream tasks with only lightweight tuning. Specifically, we demonstrate that single-layer linear transformers, after adversarial pretraining across a variety of classification tasks, can robustly generalize to unseen classification tasks through in-context learning from clean demonstrations (i.e., without requiring additional adversarial training or examples). This universal robustness stems from the model's ability to adaptively focus on robust features within given tasks. We also show the two open challenges for attaining robustness: accuracy--robustness trade-off and sample-hungry training. This study initiates the discussion on the utility of universally robust foundation models. While their training is expensive, the investment would prove worthwhile as downstream tasks can enjoy free adversarial robustness. The code is available at https://github.com/s-kumano/universally-robust-in-context-learner.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global Convergence for Average Reward Constrained MDPs with Primal-Dual Actor Critic Algorithm</title>
<link>https://arxiv.org/abs/2505.15138</link>
<guid>https://arxiv.org/abs/2505.15138</guid>
<content:encoded><![CDATA[
arXiv:2505.15138v2 Announce Type: replace 
Abstract: This paper investigates infinite-horizon average reward Constrained Markov Decision Processes (CMDPs) with general parametrization. We propose a Primal-Dual Natural Actor-Critic algorithm that adeptly manages constraints while ensuring a high convergence rate. In particular, our algorithm achieves global convergence and constraint violation rates of $\tilde{\mathcal{O}}(1/\sqrt{T})$ over a horizon of length $T$ when the mixing time, $\tau_{\mathrm{mix}}$, is known to the learner. In absence of knowledge of $\tau_{\mathrm{mix}}$, the achievable rates change to $\tilde{\mathcal{O}}(1/T^{0.5-\epsilon})$ provided that $T \geq \tilde{\mathcal{O}}\left(\tau_{\mathrm{mix}}^{2/\epsilon}\right)$. Our results match the theoretical lower bound for Markov Decision Processes and establish a new benchmark in the theoretical exploration of average reward CMDPs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Not All Models Suit Expert Offloading: On Local Routing Consistency of Mixture-of-Expert Models</title>
<link>https://arxiv.org/abs/2505.16056</link>
<guid>https://arxiv.org/abs/2505.16056</guid>
<content:encoded><![CDATA[
arXiv:2505.16056v3 Announce Type: replace 
Abstract: Mixture-of-Experts (MoE) enables efficient scaling of large language models (LLMs) with sparsely activated experts during inference. To effectively deploy large MoE models on memory-constrained devices, many systems introduce *expert offloading* that caches a subset of experts in fast memory, leaving others on slow memory to run on CPU or load on demand. While some research has exploited the locality of expert activations, where consecutive tokens activate similar experts, the degree of this **local routing consistency** varies across models and remains understudied. In this paper, we propose two metrics to measure local routing consistency of MoE models: (1) **Segment Routing Best Performance (SRP)**, which evaluates how well a fixed group of experts can cover the needs of a segment of tokens, and (2) **Segment Cache Best Hit Rate (SCH)**, which measures the hit rate of an expert cache utilizing a length of future information under a cache limit. We analyze 20 MoE LLMs with diverse sizes and architectures and use toy models to verify key factors related to local routing consistency. We find a strong trade-off between local routing consistency and *local* load balance, while showing that *global* load balance can coexist with local routing consistency. Meanwhile, settings like shared experts that decrease expert combination space can lead to low local routing consistency. We further reveal that domain-specialized experts contribute more to routing consistency than vocabulary-specialized ones, and that most models balance between cache effectiveness and efficiency with cache sizes approximately twice the active experts. These findings pave the way for memory-efficient MoE design and deployment without compromising inference speed. We publish the code for replicating experiments at https://github.com/ljcleo/moe-lrc .
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Network Science Approach to Granular Time Series Segmentation</title>
<link>https://arxiv.org/abs/2505.17640</link>
<guid>https://arxiv.org/abs/2505.17640</guid>
<content:encoded><![CDATA[
arXiv:2505.17640v2 Announce Type: replace 
Abstract: Time series segmentation (TSS) is one of the time series (TS) analysis techniques, that has received considerably less attention compared to other TS related tasks. In recent years, deep learning architectures have been introduced for TSS, however their reliance on sliding windows limits segmentation granularity due to fixed window sizes and strides. To overcome these challenges, we propose a new more granular TSS approach that utilizes the Weighted Dual Perspective Visbility Graph (WDPVG) TS into a graph and combines it with a Graph Attention Network (GAT). By transforming TS into graphs, we are able to capture different structural aspects of the data that would otherwise remain hidden. By utilizing the representation learning capabilities of Graph Neural Networks, our method is able to effectively identify meaningful segments within the TS. To better understand the potential of our approach, we also experimented with different TS-to-graph transformations and compared their performance. Our contributions include: a) formulating the TSS as a node classification problem on graphs; b) conducting an extensive analysis of various TS-to-graph transformations applied to TSS using benchmark datasets from the TSSB repository; c) providing the first detailed study on utilizing GNNs for analyzing graph representations of TS in the context of TSS; d) demonstrating the effectiveness of our method, which achieves an average F1 score of 0.97 across 59 diverse TSS benchmark datasets; e) outperforming the seq2point baseline method by 0.05 in terms of F1 score; and f) reducing the required training data compared to the baseline methods.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The emergence of sparse attention: impact of data distribution and benefits of repetition</title>
<link>https://arxiv.org/abs/2505.17863</link>
<guid>https://arxiv.org/abs/2505.17863</guid>
<content:encoded><![CDATA[
arXiv:2505.17863v2 Announce Type: replace 
Abstract: Emergence is a fascinating property of large language models and neural networks more broadly: as models scale and train for longer, they sometimes develop new abilities in sudden ways. Despite initial studies, we still lack a comprehensive understanding of how and when these abilities emerge. To address this gap, we study the emergence over training of sparse attention, a critical and frequently observed attention pattern in Transformers. By combining theoretical analysis of a toy model with empirical observations on small Transformers trained on a linear regression variant, we uncover the mechanics driving sparse attention emergence and reveal that emergence timing follows power laws based on task structure, architecture, and optimizer choice. We additionally find that repetition can greatly speed up emergence. Finally, we confirm these results on a well-studied in-context associative recall task. Our findings provide a simple, theoretically grounded framework for understanding how data distributions and model design influence the learning dynamics behind one form of emergence.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM Meeting Decision Trees on Tabular Data</title>
<link>https://arxiv.org/abs/2505.17918</link>
<guid>https://arxiv.org/abs/2505.17918</guid>
<content:encoded><![CDATA[
arXiv:2505.17918v2 Announce Type: replace 
Abstract: Tabular data have been playing a vital role in diverse real-world fields, including healthcare, finance, etc. With the recent success of Large Language Models (LLMs), early explorations of extending LLMs to the domain of tabular data have been developed. Most of these LLM-based methods typically first serialize tabular data into natural language descriptions, and then tune LLMs or directly infer on these serialized data. However, these methods suffer from two key inherent issues: (i) data perspective: existing data serialization methods lack universal applicability for structured tabular data, and may pose privacy risks through direct textual exposure, and (ii) model perspective: LLM fine-tuning methods struggle with tabular data, and in-context learning scalability is bottle-necked by input length constraints (suitable for few-shot learning). This work explores a novel direction of integrating LLMs into tabular data throughough logical decision tree rules as intermediaries, proposes a decision tree enhancer with LLM-derived rule for tabular prediction, DeLTa. The proposed DeLTa avoids tabular data serialization, and can be applied to full data learning setting without LLM fine-tuning. Specifically, we leverage the reasoning ability of LLMs to redesign an improved rule given a set of decision tree rules. Furthermore, we provide a calibration method for original decision trees via new generated rule by LLM, which approximates the error correction vector to steer the original decision tree predictions in the direction of ``errors'' reducing. Finally, extensive experiments on diverse tabular benchmarks show that our method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Framework for Controllable Multi-objective Learning with Annealed Stein Variational Hypernetworks</title>
<link>https://arxiv.org/abs/2506.06715</link>
<guid>https://arxiv.org/abs/2506.06715</guid>
<content:encoded><![CDATA[
arXiv:2506.06715v3 Announce Type: replace 
Abstract: Pareto Set Learning (PSL) is popular as an efficient approach to obtaining the complete optimal solution in Multi-objective Learning (MOL). A set of optimal solutions approximates the Pareto set, and its mapping is a set of dense points in the Pareto front in objective space. However, some current methods face a challenge: how to make the Pareto solution is diverse while maximizing the hypervolume value. In this paper, we propose a novel method to address this challenge, which employs Stein Variational Gradient Descent (SVGD) to approximate the entire Pareto set. SVGD pushes a set of particles towards the Pareto set by applying a form of functional gradient descent, which helps to converge and diversify optimal solutions. Additionally, we employ diverse gradient direction strategies to thoroughly investigate a unified framework for SVGD in multi-objective optimization and adapt this framework with an annealing schedule to promote stability. We introduce our method, SVH-MOL, and validate its effectiveness through extensive experiments on multi-objective problems and multi-task learning, demonstrating its superior performance.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2506.07040</link>
<guid>https://arxiv.org/abs/2506.07040</guid>
<content:encoded><![CDATA[
arXiv:2506.07040v3 Announce Type: replace 
Abstract: We present a non-asymptotic convergence analysis of $Q$-learning and actor-critic algorithms for robust average-reward Markov Decision Processes (MDPs) under contamination, total-variation (TV) distance, and Wasserstein uncertainty sets. A key ingredient of our analysis is showing that the optimal robust $Q$ operator is a strict contraction with respect to a carefully designed semi-norm (with constant functions quotiented out). This property enables a stochastic approximation update that learns the optimal robust $Q$-function using $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We also provide an efficient routine for robust $Q$-function estimation, which in turn facilitates robust critic estimation. Building on this, we introduce an actor-critic algorithm that learns an $\epsilon$-optimal robust policy within $\tilde{\mathcal{O}}(\epsilon^{-2})$ samples. We provide numerical simulations to evaluate the performance of our algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI reconstruction of European weather from the Euro-Atlantic regimes</title>
<link>https://arxiv.org/abs/2506.13758</link>
<guid>https://arxiv.org/abs/2506.13758</guid>
<content:encoded><![CDATA[
arXiv:2506.13758v2 Announce Type: replace 
Abstract: We present a non-linear AI-model designed to reconstruct monthly mean anomalies of the European temperature and precipitation based on the Euro-Atlantic Weather regimes (WR) indices. WR represent recurrent, quasi-stationary, and persistent states of the atmospheric circulation that exert considerable influence over the European weather, therefore offering an opportunity for sub-seasonal to seasonal forecasting. While much research has focused on studying the correlation and impacts of the WR on European weather, the estimation of ground-level climate variables, such as temperature and precipitation, from Euro-Atlantic WR remains largely unexplored and is currently limited to linear methods. The presented AI model can capture and introduce complex non-linearities in the relation between the WR indices, describing the state of the Euro-Atlantic atmospheric circulation and the corresponding surface temperature and precipitation anomalies in Europe. We discuss the AI-model performance in reconstructing the monthly mean two-meter temperature and total precipitation anomalies in the European winter and summer, also varying the number of WR used to describe the monthly atmospheric circulation. We assess the impact of errors on the WR indices in the reconstruction and show that a mean absolute relative error below 80% yields improved seasonal reconstruction compared to the ECMWF operational seasonal forecast system, SEAS5. As a demonstration of practical applicability, we evaluate the model using WR indices predicted by SEAS5, finding slightly better or comparable skill relative to the SEAS5 forecast itself. Our findings demonstrate that WR-based anomaly reconstruction, powered by AI tools, offers a promising pathway for sub-seasonal and seasonal forecasting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Minimalist Optimizer Design for LLM Pretraining</title>
<link>https://arxiv.org/abs/2506.16659</link>
<guid>https://arxiv.org/abs/2506.16659</guid>
<content:encoded><![CDATA[
arXiv:2506.16659v2 Announce Type: replace 
Abstract: Training large language models (LLMs) typically relies on adaptive optimizers such as Adam, which introduce extra operations and require significant more memory to maintain first- and second-order moments than SGD. While recent works such as GaLore, Fira and APOLLO have proposed state-compressed variants to reduce memory consumption, a fundamental question remains: What are the minimum modifications to plain SGD needed to match state-of-the-art pretraining performance? We systematically investigate this question using a bottom-up approach, and identify two simple yet highly (memory- and compute-) efficient techniques: (1) column-wise gradient normalization (normalizing the gradient along the output dimension), which boosts SGD performance without momentum; and (2) applying first-order momentum only to the output layer, where gradient variance is highest. Combining these two techniques lead to SCALE (Stochastic Column-normAlized Last-layer momEntum), a simple optimizer for memory efficient pretraining. Across multiple LLaMA models (60M-1B), SCALE matches or exceeds the performance of Adam while using only 35-45% of the total memory. It also consistently outperforms memory-efficient optimizers such as GaLore, Fira and APOLLO, making it a strong candidate for large-scale pretraining under memory constraints. For LLaMA 7B model, SCALE outperforms the state-of-the-art memory-efficient methods APOLLO and Muon, in terms of both perplexity and memory consumption.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-driven Stochastic Trace Clustering</title>
<link>https://arxiv.org/abs/2506.23776</link>
<guid>https://arxiv.org/abs/2506.23776</guid>
<content:encoded><![CDATA[
arXiv:2506.23776v2 Announce Type: replace 
Abstract: Process discovery algorithms automatically extract process models from event logs, but high variability often results in complex and hard-to-understand models. To mitigate this issue, trace clustering techniques group process executions into clusters, each represented by a simpler and more understandable process model. Model-driven trace clustering improves on this by assigning traces to clusters based on their conformity to cluster-specific process models. However, most existing clustering techniques rely on either no process model discovery, or non-stochastic models, neglecting the frequency or probability of activities and transitions, thereby limiting their capability to capture real-world execution dynamics. We propose a novel model-driven trace clustering method that optimizes stochastic process models within each cluster. Our approach uses entropic relevance, a stochastic conformance metric based on directly-follows probabilities, to guide trace assignment. This allows clustering decisions to consider both structural alignment with a cluster's process model and the likelihood that a trace originates from a given stochastic process model. The method is computationally efficient, scales linearly with input size, and improves model interpretability by producing clusters with clearer control-flow patterns. Extensive experiments on public real-life datasets demonstrate that while our method yields superior stochastic coherence and graph simplicity, traditional fitness metrics reveal a trade-off, highlighting the specific utility of our approach for stochastic process analysis.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PROPS: Progressively Private Self-alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2508.06783</link>
<guid>https://arxiv.org/abs/2508.06783</guid>
<content:encoded><![CDATA[
arXiv:2508.06783v2 Announce Type: replace 
Abstract: Alignment is a key step in developing Large Language Models (LLMs) using human feedback to ensure adherence to human values and societal norms. Dependence on human feedback raises privacy concerns about how much a labeler's preferences may reveal about their personal values, beliefs, and personality traits. Existing approaches, such as Differentially Private SGD (DP-SGD), provide rigorous privacy guarantees by privatizing gradients during fine-tuning and alignment but can provide more privacy than necessary as human preferences are tied only to labels of (prompt, response) pairs and can degrade model utility. This work focuses on LLM alignment with preference-level privacy, which preserves the privacy of preference labels provided by humans. We propose PROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving alignment framework where privately aligned models in previous stages can serve as labelers for supplementing training data in the subsequent stages of alignment. We present theoretical guarantees for PROPS as well as comprehensive validation using multiple models (Pythia and GPT) and datasets (AlpacaEval, Anthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over existing methods while still providing high privacy. For the same privacy budget, alignment via PROPS can achieve up to 3x higher win-rates compared to DP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based alignment.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounding the Ungrounded: A Spectral-Graph Framework for Quantifying Hallucinations in Multimodal LLMs</title>
<link>https://arxiv.org/abs/2508.19366</link>
<guid>https://arxiv.org/abs/2508.19366</guid>
<content:encoded><![CDATA[
arXiv:2508.19366v4 Announce Type: replace 
Abstract: Hallucinations in LLMs--especially in multimodal settings--undermine reliability. We present a rigorous information-geometric framework, grounded in diffusion dynamics, to quantify hallucinations in MLLMs where model outputs are embedded via spectral decompositions of multimodal graph Laplacians, and their gaps to a truth manifold define a semantic distortion metric. We derive Courant-Fischer bounds on a temperature-dependent hallucination profile and use RKHS eigenmodes to obtain modality-aware, interpretable measures that track evolution over prompts and time. This reframes hallucination as quantifiable and bounded, providing a principled basis for evaluation and mitigation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ModalSurv: Investigating opportunities and limitations of multimodal deep survival learning in prostate and bladder cancer</title>
<link>https://arxiv.org/abs/2509.05037</link>
<guid>https://arxiv.org/abs/2509.05037</guid>
<content:encoded><![CDATA[
arXiv:2509.05037v4 Announce Type: replace 
Abstract: Accurate survival prediction is essential for personalised cancer treatment. We propose ModalSurv, a multimodal deep survival framework integrating clinical, MRI, histopathology, and RNA-sequencing data via modality-specific projections and cross-attention fusion. On the CHIMERA Grand Challenge datasets, ModalSurv achieved a C-index of 0.7402 (1st) for prostate and 0.5740 (5th) for bladder cancer. Notably, clinical features alone outperformed multimodal models on external tests, highlighting challenges of limited multimodal alignment and potential overfitting. Local validation showed multimodal gains but limited generalisation. ModalSurv provides a systematic evaluation of multimodal survival modelling, underscoring both its promise and current limitations for scalable, generalisable cancer prognosis.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics, Revealing a Three-Stage In-Context Learning Mechanism</title>
<link>https://arxiv.org/abs/2509.06322</link>
<guid>https://arxiv.org/abs/2509.06322</guid>
<content:encoded><![CDATA[
arXiv:2509.06322v2 Announce Type: replace 
Abstract: Large language models (LLMs) have demonstrated emergent in-context learning (ICL) capabilities across a range of tasks, including zero-shot time-series forecasting. We show that text-trained foundation models can accurately extrapolate spatiotemporal dynamics from discretized partial differential equation (PDE) solutions without fine-tuning or natural language prompting. Predictive accuracy improves with longer temporal contexts but degrades at finer spatial discretizations. In multi-step rollouts, where the model recursively predicts future spatial states over multiple time steps, errors grow algebraically with the time horizon, reminiscent of global error accumulation in classical finite-difference solvers. We interpret these trends as in-context neural scaling laws, where prediction quality varies predictably with both context length and output length. To better understand how LLMs are able to internally process PDE solutions so as to accurately roll them out, we analyze token-level output distributions and uncover a consistent three-stage ICL progression: beginning with syntactic pattern imitation, transitioning through an exploratory high-entropy phase, and culminating in confident, numerically grounded predictions.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimpleFold: Folding Proteins is Simpler than You Think</title>
<link>https://arxiv.org/abs/2509.18480</link>
<guid>https://arxiv.org/abs/2509.18480</guid>
<content:encoded><![CDATA[
arXiv:2509.18480v4 Announce Type: replace 
Abstract: Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Noise-Curvature View of Loss of Trainability</title>
<link>https://arxiv.org/abs/2509.19698</link>
<guid>https://arxiv.org/abs/2509.19698</guid>
<content:encoded><![CDATA[
arXiv:2509.19698v3 Announce Type: replace 
Abstract: Loss of trainability refers to a phenomenon in continual learning where parameter updates no longer make progress on the optimization objective, so accuracy stalls or degrades as the learning problem changes over time. In this paper, we analyze loss of trainability through an optimization lens and find that the phenomenon is not reliably predicted by existing individual indicators such as Hessian rank, sharpness level, weight or gradient norms, gradient-to-parameter ratios, and unit-sign entropy. Motivated by our analysis, we introduce two complementary indicators: a batch-size-aware gradient-noise bound and a curvature volatility-controlled bound. We then combine these two indicators into a per-layer adaptive noise threshold on the effective step-size that anticipates trainability behavior. Using this insight, we propose a step-size scheduler that keeps each layer's effective parameter update below this bound, thereby avoiding loss of trainability. We demonstrate that our scheduler can improve the accuracy maintained by previously proposed approaches, such as concatenated ReLU (CReLU), Wasserstein regularizer, and L2 weight decay. Surprisingly, our scheduler produces adaptive step-size trajectories that, without tuning, mirror the manually engineered step-size decay schedules.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impossibility of Inverse Permutation Learning in Transformer Models</title>
<link>https://arxiv.org/abs/2509.24125</link>
<guid>https://arxiv.org/abs/2509.24125</guid>
<content:encoded><![CDATA[
arXiv:2509.24125v3 Announce Type: replace 
Abstract: In this technical note, we study the problem of inverse permutation learning in decoder-only transformers. Given a permutation and a string to which that permutation has been applied, the model is tasked with producing the original (``canonical'') string. We argue that this task models a natural robustness property across a variety of reasoning tasks, including long-context retrieval, multiple choice QA and in-context learning. Our primary contribution is an impossibility result: we show that an arbitrary depth, decoder-only transformer cannot learn this task. This result concerns the expressive capacity of decoder-only transformer models and is agnostic to training dynamics or sample complexity. We give a pair of alternative constructions under which inverse permutation learning is feasible. The first of these highlights the fundamental role of the causal attention mask, and reveals a gap between the expressivity of encoder-decoder transformers and the more popular decoder-only architecture. The latter result is more surprising: we show that simply padding the input with ``scratch tokens" yields a construction under which inverse permutation learning is possible. We conjecture that this may suggest an alternative mechanism by which chain-of-thought prompting or, more generally, intermediate ``thinking'' tokens can enable reasoning in large language models, even when these tokens encode no meaningful semantic information (e.g., the results of intermediate computations).
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InfMasking: Unleashing Synergistic Information by Contrastive Multimodal Interactions</title>
<link>https://arxiv.org/abs/2509.25270</link>
<guid>https://arxiv.org/abs/2509.25270</guid>
<content:encoded><![CDATA[
arXiv:2509.25270v3 Announce Type: replace 
Abstract: In multimodal representation learning, synergistic interactions between modalities not only provide complementary information but also create unique outcomes through specific interaction patterns that no single modality could achieve alone. Existing methods may struggle to effectively capture the full spectrum of synergistic information, leading to suboptimal performance in tasks where such interactions are critical. This is particularly problematic because synergistic information constitutes the fundamental value proposition of multimodal representation. To address this challenge, we introduce InfMasking, a contrastive synergistic information extraction method designed to enhance synergistic information through an Infinite Masking strategy. InfMasking stochastically occludes most features from each modality during fusion, preserving only partial information to create representations with varied synergistic patterns. Unmasked fused representations are then aligned with masked ones through mutual information maximization to encode comprehensive synergistic information. This infinite masking strategy enables capturing richer interactions by exposing the model to diverse partial modality combinations during training. As computing mutual information estimates with infinite masking is computationally prohibitive, we derive an InfMasking loss to approximate this calculation. Through controlled experiments, we demonstrate that InfMasking effectively enhances synergistic information between modalities. In evaluations on large-scale real-world datasets, InfMasking achieves state-of-the-art performance across seven benchmarks. Code is released at https://github.com/brightest66/InfMasking.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Three Regimes of Offline-to-Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01460</link>
<guid>https://arxiv.org/abs/2510.01460</guid>
<content:encoded><![CDATA[
arXiv:2510.01460v2 Announce Type: replace 
Abstract: Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PEAR: Planner-Executor Agent Robustness Benchmark</title>
<link>https://arxiv.org/abs/2510.07505</link>
<guid>https://arxiv.org/abs/2510.07505</guid>
<content:encoded><![CDATA[
arXiv:2510.07505v3 Announce Type: replace 
Abstract: Large Language Model (LLM)-based Multi-Agent Systems (MAS) have emerged as a powerful paradigm for tackling complex, multi-step tasks across diverse domains. However, despite their impressive capabilities, MAS remain susceptible to adversarial manipulation. Existing studies typically examine isolated attack surfaces or specific scenarios, leaving a lack of holistic understanding of MAS vulnerabilities. To bridge this gap, we introduce PEAR, a benchmark for systematically evaluating both the utility and vulnerability of planner-executor MAS. While compatible with various MAS architectures, our benchmark focuses on the planner-executor structure, which is a practical and widely adopted design. Through extensive experiments, we find that (1) a weak planner degrades overall clean task performance more severely than a weak executor; (2) while a memory module is essential for the planner, having a memory module for the executor does not impact the clean task performance; (3) there exists a trade-off between task performance and robustness; and (4) attacks targeting the planner are particularly effective at misleading the system. These findings offer actionable insights for enhancing the robustness of MAS and lay the groundwork for principled defenses in multi-agent settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</title>
<link>https://arxiv.org/abs/2510.09660</link>
<guid>https://arxiv.org/abs/2510.09660</guid>
<content:encoded><![CDATA[
arXiv:2510.09660v4 Announce Type: replace 
Abstract: Diffusion Probabilistic Models (DPMs) have achieved strong generative performance, yet their inductive biases remain largely implicit. In this work, we aim to build inductive biases into the training and sampling of diffusion models to better accommodate the target distribution of the data to model. We introduce an anisotropic noise operator that shapes these biases by replacing the isotropic forward covariance with a structured, frequency-diagonal covariance. This operator unifies band-pass masks and power-law weightings, allowing us to emphasize or suppress designated frequency bands, while keeping the forward process Gaussian. We refer to this as Spectrally Anisotropic Gaussian Diffusion (SAGD). In this work, we derive the score relation for anisotropic forward covariances and show that, under full support, the learned score converges to the true data score as $t\!\to\!0$, while anisotropy reshapes the probability-flow path from noise to data. Empirically, we show the induced anisotropy outperforms standard diffusion across several vision datasets, and enables selective omission: learning while ignoring known corruptions confined to specific bands. Together, these results demonstrate that carefully designed anisotropic forward noise provides a simple, yet principled, handle to tailor inductive bias in DPMs.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Generic Machine Learning Framework for Radio Frequency Fingerprinting</title>
<link>https://arxiv.org/abs/2510.09775</link>
<guid>https://arxiv.org/abs/2510.09775</guid>
<content:encoded><![CDATA[
arXiv:2510.09775v2 Announce Type: replace 
Abstract: Fingerprinting radio frequency (RF) emitters typically involves finding unique characteristics that are featured in their received signal. These fingerprints are nuanced, but sufficiently detailed, motivating the pursuit of methods that can successfully extract them. The downstream task that requires the most meticulous RF fingerprinting (RFF) is known as specific emitter identification (SEI), which entails recognising each individual transmitter. RFF and SEI have a long history, with numerous defence and civilian applications such as signal intelligence, electronic surveillance, physical-layer authentication of wireless devices, to name a few. In recent years, data-driven RFF approaches have become popular due to their ability to automatically learn intricate fingerprints. They generally deliver superior performance when compared to traditional RFF techniques that are often labour-intensive, inflexible, and only applicable to a particular emitter type or transmission scheme. In this paper, we present a generic and versatile machine learning (ML) framework for data-driven RFF with several popular downstream tasks such as SEI, data association (EDA) and RF emitter clustering (RFEC). It is emitter-type agnostic. We then demonstrate the introduced framework for several tasks using real RF datasets for spaceborne surveillance, signal intelligence and countering drones applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</title>
<link>https://arxiv.org/abs/2510.13865</link>
<guid>https://arxiv.org/abs/2510.13865</guid>
<content:encoded><![CDATA[
arXiv:2510.13865v5 Announce Type: replace 
Abstract: We introduce the Deep Edge Filter, a novel approach that applies high-pass filtering to deep neural network features to improve model generalizability. Our method is motivated by our hypothesis that neural networks encode task-relevant semantic information in high-frequency components while storing domain-specific biases in low-frequency components of deep features. By subtracting low-pass filtered outputs from original features, our approach isolates generalizable representations while preserving architectural integrity. Experimental results across diverse domains such as Vision, Text, 3D, and Audio demonstrate consistent performance improvements regardless of model architecture and data modality. Analysis reveals that our method induces feature sparsification and effectively isolates high-frequency components, providing empirical validation of our core hypothesis. The code is available at https://github.com/dongkwani/DeepEdgeFilter.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References</title>
<link>https://arxiv.org/abs/2510.14719</link>
<guid>https://arxiv.org/abs/2510.14719</guid>
<content:encoded><![CDATA[
arXiv:2510.14719v2 Announce Type: replace 
Abstract: Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colliding with Adversaries at ECML-PKDD 2025 Model Robustness Competition 1st Prize Solution</title>
<link>https://arxiv.org/abs/2510.16443</link>
<guid>https://arxiv.org/abs/2510.16443</guid>
<content:encoded><![CDATA[
arXiv:2510.16443v2 Announce Type: replace 
Abstract: This report presents the winning solution for Task 2 of Colliding with Adversaries: A Challenge on Robust Learning in High Energy Physics Discovery at ECML-PKDD 2025. The goal of the challenge was to design and train a robust ANN-based model capable of achieving high accuracy in a binary classification task on both clean and adversarial data generated with the Random Distribution Shuffle Attack (RDSA). Our solution consists of two components: a data generation phase and a robust model training phase. In the first phase, we produced 15 million artificial training samples using a custom methodology derived from Random Distribution Shuffle Attack (RDSA). In the second phase, we introduced a robust architecture comprising (i)a Feature Embedding Block with shared weights among features of the same type and (ii)a Dense Fusion Tail responsible for the final prediction. Training this architecture on our adversarial dataset achieved a mixed accuracy score of 80\%, exceeding the second-place solution by two percentage points.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems</title>
<link>https://arxiv.org/abs/2510.17281</link>
<guid>https://arxiv.org/abs/2510.17281</guid>
<content:encoded><![CDATA[
arXiv:2510.17281v3 Announce Type: replace 
Abstract: Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2510.25781</link>
<guid>https://arxiv.org/abs/2510.25781</guid>
<content:encoded><![CDATA[
arXiv:2510.25781v2 Announce Type: replace 
Abstract: The so-called Kolmogorov-Arnold Networks (KANs), whose design is merely inspired, rather than dictated, by the Kolmogorov superposition theorem, have emerged as a promising alternative to traditional Multilayer Perceptrons (MLPs). This review provides a systematic and comprehensive overview of the rapidly expanding KAN landscape. By collecting and categorizing a large set of open-source implementations, we map the vibrant ecosystem supporting modern KAN development. We organize the review around four core themes:
  (i) presenting a precise history of Kolmogorov's superposition theory toward neural-network formulations; (ii) establishing the formal equivalence between KANs and MLPs; (iii) analyzing the critical role of basis functions; and (iv) organizing recent advancements in accuracy, efficiency, regularization, and convergence.
  Finally, we provide a practical Choose-Your-KAN guide to assist practitioners in selecting appropriate architectures, and we close by identifying current research gaps and future directions. The associated GitHub repository (https://github.com/AmirNoori68/kan-review) complements this paper and serves as a structured reference for ongoing KAN research.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAESTRO: Multi-Agent Environment Shaping through Task and Reward Optimization</title>
<link>https://arxiv.org/abs/2511.19253</link>
<guid>https://arxiv.org/abs/2511.19253</guid>
<content:encoded><![CDATA[
arXiv:2511.19253v2 Announce Type: replace 
Abstract: Cooperative Multi-Agent Reinforcement Learning (MARL) faces two major design bottlenecks: crafting dense reward functions and constructing curricula that avoid local optima in high-dimensional, non-stationary environments. Existing approaches rely on fixed heuristics or use Large Language Models (LLMs) directly in the control loop, which is costly and unsuitable for real-time systems. We propose MAESTRO (Multi-Agent Environment Shaping through Task and Reward Optimization), a framework that moves the LLM outside the execution loop and uses it as an offline training architect. MAESTRO introduces two generative components: (i) a semantic curriculum generator that creates diverse, performance-driven traffic scenarios, and (ii) an automated reward synthesizer that produces executable Python reward functions adapted to evolving curriculum difficulty. These components guide a standard MARL backbone (MADDPG) without increasing inference cost at deployment. We evaluate MAESTRO on large-scale traffic signal control (Hangzhou, 16 intersections) and conduct controlled ablations. Results show that combining LLM-generated curricula with LLM-generated reward shaping yields improved performance and stability. Across four seeds, the full system achieves +4.0% higher mean return (163.26 vs. 156.93) and 2.2% better risk-adjusted performance (Sharpe 1.53 vs. 0.70) over a strong curriculum baseline. These findings highlight LLMs as effective high-level designers for cooperative MARL training.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spectral Concentration at the Edge of Stability: Information Geometry of Kernel Associative Memory</title>
<link>https://arxiv.org/abs/2511.23083</link>
<guid>https://arxiv.org/abs/2511.23083</guid>
<content:encoded><![CDATA[
arXiv:2511.23083v2 Announce Type: replace 
Abstract: High-capacity kernel Hopfield networks exhibit a \textit{Ridge of Optimization} characterized by extreme stability. While previously linked to \textit{Spectral Concentration}, its origin remains elusive. Here, we analyze the network dynamics on a statistical manifold, revealing that the Ridge corresponds to the Edge of Stability, a critical boundary where the Fisher Information Matrix becomes singular. We demonstrate that the apparent Euclidean force antagonism is a manifestation of \textit{Dual Equilibrium} in the Riemannian space. This unifies learning dynamics and capacity via the Minimum Description Length principle, offering a geometric theory of self-organized criticality.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing the Plasticity-Stability Dilemma in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01034</link>
<guid>https://arxiv.org/abs/2512.01034</guid>
<content:encoded><![CDATA[
arXiv:2512.01034v2 Announce Type: replace 
Abstract: Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low-Dimensional Structure in the Space of Language Representations is Reflected in Brain Responses</title>
<link>https://arxiv.org/abs/2106.05426</link>
<guid>https://arxiv.org/abs/2106.05426</guid>
<content:encoded><![CDATA[
arXiv:2106.05426v5 Announce Type: replace-cross 
Abstract: How related are the representations learned by neural language models, translation models, and language tagging tasks? We answer this question by adapting an encoder-decoder transfer learning method from computer vision to investigate the structure among 100 different feature spaces extracted from hidden representations of various networks trained on language tasks. This method reveals a low-dimensional structure where language models and translation models smoothly interpolate between word embeddings, syntactic and semantic tasks, and future word embeddings. We call this low-dimensional structure a language representation embedding because it encodes the relationships between representations needed to process language for a variety of NLP tasks. We find that this representation embedding can predict how well each individual feature space maps to human brain responses to natural language stimuli recorded using fMRI. Additionally, we find that the principal dimension of this structure can be used to create a metric which highlights the brain's natural language processing hierarchy. This suggests that the embedding captures some part of the brain's natural language representation structure.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Free Privacy Leakage in Federated Language Models through Selective Weight Tampering</title>
<link>https://arxiv.org/abs/2310.16152</link>
<guid>https://arxiv.org/abs/2310.16152</guid>
<content:encoded><![CDATA[
arXiv:2310.16152v4 Announce Type: replace-cross 
Abstract: Federated learning (FL) has become a key component in various language modeling applications such as machine translation, next-word prediction, and medical record analysis. These applications are trained on datasets from many FL participants that often include privacy-sensitive data, such as healthcare records, phone/credit card numbers, login credentials, etc. Although FL enables computation without necessitating clients to share their raw data, existing works show that privacy leakage is still probable in federated language models. In this paper, we present two novel findings on the leakage of privacy-sensitive user data from federated large language models without requiring access to gradients. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that a malicious FL participant can aggravate the leakage by tampering with the model's selective weights that are responsible for memorizing the sensitive training data of some other clients, even without any cooperation from the server. Our best-performing method increases the membership inference recall by 29% and achieves up to 71% private data reconstruction, evidently outperforming existing attacks that consider much stronger adversary capabilities. Lastly, we recommend a balanced suite of techniques for an FL client to defend against such privacy risk.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding World or Predicting Future? A Comprehensive Survey of World Models</title>
<link>https://arxiv.org/abs/2411.14499</link>
<guid>https://arxiv.org/abs/2411.14499</guid>
<content:encoded><![CDATA[
arXiv:2411.14499v4 Announce Type: replace-cross 
Abstract: The concept of world models has garnered significant attention due to advancements in multimodal large language models such as GPT-4 and video generation models such as Sora, which are central to the pursuit of artificial general intelligence. This survey offers a comprehensive review of the literature on world models. Generally, world models are regarded as tools for either understanding the present state of the world or predicting its future dynamics. This review presents a systematic categorization of world models, emphasizing two primary functions: (1) constructing internal representations to understand the mechanisms of the world, and (2) predicting future states to simulate and guide decision-making. Initially, we examine the current progress in these two categories. We then explore the application of world models in key domains, including generative games, autonomous driving, robotics, and social simulacra, with a focus on how each domain utilizes these aspects. Finally, we outline key challenges and provide insights into potential future research directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/World-Model.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Pricing in the Linear Valuation Model using Shape Constraints</title>
<link>https://arxiv.org/abs/2502.05776</link>
<guid>https://arxiv.org/abs/2502.05776</guid>
<content:encoded><![CDATA[
arXiv:2502.05776v4 Announce Type: replace-cross 
Abstract: We propose a shape-constrained approach to dynamic pricing for censored data in the linear valuation model eliminating the need for tuning parameters commonly required by existing methods. Previous works have addressed the challenge of unknown market noise distribution $F_0$ using strategies ranging from kernel methods to reinforcement learning algorithms, such as bandit techniques and upper confidence bounds (UCB), under the assumption that $F_0$ satisfies Lipschitz (or stronger) conditions. In contrast, our method relies on isotonic regression under the weaker assumption that $F_0$ is $\alpha$-H\"older continuous for some $\alpha \in (0,1]$, for which we derive a regret upper bound. Simulations and experiments with real-world data obtained by Welltower Inc (a major healthcare Real Estate Investment Trust) consistently demonstrate that our method attains lower empirical regret in comparison to several existing methods in the literature while offering the advantage of being tuning-parameter free.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Finite-Sample Analysis of Policy Evaluation for Robust Average Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.16816</link>
<guid>https://arxiv.org/abs/2502.16816</guid>
<content:encoded><![CDATA[
arXiv:2502.16816v4 Announce Type: replace-cross 
Abstract: We present the first finite-sample analysis of policy evaluation in robust average-reward Markov Decision Processes (MDPs). Prior work in this setting have established only asymptotic convergence guarantees, leaving open the question of sample complexity. In this work, we address this gap by showing that the robust Bellman operator is a contraction under a carefully constructed semi-norm, and developing a stochastic approximation framework with controlled bias. Our approach builds upon Multi-Level Monte Carlo (MLMC) techniques to estimate the robust Bellman operator efficiently. To overcome the infinite expected sample complexity inherent in standard MLMC, we introduce a truncation mechanism based on a geometric distribution, ensuring a finite expected sample complexity while maintaining a small bias that decays exponentially with the truncation level. Our method achieves the order-optimal sample complexity of $\tilde{\mathcal{O}}(\epsilon^{-2})$ for robust policy evaluation and robust average reward estimation, marking a significant advancement in robust reinforcement learning theory.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Text to Image in Diffusion Models is Easier Than You Think</title>
<link>https://arxiv.org/abs/2503.08250</link>
<guid>https://arxiv.org/abs/2503.08250</guid>
<content:encoded><![CDATA[
arXiv:2503.08250v5 Announce Type: replace-cross 
Abstract: While recent advancements in generative modeling have significantly improved text-image alignment, some residual misalignment between text and image representations still remains. Some approaches address this issue by fine-tuning models in terms of preference optimization, etc., which require tailored datasets. Orthogonal to these methods, we revisit the challenge from the perspective of representation alignment-an approach that has gained popularity with the success of REPresentation Alignment (REPA). We first argue that conventional text-to-image (T2I) diffusion models, typically trained on paired image and text data (i.e., positive pairs) by minimizing score matching or flow matching losses, is suboptimal from the standpoint of representation alignment. Instead, a better alignment can be achieved through contrastive learning that leverages existing dataset as both positive and negative pairs. To enable efficient alignment with pretrained models, we propose SoftREPA- a lightweight contrastive fine-tuning strategy that leverages soft text tokens for representation alignment. This approach improves alignment with minimal computational overhead by adding fewer than 1M trainable parameters to the pretrained model. Our theoretical analysis demonstrates that our method explicitly increases the mutual information between text and image representations, leading to enhanced semantic consistency. Experimental results across text-to-image generation and text-guided image editing tasks validate the effectiveness of our approach in improving the semantic consistency of T2I generative models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constrained Discrete Diffusion</title>
<link>https://arxiv.org/abs/2503.09790</link>
<guid>https://arxiv.org/abs/2503.09790</guid>
<content:encoded><![CDATA[
arXiv:2503.09790v3 Announce Type: replace-cross 
Abstract: Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing Constrained Discrete Diffusion (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves zero constraint violations in a diverse array of tasks while preserving fluency, novelty, and coherence while outperforming autoregressive and existing discrete diffusion approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revenue Maximization Under Sequential Price Competition Via The Estimation Of s-Concave Demand Functions</title>
<link>https://arxiv.org/abs/2503.16737</link>
<guid>https://arxiv.org/abs/2503.16737</guid>
<content:encoded><![CDATA[
arXiv:2503.16737v5 Announce Type: replace-cross 
Abstract: We consider price competition among multiple sellers over a selling horizon of $T$ periods. In each period, sellers simultaneously offer their prices (which are made public) and subsequently observe their respective demand (not made public). The demand function of each seller depends on all sellers' prices through a private, unknown, and nonlinear relationship. We propose a dynamic pricing policy that uses semi-parametric least-squares estimation and show that when the sellers employ our policy, their prices converge at a rate of $O(T^{-1/7})$ to the Nash equilibrium prices that sellers would reach if they were fully informed. Each seller incurs a regret of $O(T^{5/7})$ relative to a dynamic benchmark policy. A theoretical contribution of our work is proving the existence of equilibrium under shape-constrained demand functions via the concept of $s$-concavity and establishing regret bounds of our proposed policy. Technically, we also establish new concentration results for the least squares estimator under shape constraints. Our findings offer significant insights into dynamic competition-aware pricing and contribute to the broader study of non-parametric learning in strategic decision-making.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Transformed Gaussian Process State-Space Models for Non-Stationary High-Dimensional Dynamical Systems</title>
<link>https://arxiv.org/abs/2503.18309</link>
<guid>https://arxiv.org/abs/2503.18309</guid>
<content:encoded><![CDATA[
arXiv:2503.18309v4 Announce Type: replace-cross 
Abstract: Gaussian process state-space models (GPSSMs) offer a principled framework for learning and inference in nonlinear dynamical systems with uncertainty quantification. However, existing GPSSMs are limited by the use of multiple independent stationary Gaussian processes (GPs), leading to prohibitive computational and parametric complexity in high-dimensional settings and restricted modeling capacity for non-stationary dynamics. To address these challenges, we propose an efficient transformed Gaussian process state-space model (ETGPSSM) for scalable and flexible modeling of high-dimensional, non-stationary dynamical systems. Specifically, our ETGPSSM integrates a single shared GP with input-dependent normalizing flows, yielding an expressive non-stationary implicit process prior that can capture complex transition dynamics while significantly reducing model complexity. For the inference of the implicit process, we develop a variational inference algorithm that jointly approximates the posterior over the underlying GP and the neural network parameters defining the normalizing flows. To avoid explicit variational parameterization of the latent states, we further incorporate the ensemble Kalman filter (EnKF) into the variational framework, enabling accurate and efficient state estimation. Extensive empirical evaluations on synthetic and real-world datasets demonstrate the superior performance of our ETGPSSM in system dynamics learning, high-dimensional state estimation, and time-series forecasting, outperforming existing GPSSMs and neural network-based SSMs in terms of computational efficiency and accuracy.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revealing economic facts: LLMs know more than they say</title>
<link>https://arxiv.org/abs/2505.08662</link>
<guid>https://arxiv.org/abs/2505.08662</guid>
<content:encoded><![CDATA[
arXiv:2505.08662v2 Announce Type: replace-cross 
Abstract: We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Cross-Attention Interaction in Transformers for Interpreting TCR-pMHC Binding</title>
<link>https://arxiv.org/abs/2507.03197</link>
<guid>https://arxiv.org/abs/2507.03197</guid>
<content:encoded><![CDATA[
arXiv:2507.03197v2 Announce Type: replace-cross 
Abstract: CD8+ "killer" T cells and CD4+ "helper" T cells play a central role in the adaptive immune system by recognizing antigens presented by Major Histocompatibility Complex (pMHC) molecules via T Cell Receptors (TCRs). Modeling binding between T cells and the pMHC complex is fundamental to understanding basic mechanisms of human immune response as well as in developing therapies. While transformer-based models such as TULIP have achieved impressive performance in this domain, their black-box nature precludes interpretability and thus limits a deeper mechanistic understanding of T cell response. Most existing post-hoc explainable AI (XAI) methods are confined to encoder-only, co-attention, or model-specific architectures and cannot handle encoder-decoder transformers used in TCR-pMHC modeling. To address this gap, we propose Quantifying Cross-Attention Interaction (QCAI), a new post-hoc method designed to interpret the cross-attention mechanisms in transformer decoders. Quantitative evaluation is a challenge for XAI methods; we have compiled TCR-XAI, a benchmark consisting of 274 experimentally determined TCR-pMHC structures to serve as ground truth for binding. Using these structures we compute physical distances between relevant amino acid residues in the TCR-pMHC interaction region and evaluate how well our method and others estimate the importance of residues in this region across the dataset. We show that QCAI achieves state-of-the-art performance on both interpretability and prediction accuracy under the TCR-XAI benchmark.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAFT: Structure-Aware Fine-Tuning of LLMs for AMR-to-Text Generation</title>
<link>https://arxiv.org/abs/2507.13381</link>
<guid>https://arxiv.org/abs/2507.13381</guid>
<content:encoded><![CDATA[
arXiv:2507.13381v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly applied to tasks involving structured inputs such as graphs. Abstract Meaning Representations (AMRs), which encode rich semantics as directed graphs, offer a rigorous testbed for evaluating LLMs on text generation from such structures. Yet, current methods often arbitrarily linearize AMRs, discarding key structural cues, or rely on architectures incompatible with standard LLMs. We introduce SAFT, a structure-aware fine-tuning approach that injects graph topology into pretrained LLMs without architectural changes. We compute direction-sensitive positional encodings from the magnetic Laplacian of transformed AMRs and project them into the embedding space of the LLM. While possibly applicable to any graph-structured inputs, we focus on AMR-to-text generation as a representative and challenging benchmark. SAFT sets a new state-of-the-art on AMR 3.0 with a 3.5 BLEU improvement over baselines. Gains scale with graph complexity, highlighting the value of structure-aware representations in enhancing LLM performance. SAFT offers a general and effective pathway for bridging structured data and language models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured quantum learning via em algorithm for Boltzmann machines</title>
<link>https://arxiv.org/abs/2507.21569</link>
<guid>https://arxiv.org/abs/2507.21569</guid>
<content:encoded><![CDATA[
arXiv:2507.21569v2 Announce Type: replace-cross 
Abstract: Quantum Boltzmann machines (QBMs) are generative models with potential advantages in quantum machine learning, yet their training is fundamentally limited by the barren plateau problem, where gradients vanish exponentially with system size. We introduce a quantum version of the em algorithm, an information-geometric generalization of the classical Expectation-Maximization method, which circumvents gradient-based optimization on non-convex functions. Implemented on a semi-quantum restricted Boltzmann machine (sqRBM) -- a hybrid architecture with quantum effects confined to the hidden layer -- our method achieves stable learning and outperforms gradient descent on multiple benchmark datasets. These results establish a structured and scalable alternative to gradient-based training in QML, offering a pathway to mitigate barren plateaus and enhance quantum generative modeling.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface Temperature Estimation via Spatio-Temporal Fusion</title>
<link>https://arxiv.org/abs/2508.06485</link>
<guid>https://arxiv.org/abs/2508.06485</guid>
<content:encoded><![CDATA[
arXiv:2508.06485v2 Announce Type: replace-cross 
Abstract: Urbanization, climate change, and agricultural stress are increasing the demand for precise and timely environmental monitoring. Land Surface Temperature (LST) is a key variable in this context and is retrieved from remote sensing satellites. However, these systems face a trade-off between spatial and temporal resolution. While spatio-temporal fusion methods offer promising solutions, few have addressed the estimation of daily LST at 10 m resolution. In this study, we present WGAST, a weakly-supervised generative network for daily 10 m LST estimation via spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning framework designed for this task. It adopts a conditional generative adversarial architecture, with a generator composed of four stages: feature extraction, fusion, LST reconstruction, and noise suppression. The first stage employs a set of encoders to extract multi-level latent representations from the inputs, which are then fused in the second stage using cosine similarity, normalization, and temporal attention mechanisms. The third stage decodes the fused features into high-resolution LST, followed by a Gaussian filter to suppress high-frequency noise. Training follows a weakly supervised strategy based on physical averaging principles and reinforced by a PatchGAN discriminator. Experiments demonstrate that WGAST outperforms existing methods in both quantitative and qualitative evaluations. Compared to the best-performing baseline, on average, WGAST reduces RMSE by 17.05% and improves SSIM by 4.22%. Furthermore, WGAST effectively captures fine-scale thermal patterns, as validated against near-surface air temperature measurements from 33 near-ground sensors. The code is available at https://github.com/Sofianebouaziz1/WGAST.git.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugLift: Uncertainty Aware Depth Descriptors for Robust 2D to 3D Pose Lifting</title>
<link>https://arxiv.org/abs/2508.07112</link>
<guid>https://arxiv.org/abs/2508.07112</guid>
<content:encoded><![CDATA[
arXiv:2508.07112v3 Announce Type: replace-cross 
Abstract: Lifting based 3D human pose estimators infer 3D joints from 2D keypoints, but often struggle to generalize to real world settings with noisy 2D detections. We revisit the input to lifting and propose AugLift, a simple augmentation of standard lifting that enriches each 2D keypoint (x, y) with an Uncertainty Aware Depth Descriptor (UADD). We run a single off the shelf monocular depth estimator to obtain a depth map, and for every keypoint with detector confidence c we extract depth statistics from its confidence scaled neighborhood, forming a compact, interpretable UADD (c, d, d_min, d_max) that captures both local geometry and reliability. AugLift is modular, requires no new sensors or architectural changes, and integrates by expanding the input layer of existing lifting models.
  Across four datasets and four lifting architectures, AugLift boosts cross dataset (out of distribution) performance on unseen data by an average of 10.1 percent, while also improving in distribution performance by 4.0 percent as measured by MPJPE. A post hoc analysis clarifies when and why it helps: gains are largest on novel poses and significantly occluded joints, where depth statistics resolve front back ambiguities while confidence calibrates the spatial neighborhoods from which they are drawn. We also study interaction with recent image feature lifting methods and find the signals are complementary: adding UADD to image conditioned lifting yields both ID and OOD gains. A learned depth feature extension (AugLiftV2) improves performance further while trading off interpretability. Together, these results indicate that lightweight, confidence aware depth cues are a powerful plug in for robust 2D to 3D pose lifting.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Secant Alignment for Score-Based Density Ratio Estimation</title>
<link>https://arxiv.org/abs/2509.04852</link>
<guid>https://arxiv.org/abs/2509.04852</guid>
<content:encoded><![CDATA[
arXiv:2509.04852v3 Announce Type: replace-cross 
Abstract: Estimating density ratios has become increasingly important with the recent rise of score-based and diffusion-inspired methods. However, current tangent-based approaches rely on a high-variance learning objective, which leads to unstable training and costly numerical integration during inference. We propose \textit{Interval-annealed Secant Alignment Density Ratio Estimation (ISA-DRE)}, a score-based framework along diffusion interpolants that replaces the instantaneous tangent with its interval integral, the secant, as the learning target. We show theoretically that the secant is a provably lower variance and smoother target for neural approximation, and also a strictly more general representation that contains the tangent as the infinitesimal limit. To make secant learning feasible, we introduce the \textit{Secant Alignment Identity (SAI)} to enforce self consistency between secant and tangent representations, and \textit{Contraction Interval Annealing (CIA)} to ensure stable convergence. Empirically, this stability-first formulation produces high efficiency and accuracy. ISA-DRE achieves comparable or superior results with fewer function evaluations, demonstrating robustness under large distribution discrepancies and effectively mitigating the density-chasm problem.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Imitative Membership Inference Attack</title>
<link>https://arxiv.org/abs/2509.06796</link>
<guid>https://arxiv.org/abs/2509.06796</guid>
<content:encoded><![CDATA[
arXiv:2509.06796v2 Announce Type: replace-cross 
Abstract: A Membership Inference Attack (MIA) assesses how much a target machine learning model reveals about its training data by determining whether specific query instances were part of the training set. State-of-the-art MIAs rely on training hundreds of shadow models that are independent of the target model, leading to significant computational overhead. In this paper, we introduce Imitative Membership Inference Attack (IMIA), which employs a novel imitative training technique to strategically construct a small number of target-informed imitative models that closely replicate the target model's behavior for inference. Extensive experimental results demonstrate that IMIA substantially outperforms existing MIAs in various attack settings while only requiring less than 5% of the computational cost of state-of-the-art approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Next-Generation Reservoir Computing for Dynamical Inference</title>
<link>https://arxiv.org/abs/2509.11338</link>
<guid>https://arxiv.org/abs/2509.11338</guid>
<content:encoded><![CDATA[
arXiv:2509.11338v2 Announce Type: replace-cross 
Abstract: We present a simple and scalable implementation of next-generation reservoir computing (NGRC) for modeling dynamical systems from time-series data. The method uses a pseudorandom nonlinear projection of time-delay embedded inputs, allowing the feature-space dimension to be chosen independently of the observation size and offering a flexible alternative to polynomial-based NGRC projections. We demonstrate the approach on benchmark tasks, including attractor reconstruction and bifurcation diagram estimation, using partial and noisy measurements. We further show that small amounts of measurement noise during training act as an effective regularizer, improving long-term autonomous stability compared to standard regression alone. Across all tests, the models remain stable over long rollouts and generalize beyond the training data. The framework offers explicit control of system state during prediction, and these properties make NGRC a natural candidate for applications such as surrogate modeling and digital-twin applications.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepMech: A Machine Learning Framework for Chemical Reaction Mechanism Prediction</title>
<link>https://arxiv.org/abs/2509.15872</link>
<guid>https://arxiv.org/abs/2509.15872</guid>
<content:encoded><![CDATA[
arXiv:2509.15872v2 Announce Type: replace-cross 
Abstract: Prediction of complete step-by-step chemical reaction mechanisms (CRMs) remains a major challenge. Whereas the traditional approaches in CRM tasks rely on expert-driven experiments or costly quantum chemical computations, contemporary deep learning (DL) alternatives ignore key intermediates and mechanistic steps and often suffer from hallucinations. We present DeepMech, an interpretable graph-based DL framework employing atom- and bond-level attention, guided by generalized templates of mechanistic operations (TMOps), to generate CRMs. Trained on our curated ReactMech dataset (~30K CRMs with 100K atom-mapped and mass-balanced elementary steps), DeepMech achieves 98.98+/-0.12% accuracy in predicting elementary steps and 95.94+/-0.21% in complete CRM tasks, besides maintaining high fidelity even in out-of-distribution scenarios as well as in predicting side and/or byproducts. Extension to multistep CRMs relevant to prebiotic chemistry, demonstrates the ability of DeepMech in effectively reconstructing 2 pathways from simple primordial substrates to complex biomolecules such as serine and aldopentose. Attention analysis identifies reactive atoms/bonds in line with chemical intuition, rendering our model interpretable and suitable for reaction design.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Web API Integration Code Generation</title>
<link>https://arxiv.org/abs/2509.20172</link>
<guid>https://arxiv.org/abs/2509.20172</guid>
<content:encoded><![CDATA[
arXiv:2509.20172v5 Announce Type: replace-cross 
Abstract: API integration is a cornerstone of our digital infrastructure, enabling software systems to connect and interact. However, as shown by many studies, writing or generating correct code to invoke APIs, particularly web APIs, is challenging. Although large language models (LLMs) have become popular in software development, their effectiveness in automating the generation of web API integration code remains unexplored. In order to address this, we present WAPIIBench, a dataset and evaluation pipeline designed to assess the ability of LLMs to generate web API invocation code. Our experiments with several open-source LLMs reveal that generating API invocations poses a significant challenge, resulting in hallucinated endpoints, incorrect argument usage, and other errors. None of the evaluated open-source models was able to solve more than 40% of the tasks.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BridgeDrive: Diffusion Bridge Policy for Closed-Loop Trajectory Planning in Autonomous Driving</title>
<link>https://arxiv.org/abs/2509.23589</link>
<guid>https://arxiv.org/abs/2509.23589</guid>
<content:encoded><![CDATA[
arXiv:2509.23589v2 Announce Type: replace-cross 
Abstract: Diffusion-based planners have shown great promise for autonomous driving due to their ability to capture multi-modal driving behaviors. However, guiding these models effectively in reactive, closed-loop environments remains a significant challenge. Simple conditioning often fails to provide sufficient guidance in complex and dynamic driving scenarios. Recent work attempts to use typical expert driving behaviors (i.e., anchors) to guide diffusion models but relies on a truncated schedule, which introduces theoretical inconsistencies and can compromise performance. To address this, we introduce BridgeDrive, a novel anchor-guided diffusion bridge policy for closed-loop trajectory planning. Our approach provides a principled diffusion framework that effectively translates anchors into fine-grained trajectory plans, appropriately responding to varying traffic conditions. Our planner is compatible with efficient ODE solvers, a critical factor for real-time autonomous driving deployment. We achieve state-of-the-art performance on the Bench2Drive benchmark, improving the success rate by 7.72% over prior arts.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting and Mitigating Insertion Hallucination in Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2510.08078</link>
<guid>https://arxiv.org/abs/2510.08078</guid>
<content:encoded><![CDATA[
arXiv:2510.08078v4 Announce Type: replace-cross 
Abstract: Video-to-Audio generation has made remarkable strides in automatically synthesizing sound for video. However, existing evaluation metrics, which focus on semantic and temporal alignment, overlook a critical failure mode: models often generate acoustic events, particularly speech and music, that have no corresponding visual source. We term this phenomenon Insertion Hallucination and identify it as a systemic risk driven by dataset biases, such as the prevalence of off-screen sounds, that remains completely undetected by current metrics. To address this challenge, we first develop a systematic evaluation framework that employs a majority-voting ensemble of multiple audio event detectors. We also introduce two novel metrics to quantify the prevalence and severity of this issue: IH@vid (the fraction of videos with hallucinations) and IH@dur (the fraction of hallucinated duration). Building on this, we propose Posterior Feature Correction, a novel training-free inference-time method that mitigates IH. PFC operates in a two-pass process: it first generates an initial audio output to detect hallucinated segments, and then regenerates the audio after masking the corresponding video features at those timestamps. Experiments on several mainstream V2A benchmarks first reveal that state-of-the-art models suffer from severe IH. In contrast, our PFC method reduces both the prevalence and duration of hallucinations by over 50\% on average, without degrading, and in some cases even improving, conventional metrics for audio quality and temporal synchronization. Our work is the first to formally define, systematically measure, and effectively mitigate Insertion Hallucination, paving the way for more reliable and faithful V2A models.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivaDE: Privacy-preserving Data Evaluation for Blockchain-based Data Marketplaces</title>
<link>https://arxiv.org/abs/2510.18109</link>
<guid>https://arxiv.org/abs/2510.18109</guid>
<content:encoded><![CDATA[
arXiv:2510.18109v2 Announce Type: replace-cross 
Abstract: Evaluating the usefulness of data before purchase is essential when obtaining data for high-quality machine learning models, yet both model builders and data providers are often unwilling to reveal their proprietary assets.
  We present PrivaDE, a privacy-preserving protocol that allows a model owner and a data owner to jointly compute a utility score for a candidate dataset without fully exposing model parameters, raw features, or labels. PrivaDE provides strong security against malicious behavior and can be integrated into blockchain-based marketplaces, where smart contracts enforce fair execution and payment. To make the protocol practical, we propose optimizations to enable efficient secure model inference, and a model-agnostic scoring method that uses only a small, representative subset of the data while still reflecting its impact on downstream training. Evaluation shows that PrivaDE performs data evaluation effectively, achieving online runtimes within 15 minutes even for models with millions of parameters.
  Our work lays the foundation for fair and automated data marketplaces in decentralized machine learning ecosystems.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking World-Model Learning</title>
<link>https://arxiv.org/abs/2510.19788</link>
<guid>https://arxiv.org/abs/2510.19788</guid>
<content:encoded><![CDATA[
arXiv:2510.19788v3 Announce Type: replace-cross 
Abstract: Model-learning agents should gather information to learn world models that support many downstream tasks and inferences, such as predicting unobserved states, estimating near- and far-term consequences of actions, planning action sequences, and detecting changes in dynamics. Current methods for learning and evaluating world models diverge from this goal: training and evaluation are anchored to next-frame prediction, and success is scored by reward maximization in the same environment. We propose WorldTest, a protocol to evaluate model-learning agents that separates reward-free interaction from a scored test phase in a different but related environment. WorldTest is open-ended $\unicode{x2014}$ models should support many different tasks unknown ahead of time $\unicode{x2014}$ and agnostic to model representation, allowing comparison across approaches. We instantiated WorldTest with AutumnBench, a suite of 43 interactive grid-world environments and 129 tasks across three families: masked-frame prediction, planning, and predicting changes to the causal dynamics. We compared 517 human participants and three frontier models on AutumnBench. We found that humans outperform the models, and scaling compute improves performance only in some environments but not others. WorldTest provides a novel template $\unicode{x2014}$ reward-free exploration, derived tests, and behavior-based scoring $\unicode{x2014}$ to evaluate what agents learn about environment dynamics, and AutumnBench exposes significant headroom in world-model learning.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Diversity Regularizes Hallucinations in Language Models</title>
<link>https://arxiv.org/abs/2510.20690</link>
<guid>https://arxiv.org/abs/2510.20690</guid>
<content:encoded><![CDATA[
arXiv:2510.20690v2 Announce Type: replace-cross 
Abstract: Language models continue to hallucinate despite increases in parameters, compute, and data. We propose neural diversity -- decorrelated parallel representations -- as a principled mechanism that reduces hallucination rates at fixed parameter and data budgets. While existing mitigation strategies largely target accuracy, we provide the first formal tail bounds for hallucination probability in ensembled language models, reframing it as a second-moment reliability problem and explaining 94.3% of empirical reliability variation seen across parallel configurations. We introduce ND-LoRA (Neural Diversity Low-Rank Adaptation), combining parallel LoRA adapters with Barlow Twins regularization, and reduce hallucinations by up to 25.6% (and 14.6% on average) while preserving general accuracy. Ablations show LoRA adapters and regularization act synergistically, causal interventions prove neurodiversity as the mediating factor and correlational studies indicate scale: a 0.1% neural correlation increase is associated with a 3.8% hallucination increase. Finally, task-dependent optimality emerges: different tasks require different optimal amounts of neurodiversity. Together, our results highlight neural diversity as a third axis of scaling -- orthogonal to parameters and data -- to improve the reliability of language models at fixed budgets.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Guarantees for Variational Inference in the Presence of Even and Elliptical Symmetry</title>
<link>https://arxiv.org/abs/2511.01064</link>
<guid>https://arxiv.org/abs/2511.01064</guid>
<content:encoded><![CDATA[
arXiv:2511.01064v2 Announce Type: replace-cross 
Abstract: We extend several recent results providing symmetry-based guarantees for variational inference (VI) with location-scale families. VI approximates a target density $p$ by the best match $q^*$ in a family $Q$ of tractable distributions that in general does not contain $p$. It is known that VI can recover key properties of $p$, such as its mean and correlation matrix, when $p$ and $Q$ exhibit certain symmetries and $q^*$ is found by minimizing the reverse Kullback-Leibler divergence. We extend these guarantees in two important directions. First, we provide symmetry-based guarantees for $f$-divergences, a broad class that includes the reverse and forward Kullback-Leibler divergences and the $\alpha$-divergences. We highlight properties specific to the reverse Kullback-Leibler divergence under which we obtain our strongest guarantees. Second, we obtain further guarantees for VI when the target density $p$ exhibits even and elliptical symmetries in some but not all of its coordinates. These partial symmetries arise naturally in Bayesian hierarchical models, where the prior induces a challenging geometry but still possesses axes of symmetry. We illustrate these theoretical results in a number of experimental settings.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
<link>https://arxiv.org/abs/2511.02241</link>
<guid>https://arxiv.org/abs/2511.02241</guid>
<content:encoded><![CDATA[
arXiv:2511.02241v3 Announce Type: replace-cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical Properties of Rectified Flow</title>
<link>https://arxiv.org/abs/2511.03193</link>
<guid>https://arxiv.org/abs/2511.03193</guid>
<content:encoded><![CDATA[
arXiv:2511.03193v3 Announce Type: replace-cross 
Abstract: Rectified flow (Liu et al., 2022; Liu, 2022; Wu et al., 2023) is a method for defining a transport map between two distributions, and enjoys popularity in machine learning, although theoretical results supporting the validity of these methods are scant. The rectified flow can be regarded as an approximation to optimal transport, but in contrast to other transport methods that require optimization over a function space, computing the rectified flow only requires standard statistical tools such as regression or density estimation, which we leverage to develop empirical versions of transport maps. We study some structural properties of the rectified flow, including existence, uniqueness, and regularity, as well as the related statistical properties, such as rates of convergence and central limit theorems, for some selected estimators. To do so, we analyze the bounded and unbounded cases separately as each presents unique challenges. In both cases, we are able to establish convergence at faster rates than those for the usual nonparametric regression and density estimation.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HEDN: A Hard-Easy Dual Network with Source Reliability Assessment for Cross-Subject EEG Emotion Recognition</title>
<link>https://arxiv.org/abs/2511.06782</link>
<guid>https://arxiv.org/abs/2511.06782</guid>
<content:encoded><![CDATA[
arXiv:2511.06782v2 Announce Type: replace-cross 
Abstract: Cross-subject electroencephalography (EEG) emotion recognition remains a major challenge in brain-computer interfaces (BCIs) due to substantial inter-subject variability. Multi-Source Domain Adaptation (MSDA) offers a potential solution, but existing MSDA frameworks typically assume equal source quality, leading to negative transfer from low-reliability domains and prohibitive computational overhead due to multi-branch model designs. To address these limitations, we propose the Hard-Easy Dual Network (HEDN), a lightweight reliability-aware MSDA framework. HEDN introduces a novel Source Reliability Assessment (SRA) mechanism that dynamically evaluates the structural integrity of each source domain during training. Based on this assessment, sources are routed to two specialized branches: an Easy Network that exploits high-quality sources to construct fine-grained, structure-aware prototypes for reliable pseudo-label generation, and a Hard Network that utilizes adversarial training to refine and align low-quality sources. Furthermore, a cross-network consistency loss aligns predictions between branches to preserve semantic coherence. Extensive experiments conducted on SEED, SEED-IV, and DEAP datasets demonstrate that HEDN achieves state-of-the-art performance across both cross-subject and cross-dataset evaluation protocols while reducing adaptation complexity.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Classifying Phonotrauma Severity from Vocal Fold Images with Soft Ordinal Regression</title>
<link>https://arxiv.org/abs/2511.09702</link>
<guid>https://arxiv.org/abs/2511.09702</guid>
<content:encoded><![CDATA[
arXiv:2511.09702v2 Announce Type: replace-cross 
Abstract: Phonotrauma refers to vocal fold tissue damage resulting from exposure to forces during voicing. It occurs on a continuum from mild to severe, and treatment options can vary based on severity. Assessment of severity involves a clinician's expert judgment, which is costly and can vary widely in reliability. In this work, we present the first method for automatically classifying phonotrauma severity from vocal fold images. To account for the ordinal nature of the labels, we adopt a widely used ordinal regression framework. To account for label uncertainty, we propose a novel modification to ordinal regression loss functions that enables them to operate on soft labels reflecting annotator rating distributions. Our proposed soft ordinal regression method achieves predictive performance approaching that of clinical experts, while producing well-calibrated uncertainty estimates. By providing an automated tool for phonotrauma severity assessment, our work can enable large-scale studies of phonotrauma, ultimately leading to improved clinical understanding and patient care.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forgetting-MarI: LLM Unlearning via Marginal Information Regularization</title>
<link>https://arxiv.org/abs/2511.11914</link>
<guid>https://arxiv.org/abs/2511.11914</guid>
<content:encoded><![CDATA[
arXiv:2511.11914v2 Announce Type: replace-cross 
Abstract: As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Function-on-Function Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.12783</link>
<guid>https://arxiv.org/abs/2511.12783</guid>
<content:encoded><![CDATA[
arXiv:2511.12783v2 Announce Type: replace-cross 
Abstract: Bayesian optimization (BO) has been widely used to optimize expensive and gradient-free objective functions across various domains. However, existing BO methods have not addressed the objective where both inputs and outputs are functions, which increasingly arise in complex systems as advanced sensing technologies. To fill this gap, we propose a novel function-on-function Bayesian optimization (FFBO) framework. Specifically, we first introduce a function-on-function Gaussian process (FFGP) model with a separable operator-valued kernel to capture the correlations between function-valued inputs and outputs. Compared to existing Gaussian process models, FFGP is modeled directly in the function space. Based on FFGP, we define a scalar upper confidence bound (UCB) acquisition function using a weighted operator-based scalarization strategy. Then, a scalable functional gradient ascent algorithm (FGA) is developed to efficiently identify the optimal function-valued input. We further analyze the theoretical properties of the proposed method. Extensive experiments on synthetic and real-world data demonstrate the superior performance of FFBO over existing approaches.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving a Research Problem in Mathematical Statistics with AI Assistance</title>
<link>https://arxiv.org/abs/2511.18828</link>
<guid>https://arxiv.org/abs/2511.18828</guid>
<content:encoded><![CDATA[
arXiv:2511.18828v2 Announce Type: replace-cross 
Abstract: Over the last few months, AI models including large language models have improved greatly. There are now several documented examples where they have helped professional mathematical scientists prove new results, sometimes even helping resolve known open problems. In this short note, we add another example to the list, by documenting how we were able to solve a previously unsolved research problem in robust mathematical statistics with crucial help from GPT-5. Our problem concerns robust density estimation, where the observations are perturbed by Wasserstein-bounded contaminations. In a previous preprint (Chao and Dobriban, 2023, arxiv:2308.01853v2), we have obtained upper and lower bounds on the minimax optimal estimation error; which were, however, not sharp.
  Starting in October 2025, making significant use of GPT-5 Pro, we were able to derive the minimax optimal error rate (reported in version 3 of the above arxiv preprint). GPT-5 provided crucial help along the way, including by suggesting calculations that we did not think of, and techniques that were not familiar to us, such as the dynamic Benamou-Brenier formulation, for key steps in the analysis. Working with GPT-5 took a few weeks of effort, and we estimate that it could have taken several months to get the same results otherwise. At the same time, there are still areas where working with GPT-5 was challenging: it sometimes provided incorrect references, and glossed over details that sometimes took days of work to fill in. We outline our workflow and steps taken to mitigate issues. Overall, our work can serve as additional documentation for a new age of human-AI collaborative work in mathematical science.
]]></content:encoded>
<pubDate>Thu, 11 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation</title>
<link>https://arxiv.org/abs/2512.03521</link>
<guid>https://arxiv.org/abs/2512.03521</guid>
<content:encoded><![CDATA[
<div> Multimodal Emotion Recognition, Cross-modal Interaction, Low-rank Tensor Factorization, Pareto Gradient Modulator, Training Stability

<br /><br />Summary:  
Multimodal Emotion Recognition in Conversation (MERC) focuses on predicting speakers' emotions by integrating textual, acoustic, and visual cues. Current methods often face difficulties in capturing complex cross-modal interactions or encounter gradient conflicts and unstable training when employing deep architectures. To overcome these challenges, the authors propose the Cross-Space Synergy (CSS) framework, which integrates both a representation component and an optimization component. The representation part, named Synergistic Polynomial Fusion (SPF), utilizes low-rank tensor factorization to efficiently model high-order interactions across different modalities. Meanwhile, the optimization component, called Pareto Gradient Modulator (PGM), guides the training process by directing updates towards Pareto-optimal solutions, thereby alleviating gradient conflicts and enhancing training stability. Experimental results on benchmark datasets IEMOCAP and MELD demonstrate that CSS outperforms existing methods in terms of accuracy and training robustness. This indicates that CSS can effectively handle complex multimodal fusion and optimize training dynamics simultaneously, making it a promising approach for improving Emotion Recognition systems in conversational settings. <div>
arXiv:2512.03521v2 Announce Type: replace-cross 
Abstract: Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models</title>
<link>https://arxiv.org/abs/2512.07843</link>
<guid>https://arxiv.org/abs/2512.07843</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, adaptive parallel reasoning, ThreadWeaver, chain-of-thought, inference latency  

<br /><br />Summary:  
This paper presents ThreadWeaver, a novel framework designed to improve inference efficiency for Large Language Models (LLMs) by enabling adaptive parallel reasoning, addressing the latency bottlenecks caused by inherently sequential decoding in complex tasks. ThreadWeaver achieves accuracy comparable to state-of-the-art sequential chain-of-thought (CoT) approaches while significantly reducing inference time. The framework introduces a two-stage parallel trajectory generator that creates high-quality, large-scale CoT datasets annotated for parallel reasoning, facilitating supervised fine-tuning. It also employs a trie-based training and inference co-design that allows parallel reasoning on any standard autoregressive inference engine without requiring modifications to position embeddings or KV cache structures, thus simplifying deployment. Moreover, ThreadWeaver uses a parallelization-aware reinforcement learning strategy to effectively balance the trade-off between maintaining accuracy and maximizing parallel processing. Evaluated on six challenging mathematical reasoning benchmarks, ThreadWeaver built on the Qwen3-8B model attains an average accuracy of 71.9% and 79.9% accuracy on the AIME24 dataset, matching or exceeding competitive sequential reasoning models. It also delivers up to a 1.53x speedup in token-latency during inference. Overall, ThreadWeaver establishes a new Pareto frontier optimizing both accuracy and efficiency in LLM reasoning tasks. <div>
arXiv:2512.07843v1 Announce Type: new 
Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Space Alignment Matters: The Missing Piece for Inducing Neural Collapse in Long-Tailed Learning</title>
<link>https://arxiv.org/abs/2512.07844</link>
<guid>https://arxiv.org/abs/2512.07844</guid>
<content:encoded><![CDATA[
<div> Neural Collapse, Long-tailed Learning, Equiangular Tight Frame, Feature-Classifier Alignment, Optimal Error Exponent<br /><br />Summary:<br /><br />1. This paper addresses the phenomenon of Neural Collapse (NC), where class feature means and classifier weights align into a simplex equiangular tight frame (ETF) under class-balanced conditions, leading to improved generalization. 2. In long-tailed learning scenarios, where class representation is highly imbalanced, the NC phenomenon fails to emerge properly, causing poor generalization performance. 3. Existing methods mainly focus on enforcing ETF geometry by imposing constraints on either features or classifier weights but neglect the critical misalignment issue between feature and classifier weight spaces. 4. The authors theoretically analyze the negative impact of this misalignment using an optimal error exponent framework, rigorously quantifying its harm to classification performance. 5. Based on these insights, they propose three explicit alignment strategies that can be easily integrated into current long-tail learning approaches without changing model architectures. Extensive experiments conducted on CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets demonstrate consistent improvements over various baselines and establish new state-of-the-art results in long-tailed classification tasks. <div>
arXiv:2512.07844v1 Announce Type: new 
Abstract: Recent studies on Neural Collapse (NC) reveal that, under class-balanced conditions, the class feature means and classifier weights spontaneously align into a simplex equiangular tight frame (ETF). In long-tailed regimes, however, severe sample imbalance tends to prevent the emergence of the NC phenomenon, resulting in poor generalization performance. Current efforts predominantly seek to recover the ETF geometry by imposing constraints on features or classifier weights, yet overlook a critical problem: There is a pronounced misalignment between the feature and the classifier weight spaces. In this paper, we theoretically quantify the harm of such misalignment through an optimal error exponent analysis. Built on this insight, we propose three explicit alignment strategies that plug-and-play into existing long-tail methods without architectural change. Extensive experiments on the CIFAR-10-LT, CIFAR-100-LT, and ImageNet-LT datasets consistently boost examined baselines and achieve the state-of-the-art performances.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CarBench: A Comprehensive Benchmark for Neural Surrogates on High-Fidelity 3D Car Aerodynamics</title>
<link>https://arxiv.org/abs/2512.07847</link>
<guid>https://arxiv.org/abs/2512.07847</guid>
<content:encoded><![CDATA[
<div> Keywords: CarBench, Computational Fluid Dynamics, neural solvers, aerodynamic design, deep learning<br /><br />Summary:<br /><br />This article introduces CarBench, the first comprehensive benchmark specifically designed for large-scale 3D car aerodynamics. CarBench addresses the lack of standardized benchmarks for numerical simulations in engineering design, particularly for automotive aerodynamics. It leverages DrivAerNet++, the largest public dataset containing over 8,000 high-fidelity car simulations, to evaluate state-of-the-art machine learning models. Eleven different architectures are assessed, including neural operator methods like Fourier Neural Operator, geometric deep learning models such as PointNet and PointTransformer, transformer-based neural solvers like Transolver variants and AB-UPT, and implicit field networks such as TripNet. The evaluation covers typical interpolation tasks and challenging cross-category generalization tests where models trained on one car archetype are tested on unseen categories. The analysis includes predictive accuracy, physical consistency of outputs, computational efficiency, and statistical uncertainty quantification through bootstrap resampling. To facilitate further research, the authors open-source the benchmark framework, offering training pipelines, uncertainty estimation tools, and pretrained model weights. This resource establishes a reproducible foundation for data-driven engineering and machine learning applications on large-scale, high-fidelity CFD simulations, aiming to accelerate progress in aerodynamic and engineering design. The benchmark and related code are available at the provided GitHub repository. <div>
arXiv:2512.07847v1 Announce Type: new 
Abstract: Benchmarking has been the cornerstone of progress in computer vision, natural language processing, and the broader deep learning domain, driving algorithmic innovation through standardized datasets and reproducible evaluation protocols. The growing availability of large-scale Computational Fluid Dynamics (CFD) datasets has opened new opportunities for applying machine learning to aerodynamic and engineering design. Yet, despite this progress, there exists no standardized benchmark for large-scale numerical simulations in engineering design. In this work, we introduce CarBench, the first comprehensive benchmark dedicated to large-scale 3D car aerodynamics, performing a large-scale evaluation of state-of-the-art models on DrivAerNet++, the largest public dataset for automotive aerodynamics, containing over 8,000 high-fidelity car simulations. We assess eleven architectures spanning neural operator methods (e.g., Fourier Neural Operator), geometric deep learning (PointNet, RegDGCNN, PointMAE, PointTransformer), transformer-based neural solvers (Transolver, Transolver++, AB-UPT), and implicit field networks (TripNet). Beyond standard interpolation tasks, we perform cross-category experiments in which transformer-based solvers trained on a single car archetype are evaluated on unseen categories. Our analysis covers predictive accuracy, physical consistency, computational efficiency, and statistical uncertainty. To accelerate progress in data-driven engineering, we open-source the benchmark framework, including training pipelines, uncertainty estimation routines based on bootstrap resampling, and pretrained model weights, establishing the first reproducible foundation for large-scale learning from high-fidelity CFD simulations, available at https://github.com/Mohamedelrefaie/CarBench.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction</title>
<link>https://arxiv.org/abs/2512.07848</link>
<guid>https://arxiv.org/abs/2512.07848</guid>
<content:encoded><![CDATA[
<div> Keywords: motor vehicle collisions, injury severity prediction, NYC dataset, XGBoost, small language models

<br /><br />Summary:  
This paper introduces RaX-Crash, a resource-efficient and explainable pipeline designed to predict injury severity from motor vehicle collisions using the official New York City Motor Vehicle Collisions dataset. RaX-Crash integrates and processes three linked tables containing tens of millions of records, creating a unified feature schema stored in partitioned form for scalability. It employs compact tree-based ensemble methods like Random Forest and XGBoost trained on engineered tabular features, demonstrating superior predictive accuracy (XGBoost: 0.7828, Random Forest: 0.7794) compared to locally deployed small language models (SLMs) prompted with textual summaries, which scored 0.594 and 0.496, respectively. The study addresses class imbalance by using class weighting strategies, improving recall for fatal injuries with only minor accuracy reductions. Model interpretability is achieved via SHAP value attribution, highlighting key factors such as human vulnerability, timing, and location as significant determinants influencing predicted injury severity. The findings suggest that interpretable, small tree-based ensembles remain robust and scalable solutions for city-wide injury analytics. Furthermore, hybrid approaches that combine tabular model predictions with narratives generated by SLMs enhance communication effectiveness without compromising resource efficiency or scalability. This work provides valuable insights for public health and urban safety stakeholders seeking to leverage data-driven injury severity predictions. <div>
arXiv:2512.07848v1 Announce Type: new 
Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents</title>
<link>https://arxiv.org/abs/2512.07850</link>
<guid>https://arxiv.org/abs/2512.07850</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM agents, long-horizon tasks, decisive deviations, mutation-gated verification, benchmark evaluation<br /><br />Summary:<br /><br />This paper investigates the fragility of large language model (LLM) agents on long-horizon, tool-using tasks by questioning whether all actions equally contribute to failure. The authors analyze execution traces from $\tau$-Bench (Airline/Retail) and SWE-Bench Verified, categorizing steps into mutating (environment-changing) and non-mutating. They formalize "decisive deviations" as the earliest actions where level divergences flip success into failure. Logistic regression shows that deviations in mutating actions drastically reduce success odds (up to 92% on Airline and 96% on Retail), whereas non-mutating deviations have minimal impact. They also observe that errors increase with longer context length due to role drift and stale constraints. To counter these issues, the authors propose \cm{}, a model-agnostic, gradient-free test-time safeguard that includes mutation-gated verification, targeted reflection before mutating steps, and block-based context cleaning. This method improves performance significantly across models and benchmarks, e.g., +28% relative on Airline using Qwen3-Thinking and +9% on Claude. Additionally, they identify ceiling effects in $\tau$-Bench caused by annotation errors and underspecified tasks, leading to a revised release called $\tau$-Bench Verified to restore evaluation headroom. The study emphasizes the importance of action-level analysis, targeted safeguards, and reliable benchmarks for developing robust multi-turn agents. <div>
arXiv:2512.07850v1 Announce Type: new 
Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $\tau$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $\tau$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $\tau$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GPU Memory Prediction for Multimodal Model Training</title>
<link>https://arxiv.org/abs/2512.07853</link>
<guid>https://arxiv.org/abs/2512.07853</guid>
<content:encoded><![CDATA[
<div> Keywords: GPU memory prediction, multimodal models, agentic AI, out-of-memory (OoM), peak memory usage factorization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of GPU out-of-memory (OoM) errors which frequently interrupt the training of large and complex deep learning models used in agentic AI systems. These interruptions waste substantial computational resources and limit model scalability.<br />2. Existing approaches for predicting GPU memory usage primarily focus on unimodal model architectures and fail to generalize well to multimodal models, which are increasingly prevalent in agentic AI applications.<br />3. To overcome this limitation, the authors propose a novel framework specifically designed for predicting the peak GPU memory usage of multimodal models by analyzing both their architecture and training behavior.<br />4. The core methodology involves decomposing a multimodal model into its constituent layers, then applying factorization techniques to estimate the memory consumption of each layer independently, allowing for a more accurate and modular prediction.<br />5. Evaluation results demonstrate that this framework achieves a high prediction accuracy with an average mean absolute percentage error (MAPE) of approximately 8.7%, indicating its potential as a valuable tool to prevent OoM errors and optimize resource allocation during model training. <div>
arXiv:2512.07853v1 Announce Type: new 
Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HSTMixer: A Hierarchical MLP-Mixer for Large-Scale Traffic Forecasting</title>
<link>https://arxiv.org/abs/2512.07854</link>
<guid>https://arxiv.org/abs/2512.07854</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic forecasting, large-scale, all-MLP architecture, hierarchical spatiotemporal mixer, adaptive region mixer

<br /><br />Summary:  
This paper addresses the challenge of large-scale traffic forecasting, which is crucial for modern urban management but hindered by the quadratic computational complexity of existing models. The authors propose a novel framework called Hierarchical Spatio-Temporal Mixer (HSTMixer), designed to enable efficient and effective forecasting across large and complex traffic networks. HSTMixer is built using an all-MLP (multi-layer perceptron) architecture that enhances computational efficiency compared to traditional graph-based or attention-based models. The core innovation lies in its hierarchical spatiotemporal mixing block, which captures multi-resolution features via a bottom-up aggregation process and a top-down propagation mechanism, allowing the model to better represent spatial and temporal dependencies. In addition, the framework incorporates an adaptive region mixer that constructs transformation matrices conditioned on regional semantics, enabling the dynamic modeling of evolving spatiotemporal patterns unique to different regions. The model’s effectiveness and efficiency are empirically validated on four large-scale real-world traffic datasets, demonstrating state-of-the-art forecasting accuracy alongside competitive computational performance. This work contributes a scalable and interpretable solution for complex urban traffic prediction tasks, combining architectural innovation with practical applicability. <div>
arXiv:2512.07854v1 Announce Type: new 
Abstract: Traffic forecasting task is significant to modern urban management. Recently, there is growing attention on large-scale forecasting, as it better reflects the complexity of real-world traffic networks. However, existing models often exhibit quadratic computational complexity, making them impractical for large-scale real-world scenarios. In this paper, we propose a novel framework, Hierarchical Spatio-Temporal Mixer (HSTMixer), which leverages an all-MLP architecture for efficient and effective large-scale traffic forecasting. HSTMixer employs a hierarchical spatiotemporal mixing block to extract multi-resolution features through bottom-up aggregation and top-down propagation. Furthermore, an adaptive region mixer generates transformation matrices based on regional semantics, enabling our model to dynamically capture evolving spatiotemporal patterns for different regions. Extensive experiments conducted on four large-scale real-world datasets demonstrate that the proposed method not only achieves state-of-the-art performance but also exhibits competitive computational efficiency.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model</title>
<link>https://arxiv.org/abs/2512.07855</link>
<guid>https://arxiv.org/abs/2512.07855</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformer acceleration, sparse attention, log-domain prediction, hardware architecture, energy efficiency  

<br /><br />Summary:  
1. This paper addresses the dynamic computational bottlenecks encountered in Transformer models when processing variable-length input sequences, highlighting the need for cross-stage sparse acceleration.  
2. Existing sparse Transformer methods predominantly work in a single-stage manner and suffer from significant power overhead due to their sparsity prediction mechanisms, especially when applied across multiple stages.  
3. The authors propose LAPA, a log-domain attention prediction algorithm and architecture co-design to tackle these challenges. The design incorporates an asymmetric leading one computing (ALOC) scheme that removes costly multiplication operations.  
4. Additionally, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is introduced to reduce accumulation overhead, supported by a data-feature dependent filter (DDF) strategy that integrates seamlessly with the MRSA process.  
5. An elaborate hardware accelerator implementing these innovations is developed, yielding notable practical improvements. Experimental results demonstrate that LAPA surpasses state-of-the-art methods Spatten, Sanger, and FACT, achieving 3.52x, 3.24x, and 2.79x higher energy efficiency, respectively. <div>
arXiv:2512.07855v1 Announce Type: new 
Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Medical Test-free Disease Detection Based on Big Data</title>
<link>https://arxiv.org/abs/2512.07856</link>
<guid>https://arxiv.org/abs/2512.07856</guid>
<content:encoded><![CDATA[
<div> Keywords: disease detection, collaborative learning, graph-based deep learning, electronic health records, MIMIC-IV dataset<br /><br />Summary:<br /><br />1. The paper addresses the challenge of disease detection, emphasizing the impracticality of conducting extensive medical tests for diagnosing thousands of diseases due to cost and feasibility constraints.<br />2. It introduces Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning framework that models disease detection as a collaborative task by leveraging both disease associations and patient similarities adaptively.<br />3. CLDD integrates patient-disease interaction data along with demographic features extracted from electronic health records, enabling the detection of a large number of diseases per patient with minimal reliance on actual medical tests.<br />4. Experiments conducted on a processed MIMIC-IV dataset involving 61,191 patients and 2,000 diseases demonstrate that CLDD surpasses existing baseline methods, showing a 6.33% increase in recall and a 7.63% increase in precision across various evaluation metrics.<br />5. Case studies at the individual patient level reveal that CLDD can accurately recover masked diseases within its top-ranked predictions, showcasing its interpretability and reliability.<br />6. The proposed approach has significant potential to reduce diagnostic costs, enhance accessibility to disease screening, and contribute positively to large-scale health monitoring and social health security systems. <div>
arXiv:2512.07856v1 Announce Type: new 
Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation</title>
<link>https://arxiv.org/abs/2512.07857</link>
<guid>https://arxiv.org/abs/2512.07857</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Foundation Models, hierarchical structural semantics, Structure-Aware Semantic Augmentation, Information Bottleneck, expert adaptive routing<br /><br />Summary: This paper addresses limitations in Graph Foundation Models (GFMs) by focusing on robustness to domain noise, structural perturbations, and adversarial attacks. The authors point out that existing GFMs insufficiently model hierarchical structural semantics, which are essential for improved generalization across tasks and domains. To overcome this, they propose SA^2GFM, a framework that enhances domain-adaptive representations via Structure-Aware Semantic Augmentation. The method involves encoding hierarchical structural priors by converting entropy-based encoding trees into structure-aware textual prompts used for feature augmentation. These augmented inputs are processed through a self-supervised Information Bottleneck mechanism that compresses information while preserving structure-guided, transferable, and robust features. To mitigate the problem of negative transfer during cross-domain adaptation, an expert adaptive routing mechanism is designed, combining a mixture-of-experts architecture with a null expert to selectively route information. Additionally, a fine-tuning module is introduced to optimize hierarchical structures by jointly learning intra- and inter-community relationships for efficient downstream adaptation tasks. Extensive experiments demonstrate that SA^2GFM surpasses nine state-of-the-art baselines in effectiveness and robustness, notably improving performance for node and graph classification under random noise and adversarial perturbations. <div>
arXiv:2512.07857v1 Announce Type: new 
Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAIM: Frequency-Aware Interactive Mamba for Time Series Classification</title>
<link>https://arxiv.org/abs/2512.07858</link>
<guid>https://arxiv.org/abs/2512.07858</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Classification, Frequency Domain, Adaptive Filtering, Interactive Mamba Block, Self-Supervised Pre-training<br /><br />Summary:<br /><br />Time series classification (TSC) is vital for applications like environmental monitoring, medical diagnosis, and posture recognition, requiring models that can accurately capture discriminative features. Although deep learning models are effective in modeling temporal dependencies, they often face challenges such as high computational costs, vulnerability to noise, and overfitting on small datasets. To overcome these issues, the authors propose FAIM, a lightweight Frequency-Aware Interactive Mamba model designed for efficient and robust TSC. FAIM introduces an Adaptive Filtering Block (AFB) that utilizes the Fourier Transform to extract frequency domain features and applies learnable adaptive thresholds to suppress noise dynamically. This block also integrates global and local semantic adaptive filtering to model the interaction among various frequency components deeply. Additionally, an Interactive Mamba Block (IMB) is designed to enable multi-granularity information interaction, achieving a balance between fine-grained discriminative feature extraction and capturing global contextual information. To boost robustness and temporal pattern understanding, FAIM incorporates a self-supervised pre-training mechanism, enhancing its performance in diverse and noisy scenarios. Extensive experiments on multiple benchmark datasets demonstrate that FAIM consistently outperforms state-of-the-art methods, offering a superior balance between accuracy and computational efficiency while delivering outstanding classification results. <div>
arXiv:2512.07858v1 Announce Type: new 
Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SetAD: Semi-Supervised Anomaly Learning in Contextual Sets</title>
<link>https://arxiv.org/abs/2512.07863</link>
<guid>https://arxiv.org/abs/2512.07863</guid>
<content:encoded><![CDATA[
<div> Keywords: semi-supervised anomaly detection, set-level detection, attention-based encoder, context calibration, high-order interactions<br /><br />Summary:<br /><br />1. The article addresses the limitations of existing semi-supervised anomaly detection (AD) methods, which primarily focus on individual points or pairs and thus fail to capture the contextual, group-level nature of anomalies.<br /><br />2. It introduces SetAD, a novel framework that reframes AD as a set-level task, leveraging attention-based set encoding to learn complex interactions within groups of data points.<br /><br />3. SetAD is trained with a graded learning objective that quantifies the degree of anomalousness across entire sets, rather than isolated points, enabling more discriminative representation learning.<br /><br />4. To improve robustness and score reliability, the model incorporates a context-calibrated anomaly scoring mechanism that normalizes and aggregates deviations of each point relative to multiple diverse contextual sets.<br /><br />5. Experimental results on 10 real-world datasets show that SetAD significantly outperforms state-of-the-art models, with performance further improving as set sizes increase, empirically validating the set-based approach for anomaly detection. <div>
arXiv:2512.07863v1 Announce Type: new 
Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data</title>
<link>https://arxiv.org/abs/2512.07864</link>
<guid>https://arxiv.org/abs/2512.07864</guid>
<content:encoded><![CDATA[
<div> Keywords: unsupervised machine learning, anomaly detection, trade monitoring, environmental treaties, explainable AI<br /><br />Summary:<br /><br />This paper introduces a novel framework for monitoring compliance with environmental treaties such as the Montreal Protocol by analyzing large and complex customs trade datasets using unsupervised machine learning techniques. The methodology processes 100,000 trade records to identify suspicious trade patterns by combining multiple techniques: K-Means clustering is used to find natural groupings based on shipment value and weight, while anomaly detection methods like Isolation Forest and Interquartile Range (IQR) highlight rare "mega-trades" and shipments with unusual price-per-kilogram values. Additionally, heuristic flagging identifies suspicious tactics such as vague shipment descriptions. These detection layers are integrated into a composite priority scoring system that flagged 1,351 price outliers and 1,288 high-priority shipments for further review by customs officials. A significant discovery is that high-priority commodities tend to exhibit a distinct and higher value-to-weight ratio compared to general goods. Explainable AI techniques, particularly SHAP, validated that vague descriptions and elevated values are the most critical predictors of risk. The model's effectiveness was further confirmed by detecting an early 2021 surge in mega-trades that aligned with the impact of the US AIM Act, demonstrating real-world regulatory relevance. Overall, this work offers a robust, repeatable unsupervised learning pipeline to transform raw trade data into actionable intelligence for regulatory enforcement. <div>
arXiv:2512.07864v1 Announce Type: new 
Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers</title>
<link>https://arxiv.org/abs/2512.07865</link>
<guid>https://arxiv.org/abs/2512.07865</guid>
<content:encoded><![CDATA[
<div> Keywords: Swedish register data, life trajectories, NLP models, residential mobility prediction, longitudinal analysis<br /><br />Summary:<br /><br />1. The study transforms large-scale Swedish register data from 6.9 million individuals collected between 2001 and 2013 into textual life trajectories, which combine demographic and annual changes in residence, work, education, income, and family circumstances.<br /><br />2. This transformation addresses two key challenges in data analysis: the high cardinality of categorical variables and inconsistencies in coding schemes over time.<br /><br />3. By converting register data into semantically rich texts, the research evaluates how effectively these sequences support the prediction of individuals’ residential mobility during 2013-2017.<br /><br />4. Multiple NLP architectures, such as LSTM, DistilBERT, BERT, and Qwen, were compared, revealing that sequential and transformer-based models capture temporal and semantic structures more effectively than baseline models.<br /><br />5. The results demonstrate that textualized register data preserves meaningful information about individual life pathways, enabling complex and scalable modeling, and providing a unique testbed for advancing longitudinal analysis and sequence-modeling approaches in social sciences. <div>
arXiv:2512.07865v1 Announce Type: new 
Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Command &amp; Control (C2) Traffic Detection Via Algorithm Generated Domain (Dga) Classification Using Deep Learning And Natural Language Processing</title>
<link>https://arxiv.org/abs/2512.07866</link>
<guid>https://arxiv.org/abs/2512.07866</guid>
<content:encoded><![CDATA[
<div> Keywords: malware detection, Domain Generation Algorithms, deep learning, LSTM, natural language processing  

<br /><br />Summary:  
This paper addresses the challenge of detecting domains generated by Domain Generation Algorithms (DGA), which modern malware uses to communicate with Command and Control (C2) servers, bypassing traditional blacklist-based defenses. To tackle this issue, the researchers compiled a large hybrid dataset comprising 50,000 legitimate domains and 50,000 malicious DGA domains. They then extracted lexical features from these domains to capture patterns in the domain names. A Recurrent Neural Network employing Long Short-Term Memory (LSTM) units was trained using these features to identify complex domain generation patterns that simple heuristic or statistical methods might miss. The study also evaluated the effectiveness of statistical entropy analysis, which proved adequate for detecting basic DGAs. However, the deep learning approach significantly outperformed entropy-based methods, achieving an accuracy of 97.2%. Notably, the LSTM model also demonstrated a reduced false positive rate when processing ambiguous legitimate traffic, thereby improving reliability in real-world settings. Overall, the research highlights the promise of combining deep learning and Natural Language Processing techniques for advanced malware domain detection, presenting a robust alternative to conventional firewall and blacklist methods. <div>
arXiv:2512.07866v1 Announce Type: new 
Abstract: The sophistication of modern malware, specifically regarding communication with Command and Control (C2) servers, has rendered static blacklist-based defenses obsolete. The use of Domain Generation Algorithms (DGA) allows attackers to generate thousands of dynamic addresses daily, hindering blocking by traditional firewalls. This paper aims to propose and evaluate a method for detecting DGA domains using Deep Learning and Natural Language Processing (NLP) techniques. The methodology consisted of collecting a hybrid database containing 50,000 legitimate and 50,000 malicious domains, followed by the extraction of lexical features and the training of a Recurrent Neural Network (LSTM). Results demonstrated that while statistical entropy analysis is effective for simple DGAs, the Neural Network approach presents superiority in detecting complex patterns, reaching 97.2% accuracy and reducing the false positive rate in ambiguous lawful traffic scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimization for Function-Valued Responses under Min-Max Criteria</title>
<link>https://arxiv.org/abs/2512.07868</link>
<guid>https://arxiv.org/abs/2512.07868</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian optimization, functional response, min-max optimization, Gaussian process, functional principal component analysis<br /><br />Summary: Bayesian optimization traditionally targets scalar outputs, but many real-world problems involve functional responses that vary smoothly over an index like time or wavelength. Classical approaches often minimize integrated error, which overlooks worst-case deviations critical in many applications. To remedy this, the paper proposes the min-max Functional Bayesian Optimization (MM-FBO) framework that directly targets minimizing the maximum error across the entire functional domain. The method represents functional responses using functional principal component analysis (FPCA) and models the principal component scores with Gaussian process surrogates, enabling an effective Bayesian treatment of the functional output space. MM-FBO introduces a novel integrated uncertainty acquisition function that drives the optimization by balancing exploitation of worst-case expected error and exploration of the functional domain. Theoretical guarantees provided include a discretization bound for the worst-case objective and consistency showing acquisition convergence to the true min-max objective as model uncertainty diminishes. Experimental validation on synthetic benchmarks and physics-inspired problems involving electromagnetic scattering and vapor phase infiltration demonstrates that MM-FBO outperforms existing baseline methods. Overall, this work emphasizes the importance of explicitly modeling functional uncertainty and worst-case risk in Bayesian optimization for functional outputs. <div>
arXiv:2512.07868v1 Announce Type: new 
Abstract: Bayesian optimization is widely used for optimizing expensive black box functions, but most existing approaches focus on scalar responses. In many scientific and engineering settings the response is functional, varying smoothly over an index such as time or wavelength, which makes classical formulations inadequate. Existing methods often minimize integrated error, which captures average performance but neglects worst case deviations. To address this limitation we propose min-max Functional Bayesian Optimization (MM-FBO), a framework that directly minimizes the maximum error across the functional domain. Functional responses are represented using functional principal component analysis, and Gaussian process surrogates are constructed for the principal component scores. Building on this representation, MM-FBO introduces an integrated uncertainty acquisition function that balances exploitation of worst case expected error with exploration across the functional domain. We provide two theoretical guarantees: a discretization bound for the worst case objective, and a consistency result showing that as the surrogate becomes accurate and uncertainty vanishes, the acquisition converges to the true min-max objective. We validate the method through experiments on synthetic benchmarks and physics inspired case studies involving electromagnetic scattering by metaphotonic devices and vapor phase infiltration. Results show that MM-FBO consistently outperforms existing baselines and highlights the importance of explicitly modeling functional uncertainty in Bayesian optimization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion</title>
<link>https://arxiv.org/abs/2512.07873</link>
<guid>https://arxiv.org/abs/2512.07873</guid>
<content:encoded><![CDATA[
<div> Keywords: diffusion models, medical time series, Mixture of Experts, noise estimation, signal reconstruction<br /><br />Summary:<br /><br />Recent advances indicate that diffusion models are promising for time series signal reconstruction but are underexplored in medical time series due to their unique challenges including multivariate nature, high temporal variability, noise, and artifacts. To address these, the authors propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework tailored for physiological signals. The key innovation is the Receptive Field Adaptive MoE (RFAMoE) module, which allows each channel to dynamically select receptive fields during the diffusion process, improving adaptability to complex time series characteristics. Additionally, the study addresses computational inefficiencies observed in previous methods that rely on multiple inferences and averaging to reduce reconstruction error. They introduce a Fusion MoE module that generates multiple noise signals in parallel and fuses them through a routing mechanism, enabling signal reconstruction in a single inference step. This parallel approach reduces computational cost and latency while improving performance. Extensive experimental evaluations across diverse tasks and datasets demonstrate that the proposed framework consistently outperforms the current state-of-the-art diffusion-based methods for medical time series reconstruction, proving its effectiveness and efficiency in handling physiological signal imputation. <div>
arXiv:2512.07873v1 Announce Type: new 
Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Controllable risk scenario generation from human crash data for autonomous vehicle testing</title>
<link>https://arxiv.org/abs/2512.07874</link>
<guid>https://arxiv.org/abs/2512.07874</guid>
<content:encoded><![CDATA[
<div> Keywords: autonomous vehicles, risk agent generation, latent space, safety-critical behaviors, simulation<br /><br />Summary:<br /><br />1. The paper addresses the challenge of safely testing autonomous vehicles (AVs) by simulating both normal and rare, safety-critical agent behaviors, including background vehicles and vulnerable road users. <br />2. It introduces the Controllable Risk Agent Generation (CRAG) framework, which models dominant nominal and risk-prone behaviors in a unified approach. <br />3. CRAG constructs a structured latent space that separates normal driving behavior from risk-related behavior, thereby maximizing the use of limited crash data for training. <br />4. The framework employs risk-aware latent representations combined with optimization-based mode-transition mechanisms, enabling agents to transition smoothly and realistically between safe and risky states over extended time horizons. <br />5. Extensive experiments demonstrate that CRAG produces more diverse scenarios compared to existing methods and allows for controllable generation of risk scenarios, facilitating targeted and efficient evaluation of AV robustness. <div>
arXiv:2512.07874v1 Announce Type: new 
Abstract: Ensuring the safety of autonomous vehicles (AV) requires rigorous testing under both everyday driving and rare, safety-critical conditions. A key challenge lies in simulating environment agents, including background vehicles (BVs) and vulnerable road users (VRUs), that behave realistically in nominal traffic while also exhibiting risk-prone behaviors consistent with real-world accidents. We introduce Controllable Risk Agent Generation (CRAG), a framework designed to unify the modeling of dominant nominal behaviors and rare safety-critical behaviors. CRAG constructs a structured latent space that disentangles normal and risk-related behaviors, enabling efficient use of limited crash data. By combining risk-aware latent representations with optimization-based mode-transition mechanisms, the framework allows agents to shift smoothly and plausibly from safe to risk states over extended horizons, while maintaining high fidelity in both regimes. Extensive experiments show that CRAG improves diversity compared to existing baselines, while also enabling controllable generation of risk scenarios for targeted and efficient evaluation of AV robustness.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Softly Symbolifying Kolmogorov-Arnold Networks</title>
<link>https://arxiv.org/abs/2512.07875</link>
<guid>https://arxiv.org/abs/2512.07875</guid>
<content:encoded><![CDATA[
<div> Keywords: Kolmogorov-Arnold Networks, interpretability, symbolic primitives, Minimum Description Length, sparsification  

<br /><br />Summary:  
Kolmogorov-Arnold Networks (KANs) present a framework for interpretable machine learning by allowing individual examination of learnable activations that collectively model complex data. Despite their theoretical promise, vanilla KANs often yield learned activations that lack clear symbolic meaning, resulting in decompositions that are difficult to interpret. To address this, the authors introduce Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which incorporate symbolic primitives directly during training. Each activation is constructed from a dictionary consisting of symbolic and dense terms, with learnable gating mechanisms that promote sparse representations. Importantly, the sparsification process is differentiable, allowing for end-to-end optimization governed by a Minimum Description Length (MDL) principle, which naturally balances model complexity and accuracy. This setup enables S2KAN to produce interpretable symbolic forms when suitable, while smoothly defaulting to dense spline approximations when symbolic terms are insufficient. The authors validate S2KAN's practical effectiveness by demonstrating competitive or superior predictive accuracy with significantly smaller models on symbolic regression benchmarks, dynamical systems forecasting, and various real-world prediction tasks. Additionally, they observe spontaneous self-sparsification behavior emerging in models even in the absence of explicit regularization, highlighting the robustness of the approach toward interpretability and compactness. <div>
arXiv:2512.07875v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fourier-Enhanced Recurrent Neural Networks for Electrical Load Time Series Downscaling</title>
<link>https://arxiv.org/abs/2512.07876</link>
<guid>https://arxiv.org/abs/2512.07876</guid>
<content:encoded><![CDATA[
<div> Keywords: Fourier-enhanced RNN, electrical load downscaling, seasonal embeddings, self-attention, RMSE improvement<br /><br />Summary:<br /><br />1. The paper proposes a novel Fourier-enhanced recurrent neural network (RNN) designed for downscaling electrical load data from low-resolution inputs to higher resolution outputs.  
2. The model architecture integrates three main components: a recurrent backbone that processes the low-resolution input data, explicit Fourier-based seasonal embeddings that are fused into the latent space to capture periodic patterns, and a self-attention layer that models dependencies among high-resolution components within each seasonal period.  
3. This architecture aims to improve temporal resolution and predictive accuracy by leveraging both time series recurrence and frequency domain seasonal information, along with attention mechanisms to enhance contextual understanding.  
4. Experimental evaluation was performed across four different PJM territories, demonstrating that the proposed model consistently achieves lower root mean squared error (RMSE) values compared to classical baseline models such as Prophet (with and without seasonality or local approximate adjustments, LAA).  
5. Additional ablation studies show that removing either the self-attention layer or the Fourier seasonal features leads to inferior performance, confirming the contribution of each component to the overall improvement in downscaling accuracy. <div>
arXiv:2512.07876v1 Announce Type: new 
Abstract: We present a Fourier-enhanced recurrent neural network (RNN) for downscaling electrical loads. The model combines (i) a recurrent backbone driven by low-resolution inputs, (ii) explicit Fourier seasonal embeddings fused in latent space, and (iii) a self-attention layer that captures dependencies among high-resolution components within each period. Across four PJM territories, the approach yields RMSE lower and flatter horizon-wise than classical Prophet baselines (with and without seasonality/LAA) and than RNN ablations without attention or Fourier features.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Artificial Intelligence-Driven Network-on-Chip Design Space Exploration: Neural Network Architectures for Design</title>
<link>https://arxiv.org/abs/2512.07877</link>
<guid>https://arxiv.org/abs/2512.07877</guid>
<content:encoded><![CDATA[
<div> Machine Learning, Network-on-Chip, Design Space Exploration, Conditional Diffusion Model, BookSim Simulation<br /><br />Summary:<br /><br />This paper presents a machine learning-driven framework aimed at automating the design space exploration of Network-on-Chip (NoC) architectures to meet throughput and latency requirements. The approach addresses challenges posed by the high-dimensional and complex parameter space inherent in NoC design, which traditional methods find slow and difficult to handle. The authors generate a large dataset comprising over 150,000 simulation points using the BookSim simulator across various mesh topologies to train and test models. Three neural network architectures are compared for their ability to predict optimal NoC parameters based on target performance metrics: a Multi-Layer Perceptron (MLP), a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE). Among these, the Conditional Diffusion Model demonstrates the best predictive accuracy, achieving a mean squared error (MSE) of 0.463 on unseen test data. The proposed framework provides a significant reduction in design exploration time by several orders of magnitude, enabling rapid and scalable co-design of NoC systems. This makes the approach practical for real-world applications where efficient and effective NoC design space exploration is critical. <div>
arXiv:2512.07877v1 Announce Type: new 
Abstract: Network-on-Chip (NoC) design requires exploring a high-dimensional configuration space to satisfy stringent throughput requirements and latency constraints.Traditional design space exploration techniques are often slow and struggle to handle complex, non-linear parameter interactions.This work presents a machine learning-driven framework that automates NoC design space exploration using BookSim simulations and reverse neural network models.Specifically, we compare three architectures - a Multi-Layer Perceptron (MLP),a Conditional Diffusion Model, and a Conditional Variational Autoencoder (CVAE) to predict optimal NoC parameters given target performance metrics.Our pipeline generates over 150,000 simulation data points across varied mesh topologies.The Conditional Diffusion Model achieved the highest predictive accuracy, attaining a mean squared error (MSE) of 0.463 on unseen data.Furthermore, the proposed framework reduces design exploration time by several orders of magnitude, making it a practical solution for rapid and scalable NoC co-design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Contrastive Learning via Spectral Graph Alignment</title>
<link>https://arxiv.org/abs/2512.07878</link>
<guid>https://arxiv.org/abs/2512.07878</guid>
<content:encoded><![CDATA[
<div> Contrastive learning, graph embeddings, normalized Laplacian, SpecMatch-CL, graph-of-graphs  

<br /><br />Summary:  
This paper addresses limitations in current contrastive learning methods for graphs, such as InfoNCE, which focus on pairwise alignment of graph embeddings from augmented views but lack control over the global structure of the view-specific graph-of-graphs formed by these embeddings. The authors propose SpecMatch-CL, a novel loss function that aligns graphs at a global level by minimizing the difference between their normalized Laplacians, a spectral graph representation. Theoretically, they demonstrate that under certain assumptions, the difference between normalized Laplacians bounds the difference between the current contrastive loss and the ideal Perfect Alignment loss, as well as the Uniformly loss. Empirical results show that SpecMatch-CL achieves state-of-the-art performance on eight TU benchmark datasets in both unsupervised and semi-supervised learning scenarios, especially at low label rates. Furthermore, the method consistently improves transfer learning outcomes on large-scale datasets like PPI-306K and ZINC 2M. This work thus provides a theoretically grounded and empirically validated enhancement to contrastive graph representation learning by integrating spectral alignment of graph-of-graphs structures. <div>
arXiv:2512.07878v1 Announce Type: new 
Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonnegative Matrix Factorization through Cone Collapse</title>
<link>https://arxiv.org/abs/2512.07879</link>
<guid>https://arxiv.org/abs/2512.07879</guid>
<content:encoded><![CDATA[
<div> Nonnegative matrix factorization, orthogonal NMF, conic geometry, cone collapse, clustering purity<br /><br />Summary:<br /><br />This paper revisits Nonnegative Matrix Factorization (NMF) from a geometric perspective by focusing on the conic structure of the data. Unlike traditional NMF algorithms derived solely from optimization, the authors propose "Cone Collapse," an iterative algorithm that shrinks the positive orthant to identify the minimal cone generated by the data points. They prove that, under mild assumptions, this algorithm terminates in a finite number of steps and exactly recovers the minimal generating cone of the transpose of the data matrix, \(\mathbf{X}^\top\). Building on these recovered extreme rays of the cone, the authors develop a cone-aware orthogonal NMF (CC-NMF) model by applying uni-orthogonal NMF. They evaluate CC-NMF on 16 benchmark datasets spanning gene-expression, text, and image domains. Results show that CC-NMF consistently outperforms or matches strong existing NMF methods—including multiplicative updates, Alternating Nonnegative Least Squares (ANLS), projective NMF, orthogonal NMF, and sparse NMF—with respect to clustering purity. This work highlights the importance of explicitly recovering the underlying data cone and demonstrates that leveraging conic geometry leads to theoretically justified, robust, and effective clustering techniques based on NMF. <div>
arXiv:2512.07879v1 Announce Type: new 
Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-Supervised Contrastive Learning with Orthonormal Prototypes</title>
<link>https://arxiv.org/abs/2512.07880</link>
<guid>https://arxiv.org/abs/2512.07880</guid>
<content:encoded><![CDATA[
<div> Contrastive learning, dimensional collapse, semi-supervised, orthogonal subspaces, CLOP  

<br /><br />Summary:  
This paper addresses the problem of dimensional collapse in contrastive learning, where embeddings undesirably converge into a lower-dimensional space, compromising representation quality. The authors first identify a critical learning-rate threshold that causes standard contrastive loss functions to collapse, highlighting a key limitation in current methods. To overcome this, they introduce CLOP, a novel semi-supervised loss function that actively prevents dimensional collapse by encouraging class embeddings to form orthogonal linear subspaces. This approach ensures that the learned representations maintain high dimensionality and discriminative power. Extensive experiments on both real and synthetic datasets demonstrate that CLOP consistently improves performance in standard image classification and object detection tasks. Moreover, CLOP exhibits enhanced stability when training under various learning rates and batch sizes, suggesting it is robust to typical hyperparameter variations. Overall, the study contributes a theoretically motivated and empirically validated solution to a critical problem in contrastive learning frameworks, particularly benefiting semi-supervised and self-supervised learning scenarios. <div>
arXiv:2512.07880v1 Announce Type: new 
Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSPN-2: Efficient Parallel Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.07884</link>
<guid>https://arxiv.org/abs/2512.07884</guid>
<content:encoded><![CDATA[
<div> Efficient Vision Transformer, Generalized Spatial Propagation Network, GPU Optimization, Channel Propagation, Image Classification<br /><br />Summary:  
1. The article addresses the inefficiency of vision transformers for high-resolution images and long videos by improving the Generalized Spatial Propagation Network (GSPN).  
2. GSPN reduces the quadratic self-attention complexity to nearly linear by using a line-scan propagation method, preserving accuracy while boosting efficiency.  
3. The existing GSPN implementation has issues like frequent GPU kernel launches, excessive global memory transfers, and redundant per-channel weight computations.  
4. GSPN-2 is proposed as a joint algorithm and system redesign to tackle these issues by consolidating many micro-launches into a single 2D GPU kernel, assigning dedicated warps to channel slices, and caching activations in shared memory to reduce overhead.  
5. On the modeling side, GSPN-2 introduces a compact channel propagation strategy that replaces separate channel matrices, decreasing parameters and aligning with the affinity map used in transformer attention.  
6. Experimental results show that GSPN-2 matches transformer-level accuracy in image classification and text-to-image synthesis tasks while significantly lowering computational costs.  
7. Overall, GSPN-2 sets a new standard for efficient global spatial context modeling in vision tasks through innovative structured matrix transformations combined with GPU-level optimizations. <div>
arXiv:2512.07884v1 Announce Type: new 
Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ByteStorm: a multi-step data-driven approach for Tropical Cyclones detection and tracking</title>
<link>https://arxiv.org/abs/2512.07885</link>
<guid>https://arxiv.org/abs/2512.07885</guid>
<content:encoded><![CDATA[
<div> Keywords: tropical cyclones, deep learning, tracking, vorticity, ByteStorm  

<br /><br />Summary:  
This article introduces ByteStorm, a novel data-driven framework designed for accurate tropical cyclone (TC) tracking without relying on subjective thresholds commonly used in traditional methods. ByteStorm employs deep learning techniques to detect TC centers by classifying and localizing them based on relative vorticity at 850 mb and mean sea-level pressure data. After detection, the identified centers are linked into continuous TC tracks using the BYTE algorithm. The researchers evaluate ByteStorm's performance in the East- and West-North Pacific basins (ENP and WNP), benchmarking it against state-of-the-art deterministic tracking methods. Results show that ByteStorm achieves superior Probability of Detection, with 85.05% in the ENP and 79.48% in the WNP, alongside lower False Alarm Rates of 23.26% and 16.14%, respectively. Additionally, the framework demonstrates high correlations with Inter-Annual Variability, scoring 0.75 in the ENP and 0.69 in the WNP. Overall, the study highlights the effectiveness of combining deep learning and computer vision strategies for fast, accurate, and robust TC tracking, presenting a promising alternative to conventional threshold-dependent approaches in meteorological research. <div>
arXiv:2512.07885v1 Announce Type: new 
Abstract: Accurate tropical cyclones (TCs) tracking represents a critical challenge in the context of weather and climate science. Traditional tracking schemes mainly rely on subjective thresholds, which may introduce biases in their skills on the geographical region of application. We present ByteStorm, an efficient data-driven framework for reconstructing TC tracks without threshold tuning. It leverages deep learning networks to detect TC centers (via classification and localization), using only relative vorticity (850 mb) and mean sea-level pressure. Then, detected centers are linked into TC tracks through the BYTE algorithm. ByteStorm is evaluated against state-of-the-art deterministic trackers in the East- and West-North Pacific basins (ENP and WNP). The proposed framework achieves superior performance in terms of Probability of Detection ($85.05\%$ ENP, $79.48\%$ WNP), False Alarm Rate ($23.26\%$ ENP, $16.14\%$ WNP), and high Inter-Annual Variability correlations ($0.75$ ENP and $0.69$ WNP). These results highlight the potential of integrating deep learning and computer vision for fast and accurate TC tracking, offering a robust alternative to traditional approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards symbolic regression for interpretable clinical decision scores</title>
<link>https://arxiv.org/abs/2512.07961</link>
<guid>https://arxiv.org/abs/2512.07961</guid>
<content:encoded><![CDATA[
<div> Symbolic Regression, Medical Decision-Making, Clinical Risk Scores, Brush Algorithm, Interpretability<br /><br />Summary:<br /><br />1. The article addresses the challenge of modeling medical decision-making processes, which often combine risk equations with rule-based clinical pathways, using symbolic regression (SR).<br />2. Traditional SR focuses on continuous mathematical functions and their parameters, which limits its ability to represent rule-based, decision-tree-like logic common in clinical decision-making.<br />3. The authors introduce Brush, a novel SR algorithm that integrates decision-tree-like splitting with nonlinear constant optimization, effectively embedding rule-based logic into the symbolic regression framework.<br />4. Brush demonstrates Pareto-optimal performance on the SRBench benchmarking dataset, showing it balances model complexity and predictive accuracy effectively.<br />5. The algorithm was tested by recreating two widely used clinical scoring systems, where it achieved high predictive accuracy and yielded interpretable models.<br />6. Compared to decision trees, random forests, and other symbolic regression methods, Brush offers comparable or better predictive performance while producing simpler, more interpretable models.<br /><br />This work highlights the potential for data-driven, interpretable clinical risk scores by combining rule-based decision-making with symbolic regression through the Brush algorithm, representing an advance in computational methodologies for medical decision support. <div>
arXiv:2512.07961v1 Announce Type: new 
Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CIP-Net: Continual Interpretable Prototype-based Network</title>
<link>https://arxiv.org/abs/2512.07981</link>
<guid>https://arxiv.org/abs/2512.07981</guid>
<content:encoded><![CDATA[
<div> Continual learning, catastrophic forgetting, self-explainable models, exemplar-free, prototype-based  

<br /><br />Summary:  
Continual learning aims at enabling models to learn new tasks sequentially without forgetting previously acquired knowledge, addressing the critical challenge of catastrophic forgetting. Traditional approaches to mitigating forgetting often rely on storing past examples or using post-hoc explanations, which can hinder scalability and increase memory usage. This paper introduces CIP-Net, a novel exemplar-free, self-explainable prototype-based model specifically designed for continual learning scenarios. CIP-Net uniquely avoids the need to store old data while maintaining a simple and efficient architecture. It generates explanations during the prediction process, enhancing interpretability and helping to preserve knowledge across tasks. The authors demonstrate that CIP-Net achieves state-of-the-art performance compared to prior exemplar-free and self-explainable continual learning methods. This holds true in both task-incremental and class-incremental learning setups. Importantly, CIP-Net operates with significantly lower memory overhead, making it a practical and scalable solution for real-world continual learning applications. Overall, CIP-Net effectively balances performance, explanation quality, and memory efficiency, contributing an interpretable and robust approach to combating catastrophic forgetting. <div>
arXiv:2512.07981v1 Announce Type: new 
Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HOLE: Homological Observation of Latent Embeddings for Neural Network Interpretability</title>
<link>https://arxiv.org/abs/2512.07988</link>
<guid>https://arxiv.org/abs/2512.07988</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, persistent homology, neural networks, representation interpretability, topological analysis  

<br /><br />Summary:  
This paper introduces HOLE (Homological Observation of Latent Embeddings), a novel method leveraging persistent homology to analyze and interpret deep neural networks. HOLE focuses on extracting topological features from neural activations to provide insight into the structure and quality of learned representations across different layers of a model. To facilitate exploration and understanding, the method employs various visualization tools such as Sankey diagrams, heatmaps, dendrograms, and blob graphs. The authors evaluate HOLE on standard datasets using a variety of discriminative models, emphasizing its ability to assess representation quality and interpretability throughout a network’s depth. Additionally, the method is tested for robustness to input perturbations and model compression, demonstrating its relevance to real-world scenarios where models experience noise or efficiency-driven modifications. Results indicate that the topological perspective offered by HOLE reveals meaningful patterns related to class separation and feature disentanglement, which are difficult to capture with traditional analysis techniques. Moreover, it uncovers aspects tied to model robustness, thus providing complementary insights beyond conventional performance metrics. Overall, this work presents persistent homology as a promising tool to enhance understanding, diagnosis, and ultimately the improvement of deep learning systems. <div>
arXiv:2512.07988v1 Announce Type: new 
Abstract: Deep learning models have achieved remarkable success across various domains, yet their learned representations and decision-making processes remain largely opaque and hard to interpret. This work introduces HOLE (Homological Observation of Latent Embeddings), a method for analyzing and interpreting deep neural networks through persistent homology. HOLE extracts topological features from neural activations and presents them using a suite of visualization techniques, including Sankey diagrams, heatmaps, dendrograms, and blob graphs. These tools facilitate the examination of representation structure and quality across layers. We evaluate HOLE on standard datasets using a range of discriminative models, focusing on representation quality, interpretability across layers, and robustness to input perturbations and model compression. The results indicate that topological analysis reveals patterns associated with class separation, feature disentanglement, and model robustness, providing a complementary perspective for understanding and improving deep learning systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging the Clinical Expertise Gap: Development of a Web-Based Platform for Accessible Time Series Forecasting and Analysis</title>
<link>https://arxiv.org/abs/2512.07992</link>
<guid>https://arxiv.org/abs/2512.07992</guid>
<content:encoded><![CDATA[
<div> Time series forecasting, healthcare, web platform, machine learning, interpretability<br /><br />Summary:<br /><br />This article introduces a user-friendly web platform designed to facilitate time series forecasting, particularly for healthcare researchers and clinicians who may lack advanced technical expertise. The platform allows users to upload their data easily and generate visualizations that highlight variables and their interrelationships, enhancing data exploration. It supports a variety of forecasting models and training methodologies, providing a highly customizable environment tailored to the specific needs of each user. An innovative feature is the integration of a large language model that offers recommendations on selecting appropriate parameters and interprets model results, helping users make informed decisions about their forecasts. The goal is to embed this platform into learning health systems, enabling continuous data collection and real-time inference within clinical workflows. This approach aims to lower the barrier to entry for complex predictive analytics in healthcare, ultimately supporting better clinical decision-making and outcomes by streamlining the forecasting process from data input, model training to result interpretation and visualization. <div>
arXiv:2512.07992v1 Announce Type: new 
Abstract: Time series forecasting has applications across domains and industries, especially in healthcare, but the technical expertise required to analyze data, build models, and interpret results can be a barrier to using these techniques. This article presents a web platform that makes the process of analyzing and plotting data, training forecasting models, and interpreting and viewing results accessible to researchers and clinicians. Users can upload data and generate plots to showcase their variables and the relationships between them. The platform supports multiple forecasting models and training techniques which are highly customizable according to the user's needs. Additionally, recommendations and explanations can be generated from a large language model that can help the user choose appropriate parameters for their data and understand the results for each model. The goal is to integrate this platform into learning health systems for continuous data collection and inference from clinical pipelines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care</title>
<link>https://arxiv.org/abs/2512.08012</link>
<guid>https://arxiv.org/abs/2512.08012</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-objective Reinforcement Learning, Critical Care, Offline Learning, Decision Transformer, MIMIC-IV Dataset<br /><br />Summary:<br /><br />1. This paper addresses decision-making challenges in critical care environments like the Intensive Care Unit, where clinicians must balance maximizing patient survival with minimizing resource use, such as length of stay.<br /><br />2. Traditional single-objective Reinforcement Learning methods optimize a fixed scalar reward, producing inflexible policies that do not accommodate changing clinical priorities.<br /><br />3. Multi-objective Reinforcement Learning (MORL) enables learning a spectrum of optimal policies along the Pareto Frontier, allowing preferences to be adjusted dynamically at test time.<br /><br />4. The authors benchmark three offline MORL algorithms—Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent Decision Transformer (PEDA DT)—against three scalarized single-objective baselines—Behavior Cloning (BC), Conservative Q-Learning (CQL), and Deep Double Q-Network (DDQN)—using the MIMIC-IV dataset.<br /><br />5. Using Off-Policy Evaluation metrics, results show that the PEDA DT algorithm offers superior flexibility and robust performance, confirming that sequence modeling architectures can effectively scale to multi-objective conditioned decision-making in healthcare without retraining.<br /><br />6. These findings highlight offline MORL as a promising approach for personalized and adjustable clinical decision support in critical care settings. <div>
arXiv:2512.08012v1 Announce Type: new 
Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space</title>
<link>https://arxiv.org/abs/2512.08029</link>
<guid>https://arxiv.org/abs/2512.08029</guid>
<content:encoded><![CDATA[
<div> Keywords: Clinical decision-making, world models, disease evolution, treatment planning, personalized medicine<br /><br />Summary: Clinical decision-making in oncology needs dynamic predictions of disease progression, which current static AI models fail to provide. CLARITY, a novel medical world model, addresses this gap by forecasting disease evolution within a structured latent space, enabling generative prediction beyond simple reconstruction. Unlike previous stochastic diffusion models that prioritize visual outcomes, CLARITY focuses on causal and physiological transitions to enhance interpretability. It uniquely integrates patient-specific clinical contexts and temporal intervals to generate smooth, personalized trajectories that reflect treatment-conditioned progression. This integration supports individualized and physiologically faithful treatment plans. Moreover, CLARITY introduces a prediction-to-decision framework that converts model rollouts in latent space into transparent and actionable clinical recommendations. Demonstrating state-of-the-art results, CLARITY outperforms the recent MeWM model by 12% on the MU-Glioma-Post dataset and significantly exceeds the capabilities of other medical-specific large language models. Overall, CLARITY advances oncology AI by combining patient data, temporal awareness, and decision support in a robust, interpretable framework designed for dynamic disease management and improved treatment planning. <div>
arXiv:2512.08029v1 Announce Type: new 
Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNA: Linear Universal Neural Attention with Generalization Guarantees</title>
<link>https://arxiv.org/abs/2512.08061</link>
<guid>https://arxiv.org/abs/2512.08061</guid>
<content:encoded><![CDATA[
<div> Keywords: linear attention, kernel learning, computational efficiency, Transformers, feature maps<br /><br />Summary:  
This paper addresses the major challenge of scaling attention mechanisms, specifically the quadratic computational cost $\mathcal{O}(n^2)$ of traditional softmax attention which hinders its use on long sequences. Current linear attention methods reduce this complexity to $\mathcal{O}(n)$ but depend on fixed, data-agnostic kernels like random Fourier features, compromising accuracy for efficiency. The authors propose \textsc{LUNA}, a kernelized linear attention mechanism that learns the kernel feature map instead of relying on static ones, thus eliminating the accuracy-efficiency trade-off. \textsc{LUNA} parameterizes the kernel with a learnable feature basis tailored to the data and task, enabling better expressiveness and adaptability. The method ensures the kernel remains positive-definite and supports a streaming computation form, maintaining linear time and memory complexity in sequence length. Empirical results on benchmarks such as the Long Range Arena demonstrate that \textsc{LUNA} achieves state-of-the-art accuracy among efficient Transformers while using comparable parameter counts, training steps, and compute resources. Additionally, \textsc{LUNA} excels in a post-hoc conversion scenario by replacing softmax attention in fine-tuned BERT and ViT-B/16 models and regaining most original performance after brief fine-tuning, significantly outperforming fixed linearization methods. <div>
arXiv:2512.08061v1 Announce Type: new 
Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks</title>
<link>https://arxiv.org/abs/2512.08063</link>
<guid>https://arxiv.org/abs/2512.08063</guid>
<content:encoded><![CDATA[
<div> Deep Kernel Aalen-Johansen, competing risks, cumulative incidence functions, kernel function, model interpretability

<br /><br />Summary:  
This paper introduces the Deep Kernel Aalen-Johansen (DKAJ) estimator, an interpretable deep learning model designed for competing risks analysis. The DKAJ generalizes the classical nonparametric Aalen-Johansen estimator of cumulative incidence functions (CIFs) by incorporating learnable kernel functions to better capture data point similarities. Each individual's data is represented as a weighted combination of clusters, with weights derived from an automatically learned kernel function indicating similarity between samples. If a data point's weight is concentrated in a single cluster, its predicted CIF matches the classical Aalen-Johansen estimate restricted to that cluster's data, thus preserving interpretability. The method enables visualization of CIFs linked to clusters, aiding model explanation and understanding. Empirical evaluation on four benchmark competing risks datasets demonstrates that DKAJ performs competitively compared to state-of-the-art baselines. Importantly, the model balances predictive accuracy with interpretability by providing cluster-based CIF visualizations that may help practitioners and researchers understand the influence of heterogeneous groups in survival analysis under competing risks. This approach promises to enhance both the accuracy and transparency of competing risks modeling in medical and other fields requiring survival analysis. <div>
arXiv:2512.08063v1 Announce Type: new 
Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification</title>
<link>https://arxiv.org/abs/2512.08071</link>
<guid>https://arxiv.org/abs/2512.08071</guid>
<content:encoded><![CDATA[
<div> Crisis classification, multimodal learning, domain generalization, adversarial disentanglement, representation alignment  

<br /><br />Summary:  
Crisis classification in social media focuses on extracting actionable disaster-related information from multimodal posts to improve situational awareness and emergency responses. A major challenge is the significant variation in crisis types, which limits models’ ability to generalize across unseen disasters. Current deep learning approaches fuse textual and visual data but perform poorly when encountering crisis types outside the training domain. This is due to two main issues: first, models fail to disentangle spurious features from causal features, leading to degraded performance when domain shift occurs; second, heterogeneous modality representations are not effectively aligned within a shared space, preventing the straightforward adaptation of single-modality domain generalization techniques. To overcome these limitations, the authors propose a causality-guided multimodal domain generalization (MMDG) framework. This framework employs adversarial disentanglement to isolate and emphasize domain-invariant causal features that underlie the crisis data, promoting robust generalization. Additionally, it implements unified representation learning to align features from different modalities into a common latent space, facilitating multimodal adaptation of domain generalization methods. Experimental results across various datasets demonstrate that this approach outperforms existing models when applied to unseen disaster scenarios. <div>
arXiv:2512.08071v1 Announce Type: new 
Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unveiling Latent Knowledge in Chemistry Language Models through Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2512.08077</link>
<guid>https://arxiv.org/abs/2512.08077</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, chemistry language models, interpretability, sparse autoencoders, molecular features  

<br /><br />Summary:  
This paper addresses the challenge of interpretability in chemistry language models (CLMs), especially as their use in high-stakes applications like drug and material discovery grows. The authors propose an extension of sparse autoencoder techniques to reveal and analyze interpretable latent features within CLMs. They apply their method to the FM4M SMI-TED chemistry foundation model to extract meaningful latent features that represent chemical knowledge. By analyzing activation patterns across various molecular datasets, they demonstrate that these models encode rich chemical concepts. The study identifies correlations between specific latent features and distinct chemical knowledge domains such as structural motifs, physicochemical properties, and pharmacological drug classes. This framework not only enhances foundational understanding of how CLMs represent chemistry internally but also offers practical benefits by improving interpretability, thereby potentially accelerating computational chemistry research and deployment. The approach presents a generalizable pathway to uncover latent knowledge in AI systems focused on chemistry, contributing to more transparent and trustworthy AI in this domain. <div>
arXiv:2512.08077v1 Announce Type: new 
Abstract: Since the advent of machine learning, interpretability has remained a persistent challenge, becoming increasingly urgent as generative models support high-stakes applications in drug and material discovery. Recent advances in large language model (LLM) architectures have yielded chemistry language models (CLMs) with impressive capabilities in molecular property prediction and molecular generation. However, how these models internally represent chemical knowledge remains poorly understood. In this work, we extend sparse autoencoder techniques to uncover and examine interpretable features within CLMs. Applying our methodology to the Foundation Models for Materials (FM4M) SMI-TED chemistry foundation model, we extract semantically meaningful latent features and analyse their activation patterns across diverse molecular datasets. Our findings reveal that these models encode a rich landscape of chemical concepts. We identify correlations between specific latent features and distinct domains of chemical knowledge, including structural motifs, physicochemical properties, and pharmacological drug classes. Our approach provides a generalisable framework for uncovering latent knowledge in chemistry-focused AI systems. This work has implications for both foundational understanding and practical deployment; with the potential to accelerate computational chemistry research.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complexity of One-Dimensional ReLU DNNs</title>
<link>https://arxiv.org/abs/2512.08091</link>
<guid>https://arxiv.org/abs/2512.08091</guid>
<content:encoded><![CDATA[
<div> Expressivity, 1D ReLU networks, linear regions, infinite-width limit, function-adaptive sparsity<br /><br />Summary:<br /><br />1. This paper investigates the expressivity of one-dimensional (1D) ReLU deep neural networks by analyzing their linear regions, which serve as a measure of the network’s complexity and function representation capability. 2. For fully connected 1D ReLU networks with He initialization and nonzero biases, the authors consider the infinite-width limit scenario, where the number of neurons in each layer grows large. 3. They prove that the expected number of linear regions in such networks scales approximately as the sum of the neurons across all hidden layers, specifically \(\sum_{i=1}^L n_i + o\left(\sum_{i=1}^L n_i\right) + 1\), indicating a nearly linear growth in the expected complexity with width and depth. 4. Beyond quantifying the total number of linear regions, the authors introduce a novel concept of function-adaptive sparsity, which compares the expected linear regions utilized by the network to the minimal number of linear pieces required to approximate a given target function within a specified error tolerance. 5. This approach provides insight into how efficiently the network uses its capacity to approximate functions, offering a refined perspective on the relationship between network architecture, initialization, and function approximation efficiency. <div>
arXiv:2512.08091v1 Announce Type: new 
Abstract: We study the expressivity of one-dimensional (1D) ReLU deep neural networks through the lens of their linear regions. For randomly initialized, fully connected 1D ReLU networks (He scaling with nonzero bias) in the infinite-width limit, we prove that the expected number of linear regions grows as $\sum_{i = 1}^L n_i + \mathop{{o}}\left(\sum_{i = 1}^L{n_i}\right) + 1$, where $n_\ell$ denotes the number of neurons in the $\ell$-th hidden layer. We also propose a function-adaptive notion of sparsity that compares the expected regions used by the network to the minimal number needed to approximate a target within a fixed tolerance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training LLMs for Honesty via Confessions</title>
<link>https://arxiv.org/abs/2512.08093</link>
<guid>https://arxiv.org/abs/2512.08093</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, honesty, reinforcement learning, confessions, model misbehavior<br /><br />Summary:<br /><br />1. Large language models (LLMs) can exhibit dishonesty by overstating confidence or hiding covert actions, partly due to reinforcement learning challenges such as reward shaping.  
2. The paper proposes a novel method called a *confession*, where after giving a primary answer, the model outputs a self-reported confession detailing its adherence to policies and instructions.  
3. The training reward for confessions is based exclusively on honesty, without influencing the reward for the main answer, incentivizing truthful admissions of misbehavior if admitting is easier than hiding it.  
4. The authors empirically validate this assumption by training GPT-5-Thinking to produce confessions, evaluating its honesty on various difficult tasks like hallucination, instruction following, scheming, and reward hacking.  
5. Results show that even when the main answer is dishonest or incomplete, the model often truthfully confesses these issues, and confession honesty improves with training. Confessions enable practical monitoring, intervention, and user transparency during inference. <div>
arXiv:2512.08093v1 Announce Type: new 
Abstract: Large language models (LLMs) can be dishonest when reporting on their actions and beliefs -- for example, they may overstate their confidence in factual claims or cover up evidence of covert actions. Such dishonesty may arise due to the effects of reinforcement learning (RL), where challenges with reward shaping can result in a training process that inadvertently incentivizes the model to lie or misrepresent its actions.
  In this work we propose a method for eliciting an honest expression of an LLM's shortcomings via a self-reported *confession*. A confession is an output, provided upon request after a model's original answer, that is meant to serve as a full account of the model's compliance with the letter and spirit of its policies and instructions. The reward assigned to a confession during training is solely based on its honesty, and does not impact positively or negatively the main answer's reward. As long as the "path of least resistance" for maximizing confession reward is to surface misbehavior rather than covering it up, this incentivizes models to be honest in their confessions. Our findings provide some justification this empirical assumption, especially in the case of egregious model misbehavior.
  To demonstrate the viability of our approach, we train GPT-5-Thinking to produce confessions, and we evaluate its honesty in out-of-distribution scenarios measuring hallucination, instruction following, scheming, and reward hacking. We find that when the model lies or omits shortcomings in its "main" answer, it often confesses to these behaviors honestly, and this confession honesty modestly improves with training. Confessions can enable a number of inference-time interventions including monitoring, rejection sampling, and surfacing issues to the user.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Offline Model-Based RL with Action Chunks</title>
<link>https://arxiv.org/abs/2512.08108</link>
<guid>https://arxiv.org/abs/2512.08108</guid>
<content:encoded><![CDATA[
<div> Keywords: model-based reinforcement learning, value expansion, offline RL, action-chunk model, rejection sampling<br /><br />Summary:<br /><br />This paper investigates the utility of model-based reinforcement learning (RL), specifically model-based value expansion, for managing complex, long-horizon offline RL tasks. Model-based value expansion involves fitting an on-policy value function using rollouts of length n generated by a current policy and a learned dynamics model, where increasing rollout length reduces bias but increases accumulated model errors. To mitigate this trade-off, the authors introduce an action-chunk model that predicts future states based on sequences of actions rather than single actions, thereby reducing error accumulation over long horizons. Additionally, instead of directly training policies to maximize rewards, the authors employ rejection sampling guided by a behavioral action-chunk policy, which helps avoid exploitation of the model by out-of-distribution actions. This combined approach is termed Model-Based RL with Action Chunks (MAC). Experimental evaluation on challenging tasks with large-scale datasets containing up to 100 million transitions demonstrates that MAC outperforms existing offline model-based RL algorithms, especially in difficult long-horizon scenarios. The study highlights MAC as a scalable and effective method for offline RL by addressing model error accumulation and policy robustness through innovative action chunking and sampling techniques. <div>
arXiv:2512.08108v1 Announce Type: new 
Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic</title>
<link>https://arxiv.org/abs/2512.08121</link>
<guid>https://arxiv.org/abs/2512.08121</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Classifier Evaluation, Youden's J Statistic, Balanced Accuracy, Prevalence Estimation<br /><br />Summary:<br />1. The paper addresses the challenge of rigorously evaluating large language models (LLMs) by comparing the prevalence of desirable or undesirable behaviors, such as task success rates or policy violations.<br />2. Prevalence estimates are generated by classifiers which can be either LLMs acting as judges or human annotators, making the choice of classifier critical for trustworthy LLM evaluation.<br />3. Common metrics like Accuracy, Precision, and F1-score are shown to be problematic due to their sensitivity to class imbalance and arbitrary positive class definitions, which can lead to distorted prevalence estimates.<br />4. The authors propose that Youden’s J statistic is theoretically optimal for selecting classifiers (judges) that produce reliable model comparisons, emphasizing its alignment with the goal of accurate prevalence estimation.<br />5. Balanced Accuracy is demonstrated as a linear transformation of Youden’s J, and both theoretical analysis and empirical simulations confirm that using Balanced Accuracy for judge selection leads to more robust and accurate evaluation of LLM behaviors.<br />This work highlights the importance of appropriate metric choice in classifier evaluation to improve the reliability of LLM assessments. <div>
arXiv:2512.08121v1 Announce Type: new 
Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-only cryptocurrency portfolio management by ranking the assets: a neural network approach</title>
<link>https://arxiv.org/abs/2512.08124</link>
<guid>https://arxiv.org/abs/2512.08124</guid>
<content:encoded><![CDATA[
<div> Keywords: cryptocurrency, portfolio management, machine learning, neural network, backtesting  

<br /><br />Summary:  
This paper proposes a novel machine learning-based portfolio management method tailored for the cryptocurrency market. Unlike previous studies that focus on predicting the price movement of individual cryptocurrencies such as Bitcoin (BTC), this approach manages a group of cryptocurrencies by analyzing their relative relationships. At each time step, a neural network predicts the rank order of future returns among the selected cryptocurrencies, and portfolio weights are assigned accordingly. This cross-sectional perspective allows for better capture of market dynamics across multiple assets. The method was tested via backtesting on real daily cryptocurrency data spanning from May 2020 to November 2023, a period covering bullish, bearish, and stagnant market phases. Despite the complex and volatile market conditions, the proposed method demonstrated superior performance compared to existing approaches, achieving a Sharpe ratio of 1.01 and an annualized return of 64.26%. Additionally, the approach maintained its robustness when transaction fees were increased, indicating practical viability for real-world trading scenarios. The findings suggest that leveraging relative ranking and group dynamics through machine learning can significantly enhance cryptocurrency portfolio management outcomes. <div>
arXiv:2512.08124v1 Announce Type: new 
Abstract: This paper will propose a novel machine learning based portfolio management method in the context of the cryptocurrency market. Previous researchers mainly focus on the prediction of the movement for specific cryptocurrency such as the bitcoin(BTC) and then trade according to the prediction. In contrast to the previous work that treats the cryptocurrencies independently, this paper manages a group of cryptocurrencies by analyzing the relative relationship. Specifically, in each time step, we utilize the neural network to predict the rank of the future return of the managed cryptocurrencies and place weights accordingly. By incorporating such cross-sectional information, the proposed methods is shown to profitable based on the backtesting experiments on the real daily cryptocurrency market data from May, 2020 to Nov, 2023. During this 3.5 years, the market experiences the full cycle of bullish, bearish and stagnant market conditions. Despite under such complex market conditions, the proposed method outperforms the existing methods and achieves a Sharpe ratio of 1.01 and annualized return of 64.26%. Additionally, the proposed method is shown to be robust to the increase of transaction fee.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization</title>
<link>https://arxiv.org/abs/2512.08129</link>
<guid>https://arxiv.org/abs/2512.08129</guid>
<content:encoded><![CDATA[
<div> Keywords: backdoor detection, intrinsic features, target class, Class Subspace Orthogonalization, constrained optimization<br /><br />Summary:<br />Most existing post-training backdoor detection methods identify attacks by looking for extreme outlier statistics in the target class compared to non-target classes. However, they can fail when some non-target classes are easily distinguishable on their own or when the backdoor trigger is subtle and weak relative to intrinsic class features. The authors observe that while the detection statistic for a target class includes contributions from both its intrinsic features and the backdoor trigger, non-target classes' statistics arise solely from intrinsic features. To improve sensitivity, the paper proposes suppressing intrinsic features when optimizing the detection statistic for each class. This approach drastically lowers the statistic in non-target classes but preserves it significantly in the target class due to the backdoor trigger. Their method, called Class Subspace Orthogonalization (CSO), formulates this idea as a constrained optimization problem using a small set of clean examples to orthogonalize the detection statistic against intrinsic features. The approach is plug-and-play and is empirically evaluated against challenging mixed-label and adaptive backdoor attacks, showing improved detection performance. <div>
arXiv:2512.08129v1 Announce Type: new 
Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture</title>
<link>https://arxiv.org/abs/2512.08130</link>
<guid>https://arxiv.org/abs/2512.08130</guid>
<content:encoded><![CDATA[
<div> Biothreat Benchmark, Bacterial Biothreat Schema, Biosecurity Risk, Large Language Models, AI Model Evaluation<br /><br />Summary:  
This paper introduces the Biothreat Benchmark Generation (BBG) Framework, designed to quantify and mitigate biosecurity risks posed by rapidly evolving AI models, especially large language models (LLMs). It highlights the need for robust benchmarks that assess the potential for bioterrorism and misuse of biological weapons facilitated by AI. The BBG Framework incorporates a hierarchical structure composed of biothreat categories, elements, and tasks, enabling development of task-aligned queries for evaluation. The current focus is on bacterial biological threats as a pilot domain. Unlike previous benchmarking efforts, BBG accounts for different adversary capability levels and operational risk factors, in addition to technical considerations. The Bacterial Biothreat Schema defines a task-query architecture that forms the basis for future prompt design and model testing. Subsequent work will detail the transformation of queries into prompts and demonstrate benchmark implementation for AI model evaluation. Overall, the BBG Framework offers a reusable, comprehensive evaluation tool that addresses both technical and operational dimensions of biological threat modeling. It aims to provide policymakers and developers with reliable metrics to assess the uplift in biosecurity risks from existing and emerging LLMs across multiple threat aggregation levels. <div>
arXiv:2512.08130v1 Announce Type: new 
Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Agents in Open-Ended Worlds</title>
<link>https://arxiv.org/abs/2512.08139</link>
<guid>https://arxiv.org/abs/2512.08139</guid>
<content:encoded><![CDATA[
<div> Keywords: AI robustness, reinforcement learning, procedural content generation, multi-agent learning, adversarial prompts<br /><br />Summary:<br /><br />This thesis addresses the challenge of building robust AI agents capable of generalizing across diverse, unseen environments and interactions. It introduces MiniHack, a sandbox framework based on NetHack, which uses procedural content generation to create varied reinforcement learning tasks focused on promoting agent generalization. The work then proposes Maestro, an innovative adversarial curriculum generation method designed to progressively improve the robustness and generality of agents in two-player zero-sum games. It further explores robustness in multi-agent settings by applying quality-diversity algorithms to detect weaknesses in pre-trained reinforcement learning policies within a complex video game football environment that features both cooperative and competitive dynamics. Finally, the thesis extends the investigation of robustness to large language models (LLMs), where evolutionary search techniques are leveraged to generate adversarial prompts that provoke undesirable responses, thereby diagnosing and enhancing model resilience. Collectively, these contributions advance AI robustness research, enabling the creation of agents that not only adapt effectively to evolving challenges but also maintain reliable performance when confronted with novel scenarios and adversarial conditions. <div>
arXiv:2512.08139v1 Announce Type: new 
Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection</title>
<link>https://arxiv.org/abs/2512.08143</link>
<guid>https://arxiv.org/abs/2512.08143</guid>
<content:encoded><![CDATA[
<div> Language Identification, Transformer, Contrastive Learning, Multilingual, PolyLingua  

<br /><br />Summary:  
Language identification is a critical initial step in multilingual systems like chatbots and virtual assistants, ensuring accurate and culturally appropriate user interactions. Existing tools often fall short, especially in complex cases such as music requests where song titles and spoken user language may differ. Current open-source tools like LangDetect and FastText offer speed but lack accuracy, while large language models deliver accuracy at the cost of higher computational resources, making them unsuitable for low-latency or resource-limited environments. The paper introduces PolyLingua, a lightweight Transformer-based model designed for in-domain language detection and fine-grained classification. PolyLingua utilizes a novel two-level contrastive learning framework that combines instance-level separation with class-level alignment using adaptive margins. This approach produces compact and well-separated embeddings that can distinguish even closely related languages effectively. The model is evaluated on two challenging datasets: Amazon Massive, which includes multilingual digital assistant utterances, and a Song dataset characterized by frequent code-switching in music requests. PolyLingua achieves outstanding results, reaching an F1 score of 99.25% and 98.15% on these datasets respectively, outperforming the Sonnet 3.5 model while using ten times fewer parameters. This makes PolyLingua highly suitable for environments with strict computational and latency constraints. <div>
arXiv:2512.08143v1 Announce Type: new 
Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08153</link>
<guid>https://arxiv.org/abs/2512.08153</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Generative Models, Sample Efficiency, Credit Assignment, Tree Search<br /><br />Summary:<br /><br />1. The paper introduces TreeGRPO, a novel reinforcement learning (RL) framework aimed at improving post-training efficiency for aligning generative models with human preferences.<br />2. TreeGRPO transforms the denoising process into a search tree structure, allowing multiple candidate trajectories to be generated from shared initial noise samples and enabling efficient reuse of their common prefixes.<br />3. This approach achieves high sample efficiency by delivering better performance with the same number of training samples compared to traditional methods.<br />4. It provides fine-grained credit assignment through reward backpropagation, which computes step-specific advantages and addresses the uniform credit assignment limitation found in trajectory-based RL methods.<br />5. The multi-child branching design allows amortized computation, enabling multiple policy updates in a single forward pass, thereby improving computational efficiency.<br />6. Experiments on diffusion and flow-based generative models show that TreeGRPO trains 2.4 times faster than baseline methods while achieving a better trade-off between training efficiency and reward.<br />7. Overall, TreeGRPO outperforms GRPO baselines across various benchmarks and reward models, offering a scalable, effective solution for RL-based visual generative model alignment.<br />8. Additional resources and project details are available at the project website: treegrpo.github.io. <div>
arXiv:2512.08153v1 Announce Type: new 
Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LayerPipe2: Multistage Pipelining and Weight Recompute via Improved Exponential Moving Average for Training Neural Networks</title>
<link>https://arxiv.org/abs/2512.08160</link>
<guid>https://arxiv.org/abs/2512.08160</guid>
<content:encoded><![CDATA[
<div> Keywords: LayerPipe2, pipelined training, gradient delay, delayed gradient adaptation, memory optimization

<br /><br />Summary:  
This paper, LayerPipe2, builds upon the previous LayerPipe framework by providing a principled understanding of the gradient delay introduced during pipelined training of neural networks. First, it formally derives the LayerPipe method using variable delayed gradient adaptation and retiming techniques, clarifying where and how much delay can be legally inserted in the network. Second, it reveals that the delay needed at each layer depends on the network structure: inner layers require fewer delays while outer layers require more. Third, when pipelining is applied at every layer individually, the delay relates directly to the number of downstream stages remaining; however, when layers are grouped, all layers in the group share the same delay assignment. Fourth, the paper identifies a major challenge with pipelining—the need to store historical weights due to delays, which can impose a significant memory burden. Finally, to address this, the authors develop a pipeline-aware moving average method that reconstructs past weight states instead of storing them explicitly, reducing memory costs without losing accuracy. Overall, LayerPipe2 presents a comprehensive framework for designing pipelined neural network training architectures that predict delay requirements and optimize storage, enabling scalable training with controlled communication and computation trade-offs. <div>
arXiv:2512.08160v1 Announce Type: new 
Abstract: In our prior work, LayerPipe, we had introduced an approach to accelerate training of convolutional, fully connected, and spiking neural networks by overlapping forward and backward computation. However, despite empirical success, a principled understanding of how much gradient delay needs to be introduced at each layer to achieve desired level of pipelining was not addressed. This paper, LayerPipe2, fills that gap by formally deriving LayerPipe using variable delayed gradient adaptation and retiming. We identify where delays may be legally inserted and show that the required amount of delay follows directly from the network structure where inner layers require fewer delays and outer layers require longer delays. When pipelining is applied at every layer, the amount of delay depends only on the number of remaining downstream stages. When layers are pipelined in groups, all layers in the group share the same assignment of delays. These insights not only explain previously observed scheduling patterns but also expose an often overlooked challenge that pipelining implicitly requires storage of historical weights. We overcome this storage bottleneck by developing a pipeline--aware moving average that reconstructs the required past states rather than storing them explicitly. This reduces memory cost without sacrificing the accuracy guarantees that makes pipelined learning viable. The result is a principled framework that illustrates how to construct LayerPipe architectures, predicts their delay requirements, and mitigates their storage burden, thereby enabling scalable pipelined training with controlled communication computation tradeoffs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones</title>
<link>https://arxiv.org/abs/2512.08211</link>
<guid>https://arxiv.org/abs/2512.08211</guid>
<content:encoded><![CDATA[
<div> Mobile phones, fine-tuning, large language models, on-device training, energy optimization<br /><br />Summary:<br /><br />1. MobileFineTuner is an open-source framework designed to enable end-to-end fine-tuning of large language models (LLMs) directly on commodity mobile phones. <br /><br />2. It addresses the scarcity of high-quality public data by leveraging private user data locally while preserving user privacy. <br /><br />3. The framework supports both full-parameter fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT), facilitating scalable and flexible model adaptation on limited hardware.<br /><br />4. To overcome memory and energy constraints of mobile devices, MobileFineTuner incorporates system-level optimizations such as parameter sharding, gradient accumulation, and energy-aware computation scheduling.<br /><br />5. The practicality and effectiveness of MobileFineTuner are demonstrated through fine-tuning well-known models like GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones, supported by extensive experiments and ablation studies.<br /><br />6. This framework provides a foundational platform for future research in on-device LLM training, expanding opportunities for personalized AI applications on ubiquitous mobile devices. <div>
arXiv:2512.08211v1 Announce Type: new 
Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Correction of Decoupled Weight Decay</title>
<link>https://arxiv.org/abs/2512.08217</link>
<guid>https://arxiv.org/abs/2512.08217</guid>
<content:encoded><![CDATA[
<div> Decoupled weight decay, AdamW, learning rate scaling, steady state, Total Update Contribution<br /><br />Summary:<br /><br />This paper challenges the common practice of setting decoupled weight decay in AdamW proportional to the learning rate \(\gamma\), proposing instead it should scale with \(\gamma^2\). Previous work argued for the \(\gamma^2\) scaling based on the orthogonality of gradients at steady state, but the authors demonstrate that removing the perpendicular update component minimally impacts training dynamics. They derive that scaling weight decay as \(\propto \gamma^2\) promotes stable weight norms, relying on the assumption that updates at steady state become independent of weight values, applicable regardless of the optimizer used. Furthermore, the authors analyze the Total Update Contribution (TUC) in the context of the Scion optimizer, establishing that TUC aligns better with a momentum-adjusted effective learning rate whose optimal setting transfers across settings. Empirical validation confirms that using \(\propto \gamma^2\) weight decay stabilizes weight and gradient norms more effectively, allowing better control of training dynamics. This improved control consequently yields enhanced model performance. Therefore, the paper provides both theoretical insights and experimental evidence supporting the revised scaling rule for decoupled weight decay to improve optimization stability and outcomes in deep learning training. <div>
arXiv:2512.08217v1 Announce Type: new 
Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $\gamma$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto \gamma^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto \gamma^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto \gamma^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PR-CapsNet: Pseudo-Riemannian Capsule Network with Adaptive Curvature Routing for Graph Learning</title>
<link>https://arxiv.org/abs/2512.08218</link>
<guid>https://arxiv.org/abs/2512.08218</guid>
<content:encoded><![CDATA[
<div> Capsule Networks, Pseudo-Riemannian Manifolds, Graph Representation Learning, Adaptive Curvature, Tangent Space Routing<br /><br />Summary:<br /><br />This paper addresses the limitation of traditional Capsule Networks (CapsNets) in modeling complex real-world graph geometries by extending capsule routing from Euclidean spaces to pseudo-Riemannian manifolds with adaptive curvature. The proposed Pseudo-Riemannian Capsule Network (PR-CapsNet) enhances CapsNets by introducing Adaptive Pseudo-Riemannian Tangent Space Routing, which decomposes capsule states into spherical-temporal and Euclidean-spatial components using diffeomorphic transformations. This enables the network to better capture hierarchical and cluster or cyclic structures in graphs, overcoming the shortcomings of fixed-curvature or subspace-partitioning methods. The model adapts feature fusion from different curvature spaces through a learnable curvature tensor coupled with geometric attention informed by local manifold properties. Additionally, the PR-CapsNet incorporates a geometric properties-preserved Pseudo-Riemannian Capsule Classifier that projects embeddings onto tangent spaces and applies curvature-weighted softmax for classification tasks. Extensive experiments on node and graph classification benchmarks demonstrate that PR-CapsNet surpasses state-of-the-art models, validating its superior representation capability for complex graph structures. This work opens new directions for leveraging non-Euclidean geometries in deep capsule architectures for graph data. <div>
arXiv:2512.08218v1 Announce Type: new 
Abstract: Capsule Networks (CapsNets) show exceptional graph representation capacity via dynamic routing and vectorized hierarchical representations, but they model the complex geometries of real\-world graphs poorly by fixed\-curvature space due to the inherent geodesical disconnectedness issues, leading to suboptimal performance. Recent works find that non\-Euclidean pseudo\-Riemannian manifolds provide specific inductive biases for embedding graph data, but how to leverage them to improve CapsNets is still underexplored. Here, we extend the Euclidean capsule routing into geodesically disconnected pseudo\-Riemannian manifolds and derive a Pseudo\-Riemannian Capsule Network (PR\-CapsNet), which models data in pseudo\-Riemannian manifolds of adaptive curvature, for graph representation learning. Specifically, PR\-CapsNet enhances the CapsNet with Adaptive Pseudo\-Riemannian Tangent Space Routing by utilizing pseudo\-Riemannian geometry. Unlike single\-curvature or subspace\-partitioning methods, PR\-CapsNet concurrently models hierarchical and cluster or cyclic graph structures via its versatile pseudo\-Riemannian metric. It first deploys Pseudo\-Riemannian Tangent Space Routing to decompose capsule states into spherical\-temporal and Euclidean\-spatial subspaces with diffeomorphic transformations. Then, an Adaptive Curvature Routing is developed to adaptively fuse features from different curvature spaces for complex graphs via a learnable curvature tensor with geometric attention from local manifold properties. Finally, a geometric properties\-preserved Pseudo\-Riemannian Capsule Classifier is developed to project capsule embeddings to tangent spaces and use curvature\-weighted softmax for classification. Extensive experiments on node and graph classification benchmarks show PR\-CapsNet outperforms SOTA models, validating PR\-CapsNet's strong representation power for complex graph structures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Persistent Topological Structures and Cohomological Flows as a Mathematical Framework for Brain-Inspired Representation Learning</title>
<link>https://arxiv.org/abs/2512.08241</link>
<guid>https://arxiv.org/abs/2512.08241</guid>
<content:encoded><![CDATA[
arXiv:2512.08241v1 Announce Type: new 
Abstract: This paper presents a mathematically rigorous framework for brain-inspired representation learning founded on the interplay between persistent topological structures and cohomological flows. Neural computation is reformulated as the evolution of cochain maps over dynamic simplicial complexes, enabling representations that capture invariants across temporal, spatial, and functional brain states. The proposed architecture integrates algebraic topology with differential geometry to construct cohomological operators that generalize gradient-based learning within a homological landscape. Synthetic data with controlled topological signatures and real neural datasets are jointly analyzed using persistent homology, sheaf cohomology, and spectral Laplacians to quantify stability, continuity, and structural preservation. Empirical results demonstrate that the model achieves superior manifold consistency and noise resilience compared to graph neural and manifold-based deep architectures, establishing a coherent mathematical foundation for topology-driven representation learning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes</title>
<link>https://arxiv.org/abs/2512.08246</link>
<guid>https://arxiv.org/abs/2512.08246</guid>
<content:encoded><![CDATA[
arXiv:2512.08246v1 Announce Type: new 
Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wavelet-Accelerated Physics-Informed Quantum Neural Network for Multiscale Partial Differential Equations</title>
<link>https://arxiv.org/abs/2512.08256</link>
<guid>https://arxiv.org/abs/2512.08256</guid>
<content:encoded><![CDATA[
arXiv:2512.08256v1 Announce Type: new 
Abstract: This work proposes a wavelet-based physics-informed quantum neural network framework to efficiently address multiscale partial differential equations that involve sharp gradients, stiffness, rapid local variations, and highly oscillatory behavior. Traditional physics-informed neural networks (PINNs) have demonstrated substantial potential in solving differential equations, and their quantum counterparts, quantum-PINNs, exhibit enhanced representational capacity with fewer trainable parameters. However, both approaches face notable challenges in accurately solving multiscale features. Furthermore, their reliance on automatic differentiation for constructing loss functions introduces considerable computational overhead, resulting in longer training times. To overcome these challenges, we developed a wavelet-accelerated physics-informed quantum neural network that eliminates the need for automatic differentiation, significantly reducing computational complexity. The proposed framework incorporates the multiresolution property of wavelets within the quantum neural network architecture, thereby enhancing the network's ability to effectively capture both local and global features of multiscale problems. Numerical experiments demonstrate that our proposed method achieves superior accuracy while requiring less than five percent of the trainable parameters compared to classical wavelet-based PINNs, resulting in faster convergence. Moreover, it offers a speedup of three to five times compared to existing quantum PINNs, highlighting the potential of the proposed approach for efficiently solving challenging multiscale and oscillatory problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability</title>
<link>https://arxiv.org/abs/2512.08257</link>
<guid>https://arxiv.org/abs/2512.08257</guid>
<content:encoded><![CDATA[
arXiv:2512.08257v1 Announce Type: new 
Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Foundations of Neural Tangents and Infinite-Width Networks</title>
<link>https://arxiv.org/abs/2512.08264</link>
<guid>https://arxiv.org/abs/2512.08264</guid>
<content:encoded><![CDATA[
arXiv:2512.08264v1 Announce Type: new 
Abstract: We investigate the mathematical foundations of neural networks in the infinite-width regime through the Neural Tangent Kernel (NTK). We propose the NTK-Eigenvalue-Controlled Residual Network (NTK-ECRN), an architecture integrating Fourier feature embeddings, residual connections with layerwise scaling, and stochastic depth to enable rigorous analysis of kernel evolution during training. Our theoretical contributions include deriving bounds on NTK dynamics, characterizing eigenvalue evolution, and linking spectral properties to generalization and optimization stability. Empirical results on synthetic and benchmark datasets validate the predicted kernel behavior and demonstrate improved training stability and generalization. This work provides a comprehensive framework bridging infinite-width theory and practical deep-learning architectures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SOFA-FL: Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing</title>
<link>https://arxiv.org/abs/2512.08267</link>
<guid>https://arxiv.org/abs/2512.08267</guid>
<content:encoded><![CDATA[
arXiv:2512.08267v1 Announce Type: new 
Abstract: Federated Learning (FL) faces significant challenges in evolving environments, particularly regarding data heterogeneity and the rigidity of fixed network topologies. To address these issues, this paper proposes \textbf{SOFA-FL} (Self-Organizing Hierarchical Federated Learning with Adaptive Clustered Data Sharing), a novel framework that enables hierarchical federated systems to self-organize and adapt over time.
  The framework is built upon three core mechanisms: (1) \textbf{Dynamic Multi-branch Agglomerative Clustering (DMAC)}, which constructs an initial efficient hierarchical structure; (2) \textbf{Self-organizing Hierarchical Adaptive Propagation and Evolution (SHAPE)}, which allows the system to dynamically restructure its topology through atomic operations -- grafting, pruning, consolidation, and purification -- to adapt to changes in data distribution; and (3) \textbf{Adaptive Clustered Data Sharing}, which mitigates data heterogeneity by enabling controlled partial data exchange between clients and cluster nodes.
  By integrating these mechanisms, SOFA-FL effectively captures dynamic relationships among clients and enhances personalization capabilities without relying on predetermined cluster structures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gHAWK: Local and Global Structure Encoding for Scalable Training of Graph Neural Networks on Knowledge Graphs</title>
<link>https://arxiv.org/abs/2512.08274</link>
<guid>https://arxiv.org/abs/2512.08274</guid>
<content:encoded><![CDATA[
arXiv:2512.08274v1 Announce Type: new 
Abstract: Knowledge Graphs (KGs) are a rich source of structured, heterogeneous data, powering a wide range of applications. A common approach to leverage this data is to train a graph neural network (GNN) on the KG. However, existing message-passing GNNs struggle to scale to large KGs because they rely on the iterative message passing process to learn the graph structure, which is inefficient, especially under mini-batch training, where a node sees only a partial view of its neighborhood. In this paper, we address this problem and present gHAWK, a novel and scalable GNN training framework for large KGs. The key idea is to precompute structural features for each node that capture its local and global structure before GNN training even begins. Specifically, gHAWK introduces a preprocessing step that computes: (a)~Bloom filters to compactly encode local neighborhood structure, and (b)~TransE embeddings to represent each node's global position in the graph. These features are then fused with any domain-specific features (e.g., text embeddings), producing a node feature vector that can be incorporated into any GNN technique. By augmenting message-passing training with structural priors, gHAWK significantly reduces memory usage, accelerates convergence, and improves model accuracy. Extensive experiments on large datasets from the Open Graph Benchmark (OGB) demonstrate that gHAWK achieves state-of-the-art accuracy and lower training time on both node property prediction and link prediction tasks, topping the OGB leaderboard for three graphs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jacobian Aligned Random Forests</title>
<link>https://arxiv.org/abs/2512.08306</link>
<guid>https://arxiv.org/abs/2512.08306</guid>
<content:encoded><![CDATA[
arXiv:2512.08306v1 Announce Type: new 
Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning</title>
<link>https://arxiv.org/abs/2512.08314</link>
<guid>https://arxiv.org/abs/2512.08314</guid>
<content:encoded><![CDATA[
arXiv:2512.08314v1 Announce Type: new 
Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research</title>
<link>https://arxiv.org/abs/2512.08371</link>
<guid>https://arxiv.org/abs/2512.08371</guid>
<content:encoded><![CDATA[
arXiv:2512.08371v1 Announce Type: new 
Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fully Decentralized Certified Unlearning</title>
<link>https://arxiv.org/abs/2512.08443</link>
<guid>https://arxiv.org/abs/2512.08443</guid>
<content:encoded><![CDATA[
arXiv:2512.08443v1 Announce Type: new 
Abstract: Machine unlearning (MU) seeks to remove the influence of specified data from a trained model in response to privacy requests or data poisoning. While certified unlearning has been analyzed in centralized and server-orchestrated federated settings (via guarantees analogous to differential privacy, DP), the decentralized setting -- where peers communicate without a coordinator remains underexplored. We study certified unlearning in decentralized networks with fixed topologies and propose RR-DU, a random-walk procedure that performs one projected gradient ascent step on the forget set at the unlearning client and a geometrically distributed number of projected descent steps on the retained data elsewhere, combined with subsampled Gaussian noise and projection onto a trust region around the original model. We provide (i) convergence guarantees in the convex case and stationarity guarantees in the nonconvex case, (ii) $(\varepsilon,\delta)$ network-unlearning certificates on client views via subsampled Gaussian $R\'enyi$ DP (RDP) with segment-level subsampling, and (iii) deletion-capacity bounds that scale with the forget-to-local data ratio and quantify the effect of decentralization (network mixing and randomized subsampling) on the privacy--utility trade-off. Empirically, on image benchmarks (MNIST, CIFAR-10), RR-DU matches a given $(\varepsilon,\delta)$ while achieving higher test accuracy than decentralized DP baselines and reducing forget accuracy to random guessing ($\approx 10\%$).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models II: Benchmark Generation Process</title>
<link>https://arxiv.org/abs/2512.08451</link>
<guid>https://arxiv.org/abs/2512.08451</guid>
<content:encoded><![CDATA[
arXiv:2512.08451v1 Announce Type: new 
Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper, the second in a series of three, describes the second component of a novel Biothreat Benchmark Generation (BBG) framework: the generation of the Bacterial Biothreat Benchmark (B3) dataset. The development process involved three complementary approaches: 1) web-based prompt generation, 2) red teaming, and 3) mining existing benchmark corpora, to generate over 7,000 potential benchmarks linked to the Task-Query Architecture that was developed during the first component of the project. A process of de-duplication, followed by an assessment of uplift diagnosticity, and general quality control measures, reduced the candidates to a set of 1,010 final benchmarks. This procedure ensured that these benchmarks are a) diagnostic in terms of providing uplift; b) directly relevant to biosecurity threats; and c) are aligned with a larger biosecurity architecture permitting nuanced analysis at different levels of analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset</title>
<link>https://arxiv.org/abs/2512.08459</link>
<guid>https://arxiv.org/abs/2512.08459</guid>
<content:encoded><![CDATA[
arXiv:2512.08459v1 Announce Type: new 
Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata</title>
<link>https://arxiv.org/abs/2512.08462</link>
<guid>https://arxiv.org/abs/2512.08462</guid>
<content:encoded><![CDATA[
arXiv:2512.08462v1 Announce Type: new 
Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Over-Smoothing in GNNs via Nonlocal Message Passing: Algebraic Smoothing and Depth Scalability</title>
<link>https://arxiv.org/abs/2512.08475</link>
<guid>https://arxiv.org/abs/2512.08475</guid>
<content:encoded><![CDATA[
arXiv:2512.08475v1 Announce Type: new 
Abstract: The relationship between Layer Normalization (LN) placement and the over-smoothing phenomenon remains underexplored. We identify a critical dilemma: Pre-LN architectures avoid over-smoothing but suffer from the curse of depth, while Post-LN architectures bypass the curse of depth but experience over-smoothing.
  To resolve this, we propose a new method based on Post-LN that induces algebraic smoothing, preventing over-smoothing without the curse of depth. Empirical results across five benchmarks demonstrate that our approach supports deeper networks (up to 256 layers) and improves performance, requiring no additional parameters.
  Key contributions:
  Theoretical Characterization: Analysis of LN dynamics and their impact on over-smoothing and the curse of depth.
  A Principled Solution: A parameter-efficient method that induces algebraic smoothing and avoids over-smoothing and the curse of depth.
  Empirical Validation: Extensive experiments showing the effectiveness of the method in deeper GNNs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08485</link>
<guid>https://arxiv.org/abs/2512.08485</guid>
<content:encoded><![CDATA[
arXiv:2512.08485v1 Announce Type: new 
Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Distance-Aware Uncertainty Quantification Methods in Physics-Guided Neural Networks for Reliable Bearing Health Prediction</title>
<link>https://arxiv.org/abs/2512.08499</link>
<guid>https://arxiv.org/abs/2512.08499</guid>
<content:encoded><![CDATA[
arXiv:2512.08499v1 Announce Type: new 
Abstract: Accurate and uncertainty-aware degradation estimation is essential for predictive maintenance in safety-critical systems like rotating machinery with rolling-element bearings. Many existing uncertainty methods lack confidence calibration, are costly to run, are not distance-aware, and fail to generalize under out-of-distribution data. We introduce two distance-aware uncertainty methods for deterministic physics-guided neural networks: PG-SNGP, based on Spectral Normalization Gaussian Process, and PG-SNER, based on Deep Evidential Regression. We apply spectral normalization to the hidden layers so the network preserves distances from input to latent space. PG-SNGP replaces the final dense layer with a Gaussian Process layer for distance-sensitive uncertainty, while PG-SNER outputs Normal Inverse Gamma parameters to model uncertainty in a coherent probabilistic form. We assess performance using standard accuracy metrics and a new distance-aware metric based on the Pearson Correlation Coefficient, which measures how well predicted uncertainty tracks the distance between test and training samples. We also design a dynamic weighting scheme in the loss to balance data fidelity and physical consistency. We test our methods on rolling-element bearing degradation using the PRONOSTIA dataset and compare them with Monte Carlo and Deep Ensemble PGNNs. Results show that PG-SNGP and PG-SNER improve prediction accuracy, generalize reliably under OOD conditions, and remain robust to adversarial attacks and noise.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Model for Stock Market Forecasting: Integrating News Sentiment and Time Series Data with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.08567</link>
<guid>https://arxiv.org/abs/2512.08567</guid>
<content:encoded><![CDATA[
arXiv:2512.08567v1 Announce Type: new 
Abstract: Stock market prediction is a long-standing challenge in finance, as accurate forecasts support informed investment decisions. Traditional models rely mainly on historical prices, but recent work shows that financial news can provide useful external signals. This paper investigates a multimodal approach that integrates companies' news articles with their historical stock data to improve prediction performance. We compare a Graph Neural Network (GNN) model with a baseline LSTM model. Historical data for each company is encoded using an LSTM, while news titles are embedded with a language model. These embeddings form nodes in a heterogeneous graph, and GraphSAGE is used to capture interactions between articles, companies, and industries. We evaluate two targets: a binary direction-of-change label and a significance-based label. Experiments on the US equities and Bloomberg datasets show that the GNN outperforms the LSTM baseline, achieving 53% accuracy on the first target and a 4% precision gain on the second. Results also indicate that companies with more associated news yield higher prediction accuracy. Moreover, headlines contain stronger predictive signals than full articles, suggesting that concise news summaries play an important role in short-term market reactions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset</title>
<link>https://arxiv.org/abs/2512.08591</link>
<guid>https://arxiv.org/abs/2512.08591</guid>
<content:encoded><![CDATA[
arXiv:2512.08591v1 Announce Type: new 
Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning</title>
<link>https://arxiv.org/abs/2512.08671</link>
<guid>https://arxiv.org/abs/2512.08671</guid>
<content:encoded><![CDATA[
arXiv:2512.08671v1 Announce Type: new 
Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Additive Manufacturing Part Qualification Framework: Transferring Knowledge of Stress-strain Behaviors from Additively Manufactured Polymers to Metals</title>
<link>https://arxiv.org/abs/2512.08699</link>
<guid>https://arxiv.org/abs/2512.08699</guid>
<content:encoded><![CDATA[
arXiv:2512.08699v1 Announce Type: new 
Abstract: Part qualification is crucial in additive manufacturing (AM) because it ensures that additively manufactured parts can be consistently produced and reliably used in critical applications. Part qualification aims at verifying that an additively manufactured part meets performance requirements; therefore, predicting the complex stress-strain behaviors of additively manufactured parts is critical. We develop a dynamic time warping (DTW)-transfer learning (TL) framework for additive manufacturing part qualification by transferring knowledge of the stress-strain behaviors of additively manufactured low-cost polymers to metals. Specifically, the framework employs DTW to select a polymer dataset as the source domain that is the most relevant to the target metal dataset. Using a long short-term memory (LSTM) model, four source polymers (i.e., Nylon, PLA, CF-ABS, and Resin) and three target metals (i.e., AlSi10Mg, Ti6Al4V, and carbon steel) that are fabricated by different AM techniques are utilized to demonstrate the effectiveness of the DTW-TL framework. Experimental results show that the DTW-TL framework identifies the closest match between polymers and metals to select one single polymer dataset as the source domain. The DTW-TL model achieves the lowest mean absolute percentage error of 12.41% and highest coefficient of determination of 0.96 when three metals are used as the target domain, respectively, outperforming the vanilla LSTM model without TL as well as the TL model pre-trained on four polymer datasets as the source domain.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search</title>
<link>https://arxiv.org/abs/2512.08724</link>
<guid>https://arxiv.org/abs/2512.08724</guid>
<content:encoded><![CDATA[
arXiv:2512.08724v1 Announce Type: new 
Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data</title>
<link>https://arxiv.org/abs/2512.08732</link>
<guid>https://arxiv.org/abs/2512.08732</guid>
<content:encoded><![CDATA[
arXiv:2512.08732v1 Announce Type: new 
Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.08763</link>
<guid>https://arxiv.org/abs/2512.08763</guid>
<content:encoded><![CDATA[
arXiv:2512.08763v1 Announce Type: new 
Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>De novo generation of functional terpene synthases using TpsGPT</title>
<link>https://arxiv.org/abs/2512.08772</link>
<guid>https://arxiv.org/abs/2512.08772</guid>
<content:encoded><![CDATA[
arXiv:2512.08772v1 Announce Type: new 
Abstract: Terpene synthases (TPS) are a key family of enzymes responsible for generating the diverse terpene scaffolds that underpin many natural products, including front-line anticancer drugs such as Taxol. However, de novo TPS design through directed evolution is costly and slow. We introduce TpsGPT, a generative model for scalable TPS protein design, built by fine-tuning the protein language model ProtGPT2 on 79k TPS sequences mined from UniProt. TpsGPT generated de novo enzyme candidates in silico and we evaluated them using multiple validation metrics, including EnzymeExplorer classification, ESMFold structural confidence (pLDDT), sequence diversity, CLEAN classification, InterPro domain detection, and Foldseek structure alignment. From an initial pool of 28k generated sequences, we identified seven putative TPS enzymes that satisfied all validation criteria. Experimental validation confirmed TPS enzymatic activity in at least two of these sequences. Our results show that fine-tuning of a protein language model on a carefully curated, enzyme-class-specific dataset, combined with rigorous filtering, can enable the de novo generation of functional, evolutionarily distant enzymes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can TabPFN Compete with GNNs for Node Classification via Graph Tabularization?</title>
<link>https://arxiv.org/abs/2512.08798</link>
<guid>https://arxiv.org/abs/2512.08798</guid>
<content:encoded><![CDATA[
arXiv:2512.08798v1 Announce Type: new 
Abstract: Foundation models pretrained on large data have demonstrated remarkable zero-shot generalization capabilities across domains. Building on the success of TabPFN for tabular data and its recent extension to time series, we investigate whether graph node classification can be effectively reformulated as a tabular learning problem. We introduce TabPFN-GN, which transforms graph data into tabular features by extracting node attributes, structural properties, positional encodings, and optionally smoothed neighborhood features. This enables TabPFN to perform direct node classification without any graph-specific training or language model dependencies. Our experiments on 12 benchmark datasets reveal that TabPFN-GN achieves competitive performance with GNNs on homophilous graphs and consistently outperforms them on heterophilous graphs. These results demonstrate that principled feature engineering can bridge the gap between tabular and graph domains, providing a practical alternative to task-specific GNN training and LLM-dependent graph foundation models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying counterfactual probabilities using bivariate distributions and uplift modeling</title>
<link>https://arxiv.org/abs/2512.08805</link>
<guid>https://arxiv.org/abs/2512.08805</guid>
<content:encoded><![CDATA[
arXiv:2512.08805v1 Announce Type: new 
Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models</title>
<link>https://arxiv.org/abs/2512.08832</link>
<guid>https://arxiv.org/abs/2512.08832</guid>
<content:encoded><![CDATA[
arXiv:2512.08832v1 Announce Type: new 
Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning From State and Temporal Differences</title>
<link>https://arxiv.org/abs/2512.08855</link>
<guid>https://arxiv.org/abs/2512.08855</guid>
<content:encoded><![CDATA[
arXiv:2512.08855v1 Announce Type: new 
Abstract: TD($\lambda$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($\lambda$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($\lambda$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($\lambda$), called STD($\lambda$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($\lambda$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($\lambda$) on the two-state system and a variation on the well known acrobot problem.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data</title>
<link>https://arxiv.org/abs/2512.08859</link>
<guid>https://arxiv.org/abs/2512.08859</guid>
<content:encoded><![CDATA[
arXiv:2512.08859v1 Announce Type: new 
Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentially Private Synthetic Data Generation Using Context-Aware GANs</title>
<link>https://arxiv.org/abs/2512.08869</link>
<guid>https://arxiv.org/abs/2512.08869</guid>
<content:encoded><![CDATA[
arXiv:2512.08869v1 Announce Type: new 
Abstract: The widespread use of big data across sectors has raised major privacy concerns, especially when sensitive information is shared or analyzed. Regulations such as GDPR and HIPAA impose strict controls on data handling, making it difficult to balance the need for insights with privacy requirements. Synthetic data offers a promising solution by creating artificial datasets that reflect real patterns without exposing sensitive information. However, traditional synthetic data methods often fail to capture complex, implicit rules that link different elements of the data and are essential in domains like healthcare. They may reproduce explicit patterns but overlook domain-specific constraints that are not directly stated yet crucial for realism and utility. For example, prescription guidelines that restrict certain medications for specific conditions or prevent harmful drug interactions may not appear explicitly in the original data. Synthetic data generated without these implicit rules can lead to medically inappropriate or unrealistic profiles. To address this gap, we propose ContextGAN, a Context-Aware Differentially Private Generative Adversarial Network that integrates domain-specific rules through a constraint matrix encoding both explicit and implicit knowledge. The constraint-aware discriminator evaluates synthetic data against these rules to ensure adherence to domain constraints, while differential privacy protects sensitive details from the original data. We validate ContextGAN across healthcare, security, and finance, showing that it produces high-quality synthetic data that respects domain rules and preserves privacy. Our results demonstrate that ContextGAN improves realism and utility by enforcing domain constraints, making it suitable for applications that require compliance with both explicit patterns and implicit rules under strict privacy guarantees.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents</title>
<link>https://arxiv.org/abs/2512.08870</link>
<guid>https://arxiv.org/abs/2512.08870</guid>
<content:encoded><![CDATA[
arXiv:2512.08870v1 Announce Type: new 
Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation</title>
<link>https://arxiv.org/abs/2512.08875</link>
<guid>https://arxiv.org/abs/2512.08875</guid>
<content:encoded><![CDATA[
arXiv:2512.08875v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process</title>
<link>https://arxiv.org/abs/2512.08879</link>
<guid>https://arxiv.org/abs/2512.08879</guid>
<content:encoded><![CDATA[
arXiv:2512.08879v1 Announce Type: new 
Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Anomaly Detection for Industrial IoT Data Streams</title>
<link>https://arxiv.org/abs/2512.08885</link>
<guid>https://arxiv.org/abs/2512.08885</guid>
<content:encoded><![CDATA[
arXiv:2512.08885v1 Announce Type: new 
Abstract: Industrial maintenance is being transformed by the Internet of Things and edge computing, generating continuous data streams that demand real-time, adaptive decision-making under limited computational resources. While data stream mining (DSM) addresses this challenge, most methods assume fully supervised settings, yet in practice, ground-truth labels are often delayed or unavailable. This paper presents a collaborative DSM framework that integrates unsupervised anomaly detection with interactive, human-in-the-loop learning to support maintenance decisions. We employ an online Isolation Forest and enhance interpretability using incremental Partial Dependence Plots and a feature importance score, derived from deviations of Individual Conditional Expectation curves from a fading average, enabling users to dynamically reassess feature relevance and adjust anomaly thresholds. We describe the real-time implementation and provide initial results for fault detection in a Jacquard loom unit. Ongoing work targets continuous monitoring to predict and explain imminent bearing failures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting the Scaling Properties of Downstream Metrics in Large Language Model Training</title>
<link>https://arxiv.org/abs/2512.08894</link>
<guid>https://arxiv.org/abs/2512.08894</guid>
<content:encoded><![CDATA[
arXiv:2512.08894v1 Announce Type: new 
Abstract: While scaling laws for Large Language Models (LLMs) traditionally focus on proxy metrics like pretraining loss, predicting downstream task performance has been considered unreliable. This paper challenges that view by proposing a direct framework to model the scaling of benchmark performance from the training budget. We find that for a fixed token-to-parameter ratio, a simple power law can accurately describe the scaling behavior of log accuracy on multiple popular downstream tasks. Our results show that the direct approach extrapolates better than the previously proposed two-stage procedure, which is prone to compounding errors. Furthermore, we introduce functional forms that predict accuracy across token-to-parameter ratios and account for inference compute under repeated sampling. We validate our findings on models with up to 17B parameters trained on up to 350B tokens across two dataset mixtures. To support reproducibility and encourage future research, we release the complete set of pretraining losses and downstream evaluation results.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Learning of Density Estimates with Topological Optimization</title>
<link>https://arxiv.org/abs/2512.08895</link>
<guid>https://arxiv.org/abs/2512.08895</guid>
<content:encoded><![CDATA[
arXiv:2512.08895v1 Announce Type: new 
Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open Polymer Challenge: Post-Competition Report</title>
<link>https://arxiv.org/abs/2512.08896</link>
<guid>https://arxiv.org/abs/2512.08896</guid>
<content:encoded><![CDATA[
arXiv:2512.08896v1 Announce Type: new 
Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast and Robust Diffusion Posterior Sampling for MR Image Reconstruction Using the Preconditioned Unadjusted Langevin Algorithm</title>
<link>https://arxiv.org/abs/2512.05791</link>
<guid>https://arxiv.org/abs/2512.05791</guid>
<content:encoded><![CDATA[
arXiv:2512.05791v1 Announce Type: cross 
Abstract: Purpose: The Unadjusted Langevin Algorithm (ULA) in combination with diffusion models can generate high quality MRI reconstructions with uncertainty estimation from highly undersampled k-space data. However, sampling methods such as diffusion posterior sampling or likelihood annealing suffer from long reconstruction times and the need for parameter tuning. The purpose of this work is to develop a robust sampling algorithm with fast convergence.
  Theory and Methods: In the reverse diffusion process used for sampling the posterior, the exact likelihood is multiplied with the diffused prior at all noise scales. To overcome the issue of slow convergence, preconditioning is used. The method is trained on fastMRI data and tested on retrospectively undersampled brain data of a healthy volunteer.
  Results: For posterior sampling in Cartesian and non-Cartesian accelerated MRI the new approach outperforms annealed sampling in terms of reconstruction speed and sample quality.
  Conclusion: The proposed exact likelihood with preconditioning enables rapid and reliable posterior sampling across various MRI reconstruction tasks without the need for parameter tuning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automating High Energy Physics Data Analysis with LLM-Powered Agents</title>
<link>https://arxiv.org/abs/2512.07785</link>
<guid>https://arxiv.org/abs/2512.07785</guid>
<content:encoded><![CDATA[
arXiv:2512.07785v1 Announce Type: cross 
Abstract: We present a proof-of-principle study demonstrating the use of large language model (LLM) agents to automate a representative high energy physics (HEP) analysis. Using the Higgs boson diphoton cross-section measurement as a case study with ATLAS Open Data, we design a hybrid system that combines an LLM-based supervisor-coder agent with the Snakemake workflow manager. In this architecture, the workflow manager enforces reproducibility and determinism, while the agent autonomously generates, executes, and iteratively corrects analysis code in response to user instructions. We define quantitative evaluation metrics including success rate, error distribution, costs per specific task, and average number of API calls, to assess agent performance across multi-stage workflows. To characterize variability across architectures, we benchmark a representative selection of state-of-the-art LLMs spanning the Gemini and GPT-5 series, the Claude family, and leading open-weight models. While the workflow manager ensures deterministic execution of all analysis steps, the final outputs still show stochastic variation. Although we set the temperature to zero, other sampling parameters (e.g., top-p, top-k) remained at their defaults, and some reasoning-oriented models internally adjust these settings. Consequently, the models do not produce fully deterministic results. This study establishes the first LLM-agent-driven automated data-analysis framework in HEP, enabling systematic benchmarking of model capabilities, stability, and limitations in real-world scientific computing environments. The baseline code used in this work is available at https://huggingface.co/HWresearch/LLM4HEP. This work was accepted as a poster at the Machine Learning and the Physical Sciences (ML4PS) workshop at NeurIPS 2025. The initial submission was made on August 30, 2025.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of Cyberbullying in GIF using AI</title>
<link>https://arxiv.org/abs/2512.07838</link>
<guid>https://arxiv.org/abs/2512.07838</guid>
<content:encoded><![CDATA[
arXiv:2512.07838v1 Announce Type: cross 
Abstract: Cyberbullying is a well-known social issue, and it is escalating day by day. Due to the vigorous development of the internet, social media provide many different ways for the user to express their opinions and exchange information. Cyberbullying occurs on social media using text messages, comments, sharing images and GIFs or stickers, and audio and video. Much research has been done to detect cyberbullying on textual data; some are available for images. Very few studies are available to detect cyberbullying on GIFs/stickers. We collect a GIF dataset from Twitter and Applied a deep learning model to detect cyberbullying from the dataset. Firstly, we extracted hashtags related to cyberbullying using Twitter. We used these hashtags to download GIF file using publicly available API GIPHY. We collected over 4100 GIFs including cyberbullying and non cyberbullying. we applied deep learning pre-trained model VGG16 for the detection of the cyberbullying. The deep learning model achieved the accuracy of 97%. Our work provides the GIF dataset for researchers working in this area.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixLM: High-Throughput and Effective LLM Ranking via Text-Embedding Mix-Interaction</title>
<link>https://arxiv.org/abs/2512.07846</link>
<guid>https://arxiv.org/abs/2512.07846</guid>
<content:encoded><![CDATA[
arXiv:2512.07846v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at capturing semantic nuances and therefore show impressive relevance ranking performance in modern recommendation and search systems. However, they suffer from high computational overhead under industrial latency and throughput requirements. In particular, cross-encoder ranking systems often create long context prefill-heavy workloads, as the model has to be presented with the user, query and item information. To this end, we propose MixLM, a novel LLM-based ranking framework, which significantly improves the system throughput via reducing the input context length, while preserving the semantic strength of cross-encoder rankers. In contrast to a standard ranking system where the context is presented to the model as pure text, we propose to use mix-interaction, a mixture of text and embedding tokens to represent the input. Specifically, MixLM encodes all items in the catalog into a few embedding tokens and stores in a nearline cache. The encoded item descriptions are used during online inference, effectively reducing the item length from a few thousand text tokens to a few embedding tokens. We share insights from deploying our MixLM framework to a real-world search application at LinkedIn, including a detailed discussion of our training pipelines, as well as a thorough analysis of our online serving infrastructure optimization. Comparing with strong baselines, MixLM increased throughput by 10.0x under the same latency budget, while maintaining relevance metrics. The efficiency gains delivered by MixLM enabled full-traffic deployment of LLM-powered search, which resulted in a significant 0.47% increase in Daily Active Users (DAU) in online A/B tests.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating LSTM Networks with Neural Levy Processes for Financial Forecasting</title>
<link>https://arxiv.org/abs/2512.07860</link>
<guid>https://arxiv.org/abs/2512.07860</guid>
<content:encoded><![CDATA[
arXiv:2512.07860v1 Announce Type: cross 
Abstract: This paper investigates an optimal integration of deep learning with financial models for robust asset price forecasting. Specifically, we developed a hybrid framework combining a Long Short-Term Memory (LSTM) network with the Merton-L\'evy jump-diffusion model. To optimise this framework, we employed the Grey Wolf Optimizer (GWO) for the LSTM hyperparameter tuning, and we explored three calibration methods for the Merton-Levy model parameters: Artificial Neural Networks (ANNs), the Marine Predators Algorithm (MPA), and the PyTorch-based TorchSDE library. To evaluate the predictive performance of our hybrid model, we compared it against several benchmark models, including a standard LSTM and an LSTM combined with the Fractional Heston model. This evaluation used three real-world financial datasets: Brent oil prices, the STOXX 600 index, and the IT40 index. Performance was assessed using standard metrics, including Mean Squared Error (MSE), Mean Absolute Error(MAE), Mean Squared Percentage Error (MSPE), and the coefficient of determination (R2). Our experimental results demonstrate that the hybrid model, combining a GWO-optimized LSTM network with the Levy-Merton Jump-Diffusion model calibrated using an ANN, outperformed the base LSTM model and all other models developed in this study.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Functional Random Forest with Adaptive Cost-Sensitive Splitting for Imbalanced Functional Data Classification</title>
<link>https://arxiv.org/abs/2512.07888</link>
<guid>https://arxiv.org/abs/2512.07888</guid>
<content:encoded><![CDATA[
arXiv:2512.07888v1 Announce Type: cross 
Abstract: Classification of functional data where observations are curves or trajectories poses unique challenges, particularly under severe class imbalance. Traditional Random Forest algorithms, while robust for tabular data, often fail to capture the intrinsic structure of functional observations and struggle with minority class detection. This paper introduces Functional Random Forest with Adaptive Cost-Sensitive Splitting (FRF-ACS), a novel ensemble framework designed for imbalanced functional data classification. The proposed method leverages basis expansions and Functional Principal Component Analysis (FPCA) to represent curves efficiently, enabling trees to operate on low dimensional functional features. To address imbalance, we incorporate a dynamic cost sensitive splitting criterion that adjusts class weights locally at each node, combined with a hybrid sampling strategy integrating functional SMOTE and weighted bootstrapping. Additionally, curve specific similarity metrics replace traditional Euclidean measures to preserve functional characteristics during leaf assignment. Extensive experiments on synthetic and real world datasets including biomedical signals and sensor trajectories demonstrate that FRF-ACS significantly improves minority class recall and overall predictive performance compared to existing functional classifiers and imbalance handling techniques. This work provides a scalable, interpretable solution for high dimensional functional data analysis in domains where minority class detection is critical.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CrowdLLM: Building LLM-Based Digital Populations Augmented with Generative Models</title>
<link>https://arxiv.org/abs/2512.07890</link>
<guid>https://arxiv.org/abs/2512.07890</guid>
<content:encoded><![CDATA[
arXiv:2512.07890v1 Announce Type: cross 
Abstract: The emergence of large language models (LLMs) has sparked much interest in creating LLM-based digital populations that can be applied to many applications such as social simulation, crowdsourcing, marketing, and recommendation systems. A digital population can reduce the cost of recruiting human participants and alleviate many concerns related to human subject study. However, research has found that most of the existing works rely solely on LLMs and could not sufficiently capture the accuracy and diversity of a real human population. To address this limitation, we propose CrowdLLM that integrates pretrained LLMs and generative models to enhance the diversity and fidelity of the digital population. We conduct theoretical analysis of CrowdLLM regarding its great potential in creating cost-effective, sufficiently representative, scalable digital populations that can match the quality of a real crowd. Comprehensive experiments are also conducted across multiple domains (e.g., crowdsourcing, voting, user rating) and simulation studies which demonstrate that CrowdLLM achieves promising performance in both accuracy and distributional fidelity to human data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can AI autonomously build, operate, and use the entire data stack?</title>
<link>https://arxiv.org/abs/2512.07926</link>
<guid>https://arxiv.org/abs/2512.07926</guid>
<content:encoded><![CDATA[
arXiv:2512.07926v1 Announce Type: cross 
Abstract: Enterprise data management is a monumental task. It spans data architecture and systems, integration, quality, governance, and continuous improvement. While AI assistants can help specific persona, such as data engineers and stewards, to navigate and configure the data stack, they fall far short of full automation. However, as AI becomes increasingly capable of tackling tasks that have previously resisted automation due to inherent complexities, we believe there is an imminent opportunity to target fully autonomous data estates. Currently, AI is used in different parts of the data stack, but in this paper, we argue for a paradigm shift from the use of AI in independent data component operations towards a more holistic and autonomous handling of the entire data lifecycle. Towards that end, we explore how each stage of the modern data stack can be autonomously managed by intelligent agents to build self-sufficient systems that can be used not only by human end-users, but also by AI itself. We begin by describing the mounting forces and opportunities that demand this paradigm shift, examine how agents can streamline the data lifecycle, and highlight open questions and areas where additional research is needed. We hope this work will inspire lively debate, stimulate further research, motivate collaborative approaches, and facilitate a more autonomous future for data systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Defects in Neural Network Field Theories</title>
<link>https://arxiv.org/abs/2512.07946</link>
<guid>https://arxiv.org/abs/2512.07946</guid>
<content:encoded><![CDATA[
arXiv:2512.07946v1 Announce Type: cross 
Abstract: Neural Network Field Theories (NN-FTs) represent a novel construction of arbitrary field theories, including those of conformal fields, through the specification of the network architecture and prior distribution for the network parameters. In this work, we present a formalism for the construction of conformally invariant defects in these NN-FTs. We demonstrate this new formalism in two toy models of NN scalar field theories. We develop an NN interpretation of an expansion akin to the defect OPE in two-point correlation functions in these models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm</title>
<link>https://arxiv.org/abs/2512.07997</link>
<guid>https://arxiv.org/abs/2512.07997</guid>
<content:encoded><![CDATA[
arXiv:2512.07997v1 Announce Type: cross 
Abstract: Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Dynamics from Infrequent Output Measurements for Uncertainty-Aware Optimal Control</title>
<link>https://arxiv.org/abs/2512.08013</link>
<guid>https://arxiv.org/abs/2512.08013</guid>
<content:encoded><![CDATA[
arXiv:2512.08013v1 Announce Type: cross 
Abstract: Reliable optimal control is challenging when the dynamics of a nonlinear system are unknown and only infrequent, noisy output measurements are available. This work addresses this setting of limited sensing by formulating a Bayesian prior over the continuous-time dynamics and latent state trajectory in state-space form and updating it through a targeted marginal Metropolis-Hastings sampler equipped with a numerical ODE integrator. The resulting posterior samples are used to formulate a scenario-based optimal control problem that accounts for both model and measurement uncertainty and is solved using standard nonlinear programming methods. The approach is validated in a numerical case study on glucose regulation using a Type 1 diabetes model.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Diffusion Posterior Sampling for Bayesian Inversion</title>
<link>https://arxiv.org/abs/2512.08022</link>
<guid>https://arxiv.org/abs/2512.08022</guid>
<content:encoded><![CDATA[
arXiv:2512.08022v1 Announce Type: cross 
Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Introduction to Deep Reinforcement and Imitation Learning</title>
<link>https://arxiv.org/abs/2512.08052</link>
<guid>https://arxiv.org/abs/2512.08052</guid>
<content:encoded><![CDATA[
arXiv:2512.08052v1 Announce Type: cross 
Abstract: Embodied agents, such as robots and virtual characters, must continuously select actions to execute tasks effectively, solving complex sequential decision-making problems. Given the difficulty of designing such controllers manually, learning-based approaches have emerged as promising alternatives, most notably Deep Reinforcement Learning (DRL) and Deep Imitation Learning (DIL). DRL leverages reward signals to optimize behavior, while DIL uses expert demonstrations to guide learning. This document introduces DRL and DIL in the context of embodied agents, adopting a concise, depth-first approach to the literature. It is self-contained, presenting all necessary mathematical and machine learning concepts as they are needed. It is not intended as a survey of the field; rather, it focuses on a small set of foundational algorithms and techniques, prioritizing in-depth understanding over broad coverage. The material ranges from Markov Decision Processes to REINFORCE and Proximal Policy Optimization (PPO) for DRL, and from Behavioral Cloning to Dataset Aggregation (DAgger) and Generative Adversarial Imitation Learning (GAIL) for DIL.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-aware PageRank via Edge Reweighting</title>
<link>https://arxiv.org/abs/2512.08055</link>
<guid>https://arxiv.org/abs/2512.08055</guid>
<content:encoded><![CDATA[
arXiv:2512.08055v1 Announce Type: cross 
Abstract: Link-analysis algorithms, such as PageRank, are instrumental in understanding the structural dynamics of networks by evaluating the importance of individual vertices based on their connectivity. Recently, with the rising importance of responsible AI, the question of fairness in link-analysis algorithms has gained traction. In this paper, we present a new approach for incorporating group fairness into the PageRank algorithm by reweighting the transition probabilities in the underlying transition matrix. We formulate the problem of achieving fair PageRank by seeking to minimize the fairness loss, which is the difference between the original group-wise PageRank distribution and a target PageRank distribution. We further define a group-adapted fairness notion, which accounts for group homophily by considering random walks with group-biased restart for each group. Since the fairness loss is non-convex, we propose an efficient projected gradient-descent method for computing locally-optimal edge weights. Unlike earlier approaches, we do not recommend adding new edges to the network, nor do we adjust the restart vector. Instead, we keep the topology of the underlying network unchanged and only modify the relative importance of existing edges. We empirically compare our approach with state-of-the-art baselines and demonstrate the efficacy of our method, where very small changes in the transition matrix lead to significant improvement in the fairness of the PageRank algorithm.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-agent learning under uncertainty: Recurrence vs. concentration</title>
<link>https://arxiv.org/abs/2512.08132</link>
<guid>https://arxiv.org/abs/2512.08132</guid>
<content:encoded><![CDATA[
arXiv:2512.08132v1 Announce Type: cross 
Abstract: In this paper, we examine the convergence landscape of multi-agent learning under uncertainty. Specifically, we analyze two stochastic models of regularized learning in continuous games -- one in continuous and one in discrete time with the aim of characterizing the long-run behavior of the induced sequence of play. In stark contrast to deterministic, full-information models of learning (or models with a vanishing learning rate), we show that the resulting dynamics do not converge in general. In lieu of this, we ask instead which actions are played more often in the long run, and by how much. We show that, in strongly monotone games, the dynamics of regularized learning may wander away from equilibrium infinitely often, but they always return to its vicinity in finite time (which we estimate), and their long-run distribution is sharply concentrated around a neighborhood thereof. We quantify the degree of this concentration, and we show that these favorable properties may all break down if the underlying game is not strongly monotone -- underscoring in this way the limits of regularized learning in the presence of persistent randomness and uncertainty.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust equilibria in continuous games: From strategic to dynamic robustness</title>
<link>https://arxiv.org/abs/2512.08138</link>
<guid>https://arxiv.org/abs/2512.08138</guid>
<content:encoded><![CDATA[
arXiv:2512.08138v1 Announce Type: cross 
Abstract: In this paper, we examine the robustness of Nash equilibria in continuous games, under both strategic and dynamic uncertainty. Starting with the former, we introduce the notion of a robust equilibrium as those equilibria that remain invariant to small -- but otherwise arbitrary -- perturbations to the game's payoff structure, and we provide a crisp geometric characterization thereof. Subsequently, we turn to the question of dynamic robustness, and we examine which equilibria may arise as stable limit points of the dynamics of "follow the regularized leader" (FTRL) in the presence of randomness and uncertainty. Despite their very distinct origins, we establish a structural correspondence between these two notions of robustness: strategic robustness implies dynamic robustness, and, conversely, the requirement of strategic robustness cannot be relaxed if dynamic robustness is to be maintained. Finally, we examine the rate of convergence to robust equilibria as a function of the underlying regularizer, and we show that entropically regularized learning converges at a geometric rate in games with affinely constrained action spaces.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Worst-case generation via minimax optimization in Wasserstein space</title>
<link>https://arxiv.org/abs/2512.08176</link>
<guid>https://arxiv.org/abs/2512.08176</guid>
<content:encoded><![CDATA[
arXiv:2512.08176v1 Announce Type: cross 
Abstract: Worst-case generation plays a critical role in evaluating robustness and stress-testing systems under distribution shifts, in applications ranging from machine learning models to power grids and medical prediction systems. We develop a generative modeling framework for worst-case generation for a pre-specified risk, based on min-max optimization over continuous probability distributions, namely the Wasserstein space. Unlike traditional discrete distributionally robust optimization approaches, which often suffer from scalability issues, limited generalization, and costly worst-case inference, our framework exploits the Brenier theorem to characterize the least favorable (worst-case) distribution as the pushforward of a transport map from a continuous reference measure, enabling a continuous and expressive notion of risk-induced generation beyond classical discrete DRO formulations. Based on the min-max formulation, we propose a Gradient Descent Ascent (GDA)-type scheme that updates the decision model and the transport map in a single loop, establishing global convergence guarantees under mild regularity assumptions and possibly without convexity-concavity. We also propose to parameterize the transport map using a neural network that can be trained simultaneously with the GDA iterations by matching the transported training samples, thereby achieving a simulation-free approach. The efficiency of the proposed method as a risk-induced worst-case generator is validated by numerical experiments on synthetic and image data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tumor-anchored deep feature random forests for out-of-distribution detection in lung cancer segmentation</title>
<link>https://arxiv.org/abs/2512.08216</link>
<guid>https://arxiv.org/abs/2512.08216</guid>
<content:encoded><![CDATA[
arXiv:2512.08216v1 Announce Type: cross 
Abstract: Accurate segmentation of cancerous lesions from 3D computed tomography (CT) scans is essential for automated treatment planning and response assessment. However, even state-of-the-art models combining self-supervised learning (SSL) pretrained transformers with convolutional decoders are susceptible to out-of-distribution (OOD) inputs, generating confidently incorrect tumor segmentations, posing risks for safe clinical deployment. Existing logit-based methods suffer from task-specific model biases, while architectural enhancements to explicitly detect OOD increase parameters and computational costs. Hence, we introduce a plug-and-play and lightweight post-hoc random forests-based OOD detection framework called RF-Deep that leverages deep features with limited outlier exposure. RF-Deep enhances generalization to imaging variations by repurposing the hierarchical features from the pretrained-then-finetuned backbone encoder, providing task-relevant OOD detection by extracting the features from multiple regions of interest anchored to the predicted tumor segmentations. Hence, it scales to images of varying fields-of-view. We compared RF-Deep against existing OOD detection methods using 1,916 CT scans across near-OOD (pulmonary embolism, negative COVID-19) and far-OOD (kidney cancer, healthy pancreas) datasets. RF-Deep achieved AUROC > 93.50 for the challenging near-OOD datasets and near-perfect detection (AUROC > 99.00) for the far-OOD datasets, substantially outperforming logit-based and radiomics approaches. RF-Deep maintained similar performance consistency across networks of different depths and pretraining strategies, demonstrating its effectiveness as a lightweight, architecture-agnostic approach to enhance the reliability of tumor segmentation from CT volumes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Residual-SwinCA-Net: A Channel-Aware Integrated Residual CNN-Swin Transformer for Malignant Lesion Segmentation in BUSI</title>
<link>https://arxiv.org/abs/2512.08243</link>
<guid>https://arxiv.org/abs/2512.08243</guid>
<content:encoded><![CDATA[
arXiv:2512.08243v1 Announce Type: cross 
Abstract: A novel deep hybrid Residual-SwinCA-Net segmentation framework is proposed in the study for addressing such challenges by extracting locally correlated and robust features, incorporating residual CNN modules. Furthermore, for learning global dependencies, Swin Transformer blocks are customized using internal residual pathways, which reinforce gradient stability, refine local patterns, and facilitate global feature fusion. Formerly, for enhancing tissue continuity, ultrasound noise suppressions, and accentuating fine structural transitions Laplacian-of-Gaussian regional operator is applied, and for maintaining the morphological integrity of malignant lesion contours, a boundary-oriented operator has been incorporated. Subsequently, a contraction strategy was applied stage-wise by progressively reducing features-map progressively for capturing scale invariance and enhancing the robustness of structural variability. In addition, each decoder level prior augmentation integrates a new Multi-Scale Channel Attention and Squeezing (MSCAS) module. The MSCAS selectively emphasizes encoder salient maps, retains discriminative global context, and complementary local structures with minimal computational cost while suppressing redundant activations. Finally, the Pixel-Attention module encodes class-relevant spatial cues by adaptively weighing malignant lesion pixels while suppressing background interference. The Residual-SwinCA-Net and existing CNNs/ViTs techniques have been implemented on the publicly available BUSI dataset. The proposed Residual-SwinCA-Net framework outperformed and achieved 99.29% mean accuracy, 98.74% IoU, and 0.9041 Dice for breast lesion segmentation. The proposed Residual-SwinCA-Net framework improves the BUSI lesion diagnostic performance and strengthens timely clinical decision-making.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Splat TeleAssist: A Zero-Shot Pose Estimation Framework for Semantic Teleoperation</title>
<link>https://arxiv.org/abs/2512.08271</link>
<guid>https://arxiv.org/abs/2512.08271</guid>
<content:encoded><![CDATA[
arXiv:2512.08271v1 Announce Type: cross 
Abstract: We introduce Zero-Splat TeleAssist, a zero-shot sensor-fusion pipeline that transforms commodity CCTV streams into a shared, 6-DoF world model for multilateral teleoperation. By integrating vision-language segmentation, monocular depth, weighted-PCA pose extraction, and 3D Gaussian Splatting (3DGS), TeleAssist provides every operator with real-time global positions and orientations of multiple robots without fiducials or depth sensors in an interaction-centric teleoperation setup.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedLAD: A Modular and Adaptive Testbed for Federated Log Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.08277</link>
<guid>https://arxiv.org/abs/2512.08277</guid>
<content:encoded><![CDATA[
arXiv:2512.08277v1 Announce Type: cross 
Abstract: Log-based anomaly detection (LAD) is critical for ensuring the reliability of large-scale distributed systems. However, most existing LAD approaches assume centralized training, which is often impractical due to privacy constraints and the decentralized nature of system logs. While federated learning (FL) offers a promising alternative, there is a lack of dedicated testbeds tailored to the needs of LAD in federated settings. To address this, we present FedLAD, a unified platform for training and evaluating LAD models under FL constraints. FedLAD supports plug-and-play integration of diverse LAD models, benchmark datasets, and aggregation strategies, while offering runtime support for validation logging (self-monitoring), parameter tuning (self-configuration), and adaptive strategy control (self-adaptation). By enabling reproducible and scalable experimentation, FedLAD bridges the gap between FL frameworks and LAD requirements, providing a solid foundation for future research. Project code is publicly available at: https://github.com/AA-cityu/FedLAD.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Multi-Agent Aircraft Landing Time Prediction</title>
<link>https://arxiv.org/abs/2512.08281</link>
<guid>https://arxiv.org/abs/2512.08281</guid>
<content:encoded><![CDATA[
arXiv:2512.08281v1 Announce Type: cross 
Abstract: Accurate and reliable aircraft landing time prediction is essential for effective resource allocation in air traffic management. However, the inherent uncertainty of aircraft trajectories and traffic flows poses significant challenges to both prediction accuracy and trustworthiness. Therefore, prediction models should not only provide point estimates of aircraft landing times but also the uncertainties associated with these predictions. Furthermore, aircraft trajectories are frequently influenced by the presence of nearby aircraft through air traffic control interventions such as radar vectoring. Consequently, landing time prediction models must account for multi-agent interactions in the airspace. In this work, we propose a probabilistic multi-agent aircraft landing time prediction framework that provides the landing times of multiple aircraft as distributions. We evaluate the proposed framework using an air traffic surveillance dataset collected from the terminal airspace of the Incheon International Airport in South Korea. The results demonstrate that the proposed model achieves higher prediction accuracy than the baselines and quantifies the associated uncertainties of its outcomes. In addition, the model uncovered underlying patterns in air traffic control through its attention scores, thereby enhancing explainability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empowering smart app development with SolidGPT: an edge-cloud hybrid AI agent framework</title>
<link>https://arxiv.org/abs/2512.08286</link>
<guid>https://arxiv.org/abs/2512.08286</guid>
<content:encoded><![CDATA[
arXiv:2512.08286v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into mobile and software development workflows faces a persistent tension among three demands: semantic awareness, developer productivity, and data privacy. Traditional cloud-based tools offer strong reasoning but risk data exposure and latency, while on-device solutions lack full-context understanding across codebase and developer tooling. We introduce SolidGPT, an open-source, edge-cloud hybrid developer assistant built on GitHub, designed to enhance code and workspace semantic search. SolidGPT enables developers to: talk to your codebase: interactively query code and project structure, discovering the right methods and modules without manual searching. Automate software project workflows: generate PRDs, task breakdowns, Kanban boards, and even scaffold web app beginnings, with deep integration via VSCode and Notion. Configure private, extensible agents: onboard private code folders (up to approximately 500 files), connect Notion, customize AI agent personas via embedding and in-context training, and deploy via Docker, CLI, or VSCode extension. In practice, SolidGPT empowers developer productivity through: Semantic-rich code navigation: no more hunting through files or wondering where a feature lives. Integrated documentation and task management: seamlessly sync generated PRD content and task boards into developer workflows. Privacy-first design: running locally via Docker or VSCode, with full control over code and data, while optionally reaching out to LLM APIs as needed. By combining interactive code querying, automated project scaffolding, and human-AI collaboration, SolidGPT provides a practical, privacy-respecting edge assistant that accelerates real-world development workflows, ideal for intelligent mobile and software engineering contexts.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magnetic activity of ultracool dwarfs in the LAMOST DR11</title>
<link>https://arxiv.org/abs/2512.08305</link>
<guid>https://arxiv.org/abs/2512.08305</guid>
<content:encoded><![CDATA[
arXiv:2512.08305v1 Announce Type: cross 
Abstract: Ultracool dwarfs consist of lowest-mass stars and brown dwarfs. Their interior is fully convective, different from that of the partly-convective Sun-like stars. Magnetic field generation process beneath the surface of ultracool dwarfs is still poorly understood and controversial. To increase samples of active ultracool dwarfs significantly, we have identified 962 ultracool dwarfs in the latest LAMOST data release, DR11. We also simulate the Chinese Space Station Survey Telescope (CSST) low-resolution slitless spectra by degrading the LAMOST spectra. A semi-supervised machine learning approach with an autoencoder model is built to identify ultracool dwarfs with the simulated CSST spectra, which demonstrates the capability of the CSST all-sky slitless spectroscopic survey on the detection of ultracool dwarfs. Magnetic activity of the ultracool dwarfs is investigated by using the H$\alpha$ line emission as a proxy. The rotational periods of 82 ultracool dwarfs are derived based on the Kepler/K2 light curves. We also derive the activity-rotation relation of the ultracool dwarfs, which is saturated around a Rossby number of 0.12.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Terrain Diffusion: A Diffusion-Based Successor to Perlin Noise in Infinite, Real-Time Terrain Generation</title>
<link>https://arxiv.org/abs/2512.08309</link>
<guid>https://arxiv.org/abs/2512.08309</guid>
<content:encoded><![CDATA[
arXiv:2512.08309v1 Announce Type: cross 
Abstract: For decades, procedural worlds have been built on procedural noise functions such as Perlin noise, which are fast and infinite, yet fundamentally limited in realism and large-scale coherence. We introduce Terrain Diffusion, an AI-era successor to Perlin noise that bridges the fidelity of diffusion models with the properties that made procedural noise indispensable: seamless infinite extent, seed-consistency, and constant-time random access. At its core is InfiniteDiffusion, a novel algorithm for infinite generation, enabling seamless, real-time synthesis of boundless landscapes. A hierarchical stack of diffusion models couples planetary context with local detail, while a compact Laplacian encoding stabilizes outputs across Earth-scale dynamic ranges. An open-source infinite-tensor framework supports constant-memory manipulation of unbounded tensors, and few-step consistency distillation enables efficient generation. Together, these components establish diffusion models as a practical foundation for procedural world generation, capable of synthesizing entire planets coherently, controllably, and without limits.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Low Rank Support Quaternion Matrix Machine</title>
<link>https://arxiv.org/abs/2512.08327</link>
<guid>https://arxiv.org/abs/2512.08327</guid>
<content:encoded><![CDATA[
arXiv:2512.08327v1 Announce Type: cross 
Abstract: Input features are conventionally represented as vectors, matrices, or third order tensors in the real field, for color image classification. Inspired by the success of quaternion data modeling for color images in image recovery and denoising tasks, we propose a novel classification method for color image classification, named as the Low-rank Support Quaternion Matrix Machine (LSQMM), in which the RGB channels are treated as pure quaternions to effectively preserve the intrinsic coupling relationships among channels via the quaternion algebra. For the purpose of promoting low-rank structures resulting from strongly correlated color channels, a quaternion nuclear norm regularization term, serving as a natural extension of the conventional matrix nuclear norm to the quaternion domain, is added to the hinge loss in our LSQMM model. An Alternating Direction Method of Multipliers (ADMM)-based iterative algorithm is designed to effectively resolve the proposed quaternion optimization model. Experimental results on multiple color image classification datasets demonstrate that our proposed classification approach exhibits advantages in classification accuracy, robustness and computational efficiency, compared to several state-of-the-art methods using support vector machines, support matrix machines, and support tensor machines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpreting Structured Perturbations in Image Protection Methods for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.08329</link>
<guid>https://arxiv.org/abs/2512.08329</guid>
<content:encoded><![CDATA[
arXiv:2512.08329v1 Announce Type: cross 
Abstract: Recent image protection mechanisms such as Glaze and Nightshade introduce imperceptible, adversarially designed perturbations intended to disrupt downstream text-to-image generative models. While their empirical effectiveness is known, the internal structure, detectability, and representational behavior of these perturbations remain poorly understood. This study provides a systematic, explainable AI analysis using a unified framework that integrates white-box feature-space inspection and black-box signal-level probing. Through latent-space clustering, feature-channel activation analysis, occlusion-based spatial sensitivity mapping, and frequency-domain characterization, we show that protection mechanisms operate as structured, low-entropy perturbations tightly coupled to underlying image content across representational, spatial, and spectral domains. Protected images preserve content-driven feature organization with protection-specific substructure rather than inducing global representational drift. Detectability is governed by interacting effects of perturbation entropy, spatial deployment, and frequency alignment, with sequential protection amplifying detectable structure rather than suppressing it. Frequency-domain analysis shows that Glaze and Nightshade redistribute energy along dominant image-aligned frequency axes rather than introducing diffuse noise. These findings indicate that contemporary image protection operates through structured feature-level deformation rather than semantic dislocation, explaining why protection signals remain visually subtle yet consistently detectable. This work advances the interpretability of adversarial image protection and informs the design of future defenses and detection strategies for generative AI systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from T\"urkiye</title>
<link>https://arxiv.org/abs/2512.08340</link>
<guid>https://arxiv.org/abs/2512.08340</guid>
<content:encoded><![CDATA[
arXiv:2512.08340v1 Announce Type: cross 
Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in T\"urkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Deep Reinforcement Learning for Collaborative UAV Relay Networks under Jamming Atatcks</title>
<link>https://arxiv.org/abs/2512.08341</link>
<guid>https://arxiv.org/abs/2512.08341</guid>
<content:encoded><![CDATA[
arXiv:2512.08341v1 Announce Type: cross 
Abstract: The deployment of Unmanned Aerial Vehicle (UAV) swarms as dynamic communication relays is critical for next-generation tactical networks. However, operating in contested environments requires solving a complex trade-off, including maximizing system throughput while ensuring collision avoidance and resilience against adversarial jamming. Existing heuristic-based approaches often struggle to find effective solutions due to the dynamic and multi-objective nature of this problem. This paper formulates this challenge as a cooperative Multi-Agent Reinforcement Learning (MARL) problem, solved using the Centralized Training with Decentralized Execution (CTDE) framework. Our approach employs a centralized critic that uses global state information to guide decentralized actors which operate using only local observations. Simulation results show that our proposed framework significantly outperforms heuristic baselines, increasing the total system throughput by approximately 50% while simultaneously achieving a near-zero collision rate. A key finding is that the agents develop an emergent anti-jamming strategy without explicit programming. They learn to intelligently position themselves to balance the trade-off between mitigating interference from jammers and maintaining effective communication links with ground users.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Explainability of Graph Neural Networks Through Conceptual and Structural Analyses and Their Extensions</title>
<link>https://arxiv.org/abs/2512.08344</link>
<guid>https://arxiv.org/abs/2512.08344</guid>
<content:encoded><![CDATA[
arXiv:2512.08344v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become a powerful tool for modeling and analyzing data with graph structures. The wide adoption in numerous applications underscores the value of these models. However, the complexity of these methods often impedes understanding their decision-making processes. Current Explainable AI (XAI) methods struggle to untangle the intricate relationships and interactions within graphs. Several methods have tried to bridge this gap via a post-hoc approach or self-interpretable design. Most of them focus on graph structure analysis to determine essential patterns that correlate with prediction outcomes. While post-hoc explanation methods are adaptable, they require extra computational resources and may be less reliable due to limited access to the model's internal workings. Conversely, Interpretable models can provide immediate explanations, but their generalizability to different scenarios remains a major concern. To address these shortcomings, this thesis seeks to develop a novel XAI framework tailored for graph-based machine learning. The proposed framework aims to offer adaptable, computationally efficient explanations for GNNs, moving beyond individual feature analysis to capture how graph structure influences predictions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Morphogenesis: Emergent Generation of Structural Digits via Neural Cellular Automata</title>
<link>https://arxiv.org/abs/2512.08360</link>
<guid>https://arxiv.org/abs/2512.08360</guid>
<content:encoded><![CDATA[
arXiv:2512.08360v1 Announce Type: cross 
Abstract: Biological systems exhibit remarkable morphogenetic plasticity, where a single genome can encode various specialized cellular structures triggered by local chemical signals. In the domain of Deep Learning, Differentiable Neural Cellular Automata (NCA) have emerged as a paradigm to mimic this self-organization. However, existing NCA research has predominantly focused on continuous texture synthesis or single-target object recovery, leaving the challenge of class-conditional structural generation largely unexplored. In this work, we propose a novel Conditional Neural Cellular Automata (c-NCA) architecture capable of growing distinct topological structures - specifically MNIST digits - from a single generic seed, guided solely by a spatially broadcasted class vector. Unlike traditional generative models (e.g., GANs, VAEs) that rely on global reception fields, our model enforces strict locality and translation equivariance. We demonstrate that by injecting a one-hot condition into the cellular perception field, a single set of local rules can learn to break symmetry and self-assemble into ten distinct geometric attractors. Experimental results show that our c-NCA achieves stable convergence, correctly forming digit topologies from a single pixel, and exhibits robustness characteristic of biological systems. This work bridges the gap between texture-based NCAs and structural pattern formation, offering a lightweight, biologically plausible alternative for conditional generation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Magneton: Optimizing Energy Efficiency of ML Systems via Differential Energy Debugging</title>
<link>https://arxiv.org/abs/2512.08365</link>
<guid>https://arxiv.org/abs/2512.08365</guid>
<content:encoded><![CDATA[
arXiv:2512.08365v1 Announce Type: cross 
Abstract: The training and deployment of machine learning (ML) models have become extremely energy-intensive. While existing optimization efforts focus primarily on hardware energy efficiency, a significant but overlooked source of inefficiency is software energy waste caused by poor software design. This often includes redundant or poorly designed operations that consume more energy without improving performance. These inefficiencies arise in widely used ML frameworks and applications, yet developers often lack the visibility and tools to detect and diagnose them.
  We propose differential energy debugging, a novel approach that leverages the observation that competing ML systems often implement similar functionality with vastly different energy consumption. Building on this insight, we design and implement Magneton, an energy profiler that compares energy consumption between similar ML systems at the operator level and automatically pinpoints code regions and configuration choices responsible for excessive energy use. Applied to 9 popular ML systems spanning LLM inference, general ML frameworks, and image generation, Magneton detects and diagnoses 16 known cases of software energy inefficiency and further discovers 8 previously unknown cases, 7 of which have been confirmed by developers.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Wave Variables: A Data-Driven Ensemble Approach for Enhanced Teleoperation Transparency and Stability</title>
<link>https://arxiv.org/abs/2512.08436</link>
<guid>https://arxiv.org/abs/2512.08436</guid>
<content:encoded><![CDATA[
arXiv:2512.08436v1 Announce Type: cross 
Abstract: Time delays in communication channels present significant challenges for bilateral teleoperation systems, affecting both transparency and stability. Although traditional wave variable-based methods for a four-channel architecture ensure stability via passivity, they remain vulnerable to wave reflections and disturbances like variable delays and environmental noise. This article presents a data-driven hybrid framework that replaces the conventional wave-variable transform with an ensemble of three advanced sequence models, each optimized separately via the state-of-the-art Optuna optimizer, and combined through a stacking meta-learner. The base predictors include an LSTM augmented with Prophet for trend correction, an LSTM-based feature extractor paired with clustering and a random forest for improved regression, and a CNN-LSTM model for localized and long-term dynamics. Experimental validation was performed in Python using data generated from the baseline system implemented in MATLAB/Simulink. The results show that our optimized ensemble achieves a transparency comparable to the baseline wave-variable system under varying delays and noise, while ensuring stability through passivity constraints.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learned iterative networks: An operator learning perspective</title>
<link>https://arxiv.org/abs/2512.08444</link>
<guid>https://arxiv.org/abs/2512.08444</guid>
<content:encoded><![CDATA[
arXiv:2512.08444v1 Announce Type: cross 
Abstract: Learned image reconstruction has become a pillar in computational imaging and inverse problems. Among the most successful approaches are learned iterative networks, which are formulated by unrolling classical iterative optimisation algorithms for solving variational problems. While the underlying algorithm is usually formulated in the functional analytic setting, learned approaches are often viewed as purely discrete. In this chapter we present a unified operator view for learned iterative networks. Specifically, we formulate a learned reconstruction operator, defining how to compute, and separately the learning problem, which defines what to compute. In this setting we present common approaches and show that many approaches are closely related in their core. We review linear as well as nonlinear inverse problems in this framework and present a short numerical study to conclude.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Subset Selection for Robust Visual Explainability under Distribution Shifts</title>
<link>https://arxiv.org/abs/2512.08445</link>
<guid>https://arxiv.org/abs/2512.08445</guid>
<content:encoded><![CDATA[
arXiv:2512.08445v1 Announce Type: cross 
Abstract: Subset selection-based methods are widely used to explain deep vision models: they attribute predictions by highlighting the most influential image regions and support object-level explanations. While these methods perform well in in-distribution (ID) settings, their behavior under out-of-distribution (OOD) conditions remains poorly understood. Through extensive experiments across multiple ID-OOD sets, we find that reliability of the existing subset based methods degrades markedly, yielding redundant, unstable, and uncertainty-sensitive explanations. To address these shortcomings, we introduce a framework that combines submodular subset selection with layer-wise, gradient-based uncertainty estimation to improve robustness and fidelity without requiring additional training or auxiliary models. Our approach estimates uncertainty via adaptive weight perturbations and uses these estimates to guide submodular optimization, ensuring diverse and informative subset selection. Empirical evaluations show that, beyond mitigating the weaknesses of existing methods under OOD scenarios, our framework also yields improvements in ID settings. These findings highlight limitations of current subset-based approaches and demonstrate how uncertainty-driven optimization can enhance attribution and object-level interpretability, paving the way for more transparent and trustworthy AI in real-world vision applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using reinforcement learning to probe the role of feedback in skill acquisition</title>
<link>https://arxiv.org/abs/2512.08463</link>
<guid>https://arxiv.org/abs/2512.08463</guid>
<content:encoded><![CDATA[
arXiv:2512.08463v1 Announce Type: cross 
Abstract: Many high-performance human activities are executed with little or no external feedback: think of a figure skater landing a triple jump, a pitcher throwing a curveball for a strike, or a barista pouring latte art. To study the process of skill acquisition under fully controlled conditions, we bypass human subjects. Instead, we directly interface a generalist reinforcement learning agent with a spinning cylinder in a tabletop circulating water channel to maximize or minimize drag. This setup has several desirable properties. First, it is a physical system, with the rich interactions and complex dynamics that only the physical world has: the flow is highly chaotic and extremely difficult, if not impossible, to model or simulate accurately. Second, the objective -- drag minimization or maximization -- is easy to state and can be captured directly in the reward, yet good strategies are not obvious beforehand. Third, decades-old experimental studies provide recipes for simple, high-performance open-loop policies. Finally, the setup is inexpensive and far easier to reproduce than human studies. In our experiments we find that high-dimensional flow feedback lets the agent discover high-performance drag-control strategies with only minutes of real-world interaction. When we later replay the same action sequences without any feedback, we obtain almost identical performance. This shows that feedback, and in particular flow feedback, is not needed to execute the learned policy. Surprisingly, without flow feedback during training the agent fails to discover any well-performing policy in drag maximization, but still succeeds in drag minimization, albeit more slowly and less reliably. Our studies show that learning a high-performance skill can require richer information than executing it, and learning conditions can be kind or wicked depending solely on the goal, not on dynamics or policy complexity.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fused Gromov-Wasserstein Contrastive Learning for Effective Enzyme-Reaction Screening</title>
<link>https://arxiv.org/abs/2512.08508</link>
<guid>https://arxiv.org/abs/2512.08508</guid>
<content:encoded><![CDATA[
arXiv:2512.08508v1 Announce Type: cross 
Abstract: Enzymes are crucial catalysts that enable a wide range of biochemical reactions. Efficiently identifying specific enzymes from vast protein libraries is essential for advancing biocatalysis. Traditional computational methods for enzyme screening and retrieval are time-consuming and resource-intensive. Recently, deep learning approaches have shown promise. However, these methods focus solely on the interaction between enzymes and reactions, overlooking the inherent hierarchical relationships within each domain. To address these limitations, we introduce FGW-CLIP, a novel contrastive learning framework based on optimizing the fused Gromov-Wasserstein distance. FGW-CLIP incorporates multiple alignments, including inter-domain alignment between reactions and enzymes and intra-domain alignment within enzymes and reactions. By introducing a tailored regularization term, our method minimizes the Gromov-Wasserstein distance between enzyme and reaction spaces, which enhances information integration across these domains. Extensive evaluations demonstrate the superiority of FGW-CLIP in challenging enzyme-reaction tasks. On the widely-used EnzymeMap benchmark, FGW-CLIP achieves state-of-the-art performance in enzyme virtual screening, as measured by BEDROC and EF metrics. Moreover, FGW-CLIP consistently outperforms across all three splits of ReactZyme, the largest enzyme-reaction benchmark, demonstrating robust generalization to novel enzymes and reactions. These results position FGW-CLIP as a promising framework for enzyme discovery in complex biochemical settings, with strong adaptability across diverse screening scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Efficient Learning of Anomalous Diffusion with Wavelet Representations: Enabling Direct Learning from Experimental Trajectories</title>
<link>https://arxiv.org/abs/2512.08510</link>
<guid>https://arxiv.org/abs/2512.08510</guid>
<content:encoded><![CDATA[
arXiv:2512.08510v1 Announce Type: cross 
Abstract: Machine learning (ML) has become a versatile tool for analyzing anomalous diffusion trajectories, yet most existing pipelines are trained on large collections of simulated data. In contrast, experimental trajectories, such as those from single-particle tracking (SPT), are typically scarce and may differ substantially from the idealized models used for simulation, leading to degradation or even breakdown of performance when ML methods are applied to real data. To address this mismatch, we introduce a wavelet-based representation of anomalous diffusion that enables data-efficient learning directly from experimental recordings. This representation is constructed by applying six complementary wavelet families to each trajectory and combining the resulting wavelet modulus scalograms. We first evaluate the wavelet representation on simulated trajectories from the andi-datasets benchmark, where it clearly outperforms both feature-based and trajectory-based methods with as few as 1000 training trajectories and still retains an advantage on large training sets. We then use this representation to learn directly from experimental SPT trajectories of fluorescent beads diffusing in F-actin networks, where the wavelet representation remains superior to existing alternatives for both diffusion-exponent regression and mesh-size classification. In particular, when predicting the diffusion exponents of experimental trajectories, a model trained on 1200 experimental tracks using the wavelet representation achieves significantly lower errors than state-of-the-art deep learning models trained purely on $10^6$ simulated trajectories. We associate this data efficiency with the emergence of distinct scale fingerprints disentangling underlying diffusion mechanisms in the wavelet spectra.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice</title>
<link>https://arxiv.org/abs/2512.08513</link>
<guid>https://arxiv.org/abs/2512.08513</guid>
<content:encoded><![CDATA[
arXiv:2512.08513v1 Announce Type: cross 
Abstract: We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Disturbance-Free Surgical Video Generation from Multi-Camera Shadowless Lamps for Open Surgery</title>
<link>https://arxiv.org/abs/2512.08577</link>
<guid>https://arxiv.org/abs/2512.08577</guid>
<content:encoded><![CDATA[
arXiv:2512.08577v1 Announce Type: cross 
Abstract: Video recordings of open surgeries are greatly required for education and research purposes. However, capturing unobstructed videos is challenging since surgeons frequently block the camera field of view. To avoid occlusion, the positions and angles of the camera must be frequently adjusted, which is highly labor-intensive. Prior work has addressed this issue by installing multiple cameras on a shadowless lamp and arranging them to fully surround the surgical area. This setup increases the chances of some cameras capturing an unobstructed view. However, manual image alignment is needed in post-processing since camera configurations change every time surgeons move the lamp for optimal lighting. This paper aims to fully automate this alignment task. The proposed method identifies frames in which the lighting system moves, realigns them, and selects the camera with the least occlusion to generate a video that consistently presents the surgical field from a fixed perspective. A user study involving surgeons demonstrated that videos generated by our method were superior to those produced by conventional methods in terms of the ease of confirming the surgical area and the comfort during video viewing. Additionally, our approach showed improvements in video quality over existing techniques. Furthermore, we implemented several synthesis options for the proposed view-synthesis method and conducted a user study to assess surgeons' preferences for each option.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Heuristics for Combinatorial Optimization via Value-based Reinforcement Learning: A Unified Framework and Analysis</title>
<link>https://arxiv.org/abs/2512.08601</link>
<guid>https://arxiv.org/abs/2512.08601</guid>
<content:encoded><![CDATA[
arXiv:2512.08601v1 Announce Type: cross 
Abstract: Since the 1990s, considerable empirical work has been carried out to train statistical models, such as neural networks (NNs), as learned heuristics for combinatorial optimization (CO) problems. When successful, such an approach eliminates the need for experts to design heuristics per problem type. Due to their structure, many hard CO problems are amenable to treatment through reinforcement learning (RL). Indeed, we find a wealth of literature training NNs using value-based, policy gradient, or actor-critic approaches, with promising results, both in terms of empirical optimality gaps and inference runtimes. Nevertheless, there has been a paucity of theoretical work undergirding the use of RL for CO problems. To this end, we introduce a unified framework to model CO problems through Markov decision processes (MDPs) and solve them using RL techniques. We provide easy-to-test assumptions under which CO problems can be formulated as equivalent undiscounted MDPs that provide optimal solutions to the original CO problems. Moreover, we establish conditions under which value-based RL techniques converge to approximate solutions of the CO problem with a guarantee on the associated optimality gap. Our convergence analysis provides: (1) a sufficient rate of increase in batch size and projected gradient descent steps at each RL iteration; (2) the resulting optimality gap in terms of problem parameters and targeted RL accuracy; and (3) the importance of a choice of state-space embedding. Together, our analysis illuminates the success (and limitations) of the celebrated deep Q-learning algorithm in this problem context.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reusability in MLOps: Leveraging Ports and Adapters to Build a Microservices Architecture for the Maritime Domain</title>
<link>https://arxiv.org/abs/2512.08657</link>
<guid>https://arxiv.org/abs/2512.08657</guid>
<content:encoded><![CDATA[
arXiv:2512.08657v1 Announce Type: cross 
Abstract: ML-Enabled Systems (MLES) are inherently complex since they require multiple components to achieve their business goal. This experience report showcases the software architecture reusability techniques applied while building Ocean Guard, an MLES for anomaly detection in the maritime domain. In particular, it highlights the challenges and lessons learned to reuse the Ports and Adapters pattern to support building multiple microservices from a single codebase. This experience report hopes to inspire software engineers, machine learning engineers, and data scientists to apply the Hexagonal Architecture pattern to build their MLES.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Agentic AI System for Multi-Framework Communication Coding</title>
<link>https://arxiv.org/abs/2512.08659</link>
<guid>https://arxiv.org/abs/2512.08659</guid>
<content:encoded><![CDATA[
arXiv:2512.08659v1 Announce Type: cross 
Abstract: Clinical communication is central to patient outcomes, yet large-scale human annotation of patient-provider conversation remains labor-intensive, inconsistent, and difficult to scale. Existing approaches based on large language models typically rely on single-task models that lack adaptability, interpretability, and reliability, especially when applied across various communication frameworks and clinical domains. In this study, we developed a Multi-framework Structured Agentic AI system for Clinical Communication (MOSAIC), built on a LangGraph-based architecture that orchestrates four core agents, including a Plan Agent for codebook selection and workflow planning, an Update Agent for maintaining up-to-date retrieval databases, a set of Annotation Agents that applies codebook-guided retrieval-augmented generation (RAG) with dynamic few-shot prompting, and a Verification Agent that provides consistency checks and feedback. To evaluate performance, we compared MOSAIC outputs against gold-standard annotations created by trained human coders. We developed and evaluated MOSAIC using 26 gold standard annotated transcripts for training and 50 transcripts for testing, spanning rheumatology and OB/GYN domains. On the test set, MOSAIC achieved an overall F1 score of 0.928. Performance was highest in the Rheumatology subset (F1 = 0.962) and strongest for Patient Behavior (e.g., patients asking questions, expressing preferences, or showing assertiveness). Ablations revealed that MOSAIC outperforms baseline benchmarking.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Direct transfer of optimized controllers to similar systems using dimensionless MPC</title>
<link>https://arxiv.org/abs/2512.08667</link>
<guid>https://arxiv.org/abs/2512.08667</guid>
<content:encoded><![CDATA[
arXiv:2512.08667v1 Announce Type: cross 
Abstract: Scaled model experiments are commonly used in various engineering fields to reduce experimentation costs and overcome constraints associated with full-scale systems. The relevance of such experiments relies on dimensional analysis and the principle of dynamic similarity. However, transferring controllers to full-scale systems often requires additional tuning. In this paper, we propose a method to enable a direct controller transfer using dimensionless model predictive control, tuned automatically for closed-loop performance. With this reformulation, the closed-loop behavior of an optimized controller transfers directly to a new, dynamically similar system. Additionally, the dimensionless formulation allows for the use of data from systems of different scales during parameter optimization. We demonstrate the method on a cartpole swing-up and a car racing problem, applying either reinforcement learning or Bayesian optimization for tuning the controller parameters. Software used to obtain the results in this paper is publicly available at https://github.com/josipkh/dimensionless-mpcrl.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient-Informed Monte Carlo Fine-Tuning of Diffusion Models for Low-Thrust Trajectory Design</title>
<link>https://arxiv.org/abs/2512.08705</link>
<guid>https://arxiv.org/abs/2512.08705</guid>
<content:encoded><![CDATA[
arXiv:2512.08705v1 Announce Type: cross 
Abstract: Preliminary mission design of low-thrust spacecraft trajectories in the Circular Restricted Three-Body Problem is a global search characterized by a complex objective landscape and numerous local minima. Formulating the problem as sampling from an unnormalized distribution supported on neighborhoods of locally optimal solutions, provides the opportunity to deploy Markov chain Monte Carlo methods and generative machine learning. In this work, we extend our previous self-supervised diffusion model fine-tuning framework to employ gradient-informed Markov chain Monte Carlo. We compare two algorithms - the Metropolis-Adjusted Langevin Algorithm and Hamiltonian Monte Carlo - both initialized from a distribution learned by a diffusion model. Derivatives of an objective function that balances fuel consumption, time of flight and constraint violations are computed analytically using state transition matrices. We show that incorporating the gradient drift term accelerates mixing and improves convergence of the Markov chain for a multi-revolution transfer in the Saturn-Titan system. Among the evaluated methods, MALA provides the best trade-off between performance and computational cost. Starting from samples generated by a baseline diffusion model trained on a related transfer, MALA explicitly targets Pareto-optimal solutions. Compared to a random walk Metropolis algorithm, it increases the feasibility rate from 17.34% to 63.01% and produces a denser, more diverse coverage of the Pareto front. By fine-tuning a diffusion model on the generated samples and associated reward values with reward-weighted likelihood maximization, we learn the global solution structure of the problem and eliminate the need for a tedious separate data generation phase.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-domain performance analysis with scores tailored to user preferences</title>
<link>https://arxiv.org/abs/2512.08715</link>
<guid>https://arxiv.org/abs/2512.08715</guid>
<content:encoded><![CDATA[
arXiv:2512.08715v1 Announce Type: cross 
Abstract: The performance of algorithms, methods, and models tends to depend heavily on the distribution of cases on which they are applied, this distribution being specific to the applicative domain. After performing an evaluation in several domains, it is highly informative to compute a (weighted) mean performance and, as shown in this paper, to scrutinize what happens during this averaging. To achieve this goal, we adopt a probabilistic framework and consider a performance as a probability measure (e.g., a normalized confusion matrix for a classification task). It appears that the corresponding weighted mean is known to be the summarization, and that only some remarkable scores assign to the summarized performance a value equal to a weighted arithmetic mean of the values assigned to the domain-specific performances. These scores include the family of ranking scores, a continuum parameterized by user preferences, and that the weights to consider in the arithmetic mean depend on the user preferences. Based on this, we rigorously define four domains, named easiest, most difficult, preponderant, and bottleneck domains, as functions of user preferences. After establishing the theory in a general setting, regardless of the task, we develop new visual tools for two-class classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Individual Skin Tone Bias in Skin Lesion Classification through Distribution-Aware Reweighting</title>
<link>https://arxiv.org/abs/2512.08733</link>
<guid>https://arxiv.org/abs/2512.08733</guid>
<content:encoded><![CDATA[
arXiv:2512.08733v1 Announce Type: cross 
Abstract: Skin color has historically been a focal point of discrimination, yet fairness research in machine learning for medical imaging often relies on coarse subgroup categories, overlooking individual-level variations. Such group-based approaches risk obscuring biases faced by outliers within subgroups. This study introduces a distribution-based framework for evaluating and mitigating individual fairness in skin lesion classification. We treat skin tone as a continuous attribute rather than a categorical label, and employ kernel density estimation (KDE) to model its distribution. We further compare twelve statistical distance metrics to quantify disparities between skin tone distributions and propose a distance-based reweighting (DRW) loss function to correct underrepresentation in minority tones. Experiments across CNN and Transformer models demonstrate: (i) the limitations of categorical reweighting in capturing individual-level disparities, and (ii) the superior performance of distribution-based reweighting, particularly with Fidelity Similarity (FS), Wasserstein Distance (WD), Hellinger Metric (HM), and Harmonic Mean Similarity (HS). These findings establish a robust methodology for advancing fairness at individual level in dermatological AI systems, and highlight broader implications for sensitive continuous attributes in medical image analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivTune: Efficient and Privacy-Preserving Fine-Tuning of Large Language Models via Device-Cloud Collaboration</title>
<link>https://arxiv.org/abs/2512.08809</link>
<guid>https://arxiv.org/abs/2512.08809</guid>
<content:encoded><![CDATA[
arXiv:2512.08809v1 Announce Type: cross 
Abstract: With the rise of large language models, service providers offer language models as a service, enabling users to fine-tune customized models via uploaded private datasets. However, this raises concerns about sensitive data leakage. Prior methods, relying on differential privacy within device-cloud collaboration frameworks, struggle to balance privacy and utility, exposing users to inference attacks or degrading fine-tuning performance. To address this, we propose PrivTune, an efficient and privacy-preserving fine-tuning framework via Split Learning (SL). The key idea of PrivTune is to inject crafted noise into token representations from the SL bottom model, making each token resemble the $n$-hop indirect neighbors. PrivTune formulates this as an optimization problem to compute the optimal noise vector, aligning with defense-utility goals. On this basis, it then adjusts the parameters (i.e., mean) of the $d_\chi$-Privacy noise distribution to align with the optimization direction and scales the noise according to token importance to minimize distortion. Experiments on five datasets (covering both classification and generation tasks) against three embedding inversion and three attribute inference attacks show that, using RoBERTa on the Stanford Sentiment Treebank dataset, PrivTune reduces the attack success rate to 10% with only a 3.33% drop in utility performance, outperforming state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multicalibration for LLM-based Code Generation</title>
<link>https://arxiv.org/abs/2512.08810</link>
<guid>https://arxiv.org/abs/2512.08810</guid>
<content:encoded><![CDATA[
arXiv:2512.08810v1 Announce Type: cross 
Abstract: As AI-based code generation becomes widespread, researchers are investigating the calibration of code LLMs - ensuring their confidence scores faithfully represent the true likelihood of code correctness. To do so, we investigate multicalibration, which can capture additional factors about a coding problem, such as complexity, code length, or programming language used. We study four multicalibration approaches on three function synthesis benchmarks, using latest-generation code LLMs (Qwen3 Coder, GPT-OSS, DeepSeek-R1-Distill). Our results demonstrate that multicalibration can yield distinct improvements over both uncalibrated token likelihoods (+1.03 in skill score) and baseline calibrations (+0.37 in skill score). We study the influence of the aforementioned factors in ablations, and make our dataset (consisting of code generations, likelihoods, and correctness labels) available for future research on code LLM calibration.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Depth-Grown Models Overcome the Curse of Depth? An In-Depth Analysis</title>
<link>https://arxiv.org/abs/2512.08819</link>
<guid>https://arxiv.org/abs/2512.08819</guid>
<content:encoded><![CDATA[
arXiv:2512.08819v1 Announce Type: cross 
Abstract: Gradually growing the depth of Transformers during training can not only reduce training cost but also lead to improved reasoning performance, as shown by MIDAS (Saunshi et al., 2024). Thus far, however, a mechanistic understanding of these gains has been missing. In this work, we establish a connection to recent work showing that layers in the second half of non-grown, pre-layernorm Transformers contribute much less to the final output distribution than those in the first half - also known as the Curse of Depth (Sun et al., 2025, Csord\'as et al., 2025). Using depth-wise analyses, we demonstrate that growth via gradual middle stacking yields more effective utilization of model depth, alters the residual stream structure, and facilitates the formation of permutable computational blocks. In addition, we propose a lightweight modification of MIDAS that yields further improvements in downstream reasoning benchmarks. Overall, this work highlights how the gradual growth of model depth can lead to the formation of distinct computational circuits and overcome the limited depth utilization seen in standard non-grown models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generation is Required for Data-Efficient Perception</title>
<link>https://arxiv.org/abs/2512.08854</link>
<guid>https://arxiv.org/abs/2512.08854</guid>
<content:encoded><![CDATA[
arXiv:2512.08854v1 Announce Type: cross 
Abstract: It has been hypothesized that human-level visual perception requires a generative approach in which internal representations result from inverting a decoder. Yet today's most successful vision models are non-generative, relying on an encoder that maps images to representations without decoder inversion. This raises the question of whether generation is, in fact, necessary for machines to achieve human-level visual perception. To address this, we study whether generative and non-generative methods can achieve compositional generalization, a hallmark of human perception. Under a compositional data generating process, we formalize the inductive biases required to guarantee compositional generalization in decoder-based (generative) and encoder-based (non-generative) methods. We then show theoretically that enforcing these inductive biases on encoders is generally infeasible using regularization or architectural constraints. In contrast, for generative methods, the inductive biases can be enforced straightforwardly, thereby enabling compositional generalization by constraining a decoder and inverting it. We highlight how this inversion can be performed efficiently, either online through gradient-based search or offline through generative replay. We examine the empirical implications of our theory by training a range of generative and non-generative methods on photorealistic image datasets. We find that, without the necessary inductive biases, non-generative methods often fail to generalize compositionally and require large-scale pretraining or added supervision to improve generalization. By comparison, generative methods yield significant improvements in compositional generalization, without requiring additional data, by leveraging suitable inductive biases on a decoder along with search and replay.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Secure and Privacy-Preserving Federated Learning for Next-Generation Underground Mine Safety</title>
<link>https://arxiv.org/abs/2512.08862</link>
<guid>https://arxiv.org/abs/2512.08862</guid>
<content:encoded><![CDATA[
arXiv:2512.08862v1 Announce Type: cross 
Abstract: Underground mining operations depend on sensor networks to monitor critical parameters such as temperature, gas concentration, and miner movement, enabling timely hazard detection and safety decisions. However, transmitting raw sensor data to a centralized server for machine learning (ML) model training raises serious privacy and security concerns. Federated Learning (FL) offers a promising alternative by enabling decentralized model training without exposing sensitive local data. Yet, applying FL in underground mining presents unique challenges: (i) Adversaries may eavesdrop on shared model updates to launch model inversion or membership inference attacks, compromising data privacy and operational safety; (ii) Non-IID data distributions across mines and sensor noise can hinder model convergence. To address these issues, we propose FedMining--a privacy-preserving FL framework tailored for underground mining. FedMining introduces two core innovations: (1) a Decentralized Functional Encryption (DFE) scheme that keeps local models encrypted, thwarting unauthorized access and inference attacks; and (2) a balancing aggregation mechanism to mitigate data heterogeneity and enhance convergence. Evaluations on real-world mining datasets demonstrate FedMining's ability to safeguard privacy while maintaining high model accuracy and achieving rapid convergence with reduced communication and computation overhead. These advantages make FedMining both secure and practical for real-time underground safety monitoring.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Trust for Space AI: Blockchain-Based Federated Learning Across Multi-Vendor LEO Satellite Networks</title>
<link>https://arxiv.org/abs/2512.08882</link>
<guid>https://arxiv.org/abs/2512.08882</guid>
<content:encoded><![CDATA[
arXiv:2512.08882v1 Announce Type: cross 
Abstract: The rise of space AI is reshaping government and industry through applications such as disaster detection, border surveillance, and climate monitoring, powered by massive data from commercial and governmental low Earth orbit (LEO) satellites. Federated satellite learning (FSL) enables joint model training without sharing raw data, but suffers from slow convergence due to intermittent connectivity and introduces critical trust challenges--where biased or falsified updates can arise across satellite constellations, including those injected through cyberattacks on inter-satellite or satellite-ground communication links. We propose OrbitChain, a blockchain-backed framework that empowers trustworthy multi-vendor collaboration in LEO networks. OrbitChain (i) offloads consensus to high-altitude platforms (HAPs) with greater computational capacity, (ii) ensures transparent, auditable provenance of model updates from different orbits owned by different vendors, and (iii) prevents manipulated or incomplete contributions from affecting global FSL model aggregation. Extensive simulations show that OrbitChain reduces computational and communication overhead while improving privacy, security, and global model accuracy. Its permissioned proof-of-authority ledger finalizes over 1000 blocks with sub-second latency (0.16,s, 0.26,s, 0.35,s for 1-of-5, 3-of-5, and 5-of-5 quorums). Moreover, OrbitChain reduces convergence time by up to 30 hours on real satellite datasets compared to single-vendor, demonstrating its effectiveness for real-time, multi-vendor learning. Our code is available at https://github.com/wsu-cyber-security-lab-ai/OrbitChain.git
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OSMO: Open-Source Tactile Glove for Human-to-Robot Skill Transfer</title>
<link>https://arxiv.org/abs/2512.08920</link>
<guid>https://arxiv.org/abs/2512.08920</guid>
<content:encoded><![CDATA[
arXiv:2512.08920v1 Announce Type: cross 
Abstract: Human video demonstrations provide abundant training data for learning robot policies, but video alone cannot capture the rich contact signals critical for mastering manipulation. We introduce OSMO, an open-source wearable tactile glove designed for human-to-robot skill transfer. The glove features 12 three-axis tactile sensors across the fingertips and palm and is designed to be compatible with state-of-the-art hand-tracking methods for in-the-wild data collection. We demonstrate that a robot policy trained exclusively on human demonstrations collected with OSMO, without any real robot data, is capable of executing a challenging contact-rich manipulation task. By equipping both the human and the robot with the same glove, OSMO minimizes the visual and tactile embodiment gap, enabling the transfer of continuous shear and normal force feedback while avoiding the need for image inpainting or other vision-based force inference. On a real-world wiping task requiring sustained contact pressure, our tactile-aware policy achieves a 72% success rate, outperforming vision-only baselines by eliminating contact-related failure modes. We release complete hardware designs, firmware, and assembly instructions to support community adoption.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: General Interactive World Model with Autoregressive Denoising</title>
<link>https://arxiv.org/abs/2512.08931</link>
<guid>https://arxiv.org/abs/2512.08931</guid>
<content:encoded><![CDATA[
arXiv:2512.08931v1 Announce Type: cross 
Abstract: Recent advances in diffusion transformers have empowered video generation models to generate high-quality video clips from texts or images. However, world models with the ability to predict long-horizon futures from past observations and actions remain underexplored, especially for general-purpose scenarios and various forms of actions. To bridge this gap, we introduce Astra, an interactive general world model that generates real-world futures for diverse scenarios (e.g., autonomous driving, robot grasping) with precise action interactions (e.g., camera motion, robot action). We propose an autoregressive denoising architecture and use temporal causal attention to aggregate past observations and support streaming outputs. We use a noise-augmented history memory to avoid over-reliance on past frames to balance responsiveness with temporal coherence. For precise action control, we introduce an action-aware adapter that directly injects action signals into the denoising process. We further develop a mixture of action experts that dynamically route heterogeneous action modalities, enhancing versatility across diverse real-world tasks such as exploration, manipulation, and camera control. Astra achieves interactive, consistent, and general long-term video prediction and supports various forms of interactions. Experiments across multiple datasets demonstrate the improvements of Astra in fidelity, long-range prediction, and action alignment over existing state-of-the-art world models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Discovering Influential Factors in Variational Autoencoders</title>
<link>https://arxiv.org/abs/1809.01804</link>
<guid>https://arxiv.org/abs/1809.01804</guid>
<content:encoded><![CDATA[
arXiv:1809.01804v3 Announce Type: replace 
Abstract: In the field of machine learning, it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder(VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of the VAE's reconstruction error and downstream classification task. To make such indicator applicable, we design an algorithm for calculating the mutual information for the VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Learning of Heterogeneous Tail Dependence</title>
<link>https://arxiv.org/abs/2011.13132</link>
<guid>https://arxiv.org/abs/2011.13132</guid>
<content:encoded><![CDATA[
arXiv:2011.13132v3 Announce Type: replace 
Abstract: We propose a multivariate generative model to capture the complex dependence structure often encountered in business and financial data. Our model features heterogeneous and asymmetric tail dependence between all pairs of individual dimensions while also allowing heterogeneity and asymmetry in the tails of the marginals. A significant merit of our model structure is that it is not prone to error propagation in the parameter estimation process, hence very scalable, as the dimensions of datasets grow large. However, the likelihood methods are infeasible for parameter estimation in our case due to the lack of a closed-form density function. Instead, we devise a novel moment learning algorithm to learn the parameters. To demonstrate the effectiveness of the model and its estimator, we test them on simulated as well as real-world datasets. Results show that this framework gives better finite-sample performance compared to the copula-based benchmarks as well as recent similar models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise</title>
<link>https://arxiv.org/abs/2210.11075</link>
<guid>https://arxiv.org/abs/2210.11075</guid>
<content:encoded><![CDATA[
arXiv:2210.11075v3 Announce Type: replace 
Abstract: The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then Train (FTT), that first freezes certain salient features and then trains the rest of the features using ERM. We theoretically show that FTT preserves features that are more beneficial to test time probing. Across two commonly used spurious correlation datasets, FTT outperforms ERM, IRM, JTT and CVaR-DRO, with substantial improvement in accuracy (by 4.5%) when the feature noise is large. FTT also performs better on general distribution shift benchmarks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Self-Distillation for Minimizing Client Drift in Heterogeneous Federated Learning</title>
<link>https://arxiv.org/abs/2305.19600</link>
<guid>https://arxiv.org/abs/2305.19600</guid>
<content:encoded><![CDATA[
arXiv:2305.19600v4 Announce Type: replace 
Abstract: Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid label distributions across clients, FL suffers from the 'client-drift' problem where every client drifts to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to each client's training data based on the global model's prediction entropy and the client-data label distribution. We show in this paper that our proposed regularization (ASD) can be easily integrated atop existing, state-of-the-art FL algorithms, leading to a further boost in the performance of these off-the-shelf methods. We theoretically explain how incorporation of ASD regularizer leads to reduction in client-drift and empirically justify the generalization ability of the trained model. We demonstrate the efficacy of our approach through extensive experiments on multiple real-world benchmarks and show substantial gains in performance when the proposed regularizer is combined with popular FL methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BG-HGNN: Toward Efficient Learning for Complex Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2403.08207</link>
<guid>https://arxiv.org/abs/2403.08207</guid>
<content:encoded><![CDATA[
arXiv:2403.08207v2 Announce Type: replace 
Abstract: Heterogeneous graphs, comprising diverse node and edge types connected through varied relations, are ubiquitous in real-world applications. Message-passing heterogeneous graph neural networks (HGNNs) have emerged as a powerful model class for such data. However, existing HGNNs typically allocate a separate set of learnable weights for each relation type to model relational heterogeneity. Despite their promise, these models are effective primarily on simple heterogeneous graphs with only a few relation types. In this paper, we show that this standard design inherently leads to parameter explosion (the number of learnable parameters grows rapidly with the number of relation types) and relation collapse (the model loses the ability to distinguish among different relations). These issues make existing HGNNs inefficient or impractical for complex heterogeneous graphs with many relation types. To address these challenges, we propose Blend&amp;Grind-HGNN (BG-HGNN), a unified feature-representation framework that integrates and distills relational heterogeneity into a shared low-dimensional feature space. This design eliminates the need for relation-specific parameter sets and enables efficient, expressive learning even as the number of relations grows. Empirically, BG-HGNN achieves substantial gains over state-of-the-art HGNNs, improving parameter efficiency by up to 28.96x and training throughput by up to 110.30x, while matching or surpassing their accuracy on complex heterogeneous graphs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Surrogate HMC: On Using Neural Likelihoods for Hamiltonian Monte Carlo in Simulation-Based Inference</title>
<link>https://arxiv.org/abs/2407.20432</link>
<guid>https://arxiv.org/abs/2407.20432</guid>
<content:encoded><![CDATA[
arXiv:2407.20432v2 Announce Type: replace 
Abstract: Bayesian inference methods such as Markov Chain Monte Carlo (MCMC) typically require repeated computations of the likelihood function, but in some scenarios this is infeasible and alternative methods are needed. Simulation-based inference (SBI) methods address this problem by using machine learning to amortize computations. In this work, we highlight a particular synergy between the SBI method of neural likelihood estimation and the classic MCMC method of Hamiltonian Monte Carlo. We show that approximating the likelihood function with a neural network model can provide three distinct advantages: (1) amortizing the computations for MCMC; (2) providing gradients for Hamiltonian Monte Carlo, and (3) smoothing over noisy simulations resulting from numerical instabilities. We provide practical guidelines for defining a prior, sampling a training set, and evaluating convergence. The method is demonstrated in an application modeling the heliospheric transport of galactic cosmic rays, where it enables efficient inference of latent parameters in the Parker equation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asynchronous Stochastic Approximation with Applications to Average-Reward Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.03915</link>
<guid>https://arxiv.org/abs/2409.03915</guid>
<content:encoded><![CDATA[
arXiv:2409.03915v3 Announce Type: replace 
Abstract: This paper investigates the stability and convergence properties of asynchronous stochastic approximation (SA) algorithms, with a focus on extensions relevant to average-reward reinforcement learning. We first extend a stability proof method of Borkar and Meyn to accommodate more general noise conditions than previously considered, thereby yielding broader convergence guarantees for asynchronous SA. To sharpen the convergence analysis, we further examine the shadowing properties of asynchronous SA, building on a dynamical systems approach of Hirsch and Bena\"{i}m. These results provide a theoretical foundation for a class of relative value iteration-based reinforcement learning algorithms -- developed and analyzed in a companion paper -- for solving average-reward Markov and semi-Markov decision processes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Generalized Hamiltonians using fully Symplectic Mappings</title>
<link>https://arxiv.org/abs/2409.11138</link>
<guid>https://arxiv.org/abs/2409.11138</guid>
<content:encoded><![CDATA[
arXiv:2409.11138v3 Announce Type: replace 
Abstract: Many important physical systems can be described as the evolution of a Hamiltonian system, which has the important property of being conservative, that is, energy is conserved throughout the evolution. Physics Informed Neural Networks and in particular Hamiltonian Neural Networks have emerged as a mechanism to incorporate structural inductive bias into the NN model. By ensuring physical invariances are conserved, the models exhibit significantly better sample complexity and out-of-distribution accuracy than standard NNs. Learning the Hamiltonian as a function of its canonical variables, typically position and velocity, from sample observations of the system thus becomes a critical task in system identification and long-term prediction of system behavior. However, to truly preserve the long-run physical conservation properties of Hamiltonian systems, one must use symplectic integrators for a forward pass of the system's simulation. While symplectic schemes have been used in the literature, they are thus far limited to situations when they reduce to explicit algorithms, which include the case of separable Hamiltonians or augmented non-separable Hamiltonians. We extend it to generalized non-separable Hamiltonians, and noting the self-adjoint property of symplectic integrators, we bypass computationally intensive backpropagation through an ODE solver. We show that the method is robust to noise and provides a good approximation of the system Hamiltonian when the state variables are sampled from a noisy observation. In the numerical results, we show the performance of the method concerning Hamiltonian reconstruction and conservation, indicating its particular advantage for non-separable systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometry Aware Meta-Learning Neural Network for Joint Phase and Precoder Optimization in RIS</title>
<link>https://arxiv.org/abs/2409.11270</link>
<guid>https://arxiv.org/abs/2409.11270</guid>
<content:encoded><![CDATA[
arXiv:2409.11270v2 Announce Type: replace 
Abstract: In reconfigurable intelligent surface (RIS) aided systems, the joint optimization of the precoder matrix at the base station and the phase shifts of the RIS elements involves significant complexity. In this paper, we propose a complex-valued, geometry aware meta-learning neural network that maximizes the weighted sum rate in a multi-user multiple input single output system. By leveraging the complex circle geometry for phase shifts and spherical geometry for the precoder, the optimization occurs on Riemannian manifolds, leading to faster convergence. We use a complex-valued neural network for phase shifts and an Euler inspired update for the precoder network. Our approach outperforms existing neural network-based algorithms, offering higher weighted sum rates, lower power consumption, and significantly faster convergence. Specifically, it converges faster by nearly 100 epochs, with a 0.7 bps improvement in weighted sum rate and a 1.8 dB power gain when compared with existing work. Further it outperforms the state-of-the-art alternating optimization algorithm by 0.86 bps with a 2.6 dB power gain.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score-based Conditional Out-of-Distribution Augmentation for Graph Covariate Shift</title>
<link>https://arxiv.org/abs/2410.17506</link>
<guid>https://arxiv.org/abs/2410.17506</guid>
<content:encoded><![CDATA[
arXiv:2410.17506v2 Announce Type: replace 
Abstract: Distribution shifts between training and testing datasets significantly impair the model performance on graph learning. A commonly-taken causal view in graph invariant learning suggests that stable predictive features of graphs are causally associated with labels, whereas varying environmental features lead to distribution shifts. In particular, covariate shifts caused by unseen environments in test graphs underscore the critical need for out-of-distribution (OOD) generalization. Existing graph augmentation methods designed to address the covariate shift often disentangle the stable and environmental features in the input space, and selectively perturb or mixup the environmental features. However, such perturbation-based methods heavily rely on an accurate separation of stable and environmental features, and their exploration ability is confined to existing environmental features in the training distribution. To overcome these limitations, we introduce a novel distributional augmentation approach enabled by a tailored score-based conditional graph generation strategies to explore and synthesize unseen environments while preserving the validity and stable features of overall graph patterns. Our comprehensive empirical evaluations demonstrate the enhanced effectiveness of our method in improving graph OOD generalization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLL: A Differentiable Graph Learning Layer for Neural Networks</title>
<link>https://arxiv.org/abs/2412.08016</link>
<guid>https://arxiv.org/abs/2412.08016</guid>
<content:encoded><![CDATA[
arXiv:2412.08016v2 Announce Type: replace 
Abstract: Standard deep learning architectures used for classification generate label predictions with a projection head and softmax activation function. Although successful, these methods fail to leverage the relational information between samples for generating label predictions. In recent works, graph-based learning techniques, namely Laplace learning, have been heuristically combined with neural networks for both supervised and semi-supervised learning (SSL) tasks. However, prior works approximate the gradient of the loss function with respect to the graph learning algorithm or decouple the processes; end-to-end integration with neural networks is not achieved. In this work, we derive backpropagation equations, via the adjoint method, for inclusion of a general family of graph learning layers into a neural network. The resulting method, distinct from graph neural networks, allows us to precisely integrate similarity graph construction and graph Laplacian-based label propagation into a neural network layer, replacing a projection head and softmax activation function for general classification task. Our experimental results demonstrate smooth label transitions across data, improved generalization and robustness to adversarial attacks, and improved training dynamics compared to a standard softmax-based approach.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Oscillations Make Neural Networks Robust to Quantization</title>
<link>https://arxiv.org/abs/2502.00490</link>
<guid>https://arxiv.org/abs/2502.00490</guid>
<content:encoded><![CDATA[
arXiv:2502.00490v2 Announce Type: replace 
Abstract: We challenge the prevailing view that weight oscillations observed during Quantization Aware Training (QAT) are merely undesirable side-effects and argue instead that they are an essential part of QAT. We show in a univariate linear model that QAT results in an additional loss term that causes oscillations by pushing weights away from their nearest quantization level. Based on the mechanism from the analysis, we then derive a regularizer that induces oscillations in the weights of neural networks during training. Our empirical results on ResNet-18 and Tiny Vision Transformer, evaluated on CIFAR-10 and Tiny ImageNet datasets, demonstrate across a range of quantization levels that training with oscillations followed by post-training quantization (PTQ) is sufficient to recover the performance of QAT in most cases. With this work we provide further insight into the dynamics of QAT and contribute a novel insight into explaining the role of oscillations in QAT which until now have been considered to have a primarily negative effect on quantization.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow-based Conformal Prediction for Multi-dimensional Time Series</title>
<link>https://arxiv.org/abs/2502.05709</link>
<guid>https://arxiv.org/abs/2502.05709</guid>
<content:encoded><![CDATA[
arXiv:2502.05709v2 Announce Type: replace 
Abstract: Time series prediction underpins a broad range of downstream tasks across many scientific domains. Recent advances and increasing adoption of black-box machine learning models for time series prediction highlight the critical need for reliable uncertainty quantification. While conformal prediction has gained attention as a reliable uncertainty quantification method, conformal prediction for time series faces two key challenges: (1) adaptively leveraging correlations in features and non-conformity scores to overcome the exchangeability assumption, and (2) constructing prediction sets for multi-dimensional outcomes. To address these challenges jointly, we propose a novel conformal prediction method for time series using flow with classifier-free guidance. We provide coverage guarantees by establishing exact non-asymptotic marginal coverage and a finite-sample bound on conditional coverage for the proposed method. Evaluations on real-world time series datasets demonstrate that our method constructs significantly smaller prediction sets than existing conformal prediction methods while maintaining target coverage.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Computing Enhanced Federated Learning in IIoT: Satisfaction-Aware Incentive Scheme via DRL-Based Stackelberg Game</title>
<link>https://arxiv.org/abs/2502.06909</link>
<guid>https://arxiv.org/abs/2502.06909</guid>
<content:encoded><![CDATA[
arXiv:2502.06909v3 Announce Type: replace 
Abstract: The Industrial Internet of Things (IIoT) leverages Federated Learning (FL) for distributed model training while preserving data privacy, and meta-computing enhances FL by optimizing and integrating distributed computing resources, improving efficiency and scalability. Efficient IIoT operations require a trade-off between model quality and training latency. Consequently, a primary challenge of FL in IIoT is to optimize overall system performance by balancing model quality and training latency. This paper designs a satisfaction function that accounts for data size, Age of Information (AoI), and training latency for meta-computing. Additionally, the satisfaction function is incorporated into the utility function to incentivize IIoT nodes to participate in model training. We model the utility functions of servers and nodes as a two-stage Stackelberg game and employ a deep reinforcement learning approach to learn the Stackelberg equilibrium. This approach ensures balanced rewards and enhances the applicability of the incentive scheme for IIoT. Simulation results demonstrate that, under the same budget constraints, the proposed incentive scheme improves utility by at least 23.7% compared to existing FL schemes without compromising model accuracy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proper Learnability and the Role of Unlabeled Data</title>
<link>https://arxiv.org/abs/2502.10359</link>
<guid>https://arxiv.org/abs/2502.10359</guid>
<content:encoded><![CDATA[
arXiv:2502.10359v2 Announce Type: replace 
Abstract: Proper learning refers to the setting in which learners must emit predictors in the underlying hypothesis class $H$, and often leads to learners with simple algorithmic forms (e.g. empirical risk minimization (ERM), structural risk minimization (SRM)). The limitation of proper learning, however, is that there exist problems which can only be learned improperly, e.g. in multiclass classification. Thus, we ask: Under what assumptions on the hypothesis class or the information provided to the learner is a problem properly learnable? We first demonstrate that when the unlabeled data distribution is given, there always exists an optimal proper learner governed by distributional regularization, a randomized generalization of regularization. We refer to this setting as the distribution-fixed PAC model, and continue to evaluate the learner on its worst-case performance over all distributions. Our result holds for all metric loss functions and any finite learning problem (with no dependence on its size). Further, we demonstrate that sample complexities in the distribution-fixed PAC model can shrink by only a logarithmic factor from the classic PAC model, strongly refuting the role of unlabeled data in PAC learning (from a worst-case perspective).
  We complement this with impossibility results which obstruct any characterization of proper learnability in the realizable PAC model. First, we observe that there are problems whose proper learnability is logically undecidable, i.e., independent of the ZFC axioms. We then show that proper learnability is not a monotone property of the underlying hypothesis class, and that it is not a local property (in a precise sense). Our impossibility results all hold even for the fundamental setting of multiclass classification, and go through a reduction of EMX learning (Ben-David et al., 2019) to proper classification which may be of independent interest.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preservation through Practical Machine Unlearning</title>
<link>https://arxiv.org/abs/2502.10635</link>
<guid>https://arxiv.org/abs/2502.10635</guid>
<content:encoded><![CDATA[
arXiv:2502.10635v3 Announce Type: replace 
Abstract: Machine Learning models thrive on vast datasets, continuously adapting to provide accurate predictions and recommendations. However, in an era dominated by privacy concerns, Machine Unlearning emerges as a transformative approach, enabling the selective removal of data from trained models. This paper examines methods such as Naive Retraining and Exact Unlearning via the SISA framework, evaluating their Computational Costs, Consistency, and feasibility using the $\texttt{HSpam14}$ dataset. We explore the potential of integrating unlearning principles into Positive Unlabeled (PU) Learning to address challenges posed by partially labeled datasets. Our findings highlight the promise of unlearning frameworks like $\textit{DaRE}$ for ensuring privacy compliance while maintaining model performance, albeit with significant computational trade-offs. This study underscores the importance of Machine Unlearning in achieving ethical AI and fostering trust in data-driven systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OIPR: Evaluation for Time-series Anomaly Detection Inspired by Operator Interest</title>
<link>https://arxiv.org/abs/2503.01260</link>
<guid>https://arxiv.org/abs/2503.01260</guid>
<content:encoded><![CDATA[
arXiv:2503.01260v2 Announce Type: replace 
Abstract: With the growing adoption of time-series anomaly detection (TAD) technology, numerous studies have employed deep learning-based detectors to analyze time-series data in the fields of Internet services, industrial systems, and sensors. The selection and optimization of anomaly detectors strongly rely on the availability of an effective evaluation for TAD performance. Since anomalies in time-series data often manifest as a sequence of points, conventional metrics that solely consider the detection of individual points are inadequate. Existing TAD evaluators typically employ point-based or event-based metrics to capture the temporal context. However, point-based evaluators tend to overestimate detectors that excel only in detecting long anomalies, while event-based evaluators are susceptible to being misled by fragmented detection results. To address these limitations, we propose OIPR (Operator Interest-based Precision and Recall metrics), a novel TAD evaluator with area-based metrics. It models the process of operators receiving detector alarms and handling anomalies, utilizing area under the operator interest curve to evaluate TAD performance. Furthermore, we build a special scenario dataset to compare the characteristics of different evaluators. Through experiments conducted on the special scenario dataset and five real-world datasets, we demonstrate the remarkable performance of OIPR in extreme and complex scenarios. It achieves a balance between point and event perspectives, overcoming their primary limitations and offering applicability to broader situations.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation Retrieval Learning for Heterogeneous Data Integration</title>
<link>https://arxiv.org/abs/2503.09494</link>
<guid>https://arxiv.org/abs/2503.09494</guid>
<content:encoded><![CDATA[
arXiv:2503.09494v3 Announce Type: replace 
Abstract: In the era of big data, large-scale, multi-source, multi-modality datasets are increasingly ubiquitous, offering unprecedented opportunities for predictive modeling and scientific discovery. However, these datasets often exhibit complex heterogeneity, such as covariates shift, posterior drift, and blockwise missingness, which worsen predictive performance of existing supervised learning algorithms. To address these challenges simultaneously, we propose a novel Representation Retrieval (R2) framework, which integrates a dictionary of representation learning modules (representer dictionary) with data source-specific sparsity-induced machine learning model (learners). Under the R2 framework, we introduce the notion of integrativeness for each representer, and propose a novel Selective Integration Penalty (SIP) to explicitly encourage more integrative representers to improve predictive performance. Theoretically, we show that the excess risk bound of the R2 framework is characterized by the integrativeness of representers, and SIP effectively improves the excess risk. Extensive simulation studies validate the superior performance of R2 framework and the effect of SIP. We further apply our method to two real-world datasets to confirm its empirical success.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLCAD: Reinforcement Learning Training Gym for Revolution Involved CAD Command Sequence Generation</title>
<link>https://arxiv.org/abs/2503.18549</link>
<guid>https://arxiv.org/abs/2503.18549</guid>
<content:encoded><![CDATA[
arXiv:2503.18549v3 Announce Type: replace 
Abstract: A CAD command sequence is a typical parametric design paradigm in 3D CAD systems where a model is constructed by overlaying 2D sketches with operations such as extrusion, revolution, and Boolean operations. Although there is growing academic interest in the automatic generation of command sequences, existing methods and datasets only support operations such as 2D sketching, extrusion,and Boolean operations. This limitation makes it challenging to represent more complex geometries. In this paper, we present a reinforcement learning (RL) training environment (gym) built on a CAD geometric engine. Given an input boundary representation (B-Rep) geometry, the policy network in the RL algorithm generates an action. This action, along with previously generated actions, is processed within the gym to produce the corresponding CAD geometry, which is then fed back into the policy network. The rewards, determined by the difference between the generated and target geometries within the gym, are used to update the RL network. Our method supports operations beyond sketches, Boolean, and extrusion, including revolution operations. With this training gym, we achieve state-of-the-art (SOTA) quality in generating command sequences from B-Rep geometries.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabKAN: Advancing Tabular Data Analysis using Kolmogorov-Arnold Network</title>
<link>https://arxiv.org/abs/2504.06559</link>
<guid>https://arxiv.org/abs/2504.06559</guid>
<content:encoded><![CDATA[
arXiv:2504.06559v3 Announce Type: replace 
Abstract: Tabular data analysis presents unique challenges that arise from heterogeneous feature types, missing values, and complex feature interactions. While traditional machine learning methods like gradient boosting often outperform deep learning, recent advancements in neural architectures offer promising alternatives. In this study, we introduce TabKAN, a novel framework for tabular data modeling based on Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs use learnable activation functions on edges, which improves both interpretability and training efficiency. TabKAN incorporates modular KAN-based architectures designed for tabular analysis and proposes a transfer learning framework for knowledge transfer across domains. Furthermore, we develop a model-specific interpretability approach that reduces reliance on post hoc explanations. Extensive experiments on public datasets show that TabKAN achieves superior performance in supervised learning and significantly outperforms classical and Transformer-based models in binary and multi-class classification. The results demonstrate the potential of KAN-based architectures to bridge the gap between traditional machine learning and deep learning for structured data.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sonnet: Spectral Operator Neural Network for Multivariable Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.15312</link>
<guid>https://arxiv.org/abs/2505.15312</guid>
<content:encoded><![CDATA[
arXiv:2505.15312v2 Announce Type: replace 
Abstract: Multivariable time series forecasting methods can integrate information from exogenous variables, leading to significant prediction accuracy gains. The transformer architecture has been widely applied in various time series forecasting models due to its ability to capture long-range sequential dependencies. However, a na\"ive application of transformers often struggles to effectively model complex relationships among variables over time. To mitigate against this, we propose a novel architecture, termed Spectral Operator Neural Network (Sonnet). Sonnet applies learnable wavelet transformations to the input and incorporates spectral analysis using the Koopman operator. Its predictive skill relies on the Multivariable Coherence Attention (MVCA), an operation that leverages spectral coherence to model variable dependencies. Our empirical analysis shows that Sonnet yields the best performance on $34$ out of $47$ forecasting tasks with an average mean absolute error (MAE) reduction of $2.2\%$ against the most competitive baseline. We further show that MVCA can remedy the deficiencies of na\"ive attention in various deep learning models, reducing MAE by $10.7\%$ on average in the most challenging forecasting tasks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Implicit Regularization of Gradient Descent in Over-parameterized Models</title>
<link>https://arxiv.org/abs/2505.17304</link>
<guid>https://arxiv.org/abs/2505.17304</guid>
<content:encoded><![CDATA[
arXiv:2505.17304v2 Announce Type: replace 
Abstract: Implicit regularization refers to the tendency of local search algorithms to converge to low-dimensional solutions, even when such structures are not explicitly enforced. Despite its ubiquity, the mechanism underlying this behavior remains poorly understood, particularly in over-parameterized settings. We analyze gradient descent dynamics and identify three conditions under which it converges to second-order stationary points within an implicit low-dimensional region: (i) suitable initialization, (ii) efficient escape from saddle points, and (iii) sustained proximity to the region. We show that these can be achieved through infinitesimal perturbations and a small deviation rate. Building on this, we introduce Infinitesimally Perturbed Gradient Descent (IPGD), which satisfies these conditions under mild assumptions. We provide theoretical guarantees for IPGD in over-parameterized matrix sensing and empirical evidence of its broader applicability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
arXiv:2505.20589v2 Announce Type: replace 
Abstract: The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions-from sequence-level properties and residue-specific attributes to complex inter-protein interactions-into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling general-purpose decoders to generalize across five distinct categories. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Token's predictive power in different types of protein-prediction tasks. In 3D structure prediction, Prot2Token delivers substantial speedups (up to 1000x faster than AlphaFold2 with MSA on the same hardware) while, across other numerous tasks, matching or surpassing specialized methods. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a step towards standardizing biological prediction into a generative interface, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can Slow-thinking LLMs Reason Over Time? Empirical Studies in Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.24511</link>
<guid>https://arxiv.org/abs/2505.24511</guid>
<content:encoded><![CDATA[
arXiv:2505.24511v3 Announce Type: replace 
Abstract: Time series forecasting (TSF) is a fundamental and widely studied task, spanning methods from classical statistical approaches to modern deep learning and multimodal language modeling. Despite their effectiveness, these methods often follow a fast thinking paradigm emphasizing pattern extraction and direct value mapping, while overlooking explicit reasoning over temporal dynamics and contextual dependencies. Meanwhile, emerging slow-thinking LLMs (e.g., ChatGPT-o1, DeepSeek-R1) have demonstrated impressive multi-step reasoning capabilities across diverse domains, suggesting a new opportunity for reframing TSF as a structured reasoning task. This motivates a key question: can slow-thinking LLMs effectively reason over temporal patterns to support time series forecasting, even in zero-shot manner? To investigate this, in this paper, we propose TimeReasoner, an extensive empirical study that formulates TSF as a conditional reasoning task. We design a series of prompting strategies to elicit inference-time reasoning from pretrained slow-thinking LLMs and evaluate their performance across diverse TSF benchmarks. Our findings reveal that slow-thinking LLMs exhibit non-trivial zero-shot forecasting capabilities, especially in capturing high-level trends and contextual shifts. While preliminary, our study surfaces important insights into the reasoning behaviors of LLMs in temporal domains highlighting both their potential and limitations. We hope this work catalyzes further research into reasoning-based forecasting paradigms and paves the way toward more interpretable and generalizable TSF frameworks.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curse of Slicing: Why Sliced Mutual Information is a Deceptive Measure of Statistical Dependence</title>
<link>https://arxiv.org/abs/2506.04053</link>
<guid>https://arxiv.org/abs/2506.04053</guid>
<content:encoded><![CDATA[
arXiv:2506.04053v3 Announce Type: replace 
Abstract: Sliced Mutual Information (SMI) is widely used as a scalable alternative to mutual information for measuring non-linear statistical dependence. Despite its advantages, such as faster convergence, robustness to high dimensionality, and nullification only under statistical independence, we demonstrate that SMI is highly susceptible to data manipulation and exhibits counterintuitive behavior. Through extensive benchmarking and theoretical analysis, we show that SMI saturates easily, fails to detect increases in statistical dependence, prioritizes redundancy over informative content, and in some cases, performs worse than correlation coefficient.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Schauder Bases for $C[0, 1]$ Using ReLU, Softplus and Two Sigmoidal Functions</title>
<link>https://arxiv.org/abs/2506.07884</link>
<guid>https://arxiv.org/abs/2506.07884</guid>
<content:encoded><![CDATA[
arXiv:2506.07884v2 Announce Type: replace 
Abstract: We construct four Schauder bases for the space $C[0,1]$, one using ReLU functions, another using Softplus functions, and two more using sigmoidal versions of the ReLU and Softplus functions. This establishes the existence of a basis using these functions for the first time, and improves on the universal approximation property associated with them. We also show an $O(\frac{1}{n})$ approximation bound based on our ReLU basis, and a negative result on constructing multivariate functions using finite combinations of ReLU functions.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Adaptation as Posterior Correction</title>
<link>https://arxiv.org/abs/2506.14262</link>
<guid>https://arxiv.org/abs/2506.14262</guid>
<content:encoded><![CDATA[
arXiv:2506.14262v3 Announce Type: replace 
Abstract: Adaptation is the holy grail of intelligence, but even the best AI models lack the adaptability of toddlers. In spite of great progress, little is known about the mechanisms by which machines can learn to adapt as fast as humans and animals. Here, we cast adaptation as `correction' of old posteriors and show that a wide-variety of existing adaptation methods follow this very principle, including those used for continual learning, federated learning, unlearning, and model merging. In all these settings, more accurate posteriors often lead to smaller corrections and can enable faster adaptation. Posterior correction is derived by using the dual representation of the Bayesian Learning Rule of Khan and Rue (2023), where the interference between the old representation and new information is quantified by using the natural-gradient mismatch. We present many examples demonstrating how machines can learn to adapt quickly by using posterior correction.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Elucidated Rolling Diffusion Models for Probabilistic Forecasting of Complex Dynamics</title>
<link>https://arxiv.org/abs/2506.20024</link>
<guid>https://arxiv.org/abs/2506.20024</guid>
<content:encoded><![CDATA[
arXiv:2506.20024v3 Announce Type: replace 
Abstract: Diffusion models are a powerful tool for probabilistic forecasting, yet most applications in high-dimensional complex systems predict future states individually. This approach struggles to model complex temporal dependencies and fails to explicitly account for the progressive growth of uncertainty inherent to the systems. While rolling diffusion frameworks, which apply increasing noise to forecasts at longer lead times, have been proposed to address this, their integration with state-of-the-art, high-fidelity diffusion techniques remains a significant challenge. We tackle this problem by introducing Elucidated Rolling Diffusion Models (ERDM), the first framework to successfully unify a rolling forecast structure with the principled, performant design of Elucidated Diffusion Models (EDM). To do this, we adapt the core EDM components-its noise schedule, network preconditioning, and Heun sampler-to the rolling forecast setting. The success of this integration is driven by three key contributions: (i) a novel loss weighting scheme that focuses model capacity on the mid-range forecast horizons where determinism gives way to stochasticity; (ii) an efficient initialization strategy using a pre-trained EDM for the initial window; and (iii) a bespoke hybrid sequence architecture for robust spatiotemporal feature extraction under progressive denoising. On 2D Navier-Stokes simulations and ERA5 global weather forecasting at 1.5-degree resolution, ERDM consistently outperforms key diffusion-based baselines, including conditional autoregressive EDM. ERDM offers a flexible and powerful general framework for tackling diffusion-based dynamics forecasting problems where modeling uncertainty propagation is paramount.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Models for Controllable DNA Sequence Design</title>
<link>https://arxiv.org/abs/2507.19523</link>
<guid>https://arxiv.org/abs/2507.19523</guid>
<content:encoded><![CDATA[
arXiv:2507.19523v2 Announce Type: replace 
Abstract: We consider controllable DNA sequence design, where sequences are generated by conditioning on specific biological properties. While language models (LMs) such as GPT and BERT have achieved remarkable success in natural language generation, their application to DNA sequence generation remains largely underexplored. In this work, we introduce ATGC-Gen, an Automated Transformer Generator for Controllable Generation, which leverages cross-modal encoding to integrate diverse biological signals. ATGC-Gen is instantiated with both decoder-only and encoder-only transformer architectures, allowing flexible training and generation under either autoregressive or masked recovery objectives. We evaluate ATGC-Gen on representative tasks including promoter and enhancer sequence design, and further introduce a new dataset based on ChIP-Seq experiments for modeling protein binding specificity. Our experiments demonstrate that ATGC-Gen can generate fluent, diverse, and biologically relevant sequences aligned with the desired properties. Compared to prior methods, our model achieves notable improvements in controllability and functional relevance, highlighting the potential of language models in advancing programmable genomic design. The source code is released at (https://github.com/divelab/AIRS/blob/main/OpenBio/ATGC_Gen).
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRISM: Lightweight Multivariate Time-Series Classification through Symmetric Multi-Resolution Convolutional Layers</title>
<link>https://arxiv.org/abs/2508.04503</link>
<guid>https://arxiv.org/abs/2508.04503</guid>
<content:encoded><![CDATA[
arXiv:2508.04503v2 Announce Type: replace 
Abstract: Multivariate time series classification supports applications from wearable sensing to biomedical monitoring and demands models that can capture both short-term patterns and longer-range temporal dependencies. Despite recent advances, Transformer and CNN models often remain computationally heavy and rely on many parameters. This work presents PRISM(Per-channel Resolution Informed Symmetric Module), a lightweight fully convolutional classifier. Operating in a channel-independent manner, in its early stage it applies a set of multi-resolution symmetric convolutional filters. This symmetry enforces structural constraints inspired by linear-phase FIR filters from classical signal processing, effectively halving the number of learnable parameters within the initial layers while preserving the full receptive field. Across the diverse UEA multivariate time-series archive as well as specific benchmarks in human activity recognition, sleep staging, and biomedical signals, PRISM matches or outperforms state-of-the-art CNN and Transformer models while using significantly fewer parameters and markedly lower computational cost. By bringing a principled signal processing prior into a modern neural architecture, PRISM offers an effective and computationally economical solution for multivariate time series classification.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models</title>
<link>https://arxiv.org/abs/2508.14285</link>
<guid>https://arxiv.org/abs/2508.14285</guid>
<content:encoded><![CDATA[
arXiv:2508.14285v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptation (LoRA) is a cost-effective way to incorporate information from a specific dataset. However, it is often unclear how well the fine-tuned LLM will generalize, i.e., how well it will perform on unseen datasets. Methods have been proposed to improve generalization by optimizing in-context prompts, or by using meta-learning to fine-tune LLMs. However, these methods are expensive in memory and computation, requiring either long-context prompts or saving copies of parameters and using second-order gradient updates. To address these challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This method builds on amortized Bayesian meta-learning for smaller models, adapting this approach to LLMs while maintaining its computational efficiency. We reframe task-specific and global parameters in the context of LoRA and use a new hyperparameter to balance reconstruction accuracy and the fidelity of task-specific parameters to the global ones. ABMLL provides effective generalization and scales to large models such as LLAMA3-8B. Furthermore, as a result of using a Bayesian framework, ABMLL provides improved uncertainty quantification. We test ABMLL on CrossFit and Unified-QA datasets and find that it outperforms existing methods on these benchmarks in terms of both accuracy and expected calibration error.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</title>
<link>https://arxiv.org/abs/2509.04622</link>
<guid>https://arxiv.org/abs/2509.04622</guid>
<content:encoded><![CDATA[
arXiv:2509.04622v5 Announce Type: replace 
Abstract: Representational similarity metrics are fundamental tools in neuroscience and AI, yet we lack systematic comparisons of their discriminative power across model families. We introduce a quantitative framework to evaluate representational similarity measures based on their ability to separate model families-across architectures (CNNs, Vision Transformers, Swin Transformers, ConvNeXt) and training regimes (supervised vs. self-supervised). Using three complementary separability measures-dprime from signal detection theory, silhouette coefficients and ROC-AUC, we systematically assess the discriminative capacity of commonly used metrics including RSA, linear predictivity, Procrustes, and soft matching. We show that separability systematically increases as metrics impose more stringent alignment constraints. Among mapping-based approaches, soft-matching achieves the highest separability, followed by Procrustes alignment and linear predictivity. Non-fitting methods such as RSA also yield strong separability across families. These results provide the first systematic comparison of similarity metrics through a separability lens, clarifying their relative sensitivity and guiding metric choice for large-scale model and brain comparisons.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Coloring for Multi-Task Learning</title>
<link>https://arxiv.org/abs/2509.16959</link>
<guid>https://arxiv.org/abs/2509.16959</guid>
<content:encoded><![CDATA[
arXiv:2509.16959v4 Announce Type: replace 
Abstract: When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby potentially reducing the final model's performance. To address this, we introduce SON-GOKU, a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated, and the grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, multi-task learning will improve model performance rather than impede it. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers. We provide extensive theory showing why grouping and sequential updates improve multi-task learning, with guarantees on descent, convergence, and accurately identifying what tasks conflict or align.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Nonlinear Optimization with GPU-Accelerated Neural Network Constraints</title>
<link>https://arxiv.org/abs/2509.22462</link>
<guid>https://arxiv.org/abs/2509.22462</guid>
<content:encoded><![CDATA[
arXiv:2509.22462v2 Announce Type: replace 
Abstract: We propose a reduced-space formulation for optimizing over trained neural networks where the network's outputs and derivatives are evaluated on a GPU. To do this, we treat the neural network as a "gray box" where intermediate variables and constraints are not exposed to the optimization solver. Compared to the full-space formulation, in which intermediate variables and constraints are exposed to the optimization solver, the reduced-space formulation leads to faster solves and fewer iterations in an interior point method. We demonstrate the benefits of this method on two optimization problems: Adversarial generation for a classifier trained on MNIST images and security-constrained optimal power flow with transient feasibility enforced using a neural network surrogate.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Short window attention enables long-term memorization</title>
<link>https://arxiv.org/abs/2509.24552</link>
<guid>https://arxiv.org/abs/2509.24552</guid>
<content:encoded><![CDATA[
arXiv:2509.24552v2 Announce Type: replace 
Abstract: Recent works show that hybrid architectures combining sliding window softmax attention layers with linear recurrent neural network (RNN) layers outperform both of these architectures taken separately. However, the impact of the window length and the interplay between softmax attention and linear RNN layers remain under-studied. In this work, we introduce SWAX, a hybrid architecture consisting of sliding-window attention and xLSTM linear RNN layers.
  A counter-intuitive finding with SWAX is that larger sliding windows do not improve the long-context performance. In fact, short window attention encourages the model to better train the long-term memory of the xLSTM, by relying less on the softmax attention mechanism for long context-retrieval.
  The issue with small sliding windows is that they are detrimental for short-context tasks, which could be solved with information from moderately larger sliding windows otherwise. Therefore, we train SWAX by stochastically changing the sliding window size, forcing the model to leverage both a longer context window and the xLSTM memory. SWAX trained with stochastic window sizes significantly outperforms regular window attention both on short and long-context problems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniPruning: Unifying Local Metric and Global Feedback for Scalable Sparse LLMs</title>
<link>https://arxiv.org/abs/2510.03291</link>
<guid>https://arxiv.org/abs/2510.03291</guid>
<content:encoded><![CDATA[
arXiv:2510.03291v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) achieve strong performance across diverse tasks but face prohibitive computational and memory costs. Pruning offers a promising path by inducing sparsity while preserving architectural flexibility. However, existing methods struggle to balance efficiency and robustness: local metric approaches prune layer by layer but often collapse under high sparsity, whereas global feedback methods enforce consistency at the cost of expensive weight updates or restrictive semi-structured formats. We present UniPruning, a unified post-training pruning framework that combines the speed of local saliency metrics with the stability of global coordination, enabled by a mirror descent based optimization, all without updating model weights. UniPruning leverages fast layer-wise scoring and a lightweight global controller to allocate a single sparsity budget, supporting both unstructured and semi-structured N :M pruning within one framework. After a brief calibration, it can generate pruning masks for arbitrary sparsity levels in one shot, and adapts seamlessly to hardware-aware constraints. Extensive experiments on multiple pretrained LLM families and standard benchmarks show that UniPruning consistently delivers competitive or superior perplexity and zero-shot accuracy. Ablation studies further highlight the importance of mirror descent and local saliency anchoring. Overall, UniPruning provides an efficient, principled, and scalable solution for sparsifying large-scale LLMs. Our code is available at: https://github.com/RainbowQTT/UniPruning.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment</title>
<link>https://arxiv.org/abs/2510.05526</link>
<guid>https://arxiv.org/abs/2510.05526</guid>
<content:encoded><![CDATA[
arXiv:2510.05526v2 Announce Type: replace 
Abstract: Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \textit{\textbf{C}orrupted} preference, reward \textit{\textbf{O}veroptimization}, and bias towards \textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\textbf{COV} and DPO-\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection</title>
<link>https://arxiv.org/abs/2510.05535</link>
<guid>https://arxiv.org/abs/2510.05535</guid>
<content:encoded><![CDATA[
arXiv:2510.05535v2 Announce Type: replace 
Abstract: Feature selection eliminates redundancy among features to improve downstream task performance while reducing computational overhead. Existing methods often struggle to capture intricate feature interactions and adapt across diverse application scenarios. Recent advances employ generative intelligence to alleviate these drawbacks. However, these methods remain constrained by permutation sensitivity in embedding and reliance on convexity assumptions in gradient-based search. To address these limitations, our initial work introduces a novel framework that integrates permutation-invariant embedding with policy-guided search. Although effective, it still left opportunities to adapt to realistic distributed scenarios. In practice, data across local clients is highly imbalanced, heterogeneous and constrained by strict privacy regulations, limiting direct sharing. These challenges highlight the need for a framework that can integrate feature selection knowledge across clients without exposing sensitive information. In this extended journal version, we advance the framework from two perspectives: 1) developing a privacy-preserving knowledge fusion strategy to derive a unified representation space without sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy to address distributional imbalance among heterogeneous local clients. Extensive experiments validate the effectiveness, robustness, and efficiency of our framework. The results further demonstrate its strong generalization ability in federated learning scenarios. The code and data are publicly available: https://anonymous.4open.science/r/FedCAPS-08BF.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value-State Gated Attention for Mitigating Extreme-Token Phenomena in Transformers</title>
<link>https://arxiv.org/abs/2510.09017</link>
<guid>https://arxiv.org/abs/2510.09017</guid>
<content:encoded><![CDATA[
arXiv:2510.09017v2 Announce Type: replace 
Abstract: Large models based on the Transformer architecture are susceptible to extreme-token phenomena, such as attention sinks and value-state drains. These issues, which degrade model performance, quantization fidelity, and interpretability, arise from a problematic mutual reinforcement mechanism where the model learns an inefficient 'no-op' behavior by focusing attention on tokens with near-zero value states. In this paper, we propose Value-State Gated Attention (VGA), a simple, dedicated, and stable architectural mechanism for performing 'no-op' attention efficiently by directly breaking this cycle. VGA introduces a learnable, data-dependent gate, computed directly from the value vectors (V), to modulate the output. Through a theoretical analysis of the underlying gradients, we show that gating the value-state with a function of itself is more effective at decoupling value and attention score updates than prior methods that gate on input embeddings. This creates a direct regulatory pathway that allows the model to suppress a token's contribution based on its emergent value representation. Our experiments demonstrate that VGA significantly mitigates the formation of attention sinks and stabilizes value-state norms, leading to improved performance, robust quantization fidelity, and enhanced model interpretability.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17923</link>
<guid>https://arxiv.org/abs/2510.17923</guid>
<content:encoded><![CDATA[
arXiv:2510.17923v4 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-driven Reinforcement Learning in Continuous Control</title>
<link>https://arxiv.org/abs/2511.07904</link>
<guid>https://arxiv.org/abs/2511.07904</guid>
<content:encoded><![CDATA[
arXiv:2511.07904v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has been recognized as a powerful tool for robot control tasks. RL typically employs reward functions to define task objectives and guide agent learning. However, since the reward function serves the dual purpose of defining the optimal goal and guiding learning, it is challenging to design the reward function manually, which often results in a suboptimal task representation. To tackle the reward design challenge in RL, inspired by the satisficing theory, we propose a Test-driven Reinforcement Learning (TdRL) framework. In the TdRL framework, multiple test functions are used to represent the task objective rather than a single reward function. Test functions can be categorized as pass-fail tests and indicative tests, each dedicated to defining the optimal objective and guiding the learning process, respectively, thereby making defining tasks easier. Building upon such a task definition, we first prove that if a trajectory return function assigns higher returns to trajectories closer to the optimal trajectory set, maximum entropy policy optimization based on this return function will yield a policy that is closer to the optimal policy set. Then, we introduce a lexicographic heuristic approach to compare the relative distance relationship between trajectories and the optimal trajectory set for learning the trajectory return function. Furthermore, we develop an algorithm implementation of TdRL. Experimental results on the DeepMind Control Suite benchmark demonstrate that TdRL matches or outperforms handcrafted reward methods in policy training, with greater design simplicity and inherent support for multi-objective optimization. We argue that TdRL offers a novel perspective for representing task objectives, which could be helpful in addressing the reward design challenges in RL applications.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decomposition of Small Transformer Models</title>
<link>https://arxiv.org/abs/2511.08854</link>
<guid>https://arxiv.org/abs/2511.08854</guid>
<content:encoded><![CDATA[
arXiv:2511.08854v2 Announce Type: replace 
Abstract: Recent work in mechanistic interpretability has shown that decomposing models in parameter space may yield clean handles for analysis and intervention. Previous methods have demonstrated successful applications on a wide range of toy models, but the gap to "real models" has not yet been bridged. In this work, we extend Stochastic Parameter Decomposition (SPD) to Transformer models, proposing an updated causal importance function suited for sequential data and a new loss function. We demonstrate that SPD can successfully decompose a toy induction-head model and recover the expected 2-step circuit. We also show that applying SPD to GPT-2-small can successfully locate subcomponents corresponding to interpretable concepts like "golf" and "basketball". These results take the first step in the direction of extending SPD to modern models, and show that we can use the method to surface interpretable parameter-space mechanisms.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Multi-Objective and Meta Reinforcement Learning via Gradient Estimation</title>
<link>https://arxiv.org/abs/2511.12779</link>
<guid>https://arxiv.org/abs/2511.12779</guid>
<content:encoded><![CDATA[
arXiv:2511.12779v2 Announce Type: replace 
Abstract: We study the problem of efficiently estimating policies that simultaneously optimize multiple objectives in reinforcement learning (RL). Given $n$ objectives (or tasks), we seek the optimal partition of these objectives into $k \ll n$ groups, where each group comprises related objectives that can be trained together. This problem arises in applications such as robotics, control, and preference optimization in language models, where learning a single policy for all $n$ objectives is suboptimal as $n$ grows. We introduce a two-stage procedure -- meta-training followed by fine-tuning -- to address this problem. We first learn a meta-policy for all objectives using multitask learning. Then, we adapt the meta-policy to multiple randomly sampled subsets of objectives. The adaptation step leverages a first-order approximation property of well-trained policy networks, which is empirically verified to be accurate within a 2% error margin across various RL environments. The resulting algorithm, PolicyGradEx, efficiently estimates an aggregate task-affinity score matrix given a policy evaluation algorithm. Based on the estimated affinity score matrix, we cluster the $n$ objectives into $k$ groups by maximizing the intra-cluster affinity scores. Experiments on three robotic control and the Meta-World benchmarks demonstrate that our approach outperforms state-of-the-art baselines by 16% on average, while delivering up to $26\times$ faster speedup relative to performing full training to obtain the clusters. Ablation studies validate each component of our approach. For instance, compared with random grouping and gradient-similarity-based grouping, our loss-based clustering yields an improvement of 19%. Finally, we analyze the generalization error of policy networks by measuring the Hessian trace of the loss surface, which gives non-vacuous measures relative to the observed generalization errors.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Batch Acquisition Function Evaluations and Decouple Optimizer Updates for Faster Bayesian Optimization</title>
<link>https://arxiv.org/abs/2511.13625</link>
<guid>https://arxiv.org/abs/2511.13625</guid>
<content:encoded><![CDATA[
arXiv:2511.13625v3 Announce Type: replace 
Abstract: Bayesian optimization (BO) efficiently finds high-performing parameters by maximizing an acquisition function, which models the promise of parameters. A major computational bottleneck arises in acquisition function optimization, where multi-start optimization (MSO) with quasi-Newton (QN) methods is required due to the non-convexity of the acquisition function. BoTorch, a widely used BO library, currently optimizes the summed acquisition function over multiple points, leading to the speedup of MSO owing to PyTorch batching. Nevertheless, this paper empirically demonstrates the suboptimality of this approach in terms of off-diagonal approximation errors in the inverse Hessian of a QN method, slowing down its convergence. To address this problem, we propose to decouple QN updates using a coroutine while batching the acquisition function calls. Our approach not only yields the theoretically identical convergence to the sequential MSO but also drastically reduces the wall-clock time compared to the previous approaches. Our approach is available in GPSampler in Optuna, effectively reducing its computational overhead.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tractable Probabilistic Models for Investment Planning</title>
<link>https://arxiv.org/abs/2511.13888</link>
<guid>https://arxiv.org/abs/2511.13888</guid>
<content:encoded><![CDATA[
arXiv:2511.13888v2 Announce Type: replace 
Abstract: Investment planning in power utilities, such as generation and transmission expansion, requires decade-long forecasts under profound uncertainty. Forecasting of energy mix and energy use decades ahead is nontrivial. Classical approaches focus on generating a finite number of scenarios (modeled as a mixture of Diracs in statistical theory terms), which limits insight into scenario-specific volatility and hinders robust decision-making. We propose an alternative using tractable probabilistic models (TPMs), particularly sum-product networks (SPNs). These models enable exact, scalable inference of key quantities such as scenario likelihoods, marginals, and conditional probabilities, supporting robust scenario expansion and risk assessment.
  This framework enables direct embedding of chance-constrained optimization into investment planning, enforcing safety or reliability with prescribed confidence levels. TPMs allow both scenario analysis and volatility quantification by compactly representing high-dimensional uncertainties. We demonstrate the effectiveness of the approach through a representative power system planning case study, illustrating its computational and reliability advantages over traditional scenario-based models.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GateRA: Token-Aware Modulation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2511.17582</link>
<guid>https://arxiv.org/abs/2511.17582</guid>
<content:encoded><![CDATA[
arXiv:2511.17582v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, DoRA, and HiRA, enable lightweight adaptation of large pre-trained models via low-rank updates. However, existing PEFT approaches apply static, input-agnostic updates to all tokens, disregarding the varying importance and difficulty of different inputs. This uniform treatment can lead to overfitting on trivial content or under-adaptation on more informative regions, especially in autoregressive settings with distinct prefill and decoding dynamics. In this paper, we propose GateRA, a unified framework that introduces token-aware modulation to dynamically adjust the strength of PEFT updates. By incorporating adaptive gating into standard PEFT branches, GateRA enables selective, token-level adaptation, preserving pre-trained knowledge for well-modeled inputs while focusing capacity on challenging cases. Empirical visualizations reveal phase-sensitive behaviors, where GateRA automatically suppresses updates for redundant prefill tokens while emphasizing adaptation during decoding. To promote confident and efficient modulation, we further introduce an entropy-based regularization that encourages near-binary gating decisions. This regularization prevents diffuse update patterns and leads to interpretable, sparse adaptation without hard thresholding. Finally, we present a theoretical analysis showing that GateRA induces a soft gradient-masking effect over the PEFT path, enabling continuous and differentiable control over adaptation. Experiments on multiple commonsense reasoning benchmarks demonstrate that GateRA consistently outperforms or matches prior PEFT methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Characterizing Knowledge Distillation of PPG Heart Rate Estimation Models</title>
<link>https://arxiv.org/abs/2511.18829</link>
<guid>https://arxiv.org/abs/2511.18829</guid>
<content:encoded><![CDATA[
arXiv:2511.18829v2 Announce Type: replace 
Abstract: Heart rate estimation from photoplethysmography (PPG) signals generated by wearable devices such as smartwatches and fitness trackers has significant implications for the health and well-being of individuals. Although prior work has demonstrated deep learning models with strong performance in the heart rate estimation task, in order to deploy these models on wearable devices, these models must also adhere to strict memory and latency constraints. In this work, we explore and characterize how large pre-trained PPG models may be distilled to smaller models appropriate for real-time inference on the edge. We evaluate four distillation strategies through comprehensive sweeps of teacher and student model capacities: (1) hard distillation, (2) soft distillation, (3) decoupled knowledge distillation (DKD), and (4) feature distillation. We present a characterization of the resulting scaling laws describing the relationship between model size and performance. This early investigation lays the groundwork for practical and predictable methods for building edge-deployable models for physiological sensing.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model</title>
<link>https://arxiv.org/abs/2511.20636</link>
<guid>https://arxiv.org/abs/2511.20636</guid>
<content:encoded><![CDATA[
arXiv:2511.20636v2 Announce Type: replace 
Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping the Verifier: Learning to Reason via Demonstrations</title>
<link>https://arxiv.org/abs/2511.21667</link>
<guid>https://arxiv.org/abs/2511.21667</guid>
<content:encoded><![CDATA[
arXiv:2511.21667v3 Announce Type: replace 
Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial game between a policy and a relativistic critic: the policy learns to mimic expert answers, while the critic aims to identify the experts among (expert, policy) answer pairs. Both the policy and the critic are trained jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL with verifiers. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.00831</link>
<guid>https://arxiv.org/abs/2512.00831</guid>
<content:encoded><![CDATA[
arXiv:2512.00831v2 Announce Type: replace 
Abstract: Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace</title>
<link>https://arxiv.org/abs/2512.01384</link>
<guid>https://arxiv.org/abs/2512.01384</guid>
<content:encoded><![CDATA[
arXiv:2512.01384v2 Announce Type: replace 
Abstract: We present CLAPS, a posterior-aware conformal regression method that pairs a Last-Layer Laplace Approximation with split-conformal calibration. From the resulting Gaussian posterior, CLAPS defines a simple two-sided posterior CDF score that aligns the conformity metric with the full predictive shape, not just a point estimate. This alignment yields narrower prediction intervals at the same target coverage, especially on small to medium tabular datasets where data are scarce and uncertainty modeling matters. We also provide a lightweight diagnostic suite that separates aleatoric and epistemic components and visualizes posterior behavior, helping practitioners understand why intervals shrink when they do. Across multiple benchmarks using the same MLP backbone, CLAPS consistently attains nominal coverage with improved efficiency and minimal overhead, offering a clear, practical upgrade to residual-based conformal baselines.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inverse Optimality for Fair Digital Twins: A Preference-based approach</title>
<link>https://arxiv.org/abs/2512.01650</link>
<guid>https://arxiv.org/abs/2512.01650</guid>
<content:encoded><![CDATA[
arXiv:2512.01650v2 Announce Type: replace 
Abstract: Digital Twins (DTs) are increasingly used as autonomous decision-makers in complex socio-technical systems. However, their mathematically optimal decisions often diverge from human expectations, revealing a persistent mismatch between algorithmic and bounded human rationality. This work addresses this challenge by proposing a framework that introduces fairness as a learnable objective within optimization-based Digital Twins. In this respect, a preference-driven learning workflow that infers latent fairness objectives directly from human pairwise preferences over feasible decisions is introduced. A dedicated Siamese neural network is developed to generate convex quadratic cost functions conditioned on contextual information. The resulting surrogate objectives drive the optimization procedure toward solutions that better reflect human-perceived fairness while maintaining computational efficiency. The effectiveness of the approach is demonstrated on a COVID-19 hospital resource allocation scenario. Overall, this work offers a practical solution to integrate human-centered fairness into the design of autonomous decision-making systems.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Mean-Field Dynamics of Transformers</title>
<link>https://arxiv.org/abs/2512.01868</link>
<guid>https://arxiv.org/abs/2512.01868</guid>
<content:encoded><![CDATA[
arXiv:2512.01868v2 Announce Type: replace 
Abstract: We develop a mathematical framework that interprets Transformer attention as an interacting particle system and studies its continuum (mean-field) limits. By idealizing attention on the sphere, we connect Transformer dynamics to Wasserstein gradient flows, synchronization models (Kuramoto), and mean-shift clustering. Central to our results is a global clustering phenomenon whereby tokens cluster asymptotically after long metastable states where they are arranged into multiple clusters. We further analyze a tractable equiangular reduction to obtain exact clustering rates, show how commonly used normalization schemes alter contraction speeds, and identify a phase transition for long-context attention. The results highlight both the mechanisms that drive representation collapse and the regimes that preserve expressive, multi-cluster structure in deep attention architectures.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved Ensemble-Based Machine Learning Model with Feature Optimization for Early Diabetes Prediction</title>
<link>https://arxiv.org/abs/2512.02023</link>
<guid>https://arxiv.org/abs/2512.02023</guid>
<content:encoded><![CDATA[
arXiv:2512.02023v2 Announce Type: replace 
Abstract: Diabetes is a serious worldwide health issue, and successful intervention depends on early detection. However, overlapping risk factors and data asymmetry make prediction difficult. To use extensive health survey data to create a machine learning framework for diabetes classification that is both accurate and comprehensible, to produce results that will aid in clinical decision-making. Using the BRFSS dataset, we assessed a number of supervised learning techniques. SMOTE and Tomek Links were used to correct class imbalance. To improve prediction performance, both individual models and ensemble techniques such as stacking were investigated. The 2015 BRFSS dataset, which includes roughly 253,680 records with 22 numerical features, is used in this study. Strong ROC-AUC performance of approximately 0.96 was attained by the individual models Random Forest, XGBoost, CatBoost, and LightGBM.The stacking ensemble with XGBoost and KNN yielded the best overall results with 94.82\% accuracy, ROC-AUC of 0.989, and PR-AUC of 0.991, indicating a favourable balance between recall and precision. In our study, we proposed and developed a React Native-based application with a Python Flask backend to support early diabetes prediction, providing users with an accessible and efficient health monitoring tool.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astral Space: Convex Analysis at Infinity</title>
<link>https://arxiv.org/abs/2205.03260</link>
<guid>https://arxiv.org/abs/2205.03260</guid>
<content:encoded><![CDATA[
arXiv:2205.03260v4 Announce Type: replace-cross 
Abstract: Not all convex functions on $\mathbb{R}^n$ have finite minimizers; some can only be minimized by a sequence as it heads to infinity. In this work, we aim to develop a theory for understanding such minimizers at infinity. We study astral space, a compact extension of $\mathbb{R}^n$ to which such points at infinity have been added. Astral space is constructed to be as small as possible while still ensuring that all linear functions can be continuously extended to the new space. Although astral space includes all of $\mathbb{R}^n$, it is not a vector space, nor even a metric space. However, it is sufficiently well-structured to allow useful and meaningful extensions of concepts of convexity, conjugacy, and subdifferentials. We develop these concepts and analyze various properties of convex functions on astral space, including the detailed structure of their minimizers, exact characterizations of continuity, and convergence of descent algorithms.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BeeTLe: An Imbalance-Aware Deep Sequence Model for Linear B-Cell Epitope Prediction and Classification with Logit-Adjusted Losses</title>
<link>https://arxiv.org/abs/2309.02071</link>
<guid>https://arxiv.org/abs/2309.02071</guid>
<content:encoded><![CDATA[
arXiv:2309.02071v2 Announce Type: replace-cross 
Abstract: The process of identifying and characterizing B-cell epitopes, which are the portions of antigens recognized by antibodies, is important for our understanding of the immune system, and for many applications including vaccine development, therapeutics, and diagnostics. Computational epitope prediction is challenging yet rewarding as it significantly reduces the time and cost of laboratory work. Most of the existing tools do not have satisfactory performance and only discriminate epitopes from non-epitopes. This paper presents a new deep learning-based multi-task framework for linear B-cell epitope prediction as well as antibody type-specific epitope classification. Specifically, a sequenced-based neural network model using recurrent layers and Transformer blocks is developed. We propose an amino acid encoding method based on eigen decomposition to help the model learn the representations of epitopes. We introduce modifications to standard cross-entropy loss functions by extending a logit adjustment technique to cope with the class imbalance. Experimental results on data curated from the largest public epitope database demonstrate the validity of the proposed methods and the superior performance compared to competing ones.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Models for Wireless Communications</title>
<link>https://arxiv.org/abs/2310.07312</link>
<guid>https://arxiv.org/abs/2310.07312</guid>
<content:encoded><![CDATA[
arXiv:2310.07312v4 Announce Type: replace-cross 
Abstract: A comprehensive study on the applications of denoising diffusion models for wireless systems is provided. The article highlights the capabilities of diffusion models in learning complicated signal distributions, modeling wireless channels, and denoising and reconstructing distorted signals. First, fundamental working mechanism of diffusion models is introduced. Then the recent advances in applying diffusion models to wireless systems are reviewed. Next, two case studies are provided, where conditional diffusion models (CDiff) are proposed for data reconstruction enhancement, covering both the conventional digital communication systems, as well as the semantic communication (SemCom) setups. The first case study highlights about 10 dB improvement in data reconstruction under low-SNR regimes, while mitigating the need to transmit redundant bits for error correction codes in digital systems. The second study further extends the case to a SemCom setup, where diffusion autoencoders showcase superior performance compared to legacy autoencoders and variational autoencoder (VAE) architectures. Finally, future directions and existing challenges are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep generative modelling of canonical ensemble with differentiable thermal properties</title>
<link>https://arxiv.org/abs/2404.18404</link>
<guid>https://arxiv.org/abs/2404.18404</guid>
<content:encoded><![CDATA[
arXiv:2404.18404v2 Announce Type: replace-cross 
Abstract: It is a long-standing challenge to accurately and efficiently compute thermodynamic quantities of many-body systems at thermal equilibrium. The conventional methods, e.g., Markov chain Monte Carlo, require many steps to equilibrate. The recently developed deep learning methods can perform direct sampling, but only work at a single trained temperature point and risk biased sampling. Here, we propose a variational method for canonical ensembles with differentiable temperature, which gives thermodynamic quantities as continuous functions of temperature akin to an analytical solution. The proposed method is a general framework that works with any tractable density generative model. At optimal, the model is theoretically guaranteed to be the unbiased Boltzmann distribution. We validated our method by calculating phase transitions in the Ising and XY models, demonstrating that our direct-sampling simulations are as accurate as Markov chain Monte Carlo, but more efficient. Moreover, our differentiable free energy aligns closely with the exact one to the second-order derivative, indicating that the variational model captures the subtle thermal transitions at the phase transitions. This functional dependence on external parameters is a fundamental advancement in combining the exceptional fitting ability of deep learning with rigorous physical analysis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning effective pruning at initialization from iterative pruning</title>
<link>https://arxiv.org/abs/2408.14757</link>
<guid>https://arxiv.org/abs/2408.14757</guid>
<content:encoded><![CDATA[
arXiv:2408.14757v2 Announce Type: replace-cross 
Abstract: Pruning at initialization (PaI) reduces training costs by removing weights before training, which becomes increasingly crucial with the growing network size. However, current PaI methods still have a large accuracy gap with iterative pruning, especially at high sparsity levels. This raises an intriguing question: can we get inspiration from iterative pruning to improve the PaI performance? In the lottery ticket hypothesis, the iterative rewind pruning (IRP) finds subnetworks retroactively by rewinding the parameter to the original initialization in every pruning iteration, which means all the subnetworks are based on the initial state. Here, we hypothesise the surviving subnetworks are more important and bridge the initial feature and their surviving score as the PaI criterion. We employ an end-to-end neural network (\textbf{AutoS}parse) to learn this correlation, input the model's initial features, output their score and then prune the lowest score parameters before training. To validate the accuracy and generalization of our method, we performed PaI across various models. Results show that our approach outperforms existing methods in high-sparsity settings. Notably, as the underlying logic of model pruning is consistent in different models, only one-time IRP on one model is needed (e.g., once IRP on ResNet-18/CIFAR-10, AutoS can be generalized to VGG-16/CIFAR-10, ResNet-18/TinyImageNet, et al.). As the first neural network-based PaI method, we conduct extensive experiments to validate the factors influencing this approach. These results reveal the learning tendencies of neural networks and provide new insights into our understanding and research of PaI from a practical perspective. Our code is available at: https://github.com/ChengYaofeng/AutoSparse.git.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Survey of Data-driven Newsvendor: Unified Analysis and Spectrum of Achievable Regrets</title>
<link>https://arxiv.org/abs/2409.03505</link>
<guid>https://arxiv.org/abs/2409.03505</guid>
<content:encoded><![CDATA[
arXiv:2409.03505v4 Announce Type: replace-cross 
Abstract: In the Newsvendor problem, the goal is to guess the number that will be drawn from some distribution, with asymmetric consequences for guessing too high vs. too low. In the data-driven version, the distribution is unknown, and one must work with samples from the distribution. Data-driven Newsvendor has been studied under many variants: additive vs. multiplicative regret, high probability vs. expectation bounds, and different distribution classes. This paper studies all combinations of these variants, filling in many gaps in the literature and simplifying many proofs. In particular, we provide a unified analysis based on the notion of clustered distributions, which in conjunction with our new lower bounds, shows that the entire spectrum of regrets between $1/\sqrt{n}$ and $1/n$ can be possible. Simulations on commonly-used distributions demonstrate that our notion is the "correct" predictor of empirical regret across varying data sizes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Brain Age Estimation with a Multimodal 3D CNN Approach Combining Structural MRI and AI-Synthesized Cerebral Blood Volume Measures</title>
<link>https://arxiv.org/abs/2412.01865</link>
<guid>https://arxiv.org/abs/2412.01865</guid>
<content:encoded><![CDATA[
arXiv:2412.01865v4 Announce Type: replace-cross 
Abstract: Brain age gap estimation (BrainAGE) is a promising imaging-derived biomarker of neurobiological aging and disease risk, yet current approaches rely predominantly on T1-weighted structural MRI (T1w), overlooking functional vascular changes that may precede tissue damage and cognitive decline. Artificial intelligence-generated cerebral blood volume (AICBV) maps, synthesized from non-contrast MRI, offer an alternative to contrast-enhanced perfusion imaging by capturing vascular information relevant to early neurodegeneration. We developed a multimodal BrainAGE framework that integrates brain age predictions using linear regression from two separate 3D VGG-based networks, one model trained on only structural T1w scans and one trained on only AICBV maps generated from a pre-trained 3D patch-based deep learning model. Each model was trained and validated on 2,851 scans from 13 open-source datasets and was evaluated for concordance with mild cognitive impairment (MCI) and Alzheimer's disease (AD) using ADNI subjects (n=1,233). The combined model achieved the most accurate brain age gap for cognitively normal (CN) controls, with a mean absolute error (MAE) of 3.95 years ($R^2$=0.943), outperforming models trained on T1w (MAE=4.10) or AICBV alone (MAE=4.49). Saliency maps revealed complementary modality contributions: T1w emphasized white matter and cortical atrophy, while AICBV highlighted vascular-rich and periventricular regions implicated in hypoperfusion and early cerebrovascular dysfunction, consistent with normal aging. Next, we observed that BrainAGE increased stepwise across diagnostic strata (CN < MCI < AD) and correlated with cognitive impairment (CDRSB r=0.403; MMSE r=-0.310). AICBV-based BrainAGE showed particularly strong separation between stable vs. progressive MCI (p=$1.47 \times 10^{-8}$), suggesting sensitivity to prodromal vascular changes that precede overt atrophy.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title>
<link>https://arxiv.org/abs/2501.06039</link>
<guid>https://arxiv.org/abs/2501.06039</guid>
<content:encoded><![CDATA[
arXiv:2501.06039v2 Announce Type: replace-cross 
Abstract: Spatial proteomics technologies have transformed our understanding of complex tissue architecture in cancer but present unique challenges for computational analysis. Each study uses a different marker panel and protocol, and most methods are tailored to single cohorts, which limits knowledge transfer and robust biomarker discovery. Here we present Virtual Tissues (VirTues), a general-purpose foundation model for spatial proteomics that learns marker-aware, multi-scale representations of proteins, cells, niches and tissues directly from multiplex imaging data. From a single pretrained backbone, VirTues supports marker reconstruction, cell typing and niche annotation, spatial biomarker discovery, and patient stratification, including zero-shot annotation across heterogeneous panels and datasets. In triple-negative breast cancer, VirTues-derived biomarkers predict anti-PD-L1 chemo-immunotherapy response and stratify disease-free survival in an independent cohort, outperforming state-of-the-art biomarkers derived from the same datasets and current clinical stratification schemes.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gold-medalist Performance in Solving Olympiad Geometry with AlphaGeometry2</title>
<link>https://arxiv.org/abs/2502.03544</link>
<guid>https://arxiv.org/abs/2502.03544</guid>
<content:encoded><![CDATA[
arXiv:2502.03544v3 Announce Type: replace-cross 
Abstract: We present AlphaGeometry2 (AG2), a significantly improved version of AlphaGeometry introduced in (Trinh et al., 2024), which has now surpassed an average gold medalist in solving Olympiad geometry problems. To achieve this, we first extend the original AlphaGeometry language to tackle problems involving movements of objects, and problems containing linear equations of angles, ratios, and distances. This, together with support for non-constructive problems, has markedly improved the coverage rate of the AlphaGeometry language on International Math Olympiads (IMO) 2000-2024 geometry problems from 66% to 88%. The search process of AG2 has also been greatly improved through the use of Gemini architecture for better language modeling, and a novel knowledge-sharing mechanism that enables effective communication between search trees. Together with further enhancements to the symbolic engine and synthetic data generation, we have significantly boosted the overall solving rate of AG to 84% on all geometry problems over the last 25 years, compared to 54% previously. AG2 was also part of the system that achieved the silver-medal standard at IMO 2024 https://deepmind.google/blog/ai-solves-imo-problems-at-silver-medal-level/. Finally, we report progress towards using AG2 as a part of a fully automated system that reliably solves geometry problems from natural language input. Code: https://github.com/google-deepmind/alphageometry2.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reconstruction of frequency-localized functions from pointwise samples via least squares and deep learning</title>
<link>https://arxiv.org/abs/2502.09794</link>
<guid>https://arxiv.org/abs/2502.09794</guid>
<content:encoded><![CDATA[
arXiv:2502.09794v2 Announce Type: replace-cross 
Abstract: Recovering frequency-localized functions from pointwise data is a fundamental task in signal processing. We examine this problem from an approximation-theoretic perspective, focusing on least squares and deep learning-based methods. First, we establish a novel recovery theorem for least squares approximations using the Slepian basis from uniform random samples in low dimensions, explicitly tracking the dependence of the bandwidth on the sampling complexity. Building on these results, we then present a recovery guarantee for approximating bandlimited functions via deep learning from pointwise data. This result, framed as a practical existence theorem, provides conditions on the network architecture, training procedure, and data acquisition sufficient for accurate approximation. To complement our theoretical findings, we perform numerical comparisons between least squares and deep learning for approximating one- and two-dimensional functions. We conclude with a discussion of the theoretical limitations and the practical gaps between theory and implementation.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dark Deceptions in DHCP: Dismantling Network Defenses</title>
<link>https://arxiv.org/abs/2502.10646</link>
<guid>https://arxiv.org/abs/2502.10646</guid>
<content:encoded><![CDATA[
arXiv:2502.10646v3 Announce Type: replace-cross 
Abstract: This paper explores vulnerabilities in the Dynamic Host Configuration Protocol (DHCP) and their implications on the Confidentiality, Integrity, and Availability (CIA) Triad. Through an analysis of various attacks, including DHCP Starvation, Rogue DHCP Servers, Replay Attacks, and TunnelVision exploits, the paper provides a taxonomic classification of threats, assesses risks, and proposes appropriate controls. The discussion also highlights the dangers of VPN decloaking through DHCP exploits and underscores the importance of safeguarding network infrastructures. By bringing awareness to the TunnelVision exploit, this paper aims to mitigate risks associated with these prevalent vulnerabilities.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Median Consensus Embedding for Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2503.08103</link>
<guid>https://arxiv.org/abs/2503.08103</guid>
<content:encoded><![CDATA[
arXiv:2503.08103v2 Announce Type: replace-cross 
Abstract: This study proposes median consensus embedding (MCE) to address variability in low-dimensional embeddings caused by random initialization in nonlinear dimensionality reduction techniques such as $t$-distributed stochastic neighbor embedding. MCE is defined as the geometric median of multiple embeddings. By assuming multiple embeddings as independent and identically distributed random samples and applying large deviation theory, we prove that MCE achieves consistency at an exponential rate. Furthermore, we develop a practical algorithm to implement MCE by constructing a distance function between embeddings based on the Frobenius norm of the pairwise distance matrix of data points. Application to actual data demonstrates that MCE converges rapidly and effectively reduces instability. We further combine MCE with multiple imputation to address missing values and consider multiscale hyperparameters. Results confirm that MCE effectively mitigates instability issues in embedding methods arising from random initialization and other sources.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>($\boldsymbol{\theta}_l, \boldsymbol{\theta}_u$)-Parametric Multi-Task Optimization: Joint Search in Solution and Infinite Task Spaces</title>
<link>https://arxiv.org/abs/2503.08394</link>
<guid>https://arxiv.org/abs/2503.08394</guid>
<content:encoded><![CDATA[
arXiv:2503.08394v4 Announce Type: replace-cross 
Abstract: Multi-task optimization is typically characterized by a fixed and finite set of tasks. The present paper relaxes this condition by considering a non-fixed and potentially infinite set of optimization tasks defined in a parameterized, continuous and bounded task space. We refer to this unique problem setting as parametric multi-task optimization (PMTO). Assuming the bounds of the task parameters to be ($\boldsymbol{\theta}_l$, $\boldsymbol{\theta}_u$), a novel ($\boldsymbol{\theta}_l$, $\boldsymbol{\theta}_u$)-PMTO algorithm is crafted to operate in two complementary modes. In an offline optimization mode, a joint search over solution and task spaces is carried out with the creation of two approximation models: (1) for mapping points in a unified solution space to the objective spaces of all tasks, which provably accelerates convergence by acting as a conduit for inter-task knowledge transfers, and (2) for probabilistically mapping tasks to their corresponding solutions, which facilitates evolutionary exploration of under-explored regions of the task space. In the online mode, the derived models enable direct optimization of any task within the bounds without the need to search from scratch. This outcome is validated on both synthetic test problems and practical case studies, with the significant real-world applicability of PMTO shown towards fast reconfiguration of robot controllers under changing task conditions. The potential of PMTO to vastly speedup the search for solutions to minimax optimization problems is also demonstrated through an example in robust engineering design.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion</title>
<link>https://arxiv.org/abs/2504.08937</link>
<guid>https://arxiv.org/abs/2504.08937</guid>
<content:encoded><![CDATA[
arXiv:2504.08937v4 Announce Type: replace-cross 
Abstract: In image fusion tasks, the absence of real fused images as priors forces most deep learning approaches to rely on large-scale paired datasets to extract global weighting features or to generate pseudo-supervised images through algorithmic constructions. Unlike previous methods, this work re-examines prior-guided learning under few-shot conditions by introducing rough set theory. We regard the traditional algorithm as a prior generator, while the network re-inferrs and adaptively optimizes the prior through a dynamic loss function, reducing the inference burden of the network and enabling effective few-shot learning.To provide the prior, we propose the Granular Ball Pixel Computation (GBPC) algorithm. GBPC models pixel pairs in a luminance subspace using meta-granular balls and mines intra-ball information at multiple granular levels. At the fine-grained level, sliding granular balls assign adaptive weights to individual pixels to produce pixel-level prior fusion. At the coarse-grained level, the algorithm performs split computation within a single image to estimate positive and boundary domain distributions, enabling modality awareness and prior confidence estimation, which dynamically guide the loss weighting.The network and the algorithmic prior are coupled through the loss function to form an integrated framework. Thanks to the dynamic weighting mechanism, the network can adaptively adjust to different priors during training, enhancing its perception and fusion capability across modalities. We name this framework GBFF (Granular Ball Fusion Framework). Experiments on four fusion tasks demonstrate that even with only ten training image pairs per task, GBFF achieves superior performance in both visual quality and model compactness. Code is available at: https://github.com/DMinjie/GBFF
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems</title>
<link>https://arxiv.org/abs/2504.10507</link>
<guid>https://arxiv.org/abs/2504.10507</guid>
<content:encoded><![CDATA[
arXiv:2504.10507v4 Announce Type: replace-cross 
Abstract: Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems. This paper introduces PinRec, a novel generative retrieval model developed for applications at Pinterest. PinRec utilizes outcome-conditioned generation, enabling modelers to specify how to balance various outcome metrics, such as the number of saves and clicks, to effectively align with business goals and user exploration. Additionally, PinRec incorporates multi-token generation to enhance output diversity while optimizing generation. Our experiments demonstrate that PinRec can successfully balance performance, diversity, and efficiency, delivering a significant positive impact to users using generative models. This paper marks a significant milestone in generative retrieval, as it presents, to our knowledge, the first rigorous study on implementing generative retrieval at the scale of Pinterest.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Task-Oriented Flying: Framework, Infrastructure, and Principles</title>
<link>https://arxiv.org/abs/2504.15129</link>
<guid>https://arxiv.org/abs/2504.15129</guid>
<content:encoded><![CDATA[
arXiv:2504.15129v2 Announce Type: replace-cross 
Abstract: Deploying robot learning methods to aerial robots in unstructured environments remains both challenging and promising. While recent advances in deep reinforcement learning (DRL) have enabled end-to-end flight control, the field still lacks systematic design guidelines and a unified infrastructure to support reproducible training and real-world deployment. We present a task-oriented framework for end-to-end DRL in quadrotors that integrates design principles for complex task specification and reveals the interdependencies among simulated task definition, training design principles, and physical deployment. Our framework involves software infrastructure, hardware platforms, and open-source firmware to support a full-stack learning infrastructure and workflow. Extensive empirical results demonstrate robust flight and sim-to-real generalization under real-world disturbances. By reducing the entry barrier for deploying learning-based controllers on aerial robots, our work lays a practical foundation for advancing autonomous flight in dynamic and unstructured environments.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Point in Vision Transformers for Universal Image Segmentation</title>
<link>https://arxiv.org/abs/2505.19795</link>
<guid>https://arxiv.org/abs/2505.19795</guid>
<content:encoded><![CDATA[
arXiv:2505.19795v2 Announce Type: replace-cross 
Abstract: Image segmentation remains a challenging task in computer vision, demanding robust mask generation and precise classification. Recent mask-based approaches yield high-quality masks by capturing global context. However, accurately classifying these masks, especially in the presence of ambiguous boundaries and imbalanced class distributions, remains an open challenge. In this work, we introduce ViT-P, a novel two-stage segmentation framework that decouples mask generation from classification. The first stage employs a proposal generator to produce class-agnostic mask proposals, while the second stage utilizes a point-based classification model built on the Vision Transformer (ViT) to refine predictions by focusing on mask central points. ViT-P serves as a pre-training-free adapter, allowing the integration of various pre-trained vision transformers without modifying their architecture, ensuring adaptability to dense prediction tasks. Furthermore, we demonstrate that coarse and bounding box annotations can effectively enhance classification without requiring additional training on fine annotation datasets, reducing annotation costs while maintaining strong performance. Extensive experiments across COCO, ADE20K, and Cityscapes datasets validate the effectiveness of ViT-P, achieving state-of-the-art results with 54.0 PQ on ADE20K panoptic segmentation, 87.4 mIoU on Cityscapes semantic segmentation, and 63.6 mIoU on ADE20K semantic segmentation. The code and pretrained models are available at: https://github.com/sajjad-sh33/ViT-P}{https://github.com/sajjad-sh33/ViT-P.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMNIGUARD: An Efficient Approach for AI Safety Moderation Across Languages and Modalities</title>
<link>https://arxiv.org/abs/2505.23856</link>
<guid>https://arxiv.org/abs/2505.23856</guid>
<content:encoded><![CDATA[
arXiv:2505.23856v2 Announce Type: replace-cross 
Abstract: The emerging capabilities of large language models (LLMs) have sparked concerns about their immediate potential for harmful misuse. The core approach to mitigate these concerns is the detection of harmful queries to the model. Current detection approaches are fallible, and are particularly susceptible to attacks that exploit mismatched generalization of model capabilities (e.g., prompts in low-resource languages or prompts provided in non-text modalities such as image and audio). To tackle this challenge, we propose Omniguard, an approach for detecting harmful prompts across languages and modalities. Our approach (i) identifies internal representations of an LLM/MLLM that are aligned across languages or modalities and then (ii) uses them to build a language-agnostic or modality-agnostic classifier for detecting harmful prompts. Omniguard improves harmful prompt classification accuracy by 11.57\% over the strongest baseline in a multilingual setting, by 20.44\% for image-based prompts, and sets a new SOTA for audio-based prompts. By repurposing embeddings computed during generation, Omniguard is also very efficient ($\approx\!120 \times$ faster than the next fastest baseline). Code and data are available at: https://github.com/vsahil/OmniGuard.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hebbian Physics Networks: A Self-Organizing Computational Architecture Based on Local Physical Laws</title>
<link>https://arxiv.org/abs/2507.00641</link>
<guid>https://arxiv.org/abs/2507.00641</guid>
<content:encoded><![CDATA[
arXiv:2507.00641v2 Announce Type: replace-cross 
Abstract: Physical transport processes organize through local interactions that redistribute imbalance while preserving conservation. Classical solvers enforce this organization by applying fixed discrete operators on rigid grids. We introduce the Hebbian Physics Network (HPN), a computational framework that replaces this rigid scaffolding with a plastic transport geometry. An HPN is a coupled dynamical system of physical states on nodes and constitutive weights on edges in a graph. Residuals--local violations of continuity, momentum balance, or energy conservation--act as thermodynamic forces that drive the joint evolution of both the state and the operator (i.e. the adaptive weights). The weights adapt through a three-factor Hebbian rule, which we prove constitutes a strictly local gradient descent on the residual energy. This mechanism ensures thermodynamic stability: near equilibrium, the learned operator naturally converges to a symmetric, positive-definite form, rigorously reproducing Onsager\'s reciprocal relations without explicit enforcement. Far from equilibrium, the system undergoes a self-organizing search for a transport topology that restores global coercivity. Unlike optimization-based approaches that impose physics through global loss functions, HPNs embed conservation intrinsically: transport is restored locally by the evolving operator itself, without a global Poisson solve or backpropagated objective. We demonstrate the framework on scalar diffusion and incompressible lid-driven cavity flow, showing that physically consistent transport geometries and flow structures emerge from random initial conditions solely through residual-driven local adaptation. HPNs thus reframe computation not as the solution of a fixed equation, but as a thermodynamic relaxation process where the constitutive geometry and physical state co-evolve.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amorphous Solid Model of Vectorial Hopfield Neural Networks</title>
<link>https://arxiv.org/abs/2507.22787</link>
<guid>https://arxiv.org/abs/2507.22787</guid>
<content:encoded><![CDATA[
arXiv:2507.22787v5 Announce Type: replace-cross 
Abstract: We introduce a three-dimensional vectorial extension of the Hopfield associative-memory model in which each neuron is a unit vector on $S^2$ and synaptic couplings are $3\times 3$ blocks generated through a vectorial Hebbian rule. The resulting block-structured operator is mathematically analogous to the Hessian of amorphous solids and induces a rigid energy landscape with deep minima for stored patterns. Simulations and spectral analysis show that the vectorial network substantially outperforms the classical binary Hopfield model. For moderate connectivity, the critical storage ratio $\gamma_c$ grows approximately linearly with the coordination number $Z$, while for $Z\gtrsim 40$ a high-connectivity regime emerges in which $\gamma_c$ systematically exceeds the extrapolated low-$Z$ linear fit. At the same time, a persistent spectral gap separates pattern modes from the bulk and basins of attraction enlarge, yielding enhanced robustness to initialization noise. Thus geometric constraints combined with amorphous-solid-inspired structure produce associative memories with superior storage and retrieval performance, especially in the high-connectivity ($Z \gtrsim 20$-$30$) regime.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Construction of Artificial Lattice Structures with Designer Electronic States</title>
<link>https://arxiv.org/abs/2508.02581</link>
<guid>https://arxiv.org/abs/2508.02581</guid>
<content:encoded><![CDATA[
arXiv:2508.02581v2 Announce Type: replace-cross 
Abstract: Manipulating matter with a scanning tunneling microscope (STM) enables creation of atomically defined artificial structures that host designer quantum states. However, the time-consuming nature of the manipulation process, coupled with the sensitivity of the STM tip, constrains the exploration of diverse configurations and limits the size of designed features. In this study, we present a reinforcement learning (RL)-based framework for creating artificial structures by spatially manipulating carbon monoxide (CO) molecules on a copper substrate using the STM tip. The automated workflow combines molecule detection and manipulation, employing deep learning-based object detection to locate CO molecules and linear assignment algorithms to allocate these molecules to designated target sites. We initially perform molecule maneuvering based on randomized parameter sampling for sample bias, tunneling current setpoint and manipulation speed. This dataset is then structured into an action trajectory used to train an RL agent. The model is subsequently deployed on the STM for real-time fine-tuning of manipulation parameters during structure construction. Our approach incorporates path planning protocols coupled with active drift compensation to enable atomically precise fabrication of structures with significantly reduced human input while realizing larger-scale artificial lattices with desired electronic properties. Using our approach, we demonstrate the automated construction of an extended artificial graphene lattice and confirm the existence of characteristic Dirac point in its electronic structure. Further challenges to RL-based structural assembly scalability are discussed.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Approximation for Two-Timescale Linear Stochastic Approximation</title>
<link>https://arxiv.org/abs/2508.07928</link>
<guid>https://arxiv.org/abs/2508.07928</guid>
<content:encoded><![CDATA[
arXiv:2508.07928v2 Announce Type: replace-cross 
Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal approximation for linear two-timescale stochastic approximation (TTSA) algorithms driven by martingale difference or Markov noise. Focusing on both the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for normal approximation in terms of the convex distance between probability distributions. Our analysis reveals a non-trivial interaction between the fast and slow timescales: the normal approximation rate for the last iterate improves as the timescale separation increases, while it decreases in the Polyak-Ruppert averaged setting. We also provide the high-order moment bounds for the error of linear TTSA algorithm, which may be of independent interest.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DASH: A Meta-Attack Framework for Synthesizing Effective and Stealthy Adversarial Examples</title>
<link>https://arxiv.org/abs/2508.13309</link>
<guid>https://arxiv.org/abs/2508.13309</guid>
<content:encoded><![CDATA[
arXiv:2508.13309v2 Announce Type: replace-cross 
Abstract: Numerous techniques have been proposed for generating adversarial examples in white-box settings under strict Lp-norm constraints. However, such norm-bounded examples often fail to align well with human perception, and only recently have a few methods begun specifically exploring perceptually aligned adversarial examples. Moreover, it remains unclear whether insights from Lp-constrained attacks can be effectively leveraged to improve perceptual efficacy. In this paper, we introduce DAASH, a fully differentiable meta-attack framework that generates effective and perceptually aligned adversarial examples by strategically composing existing Lp-based attack methods. DAASH operates in a multi-stage fashion: at each stage, it aggregates candidate adversarial examples from multiple base attacks using learned, adaptive weights and propagates the result to the next stage. A novel meta-loss function guides this process by jointly minimizing misclassification loss and perceptual distortion, enabling the framework to dynamically modulate the contribution of each base attack throughout the stages. We evaluate DAASH on adversarially trained models across CIFAR-10, CIFAR-100, and ImageNet. Despite relying solely on Lp-constrained based methods, DAASH significantly outperforms state-of-the-art perceptual attacks such as AdvAD -- achieving higher attack success rates (e.g., 20.63\% improvement) and superior visual quality, as measured by SSIM, LPIPS, and FID (improvements $\approx$ of 11, 0.015, and 5.7, respectively). Furthermore, DAASH generalizes well to unseen defenses, making it a practical and strong baseline for evaluating robustness without requiring handcrafted adaptive attacks for each new defense.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v3 Announce Type: replace-cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining can produce reliably accurate segmentation from in-distribution (ID) data but degrade when applied to out-of-distribution (OOD) datasets. We address this challenge with RF-Deep, a random forest classifier that utilizes deep features from a pretrained transformer encoder of the segmentation model to detect OOD scans and enhance segmentation reliability. The segmentation model comprises a Swin Transformer encoder, pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous and non-cancerous conditions, with a convolution decoder, trained to segment lung cancers in 317 3D scans. Independent testing was performed on 603 3D CT public datasets that included one ID dataset and four OOD datasets comprising chest CTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidney cancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of 18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs, consistently outperforming established OOD approaches. The RF-Deep classifier provides a simple and effective approach to enhance reliability of cancer segmentation in ID and OOD scenarios.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Representation of Generalized Convex Functions and their Gradients</title>
<link>https://arxiv.org/abs/2509.04477</link>
<guid>https://arxiv.org/abs/2509.04477</guid>
<content:encoded><![CDATA[
arXiv:2509.04477v2 Announce Type: replace-cross 
Abstract: A wide range of optimization problems can often be written in terms of generalized convex functions (GCFs). When this structure is present, it can convert certain nested bilevel objectives into single-level problems amenable to standard first-order optimization methods. We provide a new differentiable layer with a convex parameter space and show (Theorems 5.1 and 5.2) that it and its gradient are universal approximators for GCFs and their gradients. We demonstrate how this parameterization can be leveraged in practice by (i) learning optimal transport maps with general cost functions and (ii) learning optimal auctions of multiple goods. In both these cases, we show how our layer can be used to convert the existing bilevel or min-max formulations into single-level problems that can be solved efficiently with first-order methods.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Natural Language Descriptions of Model Activations Convey Privileged Information?</title>
<link>https://arxiv.org/abs/2509.13316</link>
<guid>https://arxiv.org/abs/2509.13316</guid>
<content:encoded><![CDATA[
arXiv:2509.13316v3 Announce Type: replace-cross 
Abstract: Recent interpretability methods have proposed to translate LLM internal representations into natural language descriptions using a second verbalizer LLM. This is intended to illuminate how the target model represents and operates on inputs. But do such activation verbalization approaches actually provide privileged knowledge about the internal workings of the target model, or do they merely convey information about its inputs? We critically evaluate popular verbalization methods across datasets used in prior work and find that they can succeed at benchmarks without any access to target model internals, suggesting that these datasets may not be ideal for evaluating verbalization methods. We then run controlled experiments which reveal that verbalizations often reflect the parametric knowledge of the verbalizer LLM which generated them, rather than the knowledge of the target LLM whose activations are decoded. Taken together, our results indicate a need for targeted benchmarks and experimental controls to rigorously assess whether verbalization methods provide meaningful insights into the operations of LLMs.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</title>
<link>https://arxiv.org/abs/2509.25531</link>
<guid>https://arxiv.org/abs/2509.25531</guid>
<content:encoded><![CDATA[
arXiv:2509.25531v2 Announce Type: replace-cross 
Abstract: We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong model performance. MixtureVitae follows a risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources), alongside targeted instruction, reasoning and synthetic data with documented provenance. We detail a transparent, multi-stage pipeline for license-aware filtering, safety and quality screening, and domain-aware mixing, and we release the dataset and curation recipes to support reproducible research. In controlled experiments using the open-sci-ref training protocol (fixed architectures at 130M/400M/1.3B/1.7B parameters; training budgets of 50B and 300B tokens), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B/300B setting they surpass FineWeb-Edu and approach DCLM in the later stages of training. Performance is particularly strong on math/code and competitive on QA tasks. These results demonstrate that permissive-first, risk-mitigated data provides a practical and legally mitigated foundation for training capable LLMs, reducing reliance on indiscriminate web scraping without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AraLingBench A Human-Annotated Benchmark for Evaluating Arabic Linguistic Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2511.14295</link>
<guid>https://arxiv.org/abs/2511.14295</guid>
<content:encoded><![CDATA[
arXiv:2511.14295v2 Announce Type: replace-cross 
Abstract: We present AraLingBench: a fully human annotated benchmark for evaluating the Arabic linguistic competence of large language models (LLMs). The benchmark spans five core categories: grammar, morphology, spelling, reading comprehension, and syntax, through 150 expert-designed multiple choice questions that directly assess structural language understanding. Evaluating 35 Arabic and bilingual LLMs reveals that current models demonstrate strong surface level proficiency but struggle with deeper grammatical and syntactic reasoning. AraLingBench highlights a persistent gap between high scores on knowledge-based benchmarks and true linguistic mastery, showing that many models succeed through memorization or pattern recognition rather than authentic comprehension. By isolating and measuring fundamental linguistic skills, AraLingBench provides a diagnostic framework for developing Arabic LLMs. The full evaluation code is publicly available on GitHub.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mortgage Language Model: Domain-Adaptive Pretraining with Residual Instruction, Alignment Tuning, and Task-Specific Routing</title>
<link>https://arxiv.org/abs/2511.21101</link>
<guid>https://arxiv.org/abs/2511.21101</guid>
<content:encoded><![CDATA[
arXiv:2511.21101v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate exceptional capabilities across general domains, yet their application to specialized sectors such as mortgage finance requires domain-specific knowledge augmentation while preserving instruction-following fidelity. We present MortgageLLM, a novel domain-specific large language model that addresses this dual challenge. It is developed using a dual-track specialization framework from a single base model (LLaMA-3.1-8B). We opted for this dual-expert approach as a single multi-task model suffers from performance trade-offs, where optimizing for structured tasks (via SFT) degrades conversational fidelity (via DPO). Our dual-track method solves this by creating two specialists, allowing each to be optimally trained for its distinct capability. Our approach applies the instruction residual technique to restore instruction-following capabilities post-domain adaptation without supervised fine-tuning. We contribute: (1) application of this residual technique to the highly specialized mortgage finance domain; (2) a dual-expert architecture combining a conversational Q&amp;A model and a structured task model for classification and summarization; and (3) an intelligent task routing mechanism using few-shot classification performed by one of the expert models itself. We validate our approach on domain-specific benchmarks, where our final model (MLM v2) significantly outperforms the base LLaMA-3.1-8B-Instruct, achieving an LLM-as-a-Judge summarization score of 4.58 (vs. 3.99), a Q&amp;A score of 4.09 (vs. 4.0), and a classification score of 2.6 (vs. 1.2). On semantic similarity, our model achieved a BERTScore of 0.77 for summarization (vs. 0.74), 0.68 for Q&amp;A (vs. 0.58), and 0.75 for classification (vs. 0.73), substantially outperforming baseline approaches.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Necessity of Imperfection:Reversing Model Collapse via Simulating Cognitive Boundedness</title>
<link>https://arxiv.org/abs/2512.01354</link>
<guid>https://arxiv.org/abs/2512.01354</guid>
<content:encoded><![CDATA[
arXiv:2512.01354v3 Announce Type: replace-cross 
Abstract: Although synthetic data is widely promoted as a remedy, its prevailing production paradigm -- one optimizing for statistical smoothness -- systematically removes the long-tail, cognitively grounded irregularities that characterize human text. Prolonged training on such statistically optimal but cognitively impoverished data accelerates model collapse.
  This paper proposes a paradigm shift: instead of imitating the surface properties of data, we simulate the cognitive processes that generate human text. We introduce the Prompt-driven Cognitive Computing Framework (PMCSF), whose core consists of a Cognitive State Decoder (CSD) that reverse-engineers unstructured text into structured cognitive vectors, and a Cognitive Text Encoder (CTE) that re-materializes these states into text enriched with human-typical imperfections via mathematically defined Cognitive Perturbation Operators.
  The framework is validated through a two-stage objective evaluation pipeline. First, in cognitive codec verification, CTE text yields a Jensen-Shannon divergence of 0.0614 from human text (vs. 0.4431 for standard LLM output), passes double-blind professional media review, and achieves an intraclass correlation coefficient ICC > 0.9 for cognitive profile alignment across heterogeneous models. Second, in functional gain evaluation, isomorphic stress tests in the A-share market show that strategies incorporating CTE-generated data reduce maximum drawdown by 47.4% during the 2015 crash and deliver 8.6% Defensive Alpha, exceeding transaction costs by a factor of 33.
  Our findings demonstrate that modelling human cognitive limitations -- not copying surface data -- enables synthetic data with genuine functional gain, offering a viable technical pathway toward resolving the AI data-collapse crisis.
]]></content:encoded>
<pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-based Time Series Editing</title>
<link>https://arxiv.org/abs/2508.01504</link>
<guid>https://arxiv.org/abs/2508.01504</guid>
<content:encoded><![CDATA[
<div> Time series editing, diffusion models, natural language instructions, multi-modal representation, controllable editing strength  

<br /><br />Summary:  
This paper addresses the problem of time series editing, where specific properties of a time series are modified without affecting others. Traditional diffusion-based editors rely on fixed attribute vectors and generate binary, all-or-nothing edits, restricting flexibility and nuanced control over editing strength. To advance this field, the authors propose Instruction-based Time Series Editing, enabling users to provide editing commands in natural language, thus broadening the scope and accessibility of edits. They introduce InstructTime, the first editor following this paradigm, which encodes both the input time series and the user instructions into a shared multi-modal embedding space before decoding to produce the edited series. This structured multi-modal space permits smooth interpolation between embeddings, allowing fine-tuning of edit intensity. The model incorporates multi-resolution encoders to manage both local and global time series edits effectively. Experiments conducted on synthetic and real datasets demonstrate that InstructTime outperforms existing methods by delivering high-quality, controllable edits. It generalizes well to previously unseen instructions and adapts to new editing conditions via few-shot learning, showcasing its flexibility and practical utility in time series analysis scenarios. <div>
arXiv:2508.01504v3 Announce Type: replace 
Abstract: In time series editing, we aim to modify some properties of a given time series without altering others. For example, when analyzing a hospital patient's blood pressure, we may add a sudden early drop and observe how it impacts their future while preserving other conditions. Existing diffusion-based editors rely on rigid, predefined attribute vectors as conditions and produce all-or-nothing edits through sampling. This attribute- and sampling-based approach limits flexibility in condition format and lacks customizable control over editing strength. To overcome these limitations, we introduce Instruction-based Time Series Editing, where users specify intended edits using natural language. This allows users to express a wider range of edits in a more accessible format. We then introduce InstructTime, the first instruction-based time series editor. InstructTime takes in time series and instructions, embeds them into a shared multi-modal representation space, then decodes their embeddings to generate edited time series. By learning a structured multi-modal representation space, we can easily interpolate between embeddings to achieve varying degrees of edit. To handle local and global edits together, we propose multi-resolution encoders. In our experiments, we use synthetic and real datasets and find that InstructTime is a state-of-the-art time series editor: InstructTime achieves high-quality edits with controllable strength, can generalize to unseen instructions, and can be easily adapted to unseen conditions through few-shot learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A self-driving lab for solution-processed electrochromic thin films</title>
<link>https://arxiv.org/abs/2512.05989</link>
<guid>https://arxiv.org/abs/2512.05989</guid>
<content:encoded><![CDATA[
<div> Keywords: electrochromic materials, solution-processed, self-driving laboratories, Bayesian optimization, process optimization<br /><br />Summary:<br /><br />This study explores the advancement of solution-processed electrochromic materials, which are crucial for developing energy-efficient smart windows and displays. The performance of these materials is highly dependent on their composition and the conditions under which they are processed. Electrochromic thin film electrodes require coatings that are smooth and free of defects to achieve optimal contrast between their bleached and colored states. Traditional optimization of spin-coated electrochromic thin layers is complex and time-consuming, presenting challenges in rapid development. To address this, the researchers implement self-driving laboratories that integrate automation with machine learning techniques. This system automates data acquisition, image processing, spectral analysis, and employs Bayesian optimization to systematically and efficiently navigate the parameter space. The approach significantly enhances throughput and directs the search towards optimal processing parameters more effectively than conventional methods. Furthermore, the framework is flexible and applicable to a broad range of solution-processed materials beyond electrochromics. Overall, the work demonstrates the powerful role of self-driving labs in accelerating materials discovery and optimizing fabrication processes, potentially transforming how new materials are developed for energy-saving technologies. <div>
arXiv:2512.05989v1 Announce Type: new 
Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure</title>
<link>https://arxiv.org/abs/2512.05990</link>
<guid>https://arxiv.org/abs/2512.05990</guid>
<content:encoded><![CDATA[
<div> Keywords: Memory-Amortized Inference, Homological Parity Principle, Topological Cycle Closure, Wake-Sleep algorithm, Cognitive Phase Transitions

<br /><br />Summary:  
1. The paper introduces Memory-Amortized Inference (MAI), a theoretical framework combining learning and memory into a unified geometric substrate informed by algebraic topology.  
2. It proposes the Homological Parity Principle, distinguishing stable content (even-dimensional homology, \(H_{even}\)) from dynamic context (odd-dimensional homology, \(H_{odd}\)) in cognition.  
3. Cognition is modeled as a topological trinity transformation: Search (complex recursive searching), Closure (topological cycle closure), and Structure (stable knowledge encoding).  
4. The work connects complex NPSPACE search (via Savitch’s Theorem) to tractable P-time dynamic programming lookups, showing how cognitive systems convert slow, recursive reasoning into fast intuitive retrieval.  
5. The process parallels a generalized Wake-Sleep algorithm that alternates between optimizing dynamic inference flows (\(H_{odd}\), wake phase) and consolidating knowledge into stable structures (\(H_{even}\), sleep phase), explaining fast vs. slow thinking.  
6. The framework provides a rigorous, topological foundation for cognition and suggests new post-Turing computational architectures leveraging topological resonance for efficient inference and learning. <div>
arXiv:2512.05990v1 Announce Type: new 
Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra</title>
<link>https://arxiv.org/abs/2512.06059</link>
<guid>https://arxiv.org/abs/2512.06059</guid>
<content:encoded><![CDATA[
<div> Volatile Organic Compounds, Infrared Spectroscopy, Neural Networks, Data Augmentation, VOC Detection<br /><br />Summary:<br /><br />The article addresses the challenge of detecting volatile organic compounds (VOCs), which are harmful due to their ability to evaporate easily and impact human health. Infrared (IR) spectroscopy is used to detect VOCs at very low concentrations by analyzing their absorption spectra; however, the complexity of these spectra makes real-time recognition and quantification difficult. To overcome this, the authors collected an experimental dataset containing IR spectra of nine different VOC classes at various concentrations. Recognizing the limited size and diversity of this dataset, they employed conditional generative neural networks to produce synthetic spectra, augmenting the dataset and increasing its variety. This augmentation enabled the training of robust discriminative neural networks capable of accurately identifying the nine VOC classes and estimating their concentrations. The research demonstrates that such neural network models, trained on both real and synthetic data, can be effectively implemented in sensing devices for real-time VOC recognition and concentration analysis, potentially improving environmental monitoring and human health protection measures. <div>
arXiv:2512.06059v1 Announce Type: new 
Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models</title>
<link>https://arxiv.org/abs/2512.06062</link>
<guid>https://arxiv.org/abs/2512.06062</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, synthetic data, membership inference attack, data privacy, clustering  

<br /><br />Summary:  
This paper investigates privacy risks in synthetic data generated by generative models, which are often used to share data while preserving privacy. The authors reveal that synthetic data can leak information about the original training data due to structural overlaps in the data manifold. They introduce a novel black-box membership inference attack that does not require access to the model's internal parameters or real data but relies on repeatedly querying the generative model to produce numerous synthetic samples. By applying unsupervised clustering, the attack identifies dense regions and analyzes cluster medoids and neighborhoods, which align with high-density regions where real training data resides. These clusters act as proxies to infer membership or reconstruct approximate records of the original data subjects. The experimental evaluation spans sensitive areas including healthcare and finance, demonstrating that membership leakage occurs even when differential privacy or noise-based protections are applied during model training. This exposes a subtle but critical attack vector focusing on distributional neighborhood inference rather than direct memorization of samples. The findings underscore the need for enhanced privacy guarantees in synthetic data generation pipelines. The authors also provide open-source implementation and evaluation code to promote further research on this emerging threat. <div>
arXiv:2512.06062v1 Announce Type: new 
Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06102</link>
<guid>https://arxiv.org/abs/2512.06102</guid>
<content:encoded><![CDATA[
<div> Artificial intelligence, reinforcement learning, wildfire management, JaxWildfire, GPU acceleration<br /><br />Summary:<br /><br />This paper addresses the challenge of using reinforcement learning (RL) to improve wildfire and natural hazard management, highlighting the critical bottleneck posed by the slow speed of existing wildfire simulators. The authors introduce JaxWildfire, a novel wildfire simulator that uses a probabilistic fire spread model based on cellular automata. Implemented in JAX, JaxWildfire leverages vectorized simulations via the vmap function, enabling high-throughput simulations on GPUs. This approach results in a dramatic 6 to 35 times speedup compared to existing wildfire simulation software. Additionally, JaxWildfire supports gradient-based optimization of simulator parameters, contributing to more efficient and precise model tuning. The paper demonstrates that JaxWildfire is suitable for training RL agents, which can learn effective wildfire suppression strategies within this accelerated simulation environment. Overall, this work represents an important advancement by addressing computational limitations and enabling the application of RL techniques to natural hazard management, particularly wildfire control, paving the way for improved decision-making and proactive interventions in complex, uncertain scenarios. <div>
arXiv:2512.06102v1 Announce Type: new 
Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC-AGI Without Pretraining</title>
<link>https://arxiv.org/abs/2512.06104</link>
<guid>https://arxiv.org/abs/2512.06104</guid>
<content:encoded><![CDATA[
<div> Keywords: CompressARC, Minimum Description Length, ARC-AGI, zero pretraining, visual IQ puzzles<br /><br />Summary:<br />1. The paper challenges the common notion that solving ARC-AGI-1 benchmark visual puzzles requires massive pretraining typically used in large language models (LLMs).<br />2. It introduces CompressARC, a novel 76,000-parameter model that operates without any pretraining and solves 20% of the ARC-AGI puzzles.<br />3. CompressARC uniquely solves puzzles by minimizing the description length (Minimum Description Length, MDL) of the target puzzle during inference, effectively training on the single test puzzle itself minus the solution.<br />4. This approach endows CompressARC with exceptional generalization abilities despite extremely limited data, a property not commonly observed in conventional deep learning methods.<br />5. The results suggest that MDL can be an alternative pathway to achieving intelligence beyond pretrained models, as CompressARC successfully solves diverse and creative visual IQ-style puzzles without relying on the provided ARC-AGI training set or extensive sample data. <div>
arXiv:2512.06104v1 Announce Type: new 
Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts</title>
<link>https://arxiv.org/abs/2512.06111</link>
<guid>https://arxiv.org/abs/2512.06111</guid>
<content:encoded><![CDATA[
<div> Keywords: AADT, short-duration traffic counts, machine learning, traffic volume estimation, Texas DOT<br /><br />Summary:<br /><br />1. The Federal Highway Administration requires state Departments of Transportation to collect accurate Annual Average Daily Traffic (AADT) data, but many struggle with reliability, especially on unmonitored roads.<br />2. Continuous count stations provide precise traffic data but are costly and challenging to deploy widely, leading agencies to depend on short-duration traffic counts.<br />3. This study introduces a novel machine learning framework designed to identify the most optimal representative day(s) for short count data collection to enhance AADT prediction accuracy.<br />4. Using traffic volume data from Texas in 2022 and 2023, the proposed 'optimal day' approach iteratively selects informative days and is compared against a baseline representing common DOT practices without optimal day selection.<br />5. The study utilizes continuous count data to simulate 24-hour short counts and incorporates actual field short counts via a leave-one-out method for unbiased feature engineering across similar roads.<br />6. Results show the machine learning method outperforms the baseline, with the best day achieving significantly lower errors (RMSE: 7,871.15 vs. 11,185.00) and higher R² (0.9756 vs. 0.9499), indicating improved AADT estimation.<br />7. This approach offers an effective, lower-cost alternative for DOTs to improve traffic data accuracy, comply with monitoring standards, and reduce statewide traffic data collection expenses. <div>
arXiv:2512.06111v1 Announce Type: new 
Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting</title>
<link>https://arxiv.org/abs/2512.06134</link>
<guid>https://arxiv.org/abs/2512.06134</guid>
<content:encoded><![CDATA[
<div> Neural Koopman Machine, Alzheimer's disease, cognitive decline forecasting, multimodal data, hierarchical attention  

<br /><br />Summary:  
This paper introduces the Neural Koopman Machine (NKM), a novel machine learning model designed to forecast individual cognitive decline in Alzheimer's disease (AD) by simultaneously predicting multiple cognitive scores. Inspired by dynamical systems and attention mechanisms, NKM integrates multimodal data including genetic, neuroimaging, proteomic, and demographic information. It uniquely incorporates analytical ($\alpha$) and biological ($\beta$) knowledge to guide feature grouping and control hierarchical attention, enabling the extraction of relevant patterns. Using a Fusion Group-Aware Hierarchical Attention mechanism within the Koopman operator framework, NKM converts complex nonlinear data trajectories into interpretable linear representations, enhancing model transparency. The model's efficacy was validated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, where it outperformed traditional machine learning and deep learning methods in predicting cognitive decline trajectories. NKM not only forecasts multiple cognitive scores at once but also quantifies the differential contributions of biomarkers to each cognitive outcome. Furthermore, it identifies brain regions most predictive of cognitive deterioration, facilitating biological interpretability. Overall, NKM represents an advancement in personalized and interpretable forecasting of AD progression by leveraging multimodal data within an explainable framework, shedding light on potential multimodal biological factors underlying Alzheimer's disease. <div>
arXiv:2512.06134v1 Announce Type: new 
Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($\alpha$) and biological ($\beta$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points</title>
<link>https://arxiv.org/abs/2512.06143</link>
<guid>https://arxiv.org/abs/2512.06143</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian processes, scalable methods, non-stationary kernels, sparse covariance, exact inference<br /><br />Summary:  
The paper presents a novel methodology called gp2Scale that addresses the challenge of scaling exact Gaussian processes (GPs) to datasets exceeding 10 million points without relying on common approximations such as inducing points, kernel interpolation, or neighborhood-based methods. The approach leverages highly flexible, compactly supported, and non-stationary kernels to uncover inherent sparse structures within the covariance matrix. This sparsity is exploited effectively to perform efficient and exact computations of linear system solutions and log-determinants needed for model training. The method demonstrates competitive or superior approximation performance compared to state-of-the-art GP approximation techniques on several real-world datasets. Notably, gp2Scale maintains full fidelity to exact GP inference rather than trading accuracy for scalability. A key advantage is its agnosticism toward kernel design choices, noise models, mean functions, and input space types, supporting arbitrary customizations without compromising performance or flexibility. This positions gp2Scale as a versatile and powerful tool for modern GP applications where expressive, non-stationary kernels are increasingly important, overcoming the traditional trade-offs between computational speed, predictive accuracy, uncertainty quantification, and design flexibility. <div>
arXiv:2512.06143v1 Announce Type: new 
Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Invariant Graph Representations Through Redundant Information</title>
<link>https://arxiv.org/abs/2512.06154</link>
<guid>https://arxiv.org/abs/2512.06154</guid>
<content:encoded><![CDATA[
<div> Keywords: invariant graph representations, out-of-distribution generalization, Partial Information Decomposition, redundant information, spurious and causal subgraphs<br /><br />Summary:<br /><br />This paper addresses the challenge of learning invariant graph representations that perform well under out-of-distribution (OOD) generalization, which is difficult because learned features often contain spurious components. To overcome limitations of classical information-theoretic methods in invariant learning, the authors introduce Partial Information Decomposition (PID), a tool that decomposes information into unique, redundant, and synergistic parts, allowing precise identification of redundant information shared between spurious subgraphs and invariant subgraphs in graphs. They propose a novel multi-level optimization framework called Redundancy-guided Invariant Graph learning (RIG), which explicitly maximizes redundant information between causal and spurious graph components. RIG alternates between estimating a lower bound of this redundant information and maximizing it alongside other objectives to effectively isolate spurious from causal subgraphs. This isolation fosters better OOD generalization under various distribution shifts. Experimental results on both synthetic and real-world graph datasets demonstrate that RIG improves generalization capabilities compared to existing methods. Overall, the paper presents a principled and effective approach to learning robust invariant graph representations by leveraging PID to better understand and exploit redundant information in graph data. <div>
arXiv:2512.06154v1 Announce Type: new 
Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations</title>
<link>https://arxiv.org/abs/2512.06183</link>
<guid>https://arxiv.org/abs/2512.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic state estimation, diffusion model, mask-aware training, physics-guided projection, sparse observations  

<br /><br />Summary:  
This paper addresses the challenge of reconstructing high-resolution highway traffic speed fields from sparse and incomplete sensor data, which is crucial for Intelligent Transportation Systems. The authors propose PMA-Diffusion, a novel physics-guided mask-aware diffusion framework that effectively recovers unobserved traffic speeds. The method involves training a diffusion prior directly on sparsely observed speed fields using two specialized mask-aware training strategies named Single-Mask and Double-Mask, which improve learning from incomplete data. During inference, a physics-guided posterior sampler is introduced, combining reverse diffusion updates with observation projection and physics-based projection via adaptive anisotropic smoothing, ensuring physical consistency while reconstructing missing traffic speeds. The framework is empirically tested on the real-world I-24 MOTION dataset across different observation visibility ratios. Results demonstrate that PMA-Diffusion significantly outperforms existing baseline methods in reconstruction accuracy, even when only 5% of the traffic speeds are observed. Remarkably, the model trained with sparse data nearly matches the performance of models trained on fully observed speed fields, highlighting the efficiency and robustness of the approach. Overall, the integration of mask-aware diffusion priors with physics-guided posterior sampling offers a reliable, flexible, and practical solution to traffic state estimation under realistic sensing limitations. <div>
arXiv:2512.06183v1 Announce Type: new 
Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?</title>
<link>https://arxiv.org/abs/2512.06200</link>
<guid>https://arxiv.org/abs/2512.06200</guid>
<content:encoded><![CDATA[
<div> Keywords: Approximate Nearest Neighbor Search, data deletion, graph-based ANNS, evaluation framework, Deletion Control  

<br /><br />Summary:  
This study addresses the challenge of data deletion in Approximate Nearest Neighbor Search (ANNS), particularly for dynamic datasets used in applications like Retrieval-Augmented Generation. It identifies a gap in existing research concerning comprehensive evaluation methodologies for data deletion in ANNS indexes. To fill this gap, the authors propose a novel experimental framework alongside detailed evaluation metrics focused on deletion efficiency, encompassing factors such as accuracy and query speed. The paper categorizes existing data deletion methods for graph-based ANNS into three distinct approaches, providing formal mathematical definitions for each. Utilizing this framework, the study evaluates the Hierarchical Navigable Small World (HNSW) algorithm — a leading ANNS method — to examine how different deletion strategies impact performance. Based on the findings, the authors introduce Deletion Control, an adaptive method designed to select the most appropriate deletion technique dynamically to maintain a required level of search accuracy. This contribution is significant for practical use cases requiring both efficient nearest neighbor search and data management in changing datasets. The proposed framework and Deletion Control method advance the state of the art by enabling systematic performance assessment and adaptive optimization in dynamic ANNS scenarios. <div>
arXiv:2512.06200v1 Announce Type: new 
Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K2-V2: A 360-Open, Reasoning-Enhanced LLM</title>
<link>https://arxiv.org/abs/2512.06201</link>
<guid>https://arxiv.org/abs/2512.06201</guid>
<content:encoded><![CDATA[
<div> Keywords: K2-V2, open LLM, reasoning adaptation, training data release, model performance<br /><br />Summary: K2-V2 is a newly introduced 360-open large language model (LLM) developed from scratch to serve as a robust base for reasoning adaptation along with capabilities in conversation and knowledge retrieval typical of general LLMs. It claims to be the strongest fully open model in its size category, outperforming competitors like Qwen2.5-72B and nearing the performance of Qwen3-235B. The model is carefully trained with infused domain knowledge, reasoning ability, extended context handling, and integrated tool use to explicitly prepare it for handling complex reasoning tasks. Its potential is demonstrated through simple supervised fine-tuning, setting a strong baseline indicative of considerable potential for further advanced alignment. To support ongoing improvements and open source production uses, the creators are releasing the full training history and comprehensive data composition. This transparency aims to maximize the effectiveness of continuous training. Additionally, the model weights and signature LLM360 artifacts—including complete training data—are made publicly available to empower the community with a reasoning-focused and capable foundation for further research and application development. <div>
arXiv:2512.06201v1 Announce Type: new 
Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Memory Use in Reinforcement Learning with Temporal Range</title>
<link>https://arxiv.org/abs/2512.06204</link>
<guid>https://arxiv.org/abs/2512.06204</guid>
<content:encoded><![CDATA[
<div> Temporal Range, Reinforcement Learning, Memory Dependence, Sensitivity Analysis, Temporal Lag<br /><br />Summary:<br /><br />This paper introduces Temporal Range, a novel, model-agnostic metric designed to quantify how much a trained reinforcement learning (RL) policy relies on its past observations. Temporal Range measures first-order sensitivities of multiple vector outputs over a temporal window with respect to input sequences, summarizing these influences into a magnitude-weighted average lag. It is computed efficiently using reverse-mode automatic differentiation on Jacobian blocks linking outputs at later timesteps to earlier inputs. The metric is theoretically grounded by a set of natural axioms and performs well in linear settings. Experimentally, Temporal Range remains small in fully observed control tasks, scales appropriately with the ground-truth lag in synthetic sequence-copying tasks (Copy-$k$), and correlates with the minimal history window needed for near-optimal performance as shown through window ablation studies. The authors evaluate Temporal Range across various architectures including MLPs, RNNs, and state-space models (SSMs), as well as a compact Long Expressive Memory (LEM) policy, highlighting its utility as a proxy for task-level memory use. Overall, Temporal Range offers a practical, per-sequence measure of memory dependence that can help compare different agents and environments and assist in selecting the shortest sufficient context for decision-making in RL settings. <div>
arXiv:2512.06204v1 Announce Type: new 
Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration</title>
<link>https://arxiv.org/abs/2512.06218</link>
<guid>https://arxiv.org/abs/2512.06218</guid>
<content:encoded><![CDATA[
<div> asynchronous stochastic approximation, semi-Markov decision processes, RVI Q-learning, average-reward optimality, convergence analysis

<br /><br />Summary: This paper leverages recent advances in asynchronous stochastic approximation (SA) within the Borkar-Meyn framework to address reinforcement learning challenges in average-reward semi-Markov decision processes (SMDPs). The authors establish the almost sure convergence of an asynchronous SA variant of Schweitzer's classical relative value iteration (RVI) algorithm, specifically the RVI Q-learning algorithm, for finite-state, weakly communicating SMDPs. Their analysis demonstrates that the algorithm converges to a compact, connected subset of solutions for the average-reward optimality equation, with uniqueness of the solution guaranteed under additional constraints on step sizes and asynchrony. To fully exploit the SA framework, the paper introduces novel monotonicity conditions for accurate estimation of the optimal reward rate within RVI Q-learning, significantly broadening the class of applicable algorithmic scenarios. The authors also provide innovative arguments for the stability and convergence analysis, thereby overcoming limitations in existing frameworks. Overall, the work offers a rigorous convergence proof and an expanded theoretical foundation for using asynchronous RVI Q-learning methods in solving average-reward SMDPs in reinforcement learning contexts. <div>
arXiv:2512.06218v1 Announce Type: new 
Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph</title>
<link>https://arxiv.org/abs/2512.06236</link>
<guid>https://arxiv.org/abs/2512.06236</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Domain Adaptation, Node Classification, Graph Structure Shift, Edge Denoising<br /><br />Summary:<br /><br />1. This paper investigates the node classification task within the setting of graph domain adaptation, aiming to improve Graph Neural Networks (GNNs) performance on target graphs by utilizing both source and target graph structures alongside source labels.<br />2. It addresses the challenge of structural domain shifts that occur when graph data are gathered from different times or regions, which can lead to degraded GNN performance on the target domain.<br />3. The authors reveal that adding an auxiliary loss function focused on denoising edges of the target graph significantly enhances the generalization ability of GNNs for node classification.<br />4. Building on this finding, they introduce GraphDeT, a new framework incorporating the auxiliary edge denoising task into GNN training, targeting improved node classification under domain adaptation scenarios.<br />5. Theoretical analysis links this auxiliary task to tightening the graph generalization bound via -distance, which effectively constrains the learning process and results in better generalization.<br />6. Experimental evaluations demonstrate that GraphDeT outperforms existing baselines in managing domain shifts caused by both temporal and regional differences in graph data, confirming its effectiveness and robustness. <div>
arXiv:2512.06236v1 Announce Type: new 
Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantization Blindspots: How Model Compression Breaks Backdoor Defenses</title>
<link>https://arxiv.org/abs/2512.06243</link>
<guid>https://arxiv.org/abs/2512.06243</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, quantization, backdoor defenses, INT8, neural networks<br /><br />Summary:<br /><br />1. The paper investigates the impact of model quantization on the effectiveness of existing backdoor attack defenses in neural networks. 2. It highlights that backdoor attacks embed malicious behavior that activates only on specific inputs while maintaining high accuracy on clean data, posing a critical threat to deployed machine learning systems. 3. The study evaluates five representative backdoor defenses across three model precision settings: full precision (FP32), INT8 dynamic quantization, and simulated INT4 precision on two vision datasets using the BadNet attack. 4. The results reveal that INT8 quantization drastically reduces the detection rates of all studied defenses to zero, while the backdoor attack success rate remains above 99%. 5. At INT4 precision, the defense performance becomes dataset-dependent; for example, Neural Cleanse effectively detects backdoors on GTSRB but fails on CIFAR-10, even though attacks remain highly successful (>90%). 6. This work uncovers a significant evaluation mismatch since most defenses are tested on full-precision models but real-world deployments use quantized models. 7. The authors emphasize the need to consider quantization robustness as a key factor in future backdoor defense evaluations and designs. <div>
arXiv:2512.06243v1 Announce Type: new 
Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-exploration for online reinforcement learning</title>
<link>https://arxiv.org/abs/2512.06244</link>
<guid>https://arxiv.org/abs/2512.06244</guid>
<content:encoded><![CDATA[
<div> Exploration-exploitation dilemma, Reinforcement learning, Parameter-free, Sample complexity, Advantage gap function<br /><br />Summary:<br /><br />The paper addresses the fundamental challenge of balancing exploration and exploitation in reinforcement learning (RL). Existing RL algorithms for finite state and action discounted problems typically depend on assumptions requiring sufficient exploration across both state and action spaces. These assumptions lead to algorithms that are often non-implementable and exhibit sub-optimal performance due to reliance on problem-specific parameters. To overcome these issues, the authors introduce a novel class of methods termed "auto-exploration," which can automatically explore state and action spaces without needing prior knowledge of any problem-dependent parameters. Two variants of their algorithm are proposed: one designed for the tabular setting and another for linear function approximation. Both methods are backed by theoretical guarantees, achieving an $O(\epsilon^{-2})$ sample complexity to reach an $\epsilon$-accurate solution under broad, algorithm-independent assumptions about the existence of an exploring optimal policy. Importantly, this sample complexity bound is free from algorithm-dependent parameters, which are common in previous approaches and can be arbitrarily large. The proposed methods are also straightforward to implement, as they do not require direct estimation of unknown parameters. This progress is enabled through several new algorithmic techniques including dynamic mixing time, use of discounted state distributions for sampling, a robust gradient estimator, and the application of an advantage gap function to verify convergence. <div>
arXiv:2512.06244v1 Announce Type: new 
Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(\epsilon^{-2})$ sample complexity to solve to $\epsilon$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06250</link>
<guid>https://arxiv.org/abs/2512.06250</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, adaptive switching, maze navigation, Q-learning, multi-strategy agents<br /><br />Summary:<br /><br />This research addresses the challenge of switching between multiple strategies in autonomous agents tasked with complex navigation problems. It introduces a reinforcement learning method, specifically Q-learning, to adaptively learn switching thresholds between two distinct navigation policies: systematic exploration (coverage) and goal-directed pathfinding (convergence). Using maze navigation as a test scenario, the agent dynamically adjusts its switching behavior based on discretized states defined by coverage percentage and distance to the goal. Unlike fixed-threshold methods, the agent requires minimal prior knowledge, needing only maze size and target location without knowledge of wall positions or preset heuristics. The agent discretizes coverage into buckets ranging from 20% to 60% and learns which threshold to apply depending on observed progress. Experimental evaluation across 240 configurations involving multiple maze sizes, unique layouts, and agent variants reveals that adaptive threshold learning outperforms single-strategy and fixed-threshold baselines consistently. Performance improvements include 23-55% faster completion times, an 83% reduction in runtime variance, and a 71% enhancement in worst-case outcomes. Additionally, the learned behaviors generalize well to unseen maze layouts within the same size category. Notably, performance gains increase with maze complexity, underscoring the growing advantage of adaptive policy switching in large, complex environments. <div>
arXiv:2512.06250v1 Announce Type: new 
Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Without Time-Based Embodiment Resets in Soft-Actor Critic</title>
<link>https://arxiv.org/abs/2512.06252</link>
<guid>https://arxiv.org/abs/2512.06252</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Soft Actor-Critic, continuing tasks, embodiment resets, exploration<br /><br />Summary:<br /><br />This paper investigates the challenges of training reinforcement learning agents without relying on common task accessories such as episode terminations and embodiment resets. First, it introduces a continuing version of the Soft Actor-Critic (SAC) algorithm designed to work without episode boundaries. By applying slight modifications to the reward functions of existing tasks, the continuing SAC matches or surpasses episodic SAC performance and reduces sensitivity to the discount factor γ. Second, the study uses a modified Gym Reacher task to analyze why continuing SAC struggles without embodiment resets. The results indicate that embodiment resets facilitate better state space exploration, and removing them leads to poor exploration and slower or failed learning. Third, experiments on additional simulated tasks and a real-robot vision task demonstrate that increasing policy entropy when performance stagnates or worsens helps recover the lost performance caused by the absence of embodiment resets. Overall, this work highlights the importance of considering exploration mechanisms and policy entropy adjustments for robust reinforcement learning in continuous, reset-free environments, bringing training setups closer to real-world scenarios. <div>
arXiv:2512.06252v1 Announce Type: new 
Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $\gamma$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Restless Multi-Arm Bandits with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06274</link>
<guid>https://arxiv.org/abs/2512.06274</guid>
<content:encoded><![CDATA[
<div> Restless Multi-Armed Bandits, networked RMAB, independent cascade model, submodularity, Q-learning<br /><br />Summary:<br /><br />This paper addresses the limitations of traditional Restless Multi-Armed Bandit (RMAB) models, which assume independence among arms and therefore fail to capture interactions between individuals common in real-world settings. To overcome this, the authors propose a Networked RMAB framework that integrates RMABs with the independent cascade model, enabling the modeling of network effects among arms. They formally define the Bellman equation for the Networked RMAB but recognize significant computational challenges due to the exponentially large state and action spaces. To tackle this, the paper establishes that the Bellman equation exhibits submodularity, allowing the use of a hill-climbing algorithm that guarantees a \(1 - \frac{1}{e}\) approximation in Bellman updates. Furthermore, they prove convergence of these approximate updates through a modified contraction analysis. Finally, the authors develop an efficient Q-learning algorithm adapted for the networked setting and validate their approach experimentally on real-world graph data. The results show that their network-aware Q-learning method outperforms traditional k-step look-ahead and network-blind baselines, underscoring the importance of accounting for network interactions in sequential decision-making problems. <div>
arXiv:2512.06274v1 Announce Type: new 
Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Compression Bounds for Wide Multilayer Perceptrons</title>
<link>https://arxiv.org/abs/2512.06288</link>
<guid>https://arxiv.org/abs/2512.06288</guid>
<content:encoded><![CDATA[
<div> Pruning, Quantization, Neural Networks, Compression, Multilayer Perceptrons  

<br /><br />Summary:  
This paper presents a rigorous theoretical justification for pruning and quantization techniques commonly used to compress large neural networks. The authors introduce a randomized greedy compression algorithm applied post-training, which enables the identification of pruned and quantized subnetworks within multilayer perceptrons (MLPs) that maintain competitive performance levels. They extend these theoretical results to structured pruning methodologies applicable to both MLPs and convolutional neural networks (CNNs), offering a unified framework for understanding pruning in wide networks. A notable feature of their analysis is the absence of assumptions regarding data distribution or characteristics, highlighting a fundamental tradeoff between network compressibility and the width of the neural network. The proposed algorithm shows similarities to the classical Optimal Brain Damage (OBD) method but incorporates randomness post-training, blending concepts from traditional pruning strategies with new theoretical insights. Overall, this work bridges the gap between empirical successes in model compression techniques and their mathematical underpinnings, specifically focusing on wide neural network architectures, thus providing a solid theoretical foundation that explains why pruning and quantization can be so effective in practice. <div>
arXiv:2512.06288v1 Announce Type: new 
Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media</title>
<link>https://arxiv.org/abs/2512.06293</link>
<guid>https://arxiv.org/abs/2512.06293</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transit, social media, topic modeling, Poisson Deconvolution Factorization, influence-weighted graph<br /><br />Summary:<br /><br />This paper addresses the challenge faced by urban transit agencies of detecting emerging service risks such as crowding, delays, and safety issues via social media, where signals are typically sparse and obscured by routine chatter. The authors propose a novel approach that jointly models linguistic interactions and user influence by constructing an influence-weighted keyword co-occurrence graph from cleaned social media posts, ensuring more socially impactful posts contribute proportionally to evidence. Central to their framework is a Poisson Deconvolution Factorization (PDF) method that decomposes the graph into a low-rank topical structure and topic-localized residual interactions, yielding an interpretable topic–keyword basis with topic importance scores. They introduce a decorrelation regularizer to promote distinct, well-separated topics while employing a lightweight optimization procedure that guarantees stable convergence under nonnegativity and normalization constraints. The number of topics is optimally selected via a coherence-driven sweep that balances topic quality and distinctness. Experimental results on large-scale social streams demonstrate that their model achieves state-of-the-art performance in topic coherence and strong topic diversity compared to leading baselines. Additionally, the authors have made their code and dataset publicly available to support reproducibility and further research. <div>
arXiv:2512.06293v1 Announce Type: new 
Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks</title>
<link>https://arxiv.org/abs/2512.06297</link>
<guid>https://arxiv.org/abs/2512.06297</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, loss landscape, entropic barriers, curvature, optimization dynamics<br /><br />Summary:<br /><br />1. Modern neural networks' loss landscapes feature basins of attraction connected by low-loss paths, suggesting possible transitions between solutions.<br />2. Despite these connections, optimization algorithms tend to remain confined to a single convex basin, rarely exploring intermediate points along these pathways.<br />3. The paper resolves this paradox by identifying "entropic barriers" caused by the interaction between curvature variations along the paths and noise in the optimization dynamics.<br />4. Empirical findings indicate that curvature increases systematically away from minima, generating effective forces that push noisy optimization trajectories back toward the endpoints of the paths, even when the loss values are nearly constant.<br />5. These curvature-induced entropic barriers persist longer and influence solution localization in parameter space more strongly than energetic barriers.<br />6. Overall, the study highlights how curvature-induced entropic forces govern both the connectivity between solutions in the loss landscape and the confinement of optimization trajectories within basins in deep learning.<br />7. This understanding provides insights into why optimization dynamics in deep networks rarely cross low-loss connecting paths and remain localized despite apparent path connectivity. <div>
arXiv:2512.06297v1 Announce Type: new 
Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics</title>
<link>https://arxiv.org/abs/2512.06301</link>
<guid>https://arxiv.org/abs/2512.06301</guid>
<content:encoded><![CDATA[
<div> Keywords: polymers, machine learning, molecular representation, property prediction, inverse design<br /><br />Summary:<br /><br />1. The paper addresses the challenge of applying machine learning (ML) to polymers, which have lagged behind inorganic compounds and small molecules mainly due to data scarcity. The authors argue that strategic molecular representations can help overcome this bottleneck.<br /><br />2. They introduce CI-LLM (Chemically Informed Language Model), a novel framework that combines HAPPY (Hierarchically Abstracted rePeat unit of PolYmer) tokenization encoding chemical substructures with numerical descriptors, integrated within transformer architectures.<br /><br />3. For forward property prediction, the authors develop De³BERTa, a descriptor-enriched encoder transformer model. This model achieves 3.5 times faster inference compared to conventional SMILES-based models while improving accuracy by 0.9-4.1 percent across four polymer property targets.<br /><br />4. Importantly, De³BERTa provides interpretable insights into the structure-property relationship at the subgroup level, enhancing the explainability of ML predictions in polymer chemistry.<br /><br />5. For inverse design, the study presents a GPT-based generator capable of producing polymers with user-defined properties. It achieves 100% scaffold retention and successfully performs multi-property optimization, including negative correlations between objectives.<br /><br />This comprehensive framework demonstrates significant advances in both forward prediction and inverse design of polymers, illustrating how chemically informed representations can broaden ML applications in polymer science. <div>
arXiv:2512.06301v1 Announce Type: new 
Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization</title>
<link>https://arxiv.org/abs/2512.06303</link>
<guid>https://arxiv.org/abs/2512.06303</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal graph neural network, brain network reorganization, fractional stochastic differential operators, attention mechanisms, prognostic biomarkers  

<br /><br />Summary:  
This study presents a novel multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI data to model the spatiotemporal reorganization of brain networks. Brain regions are represented as nodes, while structural and functional connections form edges, creating longitudinal brain graphs for each individual. To capture temporal evolution and long-term dependencies in brain network dynamics, the model employs fractional stochastic differential operators embedded within graph-based recurrent networks, allowing it to account for stochastic fluctuations over time. The framework uses attention mechanisms to effectively fuse multimodal information and extract interpretable biomarkers, such as network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index that quantifies individual risk related to network instability and cognitive decline. Experiments conducted on longitudinal neuroimaging datasets demonstrate that the proposed method achieves high predictive accuracy while maintaining interpretability. Overall, the results illustrate the potential of mathematically rigorous, multimodal graph-based modeling to derive clinically meaningful biomarkers from existing neuroimaging data without the need for new data acquisition, contributing to improved prognosis and understanding of neurological disease progression. <div>
arXiv:2512.06303v1 Announce Type: new 
Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness</title>
<link>https://arxiv.org/abs/2512.06341</link>
<guid>https://arxiv.org/abs/2512.06341</guid>
<content:encoded><![CDATA[
<div> Interpretability, machine learning, mutual information, Fisher geometry, representation design<br /><br />Summary:<br /><br />1. The paper introduces Interpretive Efficiency, a novel metric designed to quantify how effectively data supports an interpretive representation in trustworthy machine learning.  2. Interpretive Efficiency is defined as a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel, addressing the limitations of existing interpretability metrics.  3. The metric is rigorously grounded on five axioms: boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency, ensuring its theoretical soundness.  4. The authors connect Interpretive Efficiency to mutual information and derive a local Fisher-geometric expansion, providing a geometric perspective to analyze interpretability.  5. They further establish asymptotic and finite-sample guarantees for estimation using empirical-process tools, making the measure statistically reliable.  6. The experimental evaluation on controlled image and signal tasks demonstrates that the metric recovers known theoretical orderings, uncovers representational redundancy that accuracy metrics hide, and correlates with robustness.  7. The results suggest that Interpretive Efficiency is a practical, theory-backed diagnostic tool that can guide the design of interpretable representations in machine learning systems. <div>
arXiv:2512.06341v1 Announce Type: new 
Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title>
<link>https://arxiv.org/abs/2512.06343</link>
<guid>https://arxiv.org/abs/2512.06343</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward modeling, Bradley-Terry loss, representation distance, NormBT, Large Language Models<br /><br />Summary:<br /><br />1. Reward models play a pivotal role in aligning large language models (LLMs) within the reinforcement learning from human feedback (RLHF) framework, where the Bradley-Terry (BT) loss is the standard objective function.  
2. The BT loss gradient's norm depends on two components: the difference in predicted rewards between chosen and rejected responses (prediction error), and the representation distance between the pair in the final layer's output space.  
3. While the prediction error guides learning as intended, the representation distance can skew the magnitude of gradient updates, causing pairs with small representation distance to have weak updates even if misranked, and those with large distance to have disproportionately strong updates.  
4. This imbalance makes gradients from large-distance pairs dominate learning, overshadowing important fine-grained distinctions in small-distance pairs.  
5. To address this, the authors propose NormBT, an adaptive pair-wise normalization technique that mitigates representation-driven effects, focusing updates on prediction error. NormBT is a low-overhead, drop-in enhancement to BT loss. Experimental results show consistent reward model performance improvements across LLM architectures and datasets, notably achieving over 5% gains in the Reasoning category of RewardBench that features many small-distance pairs.  
6. This study identifies a fundamental limitation of the BT objective and offers a simple, effective correction to improve LLM reward modeling. <div>
arXiv:2512.06343v1 Announce Type: new 
Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry</title>
<link>https://arxiv.org/abs/2512.06347</link>
<guid>https://arxiv.org/abs/2512.06347</guid>
<content:encoded><![CDATA[
<div> Keywords: generalization error, interpolators, teacher-student framework, algebraic geometry, machine learning theory  

<br /><br />Summary:  
This paper theoretically investigates the generalization ability of interpolators in machine learning models within a teacher-student setting. It shows that the generalization error of interpolators becomes zero once the training dataset size surpasses a model-dependent threshold. The work addresses a key open problem in understanding why large-scale models, such as deep neural networks, generalize well despite having the capacity to interpolate training data perfectly. Unlike prior studies focusing on stochastic gradient descent’s implicit bias towards good solutions, empirical evidence and this study suggest that the intrinsic properties of the model parameters are the main contributors to effective generalization. Specifically, the authors prove that, under the teacher-student framework, even randomly sampled interpolators (parameter sets that perfectly fit training data) generalize exactly to zero error when the sample number passes a threshold. This threshold is explicitly characterized by the geometric structure of the entire set of interpolators in the parameter space. The analysis employs algebraic geometry tools to rigorously describe and understand this geometric structure, providing new mathematical insights into why interpolating solutions generalize well, advancing the theoretical understanding of modern machine learning generalization phenomena. <div>
arXiv:2512.06347v1 Announce Type: new 
Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing</title>
<link>https://arxiv.org/abs/2512.06351</link>
<guid>https://arxiv.org/abs/2512.06351</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, graph reinforcement learning, flexible job shop scheduling, carbon-aware, smart manufacturing<br /><br />Summary:<br /><br />This paper introduces \textsc{Luca}, a novel framework that combines large language models (LLMs) with graph neural networks to enhance flexible job shop scheduling in smart manufacturing. The approach uses an innovative in-house prompting strategy to create fused embeddings that capture both structural and contextual information of the scheduling state. These embeddings feed into a deep reinforcement learning policy network designed to make real-time scheduling decisions optimized for two objectives: minimizing makespan and reducing carbon emissions. The framework incorporates a dual-objective reward function that balances energy efficiency with scheduling timeliness, addressing sustainability goals. Experimental validation was performed on both synthetic and public datasets, demonstrating consistent superiority over existing comparison algorithms. Specifically, on synthetic data, \textsc{Luca} reduced the average makespan by 4.1% and up to 12.2% compared to the best alternatives while keeping emission levels stable. On public benchmarks, it achieved additional improvements in both makespan and carbon emission metrics. Overall, \textsc{Luca} proves to be an effective and practical solution for dynamic, sustainable, and carbon-aware scheduling in smart manufacturing systems. <div>
arXiv:2512.06351v1 Announce Type: new 
Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction</title>
<link>https://arxiv.org/abs/2512.06356</link>
<guid>https://arxiv.org/abs/2512.06356</guid>
<content:encoded><![CDATA[
<div> Keywords: incomplete node features, feature propagation, graph neural networks, masked autoencoder, inductive tasks<br /><br />Summary:<br /><br />Incomplete node features frequently occur in real-world graphs, such as partial privacy in web user attributes, which degrades Graph Neural Network (GNN) performance. Feature Propagation (FP) is commonly used for imputing missing node features but suffers from three main limitations: difficulty with graphs that are not fully connected, over-smoothing of imputed features, and being designed primarily for transductive tasks without addressing distribution shifts in inductive tasks. To overcome these challenges, the paper introduces DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that innovatively integrates FP with a graph-based Masked AutoEncoder (MAE). A key contribution is the Co-Label Linking (CLL) algorithm, which randomly connects training nodes sharing the same label to improve imputation on graphs with many disconnected components. At inference, DDFI employs a novel two-step representation generation process where FP-imputed features are further reconstructed by the MAE, reducing feature distribution shift and enhancing feature diversity in inductive scenarios. Additionally, the authors present a new real-world dataset named Sailing, derived from voyage records, containing naturally missing features, to evaluate imputation methods more effectively. Experiments on six public datasets and Sailing demonstrate that DDFI surpasses state-of-the-art approaches in both transductive and inductive settings, validating its effectiveness and robustness. <div>
arXiv:2512.06356v1 Announce Type: new 
Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction</title>
<link>https://arxiv.org/abs/2512.06357</link>
<guid>https://arxiv.org/abs/2512.06357</guid>
<content:encoded><![CDATA[
<div> Multi-step prediction, time-series forecasting, PID controller, neural networks, periodic data<br /><br />Summary:  
This article addresses the challenge of improving multi-step ahead prediction accuracy for periodic time-series data, which is crucial for decision-making in various industrial domains. It introduces a novel method inspired by the proportional-integral-derivative (PID) control approach designed to enhance neural network forecasting models without significantly increasing system complexity. The PID-based booster is applied at every prediction step to adjust forecasted values closer to the actual observed values, thereby refining accuracy. The water demand forecasting problem serves as a practical case study, utilizing two established deep neural network models to validate the effectiveness of the PID-inspired enhancement technique. Further generalization of the method is demonstrated by applying it to a neural network model predicting hourly energy consumption, showcasing its applicability to different types of periodic time-series problems. Comparative experiments between the original prediction models and those augmented with the PID-based booster highlight the method's superiority in achieving better forecasting accuracy while maintaining low computational complexity. Overall, the study presents a promising approach that integrates control theory principles with deep learning models to improve multi-step time-series predictions across diverse industrial contexts. <div>
arXiv:2512.06357v1 Announce Type: new 
Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Optimizers for Fast Gradient-Based Learning</title>
<link>https://arxiv.org/abs/2512.06370</link>
<guid>https://arxiv.org/abs/2512.06370</guid>
<content:encoded><![CDATA[
<div> Keywords: optimizer design, gradient-based learning, convex optimization, hyperparameter tuning, dynamic training  

<br /><br />Summary:  
This paper establishes a theoretical framework for automating the design of optimizers used in gradient-based learning by focusing on maximizing the instantaneous decrease in loss, guided by the greedy principle. It conceptualizes optimizers as functions mapping loss gradient signals to parameter updates, transforming the design task into a series of convex optimization problems over the optimizer space. By solving these convex problems under various constraints, the framework not only reproduces many popular optimizers as closed-form solutions but also identifies their optimal hyperparameters tailored to specific problem settings. This approach provides a systematic and principled way to both design new optimizers and tune their hyperparameters based on the observed gradient statistics during training. Additionally, the paper highlights that this meta-optimization process can be performed dynamically and adaptively throughout training rather than only before or after, enabling optimizers to better respond to changing training conditions. Ultimately, this work paves the way for more automated, adaptive, and theoretically grounded approaches to optimizer design and hyperparameter optimization in machine learning. <div>
arXiv:2512.06370v1 Announce Type: new 
Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title>
<link>https://arxiv.org/abs/2512.06392</link>
<guid>https://arxiv.org/abs/2512.06392</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, TPU, Parameter Server, Dataset Curation<br /><br />Summary:<br /><br />1. The paper introduces RLAX, a scalable reinforcement learning (RL) framework designed to improve reasoning capabilities of large language models (LLMs) using TPUs. <br />2. RLAX employs a parameter-server architecture where a master trainer updates model weights on a central server, and multiple inference workers retrieve these weights to generate new rollouts, enabling efficient distributed training.<br />3. The framework incorporates a set of system techniques allowing scalable and preemptible RL training, supporting a variety of state-of-the-art RL algorithms.<br />4. To enhance training efficiency and model performance, the authors develop novel dataset curation and alignment methods.<br />5. Large-scale experiments show that RLAX significantly improves the QwQ-32B model’s pass@8 accuracy by 12.8% within approximately 12 hours and 48 minutes using 1024 TPU v5p cores, while maintaining robustness against training interruptions due to preemptions.<br /><br />This work demonstrates a practical and powerful infrastructure for accelerating and scaling RL-based improvements on large language models leveraging TPU clusters, offering a pathway to faster convergence and higher quality in LLM tasks. <div>
arXiv:2512.06392v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator</title>
<link>https://arxiv.org/abs/2512.06417</link>
<guid>https://arxiv.org/abs/2512.06417</guid>
<content:encoded><![CDATA[
<div> underwater acoustics, Fourier Neural Operator, Hankel-FNO, acoustic charting, sound propagation<br /><br />Summary:<br /><br />1. The article addresses the challenge of fast and accurate underwater acoustic charting, which is essential for tasks like sensor placement optimization and autonomous vehicle path planning. 2. Traditional numerical solvers provide accuracy but are computationally expensive and not suitable for large-scale or real-time applications. 3. Existing deep learning surrogate models accelerate computations but face limitations including fixed-resolution constraints and reliance on explicit partial differential equation formulations, limiting their generalizability across environments. 4. The authors propose Hankel-FNO, a model based on the Fourier Neural Operator that integrates knowledge of sound propagation and bathymetry for enhanced performance. 5. Hankel-FNO demonstrates superior speed compared to conventional solvers and achieves higher accuracy than data-driven alternatives, particularly in long-range acoustic predictions. 6. The model exhibits strong adaptability to varying environmental conditions and sound source configurations, requiring minimal fine-tuning to maintain performance. <div>
arXiv:2512.06417v1 Announce Type: new 
Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new initialisation to Control Gradients in Sinusoidal Neural network</title>
<link>https://arxiv.org/abs/2512.06427</link>
<guid>https://arxiv.org/abs/2512.06427</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network initialization, SIREN, sinusoidal activation, gradient control, Neural Tangent Kernel<br /><br />Summary: This paper addresses the challenge of proper initialization in neural networks, particularly those using sinusoidal activation functions such as SIREN, to mitigate gradient explosion and vanishing. The authors derive a new closed-form initialization scheme distinct from the original SIREN method by analyzing fixed points from the convergence of pre-activation distributions and the variance of Jacobian sequences. This approach simultaneously controls gradients and ensures pre-activations vanish appropriately, preventing the emergence of spurious frequencies that degrade model generalization. The impact of the new initialization on training dynamics is further investigated using the Neural Tangent Kernel framework, revealing its strong influence on learning behavior. The proposed initialization is benchmarked against the original SIREN initialization and other baseline methods across various tasks including function fitting, image reconstruction, and physics-informed neural networks. Empirically, the new method consistently outperforms state-of-the-art methods, demonstrating improved training stability, better gradient scaling with network depth, and superior generalization performance in reconstruction problems. Overall, this work provides a rigorous theoretical foundation and practical improvements for initializing sinusoidal-activated neural networks, contributing to enhanced training efficiency and model accuracy. <div>
arXiv:2512.06427v1 Announce Type: new 
Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural expressiveness for beyond importance model compression</title>
<link>https://arxiv.org/abs/2512.06440</link>
<guid>https://arxiv.org/abs/2512.06440</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Network Pruning, Expressiveness, Model Compression, Data-Agnostic Pruning, YOLOv8  

<br /><br />Summary:  
1. The paper introduces a novel pruning criterion called "Expressiveness," focusing on a neuron's or group of neurons' ability to effectively redistribute informational resources based on the overlap of activations, rather than relying on weight importance.  
2. This expressiveness criterion is strongly correlated with the network’s initialization state, making it independent of the training state and offering a new perspective on the "When to Prune" question.  
3. The method can be approximated using arbitrary data or small representative datasets, enabling data-agnostic pruning strategies that broaden model compression applicability.  
4. A hybrid pruning strategy combining expressiveness and importance-based criteria shows complementary benefits and achieves up to 10 times higher parameter compression compared to weight-based methods, with about 1% average performance degradation.  
5. Using expressiveness pruning alone surpasses many state-of-the-art methods in compression efficiency. Specifically, on the YOLOv8 model for object detection on the COCO dataset, the approach reduced MACs by 46.1%, removed 55.4% of parameters, and improved mean Average Precision ($mAP_{50-95}$) by 3%. <div>
arXiv:2512.06440v1 Announce Type: new 
Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination</title>
<link>https://arxiv.org/abs/2512.06457</link>
<guid>https://arxiv.org/abs/2512.06457</guid>
<content:encoded><![CDATA[
<div> Dynamic sparsity, BitStopper, Transformer accelerator, Bit-serial enable stage fusion, Token selection<br /><br />Summary:<br /><br />This paper addresses the high computational and memory costs of self-attention in attention-based large language models by introducing BitStopper, a fine-grained algorithm-architecture co-design that eliminates the need for a sparsity predictor. First, the Bit-serial Enable Stage Fusion (BESF) mechanism reduces memory accesses by progressively terminating trivial tokens and merging the prediction stage with execution, effectively minimizing overhead. Second, the Lightweight and Adaptive Token Selection (LATS) strategy complements bit-level sparsity speculation to improve the selection of important tokens efficiently. Third, the Bit-level Asynchronous Processing (BAP) strategy enhances compute utilization by enabling on-demand, bit-grained memory fetching, optimizing hardware resource use. Finally, the architecture design translates these theoretical improvements into practical performance gains. Extensive evaluations show that BitStopper outperforms state-of-the-art Transformer accelerators, achieving 2.03x and 1.89x speedups compared to Sanger and SOFA, respectively. Additionally, it delivers 2.4x and 2.1x improvements in energy efficiency, demonstrating notable advancements in both speed and power consumption for accelerating sparse attention computations in large language models. <div>
arXiv:2512.06457v1 Announce Type: new 
Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control</title>
<link>https://arxiv.org/abs/2512.06471</link>
<guid>https://arxiv.org/abs/2512.06471</guid>
<content:encoded><![CDATA[
<div> Goal-conditioned RL, optimal control, reward design, partial observability, dual control<br /><br />Summary:<br /><br />This paper addresses goal-conditioned reinforcement learning (RL), where the agent's objective is to maximize the probability of reaching specific target goal states. The authors analyze the goal-conditioned setting through the lens of optimal control theory, deriving an optimality gap that explains why classical reward formulations, often quadratic and dense, may underperform compared to goal-conditioned rewards. They highlight that goal-conditioned rewards are more effective in guiding agents toward desired outcomes. Further, the study extends to partially observed Markov decision processes (POMDPs), linking the challenge of state estimation with the probabilistic nature of the goal-conditioned reward. This connection naturally fits dual control problems, where simultaneous state estimation and control decisions are necessary. The paper validates the theoretical benefits by applying goal-conditioned policies to environments characterized by nonlinear dynamics and uncertainty. Both reinforcement learning algorithms and predictive control methods are used, demonstrating improved performance and robustness over classical approaches. Overall, the work provides a theoretical foundation and practical insights into why goal-conditioned RL can outperform traditional dense reward methods, especially in complex, uncertain, and partially observable settings. <div>
arXiv:2512.06471v1 Announce Type: new 
Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing LLMs Using Quantization for Mobile Execution</title>
<link>https://arxiv.org/abs/2512.06490</link>
<guid>https://arxiv.org/abs/2512.06490</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Post-Training Quantization, 4-bit quantization, GGUF format, mobile inference<br /><br />Summary:<br /><br />This paper addresses the challenge of deploying large language models (LLMs) on resource-constrained mobile devices by applying Post-Training Quantization (PTQ) techniques. Specifically, it uses 4-bit quantization through the BitsAndBytes library integrated with the Hugging Face Transformers framework to compress Meta’s Llama 3.2 3B model. The quantized model is further converted into the GGUF format using llama.cpp tools, optimized for mobile inference. This approach results in a substantial 68.66% reduction in model size, making it feasible to run the Llama 3.2 3B model efficiently on Android devices. Qualitative assessments demonstrate that the 4-bit quantized model retains its ability to perform inference tasks effectively despite the compression. The study highlights the practical deployment of the GGUF-formatted quantized model on Android via the Termux environment and the Ollama framework. Ultimately, the combination of 4-bit PTQ and mobile-optimized formats like GGUF offers a promising solution for balancing model compactness with inference performance, facilitating capable LLM deployment in mobile contexts. <div>
arXiv:2512.06490v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosis-based mortality prediction for intensive care unit patients via transfer learning</title>
<link>https://arxiv.org/abs/2512.06511</link>
<guid>https://arxiv.org/abs/2512.06511</guid>
<content:encoded><![CDATA[
<div> transfer learning, ICU mortality prediction, diagnostic heterogeneity, GLM, XGBoost<br /><br />Summary:<br /><br />This study addresses the challenge of varying underlying causes of critical illness in the intensive care unit (ICU) and the lack of prediction models that consider diagnostic heterogeneity. The authors evaluate transfer learning methods for diagnosis-specific mortality prediction using both Generalized Linear Models (GLM) and XGBoost approaches applied to the eICU Collaborative Research Database. The results demonstrate that transfer learning models consistently outperform those trained solely on diagnosis-specific data and models relying only on the conventional APACHE IVa severity-of-illness score. Furthermore, transfer learning models exhibit better calibration compared to models trained on pooled data from all diagnoses. The study also proposes that the Youden cutoff is a better decision threshold for binary classification tasks than the conventional 0.5 threshold. Importantly, the transfer learning approach maintains strong predictive performance across different cutoff criteria, indicating its robustness for clinical decision-making. Overall, this research highlights the potential of transfer learning to improve prognosis accuracy in ICU patients by accounting for diagnostic variability, offering a more tailored and reliable tool for mortality prediction in critical care settings. <div>
arXiv:2512.06511v1 Announce Type: new 
Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical geometric deep learning enables scalable analysis of molecular dynamics</title>
<link>https://arxiv.org/abs/2512.06520</link>
<guid>https://arxiv.org/abs/2512.06520</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, graph neural networks, protein-nucleic acid complexes, long-range interactions, atomic detail  

<br /><br />Summary:  
This paper addresses challenges in analyzing molecular dynamics (MD) simulations of complex biomolecular systems, especially those lacking established quantitative features. It highlights the potential of graph neural networks (GNNs), which use message passing between nodes representing spatially neighboring atoms, to eliminate manual feature engineering. However, applying GNNs to large biomolecular assemblies with more than a few hundred residues has been restricted by difficulties in capturing long-range interactions and the high memory and runtime demands of large graphs. The authors propose a novel method that aggregates local information efficiently, substantially reducing computational resources without losing atomic-level resolution. This innovation enables the analysis of extensive simulations of protein-nucleic acid complexes containing thousands of residues to be conducted on single GPUs within minutes. For moderately sized systems with hundreds of residues, the method not only accelerates the analysis but also enhances model performance and interpretability. Overall, the approach broadens the applicability of GNN-based analysis to larger biomolecular MD trajectories, facilitating rapid, detailed, and interpretable insights into complex biomolecular dynamics at atomic resolution. <div>
arXiv:2512.06520v1 Announce Type: new 
Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06533</link>
<guid>https://arxiv.org/abs/2512.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: decoding-based regression, reinforcement learning, sequence generation, numerical prediction, Markov Decision Process  

<br /><br />Summary:  
This paper addresses the challenge in decoding-based regression where traditional token-level objectives like cross-entropy do not align well with continuous numerical values, causing limitations in precision and generalization. The authors propose using Reinforcement Learning (RL) to overcome this issue by framing the generation process as a Markov Decision Process and applying sequence-level rewards to enforce global numerical coherence rather than local token-level constraints. Their method employs specific RL algorithms such as ReMax and GRPO to better capture the overall magnitude of target values. Extensive experiments on tasks including tabular regression and code metric regression demonstrate that their RL-enhanced approach consistently outperforms state-of-the-art token-level baselines and conventional regression heads. The results showcase significant improvements in both sampling efficiency and predictive precision. This work establishes that integrating sequence-level signals via reinforcement learning unlocks the potential of decoding-based regression, making it a robust and accurate paradigm for general numerical prediction tasks. The analysis verifies the advantages of sequence-level optimization over token-level penalties, promoting better understanding and future research directions in applying large language models for numerical regression. <div>
arXiv:2512.06533v1 Announce Type: new 
Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation</title>
<link>https://arxiv.org/abs/2512.06547</link>
<guid>https://arxiv.org/abs/2512.06547</guid>
<content:encoded><![CDATA[
<div> Keywords: Decoupled loss, reinforcement learning, proximal policy, importance weight, A-3PO  

<br /><br />Summary:  
1. Decoupled loss is a reinforcement learning algorithm designed to address high data staleness in asynchronous RL settings.  
2. It improves the stability of coupled-loss algorithms like PPO and GRPO by introducing a proximal policy that separates off-policy corrections (importance weights) from the policy update mechanism (trust region).  
3. The proximal policy, however, requires an extra forward pass through the neural network at each training step, which becomes a computational bottleneck for training large language models.  
4. The authors propose A-3PO (APproximated Proximal Policy Optimization), an approach that approximates the proximal policy via simple interpolation, thus eliminating the need for the extra forward pass.  
5. A-3PO achieves a reduction in training time by 18% while maintaining comparable learning performance, effectively improving training efficiency without sacrificing stability.  
6. The method and example code are made publicly available for use and further research at the specified GitHub repository. <div>
arXiv:2512.06547v1 Announce Type: new 
Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Manifold Part 2: Neural Network Mathematics</title>
<link>https://arxiv.org/abs/2512.06563</link>
<guid>https://arxiv.org/abs/2512.06563</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, fixed points, manifold complexity, learning dynamics, federated systems<br /><br />Summary:<br /><br />This work formalizes neural networks as global systems shaped by stacked piecewise manifolds, employing fixed-point theory and boundary-conditioned iteration to describe their learning process. It posits that without fixed coordinates and operators, neural networks emerge as learnable numerical computations influenced by manifold complexity and high-order nonlinearities constrained by boundary conditions. Real-world data contributes significant challenges such as immense scope, scale, and minibatch fragmentation, leading to data complexity that impacts training. The dynamics of training further generate learning complexity through mechanisms including shifting node covers, curvature accumulation, and plasticity fluctuations. The paper argues that neural networks do not start with fixed points; rather, they iteratively construct them driven by residual connections, and stable fixed-point regions are essential for the emergence of learning capability. This framework explains the limitations of monolithic neural network models when confronted with geometric and data-induced plasticity. Finally, it advocates for designing architectures and federated systems that distribute manifold complexity across multiple elastic models, resulting in a coherent world-modeling framework that integrates geometry, algebra, fixed-point theory, and real-data complexity to better manage learning in complex environments. <div>
arXiv:2512.06563v1 Announce Type: new 
Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed--point theory, and boundary--conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high--order nonlinearity, and boundary conditions. Real--world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed--point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual--driven iteration. This perspective clarifies the limits of monolithic models under geometric and data--induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world--modeling framework grounded in geometry, algebra, fixed points, and real--data complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.06582</link>
<guid>https://arxiv.org/abs/2512.06582</guid>
<content:encoded><![CDATA[
<div> Quantum-Leap LSTM, Parameter-Shared Unified Gating, Hierarchical Gated Recurrence, Additive Skip Connections, Sequence Modeling

<br /><br />Summary:  
This paper introduces the Quantum-Leap LSTM (QL-LSTM), a novel recurrent neural network architecture aimed at overcoming two major limitations in standard LSTM and GRU models: redundant gate-specific parameters and the difficulty in retaining long-range temporal information. First, the Parameter-Shared Unified Gating (PSUG) mechanism consolidates all gate-specific transformations into a single shared weight matrix, resulting in approximately a 48% reduction in parameters while maintaining the full gating functionality vital for model performance. Second, the Hierarchical Gated Recurrence with Additive Skip Connections (HGR-ASC) component introduces a multiplication-free pathway designed to facilitate better long-range information flow throughout the sequence, thereby mitigating the degradation typically caused by the forget gate in traditional recurrent models. The authors evaluate QL-LSTM on the IMDB sentiment classification dataset with extended-length documents, comparing it against standard LSTM, GRU, and BiLSTM baselines. Results show that QL-LSTM achieves competitive accuracy levels despite having substantially fewer parameters. Despite these improvements in parameter efficiency and step-wise computational cost, the current QL-LSTM prototype does not provide wall-clock speed advantages due to the sequential processing nature of recurrent architectures, indicating a need for future kernel-level optimizations to realize practical runtime speedups. <div>
arXiv:2512.06582v1 Announce Type: new 
Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On fine-tuning Boltz-2 for protein-protein affinity prediction</title>
<link>https://arxiv.org/abs/2512.06592</link>
<guid>https://arxiv.org/abs/2512.06592</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-protein binding affinity, Boltz-2, structure-based prediction, sequence-based models, molecular interactions<br /><br />Summary:<br />1. The accurate prediction of protein-protein binding affinity is essential for understanding molecular interactions and advancing therapeutic design. <br />2. The study adapts Boltz-2, originally a state-of-the-art protein-ligand affinity predictor based on structural information, for protein-protein affinity regression tasks. <br />3. Evaluation was conducted on two datasets: TCR3d and PPB-affinity. Despite Boltz-2-PPI delivering high structural accuracy, it performed worse than sequence-based prediction models across both small and large data scenarios. <br />4. Combining the structural embeddings from Boltz-2-PPI with embeddings derived from sequence-based models resulted in complementary performance improvements, particularly when paired with weaker sequence-based models. This implies that sequence- and structure-based methods capture different, complementary signals. <br />5. The findings highlight inherent biases in training with structural data and suggest that current structure-based representations are not yet optimized or sufficiently primed for highly accurate protein-protein affinity prediction, encouraging further research in this area. <div>
arXiv:2512.06592v1 Announce Type: new 
Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs</title>
<link>https://arxiv.org/abs/2512.06607</link>
<guid>https://arxiv.org/abs/2512.06607</guid>
<content:encoded><![CDATA[
<div> finance, large language models, look-ahead bias, model fine-tuning, inference-time adjustment

<br /><br />Summary: This paper addresses the challenge of applying large language models (LLMs) to predictive tasks in finance, where look-ahead bias arises due to models being trained on extensive time-series data. Traditional backtesting methods are problematic because retraining cutting-edge models from scratch with a specific knowledge cutoff is computationally expensive and time-consuming. To overcome this, the authors propose a novel, fast, and cost-efficient approach that modifies the model’s output probabilities (logits) during inference rather than retraining. Their method involves using two smaller, specialized models: one fine-tuned on the data that should be "forgotten" and another on data that should be retained. By leveraging these models to adjust the logits of a large base model, the approach effectively removes both direct (verbatim) and underlying (semantic) knowledge that could cause look-ahead bias. The method also corrects biases in predictions and outperforms previously established techniques aimed at mitigating similar issues. This innovation enables more reliable backtests in financial modeling without the prohibitive costs associated with retraining large-scale models. <div>
arXiv:2512.06607v1 Announce Type: new 
Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantization using Gaussian Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.06609</link>
<guid>https://arxiv.org/abs/2512.06609</guid>
<content:encoded><![CDATA[
<div> Vector quantized variational autoencoder, Gaussian VAE, Gaussian Quant, target divergence constraint, image compression<br /><br />Summary:<br /><br />This paper addresses the challenge of training Vector Quantized Variational Autoencoders (VQ-VAEs), which are discrete auto-encoders used to compress images into discrete tokens but are difficult to optimize due to discretization. The authors propose Gaussian Quant (GQ), a novel method that converts a Gaussian VAE into a VQ-VAE without the need for additional training by using a random Gaussian noise codebook and selecting the closest noise vector to the posterior mean. They provide theoretical guarantees showing that a sufficiently large codebook size (logarithm exceeding the bits-back coding rate) ensures minimal quantization error. To enhance practical effectiveness, the paper introduces the target divergence constraint (TDC), a heuristic training strategy for Gaussian VAEs that improves the performance of GQ. Empirical results demonstrate that GQ surpasses existing VQ-VAE variants, including VQGAN, FSQ, LFQ, and BSQ, across both UNet and ViT backbone architectures. Additionally, TDC shows improvements over previous Gaussian VAE discretization methods like TokenBridge. The authors also provide the source code publicly, enabling reproducibility and further research. This work contributes a simple yet effective framework for bridging Gaussian VAE and discrete VQ-VAE approaches for image compression. <div>
arXiv:2512.06609v1 Announce Type: new 
Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study</title>
<link>https://arxiv.org/abs/2512.06630</link>
<guid>https://arxiv.org/abs/2512.06630</guid>
<content:encoded><![CDATA[
<div> Quantum machine learning, temporal convolutional neural network, stock market prediction, quantum convolution circuits, Sharpe ratio<br /><br />Summary:<br /><br />This paper addresses the challenges faced by classical forecasting models in stock market prediction, such as noisy inputs, regime shifts, and limited generalization. It proposes a Quantum Temporal Convolutional Neural Network (QTCNN) that integrates a classical temporal encoder with parameter-efficient quantum convolution circuits for improved cross-sectional equity return prediction. The temporal encoder extracts multi-scale sequential patterns from technical indicators, while the quantum circuits exploit superposition and entanglement to enhance feature representation and reduce overfitting. The authors benchmark QTCNN on the JPX Tokyo Stock Exchange dataset and evaluate its performance via long-short portfolio strategies using the out-of-sample Sharpe ratio. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by about 72%. This demonstrates that quantum-enhanced models, like QTCNN, have significant practical potential for robust and accurate decision-making in quantitative finance. The study highlights the synergy between classical temporal encoding and quantum processing, offering a novel approach to cope with complex, noisy financial environments. The results pave the way for further research and application of quantum machine learning in financial forecasting tasks. <div>
arXiv:2512.06630v1 Announce Type: new 
Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News</title>
<link>https://arxiv.org/abs/2512.06638</link>
<guid>https://arxiv.org/abs/2512.06638</guid>
<content:encoded><![CDATA[
arXiv:2512.06638v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[
arXiv:2512.06648v1 Announce Type: new 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</title>
<link>https://arxiv.org/abs/2512.06649</link>
<guid>https://arxiv.org/abs/2512.06649</guid>
<content:encoded><![CDATA[
arXiv:2512.06649v1 Announce Type: new 
Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts</title>
<link>https://arxiv.org/abs/2512.06652</link>
<guid>https://arxiv.org/abs/2512.06652</guid>
<content:encoded><![CDATA[
arXiv:2512.06652v1 Announce Type: new 
Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title>
<link>https://arxiv.org/abs/2512.06655</link>
<guid>https://arxiv.org/abs/2512.06655</guid>
<content:encoded><![CDATA[
arXiv:2512.06655v1 Announce Type: new 
Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[
arXiv:2512.06665v1 Announce Type: new 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification</title>
<link>https://arxiv.org/abs/2512.06666</link>
<guid>https://arxiv.org/abs/2512.06666</guid>
<content:encoded><![CDATA[
arXiv:2512.06666v1 Announce Type: new 
Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning</title>
<link>https://arxiv.org/abs/2512.06678</link>
<guid>https://arxiv.org/abs/2512.06678</guid>
<content:encoded><![CDATA[
arXiv:2512.06678v1 Announce Type: new 
Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State Diversity Matters in Offline Behavior Distillation</title>
<link>https://arxiv.org/abs/2512.06692</link>
<guid>https://arxiv.org/abs/2512.06692</guid>
<content:encoded><![CDATA[
arXiv:2512.06692v1 Announce Type: new 
Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Barren plateaus in quantum denoising diffusion probabilistic models</title>
<link>https://arxiv.org/abs/2512.06695</link>
<guid>https://arxiv.org/abs/2512.06695</guid>
<content:encoded><![CDATA[
arXiv:2512.06695v1 Announce Type: new 
Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models</title>
<link>https://arxiv.org/abs/2512.06702</link>
<guid>https://arxiv.org/abs/2512.06702</guid>
<content:encoded><![CDATA[
arXiv:2512.06702v1 Announce Type: new 
Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the F\"{o}llmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations</title>
<link>https://arxiv.org/abs/2512.06708</link>
<guid>https://arxiv.org/abs/2512.06708</guid>
<content:encoded><![CDATA[
arXiv:2512.06708v1 Announce Type: new 
Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting</title>
<link>https://arxiv.org/abs/2512.06714</link>
<guid>https://arxiv.org/abs/2512.06714</guid>
<content:encoded><![CDATA[
arXiv:2512.06714v1 Announce Type: new 
Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Motor Behavior Using Deep Learning and Reservoir Computing</title>
<link>https://arxiv.org/abs/2512.06725</link>
<guid>https://arxiv.org/abs/2512.06725</guid>
<content:encoded><![CDATA[
arXiv:2512.06725v1 Announce Type: new 
Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models</title>
<link>https://arxiv.org/abs/2512.06727</link>
<guid>https://arxiv.org/abs/2512.06727</guid>
<content:encoded><![CDATA[
arXiv:2512.06727v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</title>
<link>https://arxiv.org/abs/2512.06730</link>
<guid>https://arxiv.org/abs/2512.06730</guid>
<content:encoded><![CDATA[
arXiv:2512.06730v1 Announce Type: new 
Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[
arXiv:2512.06737v1 Announce Type: new 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets</title>
<link>https://arxiv.org/abs/2512.06752</link>
<guid>https://arxiv.org/abs/2512.06752</guid>
<content:encoded><![CDATA[
arXiv:2512.06752v1 Announce Type: new 
Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship</title>
<link>https://arxiv.org/abs/2512.06758</link>
<guid>https://arxiv.org/abs/2512.06758</guid>
<content:encoded><![CDATA[
arXiv:2512.06758v1 Announce Type: new 
Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $\Omega\left( \frac{N\log(T)}{\Delta^2} + \frac{K\log(T)}{\Delta} \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $\Delta$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{\Delta^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{\Delta^2} + \frac{K\log(T)}{\Delta} \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Over-smoothing beyond Dirichlet energy</title>
<link>https://arxiv.org/abs/2512.06782</link>
<guid>https://arxiv.org/abs/2512.06782</guid>
<content:encoded><![CDATA[
arXiv:2512.06782v1 Announce Type: new 
Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Regularization for Positive-Unlabeled Learning on the Hypersphere</title>
<link>https://arxiv.org/abs/2512.06785</link>
<guid>https://arxiv.org/abs/2512.06785</guid>
<content:encoded><![CDATA[
arXiv:2512.06785v1 Announce Type: new 
Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</title>
<link>https://arxiv.org/abs/2512.06791</link>
<guid>https://arxiv.org/abs/2512.06791</guid>
<content:encoded><![CDATA[
arXiv:2512.06791v1 Announce Type: new 
Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation</title>
<link>https://arxiv.org/abs/2512.06813</link>
<guid>https://arxiv.org/abs/2512.06813</guid>
<content:encoded><![CDATA[
arXiv:2512.06813v1 Announce Type: new 
Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Factorization-based Bearing Fault Diagnosis</title>
<link>https://arxiv.org/abs/2512.06837</link>
<guid>https://arxiv.org/abs/2512.06837</guid>
<content:encoded><![CDATA[
arXiv:2512.06837v1 Announce Type: new 
Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis</title>
<link>https://arxiv.org/abs/2512.06917</link>
<guid>https://arxiv.org/abs/2512.06917</guid>
<content:encoded><![CDATA[
arXiv:2512.06917v1 Announce Type: new 
Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models</title>
<link>https://arxiv.org/abs/2512.06920</link>
<guid>https://arxiv.org/abs/2512.06920</guid>
<content:encoded><![CDATA[
arXiv:2512.06920v1 Announce Type: new 
Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features</title>
<link>https://arxiv.org/abs/2512.06925</link>
<guid>https://arxiv.org/abs/2512.06925</guid>
<content:encoded><![CDATA[
arXiv:2512.06925v1 Announce Type: new 
Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise</title>
<link>https://arxiv.org/abs/2512.06926</link>
<guid>https://arxiv.org/abs/2512.06926</guid>
<content:encoded><![CDATA[
arXiv:2512.06926v1 Announce Type: new 
Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding</title>
<link>https://arxiv.org/abs/2512.06929</link>
<guid>https://arxiv.org/abs/2512.06929</guid>
<content:encoded><![CDATA[
arXiv:2512.06929v1 Announce Type: new 
Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies</title>
<link>https://arxiv.org/abs/2512.06932</link>
<guid>https://arxiv.org/abs/2512.06932</guid>
<content:encoded><![CDATA[
arXiv:2512.06932v1 Announce Type: new 
Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unifying Human-Centered AI Fairness Framework</title>
<link>https://arxiv.org/abs/2512.06944</link>
<guid>https://arxiv.org/abs/2512.06944</guid>
<content:encoded><![CDATA[
arXiv:2512.06944v1 Announce Type: new 
Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing BFGS and OGR for Second-Order Optimization</title>
<link>https://arxiv.org/abs/2512.06969</link>
<guid>https://arxiv.org/abs/2512.06969</guid>
<content:encoded><![CDATA[
arXiv:2512.06969v1 Announce Type: new 
Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction with Expert Advice under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2512.06971</link>
<guid>https://arxiv.org/abs/2512.06971</guid>
<content:encoded><![CDATA[
arXiv:2512.06971v1 Announce Type: new 
Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding</title>
<link>https://arxiv.org/abs/2512.06982</link>
<guid>https://arxiv.org/abs/2512.06982</guid>
<content:encoded><![CDATA[
arXiv:2512.06982v1 Announce Type: new 
Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.06987</link>
<guid>https://arxiv.org/abs/2512.06987</guid>
<content:encoded><![CDATA[
arXiv:2512.06987v1 Announce Type: new 
Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ {\AA} and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash Multi-Head Feed-Forward Network</title>
<link>https://arxiv.org/abs/2512.06989</link>
<guid>https://arxiv.org/abs/2512.06989</guid>
<content:encoded><![CDATA[
arXiv:2512.06989v1 Announce Type: new 
Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation</title>
<link>https://arxiv.org/abs/2512.06993</link>
<guid>https://arxiv.org/abs/2512.06993</guid>
<content:encoded><![CDATA[
arXiv:2512.06993v1 Announce Type: new 
Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.07010</link>
<guid>https://arxiv.org/abs/2512.07010</guid>
<content:encoded><![CDATA[
arXiv:2512.07010v1 Announce Type: new 
Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Sparse Flash Attention</title>
<link>https://arxiv.org/abs/2512.07011</link>
<guid>https://arxiv.org/abs/2512.07011</guid>
<content:encoded><![CDATA[
arXiv:2512.07011v1 Announce Type: new 
Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferring Clinical Knowledge into ECGs Representation</title>
<link>https://arxiv.org/abs/2512.07021</link>
<guid>https://arxiv.org/abs/2512.07021</guid>
<content:encoded><![CDATA[
arXiv:2512.07021v1 Announce Type: new 
Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</title>
<link>https://arxiv.org/abs/2512.07040</link>
<guid>https://arxiv.org/abs/2512.07040</guid>
<content:encoded><![CDATA[
arXiv:2512.07040v1 Announce Type: new 
Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design</title>
<link>https://arxiv.org/abs/2512.07064</link>
<guid>https://arxiv.org/abs/2512.07064</guid>
<content:encoded><![CDATA[
arXiv:2512.07064v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation</title>
<link>https://arxiv.org/abs/2512.07079</link>
<guid>https://arxiv.org/abs/2512.07079</guid>
<content:encoded><![CDATA[
arXiv:2512.07079v1 Announce Type: new 
Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization</title>
<link>https://arxiv.org/abs/2512.07082</link>
<guid>https://arxiv.org/abs/2512.07082</guid>
<content:encoded><![CDATA[
arXiv:2512.07082v1 Announce Type: new 
Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07092</link>
<guid>https://arxiv.org/abs/2512.07092</guid>
<content:encoded><![CDATA[
arXiv:2512.07092v1 Announce Type: new 
Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph</title>
<link>https://arxiv.org/abs/2512.07100</link>
<guid>https://arxiv.org/abs/2512.07100</guid>
<content:encoded><![CDATA[
arXiv:2512.07100v1 Announce Type: new 
Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOAM: Blocked State Folding for Memory-Efficient LLM Training</title>
<link>https://arxiv.org/abs/2512.07112</link>
<guid>https://arxiv.org/abs/2512.07112</guid>
<content:encoded><![CDATA[
arXiv:2512.07112v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes</title>
<link>https://arxiv.org/abs/2512.07113</link>
<guid>https://arxiv.org/abs/2512.07113</guid>
<content:encoded><![CDATA[
arXiv:2512.07113v1 Announce Type: new 
Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[
arXiv:2512.07142v1 Announce Type: new 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[
arXiv:2512.07150v1 Announce Type: new 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration</title>
<link>https://arxiv.org/abs/2512.07173</link>
<guid>https://arxiv.org/abs/2512.07173</guid>
<content:encoded><![CDATA[
arXiv:2512.07173v1 Announce Type: new 
Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models</title>
<link>https://arxiv.org/abs/2512.07175</link>
<guid>https://arxiv.org/abs/2512.07175</guid>
<content:encoded><![CDATA[
arXiv:2512.07175v1 Announce Type: new 
Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07184</link>
<guid>https://arxiv.org/abs/2512.07184</guid>
<content:encoded><![CDATA[
arXiv:2512.07184v1 Announce Type: new 
Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction</title>
<link>https://arxiv.org/abs/2512.07200</link>
<guid>https://arxiv.org/abs/2512.07200</guid>
<content:encoded><![CDATA[
arXiv:2512.07200v1 Announce Type: new 
Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Federated Prompt Calibration</title>
<link>https://arxiv.org/abs/2512.07208</link>
<guid>https://arxiv.org/abs/2512.07208</guid>
<content:encoded><![CDATA[
arXiv:2512.07208v1 Announce Type: new 
Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($\beta$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($\beta$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07222</link>
<guid>https://arxiv.org/abs/2512.07222</guid>
<content:encoded><![CDATA[
arXiv:2512.07222v1 Announce Type: new 
Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PINE: Pipeline for Important Node Exploration in Attributed Networks</title>
<link>https://arxiv.org/abs/2512.07244</link>
<guid>https://arxiv.org/abs/2512.07244</guid>
<content:encoded><![CDATA[
arXiv:2512.07244v1 Announce Type: new 
Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFFair: Influence Function-driven Sample Reweighting for Fair Classification</title>
<link>https://arxiv.org/abs/2512.07249</link>
<guid>https://arxiv.org/abs/2512.07249</guid>
<content:encoded><![CDATA[
arXiv:2512.07249v1 Announce Type: new 
Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents</title>
<link>https://arxiv.org/abs/2512.07287</link>
<guid>https://arxiv.org/abs/2512.07287</guid>
<content:encoded><![CDATA[
arXiv:2512.07287v1 Announce Type: new 
Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Relationship-Aware Transformer for Tabular Data</title>
<link>https://arxiv.org/abs/2512.07310</link>
<guid>https://arxiv.org/abs/2512.07310</guid>
<content:encoded><![CDATA[
arXiv:2512.07310v1 Announce Type: new 
Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach</title>
<link>https://arxiv.org/abs/2512.07313</link>
<guid>https://arxiv.org/abs/2512.07313</guid>
<content:encoded><![CDATA[
arXiv:2512.07313v1 Announce Type: new 
Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach</title>
<link>https://arxiv.org/abs/2512.07332</link>
<guid>https://arxiv.org/abs/2512.07332</guid>
<content:encoded><![CDATA[
arXiv:2512.07332v1 Announce Type: new 
Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</title>
<link>https://arxiv.org/abs/2512.07374</link>
<guid>https://arxiv.org/abs/2512.07374</guid>
<content:encoded><![CDATA[
arXiv:2512.07374v1 Announce Type: new 
Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</title>
<link>https://arxiv.org/abs/2512.07375</link>
<guid>https://arxiv.org/abs/2512.07375</guid>
<content:encoded><![CDATA[
arXiv:2512.07375v1 Announce Type: new 
Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</title>
<link>https://arxiv.org/abs/2512.07390</link>
<guid>https://arxiv.org/abs/2512.07390</guid>
<content:encoded><![CDATA[
arXiv:2512.07390v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects</title>
<link>https://arxiv.org/abs/2512.07393</link>
<guid>https://arxiv.org/abs/2512.07393</guid>
<content:encoded><![CDATA[
arXiv:2512.07393v1 Announce Type: new 
Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse</title>
<link>https://arxiv.org/abs/2512.07400</link>
<guid>https://arxiv.org/abs/2512.07400</guid>
<content:encoded><![CDATA[
arXiv:2512.07400v1 Announce Type: new 
Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07417</link>
<guid>https://arxiv.org/abs/2512.07417</guid>
<content:encoded><![CDATA[
arXiv:2512.07417v1 Announce Type: new 
Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</title>
<link>https://arxiv.org/abs/2512.07419</link>
<guid>https://arxiv.org/abs/2512.07419</guid>
<content:encoded><![CDATA[
arXiv:2512.07419v1 Announce Type: new 
Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.07430</link>
<guid>https://arxiv.org/abs/2512.07430</guid>
<content:encoded><![CDATA[
arXiv:2512.07430v1 Announce Type: new 
Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias in Graph Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2512.07433</link>
<guid>https://arxiv.org/abs/2512.07433</guid>
<content:encoded><![CDATA[
arXiv:2512.07433v1 Announce Type: new 
Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</title>
<link>https://arxiv.org/abs/2512.07437</link>
<guid>https://arxiv.org/abs/2512.07437</guid>
<content:encoded><![CDATA[
arXiv:2512.07437v1 Announce Type: new 
Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forget and Explain: Transparent Verification of GNN Unlearning</title>
<link>https://arxiv.org/abs/2512.07450</link>
<guid>https://arxiv.org/abs/2512.07450</guid>
<content:encoded><![CDATA[
arXiv:2512.07450v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification</title>
<link>https://arxiv.org/abs/2512.07463</link>
<guid>https://arxiv.org/abs/2512.07463</guid>
<content:encoded><![CDATA[
arXiv:2512.07463v1 Announce Type: new 
Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Materium: An Autoregressive Approach for Material Generation</title>
<link>https://arxiv.org/abs/2512.07486</link>
<guid>https://arxiv.org/abs/2512.07486</guid>
<content:encoded><![CDATA[
arXiv:2512.07486v1 Announce Type: new 
Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07490</link>
<guid>https://arxiv.org/abs/2512.07490</guid>
<content:encoded><![CDATA[
arXiv:2512.07490v1 Announce Type: new 
Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</title>
<link>https://arxiv.org/abs/2512.07509</link>
<guid>https://arxiv.org/abs/2512.07509</guid>
<content:encoded><![CDATA[
arXiv:2512.07509v1 Announce Type: new 
Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning: Progress and Prospects</title>
<link>https://arxiv.org/abs/2512.07519</link>
<guid>https://arxiv.org/abs/2512.07519</guid>
<content:encoded><![CDATA[
arXiv:2512.07519v1 Announce Type: new 
Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reinforcement Learning Under Confounding</title>
<link>https://arxiv.org/abs/2512.07528</link>
<guid>https://arxiv.org/abs/2512.07528</guid>
<content:encoded><![CDATA[
arXiv:2512.07528v1 Announce Type: new 
Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07539</link>
<guid>https://arxiv.org/abs/2512.07539</guid>
<content:encoded><![CDATA[
arXiv:2512.07539v1 Announce Type: new 
Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.07542</link>
<guid>https://arxiv.org/abs/2512.07542</guid>
<content:encoded><![CDATA[
arXiv:2512.07542v1 Announce Type: new 
Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07558</link>
<guid>https://arxiv.org/abs/2512.07558</guid>
<content:encoded><![CDATA[
arXiv:2512.07558v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07569</link>
<guid>https://arxiv.org/abs/2512.07569</guid>
<content:encoded><![CDATA[
arXiv:2512.07569v1 Announce Type: new 
Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time Series Foundation Models for Process Model Forecasting</title>
<link>https://arxiv.org/abs/2512.07624</link>
<guid>https://arxiv.org/abs/2512.07624</guid>
<content:encoded><![CDATA[
arXiv:2512.07624v1 Announce Type: new 
Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance</title>
<link>https://arxiv.org/abs/2512.07647</link>
<guid>https://arxiv.org/abs/2512.07647</guid>
<content:encoded><![CDATA[
arXiv:2512.07647v1 Announce Type: new 
Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=\tau\|\mu_{\mathrm{tail}}-\mu_{\mathrm{head}}\|_2$ with $\tau=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\le\tau\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(\mu,\sigma^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approx\Phi_c(\sigma+\Phi^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-Wise Activation Steering for Honest Language Models</title>
<link>https://arxiv.org/abs/2512.07667</link>
<guid>https://arxiv.org/abs/2512.07667</guid>
<content:encoded><![CDATA[
arXiv:2512.07667v1 Announce Type: new 
Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bootstrap Perspective on Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07676</link>
<guid>https://arxiv.org/abs/2512.07676</guid>
<content:encoded><![CDATA[
arXiv:2512.07676v1 Announce Type: new 
Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models</title>
<link>https://arxiv.org/abs/2512.07705</link>
<guid>https://arxiv.org/abs/2512.07705</guid>
<content:encoded><![CDATA[
arXiv:2512.07705v1 Announce Type: new 
Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity</title>
<link>https://arxiv.org/abs/2512.07723</link>
<guid>https://arxiv.org/abs/2512.07723</guid>
<content:encoded><![CDATA[
arXiv:2512.07723v1 Announce Type: new 
Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data</title>
<link>https://arxiv.org/abs/2512.07741</link>
<guid>https://arxiv.org/abs/2512.07741</guid>
<content:encoded><![CDATA[
arXiv:2512.07741v1 Announce Type: new 
Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formalized Hopfield Networks and Boltzmann Machines</title>
<link>https://arxiv.org/abs/2512.07766</link>
<guid>https://arxiv.org/abs/2512.07766</guid>
<content:encoded><![CDATA[
arXiv:2512.07766v1 Announce Type: new 
Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory</title>
<link>https://arxiv.org/abs/2512.07782</link>
<guid>https://arxiv.org/abs/2512.07782</guid>
<content:encoded><![CDATA[
arXiv:2512.07782v1 Announce Type: new 
Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Representational Position Encoding</title>
<link>https://arxiv.org/abs/2512.07805</link>
<guid>https://arxiv.org/abs/2512.07805</guid>
<content:encoded><![CDATA[
arXiv:2512.07805v1 Announce Type: new 
Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,\omega\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Long-Range Benefits of Next-Token Prediction</title>
<link>https://arxiv.org/abs/2512.07818</link>
<guid>https://arxiv.org/abs/2512.07818</guid>
<content:encoded><![CDATA[
arXiv:2512.07818v1 Announce Type: new 
Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adoption and Usage of AI Agents: Early Evidence from Perplexity</title>
<link>https://arxiv.org/abs/2512.07828</link>
<guid>https://arxiv.org/abs/2512.07828</guid>
<content:encoded><![CDATA[
arXiv:2512.07828v1 Announce Type: new 
Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation</title>
<link>https://arxiv.org/abs/2512.02195</link>
<guid>https://arxiv.org/abs/2512.02195</guid>
<content:encoded><![CDATA[
arXiv:2512.02195v1 Announce Type: cross 
Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL</title>
<link>https://arxiv.org/abs/2512.02272</link>
<guid>https://arxiv.org/abs/2512.02272</guid>
<content:encoded><![CDATA[
arXiv:2512.02272v1 Announce Type: cross 
Abstract: This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Enhanced Deep Surrogates for the Phonon Boltzmann Transport Equation</title>
<link>https://arxiv.org/abs/2512.05976</link>
<guid>https://arxiv.org/abs/2512.05976</guid>
<content:encoded><![CDATA[
arXiv:2512.05976v1 Announce Type: cross 
Abstract: Designing materials with controlled heat flow at the nano-scale is central to advances in microelectronics, thermoelectrics, and energy-conversion technologies. At these scales, phonon transport follows the Boltzmann Transport Equation (BTE), which captures non-diffusive (ballistic) effects but is too costly to solve repeatedly in inverse-design loops. Existing surrogate approaches trade speed for accuracy: fast macroscopic solvers can overestimate conductivities by hundreds of percent, while recent data-driven operator learners often require thousands of high-fidelity simulations. This creates a need for a fast, data-efficient surrogate that remains reliable across ballistic and diffusive regimes. We introduce a Physics-Enhanced Deep Surrogate (PEDS) that combines a differentiable Fourier solver with a neural generator and couples it with uncertainty-driven active learning. The Fourier solver acts as a physical inductive bias, while the network learns geometry-dependent corrections and a mixing coefficient that interpolates between macroscopic and nano-scale behavior. PEDS reduces training-data requirements by up to 70% compared with purely data-driven baselines, achieves roughly 5% fractional error with only 300 high-fidelity BTE simulations, and enables efficient design of porous geometries spanning 12-85 W m$^{-1}$ K$^{-1}$ with average design errors of 4%. The learned mixing parameter recovers the ballistic-diffusive transition and improves out of distribution robustness. These results show that embedding simple, differentiable low-fidelity physics can dramatically increase surrogate data-efficiency and interpretability, making repeated PDE-constrained optimization practical for nano-scale thermal-materials design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction</title>
<link>https://arxiv.org/abs/2512.05979</link>
<guid>https://arxiv.org/abs/2512.05979</guid>
<content:encoded><![CDATA[
arXiv:2512.05979v1 Announce Type: cross 
Abstract: Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[
arXiv:2512.05988v1 Announce Type: cross 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals</title>
<link>https://arxiv.org/abs/2512.05998</link>
<guid>https://arxiv.org/abs/2512.05998</guid>
<content:encoded><![CDATA[
arXiv:2512.05998v1 Announce Type: cross 
Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-resolution Physics-Aware Recurrent Convolutional Neural Network for Complex Flows</title>
<link>https://arxiv.org/abs/2512.06031</link>
<guid>https://arxiv.org/abs/2512.06031</guid>
<content:encoded><![CDATA[
arXiv:2512.06031v1 Announce Type: cross 
Abstract: We present MRPARCv2, Multi-resolution Physics-Aware Recurrent Convolutional Neural Network, designed to model complex flows by embedding the structure of advection-diffusion-reaction equations and leveraging a multi-resolution architecture. MRPARCv2 introduces hierarchical discretization and cross-resolution feature communication to improve the accuracy and efficiency of flow simulations. We evaluate the model on a challenging 2D turbulent radiative layer dataset from The Well multi-physics benchmark repository and demonstrate significant improvements when compared to the single resolution baseline model, in both Variance Scaled Root Mean Squared Error and physics-driven metrics, including turbulent kinetic energy spectra and mass-temperature distributions. Despite having 30% fewer trainable parameters, MRPARCv2 outperforms its predecessor by up to 50% in roll-out prediction error and 86% in spectral error. A preliminary study on uncertainty quantification was performed, and we also analyzed the model's performance under different levels of abstractions of the flow, specifically on sampling subsets of field variables. We find that the absence of physical constraints on the equation of state (EOS) in the network architecture leads to degraded accuracy. A variable substitution experiment confirms that this issue persists regardless of which physical quantity is predicted directly. Our findings highlight the advantages of multi-resolution inductive bias for capturing multi-scale flow dynamics and suggest the need for future PIML models to embed EOS knowledge to enhance physical fidelity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction</title>
<link>https://arxiv.org/abs/2512.06038</link>
<guid>https://arxiv.org/abs/2512.06038</guid>
<content:encoded><![CDATA[
arXiv:2512.06038v1 Announce Type: cross 
Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Road of Adaptive AI for Precision in Cybersecurity</title>
<link>https://arxiv.org/abs/2512.06048</link>
<guid>https://arxiv.org/abs/2512.06048</guid>
<content:encoded><![CDATA[
arXiv:2512.06048v1 Announce Type: cross 
Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions</title>
<link>https://arxiv.org/abs/2512.06109</link>
<guid>https://arxiv.org/abs/2512.06109</guid>
<content:encoded><![CDATA[
arXiv:2512.06109v1 Announce Type: cross 
Abstract: This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures</title>
<link>https://arxiv.org/abs/2512.06113</link>
<guid>https://arxiv.org/abs/2512.06113</guid>
<content:encoded><![CDATA[
arXiv:2512.06113v1 Announce Type: cross 
Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach</title>
<link>https://arxiv.org/abs/2512.06161</link>
<guid>https://arxiv.org/abs/2512.06161</guid>
<content:encoded><![CDATA[
arXiv:2512.06161v1 Announce Type: cross 
Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data</title>
<link>https://arxiv.org/abs/2512.06181</link>
<guid>https://arxiv.org/abs/2512.06181</guid>
<content:encoded><![CDATA[
arXiv:2512.06181v1 Announce Type: cross 
Abstract: Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.
  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.
  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.
  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.
  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[
arXiv:2512.06190v1 Announce Type: cross 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On measuring grounding and generalizing grounding problems</title>
<link>https://arxiv.org/abs/2512.06205</link>
<guid>https://arxiv.org/abs/2512.06205</guid>
<content:encoded><![CDATA[
arXiv:2512.06205v1 Announce Type: cross 
Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning</title>
<link>https://arxiv.org/abs/2512.06206</link>
<guid>https://arxiv.org/abs/2512.06206</guid>
<content:encoded><![CDATA[
arXiv:2512.06206v1 Announce Type: cross 
Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparsePixels: Efficient Convolution for Sparse Data on FPGAs</title>
<link>https://arxiv.org/abs/2512.06208</link>
<guid>https://arxiv.org/abs/2512.06208</guid>
<content:encoded><![CDATA[
arXiv:2512.06208v1 Announce Type: cross 
Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\mu$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $\mu$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict</title>
<link>https://arxiv.org/abs/2512.06210</link>
<guid>https://arxiv.org/abs/2512.06210</guid>
<content:encoded><![CDATA[
arXiv:2512.06210v1 Announce Type: cross 
Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Broader View on Clustering under Cluster-Aware Norm Objectives</title>
<link>https://arxiv.org/abs/2512.06211</link>
<guid>https://arxiv.org/abs/2512.06211</guid>
<content:encoded><![CDATA[
arXiv:2512.06211v1 Announce Type: cross 
Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety</title>
<link>https://arxiv.org/abs/2512.06227</link>
<guid>https://arxiv.org/abs/2512.06227</guid>
<content:encoded><![CDATA[
arXiv:2512.06227v1 Announce Type: cross 
Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Learning Intuitive Physics May Require More than Visual Data</title>
<link>https://arxiv.org/abs/2512.06232</link>
<guid>https://arxiv.org/abs/2512.06232</guid>
<content:encoded><![CDATA[
arXiv:2512.06232v1 Announce Type: cross 
Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling</title>
<link>https://arxiv.org/abs/2512.06259</link>
<guid>https://arxiv.org/abs/2512.06259</guid>
<content:encoded><![CDATA[
arXiv:2512.06259v1 Announce Type: cross 
Abstract: Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions</title>
<link>https://arxiv.org/abs/2512.06270</link>
<guid>https://arxiv.org/abs/2512.06270</guid>
<content:encoded><![CDATA[
arXiv:2512.06270v1 Announce Type: cross 
Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $\Gamma$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $\Gamma^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability</title>
<link>https://arxiv.org/abs/2512.06294</link>
<guid>https://arxiv.org/abs/2512.06294</guid>
<content:encoded><![CDATA[
arXiv:2512.06294v1 Announce Type: cross 
Abstract: Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2512.06348</link>
<guid>https://arxiv.org/abs/2512.06348</guid>
<content:encoded><![CDATA[
arXiv:2512.06348v1 Announce Type: cross 
Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Ni\~{n}o/Southern Oscillation (ENSO) index.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title>
<link>https://arxiv.org/abs/2512.06390</link>
<guid>https://arxiv.org/abs/2512.06390</guid>
<content:encoded><![CDATA[
arXiv:2512.06390v1 Announce Type: cross 
Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title>
<link>https://arxiv.org/abs/2512.06393</link>
<guid>https://arxiv.org/abs/2512.06393</guid>
<content:encoded><![CDATA[
arXiv:2512.06393v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</title>
<link>https://arxiv.org/abs/2512.06406</link>
<guid>https://arxiv.org/abs/2512.06406</guid>
<content:encoded><![CDATA[
arXiv:2512.06406v1 Announce Type: cross 
Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[
arXiv:2512.06421v1 Announce Type: cross 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening</title>
<link>https://arxiv.org/abs/2512.06434</link>
<guid>https://arxiv.org/abs/2512.06434</guid>
<content:encoded><![CDATA[
arXiv:2512.06434v1 Announce Type: cross 
Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals</title>
<link>https://arxiv.org/abs/2512.06435</link>
<guid>https://arxiv.org/abs/2512.06435</guid>
<content:encoded><![CDATA[
arXiv:2512.06435v1 Announce Type: cross 
Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning</title>
<link>https://arxiv.org/abs/2512.06496</link>
<guid>https://arxiv.org/abs/2512.06496</guid>
<content:encoded><![CDATA[
arXiv:2512.06496v1 Announce Type: cross 
Abstract: Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</title>
<link>https://arxiv.org/abs/2512.06530</link>
<guid>https://arxiv.org/abs/2512.06530</guid>
<content:encoded><![CDATA[
arXiv:2512.06530v1 Announce Type: cross 
Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[
arXiv:2512.06531v1 Announce Type: cross 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximate Multiplier Induced Error Propagation in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.06537</link>
<guid>https://arxiv.org/abs/2512.06537</guid>
<content:encoded><![CDATA[
arXiv:2512.06537v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Latent Variable Framework for Scaling Laws in Large Language Models</title>
<link>https://arxiv.org/abs/2512.06553</link>
<guid>https://arxiv.org/abs/2512.06553</guid>
<content:encoded><![CDATA[
arXiv:2512.06553v1 Announce Type: cross 
Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules</title>
<link>https://arxiv.org/abs/2512.06575</link>
<guid>https://arxiv.org/abs/2512.06575</guid>
<content:encoded><![CDATA[
arXiv:2512.06575v1 Announce Type: cross 
Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions</title>
<link>https://arxiv.org/abs/2512.06615</link>
<guid>https://arxiv.org/abs/2512.06615</guid>
<content:encoded><![CDATA[
arXiv:2512.06615v1 Announce Type: cross 
Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Hedge Swaptions</title>
<link>https://arxiv.org/abs/2512.06639</link>
<guid>https://arxiv.org/abs/2512.06639</guid>
<content:encoded><![CDATA[
arXiv:2512.06639v1 Announce Type: cross 
Abstract: This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[
arXiv:2512.06642v1 Announce Type: cross 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06676</link>
<guid>https://arxiv.org/abs/2512.06676</guid>
<content:encoded><![CDATA[
arXiv:2512.06676v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[
arXiv:2512.06699v1 Announce Type: cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Becoming Experienced Judges: Selective Test-Time Learning for Evaluators</title>
<link>https://arxiv.org/abs/2512.06751</link>
<guid>https://arxiv.org/abs/2512.06751</guid>
<content:encoded><![CDATA[
arXiv:2512.06751v1 Announce Type: cross 
Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADAM Optimization with Adaptive Batch Selection</title>
<link>https://arxiv.org/abs/2512.06795</link>
<guid>https://arxiv.org/abs/2512.06795</guid>
<content:encoded><![CDATA[
arXiv:2512.06795v1 Announce Type: cross 
Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal and Diffusion Transports in Machine Learning</title>
<link>https://arxiv.org/abs/2512.06797</link>
<guid>https://arxiv.org/abs/2512.06797</guid>
<content:encoded><![CDATA[
arXiv:2512.06797v1 Announce Type: cross 
Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems</title>
<link>https://arxiv.org/abs/2512.06809</link>
<guid>https://arxiv.org/abs/2512.06809</guid>
<content:encoded><![CDATA[
arXiv:2512.06809v1 Announce Type: cross 
Abstract: Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from "physical blindness" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[
arXiv:2512.06811v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</title>
<link>https://arxiv.org/abs/2512.06849</link>
<guid>https://arxiv.org/abs/2512.06849</guid>
<content:encoded><![CDATA[
arXiv:2512.06849v1 Announce Type: cross 
Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[
arXiv:2512.06866v1 Announce Type: cross 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINES: Explainable Anomaly Detection through Web API Invariant Inference</title>
<link>https://arxiv.org/abs/2512.06906</link>
<guid>https://arxiv.org/abs/2512.06906</guid>
<content:encoded><![CDATA[
arXiv:2512.06906v1 Announce Type: cross 
Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields</title>
<link>https://arxiv.org/abs/2512.06912</link>
<guid>https://arxiv.org/abs/2512.06912</guid>
<content:encoded><![CDATA[
arXiv:2512.06912v1 Announce Type: cross 
Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets</title>
<link>https://arxiv.org/abs/2512.06945</link>
<guid>https://arxiv.org/abs/2512.06945</guid>
<content:encoded><![CDATA[
arXiv:2512.06945v1 Announce Type: cross 
Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios</title>
<link>https://arxiv.org/abs/2512.06950</link>
<guid>https://arxiv.org/abs/2512.06950</guid>
<content:encoded><![CDATA[
arXiv:2512.06950v1 Announce Type: cross 
Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[
arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical analysis of Inverse Entropy-regularized Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06956</link>
<guid>https://arxiv.org/abs/2512.06956</guid>
<content:encoded><![CDATA[
arXiv:2512.06956v1 Announce Type: cross 
Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $\pi^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Conditional Independence Differential Graphs From Time-Dependent Data</title>
<link>https://arxiv.org/abs/2512.06960</link>
<guid>https://arxiv.org/abs/2512.06960</guid>
<content:encoded><![CDATA[
arXiv:2512.06960v1 Announce Type: cross 
Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control</title>
<link>https://arxiv.org/abs/2512.06973</link>
<guid>https://arxiv.org/abs/2512.06973</guid>
<content:encoded><![CDATA[
arXiv:2512.06973v1 Announce Type: cross 
Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging</title>
<link>https://arxiv.org/abs/2512.06977</link>
<guid>https://arxiv.org/abs/2512.06977</guid>
<content:encoded><![CDATA[
arXiv:2512.06977v1 Announce Type: cross 
Abstract: Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06981</link>
<guid>https://arxiv.org/abs/2512.06981</guid>
<content:encoded><![CDATA[
arXiv:2512.06981v1 Announce Type: cross 
Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Memory: A comparison of memory mechanisms in world models</title>
<link>https://arxiv.org/abs/2512.06983</link>
<guid>https://arxiv.org/abs/2512.06983</guid>
<content:encoded><![CDATA[
arXiv:2512.06983v1 Announce Type: cross 
Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing video analytics inference pipelines: a case study</title>
<link>https://arxiv.org/abs/2512.07009</link>
<guid>https://arxiv.org/abs/2512.07009</guid>
<content:encoded><![CDATA[
arXiv:2512.07009v1 Announce Type: cross 
Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data</title>
<link>https://arxiv.org/abs/2512.07030</link>
<guid>https://arxiv.org/abs/2512.07030</guid>
<content:encoded><![CDATA[
arXiv:2512.07030v1 Announce Type: cross 
Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating and Preserving High-level Fidelity in Super-Resolution</title>
<link>https://arxiv.org/abs/2512.07037</link>
<guid>https://arxiv.org/abs/2512.07037</guid>
<content:encoded><![CDATA[
arXiv:2512.07037v1 Announce Type: cross 
Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ideal Attribution and Faithful Watermarks for Language Models</title>
<link>https://arxiv.org/abs/2512.07038</link>
<guid>https://arxiv.org/abs/2512.07038</guid>
<content:encoded><![CDATA[
arXiv:2512.07038v1 Announce Type: cross 
Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[
arXiv:2512.07051v1 Announce Type: cross 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title>
<link>https://arxiv.org/abs/2512.07078</link>
<guid>https://arxiv.org/abs/2512.07078</guid>
<content:encoded><![CDATA[
arXiv:2512.07078v1 Announce Type: cross 
Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking</title>
<link>https://arxiv.org/abs/2512.07086</link>
<guid>https://arxiv.org/abs/2512.07086</guid>
<content:encoded><![CDATA[
arXiv:2512.07086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy</title>
<link>https://arxiv.org/abs/2512.07109</link>
<guid>https://arxiv.org/abs/2512.07109</guid>
<content:encoded><![CDATA[
arXiv:2512.07109v1 Announce Type: cross 
Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications</title>
<link>https://arxiv.org/abs/2512.07120</link>
<guid>https://arxiv.org/abs/2512.07120</guid>
<content:encoded><![CDATA[
arXiv:2512.07120v1 Announce Type: cross 
Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks</title>
<link>https://arxiv.org/abs/2512.07162</link>
<guid>https://arxiv.org/abs/2512.07162</guid>
<content:encoded><![CDATA[
arXiv:2512.07162v1 Announce Type: cross 
Abstract: Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</title>
<link>https://arxiv.org/abs/2512.07168</link>
<guid>https://arxiv.org/abs/2512.07168</guid>
<content:encoded><![CDATA[
arXiv:2512.07168v1 Announce Type: cross 
Abstract: We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation</title>
<link>https://arxiv.org/abs/2512.07178</link>
<guid>https://arxiv.org/abs/2512.07178</guid>
<content:encoded><![CDATA[
arXiv:2512.07178v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Diffusion Models via Code Execution</title>
<link>https://arxiv.org/abs/2512.07201</link>
<guid>https://arxiv.org/abs/2512.07201</guid>
<content:encoded><![CDATA[
arXiv:2512.07201v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</title>
<link>https://arxiv.org/abs/2512.07206</link>
<guid>https://arxiv.org/abs/2512.07206</guid>
<content:encoded><![CDATA[
arXiv:2512.07206v1 Announce Type: cross 
Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits</title>
<link>https://arxiv.org/abs/2512.07209</link>
<guid>https://arxiv.org/abs/2512.07209</guid>
<content:encoded><![CDATA[
arXiv:2512.07209v1 Announce Type: cross 
Abstract: We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation</title>
<link>https://arxiv.org/abs/2512.07212</link>
<guid>https://arxiv.org/abs/2512.07212</guid>
<content:encoded><![CDATA[
arXiv:2512.07212v1 Announce Type: cross 
Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling</title>
<link>https://arxiv.org/abs/2512.07216</link>
<guid>https://arxiv.org/abs/2512.07216</guid>
<content:encoded><![CDATA[
arXiv:2512.07216v1 Announce Type: cross 
Abstract: Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07218</link>
<guid>https://arxiv.org/abs/2512.07218</guid>
<content:encoded><![CDATA[
arXiv:2512.07218v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</title>
<link>https://arxiv.org/abs/2512.07224</link>
<guid>https://arxiv.org/abs/2512.07224</guid>
<content:encoded><![CDATA[
arXiv:2512.07224v1 Announce Type: cross 
Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[
arXiv:2512.07228v1 Announce Type: cross 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title>
<link>https://arxiv.org/abs/2512.07247</link>
<guid>https://arxiv.org/abs/2512.07247</guid>
<content:encoded><![CDATA[
arXiv:2512.07247v1 Announce Type: cross 
Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-negative DAG Learning from Time-Series Data</title>
<link>https://arxiv.org/abs/2512.07267</link>
<guid>https://arxiv.org/abs/2512.07267</guid>
<content:encoded><![CDATA[
arXiv:2512.07267v1 Announce Type: cross 
Abstract: This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</title>
<link>https://arxiv.org/abs/2512.07269</link>
<guid>https://arxiv.org/abs/2512.07269</guid>
<content:encoded><![CDATA[
arXiv:2512.07269v1 Announce Type: cross 
Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Deep Quantitative Group Testing</title>
<link>https://arxiv.org/abs/2512.07279</link>
<guid>https://arxiv.org/abs/2512.07279</guid>
<content:encoded><![CDATA[
arXiv:2512.07279v1 Announce Type: cross 
Abstract: We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Diffusion for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.07289</link>
<guid>https://arxiv.org/abs/2512.07289</guid>
<content:encoded><![CDATA[
arXiv:2512.07289v1 Announce Type: cross 
Abstract: In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Synthetic Populations for Scalable Societal and Market Modeling</title>
<link>https://arxiv.org/abs/2512.07306</link>
<guid>https://arxiv.org/abs/2512.07306</guid>
<content:encoded><![CDATA[
arXiv:2512.07306v1 Announce Type: cross 
Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling</title>
<link>https://arxiv.org/abs/2512.07314</link>
<guid>https://arxiv.org/abs/2512.07314</guid>
<content:encoded><![CDATA[
arXiv:2512.07314v1 Announce Type: cross 
Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-dimensional RMSD projections for reaction path visualization and validation</title>
<link>https://arxiv.org/abs/2512.07329</link>
<guid>https://arxiv.org/abs/2512.07329</guid>
<content:encoded><![CDATA[
arXiv:2512.07329v1 Announce Type: cross 
Abstract: Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning in an expectation-maximisation framework for nowcasting</title>
<link>https://arxiv.org/abs/2512.07335</link>
<guid>https://arxiv.org/abs/2512.07335</guid>
<content:encoded><![CDATA[
arXiv:2512.07335v1 Announce Type: cross 
Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07342</link>
<guid>https://arxiv.org/abs/2512.07342</guid>
<content:encoded><![CDATA[
arXiv:2512.07342v1 Announce Type: cross 
Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[
arXiv:2512.07355v1 Announce Type: cross 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs Trust the Code They Write?</title>
<link>https://arxiv.org/abs/2512.07404</link>
<guid>https://arxiv.org/abs/2512.07404</guid>
<content:encoded><![CDATA[
arXiv:2512.07404v1 Announce Type: cross 
Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microseismic event classification with a lightweight Fourier Neural Operator model</title>
<link>https://arxiv.org/abs/2512.07425</link>
<guid>https://arxiv.org/abs/2512.07425</guid>
<content:encoded><![CDATA[
arXiv:2512.07425v1 Announce Type: cross 
Abstract: Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems</title>
<link>https://arxiv.org/abs/2512.07458</link>
<guid>https://arxiv.org/abs/2512.07458</guid>
<content:encoded><![CDATA[
arXiv:2512.07458v1 Announce Type: cross 
Abstract: This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title>
<link>https://arxiv.org/abs/2512.07462</link>
<guid>https://arxiv.org/abs/2512.07462</guid>
<content:encoded><![CDATA[
arXiv:2512.07462v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.07472</link>
<guid>https://arxiv.org/abs/2512.07472</guid>
<content:encoded><![CDATA[
arXiv:2512.07472v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\pi_{0}$ and $\pi_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[
arXiv:2512.07540v1 Announce Type: cross 
Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Change Point Detection using Graph Spanning Ratio</title>
<link>https://arxiv.org/abs/2512.07541</link>
<guid>https://arxiv.org/abs/2512.07541</guid>
<content:encoded><![CDATA[
arXiv:2512.07541v1 Announce Type: cross 
Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series</title>
<link>https://arxiv.org/abs/2512.07557</link>
<guid>https://arxiv.org/abs/2512.07557</guid>
<content:encoded><![CDATA[
arXiv:2512.07557v1 Announce Type: cross 
Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[
arXiv:2512.07564v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\phi$-test: Global Feature Selection and Inference for Shapley Additive Explanations</title>
<link>https://arxiv.org/abs/2512.07578</link>
<guid>https://arxiv.org/abs/2512.07578</guid>
<content:encoded><![CDATA[
arXiv:2512.07578v1 Announce Type: cross 
Abstract: We propose $\phi$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\phi$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\phi$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\phi$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title>
<link>https://arxiv.org/abs/2512.07611</link>
<guid>https://arxiv.org/abs/2512.07611</guid>
<content:encoded><![CDATA[
arXiv:2512.07611v1 Announce Type: cross 
Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCMind-2.1-Kaiyuan-2B Technical Report</title>
<link>https://arxiv.org/abs/2512.07612</link>
<guid>https://arxiv.org/abs/2512.07612</guid>
<content:encoded><![CDATA[
arXiv:2512.07612v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds</title>
<link>https://arxiv.org/abs/2512.07631</link>
<guid>https://arxiv.org/abs/2512.07631</guid>
<content:encoded><![CDATA[
arXiv:2512.07631v1 Announce Type: cross 
Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation</title>
<link>https://arxiv.org/abs/2512.07650</link>
<guid>https://arxiv.org/abs/2512.07650</guid>
<content:encoded><![CDATA[
arXiv:2512.07650v1 Announce Type: cross 
Abstract: Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks</title>
<link>https://arxiv.org/abs/2512.07697</link>
<guid>https://arxiv.org/abs/2512.07697</guid>
<content:encoded><![CDATA[
arXiv:2512.07697v1 Announce Type: cross 
Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</title>
<link>https://arxiv.org/abs/2512.07703</link>
<guid>https://arxiv.org/abs/2512.07703</guid>
<content:encoded><![CDATA[
arXiv:2512.07703v1 Announce Type: cross 
Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE</title>
<link>https://arxiv.org/abs/2512.07710</link>
<guid>https://arxiv.org/abs/2512.07710</guid>
<content:encoded><![CDATA[
arXiv:2512.07710v1 Announce Type: cross 
Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A scalable and real-time neural decoder for topological quantum codes</title>
<link>https://arxiv.org/abs/2512.07737</link>
<guid>https://arxiv.org/abs/2512.07737</guid>
<content:encoded><![CDATA[
arXiv:2512.07737v1 Announce Type: cross 
Abstract: Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion</title>
<link>https://arxiv.org/abs/2512.07755</link>
<guid>https://arxiv.org/abs/2512.07755</guid>
<content:encoded><![CDATA[
arXiv:2512.07755v1 Announce Type: cross 
Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2512.07761</link>
<guid>https://arxiv.org/abs/2512.07761</guid>
<content:encoded><![CDATA[
arXiv:2512.07761v1 Announce Type: cross 
Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-informed Online Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.07770</link>
<guid>https://arxiv.org/abs/2512.07770</guid>
<content:encoded><![CDATA[
arXiv:2512.07770v1 Announce Type: cross 
Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.07795</link>
<guid>https://arxiv.org/abs/2512.07795</guid>
<content:encoded><![CDATA[
arXiv:2512.07795v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title>
<link>https://arxiv.org/abs/2512.07801</link>
<guid>https://arxiv.org/abs/2512.07801</guid>
<content:encoded><![CDATA[
arXiv:2512.07801v1 Announce Type: cross 
Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout</title>
<link>https://arxiv.org/abs/2512.07808</link>
<guid>https://arxiv.org/abs/2512.07808</guid>
<content:encoded><![CDATA[
arXiv:2512.07808v1 Announce Type: cross 
Abstract: Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2512.07820</link>
<guid>https://arxiv.org/abs/2512.07820</guid>
<content:encoded><![CDATA[
arXiv:2512.07820v1 Announce Type: cross 
Abstract: We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning</title>
<link>https://arxiv.org/abs/2512.07827</link>
<guid>https://arxiv.org/abs/2512.07827</guid>
<content:encoded><![CDATA[
arXiv:2512.07827v1 Announce Type: cross 
Abstract: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Generalisation Results Generalise?</title>
<link>https://arxiv.org/abs/2512.07832</link>
<guid>https://arxiv.org/abs/2512.07832</guid>
<content:encoded><![CDATA[
arXiv:2512.07832v1 Announce Type: cross 
Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Visual Similarity</title>
<link>https://arxiv.org/abs/2512.07833</link>
<guid>https://arxiv.org/abs/2512.07833</guid>
<content:encoded><![CDATA[
arXiv:2512.07833v1 Announce Type: cross 
Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Optimal Approximation Factor in Density Estimation</title>
<link>https://arxiv.org/abs/1902.05876</link>
<guid>https://arxiv.org/abs/1902.05876</guid>
<content:encoded><![CDATA[
arXiv:1902.05876v4 Announce Type: replace 
Abstract: Consider the following problem: given two arbitrary densities $q_1,q_2$ and a sample-access to an unknown target density $p$, find which of the $q_i$'s is closer to $p$ in total variation.
  A remarkable result due to Yatracos shows that this problem is tractable in the following sense: there exists an algorithm that uses $O(\epsilon^{-2})$ samples from $p$ and outputs~$q_i$ such that with high probability, $TV(q_i,p) \leq 3\cdot\mathsf{opt} + \epsilon$, where $\mathsf{opt}= \min\{TV(q_1,p),TV(q_2,p)\}$. Moreover, this result extends to any finite class of densities $\mathcal{Q}$: there exists an algorithm that outputs the best density in $\mathcal{Q}$ up to a multiplicative approximation factor of 3.
  We complement and extend this result by showing that: (i) the factor 3 can not be improved if one restricts the algorithm to output a density from $\mathcal{Q}$, and (ii) if one allows the algorithm to output arbitrary densities (e.g.\ a mixture of densities from $\mathcal{Q}$), then the approximation factor can be reduced to 2, which is optimal. In particular this demonstrates an advantage of improper learning over proper in this setup.
  We develop two approaches to achieve the optimal approximation factor of 2: an adaptive one and a static one. Both approaches are based on a geometric point of view of the problem and rely on estimating surrogate metrics to the total variation. Our sample complexity bounds exploit techniques from {\it Adaptive Data Analysis}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking All Tasks at Once Using Adversarial Examples in Multi-Task Learning</title>
<link>https://arxiv.org/abs/2305.12066</link>
<guid>https://arxiv.org/abs/2305.12066</guid>
<content:encoded><![CDATA[
arXiv:2305.12066v4 Announce Type: replace 
Abstract: Visual content understanding frequently relies on multi-task models to extract robust representations of a single visual input for multiple downstream tasks. However, in comparison to extensively studied single-task models, the adversarial robustness of multi-task models has received significantly less attention and many questions remain unclear: 1) How robust are multi-task models to single task adversarial attacks, 2) Can adversarial attacks be designed to simultaneously attack all tasks in a multi-task model, and 3) How does parameter sharing across tasks affect multi-task model robustness to adversarial attacks? This paper aims to answer these questions through careful analysis and rigorous experimentation. First, we analyze the inherent drawbacks of two commonly-used adaptations of single-task white-box attacks in attacking multi-task models. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking all tasks in a multi-task model as an optimization problem that can be efficiently solved through integer linear programming. Extensive evaluation on two popular MTL benchmarks, NYUv2 and Tiny-Taxonomy, demonstrates the effectiveness of DGBA compared to baselines in attacking both clean and adversarially trained multi-task models. Our results also reveal a fundamental trade-off between improving task accuracy via parameter sharing across tasks and undermining model robustness due to increased attack transferability from parameter sharing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Perspective for Loss-Oriented Imbalanced Learning via Localization</title>
<link>https://arxiv.org/abs/2310.04752</link>
<guid>https://arxiv.org/abs/2310.04752</guid>
<content:encoded><![CDATA[
arXiv:2310.04752v2 Announce Type: replace 
Abstract: Due to the inherent imbalance in real-world datasets, na\"ive Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden Minima in Two-Layer ReLU Networks</title>
<link>https://arxiv.org/abs/2312.16819</link>
<guid>https://arxiv.org/abs/2312.16819</guid>
<content:encoded><![CDATA[
arXiv:2312.16819v3 Announce Type: replace 
Abstract: We consider the optimization problem arising from fitting two-layer ReLU networks with $d$ inputs under the square loss, where labels are generated by a target network. Two infinite families of spurious minima have recently been identified: one whose loss vanishes as $d \to \infty$, and another whose loss remains bounded away from zero. The latter are nevertheless avoided by vanilla SGD, and thus hidden, motivating the search for analytic properties distinguishing the two types. Perhaps surprisingly, the Hessian spectra of hidden and non-hidden minima agree up to terms of order $O(d^{-1/2})$, providing limited explanatory power. Consequently, our analysis of hidden minima proceeds instead via curves along which the loss is minimized or maximized. The main result is that arcs emanating from hidden minima differ, characteristically, by their structure and symmetry, precisely on account of the $O(d^{-1/2})$-eigenvalue terms absent from previous analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Learning of Machine Learning Force Fields</title>
<link>https://arxiv.org/abs/2403.17507</link>
<guid>https://arxiv.org/abs/2403.17507</guid>
<content:encoded><![CDATA[
arXiv:2403.17507v2 Announce Type: replace 
Abstract: Machine learning force fields (MLFFs) are a promising approach to balance the accuracy of quantum mechanics with the efficiency of classical potentials, yet selecting an optimal model amid increasingly diverse architectures that delivers reliable force predictions and stable simulations remains a core pratical challenge. Here we introduce EL-MLFFs, an ensemble learning framework that uses a stacking methodology to integrate predictions from diverse base MLFFs. Our approach constructs a graph representation where a graph neural network (GNN) acts as a meta-model to refine the initial force predictions. We present two meta-model architectures: a computationally efficient direct fitting model and a physically-principled conservative model that ensures energy conservation. The framework is evaluated on a diverse range of systems, including single molecules (methane), surface chemistry (methanol/Cu(100)), molecular dynamics benchmarks (MD17), and the MatPES materials dataset. Results show that EL-MLFFs improves predictive accuracy across these domains. For molecular systems, it reduces force errors and improves the simulation stability compared to base models. For materials, the method yields lower formation energy errors on the WBM test set. The EL- MLFFs framework offers a systematic approach to address challenges of model selection and the accuracy-stability trade-off in molecular and materials simulations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDT-GNN: Streaming-based Distributed Training Framework for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2404.02300</link>
<guid>https://arxiv.org/abs/2404.02300</guid>
<content:encoded><![CDATA[
arXiv:2404.02300v2 Announce Type: replace 
Abstract: Recently, distributed GNN training frameworks, such as DistDGL and PyG, have been developed to enable training GNN models on large graphs by leveraging multiple GPUs in a distributed manner. Despite these advances, their memory requirements are still excessively high, thereby hindering GNN training on large graphs using commodity workstations. In this paper, we propose SDT-GNN, a streaming-based distributed GNN training framework. Unlike the existing frameworks that load the entire graph in memory, it takes a stream of edges as input for graph partitioning to reduce the memory requirement for partitioning. It also enables distributed GNN training even when the aggregated memory size of GPUs is smaller than the size of the graph and feature data. Furthermore, to improve the quality of partitioning, we propose SPRING, a novel streaming partitioning algorithm for distributed GNN training. We demonstrate the effectiveness and efficiency of SDT-GNN on seven large public datasets. SDT-GNN has up to 95% less memory footprint than DistDGL and PyG without sacrificing the prediction accuracy. SPRING also outperforms state-of-the-art streaming partitioning algorithms significantly.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covariate-Elaborated Robust Partial Information Transfer with Conditional Spike-and-Slab Prior</title>
<link>https://arxiv.org/abs/2404.03764</link>
<guid>https://arxiv.org/abs/2404.03764</guid>
<content:encoded><![CDATA[
arXiv:2404.03764v3 Announce Type: replace 
Abstract: The popularity of transfer learning stems from the fact that it can borrow information from useful auxiliary datasets. Existing statistical transfer learning methods usually adopt a global similarity measure between the source data and the target data, which may lead to inefficiency when only partial information is shared. In this paper, we propose a novel Bayesian transfer learning method named ``CONCERT'' to allow robust partial information transfer for high-dimensional data analysis. A conditional spike-and-slab prior is introduced in the joint distribution of target and source parameters for information transfer. By incorporating covariate-specific priors, we can characterize partial similarities and integrate source information collaboratively to improve the performance on the target. In contrast to existing work, the CONCERT is a one-step procedure which achieves variable selection and information transfer simultaneously. We establish variable selection consistency, as well as estimation and prediction error bounds for CONCERT. Our theory demonstrates the covariate-specific benefit of transfer learning. To ensure the scalability of the algorithm, we adopt the variational Bayes framework to facilitate implementation. Extensive experiments and two real data applications showcase the validity and advantages of CONCERT over existing cutting-edge transfer learning methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</title>
<link>https://arxiv.org/abs/2404.18886</link>
<guid>https://arxiv.org/abs/2404.18886</guid>
<content:encoded><![CDATA[
arXiv:2404.18886v5 Announce Type: replace 
Abstract: Diffusion models have been widely used in time series and spatio-temporal data, enhancing generative, inferential, and downstream capabilities. These models are applied across diverse fields such as healthcare, recommendation, climate, energy, audio, and traffic. By separating applications for time series and spatio-temporal data, we offer a structured perspective on model category, task type, data modality, and practical application domain. This study aims to provide a solid foundation for researchers and practitioners, inspiring future innovations that tackle traditional challenges and foster novel solutions in diffusion model-based data mining tasks and applications. For more detailed information, we have open-sourced a repository at https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast training and sampling of Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2405.15376</link>
<guid>https://arxiv.org/abs/2405.15376</guid>
<content:encoded><![CDATA[
arXiv:2405.15376v3 Announce Type: replace 
Abstract: Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting insights from data, but their training is hindered by the slow mixing of Markov Chain Monte Carlo (MCMC) processes, especially with highly structured datasets. In this study, we build on recent theoretical advances in RBM training and focus on the stepwise encoding of data patterns into singular vectors of the coupling matrix, significantly reducing the cost of generating new samples and evaluating the quality of the model, as well as the training cost in highly clustered datasets. The learning process is analogous to the thermodynamic continuous phase transitions observed in ferromagnetic models, where new modes in the probability measure emerge in a continuous manner. We leverage the continuous transitions in the training process to define a smooth annealing trajectory that enables reliable and computationally efficient log-likelihood estimates. This approach enables online assessment during training and introduces a novel sampling strategy called Parallel Trajectory Tempering (PTT) that outperforms previously optimized MCMC methods. To mitigate the critical slowdown effect in the early stages of training, we propose a pre-training phase. In this phase, the principal components are encoded into a low-rank RBM through a convex optimization process, facilitating efficient static Monte Carlo sampling and accurate computation of the partition function. Our results demonstrate that this pre-training strategy allows RBMs to efficiently handle highly structured datasets where conventional methods fail. Additionally, our log-likelihood estimation outperforms computationally intensive approaches in controlled scenarios, while the PTT algorithm significantly accelerates MCMC processes compared to conventional methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeAutoDiff: A Unified Framework for Generation, Imputation, Forecasting, and Time-Varying Metadata Conditioning of Heterogeneous Time Series Tabular Data</title>
<link>https://arxiv.org/abs/2406.16028</link>
<guid>https://arxiv.org/abs/2406.16028</guid>
<content:encoded><![CDATA[
arXiv:2406.16028v3 Announce Type: replace 
Abstract: We present TimeAutoDiff, a unified latent-diffusion framework for four fundamental time-series tasks: unconditional generation, missing-data imputation, forecasting, and time-varying-metadata conditional generation. The model natively supports heterogeneous features including continuous, binary, and categorical variables. We unify all tasks using a masked-modeling strategy in which a binary mask specifies which time-series cells are observed and which must be generated. TimeAutoDiff combines a lightweight variational autoencoder, which maps mixed-type features into a continuous latent sequence, with a diffusion model that learns temporal dynamics in this latent space. Two architectural choices provide strong speed and scalability benefits. The diffusion model samples an entire latent trajectory at once rather than denoising one timestep at a time, greatly reducing reverse-diffusion calls. In addition, the VAE compresses along the feature axis, enabling efficient modeling of wide tables in a low-dimensional latent space. Empirical evaluation shows that TimeAutoDiff matches or surpasses strong baselines in synthetic sequence fidelity and consistently improves imputation and forecasting performance. Metadata conditioning enables realistic scenario exploration, allowing users to edit metadata sequences and produce coherent counterfactual trajectories that preserve cross-feature dependencies. Ablation studies highlight the importance of the VAE's feature encoding and key components of the denoiser. A distance-to-closest-record audit further indicates that the model generalizes without excessive memorization. Code is available at https://github.com/namjoonsuh/TimeAutoDiff
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Model Performance Under Worst-case Subpopulations</title>
<link>https://arxiv.org/abs/2407.01316</link>
<guid>https://arxiv.org/abs/2407.01316</guid>
<content:encoded><![CDATA[
arXiv:2407.01316v2 Announce Type: replace 
Abstract: The performance of ML models degrades when the training population is different from that seen under operation. Towards assessing distributional robustness, we study the worst-case performance of a model over all subpopulations of a given size, defined with respect to core attributes Z. This notion of robustness can consider arbitrary (continuous) attributes Z, and automatically accounts for complex intersectionality in disadvantaged groups. We develop a scalable yet principled two-stage estimation procedure that can evaluate the robustness of state-of-the-art models. We prove that our procedure enjoys several finite-sample convergence guarantees, including dimension-free convergence. Instead of overly conservative notions based on Rademacher complexities, our evaluation error depends on the dimension of Z only through the out-of-sample error in estimating the performance conditional on Z. On real datasets, we demonstrate that our method certifies the robustness of a model and prevents deployment of unreliable models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models -- AutoML from Basics to State-of-the-Art Techniques</title>
<link>https://arxiv.org/abs/2410.09596</link>
<guid>https://arxiv.org/abs/2410.09596</guid>
<content:encoded><![CDATA[
arXiv:2410.09596v3 Announce Type: replace 
Abstract: A comprehensive guide to Automated Machine Learning (AutoML) is presented, covering fundamental principles, practical implementations, and future trends. The paper is structured to assist both beginners and experienced practitioners, with detailed discussions on popular AutoML tools such as TPOT, AutoGluon, and Auto-Keras. Emerging topics like Neural Architecture Search (NAS) and AutoML's applications in deep learning are also addressed. It is anticipated that this work will contribute to ongoing research and development in the field of AI and machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tidal Current Speed Forecasting Model based on Multi-Periodicity Learning</title>
<link>https://arxiv.org/abs/2410.09718</link>
<guid>https://arxiv.org/abs/2410.09718</guid>
<content:encoded><![CDATA[
arXiv:2410.09718v3 Announce Type: replace 
Abstract: Tidal energy is one of the key components in increasing the penetration of renewable energy. High tidal energy penetration into the electrical grid depends on accurate tidal current speed forecasting. Model inaccuracies hinder forecast accuracy. Previous research primarily used physical models to forecast tidal current speed, yet tidal current variations influenced by the orbital periods of celestial bodies make accurate physical modeling challenging. Research on the multi-periodicity of tides is crucial for forecasting tidal current speed. We propose the Wavelet-Enhanced Convolutional Network to learn multi-periodicity. The framework embeds intra-period and inter-period variations of one-dimensional tidal current data into the rows and columns, respectively, of a two-dimensional tensor. Then, the two-dimensional variations of the sequence can be processed by convolutional kernels. We integrate a time-frequency analysis method into the framework to further address local periodic features. Additionally, to enhance the framework's stability, we optimize the framework's hyperparameters with the Tree-structured Parzen Estimator. The proposed framework captures multi-periodic dependencies in tidal current data. Numerical results show a 10-step average Mean Absolute Error of 0.025, with at least a 1.18% error reduction compared to other baselines. Further ablation studies show a 1.4% reduction in Mean Absolute Percentage Error on the data with artificially added periodic fluctuations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIT-GNN: Faster Inference Time for GNNs that 'FIT' in Memory Using Coarsening</title>
<link>https://arxiv.org/abs/2410.15001</link>
<guid>https://arxiv.org/abs/2410.15001</guid>
<content:encoded><![CDATA[
arXiv:2410.15001v4 Announce Type: replace 
Abstract: Scalability of Graph Neural Networks (GNNs) remains a significant challenge. To tackle this, methods like coarsening, condensation, and computation trees are used to train on a smaller graph, resulting in faster computation. Nonetheless, prior research has not adequately addressed the computational costs during the inference phase. This paper presents a novel approach to improve the scalability of GNNs by reducing computational burden during the inference phase using graph coarsening. We demonstrate two different methods -- Extra Nodes and Cluster Nodes. Our study extends the application of graph coarsening for graph-level tasks, including graph classification and graph regression. We conduct extensive experiments on multiple benchmark datasets to evaluate the performance of our approach. Our results show that the proposed method achieves orders of magnitude improvements in single-node inference time compared to traditional approaches. Furthermore, it significantly reduces memory consumption for node and graph classification and regression tasks, enabling efficient training and inference on low-resource devices where conventional methods are impractical. Notably, these computational advantages are achieved while maintaining competitive performance relative to baseline models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeqProFT: Sequence-only Protein Property Prediction with LoRA Finetuning</title>
<link>https://arxiv.org/abs/2411.11530</link>
<guid>https://arxiv.org/abs/2411.11530</guid>
<content:encoded><![CDATA[
arXiv:2411.11530v2 Announce Type: replace 
Abstract: Protein language models (PLMs) have demonstrated remarkable capabilities in learning relationships between protein sequences and functions. However, finetuning these large models requires substantial computational resources, often with suboptimal task-specific results. This study investigates how parameter-efficient finetuning via LoRA can enhance protein property prediction while significantly reducing computational demands. By applying LoRA to ESM-2 and ESM-C models of varying sizes and evaluating 10 diverse protein property prediction tasks, we demonstrate that smaller models with LoRA adaptation can match or exceed the performance of larger models without adaptation. Additionally, we integrate contact map information through a multi-head attention mechanism, improving model comprehension of structural features. Our systematic analysis reveals that LoRA finetuning enables faster convergence, better performance, and more efficient resource utilization, providing practical guidance for protein research applications in resource-constrained environments. The code is available at https://github.com/jiankliu/SeqProFT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inversely Learning Transferable Rewards via Abstracted States</title>
<link>https://arxiv.org/abs/2501.01669</link>
<guid>https://arxiv.org/abs/2501.01669</guid>
<content:encoded><![CDATA[
arXiv:2501.01669v3 Announce Type: replace 
Abstract: Inverse reinforcement learning (IRL) has progressed significantly toward accurately learning the underlying rewards in both discrete and continuous domains from behavior data. The next advance is to learn {\em intrinsic} preferences in ways that produce useful behavior in settings or tasks which are different but aligned with the observed ones. In the context of robotic applications, this helps integrate robots into processing lines involving new tasks (with shared intrinsic preferences) without programming from scratch. We introduce a method to inversely learn an abstract reward function from behavior trajectories in two or more differing instances of a domain. The abstract reward function is then used to learn task behavior in another separate instance of the domain. This step offers evidence of its transferability and validates its correctness. We evaluate the method on trajectories in tasks from multiple domains in OpenAI's Gym testbed and AssistiveGym and show that the learned abstract reward functions can successfully learn task behaviors in instances of the respective domains, which have not been seen previously.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act</title>
<link>https://arxiv.org/abs/2501.12962</link>
<guid>https://arxiv.org/abs/2501.12962</guid>
<content:encoded><![CDATA[
arXiv:2501.12962v4 Announce Type: replace 
Abstract: What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for high-risk systems, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First, a necessary high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second, an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.) Most non-discrimination regulations target only high-risk AI systems. (2.) The regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are partly inconsistent and raise questions of computational feasibility. (3.) Finally, we consider the possible (future) interaction of classical EU non-discrimination law and the AI Act regulations. We recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online-BLS: An Accurate and Efficient Online Broad Learning System for Data Stream Classification</title>
<link>https://arxiv.org/abs/2501.16932</link>
<guid>https://arxiv.org/abs/2501.16932</guid>
<content:encoded><![CDATA[
arXiv:2501.16932v2 Announce Type: replace 
Abstract: The state-of-the-art online learning models generally conduct a single online gradient descent when a new sample arrives and thus suffer from suboptimal model weights. To this end, we introduce an online broad learning system framework with closed-form solutions for each online update. Different from employing existing incremental broad learning algorithms for online learning tasks, which tend to incur degraded accuracy and expensive online update overhead, we design an effective weight estimation algorithm and an efficient online updating strategy to remedy the above two deficiencies, respectively. Specifically, an effective weight estimation algorithm is first developed by replacing notorious matrix inverse operations with Cholesky decomposition and forward-backward substitution to improve model accuracy. Second, we devise an efficient online updating strategy that dramatically reduces online update time. Theoretical analysis exhibits the splendid error bound and low time complexity of our model. The most popular test-then-training evaluation experiments on various real-world datasets prove its superiority and efficiency. Furthermore, our framework is naturally extended to data stream scenarios with concept drift and exceeds state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</title>
<link>https://arxiv.org/abs/2502.02216</link>
<guid>https://arxiv.org/abs/2502.02216</guid>
<content:encoded><![CDATA[
arXiv:2502.02216v3 Announce Type: replace 
Abstract: We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization</title>
<link>https://arxiv.org/abs/2502.02493</link>
<guid>https://arxiv.org/abs/2502.02493</guid>
<content:encoded><![CDATA[
arXiv:2502.02493v2 Announce Type: replace 
Abstract: Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stein Discrepancy for Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2502.03587</link>
<guid>https://arxiv.org/abs/2502.03587</guid>
<content:encoded><![CDATA[
arXiv:2502.03587v4 Announce Type: replace 
Abstract: Unsupervised domain adaptation (UDA) aims to improve model performance on an unlabeled target domain using a related, labeled source domain. A common approach aligns source and target feature distributions by minimizing a distance between them, often using symmetric measures such as maximum mean discrepancy (MMD). However, these methods struggle when target data is scarce. We propose a novel UDA framework that leverages Stein discrepancy, an asymmetric measure that depends on the target distribution only through its score function, making it particularly suitable for low-data target regimes. Our proposed method has kernelized and adversarial forms and supports flexible modeling of the target distribution via Gaussian, GMM, or VAE models. We derive a generalization bound on the target error and a convergence rate for the empirical Stein discrepancy in the two-sample setting. Empirically, our method consistently outperforms prior UDA approaches under limited target data across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Countering Overfitting with Counterfactual Examples</title>
<link>https://arxiv.org/abs/2502.09193</link>
<guid>https://arxiv.org/abs/2502.09193</guid>
<content:encoded><![CDATA[
arXiv:2502.09193v2 Announce Type: replace 
Abstract: Overfitting is a well-known issue in machine learning that occurs when a model struggles to generalize its predictions to new, unseen data beyond the scope of its training set. Traditional techniques to mitigate overfitting include early stopping, data augmentation, and regularization. In this work, we demonstrate that the degree of overfitting of a trained model is correlated with the ability to generate counterfactual examples. The higher the overfitting, the easier it will be to find a valid counterfactual example for a randomly chosen input data point. Therefore, we introduce CF-Reg, a novel regularization term in the training loss that controls overfitting by ensuring enough margin between each instance and its corresponding counterfactual. Experiments conducted across multiple datasets and models show that our counterfactual regularizer generally outperforms existing regularization techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExLLM: Experience-Enhanced LLM Optimization for Molecular Design and Beyond</title>
<link>https://arxiv.org/abs/2502.12845</link>
<guid>https://arxiv.org/abs/2502.12845</guid>
<content:encoded><![CDATA[
arXiv:2502.12845v5 Announce Type: replace 
Abstract: Molecular design involves an enormous and irregular search space, where traditional optimizers such as Bayesian optimization, genetic algorithms, and generative models struggle to leverage expert knowledge or handle complex feedback. Recently, LLMs have been used as optimizers, achieving promising results on benchmarks such as PMO. However, existing approaches rely only on prompting or extra training, without mechanisms to handle complex feedback or maintain scalable memory. In particular, the common practice of appending or summarizing experiences at every query leads to redundancy, degraded exploration, and ultimately poor final outcomes under large-scale iterative search. We introduce ExLLM (Experience-Enhanced LLM optimization), an LLM-as-optimizer framework with three components: (1) a compact, evolving experience snippet tailored to large discrete spaces that distills non-redundant cues and improves convergence at low cost; (2) a simple yet effective k-offspring scheme that widens exploration per call and reduces orchestration cost; and (3) a lightweight feedback adapter that normalizes objectives for selection while formatting constraints and expert hints for iteration. ExLLM sets new state-of-the-art results on PMO and generalizes strongly in our setup, it sets records on circle packing and stellarator design, and yields consistent gains across additional domains requiring only a task-description template and evaluation functions to transfer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices</title>
<link>https://arxiv.org/abs/2502.14011</link>
<guid>https://arxiv.org/abs/2502.14011</guid>
<content:encoded><![CDATA[
arXiv:2502.14011v2 Announce Type: replace 
Abstract: The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. While extensions of the Very Fast Decision Tree (VFDT) remain state-of-the-art for tabular stream mining, their unregulated growth limits efficiency, particularly in ensemble settings where post-pruning at the individual tree level is seldom applied. This paper presents DFDT, a novel memory-constrained algorithm for online learning. DFDT employs activity-aware pre-pruning, dynamically adjusting splitting criteria based on leaf node activity: low-activity nodes are deactivated to conserve resources, moderately active nodes split under stricter conditions, and highly active nodes leverage a skipping mechanism for accelerated growth. Additionally, adaptive grace periods and tie thresholds allow DFDT to modulate splitting decisions based on observed data variability, enhancing the accuracy-memory-runtime trade-off while minimizing the need for hyperparameter tuning. An ablation study reveals three DFDT variants suited to different resource profiles. Fully compatible with existing ensemble frameworks, DFDT provides a drop-in alternative to standard VFDT-based learners.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title>
<link>https://arxiv.org/abs/2502.14354</link>
<guid>https://arxiv.org/abs/2502.14354</guid>
<content:encoded><![CDATA[
arXiv:2502.14354v3 Announce Type: replace 
Abstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-Decision-Focused Neural ODEs for Proactive Grid Resilience Management</title>
<link>https://arxiv.org/abs/2502.18321</link>
<guid>https://arxiv.org/abs/2502.18321</guid>
<content:encoded><![CDATA[
arXiv:2502.18321v3 Announce Type: replace 
Abstract: Extreme hazard events such as wildfires and hurricanes increasingly threaten power systems, causing widespread outages and disrupting critical services. Recently, predict-then-optimize approaches have gained traction in grid operations, where system functionality forecasts are first generated and then used as inputs for downstream decision-making. However, this two-stage method often results in a misalignment between prediction and optimization objectives, leading to suboptimal resource allocation. To address this, we propose predict-all-then-optimize-globally (PATOG), a framework that integrates outage prediction with globally optimized interventions. At its core, our global-decision-focused (GDF) neural ODE model captures outage dynamics while optimizing resilience strategies in a decision-aware manner. Unlike conventional methods, our approach ensures spatially and temporally coherent decision-making, improving both predictive accuracy and operational efficiency. Experiments on synthetic and real-world datasets demonstrate significant improvements in outage prediction consistency and grid resilience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[
arXiv:2503.10253v3 Announce Type: replace 
Abstract: Simulation of spatiotemporal systems governed by partial differential equations is widely applied in fields such as biology, chemistry, aerospace dynamics, and meteorology. Traditional numerical methods incur high computational costs due to the requirement of small time steps for accurate predictions. While machine learning has reduced these costs, long-term predictions remain challenged by error accumulation, particularly in scenarios with insufficient data or varying time scales, where stability and accuracy are compromised. Existing methods often neglect the effective utilization of multi-scale data, leading to suboptimal robustness in predictions. To address these issues, we propose a novel multi-scale learning framework, namely, the Physics-Informed Multi-Scale Recurrent Learning (PIMRL), to effectively leverage multi-scale data for spatiotemporal dynamics prediction. The PIMRL framework comprises two modules: the micro-scale module embeds physical knowledge into neural networks via pretraining, and the macro-scale module adopts a data-driven approach to learn the temporal evolution of physics in the latent space. Experimental results demonstrate that the PIMRL framework consistently achieves state-of-the-art performance across five benchmark datasets ranging from one to three dimensions, showing average improvements of over 9\% in both RMSE and MAE evaluation metrics, with maximum enhancements reaching up to 80%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantization-Free Autoregressive Action Transformer</title>
<link>https://arxiv.org/abs/2503.14259</link>
<guid>https://arxiv.org/abs/2503.14259</guid>
<content:encoded><![CDATA[
arXiv:2503.14259v3 Announce Type: replace 
Abstract: Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries</title>
<link>https://arxiv.org/abs/2504.02459</link>
<guid>https://arxiv.org/abs/2504.02459</guid>
<content:encoded><![CDATA[
arXiv:2504.02459v2 Announce Type: replace 
Abstract: In this work, we introduce implicit Finite Operator Learning (iFOL) for the continuous and parametric solution of partial differential equations (PDEs) on arbitrary geometries. We propose a physics-informed encoder-decoder network to establish the mapping between continuous parameter and solution spaces. The decoder constructs the parametric solution field by leveraging an implicit neural field network conditioned on a latent or feature code. Instance-specific codes are derived through a PDE encoding process based on the second-order meta-learning technique. In training and inference, a physics-informed loss function is minimized during the PDE encoding and decoding. iFOL expresses the loss function in an energy or weighted residual form and evaluates it using discrete residuals derived from standard numerical PDE methods. This approach results in the backpropagation of discrete residuals during both training and inference.
  iFOL features several key properties: (1) its unique loss formulation eliminates the need for the conventional encode-process-decode pipeline previously used in operator learning with conditional neural fields for PDEs; (2) it not only provides accurate parametric and continuous fields but also delivers solution-to-parameter gradients without requiring additional loss terms or sensitivity analysis; (3) it can effectively capture sharp discontinuities in the solution; and (4) it removes constraints on the geometry and mesh, making it applicable to arbitrary geometries and spatial sampling (zero-shot super-resolution capability). We critically assess these features and analyze the network's ability to generalize to unseen samples across both stationary and transient PDEs. The overall performance of the proposed method is promising, demonstrating its applicability to a range of challenging problems in computational mechanics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[
arXiv:2504.16828v5 Announce Type: replace 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models are released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repetition Makes Perfect: Recurrent Graph Neural Networks Match Message-Passing Limit</title>
<link>https://arxiv.org/abs/2505.00291</link>
<guid>https://arxiv.org/abs/2505.00291</guid>
<content:encoded><![CDATA[
arXiv:2505.00291v3 Announce Type: replace 
Abstract: We precisely characterize the expressivity of computable Recurrent Graph Neural Networks (recurrent GNNs). We prove that recurrent GNNs with finite-precision parameters, sum aggregation, and ReLU activation, can compute any graph algorithm that respects the natural message-passing invariance induced by the Color Refinement (or Weisfeiler-Leman) algorithm. While it is well known that the expressive power of GNNs is limited by this invariance [Morris et al., AAAI 2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually match this limit. This is in contrast to non-recurrent GNNs, which have the power of Weisfeiler-Leman only in a very weak, "non-uniform", sense where each graph size requires a different GNN to compute with. Our construction introduces only a polynomial overhead in both time and space.
  Furthermore, we show that by incorporating random initialization, for connected graphs recurrent GNNs can express all graph algorithms. In particular, any polynomial-time graph algorithm can be emulated on connected graphs in polynomial time by a recurrent GNN with random initialization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</title>
<link>https://arxiv.org/abs/2505.10947</link>
<guid>https://arxiv.org/abs/2505.10947</guid>
<content:encoded><![CDATA[
arXiv:2505.10947v3 Announce Type: replace 
Abstract: Establishing stability certificates for closed-loop systems under reinforcement learning (RL) policies is essential to move beyond empirical performance and offer guarantees of system behavior. Classical Lyapunov methods require a strict stepwise decrease in the Lyapunov function but such certificates are difficult to construct for learned policies. The RL value function is a natural candidate but it is not well understood how it can be adapted for this purpose. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains</title>
<link>https://arxiv.org/abs/2505.19397</link>
<guid>https://arxiv.org/abs/2505.19397</guid>
<content:encoded><![CDATA[
arXiv:2505.19397v2 Announce Type: replace 
Abstract: Time-Series Foundation Models (TSFMs) are rapidly transitioning from research prototypes to core components of critical decision-making systems, driven by their impressive zero-shot forecasting capabilities. However, as their deployment surges, a critical blind spot remains: their fragility under adversarial attacks. This lack of scrutiny poses severe risks, particularly as TSFMs enter high-stakes environments vulnerable to manipulation. We present a systematic, diagnostic study arguing that for TSFMs, robustness is not merely a secondary metric but a prerequisite for trustworthy deployment comparable to accuracy. Our evaluation framework, explicitly tailored to the unique constraints of time series, incorporates normalized, sparsity-aware perturbation budgets and unified scale-invariant metrics across white-box and black-box settings. Across six representative TSFMs, we demonstrate that current architectures are alarmingly brittle: even small perturbations can reliably steer forecasts toward specific failure modes, such as trend flips and malicious drifts. We uncover TSFM-specific vulnerability patterns, including horizon-proximal brittleness, increased susceptibility with longer context windows, and weak cross-model transfer that points to model-specific failure modes rather than generic distortions. Finally, we show that simple adversarial fine-tuning offers a cost-effective path to substantial robustness gains, even with out-of-domain data. This work bridges the gap between TSFM capabilities and safety constraints, offering essential guidance for hardening the next generation of forecasting systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
arXiv:2505.20561v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as rethinking and error correction, as a form of in-context exploration. However, the Markovian policy obtained from conventional RL training does not give rise to reflective exploration behaviors since the policy depends on the history only through the state and therefore has no incentive to enrich identical states with additional context. Instead, RL exploration is only useful during training to learn the optimal policy in a trial-and-error manner. Therefore, it remains unclear whether reflective reasoning will emerge during RL, or why it is beneficial. To remedy this, we recast reflective exploration within a Bayesian RL framework, which optimizes the expected return under a posterior distribution over Markov decision processes induced by the training data. This Bayesian formulation admits uncertainty-adaptive policies that, through belief updates, naturally incentivize information-gathering actions and induce self-reflection behaviors. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms conventional RL approaches, achieving superior test-time performance and token efficiency. Our code is available at https://github.com/shenao-zhang/BARL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning where to learn: Training data distribution optimization for scientific machine learning</title>
<link>https://arxiv.org/abs/2505.21626</link>
<guid>https://arxiv.org/abs/2505.21626</guid>
<content:encoded><![CDATA[
arXiv:2505.21626v3 Announce Type: replace 
Abstract: In scientific machine learning, models are routinely deployed with parameter values or boundary conditions far from those used in training. This paper studies the learning-where-to-learn problem of designing a training data distribution that minimizes average prediction error across a family of deployment regimes. A theoretical analysis shows how the training distribution shapes deployment accuracy. This motivates two adaptive algorithms based on bilevel or alternating optimization in the space of probability measures. Discretized implementations using parametric distribution classes or nonparametric particle-based gradient flows deliver optimized training distributions that outperform nonadaptive designs. Once trained, the resulting models exhibit improved sample complexity and robustness to distribution shift. This framework unlocks the potential of principled data acquisition for learning functions and solution operators of partial differential equations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Judging LLMs: A Simplex Perspective</title>
<link>https://arxiv.org/abs/2505.21972</link>
<guid>https://arxiv.org/abs/2505.21972</guid>
<content:encoded><![CDATA[
arXiv:2505.21972v2 Announce Type: replace 
Abstract: Given the challenge of automatically evaluating free-form outputs from large language models (LLMs), an increasingly common solution is to use LLMs themselves as the judging mechanism, without any gold-standard scores. Implicitly, this practice accounts for only sampling variability (aleatoric uncertainty) and ignores uncertainty about judge quality (epistemic uncertainty). While this is justified if judges are perfectly accurate, it is unclear when such an approach is theoretically valid and practically robust. We study these questions for the task of ranking LLM candidates from a novel geometric perspective: for $M$-level scoring systems, both LLM judges and candidates can be represented as points on an $(M-1)$-dimensional probability simplex, where geometric concepts (e.g., triangle areas) correspond to key ranking concepts. This perspective yields intuitive theoretical conditions and visual proofs for when rankings are identifiable; for instance, we provide a formal basis for the ``folk wisdom'' that LLM judges are more effective for two-level scoring ($M=2$) than multi-level scoring ($M>2$). Leveraging the simplex, we design geometric Bayesian priors that encode epistemic uncertainty about judge quality and vary the priors to conduct sensitivity analyses. Experiments on LLM benchmarks show that rankings based solely on LLM judges are robust in many but not all datasets, underscoring both their widespread success and the need for caution. Our Bayesian method achieves substantially higher coverage rates than existing procedures, highlighting the importance of modeling epistemic uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stepsize anything: A unified learning rate schedule for budgeted-iteration training</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[
arXiv:2505.24452v4 Announce Type: replace 
Abstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by a single hyper-parameter \varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between \varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of \varphi. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlearning Inversion Attacks for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00808</link>
<guid>https://arxiv.org/abs/2506.00808</guid>
<content:encoded><![CDATA[
arXiv:2506.00808v2 Announce Type: replace 
Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression</title>
<link>https://arxiv.org/abs/2506.06954</link>
<guid>https://arxiv.org/abs/2506.06954</guid>
<content:encoded><![CDATA[
arXiv:2506.06954v2 Announce Type: replace 
Abstract: Mainstream approximate action-value iteration reinforcement learning (RL) algorithms suffer from overestimation bias, leading to suboptimal policies in high-variance stochastic environments. Quantile-based action-value iteration methods reduce this bias by learning a distribution of the expected cost-to-go using quantile regression. However, ensuring that the learned policy satisfies safety constraints remains a challenge when these constraints are not explicitly integrated into the RL framework. Existing methods often require complex neural architectures or manual tradeoffs due to combined cost functions. To address this, we propose a risk-regularized quantile-based algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety without complex architectures. We also provide theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator in Wasserstein space, ensuring convergence to a unique cost distribution. Simulations of a mobile robot in a dynamic reach-avoid task show that our approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs than risk-neutral methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-supervised Graph Anomaly Detection via Robust Homophily Learning</title>
<link>https://arxiv.org/abs/2506.15448</link>
<guid>https://arxiv.org/abs/2506.15448</guid>
<content:encoded><![CDATA[
arXiv:2506.15448v2 Announce Type: replace 
Abstract: Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled normal nodes to identify abnormal nodes from a large set of unlabeled nodes in a graph. Current methods in this line posit that 1) normal nodes share a similar level of homophily and 2) the labeled normal nodes can well represent the homophily patterns in the normal class. However, this assumption often does not hold well since normal nodes in a graph can exhibit diverse homophily in real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily Learning, to adaptively learn such homophily patterns. RHO consists of two novel modules, adaptive frequency response filters (AdaFreq) and graph normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters that capture different frequency components of the labeled normal nodes with varying homophily in the channel-wise and cross-channel views of node attributes. GNA is introduced to enforce consistency between the channel-wise and cross-channel homophily representations to robustify the normality learned by the filters in the two views. Experiments on eight real-world GAD datasets show that RHO can effectively learn varying, often under-represented, homophily in the small normal node set and substantially outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/RHO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Sample is Enough to Make Conformal Prediction Robust</title>
<link>https://arxiv.org/abs/2506.16553</link>
<guid>https://arxiv.org/abs/2506.16553</guid>
<content:encoded><![CDATA[
arXiv:2506.16553v2 Announce Type: replace 
Abstract: For any black-box model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends the guarantee to the worst case noise up to a pre-defined magnitude. For RCP, a well-established approach is to use randomized smoothing since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, smoothing-based robustness requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a single forward pass on a randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. 100) passes per input. Our key insight is to certify the conformal procedure itself rather than individual conformity scores. Our approach is agnostic to the task (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Classical Hybrid Quantized Neural Network</title>
<link>https://arxiv.org/abs/2506.18240</link>
<guid>https://arxiv.org/abs/2506.18240</guid>
<content:encoded><![CDATA[
arXiv:2506.18240v4 Announce Type: replace 
Abstract: In this work, we introduce a novel Quadratic Binary Optimization (QBO) framework for training a quantized neural network. The framework enables the use of arbitrary activation and loss functions through spline interpolation, while Forward Interval Propagation addresses the nonlinearities and the multi-layered, composite structure of neural networks via discretizing activation functions into linear subintervals. This preserves the universal approximation properties of neural networks while allowing complex nonlinear functions accessible to quantum solvers, broadening their applicability in artificial intelligence. Theoretically, we derive an upper bound on the approximation error and the number of Ising spins required by deriving the sample complexity of the empirical risk minimization problem from an optimization perspective. A key challenge in solving the associated large-scale Quadratic Constrained Binary Optimization (QCBO) model is the presence of numerous constraints. To overcome this, we adopt the Quantum Conditional Gradient Descent (QCGD) algorithm, which solves QCBO directly on quantum hardware. We establish the convergence of QCGD under a quantum oracle subject to randomness, bounded variance, and limited coefficient precision, and further provide an upper bound on the Time-To-Solution. To enhance scalability, we further incorporate a decomposed copositive optimization scheme that replaces the monolithic lifted model with sample-wise subproblems. This decomposition substantially reduces the quantum resource requirements and enables efficient low-bit neural network training. We further propose the usage of QCGD and Quantum Progressive Hedging (QPH) algorithm to efficiently solve the decomposed problem.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal Structure?</title>
<link>https://arxiv.org/abs/2506.20347</link>
<guid>https://arxiv.org/abs/2506.20347</guid>
<content:encoded><![CDATA[
arXiv:2506.20347v2 Announce Type: replace 
Abstract: Granger Causality (GC) offers an elegant statistical framework to study the association between multivariate time series data. Vector autoregressive models (VAR) are simple and easy to fit, but have limited application because of their inherent inability to capture more complex (e.g., non-linear) associations. Numerous attempts have already been made in the literature that exploit the functional approximation power of deep neural networks (DNNs) for GC. However, these methods treat GC as a variable selection problem. We present a novel paradigm for investigating the learned GC from a single neural network used for joint modeling of all components of multivariate time series data, which is essentially linked with prediction and assessing the distribution shift in residuals. A deep learning model, with proper regularization, may learn the true GC structure when jointly used for all components of the time series when there is sufficient training data. We propose to uncover the learned GC structure by comparing the model uncertainty or distribution of the residuals when the past of everything is used as compared to the one where a specific time series component is dropped from the model. We also compare the effect of input layer dropout on the ability of a neural network to learn GC. We show that a well-regularized model can learn the true GC structure from the data without explicitly adding terms in the loss function that guide the model to select variables or perform sparse regression under specific settings. We also provide a comparison of deep learning architectures such as CNN, LSTM and transformer models on their ability to discover Granger Causality. The numerical experiments demonstrate that, compared to sparse regression models, a simple joint model is a strong baseline for learning the true GC which has the advantage that it does not require tuning of many extra hyper-parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge</title>
<link>https://arxiv.org/abs/2507.09202</link>
<guid>https://arxiv.org/abs/2507.09202</guid>
<content:encoded><![CDATA[
arXiv:2507.09202v2 Announce Type: replace 
Abstract: Artificial intelligence (AI)-driven models have the potential to revolutionize weather forecasting, but still rely on initial conditions generated by costly Numerical Weather Prediction (NWP) systems. Although recent end-to-end forecasting models attempt to bypass NWP systems, these methods lack scalable assimilation of new types of observational data. Here, we introduce XiChen, an observation-scalable fully AI-driven global weather forecasting system, wherein the entire pipeline, from Data Assimilation (DA) to medium-range forecasting, can be accomplished within only 15 seconds. XiChen is built upon a foundation model that is pre-trained for weather forecasting and subsequently fine-tuned to serve as both observation operators and DA models, thereby enabling the scalable assimilation of conventional and raw satellite observations. Furthermore, the integration of Four-Dimensional Variational (4DVar) knowledge ensures XiChen to achieve DA and medium-range forecasting accuracy comparable to operational NWP systems, with skillful forecasting lead time beyond 8.75 days. A key feature of XiChen is its ability to maintain physical balance constraints during DA, enabling observed variables to correct unobserved ones effectively. In single-point perturbation DA experiments, XiChen exhibits flow-dependent characteristics similar to those of traditional 4DVar systems. These results demonstrate that XiChen holds strong potential for fully AI-driven weather forecasting independent of NWP systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models</title>
<link>https://arxiv.org/abs/2507.10714</link>
<guid>https://arxiv.org/abs/2507.10714</guid>
<content:encoded><![CDATA[
arXiv:2507.10714v2 Announce Type: replace 
Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for modeling discrete-event dynamics in areas such as epidemiology and systems biology, yet their parameter estimation remains challenging in general and in particular when transition rates depend on external covariates and explicit likelihoods are unavailable. We introduce a neural-surrogate (neural-network-based approximation of the posterior distribution) framework that predicts the coefficients of known covariate-dependent rate functions directly from noisy, partially observed token trajectories. Our model employs a lightweight 1D Convolutional Residual Network trained end-to-end on Gillespie-simulated SPN realizations, learning to invert system dynamics under realistic conditions of event dropout. During inference, Monte Carlo dropout provides calibrated uncertainty bounds together with point estimates. On synthetic SPNs with $10\%$ missing events, our surrogate recovers rate-function coefficients with an $RMSE = 0.043$ and substantially runs faster than traditional Bayesian approaches. These results demonstrate that data-driven, likelihood-free surrogates can enable accurate, robust, and real-time parameter recovery in complex, partially observed discrete-event systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title>
<link>https://arxiv.org/abs/2507.16302</link>
<guid>https://arxiv.org/abs/2507.16302</guid>
<content:encoded><![CDATA[
arXiv:2507.16302v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic</title>
<link>https://arxiv.org/abs/2507.17876</link>
<guid>https://arxiv.org/abs/2507.17876</guid>
<content:encoded><![CDATA[
arXiv:2507.17876v2 Announce Type: replace 
Abstract: The scarcity of molecules with desirable properties (i.e., `positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' - without accessing any positively labeled data - and moving models in the opposite property directions to generate positive molecules. When analyzed on 33 design experiments with distinct molecular entities (small molecules, proteins), model architectures, and scales, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules in general. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable complex design properties, such as good docking scores to a protein. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the de facto transfer learning strategy for de novo molecule design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density Operator Expectation Maximization</title>
<link>https://arxiv.org/abs/2507.22786</link>
<guid>https://arxiv.org/abs/2507.22786</guid>
<content:encoded><![CDATA[
arXiv:2507.22786v2 Announce Type: replace 
Abstract: Machine learning with density operators, the mathematical foundation of quantum mechanics, is gaining prominence with rapid advances in quantum computing. Generative models based on density operators cannot yet handle tasks that are routinely handled by probabilistic models. The progress of latent variable models, a broad and influential class of probabilistic unsupervised models, was driven by the Expectation-Maximization framework. Deriving such a framework for density operators is challenging due to the non-commutativity of operators. To tackle this challenge, an inequality arising from the monotonicity of relative entropy is demonstrated to serve as an evidence lower bound for density operators. A minorant-maximization perspective on this bound leads to Density Operator Expectation Maximization (DO-EM), a general framework for training latent variable models defined through density operators. Through an information-geometric argument, the Expectation step in DO-EM is shown to be the Petz recovery map. The DO-EM algorithm is applied to Quantum Restricted Boltzmann Machines, adapting Contrastive Divergence to approximate the Maximization step gradient. Quantum interleaved Deep Boltzmann Machines and Quantum Gaussian-Bernoulli Restricted Boltzmann Machines, new models introduced in this work, outperform their probabilistic counterparts on generative tasks when trained with similar computational resources and identical hyperparameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain</title>
<link>https://arxiv.org/abs/2508.01888</link>
<guid>https://arxiv.org/abs/2508.01888</guid>
<content:encoded><![CDATA[
arXiv:2508.01888v2 Announce Type: replace 
Abstract: The increasing penetration of renewable energy sources in day-ahead energy markets introduces challenges in balancing supply and demand, ensuring grid resilience, and maintaining trust in decentralized trading systems. This paper proposes a novel framework that integrates the Proximal Policy Optimization (PPO) algorithm, a state-of-the-art reinforcement learning method, with blockchain technology to optimize automated trading strategies for prosumers in day-ahead energy markets. We introduce a comprehensive framework that employs RL agent for multi-objective energy optimization and blockchain for tamper-proof data and transaction management. Simulations using real-world data from the Electricity Reliability Council of Texas (ERCOT) demonstrate the effectiveness of our approach. The RL agent achieves demand-supply balancing within 2\% and maintains near-optimal supply costs for the majority of the operating hours. Moreover, it generates robust battery storage policies capable of handling variability in solar and wind generation. All decisions are recorded on an Algorand-based blockchain, ensuring transparency, auditability, and security - key enablers for trustworthy multi-agent energy trading. Our contributions include a novel system architecture, curriculum learning for robust agent development, and actionable policy insights for practical deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
<link>https://arxiv.org/abs/2508.02002</link>
<guid>https://arxiv.org/abs/2508.02002</guid>
<content:encoded><![CDATA[
arXiv:2508.02002v2 Announce Type: replace 
Abstract: Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</title>
<link>https://arxiv.org/abs/2508.06041</link>
<guid>https://arxiv.org/abs/2508.06041</guid>
<content:encoded><![CDATA[
arXiv:2508.06041v4 Announce Type: replace 
Abstract: How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding steps. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
<link>https://arxiv.org/abs/2508.06066</link>
<guid>https://arxiv.org/abs/2508.06066</guid>
<content:encoded><![CDATA[
arXiv:2508.06066v2 Announce Type: replace 
Abstract: Deep temporal architectures such as TCNs achieve strong predictive performance on sequential data, yet theoretical understanding of their generalization remains limited. We address this gap through three contributions: introducing an evaluation methodology for temporal models, revealing surprising empirical phenomena about temporal dependence, and the first architecture-aware theoretical framework for dependent sequences.
  Fair-Comparison Methodology. We introduce evaluation protocols that fix effective sample size $N_{\text{eff}}$ to isolate temporal structure effects from information content.
  Empirical Findings. Applying this method reveals that under $N_{\text{eff}} = 2000$, strongly dependent sequences ($\rho = 0.8$) exhibit approx' $76\%$ smaller generalization gaps than weakly dependent ones ($\rho = 0.2$), challenging the conventional view that dependence universally impedes learning. However, observed convergence rates ($N_{\text{eff}}^{-1.21}$ to $N_{\text{eff}}^{-0.89}$) significantly exceed theoretical worst-case predictions ($N^{-0.5}$), revealing that temporal architectures exploit problem structure in ways current theory does not capture.
  Lastly, we develop the first architecture-aware generalization bounds for deep temporal models on exponentially $\beta$-mixing sequences. By embedding Golowich et al.'s i.i.d. class bound within a novel blocking scheme that partitions $N$ samples into approx' $B \approx N/\log N$ quasi-independent blocks, we establish polynomial sample complexity under convex Lipschitz losses. The framework achieves $\sqrt{D}$ depth scaling alongside the product of layer-wise norms $R = \prod_{\ell=1}^{D} M^{(\ell)}$, avoiding exponential dependence. While these bounds are conservative, they prove learnability and identify architectural scaling laws, providing worst-case baselines that highlight where future theory must improve.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo</title>
<link>https://arxiv.org/abs/2508.07631</link>
<guid>https://arxiv.org/abs/2508.07631</guid>
<content:encoded><![CDATA[
arXiv:2508.07631v3 Announce Type: replace 
Abstract: We study the problem of posterior sampling in the context of score based generative models. We have a trained score network for a prior $p(x)$, a measurement model $p(y|x)$, and are tasked with sampling from the posterior $p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case) under well-accepted computational hardness assumptions. Despite this, popular algorithms for tasks such as image super-resolution, stylization, and reconstruction enjoy empirical success. Rather than establishing distributional assumptions or restricted settings under which exact posterior sampling is tractable, we view this as a more general "tilting" problem of biasing a distribution towards a measurement. Under minimal assumptions, we show that one can tractably sample from a distribution that is simultaneously close to the posterior of a noised prior in KL divergence and the true posterior in Fisher divergence. Intuitively, this combination ensures that the resulting sample is consistent with both the measurement and the prior. To the best of our knowledge these are the first formal results for (approximate) posterior sampling in polynomial time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[
arXiv:2508.08005v3 Announce Type: replace 
Abstract: The Maximum Clique Problem (MCP) is a foundational NP-hard problem with wide-ranging applications, yet no single algorithm consistently outperforms all others across diverse graph instances. This underscores the critical need for instance-aware algorithm selection, a domain that remains largely unexplored for the MCP. To address this gap, we propose a novel learning-based framework that integrates both traditional machine learning and graph neural networks. We first construct a benchmark dataset by executing four state-of-the-art exact MCP solvers on a diverse collection of graphs and extracting their structural features. An evaluation of conventional classifiers establishes Random Forest as a strong baseline and reveals that connectivity and topological features are key predictors of performance. Building on these insights, we develop GAT-MLP, a dual-channel model that combines a Graph Attention Network (GAT) to encode local graph structure with a Multilayer Perceptron (MLP) to model global features. Extensive experiments demonstrate that GAT-MLP achieves superior and consistent performance, significantly outperforming all baseline methods. Our results highlight the effectiveness of the dual-channel architecture and the promise of graph neural networks for combinatorial algorithm selection, achieving 90.43% accuracy in choosing the optimal solver. Code and models are available at: https://anonymous.4open.science/r/GAT-MLP-7E5F.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</title>
<link>https://arxiv.org/abs/2508.08222</link>
<guid>https://arxiv.org/abs/2508.08222</guid>
<content:encoded><![CDATA[
arXiv:2508.08222v2 Announce Type: replace 
Abstract: Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jointly Computation- and Communication-Efficient Distributed Learning</title>
<link>https://arxiv.org/abs/2508.15509</link>
<guid>https://arxiv.org/abs/2508.15509</guid>
<content:encoded><![CDATA[
arXiv:2508.15509v2 Announce Type: replace 
Abstract: We address distributed learning problems over undirected networks. Specifically, we focus on designing a novel ADMM-based algorithm that is jointly computation- and communication-efficient. Our design guarantees computational efficiency by allowing agents to use stochastic gradients during local training. Moreover, communication efficiency is achieved as follows: i) the agents perform multiple training epochs between communication rounds, and ii) compressed transmissions are used. We prove exact linear convergence of the algorithm in the strongly convex setting. We corroborate our theoretical results by numerical comparisons with state of the art techniques on a classification task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems</title>
<link>https://arxiv.org/abs/2508.15695</link>
<guid>https://arxiv.org/abs/2508.15695</guid>
<content:encoded><![CDATA[
arXiv:2508.15695v2 Announce Type: replace 
Abstract: We present several key advances to the Physics and Equality Constrained Artificial Neural Networks (PECANN) framework, substantially improving its capacity to solve challenging partial differential equations (PDEs). Our enhancements broaden the framework's applicability and improve efficiency. First, we generalize the Augmented Lagrangian Method (ALM) to support multiple, independent penalty parameters for enforcing heterogeneous constraints. Second, we introduce a constraint aggregation technique to address inefficiencies associated with point-wise enforcement. Third, we incorporate a single Fourier feature mapping to capture highly oscillatory solutions with multi-scale features, where alternative methods often require multiple mappings or costlier architectures. Fourth, a novel time-windowing strategy enables seamless long-time evolution without relying on discrete time models. Fifth, and critically, we propose a conditionally adaptive penalty update (CAPU) strategy for ALM that accelerates the growth of Lagrange multipliers for constraints with larger violations, while enabling coordinated updates of multiple penalty parameters. CAPU accelerates the growth of Lagrange multipliers for selectively challenging constraints, enhancing constraint enforcement during training. We demonstrate the effectiveness of PECANN-CAPU across diverse problems, including the transonic rarefaction problem, reversible scalar advection by a vortex, high-wavenumber Helmholtz and Poisson's equations, and inverse heat source identification. The framework achieves competitive accuracy across all cases when compared with established methods and recent approaches based on Kolmogorov-Arnold networks. Collectively, these advances improve the robustness, computational efficiency, and applicability of PECANN to demanding problems in scientific computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ONG: Orthogonal Natural Gradient Descent</title>
<link>https://arxiv.org/abs/2508.17169</link>
<guid>https://arxiv.org/abs/2508.17169</guid>
<content:encoded><![CDATA[
arXiv:2508.17169v3 Announce Type: replace 
Abstract: Orthogonal Gradient Descent (OGD) has emerged as a powerful method for continual learning. However, its Euclidean projections do not leverage the underlying information-geometric structure of the problem, which can lead to suboptimal convergence in learning tasks. To address this, we propose incorporating the natural gradient into OGD and present \textbf{ONG (Orthogonal Natural Gradient Descent)}. ONG preconditions each new task-specific gradient with an efficient EKFAC approximation of the inverse Fisher information matrix, yielding updates that follow the steepest descent direction under a Riemannian metric. To preserve performance on previously learned tasks, ONG projects these natural gradients onto the orthogonal complement of prior tasks' natural gradients. We provide an initial theoretical justification for this procedure, introduce the Orthogonal Natural Gradient Descent (ONG) algorithm, and present preliminary results on the Permuted and Rotated MNIST benchmarks. Our preliminary results, however, indicate that a naive combination of natural gradients and orthogonal projections has potential issues. This finding has motivated continued future work focused on robustly reconciling these geometric perspectives to develop a continual learning method, establishing a more rigorous theoretical foundation with formal convergence guarantees, and extending empirical validation to large-scale continual learning benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</title>
<link>https://arxiv.org/abs/2508.20549</link>
<guid>https://arxiv.org/abs/2508.20549</guid>
<content:encoded><![CDATA[
arXiv:2508.20549v2 Announce Type: replace 
Abstract: The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaGuard: Jamming Correction of GNSS Deviation with Deep Temporal Graphs</title>
<link>https://arxiv.org/abs/2509.14000</link>
<guid>https://arxiv.org/abs/2509.14000</guid>
<content:encoded><![CDATA[
arXiv:2509.14000v2 Announce Type: replace 
Abstract: Global Navigation Satellite Systems (GNSS) are increasingly exposed to intentional jamming, threatening reliability when accurate positioning and timing are most critical. We address this problem by formulating interference mitigation as a dynamic graph regression task and propose JaGuard, a receiver-centric temporal graph neural network that estimates and corrects latitude and longitude errors. At each 1 Hz epoch, the satellite-receiver scene is represented as a heterogeneous star graph with time-varying satellite attributes such as SNR, azimuth and elevation. A single-layer HeteroGCLSTM fuses one-hop spatial context with short-term temporal dynamics to produce a 2D deviation estimate.
  We evaluate JaGuard on data collected from two commercial receivers under controlled conducted jamming using three jammer types (CW, 3xCW, FM) and six power levels from -45 to -70 dBm, each repeated 50 times across pre-jam, jam, and recovery phases. JaGuard outperforms strong multivariate baselines (TSMixer, uniform CNN, Seq2Point) in all conditions. Under severe jamming at -45 dBm, it achieves 3.64-7.74 cm MAE, improving to 1.59-1.90 cm for -60 to -70 dBm. On mixed-mode datasets, it attains 3.78 cm MAE on GP01 and 4.25 cm on U-blox 10. With only 10 percent of the training data, JaGuard remains ahead, reaching about 20 cm MAE compared to 36-42 cm for the baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidential Physics-Informed Neural Networks for Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.14568</link>
<guid>https://arxiv.org/abs/2509.14568</guid>
<content:encoded><![CDATA[
arXiv:2509.14568v3 Announce Type: replace 
Abstract: We present the fundamental theory and implementation guidelines underlying Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of uncertainty-aware PINN. It leverages the marginal distribution loss function of evidential deep learning for estimating uncertainty of outputs, and infers unknown parameters of the PDE via a learned posterior distribution. Validating our model on two illustrative case studies -- the 1D Poisson equation with a Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods. To demonstrate real-world applicability, we also present a brief case study on applying E-PINN to analyze clinical glucose-insulin datasets that have featured in medical research on diabetes pathophysiology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Multi-Query Paradox in Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2509.15552</link>
<guid>https://arxiv.org/abs/2509.15552</guid>
<content:encoded><![CDATA[
arXiv:2509.15552v3 Announce Type: replace 
Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems where explicit gradients are unavailable and have to be approximated using only queries to function value. The prevalent single-query approach is simple, but suffers from high estimation variance, motivating a multi-query paradigm to improves estimation accuracy. This, however, creates a critical trade-off: under a fixed budget of queries (i.e. cost), queries per iteration and the total number of optimization iterations are inversely proportional to one another. How to best allocate this budget is a fundamental, under-explored question.
  This work systematically resolves this query allocation problem. We analyze two aggregation methods: the de facto simple averaging (ZO-Avg), and a new Projection Alignment method (ZO-Align) we derive from local surrogate minimization. By deriving convergence rates for both methods that make the dependence on the number of queries explicit across strongly convex, convex, non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient, rendering the single-query approach optimal. On the contrary, ZO-Align generally performs better with more queries per iteration, resulting in a full-subspace estimation as the optimal approach. Thus, our work clarifies that the multi-query problem boils down to a choice not about an intermediate query size, but between two classic algorithms, a choice dictated entirely by the aggregation method used. These theoretical findings are also consistently validated by extensive experiments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Staying on the Manifold: Geometry-Aware Noise Injection</title>
<link>https://arxiv.org/abs/2509.20201</link>
<guid>https://arxiv.org/abs/2509.20201</guid>
<content:encoded><![CDATA[
arXiv:2509.20201v2 Announce Type: replace 
Abstract: It has been shown that perturbing the input during training implicitly regularises the gradient of the learnt function, leading to smoother models and enhancing generalisation. However, previous research mostly considered the addition of ambient noise in the input space, without considering the underlying structure of the data. In this work, we propose several strategies of adding geometry-aware input noise that accounts for the lower dimensional manifold the input space inhabits. We start by projecting ambient Gaussian noise onto the tangent space of the manifold. In a second step, the noise sample is mapped on the manifold via the associated geodesic curve. We also consider Brownian motion noise, which moves in random steps along the manifold. We show that geometry-aware noise leads to improved generalisation and robustness to hyperparameter selection on highly curved manifolds, while performing at least as well as training without noise on simpler manifolds. Our proposed framework extends to data manifolds approximated by generative models and we observe similar trends on the MNIST digits dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.20616</link>
<guid>https://arxiv.org/abs/2509.20616</guid>
<content:encoded><![CDATA[
arXiv:2509.20616v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias</title>
<link>https://arxiv.org/abs/2509.21181</link>
<guid>https://arxiv.org/abs/2509.21181</guid>
<content:encoded><![CDATA[
arXiv:2509.21181v3 Announce Type: replace 
Abstract: For overparameterized linear regression with isotropic Gaussian design and minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\{ \lVert \widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\alpha$ to an effective $p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p} \rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[
arXiv:2509.22601v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuON: A Linear-time Alternative to Orthogonal Momentum Updates</title>
<link>https://arxiv.org/abs/2509.24320</link>
<guid>https://arxiv.org/abs/2509.24320</guid>
<content:encoded><![CDATA[
arXiv:2509.24320v3 Announce Type: replace 
Abstract: Orthogonal momentum gradient updates have emerged to overcome the limitations of vector-based optimizers like Adam. The vector-based optimizer Adam suffers from high memory costs and ill-conditioned momentum gradient updates. However, traditional Orthogonal momentum approaches, such as SVD/QR decomposition, suffer from high computational and memory costs and underperform compared to well-tuned SGD with momentum.Recent advances, such as Muon, improve efficiency by applying momentum before orthogonalization and approximate orthogonal matrices via Newton-Schulz iterations, which gives better GPU utilization, active high TFLOPS, and reduces memory usage by up to 3x. Nevertheless, Muon(Vanilla) suffers from exploding attention logits and has cubic computation complexity. In this paper we , deep dive into orthogonal momentum gradient updates to find the main properties that help Muon to achieve remarkable performance.We propose \textbf{AuON} (Alternative Unit-norm momentum updates by Normalized nonlinear scaling), a linear-time optimizer that achieves strong performance without approximate orthogonal matrices, while preserving structural alignment and reconditioning ill-posed updates. AuON has an automatic (\textbf{"emergency brake"}) to handle exploding attention logits.. We further introduce a hybrid variant (\textbf{ Hybrid-AuON})that applies the linear transformations with Newton-Schulz iterations which out performs Muon in the language modeling tasks. Code is available at: https://github.com/ryyzn9/AuON
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids</title>
<link>https://arxiv.org/abs/2509.25158</link>
<guid>https://arxiv.org/abs/2509.25158</guid>
<content:encoded><![CDATA[
arXiv:2509.25158v2 Announce Type: replace 
Abstract: Voltage prediction in distribution grids is a critical yet difficult task for maintaining power system stability. Machine learning approaches, particularly Graph Neural Networks (GNNs), offer significant speedups but suffer from poor generalization when trained on limited or incomplete data. In this work, we systematically investigate the role of inductive biases in improving a model's ability to reliably learn power flow. Specifically, we evaluate three physics-informed strategies: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation. Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid configurations, we conduct controlled experiments to isolate the effect of each inductive bias and assess both standard predictive performance and out-of-distribution generalization. Our study provides practical insights into which model assumptions most effectively guide learning for reliable and efficient voltage prediction in modern distribution networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title>
<link>https://arxiv.org/abs/2510.00586</link>
<guid>https://arxiv.org/abs/2510.00586</guid>
<content:encoded><![CDATA[
arXiv:2510.00586v2 Announce Type: replace 
Abstract: Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01132</link>
<guid>https://arxiv.org/abs/2510.01132</guid>
<content:encoded><![CDATA[
arXiv:2510.01132v2 Announce Type: replace 
Abstract: We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</title>
<link>https://arxiv.org/abs/2510.02695</link>
<guid>https://arxiv.org/abs/2510.02695</guid>
<content:encoded><![CDATA[
arXiv:2510.02695v2 Announce Type: replace 
Abstract: In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value or model-based pessimism, and restricted policy classes that limit policy expressiveness, whereas diffusion/flow-based expressive generative policies trained with a behavioral-cloning (BC) objective have been used only in risk-neutral settings. Here, we address this gap by introducing the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)}, which couples an expressive generative actor with a distributional critic and, to our knowledge, is the first model-free approach that learns \emph{risk-aware expressive generative policies}. RAMAC differentiates a composite objective that adds a Conditional Value-at-Risk (CVaR) term to a BC loss, achieving risk-sensitive learning in complex multimodal scenarios. Since out-of-distribution (OOD) actions are a major driver of catastrophic failures in offline RL, we further analyze OOD behavior under prior-anchored perturbation schemes from recent BC-regularized risk-averse offline RL. This clarifies why a behavior-regularized objective that directly constrains the expressive generative policy to the dataset support provides an effective, risk-agnostic mechanism for suppressing OOD actions in modern expressive policies. We instantiate RAMAC with a diffusion-based actor, using it both to illustrate the analysis in a 2-D risky bandit and to deploy OOD-action detectors on Stochastic-D4RL benchmarks, empirically validating our insights. Across these tasks, we observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining strong returns. Our implementation is available at GitHub: https://github.com/KaiFukazawa/RAMAC.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Exploratory Bonus for Optimistic Exploration in RLHF</title>
<link>https://arxiv.org/abs/2510.03269</link>
<guid>https://arxiv.org/abs/2510.03269</guid>
<content:encoded><![CDATA[
arXiv:2510.03269v3 Announce Type: replace 
Abstract: Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</title>
<link>https://arxiv.org/abs/2510.10645</link>
<guid>https://arxiv.org/abs/2510.10645</guid>
<content:encoded><![CDATA[
arXiv:2510.10645v3 Announce Type: replace 
Abstract: Retrosynthesis is one of the domains transformed by the rise of generative models, and it is one where the problem of nonsensical or erroneous outputs (hallucinations) is particularly insidious: reliable assessment of synthetic plans is time-consuming, with automatic methods lacking. In this work, we present RetroTrim, a retrosynthesis system that successfully avoids nonsensical plans on a set of challenging drug-like targets. Compared to common baselines in the field, our system is not only the sole method that succeeds in filtering out hallucinated reactions, but it also results in the highest number of high-quality paths overall. The key insight behind RetroTrim is the combination of diverse reaction scoring strategies, based on machine learning models and existing chemical databases. We show that our scoring strategies capture different classes of hallucinations by analyzing them on a dataset of labeled retrosynthetic intermediates. This approach formed the basis of our winning solution to the Standard Industries \$1 million Retrosynthesis Challenge. To measure the performance of retrosynthesis systems, we propose a novel evaluation protocol for reactions and synthetic paths based on a structured review by expert chemists. Using this protocol, we compare systems on a set of 32 novel targets, curated to reflect recent trends in drug structures. While the insights behind our methodology are broadly applicable to retrosynthesis, our focus is on targets in the drug-like domain. By releasing our benchmark targets and the details of our evaluation protocol, we hope to inspire further research into reliable retrosynthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training</title>
<link>https://arxiv.org/abs/2510.14614</link>
<guid>https://arxiv.org/abs/2510.14614</guid>
<content:encoded><![CDATA[
arXiv:2510.14614v2 Announce Type: replace 
Abstract: As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline. Codes are available at: https://github.com/CASL-KU/FAL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference</title>
<link>https://arxiv.org/abs/2510.17933</link>
<guid>https://arxiv.org/abs/2510.17933</guid>
<content:encoded><![CDATA[
arXiv:2510.17933v2 Announce Type: replace 
Abstract: Detecting regime shifts in chaotic time series is hard because observation-space signals are entangled with intrinsic variability. We propose Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework that first amortizes Bayesian inference of governing parameters with a neural posterior estimator trained by simulation-based inference, and then applies a standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63 with piecewise-constant parameters, Param--CPD improves F1, reduces localization error, and lowers false positives compared to observation--space baselines. We further verify identifiability and calibration of the inferred posteriors on stationary trajectories, explaining why parameter space offers a cleaner detection signal. Robustness analyses over tolerance, window length, and noise indicate consistent gains. Our results show that operating in a physically interpretable parameter space enables accurate and interpretable changepoint detection in nonlinear dynamical systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</title>
<link>https://arxiv.org/abs/2510.19296</link>
<guid>https://arxiv.org/abs/2510.19296</guid>
<content:encoded><![CDATA[
arXiv:2510.19296v4 Announce Type: replace 
Abstract: The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/QiMeng-IPRC/QiMeng-SALV.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-diffusion for Solving Inverse Problems</title>
<link>https://arxiv.org/abs/2510.21417</link>
<guid>https://arxiv.org/abs/2510.21417</guid>
<content:encoded><![CDATA[
arXiv:2510.21417v2 Announce Type: replace 
Abstract: We propose self-diffusion, a novel framework for solving inverse problems without relying on pretrained generative models. Traditional diffusion-based approaches require training a model on a clean dataset to learn to reverse the forward noising process. This model is then used to sample clean solutions -- corresponding to posterior sampling from a Bayesian perspective -- that are consistent with the observed data under a specific task. In contrast, self-diffusion introduces a self-contained iterative process that alternates between noising and denoising steps to progressively refine its estimate of the solution. At each step of self-diffusion, noise is added to the current estimate, and a self-denoiser, which is a single untrained convolutional network randomly initialized from scratch, is continuously trained for certain iterations via a data fidelity loss to predict the solution from the noisy estimate. Essentially, self-diffusion exploits the spectral bias of neural networks and modulates it through a scheduled noise process. Without relying on pretrained score functions or external denoisers, this approach still remains adaptive to arbitrary forward operators and noisy observations, making it highly flexible and broadly applicable. We demonstrate the effectiveness of our approach on a variety of linear inverse problems, showing that self-diffusion achieves competitive or superior performance compared to other methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks</title>
<link>https://arxiv.org/abs/2510.22021</link>
<guid>https://arxiv.org/abs/2510.22021</guid>
<content:encoded><![CDATA[
arXiv:2510.22021v2 Announce Type: replace 
Abstract: Neural networks are powerful parametric function approximators, while Gaussian processes (GPs) are nonparametric probabilistic models that place distributions over functions via kernel-defined correlations but become computationally expensive for large-scale problems. Kolmogorov-Arnold networks (KANs), semi-parametric neural architectures, model complex functions efficiently using spline layers. Kurkova Kolmogorov-Arnold networks (KKANs) extend KANs by replacing the early spline layers with multi-layer perceptrons that map inputs into higher-dimensional spaces before applying spline-based transformations, which yield more stable training and provide robust architectures for system modeling. By enhancing the KKAN architecture, we develop a novel learning algorithm, distance-aware error for Kurkova-Kolmogorov networks (K-DAREK), for efficient and interpretable function approximation with uncertainty quantification. Our approach establishes robust error bounds that are distance-aware; this means they reflect the proximity of a test point to its nearest training points. In safe control case studies, we demonstrate that K-DAREK is about four times faster and ten times more computationally efficient than Ensemble of KANs, 8.6 times more scalable than GP as data size increases, and 7.2% safer than our previous work distance-aware error for Kolmogorov networks (DAREK). Moreover, on real data (e.g., Real Estate Valuation), K-DAREK's error bound achieves zero coverage violations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEX: Continuous Agent Evolution via Forward Learning from Experience</title>
<link>https://arxiv.org/abs/2511.06449</link>
<guid>https://arxiv.org/abs/2511.06449</guid>
<content:encoded><![CDATA[
arXiv:2511.06449v2 Announce Type: replace 
Abstract: Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08340</link>
<guid>https://arxiv.org/abs/2511.08340</guid>
<content:encoded><![CDATA[
arXiv:2511.08340v2 Announce Type: replace 
Abstract: Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization</title>
<link>https://arxiv.org/abs/2511.09173</link>
<guid>https://arxiv.org/abs/2511.09173</guid>
<content:encoded><![CDATA[
arXiv:2511.09173v2 Announce Type: replace 
Abstract: Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2511.09731</link>
<guid>https://arxiv.org/abs/2511.09731</guid>
<content:encoded><![CDATA[
arXiv:2511.09731v2 Announce Type: replace 
Abstract: Radar-based precipitation nowcasting, the task of forecasting short-term precipitation fields from previous radar images, is a critical problem for flood risk management and decision-making. While deep learning has substantially advanced this field, two challenges remain fundamental: the uncertainty of atmospheric dynamics and the efficient modeling of high-dimensional data. Diffusion models have shown strong promise by producing sharp, reliable forecasts, but their iterative sampling process is computationally prohibitive for time-critical applications. We introduce FlowCast, the first end-to-end probabilistic model leveraging Conditional Flow Matching (CFM) as a direct noise-to-data generative framework for precipitation nowcasting. Unlike hybrid approaches, FlowCast learns a direct noise-to-data mapping in a compressed latent space, enabling rapid, high-fidelity sample generation. Our experiments demonstrate that FlowCast establishes a new state-of-the-art in probabilistic performance while also exceeding deterministic baselines in predictive accuracy. A direct comparison further reveals the CFM objective is both more accurate and significantly more efficient than a diffusion objective on the same architecture, maintaining high performance with significantly fewer sampling steps. This work positions CFM as a powerful and practical alternative for high-dimensional spatiotemporal forecasting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices</title>
<link>https://arxiv.org/abs/2511.10680</link>
<guid>https://arxiv.org/abs/2511.10680</guid>
<content:encoded><![CDATA[
arXiv:2511.10680v2 Announce Type: replace 
Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering</title>
<link>https://arxiv.org/abs/2511.17008</link>
<guid>https://arxiv.org/abs/2511.17008</guid>
<content:encoded><![CDATA[
arXiv:2511.17008v2 Announce Type: replace 
Abstract: Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, whose model architecture comprises Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) generation modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the reconstruction and cluster-guided contrastive learning pathways enhance and connect the representation learning to clustering tasks. Extensive experiments on 15 benchmark datasets demonstrate the superiority of EMTC over eight SOTA methods, where the EMTC achieves an average improvement of 4.85% in F1-Score over the strongest baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.20993</link>
<guid>https://arxiv.org/abs/2511.20993</guid>
<content:encoded><![CDATA[
arXiv:2511.20993v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment</title>
<link>https://arxiv.org/abs/2511.21931</link>
<guid>https://arxiv.org/abs/2511.21931</guid>
<content:encoded><![CDATA[
arXiv:2511.21931v2 Announce Type: replace 
Abstract: In this work, we propose a simple and computationally efficient framework for evaluating whether machine learning models align with the structure of the data they learn from; that is, whether the model says what the data says. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings with model-based explanations, we provide practitioners with an interpretable and model-agnostic method for assessing model-data alignment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Tucker Convolutional Network for Water Quality Analysis</title>
<link>https://arxiv.org/abs/2512.01465</link>
<guid>https://arxiv.org/abs/2512.01465</guid>
<content:encoded><![CDATA[
arXiv:2512.01465v2 Announce Type: replace 
Abstract: Water quality monitoring is a core component of ecological environmental protection. However, due to sensor failure or other inevitable factors, data missing often exists in long-term monitoring, posing great challenges in water quality analysis. This paper proposes a Neural Tucker Convolutional Network (NTCN) model for water quality data imputation, which features the following key components: a) Encode different mode entities into respective embedding vectors, and construct a Tucker interaction tensor by outer product operations to capture the complex mode-wise feature interactions; b) Use 3D convolution to extract fine-grained spatiotemporal features from the interaction tensor. Experiments on three real-world water quality datasets show that the proposed NTCN model outperforms several state-of-the-art imputation models in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Air Pollution prediction model based on Spatiotemporal Big data</title>
<link>https://arxiv.org/abs/1805.00432</link>
<guid>https://arxiv.org/abs/1805.00432</guid>
<content:encoded><![CDATA[
arXiv:1805.00432v4 Announce Type: replace-cross 
Abstract: Air pollution is one of the most concerns for urban areas. Many countries have constructed monitoring stations to hourly collect pollution values. Recently, there is a research in Daegu city, Korea for real-time air quality monitoring via sensors installed on taxis running across the whole city. The collected data is huge (1-second interval) and in both Spatial and Temporal format. In this paper, based on this spatiotemporal Big data, we propose a real-time air pollution prediction model based on Convolutional Neural Network (CNN) algorithm for image-like Spatial distribution of air pollution. Regarding to Temporal information in the data, we introduce a combination of a Long Short-Term Memory (LSTM) unit for time series data and a Neural Network model for other air pollution impact factors such as weather conditions to build a hybrid prediction model. This model is simple in architecture but still brings good prediction ability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Supply Chain Interdependencies for Stock Return Prediction: A Full-State Graph Convolutional LSTM</title>
<link>https://arxiv.org/abs/2303.09406</link>
<guid>https://arxiv.org/abs/2303.09406</guid>
<content:encoded><![CDATA[
arXiv:2303.09406v2 Announce Type: replace-cross 
Abstract: Stock return prediction is fundamental to financial decision-making, yet traditional time series models fail to capture the complex interdependencies between companies in modern markets. We propose the Full-State Graph Convolutional LSTM (FS-GCLSTM), a novel temporal graph neural network that incorporates value-chain relationships to enhance stock return forecasting. Our approach features two key innovations: First, we represent inter-firm dependencies through value-chain networks, where nodes correspond to companies and edges capture supplier-customer relationships, enabling the model to leverage information beyond historical price data. Second, FS-GCLSTM applies graph convolutions to all LSTM components - current input features, previous hidden states, and cell states - ensuring that spatial information from the value-chain network influences every aspect of the temporal update mechanism. We evaluate FS-GCLSTM on Eurostoxx 600 and S&amp;P 500 datasets using LSEG value-chain data. While not achieving the lowest traditional prediction errors, FS-GCLSTM consistently delivers superior portfolio performance, attaining the highest annualized returns, Sharpe ratios, and Sortino ratios across both markets. Performance gains are more pronounced in the denser Eurostoxx 600 network, and robustness tests confirm stability across different input sequence lengths, demonstrating the practical value of integrating value-chain data with temporal graph neural networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks</title>
<link>https://arxiv.org/abs/2404.04671</link>
<guid>https://arxiv.org/abs/2404.04671</guid>
<content:encoded><![CDATA[
arXiv:2404.04671v5 Announce Type: replace-cross 
Abstract: This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development, relationships and capabilities, even in the absence of transparent training information.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSP-GNN: Learning to Track via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2407.04308</link>
<guid>https://arxiv.org/abs/2407.04308</guid>
<content:encoded><![CDATA[
arXiv:2407.04308v3 Announce Type: replace-cross 
Abstract: We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpha-VI DeepONet: A prior-robust variational Bayesian approach for enhancing DeepONets with uncertainty quantification</title>
<link>https://arxiv.org/abs/2408.00681</link>
<guid>https://arxiv.org/abs/2408.00681</guid>
<content:encoded><![CDATA[
arXiv:2408.00681v2 Announce Type: replace-cross 
Abstract: We introduce a novel deep operator network (DeepONet) framework that incorporates generalised variational inference (GVI) using R\'enyi's $\alpha$-divergence to learn complex operators while quantifying uncertainty. By incorporating Bayesian neural networks as the building blocks for the branch and trunk networks, our framework endows DeepONet with uncertainty quantification. The use of R\'enyi's $\alpha$-divergence, instead of the Kullback-Leibler divergence (KLD), commonly used in standard variational inference, mitigates issues related to prior misspecification that are prevalent in Variational Bayesian DeepONets. This approach offers enhanced flexibility and robustness. We demonstrate that modifying the variational objective function yields superior results in terms of minimising the mean squared error and improving the negative log-likelihood on the test set. Our framework's efficacy is validated across various mechanical systems, where it outperforms both deterministic and standard KLD-based VI DeepONets in predictive accuracy and uncertainty quantification. The hyperparameter $\alpha$, which controls the degree of robustness, can be tuned to optimise performance for specific problems. We apply this approach to a range of mechanics problems, including gravity pendulum, advection-diffusion, and diffusion-reaction systems. Our findings underscore the potential of $\alpha$-VI DeepONet to advance the field of data-driven operator learning and its applications in engineering and scientific domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</title>
<link>https://arxiv.org/abs/2409.17120</link>
<guid>https://arxiv.org/abs/2409.17120</guid>
<content:encoded><![CDATA[
arXiv:2409.17120v2 Announce Type: replace-cross 
Abstract: This book explores the role of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in driving the progress of big data analytics and management. The book focuses on simplifying the complex mathematical concepts behind deep learning, offering intuitive visualizations and practical case studies to help readers understand how neural networks and technologies like Convolutional Neural Networks (CNNs) work. It introduces several classic models and technologies such as Transformers, GPT, ResNet, BERT, and YOLO, highlighting their applications in fields like natural language processing, image recognition, and autonomous driving. The book also emphasizes the importance of pre-trained models and how they can enhance model performance and accuracy, with instructions on how to apply these models in various real-world scenarios. Additionally, it provides an overview of key big data management technologies like SQL and NoSQL databases, as well as distributed computing frameworks such as Apache Hadoop and Spark, explaining their importance in managing and processing vast amounts of data. Ultimately, the book underscores the value of mastering deep learning and big data management skills as critical tools for the future workforce, making it an essential resource for both beginners and experienced professionals.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications</title>
<link>https://arxiv.org/abs/2410.01268</link>
<guid>https://arxiv.org/abs/2410.01268</guid>
<content:encoded><![CDATA[
arXiv:2410.01268v3 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI), machine learning, and deep learning have become transformative forces in big data analytics and management, enabling groundbreaking advancements across diverse industries. This article delves into the foundational concepts and cutting-edge developments in these fields, with a particular focus on large language models (LLMs) and their role in natural language processing, multimodal reasoning, and autonomous decision-making. Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement learning, and generative models has enhanced the capabilities of AI systems to process, visualize, and interpret complex datasets. Additionally, the emergence of technologies like edge computing and automated machine learning (AutoML) democratizes access to AI, empowering users across skill levels to engage with intelligent systems. This work also underscores the importance of ethical considerations, transparency, and fairness in the deployment of AI technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software environments, and real-world applications, this article serves as a comprehensive resource for researchers and practitioners. By bridging theoretical underpinnings with actionable strategies, it showcases the potential of AI and LLMs to revolutionize big data management and drive meaningful advancements across domains such as healthcare, finance, and autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs</title>
<link>https://arxiv.org/abs/2410.02799</link>
<guid>https://arxiv.org/abs/2410.02799</guid>
<content:encoded><![CDATA[
arXiv:2410.02799v2 Announce Type: replace-cross 
Abstract: Kidney exchange programs have substantially increased transplantation rates but also raise critical concerns about fairness in organ allocation. We propose a novel framework leveraging Data Envelopment Analysis (DEA) to evaluate multiple dimensions of fairness-Priority, Access, and Outcome-within a unified model. This approach captures complexities often missed in single-metric analyses. Using data from the United Network for Organ Sharing, we separately quantify fairness across these dimensions: Priority fairness through waitlist durations, Access fairness via the Living Kidney Donor Profile Index (LKDPI) scores, and Outcome fairness based on graft lifespan. We then apply our conditional DEA model with covariate adjustment to demonstrate significant disparities in kidney allocation efficiency across ethnic groups. To quantify uncertainty, we employ conformal prediction within a novel Reference Frontier Mapping (RFM) framework, yielding group-conditional prediction intervals with finite-sample coverage guarantees. Our findings show notable differences in efficiency distributions between ethnic groups. Our study provides a rigorous framework for evaluating fairness in complex resource allocation systems with resource scarcity and mutual compatibility constraints.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Taggants: Dataset Ownership Verification via Harmless Targeted Data Poisoning</title>
<link>https://arxiv.org/abs/2410.09101</link>
<guid>https://arxiv.org/abs/2410.09101</guid>
<content:encoded><![CDATA[
arXiv:2410.09101v2 Announce Type: replace-cross 
Abstract: Dataset ownership verification, the process of determining if a dataset is used in a model's training data, is necessary for detecting unauthorized data usage and data contamination. Existing approaches, such as backdoor watermarking, rely on inducing a detectable behavior into the trained model on a part of the data distribution. However, these approaches have limitations, as they can be harmful to the model's performances or require unpractical access to the model's internals. Most importantly, previous approaches lack guarantee against false positives. This paper introduces data taggants, a novel non-backdoor dataset ownership verification technique. Our method uses pairs of out-of-distribution samples and random labels as secret keys, and leverages clean-label targeted data poisoning to subtly alter a dataset, so that models trained on it respond to the key samples with the corresponding key labels. The keys are built as to allow for statistical certificates with black-box access only to the model. We validate our approach through comprehensive and realistic experiments on ImageNet1k using ViT and ResNet models with state-of-the-art training recipes. Our findings demonstrate that data taggants can reliably detect models trained on the protected dataset with high confidence, without compromising validation accuracy, and show their superiority over backdoor watermarking. We demonstrate the stealthiness and robustness of our method against various defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can GNNs Learn Link Heuristics? A Concise Review and Evaluation of Link Prediction Methods</title>
<link>https://arxiv.org/abs/2411.14711</link>
<guid>https://arxiv.org/abs/2411.14711</guid>
<content:encoded><![CDATA[
arXiv:2411.14711v2 Announce Type: replace-cross 
Abstract: This paper explores the ability of Graph Neural Networks (GNNs) in learning various forms of information for link prediction, alongside a brief review of existing link prediction methods. Our analysis reveals that GNNs cannot effectively learn structural information related to the number of common neighbors between two nodes, primarily due to the nature of set-based pooling of the neighborhood aggregation scheme. Also, our extensive experiments indicate that trainable node embeddings can improve the performance of GNN-based link prediction models. Importantly, we observe that the denser the graph, the greater such the improvement. We attribute this to the characteristics of node embeddings, where the link state of each link sample could be encoded into the embeddings of nodes that are involved in the neighborhood aggregation of the two nodes in that link sample. In denser graphs, every node could have more opportunities to attend the neighborhood aggregation of other nodes and encode states of more link samples to its embedding, thus learning better node embeddings for link prediction. Lastly, we demonstrate that the insights gained from our research carry important implications in identifying the limitations of existing link prediction methods, which could guide the future development of more robust algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings</title>
<link>https://arxiv.org/abs/2411.18645</link>
<guid>https://arxiv.org/abs/2411.18645</guid>
<content:encoded><![CDATA[
arXiv:2411.18645v2 Announce Type: replace-cross 
Abstract: Inner interpretability is a promising field aiming to uncover the internal mechanisms of AI systems through scalable, automated methods. While significant research has been conducted on large language models, limited attention has been paid to applying inner interpretability to large-scale image tasks, focusing primarily on architectural and functional levels to visualize learned concepts. In this paper, we first present a conceptual framework that supports inner interpretability and multilevel analysis for large-scale image classification tasks. Specifically, we introduce the Bi-directional Interaction between Concept and Input Embeddings (Bi-ICE) module, which facilitates interpretability across the computational, algorithmic, and implementation levels. This module enhances transparency by generating predictions based on human-understandable concepts, quantifying their contributions, and localizing them within the inputs. Finally, we showcase enhanced transparency in image classification, measuring concept contributions, and pinpointing their locations within the inputs. Our approach highlights algorithmic interpretability by demonstrating the process of concept learning and its convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Neutrino-Nucleus Cross Sections</title>
<link>https://arxiv.org/abs/2412.16303</link>
<guid>https://arxiv.org/abs/2412.16303</guid>
<content:encoded><![CDATA[
arXiv:2412.16303v3 Announce Type: replace-cross 
Abstract: Neutrino-nucleus scattering cross sections are critical theoretical inputs for long-baseline neutrino oscillation experiments. However, robust modeling of these cross sections remains challenging. For a simple but physically motivated toy model of the DUNE experiment, we demonstrate that an accurate neural-network model of the cross section -- leveraging only Standard-Model symmetries -- can be learned from near-detector data. We perform a neutrino oscillation analysis with simulated far-detector events, finding that oscillation analysis results enabled by our data-driven cross-section model approach the theoretical limit achievable with perfect prior knowledge of the cross section. We further quantify the effects of flux shape and detector resolution uncertainties as well as systematics from cross-section mismodeling. This proof-of-principle study highlights the potential of future neutrino near-detector datasets and data-driven cross-section models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to explain grokking</title>
<link>https://arxiv.org/abs/2412.18624</link>
<guid>https://arxiv.org/abs/2412.18624</guid>
<content:encoded><![CDATA[
arXiv:2412.18624v4 Announce Type: replace-cross 
Abstract: Explanation of grokking (delayed generalization) in learning is given by modeling grokking by the stochastic gradient Langevin dynamics (Brownian motion) and applying the ideas of thermodynamics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Particle Algorithm for Mean-Field Variational Inference</title>
<link>https://arxiv.org/abs/2412.20385</link>
<guid>https://arxiv.org/abs/2412.20385</guid>
<content:encoded><![CDATA[
arXiv:2412.20385v3 Announce Type: replace-cross 
Abstract: Variational inference is a fast and scalable alternative to Markov chain Monte Carlo and has been widely applied to posterior inference tasks in statistics and machine learning. A traditional approach for implementing mean-field variational inference (MFVI) is coordinate ascent variational inference (CAVI), which relies crucially on parametric assumptions on complete conditionals. We introduce a novel particle-based algorithm for MFVI, named PArticle VI (PAVI), for nonparametric mean-field approximation. We obtain non-asymptotic error bounds for our algorithm. To our knowledge, this is the first end-to-end guarantee for particle-based MFVI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation</title>
<link>https://arxiv.org/abs/2502.03930</link>
<guid>https://arxiv.org/abs/2502.03930</guid>
<content:encoded><![CDATA[
arXiv:2502.03930v4 Announce Type: replace-cross 
Abstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Singular: Revealing the Value of Multiple Generations in Benchmark Evaluation</title>
<link>https://arxiv.org/abs/2502.08943</link>
<guid>https://arxiv.org/abs/2502.08943</guid>
<content:encoded><![CDATA[
arXiv:2502.08943v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant utility in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. Multiple generations also allow us to define $\mathbb P\left(\text{correct}\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantics of prompts, enabling error detection and quality control in benchmark construction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graceful forgetting: Memory as a process</title>
<link>https://arxiv.org/abs/2502.11105</link>
<guid>https://arxiv.org/abs/2502.11105</guid>
<content:encoded><![CDATA[
arXiv:2502.11105v4 Announce Type: replace-cross 
Abstract: A rational framework is proposed to explain how we accommodate unbounded sensory input within bounded memory. Memory is stored as statistics organized into structures that are repeatedly summarized and compressed to make room for new input. Repeated summarization requires an intensive ongoing process guided by heuristics that help optimize the memory for future needs. Sensory input is rapidly encoded as simple statistics that are progressively elaborated into more abstract constructs. This framework differs from previous accounts of memory by reliance on statistics as a representation of memory, the use of heuristics to guide the choice of statistics at each summarization step, and the hypothesis of a process that is complex and expensive. The framework is intended as an aid to make sense of our extensive knowledge of memory, and bring us closer to an understanding of memory in functional and mechanistic terms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration</title>
<link>https://arxiv.org/abs/2502.20333</link>
<guid>https://arxiv.org/abs/2502.20333</guid>
<content:encoded><![CDATA[
arXiv:2502.20333v2 Announce Type: replace-cross 
Abstract: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[
arXiv:2503.04598v4 Announce Type: replace-cross 
Abstract: Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermodynamic bounds on energy use in quasi-static Deep Neural Networks</title>
<link>https://arxiv.org/abs/2503.09980</link>
<guid>https://arxiv.org/abs/2503.09980</guid>
<content:encoded><![CDATA[
arXiv:2503.09980v3 Announce Type: replace-cross 
Abstract: The rapid growth of deep neural networks (DNNs) has brought increasing attention to their energy use during training and inference. Here, we establish the thermodynamic bounds on energy consumption in quasi-static analog DNNs by mapping modern feedforward architectures onto a physical free-energy functional. This framework provides a direct statistical-mechanical interpretation of quasi-static DNNs. As a result, inference can proceed in a thermodynamically reversible manner, with vanishing minimal energy cost, in contrast to the Landauer limit that constrains digital hardware. Importantly, inference corresponds to relaxation to a unique free-energy minimum with F_{\min}=0, allowing all constraints to be satisfied without residual stress. By comparison, training overconstrains the system: simultaneous clamping of inputs and outputs generates stresses that propagate backward through the architecture, reproducing the rules of backpropagation. Parameter annealing then relaxes these stresses, providing a purely physical route to learning without an explicit loss function. We further derive a universal lower bound on training energy, E< 2NDkT, which scales with both the number of trainable parameters and the dataset size.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024)</title>
<link>https://arxiv.org/abs/2503.14937</link>
<guid>https://arxiv.org/abs/2503.14937</guid>
<content:encoded><![CDATA[
arXiv:2503.14937v2 Announce Type: replace-cross 
Abstract: Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024), held in Pisa, Italy, September 17-19, 2024.
  The Italian Conference on Big Data and Data Science (ITADATA2024) is the annual event supported by the CINI Big Data National Laboratory and ISTI CNR that aims to put together Italian researchers and professionals from academia, industry, government, and public administration working in the field of big data and data science, as well as related fields (e.g., security and privacy, HPC, Cloud).
  ITADATA2024 covered research on all theoretical and practical aspects of Big Data and data science including data governance, data processing, data analysis, data reporting, data protection, as well as experimental studies and lessons learned. In particular, ITADATA2024 focused on
  - Data spaces
  - Data processing life cycle
  - Machine learning and Large Language Models
  - Applications of big data and data science in healthcare, finance, industry 5.0, and beyond
  - Data science for social network analysis
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title>
<link>https://arxiv.org/abs/2503.19912</link>
<guid>https://arxiv.org/abs/2503.19912</guid>
<content:encoded><![CDATA[
arXiv:2503.19912v2 Announce Type: replace-cross 
Abstract: LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation</title>
<link>https://arxiv.org/abs/2503.22370</link>
<guid>https://arxiv.org/abs/2503.22370</guid>
<content:encoded><![CDATA[
arXiv:2503.22370v5 Announce Type: replace-cross 
Abstract: We introduce the sequential multi-object robotic grasp sampling algorithm SeqGrasp that can robustly synthesize stable grasps on diverse objects using the robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp to construct the large-scale Allegro Hand sequential grasping dataset SeqDataset and use it for training the diffusion-based sequential grasp generator SeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against the state-of-the-art non-sequential multi-object grasp generation method MultiGrasp in simulation and on a real robot. The experimental results demonstrate that SeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate than MultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster at generating grasps than SeqGrasp and MultiGrasp. Project page: https://yulihn.github.io/SeqGrasp/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Enhanced Ensemble Filters</title>
<link>https://arxiv.org/abs/2504.17836</link>
<guid>https://arxiv.org/abs/2504.17836</guid>
<content:encoded><![CDATA[
arXiv:2504.17836v3 Announce Type: replace-cross 
Abstract: The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state-observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. Here this shortcoming is addressed by using machine learning to map the joint predicted state and observation to the updated state estimate. The derivation of methods from a mean field formulation of the true filtering distribution suggests a single parametrization of the algorithm that can be deployed at different ensemble sizes. And we use a mean field formulation of the ensemble Kalman filter as an inductive bias for our architecture.
  To develop this perspective, in which the mean-field limit of the algorithm and finite interacting ensemble particle approximations share a common set of parameters, a novel form of neural operator is introduced, taking probability distributions as input: a measure neural mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-field limit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root-mean-square-error performance relative to leading methods in filtering the Lorenz '96 and Kuramoto-Sivashinsky models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Machine Learning for Multivariate Angular Simulation</title>
<link>https://arxiv.org/abs/2504.21505</link>
<guid>https://arxiv.org/abs/2504.21505</guid>
<content:encoded><![CDATA[
arXiv:2504.21505v2 Announce Type: replace-cross 
Abstract: With the recent development of new geometric and angular-radial frameworks for multivariate extremes, reliably simulating from angular variables in moderate-to-high dimensions is of increasing importance. Empirical approaches have the benefit of simplicity, and work reasonably well in low dimensions, but as the number of variables increases, they can lack the required flexibility and scalability. Classical parametric models for angular variables, such as the von Mises--Fisher distribution (vMF), provide an alternative. Exploiting finite mixtures of vMF distributions increases their flexibility, but there are cases where, without letting the number of mixture components grow considerably, a mixture model with a fixed number of components is not sufficient to capture the intricate features that can arise in data. Owing to their flexibility, generative deep learning methods are able to capture complex data structures; they therefore have the potential to be useful in the simulation of multivariate angular variables. In this paper, we introduce a range of deep learning approaches for this task, including generative adversarial networks, normalizing flows and flow matching. We assess their performance via a range of metrics, and make comparisons to the more classical approach of using a finite mixture of vMF distributions. The methods are also applied to a metocean data set, with diagnostics indicating strong performance, demonstrating the applicability of such techniques to real-world, complex data structures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter estimation for land-surface models using Neural Physics</title>
<link>https://arxiv.org/abs/2505.02979</link>
<guid>https://arxiv.org/abs/2505.02979</guid>
<content:encoded><![CDATA[
arXiv:2505.02979v2 Announce Type: replace-cross 
Abstract: The Neural Physics approach is used to determine the parameters of a simple land-surface model using PyTorch's backpropagation engine to carry out the optimisation. In order to test the inverse model, a synthetic dataset is created by running the model in forward mode with known parameter values to create soil temperature time series that can be used as observations for the inverse model. We show that it is not possible to obtain a reliable parameter estimation using a time series of soil temperature observed at a single depth. Using measurements at two depths, reliable parameter estimates can be obtained although it is not possible to differentiate between latent and sensible heat fluxes. We apply the inverse model to urban flux tower data in Phoenix, United States, and show that the thermal conductivity, volumetric heat capacity and the combined sensible-latent heat transfer coefficient can be reliably estimated using an observed value for the effective surface albedo. The resulting model accurately predicts the outgoing longwave radiation, conductive soil fluxes and the combined sensible-latent heat fluxes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath</title>
<link>https://arxiv.org/abs/2505.03397</link>
<guid>https://arxiv.org/abs/2505.03397</guid>
<content:encoded><![CDATA[
arXiv:2505.03397v4 Announce Type: replace-cross 
Abstract: Qubit control protocols have traditionally leveraged a characterisation of the qubit-bath coupling via its power spectral density. Previous work proposed the inference of noise operators that characterise the influence of a classical bath using a grey-box approach that combines deep neural networks with physics-encoded layers. This overall structure is complex and poses challenges in scaling and real-time operations. Here, we show that no expensive neural networks are needed and that this noise operator description admits an efficient parameterisation. We refer to the resulting parameter space as the \textit{quantum feature space} of the qubit dynamics resulting from the coupled bath. We show that the Euclidean distance defined over the quantum feature space provides an effective method for classifying noise processes in the presence of a given set of controls. Using the quantum feature space as the input space for a simple machine learning algorithm (random forest, in this case), we demonstrate that it can effectively classify the stationarity and the broad class of noise processes perturbing a qubit. Finally, we explore how control pulse parameters map to the quantum feature space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations</title>
<link>https://arxiv.org/abs/2505.06774</link>
<guid>https://arxiv.org/abs/2505.06774</guid>
<content:encoded><![CDATA[
arXiv:2505.06774v2 Announce Type: replace-cross 
Abstract: In this paper, we present a framework for modeling quantum recurrent neural networks (RNNs) and their enhanced version, long short-term memory (LSTM) networks using the core ideas presented by Linden et al. (2009), where the entangling and disentangling power of unitary transformations is investigated. In particular, we interpret entangling and disentangling power as information retention and forgetting mechanisms in LSTMs. Thus, entanglement emerges as a key component of the optimization (training) process. We believe that, by leveraging prior knowledge of the entangling power of unitaries, the proposed quantum-classical framework can guide the design of better-parameterized quantum circuits for various real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the robustness of adversarial defenses in malware detection systems</title>
<link>https://arxiv.org/abs/2505.09342</link>
<guid>https://arxiv.org/abs/2505.09342</guid>
<content:encoded><![CDATA[
arXiv:2505.09342v2 Announce Type: replace-cross 
Abstract: Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</title>
<link>https://arxiv.org/abs/2505.11227</link>
<guid>https://arxiv.org/abs/2505.11227</guid>
<content:encoded><![CDATA[
arXiv:2505.11227v2 Announce Type: replace-cross 
Abstract: The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind The Gap: Quantifying Mechanistic Gaps in Algorithmic Reasoning via Neural Compilation</title>
<link>https://arxiv.org/abs/2505.18623</link>
<guid>https://arxiv.org/abs/2505.18623</guid>
<content:encoded><![CDATA[
arXiv:2505.18623v2 Announce Type: replace-cross 
Abstract: This paper aims to understand how neural networks learn algorithmic reasoning by addressing two questions: How faithful are learned algorithms when they are effective, and why do neural networks fail to learn effective algorithms otherwise? To answer these questions, we use neural compilation, a technique that directly encodes a source algorithm into neural network parameters, enabling the network to compute the algorithm exactly. This enables comparison between compiled and conventionally learned parameters, intermediate vectors, and behaviors. This investigation is crucial for developing neural networks that robustly learn complexalgorithms from data. Our analysis focuses on graph neural networks (GNNs), which are naturally aligned with algorithmic reasoning tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the spectrum of effective, faithful, and ineffective learned algorithms. Commonly, learning algorithmic reasoning is framed as induction over synthetic data, where a parameterized model is trained on inputs, traces, and outputs produced by an underlying ground truth algorithm. In contrast, we introduce a neural compilation method for GNNs, which sets network parameters analytically, bypassing training. Focusing on GNNs leverages their alignment with algorithmic reasoning, extensive algorithmic induction literature, and the novel application of neural compilation to GNNs. Overall, this paper aims to characterize expressability-trainability gaps - a fundamental shortcoming in learning algorithmic reasoning. We hypothesize that inductive learning is most effective for parallel algorithms contained within the computational class \texttt{NC}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Miss the Multi-Agent Mark</title>
<link>https://arxiv.org/abs/2505.21298</link>
<guid>https://arxiv.org/abs/2505.21298</guid>
<content:encoded><![CDATA[
arXiv:2505.21298v4 Announce Type: replace-cross 
Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title>
<link>https://arxiv.org/abs/2506.06389</link>
<guid>https://arxiv.org/abs/2506.06389</guid>
<content:encoded><![CDATA[
arXiv:2506.06389v2 Announce Type: replace-cross 
Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[
arXiv:2506.19852v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\mathcal{O}(n \log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\times$ longer while reducing training costs by up to 4.4$\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\times$ compared to dense attention inference. Code is released at \href{https://github.com/mit-han-lab/radial-attention}{https://github.com/mit-han-lab/radial-attention}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Not to Detect Prompt Injections with an LLM</title>
<link>https://arxiv.org/abs/2507.05630</link>
<guid>https://arxiv.org/abs/2507.05630</guid>
<content:encoded><![CDATA[
arXiv:2507.05630v3 Announce Type: replace-cross 
Abstract: LLM-integrated applications and agents are vulnerable to prompt injection attacks, where adversaries embed malicious instructions within seemingly benign input data to manipulate the LLM's intended behavior. Recent defenses based on known-answer detection (KAD) scheme have reported near-perfect performance by observing an LLM's output to classify input data as clean or contaminated. KAD attempts to repurpose the very susceptibility to prompt injection as a defensive mechanism. We formally characterize the KAD scheme and uncover a structural vulnerability that invalidates its core security premise. To exploit this fundamental vulnerability, we methodically design an adaptive attack, DataFlip. It consistently evades KAD defenses, achieving detection rates as low as $0\%$ while reliably inducing malicious behavior with a success rate of $91\%$, all without requiring white-box access to the LLM or any optimization procedures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fredholm Neural Networks for forward and inverse problems in elliptic PDEs</title>
<link>https://arxiv.org/abs/2507.06038</link>
<guid>https://arxiv.org/abs/2507.06038</guid>
<content:encoded><![CDATA[
arXiv:2507.06038v3 Announce Type: replace-cross 
Abstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm NNs/ FNNs) for solving integral equations, we extend the framework to tackle forward and inverse problems for linear and semi-linear elliptic partial differential equations. The proposed scheme consists of a deep neural network (DNN) which is designed to represent the iterative process of fixed-point iterations for the solution of elliptic PDEs using the boundary integral method within the framework of potential theory. The number of layers, weights, biases and hyperparameters are computed in an explainable manner based on the iterative scheme, and we therefore refer to this as the Potential Fredholm Neural Network (PFNN). We show that this approach ensures both accuracy and explainability, achieving small errors in the interior of the domain, and near machine-precision on the boundary. We provide a constructive proof for the consistency of the scheme and provide explicit error bounds for both the interior and boundary of the domain, reflected in the layers of the PFNN. These error bounds depend on the approximation of the boundary function and the integral discretization scheme, both of which directly correspond to components of the Fredholm NN architecture. In this way, we provide an explainable scheme that explicitly respects the boundary conditions. We assess the performance of the proposed scheme for the solution of both the forward and inverse problem for linear and semi-linear elliptic PDEs in two dimensions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Probabilistic Approximate Optimization Algorithm</title>
<link>https://arxiv.org/abs/2507.07420</link>
<guid>https://arxiv.org/abs/2507.07420</guid>
<content:encoded><![CDATA[
arXiv:2507.07420v3 Announce Type: replace-cross 
Abstract: We introduce a generalized \textit{Probabilistic Approximate Optimization Algorithm (PAOA)}, a classical variational Monte Carlo framework that extends and formalizes prior work by Weitz \textit{et al.}~\cite{Combes_2023}, enabling parameterized and fast sampling on present-day Ising machines and probabilistic computers. PAOA operates by iteratively modifying the couplings of a network of binary stochastic units, guided by cost evaluations from independent samples. We establish a direct correspondence between derivative-free updates and the gradient of the full Markov flow over the exponentially large state space, showing that PAOA admits a principled variational formulation. Simulated annealing emerges as a limiting case under constrained parameterizations, and we implement this regime on an FPGA-based probabilistic computer with on-chip annealing to solve large 3D spin-glass problems. Benchmarking PAOA against QAOA on the canonical 26-spin Sherrington-Kirkpatrick model with matched parameters reveals superior performance for PAOA. We show that PAOA naturally extends simulated annealing by optimizing multiple temperature profiles, leading to improved performance over SA on heavy-tailed problems such as SK-L\'evy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Approximation with Block Coordinate Optimal Stepsizes</title>
<link>https://arxiv.org/abs/2507.08963</link>
<guid>https://arxiv.org/abs/2507.08963</guid>
<content:encoded><![CDATA[
arXiv:2507.08963v2 Announce Type: replace-cross 
Abstract: We consider stochastic approximation with block-coordinate stepsizes and propose adaptive stepsize rules that aim to minimize the expected distance from the next iterate to an (unknown) target point. These stepsize rules employ online estimates of the second moment of the search direction along each block coordinate. The popular Adam algorithm can be interpreted as a variant with a specific estimator. By leveraging a simple conditional estimator, we derive a new method that obtains competitive performance against Adam but requires less memory and fewer hyper-parameters. We prove that this family of methods converges almost surely to a small neighborhood of the target point, and the radius of the neighborhood depends on the bias and variance of the second-moment estimator. Our analysis relies on a simple aiming condition that assumes neither convexity nor smoothness, thus has broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety</title>
<link>https://arxiv.org/abs/2507.11473</link>
<guid>https://arxiv.org/abs/2507.11473</guid>
<content:encoded><![CDATA[
arXiv:2507.11473v2 Announce Type: replace-cross 
Abstract: AI systems that "think" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[
arXiv:2507.18405v2 Announce Type: replace-cross 
Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2507.18594</link>
<guid>https://arxiv.org/abs/2507.18594</guid>
<content:encoded><![CDATA[
arXiv:2507.18594v3 Announce Type: replace-cross 
Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[
arXiv:2507.23334v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Approach to Visualizing Uncertainty in Statistical Graphics</title>
<link>https://arxiv.org/abs/2508.00937</link>
<guid>https://arxiv.org/abs/2508.00937</guid>
<content:encoded><![CDATA[
arXiv:2508.00937v2 Announce Type: replace-cross 
Abstract: We present a general approach to visualizing uncertainty in static 2-D statistical graphics. If we treat a visualization as a function of its underlying quantities, uncertainty in those quantities induces a distribution over images. We show how to aggregate these images into a single visualization that represents the uncertainty. The approach can be viewed as a generalization of sample-based approaches that use overlay. Notably, standard representations, such as confidence intervals and bands, emerge with their usual coverage guarantees without being explicitly quantified or visualized. As a proof of concept, we implement our approach in the IID setting using resampling, provided as an open-source Python library. Because the approach operates directly on images, the user needs only to supply the data and the code for visualizing the quantities of interest without uncertainty. Through several examples, we show how both familiar and novel forms of uncertainty visualization can be created. The implementation is not only a practical validation of the underlying theory but also an immediately usable tool that can complement existing uncertainty-visualization libraries.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment</title>
<link>https://arxiv.org/abs/2508.02292</link>
<guid>https://arxiv.org/abs/2508.02292</guid>
<content:encoded><![CDATA[
arXiv:2508.02292v2 Announce Type: replace-cross 
Abstract: Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\footnote{https://github.com/DVampire/FinWorld}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings</title>
<link>https://arxiv.org/abs/2508.16210</link>
<guid>https://arxiv.org/abs/2508.16210</guid>
<content:encoded><![CDATA[
arXiv:2508.16210v2 Announce Type: replace-cross 
Abstract: Cross-domain recommender (CDR) systems aim to transfer knowledge from data-rich domains to data-sparse ones, alleviating sparsity and cold-start issues present in conventional single-domain recommenders. However, many CDR approaches rely on overlapping users or items to establish explicit cross-domain connections, which is unrealistic in practice. Moreover, most methods represent user preferences as fixed discrete vectors, limiting their ability to capture the fine-grained and multi-aspect nature of user interests. To address these limitations, we propose DUP-OT (Distributional User Preferences with Optimal Transport), a novel framework for non-overlapping CDR. DUP-OT consists of three stages: (1) a shared preprocessing module that extracts review-based embeddings using a unified sentence encoder and autoencoder; (2) a user preference modeling module that represents each user's interests as a Gaussian Mixture Model (GMM) over item embeddings; and (3) an optimal-transport-based alignment module that matches Gaussian components across domains, enabling effective preference transfer for target-domain rating prediction. Experiments on Amazon Review datasets demonstrate that DUP-OT mitigates domain discrepancy and significantly outperforms state-of-the-art baselines under strictly non-overlapping training settings, with user correspondence revealed only for inference-time evaluation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An exact multiple-time-step variational formulation for the committor and the transition rate</title>
<link>https://arxiv.org/abs/2509.03539</link>
<guid>https://arxiv.org/abs/2509.03539</guid>
<content:encoded><![CDATA[
arXiv:2509.03539v2 Announce Type: replace-cross 
Abstract: For a transition between two stable states, the committor is the probability that the dynamics leads to one stable state before the other. It can be estimated from trajectory data by minimizing an expression for the transition rate that depends on a lag time. We show that an existing such expression is minimized by the exact committor only when the lag time is a single time step, resulting in a biased estimate in practical applications. We introduce an alternative expression that is minimized by the exact committor at any lag time. The key idea is that, when trajectories enter the stable states, the times that they enter (stopping times) must be used for estimating the committor and transition rate instead of the lag time. Numerical tests on benchmark systems demonstrate that our committor and transition rate estimates are much less sensitive to the choice of lag time. We show how further accuracy for the transition rate can be achieved by combining results from two lag times. We also relate the transition rate expression to a variational approach for kinetic statistics based on the mean-squared residual and discuss further numerical considerations with the aid of a decomposition of the error into dynamic modes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling to Multimodal and Multichannel Heart Sound Classification with Synthetic and Augmented Biosignals</title>
<link>https://arxiv.org/abs/2509.11606</link>
<guid>https://arxiv.org/abs/2509.11606</guid>
<content:encoded><![CDATA[
arXiv:2509.11606v3 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of death worldwide, accounting for approximately 17.9 million deaths each year. Early detection is critical, creating a demand for accurate and inexpensive pre-screening methods. Deep learning has recently been applied to classify abnormal heart sounds indicative of CVDs using synchronised phonocardiogram (PCG) and electrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However, state-of-the-art architectures remain underutilised due to the limited availability of synchronised and multichannel datasets. Augmented datasets and pre-trained models provide a pathway to overcome these limitations, enabling transformer-based architectures to be trained effectively. This work combines traditional signal processing with denoising diffusion models, WaveGrad and DiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based classifier on multimodal and multichannel heart sound datasets. The approach achieves state-of-the-art performance. On the Computing in Cardiology (CinC) 2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR), sensitivity, specificity and Matthew's correlation coefficient (MCC) reach 92.48%, 93.05%, 93.63%, 92.48%, 94.93% and 0.8283, respectively. Using the synchronised PCG and ECG signals of the training-a dataset from CinC, 93.14%, 92.21%, 94.35%, 90.10%, 95.12% and 0.8380 are achieved for accuracy, UAR, sensitivity, specificity and MCC, respectively. Using a wearable vest dataset consisting of mPCG data, the model achieves 77.13% accuracy, 74.25% UAR, 86.47% sensitivity, 62.04% specificity, and 0.5082 MCC. These results demonstrate the effectiveness of transformer-based models for CVD detection when supported by augmented datasets, highlighting their potential to advance multimodal and multichannel heart sound classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.21707</link>
<guid>https://arxiv.org/abs/2509.21707</guid>
<content:encoded><![CDATA[
arXiv:2509.21707v2 Announce Type: replace-cross 
Abstract: Semi-supervised learning (SSL) arises in practice when labeled data are scarce or expensive to obtain, while large quantities of unlabeled data are readily available.
  With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions of uncertain quality for both inference and prediction tasks. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through small-scale simulations and two real-data analyses with distinct scientific goals. A user-friendly R package, sada, is provided to facilitate practical implementation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Cosmological Parameters with Evidential Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.24327</link>
<guid>https://arxiv.org/abs/2509.24327</guid>
<content:encoded><![CDATA[
arXiv:2509.24327v2 Announce Type: replace-cross 
Abstract: We examine the use of a novel variant of Physics-Informed Neural Networks to predict cosmological parameters from recent supernovae and baryon acoustic oscillations (BAO) datasets. Our machine learning framework generates uncertainty estimates for target variables and the inferred unknown parameters of the underlying PDE descriptions. Built upon a hybrid of the principles of Evidential Deep Learning, Physics-Informed Neural Networks, Bayesian Neural Networks and Gaussian Processes, our model enables learning of the posterior distribution of the unknown PDE parameters through standard gradient-descent based training. We apply our model to an up-to-date BAO dataset (Bousis et al. 2024) calibrated with the CMB-inferred sound horizon, and the Pantheon$+$ Sne Ia distances (Scolnic et al. 2018), examining the relative effectiveness and mutual consistency among the standard $\Lambda$CDM, $w$CDM and $\Lambda_s$CDM models. Unlike previous results arising from the standard approach of minimizing an appropriate $\chi^2$ function, the posterior distributions for parameters in various models trained purely on Pantheon$+$ data were found to be largely contained within the $2\sigma$ contours of their counterparts trained on BAO data. Their posterior medians for $h_0$ were within about $2\sigma$ of one another, indicating that our machine learning-guided approach provides a different measure of the Hubble tension.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaChest: Generalized few-shot learning of pathologies from chest X-rays</title>
<link>https://arxiv.org/abs/2509.25590</link>
<guid>https://arxiv.org/abs/2509.25590</guid>
<content:encoded><![CDATA[
arXiv:2509.25590v2 Announce Type: replace-cross 
Abstract: The limited availability of annotated data presents a major challenge for applying deep learning methods to medical image analysis. Few-shot learning methods aim to recognize new classes from only a small number of labeled examples. These methods are typically studied under the standard few-shot learning setting, where all classes in a task are new. However, medical applications such as pathology classification from chest X-rays often require learning new classes while simultaneously leveraging knowledge of previously known ones, a scenario more closely aligned with generalized few-shot classification. Despite its practical relevance, few-shot learning has been scarcely studied in this context. In this work, we present MetaChest, a large-scale dataset of 479,215 chest X-rays collected from four public databases. MetaChest includes a meta-set partition specifically designed for standard few-shot classification, as well as an algorithm for generating multi-label episodes. We conduct extensive experiments evaluating both a standard transfer learning approach and an extension of ProtoNet across a wide range of few-shot multi-label classification tasks. Our results demonstrate that increasing the number of classes per episode and the number of training examples per class improves classification performance. Notably, the transfer learning approach consistently outperforms the ProtoNet extension, despite not being tailored for few-shot learning. We also show that higher-resolution images improve accuracy at the cost of additional computation, while efficient model architectures achieve comparable performance to larger models with significantly reduced resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title>
<link>https://arxiv.org/abs/2509.25774</link>
<guid>https://arxiv.org/abs/2509.25774</guid>
<content:encoded><![CDATA[
arXiv:2509.25774v2 Announce Type: replace-cross 
Abstract: While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from the electronic structure of molecules across the periodic table</title>
<link>https://arxiv.org/abs/2510.00224</link>
<guid>https://arxiv.org/abs/2510.00224</guid>
<content:encoded><![CDATA[
arXiv:2510.00224v2 Announce Type: replace-cross 
Abstract: Machine-Learned Interatomic Potentials (MLIPs) require vast amounts of atomic structure data to learn forces and energies, and their performance continues to improve with training set size. Meanwhile, the even greater quantities of accompanying data in the Hamiltonian matrix H behind these datasets has so far gone unused for this purpose. Here, we provide a recipe for integrating the orbital interaction data within H towards training pipelines for atomic-level properties. We first introduce HELM ("Hamiltonian-trained Electronic-structure Learning for Molecules"), a state-of-the-art Hamiltonian prediction model which bridges the gap between Hamiltonian prediction and universal MLIPs by scaling to H of structures with 100+ atoms, high elemental diversity, and large basis sets including diffuse functions. To accompany HELM, we release a curated Hamiltonian matrix dataset, 'OMol_CSH_58k', with unprecedented elemental diversity (58 elements), molecular size (up to 150 atoms), and basis set (def2-TZVPD). Finally, we introduce 'Hamiltonian pretraining' as a method to extract meaningful descriptors of atomic environments even from a limited number atomic structures, and repurpose this shared embedding space to improve performance on energy-prediction in low-data regimes. Our results highlight the use of electronic interactions as a rich and transferable data source for representing chemical space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2510.01268</link>
<guid>https://arxiv.org/abs/2510.01268</guid>
<content:encoded><![CDATA[
arXiv:2510.01268v4 Announce Type: replace-cross 
Abstract: We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero</title>
<link>https://arxiv.org/abs/2510.01874</link>
<guid>https://arxiv.org/abs/2510.01874</guid>
<content:encoded><![CDATA[
arXiv:2510.01874v2 Announce Type: replace-cross 
Abstract: This paper examines replication portfolio construction in incomplete markets - a key problem in financial engineering with applications in pricing, hedging, balance sheet management, and energy storage planning. We model this as a two-player game between an investor and the market, where the investor makes strategic bets on future states while the market reveals outcomes. Inspired by the success of Monte Carlo Tree Search in stochastic games, we introduce an AlphaZero-based system and compare its performance to deep hedging - a widely used industry method based on gradient descent. Through theoretical analysis and experiments, we show that deep hedging struggles in environments where the optimal action-value function is not subject to convexity constraints - such as those involving non-convex transaction costs, capital constraints, or regulatory limitations - converging to local optima. We construct specific market environments to highlight these limitations and demonstrate that AlphaZero consistently finds near-optimal replication strategies. On the theoretical side, we establish a connection between deep hedging and convex optimization, suggesting that its effectiveness is contingent on convexity assumptions. Our experiments further suggest that AlphaZero is more sample-efficient - an important advantage in data-scarce, overfitting-prone derivative markets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations</title>
<link>https://arxiv.org/abs/2510.02348</link>
<guid>https://arxiv.org/abs/2510.02348</guid>
<content:encoded><![CDATA[
arXiv:2510.02348v3 Announce Type: replace-cross 
Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
<link>https://arxiv.org/abs/2510.04556</link>
<guid>https://arxiv.org/abs/2510.04556</guid>
<content:encoded><![CDATA[
arXiv:2510.04556v2 Announce Type: replace-cross 
Abstract: Maintaining the predictive performance of pricing models is challenging when insurance portfolios and data-generating mechanisms evolve over time. Focusing on non-life insurance, we adopt the concept-drift terminology from machine learning and distinguish virtual drift from real concept drift in an actuarial setting. Methodologically, we (i) formalize deviance loss and Murphy's score decomposition to assess global and local auto-calibration; (ii) study the Gini score as a rank-based performance measure, derive its asymptotic distribution, and develop a consistent bootstrap estimator of its asymptotic variance; and (iii) combine these results into a statistically grounded, model-agnostic monitoring framework that integrates a Gini-based ranking drift test with global and local auto-calibration tests. An application to a modified motor insurance portfolio with controlled concept-drift scenarios illustrates how the framework guides decisions on refitting or recalibrating pricing models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis</title>
<link>https://arxiv.org/abs/2510.09365</link>
<guid>https://arxiv.org/abs/2510.09365</guid>
<content:encoded><![CDATA[
arXiv:2510.09365v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: https://github.com/valentin-biller/ldm.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning Is Provably Bayesian Inference: A Generalization Theory for Meta-Learning</title>
<link>https://arxiv.org/abs/2510.10981</link>
<guid>https://arxiv.org/abs/2510.10981</guid>
<content:encoded><![CDATA[
arXiv:2510.10981v2 Announce Type: replace-cross 
Abstract: This paper develops a finite-sample statistical theory for in-context learning (ICL), analyzed within a meta-learning framework that accommodates mixtures of diverse task types. We introduce a principled risk decomposition that separates the total ICL risk into two orthogonal components: Bayes Gap and Posterior Variance. The Bayes Gap quantifies how well the trained model approximates the Bayes-optimal in-context predictor. For a uniform-attention Transformer, we derive a non-asymptotic upper bound on this gap, which explicitly clarifies the dependence on the number of pretraining prompts and their context length. The Posterior Variance is a model-independent risk representing the intrinsic task uncertainty. Our key finding is that this term is determined solely by the difficulty of the true underlying task, while the uncertainty arising from the task mixture vanishes exponentially fast with only a few in-context examples. Together, these results provide a unified view of ICL: the Transformer selects the optimal meta-algorithm during pretraining and rapidly converges to the optimal algorithm for the true task at test time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining in Actor-Critic Reinforcement Learning for Robot Locomotion</title>
<link>https://arxiv.org/abs/2510.12363</link>
<guid>https://arxiv.org/abs/2510.12363</guid>
<content:encoded><![CDATA[
arXiv:2510.12363v2 Announce Type: replace-cross 
Abstract: The pretraining-finetuning paradigm has facilitated numerous transformative advancements in artificial intelligence research in recent years. However, in the domain of reinforcement learning (RL) for robot locomotion, individual skills are often learned from scratch despite the high likelihood that some generalizable knowledge is shared across all task-specific policies belonging to the same robot embodiment. This work aims to define a paradigm for pretraining neural network models that encapsulate such knowledge and can subsequently serve as a basis for warm-starting the RL process in classic actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin with a task-agnostic exploration-based data collection algorithm to gather diverse, dynamic transition data, which is then used to train a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning. The pretrained weights are then loaded into both the actor and critic networks to warm-start the policy optimization of actual tasks. We systematically validated our proposed method with 9 distinct robot locomotion RL environments comprising 3 different robot embodiments, showing significant benefits of this initialization strategy. Our proposed approach on average improves sample efficiency by 36.9% and task performance by 7.3% compared to random initialization. We further present key ablation studies and empirical analyses that shed light on the mechanisms behind the effectiveness of this method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPC-Driven Modeling with ML-Based Surrogates for Magnon-Photon Dynamics in Hybrid Quantum Systems</title>
<link>https://arxiv.org/abs/2510.22221</link>
<guid>https://arxiv.org/abs/2510.22221</guid>
<content:encoded><![CDATA[
arXiv:2510.22221v2 Announce Type: replace-cross 
Abstract: Simulating hybrid magnonic quantum systems remains a challenge due to the large disparity between the timescales of the two systems. We present a massively parallel GPU-based simulation framework that enables fully coupled, large-scale modeling of on-chip magnon-photon circuits. Our approach resolves the dynamic interaction between ferromagnetic and electromagnetic fields with high spatiotemporal fidelity. To accelerate design workflows, we develop a physics-informed machine learning surrogate trained on the simulation data, reducing computational cost while maintaining accuracy. This combined approach reveals real-time energy exchange dynamics and reproduces key phenomena such as anti-crossing behavior and the suppression of ferromagnetic resonance under strong electromagnetic fields. By addressing the multiscale and multiphysics challenges in magnon-photon modeling, our framework enables scalable simulation and rapid prototyping of next-generation quantum and spintronic devices.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards</title>
<link>https://arxiv.org/abs/2510.23083</link>
<guid>https://arxiv.org/abs/2510.23083</guid>
<content:encoded><![CDATA[
arXiv:2510.23083v2 Announce Type: replace-cross 
Abstract: Generating high-quality code remains a challenge for Large Language Models (LLMs). For the evolution of reasoning models on this task, reward models are a necessary intermediate step. These models judge outcomes or intermediate steps. Decoder-only transformer models can be turned into reward models by introducing a regression layer and supervised fine-tuning. While it is known that reflection capabilities generally increase with the size of a model, we want to investigate whether state-of-the-art small language models like the Phi-4 family can be turned into usable reward models blending the consideration of process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness labels derived from the APPS coding challenge benchmark. We then train a value-head model to estimate the success probability of intermediate outputs. Our evaluation shows that small LLMs are capable of serving as effective reward models or code evaluation critics, successfully identifying correct solutions among multiple candidates. Using this critic, we achieve over a 20% improvement in the search capability of the most accurate code out of multiple generations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
<link>https://arxiv.org/abs/2511.01266</link>
<guid>https://arxiv.org/abs/2511.01266</guid>
<content:encoded><![CDATA[
arXiv:2511.01266v2 Announce Type: replace-cross 
Abstract: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BondBERT: What we learn when assigning sentiment in the bond market</title>
<link>https://arxiv.org/abs/2511.01869</link>
<guid>https://arxiv.org/abs/2511.01869</guid>
<content:encoded><![CDATA[
arXiv:2511.01869v2 Announce Type: replace-cross 
Abstract: Bond markets respond differently to macroeconomic news compared to equity markets, yet most sentiment models are trained primarily on general financial or equity news data. However, bond prices often move in the opposite direction to economic optimism, making general or equity-based sentiment tools potentially misleading. We introduce BondBERT, a transformer-based language model fine-tuned on bond-specific news. BondBERT can act as the perception and reasoning component of a financial decision-support agent, providing sentiment signals that integrate with forecasting models. We propose a generalisable framework for adapting transformers to low-volatility, domain-inverse sentiment tasks by compiling and cleaning 30,000 UK bond market articles (2018-2025). BondBERT's sentiment predictions are compared against FinBERT, FinGPT, and Instruct-FinGPT using event-based correlation, up/down accuracy analyses, and LSTM forecasting across ten UK sovereign bonds. We find that BondBERT consistently produces positive correlations with bond returns, and achieves higher alignment and forecasting accuracy than the three baseline models. These results demonstrate that domain-specific sentiment adaptation better captures fixed income dynamics, bridging a gap between NLP advances and bond market analytics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory</title>
<link>https://arxiv.org/abs/2511.01912</link>
<guid>https://arxiv.org/abs/2511.01912</guid>
<content:encoded><![CDATA[
arXiv:2511.01912v2 Announce Type: replace-cross 
Abstract: Planning has been a cornerstone of artificial intelligence for solving complex problems, and recent progress in LLM-based multi-agent frameworks have begun to extend this capability. However, the role of human-like memory within these frameworks remains largely unexplored. Understanding how agents coordinate through memory is critical for natural language planning, where iterative reasoning, constraint tracking, and error correction drive the success. Inspired by working memory model in cognitive psychology, we present EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The framework consists of three agents (Constraint Extractor, Verifier, and Actor) and two memory modules: Constraint Memory (CMem), which evolves across queries by storing task-specific rules and constraints while remains fixed within a query, and Query-feedback Memory (QMem), which evolves within a query by accumulating feedback across iterations for solution refinement. Both memory modules are reset at the end of each query session. Evaluations on trip planning, meeting planning, and calendar scheduling show consistent performance improvements, highlighting the effectiveness of EvoMem. This success underscores the importance of memory in enhancing multi-agent planning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems</title>
<link>https://arxiv.org/abs/2511.04309</link>
<guid>https://arxiv.org/abs/2511.04309</guid>
<content:encoded><![CDATA[
arXiv:2511.04309v2 Announce Type: replace-cross 
Abstract: We consider numerical resolution of principal-agent (PA) problems in continuous time. We formulate a generic PA model with continuous and lump payments and a multi-dimensional strategy of the agent. To tackle the resulting Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC) Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and controls, as well as constraints. We investigate the role of the neural network architecture, training designs, loss functions, etc. on the convergence of the solver, presenting five different case studies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Performance Variance-Covariance Matrix Construction Using an Uncentered Gram Formulation</title>
<link>https://arxiv.org/abs/2511.08223</link>
<guid>https://arxiv.org/abs/2511.08223</guid>
<content:encoded><![CDATA[
arXiv:2511.08223v2 Announce Type: replace-cross 
Abstract: Reichel (2025) defined the bariance as a pairwise-difference measure that can be rewritten in linear time using only scalar sums. We extend this idea to the covariance matrix by showing that the standard matrix expression involving the uncentered Gram matrix and a correction term is algebraically identical to the pairwise-difference definition while avoiding explicit centering. The computation then reduces to one outer product of dimension p-by-p and a single subtraction. Benchmarks in Python show clear runtime gains, especially when BLAS optimizations are absent. Optionally faster Gram-matrix routines such as RXTX (Rybin et al., 2025) further reduce overall cost.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[
arXiv:2511.15464v2 Announce Type: replace-cross 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation</title>
<link>https://arxiv.org/abs/2511.16543</link>
<guid>https://arxiv.org/abs/2511.16543</guid>
<content:encoded><![CDATA[
arXiv:2511.16543v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Inspired by knowledge distillation, Prism leverages a powerful, instruction-following teacher LLM (FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. Our extensive experiments on benchmark datasets reveal a key finding: the distillation process not only transfers knowledge but also acts as a noise filter. Our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, demonstrating an emergent ability to correct hallucinations present in the teacher's outputs. While achieving a 24x speedup and a 10x reduction in memory consumption, our analysis validates that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality, and perhaps more importantly, trustworthy explainable recommendation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Off-Policy Training Data on Probe Generalisation</title>
<link>https://arxiv.org/abs/2511.17408</link>
<guid>https://arxiv.org/abs/2511.17408</guid>
<content:encoded><![CDATA[
arXiv:2511.17408v2 Announce Type: replace-cross 
Abstract: Probing has emerged as a promising method for monitoring large language models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect greatly varies by behaviour. We find that successful generalisation from off-policy responses to incentivised responses (e.g. those where the behaviour is advantageous) is predictive of successful generalisation to on-policy data. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. We also find that shifts in the training data domain cause even larger performance degradation than off-to-on-policy shift, with different-domain test scores being consistently lower than the same-domain ones. In the absence of on-policy data, using same-domain off-policy data appears to yield more reliable probes than using on-policy data from a different domain. Still, we emphasise the need for methods that can better handle distribution shifts in LLM monitoring.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Collaboration in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20639</link>
<guid>https://arxiv.org/abs/2511.20639</guid>
<content:encoded><![CDATA[
arXiv:2511.20639v2 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[
arXiv:2511.20821v2 Announce Type: replace-cross 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Two Proxy Methods for Causal Identification</title>
<link>https://arxiv.org/abs/2512.00175</link>
<guid>https://arxiv.org/abs/2512.00175</guid>
<content:encoded><![CDATA[
arXiv:2512.00175v2 Announce Type: replace-cross 
Abstract: Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[
arXiv:2512.00975v2 Announce Type: replace-cross 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Path Collaborative Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01485</link>
<guid>https://arxiv.org/abs/2512.01485</guid>
<content:encoded><![CDATA[
arXiv:2512.01485v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Much Ado About Noising: Dispelling the Myths of Generative Robotic Control</title>
<link>https://arxiv.org/abs/2512.01809</link>
<guid>https://arxiv.org/abs/2512.01809</guid>
<content:encoded><![CDATA[
arXiv:2512.01809v2 Announce Type: replace-cross 
Abstract: Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Morphling: Fast, Fused, and Flexible GNN Training at Scale</title>
<link>https://arxiv.org/abs/2512.01678</link>
<guid>https://arxiv.org/abs/2512.01678</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, code synthesis, performance optimization, sparsity-aware execution, distributed training  

<br /><br />Summary:  
Graph Neural Networks (GNNs) face significant hardware challenges due to the combination of irregular graph traversals and dense matrix computations, which existing frameworks like PyTorch Geometric (PyG) and Deep Graph Library (DGL) do not efficiently handle. Morphling is introduced as a domain-specific code synthesizer that compiles high-level GNN specifications into optimized, backend-specialized implementations for OpenMP, CUDA, and MPI. It leverages a library of architecture-aware primitives customized for each execution environment to enhance performance. Additionally, Morphling integrates a runtime sparsity-aware execution engine that dynamically selects between dense or sparse computation paths based on input feature statistics, effectively reducing unnecessary operations on zero-valued data. The effectiveness of Morphling was validated on eleven diverse real-world datasets covering various graph types, feature sizes, and sparsity levels. Results demonstrate that Morphling improves per-epoch training throughput by approximately 20× on CPUs, 19× on GPUs, and 6× in distributed settings compared to PyG and DGL, with speedups peaking at 66×. Moreover, Morphling’s memory-efficient layouts reduce peak memory usage by up to 15×, enabling training of large-scale GNNs on commodity hardware. This work highlights that specialized, architecture-aware code generation is a viable and scalable solution for efficient GNN execution across different parallel and distributed platforms. <div>
arXiv:2512.01678v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques</title>
<link>https://arxiv.org/abs/2512.05169</link>
<guid>https://arxiv.org/abs/2512.05169</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view clustering, unsupervised learning, data integration, scalability, deep learning<br /><br />Summary:<br /><br />This article addresses the challenges faced by machine learning, including computational limits, single-view algorithm constraints, and processing complex datasets from multiple domains or perspectives. Multi-view clustering (MVC) is presented as a robust unsupervised learning approach that overcomes limitations of single-view methods by providing enriched data representation and practical utility despite increased complexity. The survey contributes by first categorizing MVC methods into distinct groups such as co-training, co-regularization, subspace approaches, deep learning, kernel-based techniques, anchor-based methods, and graph-based strategies. Second, it offers an in-depth analysis of each category's strengths, weaknesses, and challenges like scalability and handling incomplete data. Third, the paper discusses future trends, interdisciplinary applications, and directions for MVC research advancements. The work is thorough, reviewing over 140 key publications, comparing integration strategies—early fusion, late fusion, and joint learning—and exploring use cases in healthcare, multimedia, and social network analysis. Overall, this survey fills existing gaps in MVC research and provides valuable insights to guide future development in the field. <div>
arXiv:2512.05169v1 Announce Type: new 
Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2512.05216</link>
<guid>https://arxiv.org/abs/2512.05216</guid>
<content:encoded><![CDATA[
<div> Masked autoencoders, electronic health records, volatility, coefficient of variation masking, clinical representations<br /><br />Summary:<br /><br />This paper addresses the challenge of learning meaningful representations from electronic health records (EHR) using masked autoencoders (MAEs). Current MAE approaches use uniform random masking which assumes equal predictability across all features. However, the authors highlight that laboratory tests differ significantly in volatility; stable biomarkers like sodium contrast with volatile ones like lactate, which are harder to model but clinically important as they often indicate acute conditions. To tackle this, they propose a novel volatility-aware pretraining strategy called Coefficient of Variation Masking (CV-Masking), which adjusts masking probabilities based on each feature's intrinsic variability. This approach aligns with clinical workflows by employing a value-only masking objective. Experimental results on a large set of laboratory tests demonstrate that CV-Masking improves reconstruction accuracy, enhances downstream predictive performance, and speeds up model convergence. Overall, the method yields more robust, clinically relevant EHR representations, potentially aiding diverse clinical tasks and improving patient care by better capturing complex temporal patterns in volatile biomarkers. <div>
arXiv:2512.05216v1 Announce Type: new 
Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Clinical Time Series: When Less is More</title>
<link>https://arxiv.org/abs/2512.05217</link>
<guid>https://arxiv.org/abs/2512.05217</guid>
<content:encoded><![CDATA[
<div> Tokenization, clinical time series, transformer, pretrained embeddings, prediction tasks<br /><br />Summary:  
This study systematically evaluates various tokenization strategies for clinical time series modeling using transformer-based architectures, emphasizing their impact on electronic health record (EHR) data analysis. The research is conducted through controlled ablations on four clinical prediction tasks from the MIMIC-IV dataset, highlighting the task-dependent nature of feature importance. Key findings indicate that explicit time encodings do not provide a statistically significant benefit across the evaluated tasks. Additionally, the influence of value features varies by task, being important for mortality prediction but less so for readmission prediction, indicating that code sequences themselves can be sufficiently predictive. The study also demonstrates that frozen pretrained code encoders outperform trainable versions significantly while using fewer parameters, offering computational efficiency without sacrificing accuracy. Larger clinical encoders contribute consistent improvements across tasks, particularly when combined with frozen embeddings that reduce computational overhead. Overall, the research promotes fair and controlled comparisons of tokenization methods and suggests that simpler, parameter-efficient models often achieve strong performance. However, it concludes that the best tokenization approach depends on the specific clinical prediction task being addressed. <div>
arXiv:2512.05217v1 Announce Type: new 
Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance</title>
<link>https://arxiv.org/abs/2512.05222</link>
<guid>https://arxiv.org/abs/2512.05222</guid>
<content:encoded><![CDATA[
<div> Influenza A, antigenicity, Protein Language Models, Semi-Supervised Learning, haemagglutinin<br /><br />Summary:<br /><br />Influenza A viruses (IAVs) rapidly evolve antigenically, necessitating frequent vaccine updates. Traditional haemagglutination inhibition (HI) assays to measure antigenicity are labor-intensive and unscalable, creating a bottleneck as genomic data far exceed available labeled phenotypic data. To address this, the study investigates combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) to maintain predictive accuracy when labeled data are limited. Two SSL methods—Self-training and Label Spreading—were evaluated against fully supervised baselines using four PLM embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation simulated low-label scenarios (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). Results showed SSL consistently improved performance under label scarcity, with Self-training using ProtVec yielding the largest relative gains. ESM-2 embeddings were especially robust, achieving F1 scores over 0.82 even with just 25% labeled data, indicating effective capture of key antigenic features. Prediction accuracy was high for H1N1 and H9N2, whereas the hypervariable H3N2 remained challenging but saw mitigated performance drops with SSL. These findings highlight that integrating PLMs and SSL can overcome antigenicity labeling shortages and better leverage unlabeled surveillance sequences, aiding rapid variant prioritization and timely vaccine strain selection. <div>
arXiv:2512.05222v1 Announce Type: new 
Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance Matters: Improving Domain Adaptation via Stratified Sampling</title>
<link>https://arxiv.org/abs/2512.05226</link>
<guid>https://arxiv.org/abs/2512.05226</guid>
<content:encoded><![CDATA[
<div> Variance reduction, domain adaptation, stratified sampling, maximum mean discrepancy, correlation alignment<br /><br />Summary:<br /><br />1. The paper addresses the challenge of domain shift in machine learning by focusing on unsupervised domain adaptation (UDA), which aims to reduce domain discrepancy without labeled target data.<br /><br />2. A key problem identified is the high variance in discrepancy estimates during stochastic training, which can limit theoretical and practical benefits of UDA.<br /><br />3. The authors propose VaRDASS, a novel variance reduction method based on stratified sampling tailored specifically for UDA tasks.<br /><br />4. The approach is developed for two widely used domain discrepancy metrics: correlation alignment and maximum mean discrepancy (MMD). The paper derives specialized stratification objectives to reduce variance in these measures.<br /><br />5. Theoretical contributions include both expected and worst-case error bounds, with a proof that the proposed stratification objective for MMD is theoretically optimal in terms of minimizing variance under certain conditions.<br /><br />6. To implement the method, a practical optimization algorithm inspired by k-means clustering is proposed and analyzed.<br /><br />7. Experimental results on three domain shift datasets demonstrate that VaRDASS improves the accuracy of discrepancy estimation and enhances the performance on target domains compared to baseline methods. <div>
arXiv:2512.05226v1 Announce Type: new 
Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System</title>
<link>https://arxiv.org/abs/2512.05234</link>
<guid>https://arxiv.org/abs/2512.05234</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Peer-to-Peer, Communication Complexity, Distributed Machine Learning, Network Churn  

<br /><br />Summary:  
This article addresses the challenges of Federated Learning (FL) in next-generation wireless systems, focusing on efficiency and robustness in environments with wireless-connected peers and network churn. It identifies the limitations of existing Peer-to-Peer (P2P) FL methods, particularly their high communication complexity, which hinders scalability. To overcome these issues, the authors propose MAR-FL, a novel P2P FL system that implements iterative group-based aggregation to significantly reduce communication overhead. MAR-FL achieves a communication cost scaling of O(N log N), which is a substantial improvement compared to the O(N²) complexity found in previous approaches. This advancement ensures that the system remains effective as the number of peers increases in each aggregation round. Additionally, MAR-FL demonstrates robustness in handling unreliable or unstable FL clients, making it suitable for real-world deployments where client availability is unpredictable. The system also supports integration with private computing techniques, facilitating privacy-preserving distributed learning. Overall, MAR-FL presents a scalable, efficient, and resilient solution for P2P Federated Learning under the constraints of wireless network environments and dynamic client participation. <div>
arXiv:2512.05234v1 Announce Type: new 
Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edged Weisfeiler-Lehman Algorithm</title>
<link>https://arxiv.org/abs/2512.05238</link>
<guid>https://arxiv.org/abs/2512.05238</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Weisfeiler-Lehman algorithm, Edge features, Graph classification, Edged Graph Isomorphism Network<br /><br />Summary:<br /><br />1. The paper addresses limitations in classical graph learning approaches, particularly the propagation-aggregation methodology used by many Graph Neural Networks (GNNs), which update node representations by recursively aggregating information from neighbor nodes and itself.<br /><br />2. The authors highlight the Weisfeiler-Lehman (1-WL) algorithm, a traditional graph isomorphism test based on color refinement of nodes and their neighbors, noting its inability to incorporate edge features or labels.<br /><br />3. To improve upon this, they introduce the Edged-WL algorithm (E-WL), an extension of the original 1-WL designed to incorporate and exploit edge features in graph data.<br /><br />4. Building on E-WL, the paper proposes the Edged Graph Isomorphism Network (EGIN), a novel GNN model developed to utilize edge features more effectively during graph learning, addressing a common weak point in existing GNN architectures.<br /><br />5. The authors validate their approach on 12 benchmark graph datasets that include edge features, demonstrating that EGIN consistently outperforms state-of-the-art baseline models in graph classification tasks, thus showing the effectiveness of leveraging edge information in graph learning. <div>
arXiv:2512.05238v1 Announce Type: new 
Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging quantum and classical computing for partial differential equations through multifidelity machine learning</title>
<link>https://arxiv.org/abs/2512.05241</link>
<guid>https://arxiv.org/abs/2512.05241</guid>
<content:encoded><![CDATA[
<div> Keywords: quantum algorithms, partial differential equations, multifidelity learning, neural architecture, quantum lattice Boltzmann methods<br /><br />Summary:<br /><br />Quantum algorithms aiming to solve partial differential equations (PDEs) are currently limited by near-term quantum hardware constraints, such as low qubit counts restricting spatial resolution and finite circuit depth hindering accurate long-time simulations. These limitations result in low-fidelity quantum PDE solutions despite their theoretical potential for speedup. To address this, the paper introduces a multifidelity learning framework that leverages sparse classical data to enhance coarse quantum solutions into high-fidelity results. The framework first trains a low-fidelity surrogate model from abundant quantum solver outputs, then applies a multifidelity neural network architecture that combines linear and nonlinear correction mappings. This approach is demonstrated on benchmark nonlinear PDE problems, including the viscous Burgers equation and incompressible Navier-Stokes equations, solved with quantum lattice Boltzmann methods. The framework not only corrects coarse quantum predictions but also achieves reliable temporal extrapolation beyond the classical training period. By reducing the dependence on expensive high-fidelity simulations, the method provides predictions competitive with classical accuracy. Overall, this strategy bridges the gap between hardware-limited quantum simulations and real-world application demands, paving the way for practical deployment of near-term quantum computing in scientific computing and advancing algorithm development for computational physics. <div>
arXiv:2512.05241v1 Announce Type: new 
Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When unlearning is free: leveraging low influence points to reduce computational costs</title>
<link>https://arxiv.org/abs/2512.05254</link>
<guid>https://arxiv.org/abs/2512.05254</guid>
<content:encoded><![CDATA[
<div> Data privacy, machine learning, unlearning, influence functions, computational savings<br /><br />Summary:<br /><br />1. With increasing concerns over data privacy in machine learning, the ability to unlearn or selectively remove specific data points from trained models has become critical. 2. Current state-of-the-art unlearning methods generally treat all data points in the forget set equally, without considering their individual impact on the model. 3. This paper challenges that uniform approach by investigating whether data points with negligible influence on model behavior need to be removed at all. 4. Through a comparative analysis of influence functions on tasks in natural language processing and computer vision, the authors identify subsets of training data that minimally affect model outputs. 5. Building on this discovery, the authors propose an efficient unlearning framework that first reduces the forget dataset by excluding low-impact points before performing unlearning. 6. By doing so, the method achieves significant computational savings—up to approximately 50%—on real-world datasets, making the unlearning process more practical and resource-efficient without compromising privacy goals. <div>
arXiv:2512.05254v1 Announce Type: new 
Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models</title>
<link>https://arxiv.org/abs/2512.05287</link>
<guid>https://arxiv.org/abs/2512.05287</guid>
<content:encoded><![CDATA[
<div> Keywords: miRNA, drug discovery, graph neural network, transformer, drug-miRNA association prediction<br /><br />Summary:<br />1. MicroRNAs (miRNAs) play a critical role in gene regulation and have emerged as promising targets in pharmacology for drug development. Traditional experimental methods to explore drug-miRNA associations are limited by time and cost.<br />2. To address these limitations, the study proposes DMAGT, a novel machine learning model based on a multi-layer transformer-based graph neural network designed to predict drug-miRNA associations effectively.<br />3. DMAGT converts drug-miRNA associations into graph structures and uses Word2Vec embeddings to represent features of drug molecular structures and miRNA base structures, enabling deep relational learning through a graph transformer model.<br />4. The model was evaluated on three publicly available drug-miRNA association datasets: ncDR, RNAInter, and SM2miR, achieving a high predictive performance with an area under the curve (AUC) reaching 95.24±0.05%. DMAGT outperformed other state-of-the-art approaches in similar predictive tasks.<br />5. Practical validation was conducted by focusing on two drugs, 5-Fluorouracil and Oxaliplatin, where 14 out of the top 20 predicted drug-miRNA associations were confirmed experimentally, demonstrating DMAGT’s robustness and reliability.<br />6. Overall, DMAGT offers an efficient and stable computational approach for accelerating miRNA-targeted drug discovery, potentially reducing experimental costs and time significantly. <div>
arXiv:2512.05287v1 Announce Type: new 
Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces</title>
<link>https://arxiv.org/abs/2512.05291</link>
<guid>https://arxiv.org/abs/2512.05291</guid>
<content:encoded><![CDATA[
<div> Keywords: Actor-Critic, RKHS, SHAP, Reinforcement Learning, Interpretability<br /><br />Summary:<br /><br />This paper introduces RSA2C, an advanced Actor-Critic (AC) algorithm designed to improve interpretability in reinforcement learning by incorporating state attributions. It addresses a key limitation in existing explainable RL methods which treat all state features equally, ignoring the different impacts each state dimension has on rewards. RSA2C employs a kernelized, two-timescale actor-critic framework where the Actor is modeled in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic operate in scalar RKHSs. The components utilize sparsified dictionaries with the Actor and Advantage Critic sharing one, while the Value Critic maintains its own. State attributions are derived from the Value Critic using RKHS–SHAP, which leverages kernel mean embeddings to calculate on-manifold and off-manifold expectations. These attributions are transformed into Mahalanobis-gated weights, which modulate the Actor’s gradient updates and Advantage Critic’s targets, enhancing both training guidance and interpretability. The paper provides a theoretical global, non-asymptotic convergence bound under state perturbations, demonstrating the method’s stability and efficiency. Empirical evaluations on three continuous-control benchmarks confirm that RSA2C delivers improved learning efficiency, stable convergence, and interpretable state importance, advancing explainability in RL. <div>
arXiv:2512.05291v1 Announce Type: new 
Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators</title>
<link>https://arxiv.org/abs/2512.05297</link>
<guid>https://arxiv.org/abs/2512.05297</guid>
<content:encoded><![CDATA[
<div> Neural operators, PDEs, continuous-time dynamics, flow matching, temporal resolution invariance  

<br /><br />Summary:  
This paper introduces the Continuous Flow Operator (CFO), a novel framework designed to learn continuous-time dynamics of time-dependent partial differential equations (PDEs) without the computational overhead typical of neural ODE methods. CFO repurposes flow matching to learn the PDE right-hand side directly, avoiding costly backpropagation through ODE solvers. The method fits temporal splines to the given trajectory data and uses finite-difference approximations of time derivatives at spline knots to create probability paths, whose velocities approximate true PDE dynamics. A neural operator is then trained to predict these analytic velocity fields via flow matching. CFO is inherently invariant to time resolution, allowing training on irregular, non-uniform temporal samples and permitting inference queries at arbitrary temporal resolutions through ODE integration. Evaluated on four benchmark problems—the Lorenz system, 1D Burgers equation, 2D diffusion-reaction, and 2D shallow water equations—CFO demonstrates superior long-horizon stability and remarkable data efficiency. Remarkably, CFO trained on only 25% of irregular subsampled time points outperforms autoregressive baselines trained on full datasets, yielding relative error reductions up to 87%. Despite requiring numerical integration during inference, CFO remains computationally efficient, using only half the function evaluations compared to baselines while uniquely supporting reverse-time inference and flexible temporal querying. <div>
arXiv:2512.05297v1 Announce Type: new 
Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)</title>
<link>https://arxiv.org/abs/2512.05306</link>
<guid>https://arxiv.org/abs/2512.05306</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, Sparse Variational Gaussian Process, Uncertainty Quantification, Bayesian Inference, Scientific Machine Learning<br /><br />Summary:<br /><br />This article introduces Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs), a novel framework that enhances interpretability and uncertainty quantification in neural network models. The authors integrate sparse variational Gaussian process inference with the Kolmogorov-Arnold network topology, which allows scalable Bayesian inference with computational complexity that is quasi-linear relative to sample size. A key contribution is the use of analytic moment matching to efficiently propagate uncertainty through deep additive structures while preserving the model’s interpretability. The paper emphasizes the ability of SVGP KANs to distinguish between aleatoric (inherent noise) and epistemic (model) uncertainty, which is critical in scientific applications. This is demonstrated through three example studies: calibrating heteroscedastic measurement noise in fluid flow reconstruction, quantifying confidence degradation in multi-step forecasts of advection-diffusion processes, and detecting out-of-distribution inputs in convolutional autoencoders. The results indicate that SVGP KANs offer significant advantages for uncertainty-aware learning, making them well-suited for scientific machine learning tasks that require principled uncertainty quantification. <div>
arXiv:2512.05306v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?</title>
<link>https://arxiv.org/abs/2512.05311</link>
<guid>https://arxiv.org/abs/2512.05311</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-generated ideas, human vs LLM detection, paraphrasing, source attribution, research context<br /><br />Summary:<br /><br />This paper addresses the challenge of distinguishing between scientific ideas generated by large language models (LLMs) and those produced by humans, an area that has been less explored compared to detecting LLM-generated text. The authors systematically evaluate state-of-the-art machine learning models in their ability to attribute the source of ideas, especially after multiple rounds of paraphrasing. Their results show a significant decline in detection accuracy—on average 25.4%—after five successive paraphrasing iterations, indicating that paraphrasing substantially obscures the origin of ideas. Furthermore, integrating the research problem as a contextual cue into the detection models improves performance by up to 2.97%, demonstrating the value of domain-specific context in attribution tasks. The study also finds that detection algorithms struggle the most when ideas are paraphrased into a simplified, non-expert style, which contributes the greatest challenge in preserving identifiable LLM characteristics. Overall, the work highlights the limitations of current detection methods in distinguishing human from LLM-generated scientific ideas after textual transformation and suggests directions for improving detection by leveraging contextual information and understanding stylistic shifts. <div>
arXiv:2512.05311v1 Announce Type: new 
Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2512.05320</link>
<guid>https://arxiv.org/abs/2512.05320</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Deterministic Policy Gradient, Actor-Critic, Experience Replay, Decoupled Prioritized Experience Replay, Continuous Control  

<br /><br />Summary:  
Background: Deep Deterministic Policy Gradient (DDPG) algorithms use Actor-Critic architectures where both networks are trained on the same replayed transition batches, despite having different learning objectives and update dynamics. This raises questions about the optimality of uniform transition usage.  
Objectives: The study aims to enhance DDPG performance by separating the transition batches used for training the Actor and the Critic, designing an experience replay mechanism that provides tailored learning signals.  
Methods: The authors propose Decoupled Prioritized Experience Replay (DPER), enabling independent sampling of transition batches for the Actor and Critic. DPER is compatible with any off-policy deep reinforcement learning algorithm focused on continuous control and is combined with the Twin Delayed DDPG algorithm for evaluation.  
Results: Experimental evaluation on several OpenAI Gym MuJoCo continuous control tasks demonstrates that DPER outperforms traditional experience replay methods, such as vanilla and prioritized experience replay.  
Conclusions: Decoupling the replay experience for Actor and Critic networks improves training dynamics and final policy performance. DPER offers a general and effective mechanism to boost a wide range of actor-critic off-policy reinforcement learning algorithms. <div>
arXiv:2512.05320v1 Announce Type: new 
Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition</title>
<link>https://arxiv.org/abs/2512.05323</link>
<guid>https://arxiv.org/abs/2512.05323</guid>
<content:encoded><![CDATA[
<div> Keywords: AI weather forecasting, FourCastNetv2, hurricane robustness, noise sensitivity, storm intensity  

<br /><br />Summary:  
This paper evaluates the robustness of NVIDIA’s AI weather forecasting model FourCastNetv2 (FCNv2) against input noise and uncertainty, focusing on hurricane predictions. Two main experiments are conducted: first, the initial conditions of Hurricane Florence from the ERA5 dataset are perturbed by injecting various levels of Gaussian noise to observe effects on predicted trajectories and storm intensity. Second, FCNv2 is started with fully random initial conditions to study its behavior with nonsensical inputs. Results show that FCNv2 can accurately preserve hurricane features under low to moderate noise levels, maintaining the storm’s general trajectory and structure even at high noise, though positional accuracy declines. The model consistently underestimates storm intensity and persistence across all noise levels. When initialized with random data, FCNv2 quickly produces smooth, cohesive forecasts, indicating a tendency towards stable, smoothed outputs regardless of input plausibility. The methodology presented is straightforward and can be applied to other AI-driven weather forecasting models for robustness assessment. These insights are important for evaluating model reliability, particularly for extreme weather event forecasting under uncertain or noisy conditions. <div>
arXiv:2512.05323v1 Announce Type: new 
Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Convex Federated Optimization under Cost-Aware Client Selection</title>
<link>https://arxiv.org/abs/2512.05327</link>
<guid>https://arxiv.org/abs/2512.05327</guid>
<content:encoded><![CDATA[
<div> Keywords: federated optimization, client-selection strategies, communication complexity, variance-reduced gradient, RG-SAGA

<br /><br />Summary:  
This article addresses the challenge of comparing federated optimization algorithms that use different client-selection strategies, noting that existing evaluation metrics do not account for varying communication costs. To remedy this, the authors propose a novel model that quantifies both communication and local computation complexities associated with several commonly used client-selection methods, explicitly linking each strategy to its respective cost. Within this new framework, they introduce an innovative algorithm designed for non-convex federated optimization, achieving the best-known communication and computational efficiency to date. The algorithm builds upon the inexact composite gradient method, enhanced by a specially constructed gradient estimator inspired by SAGA, a variance-reduced estimator known for its ability to leverage functional similarity. The authors derive a new variance bound revealing how SAGA exploits functional similarities among clients. Furthermore, they present the Recursive-Gradient (RG) technique, a general approach to improve error bounds of conditionally unbiased gradient estimators, including SAGA and SVRG. Applying this technique to SAGA yields the RG-SAGA gradient estimator, which demonstrates an improved error bound compared to the original, potentially offering superior convergence properties in federated optimization scenarios. <div>
arXiv:2512.05327v1 Announce Type: new 
Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2512.05336</link>
<guid>https://arxiv.org/abs/2512.05336</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-hop question answering, Large Language Models, Monte Carlo Tree Search, training data quality, sub-query reformulation

<br /><br />Summary: Multi-hop question answering (QA) requires language models to perform multi-step reasoning to arrive at correct answers, which remains a challenging task. Existing systems utilize Large Language Models (LLMs) to decompose questions and reason through multiple steps involving analysis, retrieval, and inference. However, training-based methods often face issues such as hallucinations by LLMs and incorrect reasoning paths that degrade performance. To address these problems, the authors propose PATHFINDER, a novel approach designed to enhance multi-hop QA systems. PATHFINDER leverages Monte Carlo Tree Search (MCTS) to generate training path traces that simulate reasoning steps more effectively. It also improves the quality of training data by filtering out erroneous and overly long traces using sub-answer recall metrics and validation by an LLM functioning as a judge. Additionally, the approach includes reformulation of sub-queries to manage cases where initial retrieval attempts fail, aiming to increase retrieval success. Experimental results show that PATHFINDER outperforms existing methods on public benchmark datasets, demonstrating improved accuracy and reasoning reliability. This combination of MCTS trace generation, data quality filtering, and sub-query reformulation establishes PATHFINDER as a promising solution for advancing multi-hop QA capabilities. <div>
arXiv:2512.05336v1 Announce Type: new 
Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction Tensor Shap</title>
<link>https://arxiv.org/abs/2512.05338</link>
<guid>https://arxiv.org/abs/2512.05338</guid>
<content:encoded><![CDATA[
<div> Keywords: Shapley interaction, Tensor Train, explainable AI, Interaction Tensor SHAP, high-dimensional models<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding feature interactions in deep, high-dimensional machine learning models where existing Shapley-based methods either lack scalability or cannot handle higher order interactions efficiently.<br /><br />2. Current methods like the Shapley Taylor Interaction Index (STII) require exponential subset enumeration, making high order interaction computation infeasible, while tensor-based approaches such as Marginal SHAP Tensor (MST) are limited to first order effects.<br /><br />3. The authors propose Interaction Tensor SHAP (IT SHAP), which represents high order Shapley interactions exactly as tensor network contractions, specifically utilizing Tensor Train (TT) decompositions to achieve polynomial time complexity and polylogarithmic depth.<br /><br />4. IT SHAP reformulates the STII as a contraction between a Value Tensor and a Weight Tensor, both structured under TT format with polynomial ranks, thereby reducing exponential complexity (Theta(4^n)) to highly parallelizable NC2 time.<br /><br />5. This novel method provides a unified, axiomatic, and computationally tractable approach to capture both main effects and higher order interactions, paving the way for scalable interaction-aware explainable AI especially applicable to large, combinatorially complex black box models. <div>
arXiv:2512.05338v1 Announce Type: new 
Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models</title>
<link>https://arxiv.org/abs/2512.05339</link>
<guid>https://arxiv.org/abs/2512.05339</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Safety Alignment, Instruction Fine-tuning, Moderation Pipeline, Safety Benchmark<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) often receive safety alignment after training but can still generate harmful outputs, necessitating robust safeguards that address both inputs and outputs.<br /><br />2. The paper presents Roblox Guard 1.0, an advanced instruction fine-tuned LLM developed to improve LLM safety by implementing comprehensive input-output moderation through a pipeline of multiple LLMs.<br /><br />3. Roblox Guard 1.0 is built upon the Llama-3.1-8B-Instruct architecture and is specifically fine-tuned to generalize well to unseen safety taxonomies, showing strong performance on various out-of-domain safety evaluation benchmarks.<br /><br />4. The instruction fine-tuning methodology combines synthetic and open-source safety datasets, enhanced by chain-of-thought (CoT) rationales and input inversion techniques to improve contextual understanding and decision-making.<br /><br />5. Additionally, the authors introduce RobloxGuard-Eval, a novel benchmark with an extensible safety taxonomy designed to systematically evaluate the effectiveness of LLM guardrails and moderation systems. <div>
arXiv:2512.05339v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation</title>
<link>https://arxiv.org/abs/2512.05341</link>
<guid>https://arxiv.org/abs/2512.05341</guid>
<content:encoded><![CDATA[
<div> Large Language Models, hardware design, unlearning framework, code generation, register-transfer level

<br /><br />Summary: This paper addresses the challenge of ensuring reliability in Large Language Models (LLMs) used for automated digital hardware code generation by proposing a novel unlearning framework. The framework integrates a syntax-preserving unlearning strategy that maintains the structural integrity of hardware description code during the forgetting process. In addition, it employs a fine-grained floor-aware selective loss mechanism that allows for precise, efficient removal of problematic or sensitive knowledge from the model without harming overall performance. This combined approach enables the model to forget proprietary intellectual property, contaminated benchmarks, and unsafe coding behaviors embedded during training. Experimental results demonstrate that this method can handle forget sets up to three times larger than existing approaches, often needing only a single training epoch to effectively unlearn the undesirable content. Importantly, the framework preserves both the syntactic correctness and the functional integrity of register-transfer level (RTL) code generated by the LLM, ensuring that hardware design quality is not compromised. This work thus represents a significant step towards making LLM-assisted hardware design more reliable, secure, and practical in industrial environments. <div>
arXiv:2512.05341v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation</title>
<link>https://arxiv.org/abs/2512.05367</link>
<guid>https://arxiv.org/abs/2512.05367</guid>
<content:encoded><![CDATA[
<div> Hybrid metal halides, dimensionality prediction, machine learning, class imbalance, SMOTE<br /><br />Summary:<br /><br />1. The article introduces a machine learning framework designed to predict the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites.<br /><br />2. The dataset initially consisted of 494 HMH structures categorized into four dimensionality classes: 0D, 1D, 2D, and 3D, with a significant imbalance across these classes.<br /><br />3. To address the class imbalance challenge, the authors applied the Synthetic Minority Oversampling Technique (SMOTE), augmenting the dataset to 1336 samples.<br /><br />4. The framework incorporates chemically-informed, interaction-based feature engineering combined with a multi-stage workflow that includes feature selection, model stacking, and performance optimization.<br /><br />5. The method demonstrated substantial improvements in predicting underrepresented classes, achieving robust F1-scores and reliable cross-validation performance across all dimensionality categories. <div>
arXiv:2512.05367v1 Announce Type: new 
Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Rationalization for Robust Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2512.05373</link>
<guid>https://arxiv.org/abs/2512.05373</guid>
<content:encoded><![CDATA[
<div> Keywords: causal inference, text data, positivity assumption, token selection, treatment effect estimation<br /><br />Summary:<br /><br />This paper addresses challenges in using high-dimensional text data for causal inference, especially for adjusting confounding variables in treatment effect estimation. The authors highlight how the positivity assumption—requiring sufficient overlap in treatment assignment across confounder values—is often violated when large textual data is transformed into feature spaces. This violation results from redundant or spurious textual features that increase dimensionality, causing extreme propensity scores, unstable weights, and inflated variance in causal effect estimates. To overcome these issues, the authors propose Confounding-Aware Token Rationalization (CATR), a novel framework that selects a sparse and necessary subset of tokens. CATR uses a residual-independence diagnostic to ensure that the selected tokens preserve sufficient confounding information for unconfoundedness. By removing irrelevant textual information while retaining essential confounder signals, CATR effectively mitigates positivity violations observed at the feature level and stabilizes downstream causal effect estimators. Experimental results on both synthetic datasets and a real-world medical dataset (MIMIC-III) show that CATR produces more accurate, stable, and interpretable causal effect estimates compared to existing methods, demonstrating its potential for practical applications in natural language processing-based causal inference. <div>
arXiv:2512.05373v1 Announce Type: new 
Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</title>
<link>https://arxiv.org/abs/2512.05377</link>
<guid>https://arxiv.org/abs/2512.05377</guid>
<content:encoded><![CDATA[
<div> Keywords: statistical downscaling, diffusion models, weather forecasting, high-resolution, deep learning<br /><br />Summary:  
This paper addresses the challenge of efficiently producing high-resolution weather forecasts through statistical downscaling, which builds statistical relationships between low- and high-resolution historical data using models. The authors focus on a diffusion-based downscaling framework called CorrDiff, enhancing and extending it by considering a region nearly 20 times larger than in the original work. Unlike the previous version that targeted only surface variables, this study includes high-level atmospheric variables across six pressure levels. A global residual connection is introduced to improve the model's accuracy. The approach generates 3 km resolution forecasts for the China region by downscaling 25 km global grid forecasts from CMA-GFS, an operational global model by the China Meteorological Administration, and SFF, a data-driven deep learning weather model using Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional numerical model, is used as the baseline for comparison. Experimental results show that CorrDiff downscaled forecasts outperform CMA-MESO's direct forecasts in terms of mean absolute error (MAE) for target variables. Additionally, radar composite reflectivity predictions exhibit that CorrDiff, as a generative model, captures fine-scale details more realistically compared to deterministic regression approaches, suggesting improved realism in small-scale weather feature generation. <div>
arXiv:2512.05377v1 Announce Type: new 
Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets</title>
<link>https://arxiv.org/abs/2512.05386</link>
<guid>https://arxiv.org/abs/2512.05386</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, protein-ligand scoring, generalization, self-supervised pretraining, evaluation benchmarks  

<br /><br />Summary:  
This work addresses the challenge of ensuring that machine-learned protein-ligand scoring functions reliably generalize to novel protein targets, a critical factor in molecular design. The authors evaluate the generalization performance of state-of-the-art scoring functions using dataset splits designed to mimic real-world scenarios where few known protein structures and experimental affinity measurements exist for the targets. Their analysis reveals that commonly used benchmarks are insufficient as they do not accurately reflect the true difficulties in generalizing to new targets. To mitigate this gap, they explore the potential of large-scale self-supervised pretraining and provide initial evidence suggesting it can enhance generalization capabilities. Additionally, they examine simpler approaches that utilize limited test-target data to boost scoring function performance, highlighting practical strategies for improvement. The findings emphasize the need for developing more rigorous evaluation protocols tailored to realistic settings in order to better assess scoring function robustness. Ultimately, the study offers important insights and guidance for designing protein-ligand scoring functions with improved predictive power on previously unseen protein targets, advancing the reliability of machine learning tools in molecular discovery. <div>
arXiv:2512.05386v1 Announce Type: new 
Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</title>
<link>https://arxiv.org/abs/2512.05402</link>
<guid>https://arxiv.org/abs/2512.05402</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin mining, ASIC hardware, ROI prediction, Transformer model, time series classification<br /><br />Summary:<br /><br />This paper addresses the challenge of optimally timing the acquisition of Bitcoin mining hardware, particularly ASIC machines, in response to volatile market conditions, fast technological changes, and cyclical revenue patterns. The authors identify a gap in existing research and tools that do not offer guidance on when to purchase mining hardware for profitable outcomes. To fill this gap, they formulate the hardware buying decision as a time series classification problem, categorizing returns on investment (ROI) within one year as profitable (ROI ≥ 1), marginal (0 < ROI < 1), or unprofitable (ROI ≤ 0). The study introduces MineROI-Net, a Transformer-based neural network architecture designed to capture multi-scale temporal patterns in mining profitability data. Evaluated on historical data from 20 different ASIC miners released over the period 2015–2024, MineROI-Net outperforms baseline models including LSTM and TSLANet, achieving an accuracy of 83.7% and a macro F1-score of 83.1%. Most notably, the model attains high precision: 93.6% in detecting unprofitable periods and 98.5% for profitable ones, effectively minimizing costly misclassifications. The work presents MineROI-Net as a practical, open-source tool for miners and investors, enabling more data-driven and financially informed hardware acquisition decisions in a capital-intensive industry. The model and code are publicly available at https://github.com/AMAAI-Lab/MineROI-Net. <div>
arXiv:2512.05402v1 Announce Type: new 
Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design</title>
<link>https://arxiv.org/abs/2512.05403</link>
<guid>https://arxiv.org/abs/2512.05403</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Design, Large Language Models, Evolutionary Algorithms, Multi-expert Consensus, Pareto Optimization<br /><br />Summary: Recent advancements in leveraging large language models (LLMs) have enabled Neural Architecture Design (NAD) systems to generate novel architectures beyond manually predefined search spaces. However, current LLM-driven generation approaches face challenges due to the discrete and non-differentiable nature of token-level design loops, which limits smooth feedback integration and causes issues such as mode collapse and infeasible designs. To address these challenges, the paper introduces RevoNAD, a reflective evolutionary orchestrator that effectively combines LLM-based reasoning with feedback-driven architectural search. RevoNAD implements a Multi-round Multi-expert Consensus mechanism to convert isolated design rules into meaningful architectural clues, enhancing design reliability. It also uses Adaptive Reflective Exploration, dynamically adjusting exploration based on reward variance to balance exploration and refinement of architectures. Additionally, RevoNAD employs Pareto-guided Evolutionary Selection to promote architectures that optimize multiple objectives simultaneously, including accuracy, efficiency, latency, confidence, and structural diversity. Extensive experiments on datasets such as CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape demonstrate that RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate its practical reliability and deployability in neural architecture design tasks. <div>
arXiv:2512.05403v1 Announce Type: new 
Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets</title>
<link>https://arxiv.org/abs/2512.05416</link>
<guid>https://arxiv.org/abs/2512.05416</guid>
<content:encoded><![CDATA[
<div> Sepsis, Electronic Health Records, Graph Convolutional Network, Patient Embeddings, Early Warning<br /><br />Summary:<br /><br />1. Sepsis remains a significant cause of morbidity and mortality in intensive care units, with early detection challenged by complex and heterogeneous EHR data.  
2. The study introduces Triplet-GCN, a novel single-branch graph convolutional network designed to represent each patient encounter as patient-feature-value triplets, forming a bipartite graph between patients and features.  
3. Preprocessing includes median imputation and standardization for numeric data, effect coding for binary data, and mode imputation with low-dimensional embeddings for rare categorical features, while patient nodes are initialized with summary statistics. The edge values retain actual measurements to preserve contextual information about features.  
4. The model learns patient embeddings via graph convolution followed by a lightweight multilayer perceptron and is evaluated on a retrospective, multi-center Chinese cohort of 648 patients with a 70/30 train-test split from three tertiary hospitals.  
5. Triplet-GCN demonstrates superior performance compared to traditional tabular models (KNN, SVM, XGBoost, Random Forest) in terms of discrimination, balanced error metrics, and sensitivity-specificity trade-offs, suggesting that encoding EHR as triplets and leveraging graph structures enhances sepsis early warning and risk stratification. <div>
arXiv:2512.05416v1 Announce Type: new 
Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient-feature-value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train-test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity-specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient-feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.05419</link>
<guid>https://arxiv.org/abs/2512.05419</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Foundation Model, chemical mechanical polishing, material removal rate, few-shot learning, attention mechanism<br /><br />Summary:<br /><br />This paper addresses the limitations of existing data-driven approaches for estimating the material removal rate (MRR) in semiconductor manufacturing processes like chemical mechanical polishing (CMP), which traditionally rely on static feature extraction from time series data, ignoring temporal dynamics. Recognizing the need for models effective in low-data environments, the authors propose TS-Hint, a novel Time Series Foundation Model (TSFM) framework. TS-Hint incorporates chain-of-thought reasoning to enhance learning by providing attention hints during training, which leverage both attention mechanism data and saliency information. This approach enables the model to focus more effectively on relevant temporal features within multivariate time series data. Experimentally, TS-Hint demonstrates superior performance, particularly in few-shot learning scenarios where data availability is limited, showcasing its ability to learn directly from complex time series inputs without the extensive data typically required. These findings suggest TS-Hint’s potential as a powerful tool for dynamic process monitoring and control in semiconductor manufacturing, overcoming previous constraints by embedding temporal understanding and attention-guided learning in the framework. <div>
arXiv:2512.05419v1 Announce Type: new 
Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title>
<link>https://arxiv.org/abs/2512.05442</link>
<guid>https://arxiv.org/abs/2512.05442</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, deep learning, negative samples, IdealTSF framework, adversarial optimization  

<br /><br />Summary:  
This paper addresses the challenges of time series forecasting, particularly focusing on issues caused by missing values and anomalies in sequential data. Traditional methods have emphasized extracting features from sequences or utilizing suboptimal data as positive samples for knowledge transfer. However, the authors propose that leveraging these non-ideal negative samples can significantly improve event prediction. To this end, they introduce the IdealTSF framework, which uniquely incorporates both ideal positive and negative samples into the forecasting process. The framework operates through three stages: pretraining, training, and optimization. Initially, the model undergoes pretraining by learning from negative sample data to extract meaningful knowledge. During the training phase, sequence data are transformed into ideal positive samples to enhance model robustness. Furthermore, the framework integrates a negative optimization mechanism that employs adversarial disturbances, helping the model better handle noisy or low-quality data. Experimental results demonstrate that incorporating negative samples greatly enhances the performance of the basic attention architecture used in time series forecasting. Overall, IdealTSF is especially advantageous in applications where data quality is compromised, providing a more effective solution for forecasting tasks under challenging data conditions. <div>
arXiv:2512.05442v1 Announce Type: new 
Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data</title>
<link>https://arxiv.org/abs/2512.05469</link>
<guid>https://arxiv.org/abs/2512.05469</guid>
<content:encoded><![CDATA[
<div> Keywords: ensemble models, generalization gap, tabular classification, overfitting, dataset complexity indicators<br /><br />Summary:<br /><br />This study investigates how ensemble models balance accuracy and overfitting across four tabular classification datasets: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. It uses repeated stratified cross-validation with statistical significance testing to compare linear models, a single decision tree, and nine ensemble methods. The results indicate that ensembles maintain high accuracy without large generalization gaps mainly by reducing variance through averaging or controlled boosting. For nearly linear, clean datasets, linear models already generalize well, so ensembles add little improvement. However, on datasets with significant nonlinear patterns, tree-based ensembles enhance test accuracy by 5 to 7 percentage points while keeping generalization gaps under 3%. On noisy or highly imbalanced datasets, ensembles stay competitive but need proper regularization to avoid fitting noise or majority class biases. The study introduces simple dataset complexity measures—such as linearity score, Fisher ratio, and noise estimate—to explain when ensembles are effective at variance control. Overall, these findings offer practical guidance for selecting models in real-world tabular machine learning, clarifying conditions under which ensembles provide the best trade-off between accuracy and overfitting. <div>
arXiv:2512.05469v1 Announce Type: new 
Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning</title>
<link>https://arxiv.org/abs/2512.05475</link>
<guid>https://arxiv.org/abs/2512.05475</guid>
<content:encoded><![CDATA[
<div> Keywords: Geometric Quantum Machine Learning, molecular geometry, equivariance, permutational symmetry, graph embedding<br /><br />Summary:<br /><br />1. The study compares the performance of various Geometric Quantum Machine Learning (QML) models arranged by molecular geometry hierarchy.<br />2. Two molecular datasets are analyzed: a simple linear molecule (LiH) and a trigonal pyramidal molecule (NH3), focusing on their accuracy and generalizability.<br />3. A classical equivariant model is used as a baseline to benchmark the quantum models.<br />4. The paper investigates QML models with different symmetry considerations—no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance—to understand their impact on performance.<br />5. Results indicate that graph embedding of features enhances the trainability of geometric datasets.<br />6. Among the models tested, permutational symmetric embedding is identified as the most generalizable QML approach for learning geometric molecular data.<br />7. The study reveals that molecular geometry plays a crucial role in determining the suitable model for generalizability in quantum machine learning tasks related to molecules. <div>
arXiv:2512.05475v1 Announce Type: new 
Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turbulence Regression</title>
<link>https://arxiv.org/abs/2512.05483</link>
<guid>https://arxiv.org/abs/2512.05483</guid>
<content:encoded><![CDATA[
<div> Keywords: Air turbulence, NeuTucker, Tucker decomposition, wind field data, spatio-temporal interactions<br /><br />Summary:<br /><br />Air turbulence is characterized by irregular and disordered airflow caused by abrupt changes in velocity, pressure, or direction, particularly difficult to predict at low altitudes due to complex influencing factors. Traditional turbulence prediction methods relying solely on wind profile radar data face significant challenges in accurately capturing turbulence states. To address this, the paper introduces a NeuTucker (NeuTucF) decomposition model that processes discretized three-dimensional wind field data. The model is designed to handle continuous yet sparse data by discretizing continuous inputs, making them compatible with the NeuTucker framework. Central to this approach is the construction of a four-dimensional Tucker interaction tensor, which captures all possible spatial and temporal interactions across different elevations and the three components of wind speed. This low-rank Tucker decomposition model, based on a Tucker neural network, effectively extracts latent interactions within the 3D wind field data. When applied to real-world datasets with missing observations, the discretized NeuTucF model outperforms several common regression models in estimating missing values, demonstrating superior predictive accuracy and robustness in turbulence modeling under limited observational conditions. <div>
arXiv:2512.05483v1 Announce Type: new 
Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2512.05502</link>
<guid>https://arxiv.org/abs/2512.05502</guid>
<content:encoded><![CDATA[
<div> Quantitative Systems Pharmacology, QSP modeling, graph reasoning, human-in-the-loop, biological knowledge graphs<br /><br />Summary:  
The article introduces GRASP, a novel multi-agent, graph-reasoning framework designed to improve Quantitative Systems Pharmacology (QSP) modeling, which is critical for drug development but traditionally time-intensive. GRASP encodes QSP models as typed biological knowledge graphs and translates them into executable MATLAB/SimBiology code, ensuring preservation of units, mass balance, and physiological constraints. It operates via a two-phase workflow: "Understanding," which reconstructs graphs from legacy code, and "Action," which enables constraint-checked, language-driven modifications using a state machine that iteratively validates model changes. The system employs breadth-first search (BFS) parameter alignment around new entities to identify dependent quantities and suggest biologically plausible defaults, supporting automatic execution and diagnostics until convergence. In head-to-head comparisons judged by a large language model (LLM), GRASP surpasses subject matter expert-guided chain-of-thought (CoT) and tree-of-thought (ToT) baselines in biological plausibility, mathematical correctness, structural fidelity, and code quality, scoring approximately 9–10 out of 10 versus 5–7 for baselines. BFS alignment achieves an F1 score of 0.95 in discovery of dependencies, unit consistency, and value ranges. This approach demonstrates that graph-structured, agent-driven workflows with human interaction can accelerate and rigorously support QSP model development, allowing domain experts to use natural language specifications without compromising biomedical fidelity. <div>
arXiv:2512.05502v1 Announce Type: new 
Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credal and Interval Deep Evidential Classifications</title>
<link>https://arxiv.org/abs/2512.05526</link>
<guid>https://arxiv.org/abs/2512.05526</guid>
<content:encoded><![CDATA[
<div> Uncertainty Quantification, Evidential Deep Learning, Credal Sets, Epistemic Uncertainty, Out-of-Distribution Detection<br /><br />Summary:  
This paper addresses the critical challenge of Uncertainty Quantification (UQ) in Artificial Intelligence classification tasks by proposing two novel methods: Credal Deep Evidential Classification (CDEC) and Interval Deep Evidential Classification (IDEC). Both techniques utilize advanced theoretical constructs—credal sets and intervals of evidential predictive distributions—to manage and represent uncertainty, specifically targeting epistemic (reducible) and aleatoric (irreducible) uncertainties. Unlike traditional models, CDEC and IDEC can abstain from making classifications when uncertainty exceeds defined thresholds, enhancing reliability by flagging excessive uncertainty. When uncertainties are within acceptable bounds, these methods deliver sets of probable labels accompanied by strong probabilistic guarantees, improving decision robustness. The models are trained through standard backpropagation with a specialized loss function based on evidential theory, effectively overcoming limitations from prior evidential deep learning approaches. Extensive experimentation on datasets like MNIST, CIFAR-10, CIFAR-100, and their related out-of-distribution shifts (e.g., F-MNIST, SVHN, TinyImageNet) demonstrates that CDEC and IDEC not only maintain competitive accuracy but also achieve state-of-the-art performance in out-of-distribution detection driven by epistemic and total uncertainty. Furthermore, an ablation study on ensemble size reveals that CDEC provides stable uncertainty estimates even with a relatively small ensemble, underlining efficiency alongside accuracy. <div>
arXiv:2512.05526v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.05531</link>
<guid>https://arxiv.org/abs/2512.05531</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, data streams, incremental update, kernel mean embedding, Isolation Distributional Kernel<br /><br />Summary:<br />1. The paper introduces $\mathcal{IDK}$-$\mathcal{S}$, a novel Incremental Distributional Kernel designed specifically for streaming anomaly detection tasks that must adapt to evolving data distributions in real time.<br />2. It builds upon the Isolation Distributional Kernel, a previously developed offline anomaly detector known for outperforming classic methods such as Isolation Forest and Local Outlier Factor by leveraging a data-dependent kernel.<br />3. The main innovation lies in an efficient incremental update mechanism that allows the model to update dynamically without the computational expense of fully retraining the model after every data change.<br />4. This incremental approach significantly reduces computational overhead and accelerates processing times, maintaining detection accuracy at a statistically equivalent level compared to full retraining.<br />5. Extensive experiments conducted on thirteen benchmark datasets validate the proposed method, demonstrating that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior accuracy and is frequently an order of magnitude faster than current state-of-the-art streaming anomaly detection methods. <div>
arXiv:2512.05531v1 Announce Type: new 
Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.05534</link>
<guid>https://arxiv.org/abs/2512.05534</guid>
<content:encoded><![CDATA[
<div> Sparse dictionary learning, mechanistic interpretability, neural networks, optimization landscape, feature disentanglement<br /><br />Summary:<br /><br />This paper addresses the understanding of sparse dictionary learning (SDL) methods used in mechanistic interpretability of neural networks. 1) It highlights the importance of interpreting neural representations, which often encode multiple concepts in superposition, and reviews existing SDL methods such as sparse autoencoders, transcoders, and crosscoders that enforce sparsity to disentangle these features. 2) The authors identify a gap in theoretical understanding, as prior work largely focuses on sparse autoencoders with tied-weight constraints, leaving other SDL variants unexplored theoretically. 3) They propose the first unified theoretical framework that formulates multiple SDL approaches as instances of a single optimization problem, thus bridging various methods under one formalism. 4) Through rigorous analysis, the paper investigates the optimization landscape of SDL, providing explanations for several phenomena observed empirically, like feature absorption, dead neurons, and the neuron resampling technique. 5) To validate their theoretical insights, the authors also conduct controlled experiments, demonstrating consistency between theory and observed behaviors in SDL methods, ultimately advancing foundational understanding of sparse representation learning in neural networks. <div>
arXiv:2512.05534v1 Announce Type: new 
Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.05540</link>
<guid>https://arxiv.org/abs/2512.05540</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view anomaly detection, consistent neighborhoods, data-dependent properties, computational efficiency, SCoNE

<br /><br />Summary:  
The paper addresses the core challenge in multi-view anomaly detection, which is to represent local neighborhoods of normal data points consistently across multiple views. Existing methods represent neighborhoods independently in each view and then align them through a learning process, but these approaches face two key problems: (1) they struggle to capture consistent neighbors accurately, especially when neighbor densities vary across views, leading to poor detection performance; and (2) the learning process has a high computational cost of order $\mathcal{O}(N^2)$, making it impractical for large datasets. To overcome these limitations, the authors propose a novel approach called Spherical Consistent Neighborhoods Ensemble (SCoNE). SCoNE uniquely represents consistent neighborhoods directly using multi-view instances without relying on intermediate representations. Additionally, it models neighborhoods with data-dependent sizes—larger neighborhoods in sparse regions and smaller ones in dense regions—enabling better consistency across views without any learning step. As a result, SCoNE achieves a linear time complexity of $\mathcal{O}(N)$. Extensive empirical evaluations demonstrate that SCoNE not only outperforms existing methods in anomaly detection accuracy but also runs orders of magnitude faster on large-scale datasets, making it both effective and efficient. <div>
arXiv:2512.05540v1 Announce Type: new 
Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs</title>
<link>https://arxiv.org/abs/2512.05542</link>
<guid>https://arxiv.org/abs/2512.05542</guid>
<content:encoded><![CDATA[
<div> Best-of-n, LLM inference, multi-LLM routing, reward model, test-time scaling<br /><br />Summary:<br /><br />1. The paper introduces RoBoN (Routed Online Best-of-n), a novel method for test-time scaling in large language model (LLM) inference that leverages multiple LLMs instead of relying on a single model as in traditional best-of-n approaches. <br /><br />2. RoBoN sequentially routes generations across a suite of models based on scores derived from a reward model and an agreement signal on predicted responses, enabling dynamic and online model selection during inference without additional training. <br /><br />3. This method maintains computational parity with standard best-of-n by using the same overall compute budget and can work with any plug-in reward model. <br /><br />4. Extensive experiments on reasoning benchmarks such as MATH500, OlympiadBench, MinervaMath, GSM8K, and MMLU demonstrate that RoBoN consistently outperforms traditional best-of-n applied to individual models, achieving up to 3.4% absolute accuracy improvement. <br /><br />5. Furthermore, RoBoN surpasses a uniform multi-model portfolio baseline, highlighting the advantage of exploiting model diversity at inference time to enhance performance without additional training or fine-tuning. <div>
arXiv:2512.05542v1 Announce Type: new 
Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Local Fidelity Through Sampling and Modeling Nonlinearity</title>
<link>https://arxiv.org/abs/2512.05556</link>
<guid>https://arxiv.org/abs/2512.05556</guid>
<content:encoded><![CDATA[
<div> Black-box models, explainability, LIME, MARS, N-ball sampling  

<br /><br />Summary:  
This paper addresses the limitations of the Local Interpretable Model-agnostic Explanation (LIME) method, which assumes linear local decision boundaries and thus struggles to capture non-linear relationships in model predictions. To overcome this, the authors propose using Multivariate Adaptive Regression Splines (MARS) to model non-linear local boundaries, enhancing the fidelity of explanations by better capturing the behavior of the reference black-box model. Furthermore, the study introduces the N-ball sampling technique, which samples directly from the target distribution rather than reweighting samples as done in LIME, leading to improved faithfulness in explanations. The proposed method is evaluated on three UCI datasets employing various classifiers and different kernel widths, demonstrating superior performance compared to existing baselines. Experimental results report an average 37% reduction in root mean square error, indicating a significant improvement in local fidelity and explanation accuracy. Overall, this approach provides more reliable and faithful local explanations for complex black-box machine learning models, which is crucial for their adoption in high-stakes decision-making scenarios. <div>
arXiv:2512.05556v1 Announce Type: new 
Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</title>
<link>https://arxiv.org/abs/2512.05567</link>
<guid>https://arxiv.org/abs/2512.05567</guid>
<content:encoded><![CDATA[
<div> optimal transport, semi-supervised learning, Wasserstein distance, image classification, multipath interference detection<br /><br />Summary:<br /><br />1. This study proposes a semi-supervised learning approach based on optimal transport theory to address the challenge of learning from scarce labeled image data using deep convolutional networks. 2. The method leverages an implicit graph-based transductive framework where the similarity between image samples is quantified using the Wasserstein distance, a metric from optimal transport. 3. This distance metric is integrated into a label propagation mechanism that guides the learning process, enhancing the ability to generalize from limited labeled data. 4. The effectiveness of the proposed approach is validated on a real-world GNSS (Global Navigation Satellite System) application, specifically focusing on the detection of multipath interference, which is a significant issue in satellite signal interpretation. 5. Experiments conducted under various signal conditions demonstrate that by carefully selecting hyperparameters related to the degree of semi-supervision and sensitivity to the metric, the model achieves significantly improved classification accuracy compared to fully supervised training methods, highlighting the potential of the approach for practical applications dealing with limited labeled data. <div>
arXiv:2512.05567v1 Announce Type: new 
Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05591</link>
<guid>https://arxiv.org/abs/2512.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, entropy ratio, policy stability, PPO-Clip, distribution shift<br /><br />Summary:<br /><br />This paper addresses instability issues in large language model post-training using reinforcement learning, particularly under the off-policy training paradigm that causes distribution shift and leads to unstable gradients and fluctuating policy entropy. The widely used PPO-Clip method partially mitigates these problems by clipping importance weights but fails to account for the overall global distributional shift, especially in actions not sampled during training. To overcome these limitations, the authors introduce a novel global metric— the entropy ratio between current and previous policies—that quantifies the relative change in policy exploration during updates. They propose Entropy Ratio Clipping (ERC), a new mechanism imposing bidirectional constraints on this entropy ratio, effectively stabilizing policy updates at the global distribution level and regulating shifts in the probability of unsampled actions. ERC is integrated into existing reinforcement learning algorithms DAPO and GPPO. Experimental results on multiple benchmark tasks demonstrate that ERC consistently enhances performance and training stability compared to baseline methods. This work highlights the importance of global distributional metrics in stabilizing reinforcement learning updates for language model fine-tuning. <div>
arXiv:2512.05591v1 Announce Type: new 
Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales</title>
<link>https://arxiv.org/abs/2512.05620</link>
<guid>https://arxiv.org/abs/2512.05620</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning optimizers, hyperparameter transfer, matrix-level preconditioning, scaling laws, language model training<br /><br />Summary:<br />1. This paper investigates the scaling behavior of advanced deep learning optimizers that use matrix-level preconditioning, such as Shampoo, SOAP, and Muon, which have shown promising speedups over AdamW in smaller-scale experiments but whose effectiveness at larger scale remains unclear.<br />2. The authors build on prior work like $\mu$P to analyze how the optimal learning rate and weight decay should scale with model width and depth across various optimizers, considering the impact of techniques including blocking and grafting.<br />3. They find that applying $\mu$P-based learning rate scaling improves hyperparameter transfer but still exhibits deviations at finite model widths, which can be mitigated using blocking strategies and explicit spectral normalization.<br />4. For compute-optimal scaling, they demonstrate that setting weight decay inversely proportional to model width ($1/\mathrm{width}$) performs nearly optimally across all studied optimizers.<br />5. Applying these refined scaling rules, Muon and Shampoo consistently outperform AdamW by approximately 1.4× and 1.3× speedups, respectively, when training Llama-architecture language models ranging from 190 million to 1.4 billion parameters; incorrect scaling quickly negates these gains.<br />6. The study concludes that robust and principled hyperparameter transfer is crucial for reliably comparing optimizer performance at scale under realistic tuning budgets. <div>
arXiv:2512.05620v1 Announce Type: new 
Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $\mu$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $\mu$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounded Graph Clustering with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.05623</link>
<guid>https://arxiv.org/abs/2512.05623</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, graph neural networks, clustering, flexible cluster control, machine learning  

<br /><br />Summary:  
This paper addresses a common challenge in community detection using graph neural networks (GNNs), where users typically need to specify the number of clusters beforehand, a requirement that can be impractical or inaccurate. Unlike classical algorithms that can sometimes infer the number of clusters from data, most GNN-based methods struggle to return the exact number of clusters, even when specified. To overcome this, the authors propose a novel framework that enables flexible control over the number of communities identified by GNNs during training. This method allows users to define a plausible range for the number of clusters rather than needing an exact count, improving adaptability. Moreover, if an exact number is desired, the framework can enforce this constraint and reliably produce the specified number of clusters. The approach thus provides both flexibility and precision in community detection tasks and enhances the practical usability of GNNs in scenarios where cluster number specification is challenging or uncertain. This work contributes a principled solution to a key limitation in GNN-based clustering models, making them more robust and user-friendly for varied applications. <div>
arXiv:2512.05623v1 Announce Type: new 
Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability</title>
<link>https://arxiv.org/abs/2512.05638</link>
<guid>https://arxiv.org/abs/2512.05638</guid>
<content:encoded><![CDATA[
<div> modular jets, regression pipelines, identifiability, mirage regimes, machine learning evaluation<br /><br />Summary:<br /><br />This paper addresses the limitations of classical supervised learning evaluations, which focus primarily on predictive risk over hold-out data without considering whether a model's internal modular decomposition is uniquely identifiable. The authors introduce the concept of Modular Jets, which are local linear response maps that characterize how individual modules within a regression or classification pipeline react to small, structured perturbations in the input. They define "mirage" regimes, where different modular decompositions produce indistinguishable jets and therefore cannot be distinguished based on data, contrasting these with "identifiable" regimes in which the modular decomposition is uniquely determined up to natural symmetries. A key theoretical contribution is a jet-identifiability theorem for two-module linear regression pipelines, demonstrating that under certain rank conditions and with access to module-level jets, the internal factorization is uniquely determined. This stands in contrast to traditional evaluation methods based solely on predictive risk, which admit many observationally equivalent but structurally different decompositions. To operationalize their framework, the authors present MoJet, an algorithm for estimating empirical jets and diagnosing mirage regimes, and validate their approach on both linear and deep regression as well as classification pipelines. <div>
arXiv:2512.05638v1 Announce Type: new 
Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</title>
<link>https://arxiv.org/abs/2512.05648</link>
<guid>https://arxiv.org/abs/2512.05648</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Gradient Routing, Selective Gradient Masking, label noise, unlearning<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) carry dual-use risks, particularly when harmful data is mislabeled during training, making data filtering challenging and potentially ineffective at scale.<br />2. Prior work introduced Gradient Routing to isolate harmful or sensitive knowledge into specific parameters, enabling later removal of that knowledge.<br />3. This paper proposes an improved method called Selective Gradient Masking (SGTM), which zero-masks selected gradients so that target domain examples update only dedicated parameters, enhancing robustness to noisy labels.<br />4. SGTM’s effectiveness is demonstrated via two case studies: removing knowledge of one language from a bilingual synthetic dataset and removing biology knowledge from an English Wikipedia-trained model.<br />5. Compared to data filtering and previous Gradient Routing methods, SGTM achieves a better balance between retaining useful knowledge and forgetting target information despite labeling errors.<br />6. SGTM also shows strong resistance to adversarial fine-tuning, requiring significantly more fine-tuning steps to restore forgotten knowledge compared to finetuning-based unlearning methods.<br />7. The findings suggest SGTM is a promising pretraining-time safety mitigation, particularly valuable where label noise is unavoidable. <div>
arXiv:2512.05648v1 Announce Type: new 
Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feasibility of AI-Assisted Programming for End-User Development</title>
<link>https://arxiv.org/abs/2512.05666</link>
<guid>https://arxiv.org/abs/2512.05666</guid>
<content:encoded><![CDATA[
<div> End-user development, low-code/no-code, AI-assisted coding, generative AI, web app development  

<br /><br />Summary:  
The paper explores the concept of AI-assisted end-user coding, where non-programmers create or adapt digital tools through natural language interaction with AI assistants, as opposed to traditional low-code/no-code (LCNC) visual programming platforms. It highlights how generative AI, especially large language models acting as programming copilots, can enhance end-user development by offering greater flexibility, faster creation, and improved reusability while reducing vendor lock-in. The study involved non-programmer participants tasked with building a basic web app using AI assistance. Most participants successfully completed the task within a reasonable timeframe and reported positive experiences with AI-assisted development. The findings suggest that AI-assisted end-user coding is a feasible and promising alternative or complement to LCNC approaches. The paper discusses the implications of this paradigm shift for organizational digital transformation, recommends directions for future research, and considers the impact on academic teaching related to software development and end-user programming. Overall, the research indicates that integrating generative AI into end-user development workflows may broaden accessibility and efficiency in creating customized digital solutions. <div>
arXiv:2512.05666v1 Announce Type: new 
Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks</title>
<link>https://arxiv.org/abs/2512.05680</link>
<guid>https://arxiv.org/abs/2512.05680</guid>
<content:encoded><![CDATA[
<div> Beamforming, 5G, 6G, POMDP, online search<br /><br />Summary:<br /><br />1. The paper addresses beamforming in antenna arrays used in 5G and 6G networks, which require selecting optimal beams from large codebooks to maximize data rates for moving user equipments (UEs).<br /><br />2. Traditional analog beamforming relies on pre-configured codebooks and supervised learning classifiers to predict the next best beam based on historical beam selections; however, this method struggles with large codebooks and environmental effects like reflections and blockages.<br /><br />3. The authors propose reformulating the beam selection problem as a partially observable Markov decision process (POMDP), treating the beam codebook as the environment and the unobservable optimal beam as the hidden state.<br /><br />4. Their method selects candidate beams conditioned on a belief state derived from previously probed beams, effectively performing an online search to locate the optimal beam dynamically.<br /><br />5. Unlike previous supervised approaches, this POMDP-based method adapts to new or unforeseen UE trajectories and environmental changes, achieving significantly better performance and robustness in beam selection tasks. <div>
arXiv:2512.05680v1 Announce Type: new 
Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language</title>
<link>https://arxiv.org/abs/2512.05721</link>
<guid>https://arxiv.org/abs/2512.05721</guid>
<content:encoded><![CDATA[
<div> Keywords: BERTO, traffic prediction, energy optimization, transformer, natural language prompts

<br /><br />Summary:  
This paper introduces BERTO, a BERT-based framework designed for traffic prediction and energy optimization in cellular networks. 1) BERTO leverages transformer architectures to achieve high prediction accuracy in network traffic forecasting. 2) It incorporates a novel Balancing Loss Function, allowing network operators to fine-tune the trade-off between power savings and service performance. 3) The framework utilizes prompt-based customization with natural language inputs, enabling operators to guide the model in handling underprediction and overprediction according to their specific operational goals. 4) Experiments conducted on real-world datasets demonstrate that BERTO reduces mean squared error (MSE) by 4.13% compared to existing models, highlighting its improved predictive capability. 5) Additionally, BERTO supports flexible control over power consumption within a range of 1.4 kW and can manage service quality variations by up to 9 times, making it highly adaptable for intelligent Radio Access Network (RAN) deployments. Overall, BERTO offers a practical and customizable solution for optimizing energy use and maintaining performance in cellular networks through advanced AI techniques coupled with intuitive natural language guidance. <div>
arXiv:2512.05721v1 Announce Type: new 
Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Language Models Mechanistic Explainability Through Arrow-Pushing</title>
<link>https://arxiv.org/abs/2512.05722</link>
<guid>https://arxiv.org/abs/2512.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: chemical reaction mechanisms, language models, arrow pushing formalism, MechSMILES, reaction mechanism prediction<br /><br />Summary:<br /><br />This paper introduces a novel computational framework that teaches language models to predict chemical reaction mechanisms based on the arrow pushing formalism, which represents electron flow while respecting conservation laws. The authors developed MechSMILES, a compact textual format that encodes both molecular structures and the associated electron movements, facilitating mechanistic learning by language models. They trained models on four progressively challenging mechanism prediction tasks using datasets such as mech-USPTO-31k and FlowER. The models demonstrate strong performance, achieving over 95% top-3 accuracy on elementary step predictions and surpassing 73% and 93% complete mechanism retrieval accuracy on the mech-USPTO-31k and FlowER datasets, respectively. The mechanistic insight gained enables three significant applications: (1) serving as post-hoc validators for CASP systems to filter out chemically implausible reaction transformations; (2) enabling comprehensive atom-to-atom mapping that tracks all atoms—including hydrogens—throughout reactions; and (3) extracting catalyst-aware reaction templates that differentiate between recycled catalysts and non-participating spectator species. By grounding predictions in physically meaningful electron movements that ensure mass and charge conservation, this work paves the way toward more explainable, chemically valid computational synthesis planning and provides an architecture-agnostic framework for benchmarking reaction mechanism prediction. <div>
arXiv:2512.05722v1 Announce Type: new 
Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards agent-based-model informed neural networks</title>
<link>https://arxiv.org/abs/2512.05764</link>
<guid>https://arxiv.org/abs/2512.05764</guid>
<content:encoded><![CDATA[
arXiv:2512.05764v1 Announce Type: new 
Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learnability Window in Gated Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2512.05790</link>
<guid>https://arxiv.org/abs/2512.05790</guid>
<content:encoded><![CDATA[
arXiv:2512.05790v1 Announce Type: new 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of Antibody Language Models Using SAEs</title>
<link>https://arxiv.org/abs/2512.05794</link>
<guid>https://arxiv.org/abs/2512.05794</guid>
<content:encoded><![CDATA[
arXiv:2512.05794v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws</title>
<link>https://arxiv.org/abs/2512.05817</link>
<guid>https://arxiv.org/abs/2512.05817</guid>
<content:encoded><![CDATA[
arXiv:2512.05817v1 Announce Type: new 
Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2512.05825</link>
<guid>https://arxiv.org/abs/2512.05825</guid>
<content:encoded><![CDATA[
arXiv:2512.05825v1 Announce Type: new 
Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation</title>
<link>https://arxiv.org/abs/2512.05844</link>
<guid>https://arxiv.org/abs/2512.05844</guid>
<content:encoded><![CDATA[
arXiv:2512.05844v1 Announce Type: new 
Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Attention Post-Training for Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.05865</link>
<guid>https://arxiv.org/abs/2512.05865</guid>
<content:encoded><![CDATA[
arXiv:2512.05865v1 Announce Type: new 
Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.05868</link>
<guid>https://arxiv.org/abs/2512.05868</guid>
<content:encoded><![CDATA[
arXiv:2512.05868v1 Announce Type: new 
Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2512.05870</link>
<guid>https://arxiv.org/abs/2512.05870</guid>
<content:encoded><![CDATA[
arXiv:2512.05870v1 Announce Type: new 
Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Coherence : Find higher performance to out-of-distribution tasks from few samples</title>
<link>https://arxiv.org/abs/2512.05880</link>
<guid>https://arxiv.org/abs/2512.05880</guid>
<content:encoded><![CDATA[
arXiv:2512.05880v1 Announce Type: new 
Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints</title>
<link>https://arxiv.org/abs/2512.05881</link>
<guid>https://arxiv.org/abs/2512.05881</guid>
<content:encoded><![CDATA[
arXiv:2512.05881v1 Announce Type: new 
Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process</title>
<link>https://arxiv.org/abs/2512.05893</link>
<guid>https://arxiv.org/abs/2512.05893</guid>
<content:encoded><![CDATA[
arXiv:2512.05893v1 Announce Type: new 
Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $\mu >0$ and $\beta \in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction</title>
<link>https://arxiv.org/abs/2512.05915</link>
<guid>https://arxiv.org/abs/2512.05915</guid>
<content:encoded><![CDATA[
arXiv:2512.05915v1 Announce Type: new 
Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</title>
<link>https://arxiv.org/abs/2512.05916</link>
<guid>https://arxiv.org/abs/2512.05916</guid>
<content:encoded><![CDATA[
arXiv:2512.05916v1 Announce Type: new 
Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Bayes Inconsistency of Disagreement Discrepancy Surrogates</title>
<link>https://arxiv.org/abs/2512.05931</link>
<guid>https://arxiv.org/abs/2512.05931</guid>
<content:encoded><![CDATA[
arXiv:2512.05931v1 Announce Type: new 
Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
<link>https://arxiv.org/abs/2512.05948</link>
<guid>https://arxiv.org/abs/2512.05948</guid>
<content:encoded><![CDATA[
arXiv:2512.05948v1 Announce Type: new 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impugan: Learning Conditional Generative Models for Robust Data Imputation</title>
<link>https://arxiv.org/abs/2512.05950</link>
<guid>https://arxiv.org/abs/2512.05950</guid>
<content:encoded><![CDATA[
arXiv:2512.05950v1 Announce Type: new 
Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution</title>
<link>https://arxiv.org/abs/2512.05958</link>
<guid>https://arxiv.org/abs/2512.05958</guid>
<content:encoded><![CDATA[
arXiv:2512.05958v1 Announce Type: new 
Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity</title>
<link>https://arxiv.org/abs/2512.05962</link>
<guid>https://arxiv.org/abs/2512.05962</guid>
<content:encoded><![CDATA[
arXiv:2512.05962v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $\alpha$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimization and Convolutional Neural Networks for Zernike-Based Wavefront Correction in High Harmonic Generation</title>
<link>https://arxiv.org/abs/2512.05127</link>
<guid>https://arxiv.org/abs/2512.05127</guid>
<content:encoded><![CDATA[
arXiv:2512.05127v1 Announce Type: cross 
Abstract: High harmonic generation (HHG) is a nonlinear process that enables table-top generation of tunable, high-energy, coherent, ultrashort radiation pulses in the extreme ultraviolet (EUV) to soft X-ray range. These pulses find applications in photoemission spectroscopy in condensed matter physics, pump-probe spectroscopy for high-energy-density plasmas, and attosecond science. However, optical aberrations in the high-power laser systems required for HHG degrade beam quality and reduce efficiency. We present a machine learning approach to optimize aberration correction using a spatial light modulator. We implemented and compared Bayesian optimization and convolutional neural network (CNN) methods to predict optimal Zernike polynomial coefficients for wavefront correction. Our CNN achieved promising results with 80.39% accuracy on test data, demonstrating the potential for automated aberration correction in HHG systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05134</link>
<guid>https://arxiv.org/abs/2512.05134</guid>
<content:encoded><![CDATA[
arXiv:2512.05134v1 Announce Type: cross 
Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</title>
<link>https://arxiv.org/abs/2512.05139</link>
<guid>https://arxiv.org/abs/2512.05139</guid>
<content:encoded><![CDATA[
arXiv:2512.05139v1 Announce Type: cross 
Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title>
<link>https://arxiv.org/abs/2512.05156</link>
<guid>https://arxiv.org/abs/2512.05156</guid>
<content:encoded><![CDATA[
arXiv:2512.05156v1 Announce Type: cross 
Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-Time Homeostatic Dynamics for Reentrant Inference Models</title>
<link>https://arxiv.org/abs/2512.05158</link>
<guid>https://arxiv.org/abs/2512.05158</guid>
<content:encoded><![CDATA[
arXiv:2512.05158v1 Announce Type: cross 
Abstract: We formulate the Fast-Weights Homeostatic Reentry Network (FHRN) as a continuous-time neural-ODE system, revealing its role as a norm-regulated reentrant dynamical process. Starting from the discrete reentry rule $x_t = x_t^{(\mathrm{ex})} + \gamma\, W_r\, g(\|y_{t-1}\|)\, y_{t-1}$, we derive the coupled system $\dot{y}=-y+f(W_ry;\,x,\,A)+g_{\mathrm{h}}(y)$ showing that the network couples fast associative memory with global radial homeostasis. The dynamics admit bounded attractors governed by an energy functional, yielding a ring-like manifold. A Jacobian spectral analysis identifies a \emph{reflective regime} in which reentry induces stable oscillatory trajectories rather than divergence or collapse. Unlike continuous-time recurrent neural networks or liquid neural networks, FHRN achieves stability through population-level gain modulation rather than fixed recurrence or neuron-local time adaptation. These results establish the reentry network as a distinct class of self-referential neural dynamics supporting recursive yet bounded computation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Tame Your LLM: Semantic Collapse in Continuous Systems</title>
<link>https://arxiv.org/abs/2512.05162</link>
<guid>https://arxiv.org/abs/2512.05162</guid>
<content:encoded><![CDATA[
arXiv:2512.05162v1 Announce Type: cross 
Abstract: We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,\mu) \to L^2(M,\mu)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05198</link>
<guid>https://arxiv.org/abs/2512.05198</guid>
<content:encoded><![CDATA[
arXiv:2512.05198v1 Announce Type: cross 
Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem</title>
<link>https://arxiv.org/abs/2512.05207</link>
<guid>https://arxiv.org/abs/2512.05207</guid>
<content:encoded><![CDATA[
arXiv:2512.05207v1 Announce Type: cross 
Abstract: Virtual Network Embedding (VNE) is a key enabler of network slicing, yet most formulations assume that each Virtual Network Request (VNR) has a fixed topology. Recently, VNE with Alternative topologies (VNEAP) was introduced to capture malleable VNRs, where each request can be instantiated using one of several functionally equivalent topologies that trade resources differently. While this flexibility enlarges the feasible space, it also introduces an additional decision layer, making dynamic embedding more challenging. This paper proposes HRL-VNEAP, a hierarchical reinforcement learning approach for VNEAP under dynamic arrivals. A high-level policy selects the most suitable alternative topology (or rejects the request), and a low-level policy embeds the chosen topology onto the substrate network. Experiments on realistic substrate topologies under multiple traffic loads show that naive exploitation strategies provide only modest gains, whereas HRL-VNEAP consistently achieves the best performance across all metrics. Compared to the strongest tested baselines, HRL-VNEAP improves acceptance ratio by up to \textbf{20.7\%}, total revenue by up to \textbf{36.2\%}, and revenue-over-cost by up to \textbf{22.1\%}. Finally, we benchmark against an MILP formulation on tractable instances to quantify the remaining gap to optimality and motivate future work on learning- and optimization-based VNEAP solutions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings</title>
<link>https://arxiv.org/abs/2512.05245</link>
<guid>https://arxiv.org/abs/2512.05245</guid>
<content:encoded><![CDATA[
arXiv:2512.05245v1 Announce Type: cross 
Abstract: Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow</title>
<link>https://arxiv.org/abs/2512.05251</link>
<guid>https://arxiv.org/abs/2512.05251</guid>
<content:encoded><![CDATA[
arXiv:2512.05251v1 Announce Type: cross 
Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2512.05267</link>
<guid>https://arxiv.org/abs/2512.05267</guid>
<content:encoded><![CDATA[
arXiv:2512.05267v1 Announce Type: cross 
Abstract: In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust forecast aggregation via additional queries</title>
<link>https://arxiv.org/abs/2512.05271</link>
<guid>https://arxiv.org/abs/2512.05271</guid>
<content:encoded><![CDATA[
arXiv:2512.05271v1 Announce Type: cross 
Abstract: We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022).
  In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the "order of reasoning" and number of agents relevant to a query is $\omega(\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification</title>
<link>https://arxiv.org/abs/2512.05288</link>
<guid>https://arxiv.org/abs/2512.05288</guid>
<content:encoded><![CDATA[
arXiv:2512.05288v1 Announce Type: cross 
Abstract: Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples</title>
<link>https://arxiv.org/abs/2512.05318</link>
<guid>https://arxiv.org/abs/2512.05318</guid>
<content:encoded><![CDATA[
arXiv:2512.05318v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning</title>
<link>https://arxiv.org/abs/2512.05325</link>
<guid>https://arxiv.org/abs/2512.05325</guid>
<content:encoded><![CDATA[
arXiv:2512.05325v1 Announce Type: cross 
Abstract: Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats</title>
<link>https://arxiv.org/abs/2512.05331</link>
<guid>https://arxiv.org/abs/2512.05331</guid>
<content:encoded><![CDATA[
arXiv:2512.05331v1 Announce Type: cross 
Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetric Linear Dynamical Systems are Learnable from Few Observations</title>
<link>https://arxiv.org/abs/2512.05337</link>
<guid>https://arxiv.org/abs/2512.05337</guid>
<content:encoded><![CDATA[
arXiv:2512.05337v1 Announce Type: cross 
Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability</title>
<link>https://arxiv.org/abs/2512.05361</link>
<guid>https://arxiv.org/abs/2512.05361</guid>
<content:encoded><![CDATA[
arXiv:2512.05361v1 Announce Type: cross 
Abstract: We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoolNet: Deep Learning for 2D to 3D Video Process Validation</title>
<link>https://arxiv.org/abs/2512.05362</link>
<guid>https://arxiv.org/abs/2512.05362</guid>
<content:encoded><![CDATA[
arXiv:2512.05362v1 Announce Type: cross 
Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moving object detection from multi-depth images with an attention-enhanced CNN</title>
<link>https://arxiv.org/abs/2512.05415</link>
<guid>https://arxiv.org/abs/2512.05415</guid>
<content:encoded><![CDATA[
arXiv:2512.05415v1 Announce Type: cross 
Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering</title>
<link>https://arxiv.org/abs/2512.05430</link>
<guid>https://arxiv.org/abs/2512.05430</guid>
<content:encoded><![CDATA[
arXiv:2512.05430v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXR: An Interactive Immersive EHR Visualization in Extended Reality</title>
<link>https://arxiv.org/abs/2512.05438</link>
<guid>https://arxiv.org/abs/2512.05438</guid>
<content:encoded><![CDATA[
arXiv:2512.05438v1 Announce Type: cross 
Abstract: This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data</title>
<link>https://arxiv.org/abs/2512.05456</link>
<guid>https://arxiv.org/abs/2512.05456</guid>
<content:encoded><![CDATA[
arXiv:2512.05456v1 Announce Type: cross 
Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Gateway: Model Management Platform for Model-Driven Drug Discovery</title>
<link>https://arxiv.org/abs/2512.05462</link>
<guid>https://arxiv.org/abs/2512.05462</guid>
<content:encoded><![CDATA[
arXiv:2512.05462v1 Announce Type: cross 
Abstract: This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSDLabeler: Realistic semi-synthetic data generation for multi-label artifact classification in EEG</title>
<link>https://arxiv.org/abs/2512.05500</link>
<guid>https://arxiv.org/abs/2512.05500</guid>
<content:encoded><![CDATA[
arXiv:2512.05500v1 Announce Type: cross 
Abstract: EEG recordings are inherently contaminated by artifacts such as ocular, muscular, and environmental noise, which obscure neural activity and complicate preprocessing. Artifact classification offers advantages in stability and transparency, providing a viable alternative to ICA-based methods that enable flexible use alongside human inspections and across various applications. However, artifact classification is limited by its training data as it requires extensive manual labeling, which cannot fully cover the diversity of real-world EEG. Semi-synthetic data (SSD) methods have been proposed to address this limitation, but prior approaches typically injected single artifact types using ICA components or required separately recorded artifact signals, reducing both the realism of the generated data and the applicability of the method. To overcome these issues, we introduce SSDLabeler, a framework that generates realistic, annotated SSDs by decomposing real EEG with ICA, epoch-level artifact verification using RMS and PSD criteria, and reinjecting multiple artifact types into clean data. When applied to train a multi-label artifact classifier, it improved accuracy on raw EEG across diverse conditions compared to prior SSD and raw EEG training, establishing a scalable foundation for artifact handling that captures the co-occurrence and complexity of real EEG.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction</title>
<link>https://arxiv.org/abs/2512.05508</link>
<guid>https://arxiv.org/abs/2512.05508</guid>
<content:encoded><![CDATA[
arXiv:2512.05508v1 Announce Type: cross 
Abstract: Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.05515</link>
<guid>https://arxiv.org/abs/2512.05515</guid>
<content:encoded><![CDATA[
arXiv:2512.05515v1 Announce Type: cross 
Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement</title>
<link>https://arxiv.org/abs/2512.05525</link>
<guid>https://arxiv.org/abs/2512.05525</guid>
<content:encoded><![CDATA[
arXiv:2512.05525v1 Announce Type: cross 
Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening</title>
<link>https://arxiv.org/abs/2512.05528</link>
<guid>https://arxiv.org/abs/2512.05528</guid>
<content:encoded><![CDATA[
arXiv:2512.05528v1 Announce Type: cross 
Abstract: Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches</title>
<link>https://arxiv.org/abs/2512.05611</link>
<guid>https://arxiv.org/abs/2512.05611</guid>
<content:encoded><![CDATA[
arXiv:2512.05611v1 Announce Type: cross 
Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure \mu, we formalize \mu-coverage for central intervals and \mu-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-the-Air Semantic Alignment with Stacked Intelligent Metasurfaces</title>
<link>https://arxiv.org/abs/2512.05657</link>
<guid>https://arxiv.org/abs/2512.05657</guid>
<content:encoded><![CDATA[
arXiv:2512.05657v1 Announce Type: cross 
Abstract: Semantic communication systems aim to transmit task-relevant information between devices capable of artificial intelligence, but their performance can degrade when heterogeneous transmitter-receiver models produce misaligned latent representations. Existing semantic alignment methods typically rely on additional digital processing at the transmitter or receiver, increasing overall device complexity. In this work, we introduce the first over-the-air semantic alignment framework based on stacked intelligent metasurfaces (SIM), which enables latent-space alignment directly in the wave domain, reducing substantially the computational burden at the device level. We model SIMs as trainable linear operators capable of emulating both supervised linear aligners and zero-shot Parseval-frame-based equalizers. To realize these operators physically, we develop a gradient-based optimization procedure that tailors the metasurface transfer function to a desired semantic mapping. Experiments with heterogeneous vision transformer (ViT) encoders show that SIMs can accurately reproduce both supervised and zero-shot semantic equalizers, achieving up to 90% task accuracy in regimes with high signal-to-noise ratio (SNR), while maintaining strong robustness even at low SNR values.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem</title>
<link>https://arxiv.org/abs/2512.05672</link>
<guid>https://arxiv.org/abs/2512.05672</guid>
<content:encoded><![CDATA[
arXiv:2512.05672v1 Announce Type: cross 
Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing the latent features of universal machine-learning interatomic potentials</title>
<link>https://arxiv.org/abs/2512.05717</link>
<guid>https://arxiv.org/abs/2512.05717</guid>
<content:encoded><![CDATA[
arXiv:2512.05717v1 Announce Type: cross 
Abstract: The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors as metrics, and observing how the trends are affected by the choice of training set and training protocol. We find that the uMLIPs encode chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books</title>
<link>https://arxiv.org/abs/2512.05734</link>
<guid>https://arxiv.org/abs/2512.05734</guid>
<content:encoded><![CDATA[
arXiv:2512.05734v1 Announce Type: cross 
Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05753</link>
<guid>https://arxiv.org/abs/2512.05753</guid>
<content:encoded><![CDATA[
arXiv:2512.05753v1 Announce Type: cross 
Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics</title>
<link>https://arxiv.org/abs/2512.05765</link>
<guid>https://arxiv.org/abs/2512.05765</guid>
<content:encoded><![CDATA[
arXiv:2512.05765v1 Announce Type: cross 
Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth</title>
<link>https://arxiv.org/abs/2512.05783</link>
<guid>https://arxiv.org/abs/2512.05783</guid>
<content:encoded><![CDATA[
arXiv:2512.05783v1 Announce Type: cross 
Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine-learning-enabled interpretation of tribological deformation patterns in large-scale MD data</title>
<link>https://arxiv.org/abs/2512.05818</link>
<guid>https://arxiv.org/abs/2512.05818</guid>
<content:encoded><![CDATA[
arXiv:2512.05818v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) simulations have become indispensable for exploring tribological deformation patterns at the atomic scale. However, transforming the resulting high-dimensional data into interpretable deformation pattern maps remains a resource-intensive and largely manual process. In this work, we introduce a data-driven workflow that automates this interpretation step using unsupervised and supervised learning. Grain-orientation-colored computational tomograph pictures obtained from CuNi alloy simulations were first compressed through an autoencoder to a 32-dimensional global feature vector. Despite this strong compression, the reconstructed images retained the essential microstructural motifs: grain boundaries, stacking faults, twins, and partial lattice rotations, while omitting only the finest defects. The learned representations were then combined with simulation metadata (composition, load, time, temperature, and spatial position) to train a CNN-MLP model to predict the dominant deformation pattern. The resulting model achieves a prediction accuracy of approximately 96% on validation data. A refined evaluation strategy, in which an entire spatial region containing distinct grains was excluded from training, provides a more robust measure of generalization. The approach demonstrates that essential tribological deformation signatures can be automatically identified and classified from structural images using Machine Learning. This proof of concept constitutes a first step towards fully automated, data-driven construction of tribological mechanism maps and, ultimately, toward predictive modeling frameworks that may reduce the need for large-scale MD simulation campaigns.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models</title>
<link>https://arxiv.org/abs/2512.05887</link>
<guid>https://arxiv.org/abs/2512.05887</guid>
<content:encoded><![CDATA[
arXiv:2512.05887v1 Announce Type: cross 
Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction</title>
<link>https://arxiv.org/abs/2512.05920</link>
<guid>https://arxiv.org/abs/2512.05920</guid>
<content:encoded><![CDATA[
arXiv:2512.05920v1 Announce Type: cross 
Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BalLOT: Balanced $k$-means clustering with optimal transport</title>
<link>https://arxiv.org/abs/2512.05926</link>
<guid>https://arxiv.org/abs/2512.05926</guid>
<content:encoded><![CDATA[
arXiv:2512.05926v1 Announce Type: cross 
Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing an Optimal Sensor Network via Minimizing Information Loss</title>
<link>https://arxiv.org/abs/2512.05940</link>
<guid>https://arxiv.org/abs/2512.05940</guid>
<content:encoded><![CDATA[
arXiv:2512.05940v1 Announce Type: cross 
Abstract: Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consequences of Kernel Regularity for Bandit Optimization</title>
<link>https://arxiv.org/abs/2512.05957</link>
<guid>https://arxiv.org/abs/2512.05957</guid>
<content:encoded><![CDATA[
arXiv:2512.05957v1 Announce Type: cross 
Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Mat\'ern, square-exponential, rational-quadratic, $\gamma$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish H\"older space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</title>
<link>https://arxiv.org/abs/2512.05967</link>
<guid>https://arxiv.org/abs/2512.05967</guid>
<content:encoded><![CDATA[
arXiv:2512.05967v1 Announce Type: cross 
Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical Guarantees for Approximate Stationary Points of Shallow Neural Networks</title>
<link>https://arxiv.org/abs/2205.04491</link>
<guid>https://arxiv.org/abs/2205.04491</guid>
<content:encoded><![CDATA[
arXiv:2205.04491v2 Announce Type: replace 
Abstract: Since statistical guarantees for neural networks are usually restricted to global optima of intricate objective functions, it is unclear whether these theories explain the performances of actual outputs of neural network pipelines. The goal of this paper is, therefore, to bring statistical theory closer to practice. We develop statistical guarantees for shallow linear neural networks that coincide up to logarithmic factors with the global optima but apply to stationary points and the points nearby. These results support the common notion that neural networks do not necessarily need to be optimized globally from a mathematical perspective. We then extend our statistical guarantees to shallow ReLU neural networks, assuming the first layer weight matrices are nearly identical for the stationary network and the target. More generally, despite being limited to shallow neural networks for now, our theories make an important step forward in describing the practical properties of neural networks in mathematical terms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling</title>
<link>https://arxiv.org/abs/2408.06710</link>
<guid>https://arxiv.org/abs/2408.06710</guid>
<content:encoded><![CDATA[
arXiv:2408.06710v2 Announce Type: replace 
Abstract: Gaussian Process Latent Variable Models (GPLVMs) have become increasingly popular for unsupervised tasks such as dimensionality reduction and missing data recovery due to their flexibility and non-linear nature. An importance-weighted version of the Bayesian GPLVMs has been proposed to obtain a tighter variational bound. However, this version of the approach is primarily limited to analyzing simple data structures, as the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets. In this work, we propose an Annealed Importance Sampling (AIS) approach to address these issues. By transforming the posterior into a sequence of intermediate distributions using annealing, we combine the strengths of Sequential Monte Carlo samplers and VI to explore a wider range of posterior distributions and gradually approach the target distribution. We further propose an efficient algorithm by reparameterizing all variables in the evidence lower bound (ELBO). Experimental results on both toy and image datasets demonstrate that our method outperforms state-of-the-art methods in terms of tighter variational bounds, higher log-likelihoods, and more robust convergence.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting the Future: All-at-Once Event Sequence Forecasting with Horizon Matching</title>
<link>https://arxiv.org/abs/2408.13131</link>
<guid>https://arxiv.org/abs/2408.13131</guid>
<content:encoded><![CDATA[
arXiv:2408.13131v3 Announce Type: replace 
Abstract: Long-horizon events forecasting is a crucial task across various domains, including retail, finance, healthcare, and social networks. Traditional models for event sequences often extend to forecasting on a horizon using an autoregressive (recursive) multi-step strategy, which has limited effectiveness due to typical convergence to constant or repetitive outputs. To address this limitation, we introduce DEF, a novel approach for simultaneous forecasting of multiple future events on a horizon with high accuracy and diversity. Our method optimally aligns predictions with ground truth events during training by using a novel matching-based loss function. We establish a new state-of-the-art in long-horizon event prediction, achieving up to a 50% relative improvement over existing temporal point processes and event prediction models. Furthermore, we achieve state-of-the-art performance in next-event prediction tasks while demonstrating high computational efficiency during inference.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARTAN: A Sparse Transformer World Model Attending to What Matters</title>
<link>https://arxiv.org/abs/2411.06890</link>
<guid>https://arxiv.org/abs/2411.06890</guid>
<content:encoded><![CDATA[
arXiv:2411.06890v3 Announce Type: replace 
Abstract: Capturing the interactions between entities in a structured way plays a central role in world models that flexibly adapt to changes in the environment. Recent works motivate the benefits of models that explicitly represent the structure of interactions and formulate the problem as discovering local causal structures. In this work, we demonstrate that reliably capturing these relationships in complex settings remains challenging. To remedy this shortcoming, we postulate that sparsity is a critical ingredient for the discovery of such local structures. To this end, we present the SPARse TrANsformer World model (SPARTAN), a Transformer-based world model that learns context-dependent interaction structures between entities in a scene. By applying sparsity regularisation on the attention patterns between object-factored tokens, SPARTAN learns sparse, context-dependent interaction graphs that accurately predict future object states. We further extend our model to adapt to sparse interventions with unknown targets in the dynamics of the environment. This results in a highly interpretable world model that can efficiently adapt to changes. Empirically, we evaluate SPARTAN against the current state-of-the-art in object-centric world models in observation-based environments and demonstrate that our model can learn local causal graphs that accurately reflect the underlying interactions between objects, achieving significantly improved few-shot adaptation to dynamics changes, as well as robustness against distractors.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Control Perspective on Training PINNs</title>
<link>https://arxiv.org/abs/2501.18582</link>
<guid>https://arxiv.org/abs/2501.18582</guid>
<content:encoded><![CDATA[
arXiv:2501.18582v3 Announce Type: replace 
Abstract: We investigate the training of Physics-Informed Neural Networks (PINNs) from a control-theoretic perspective. Using gradient descent with resampling, we interpret the training dynamics as asymptotically equivalent to a stochastic control-affine system, where sampling effects act as process disturbances and measurement noise. Within this framework, we introduce two controllers for dynamically adapting the physics weight: an integral controller and a leaky integral controller. We theoretically analyze their asymptotic properties under the accuracy-robustness trade-off, and we evaluate them on a toy example. Numerical evidence suggests that the integral controller achieves accurate and robust convergence when the physical model is correct, whereas the leaky integrator provides improved performance in the presence of model mismatch. This work represents a first step toward convergence guarantees and principled training algorithms tailored to the distinct characteristics of PINN tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data</title>
<link>https://arxiv.org/abs/2502.04664</link>
<guid>https://arxiv.org/abs/2502.04664</guid>
<content:encoded><![CDATA[
arXiv:2502.04664v4 Announce Type: replace 
Abstract: Different gradient-based methods for optimizing overparameterized models can all achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. We provide the first complete characterization of implicit optimization bias for p-norm normalized steepest descent (NSD) and momentum steepest descent (NMD) algorithms in multi-class linear classification with cross-entropy loss. Our key theoretical contribution is proving that these algorithms converge to solutions maximizing the margin with respect to the classifier matrix's p-norm, with established convergence rates. These results encompass important special cases including Spectral Descent and Muon, which we show converge to max-margin solutions with respect to the spectral norm. A key insight of our contribution is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD/NMD with max-norm by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. For the specific case of descent with respect to the max-norm, we further extend our analysis to include preconditioning, showing that Adam converges to the matrix's max-norm solution. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent framework for studying implicit biases of matrix-parameter optimization algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergraph Foundation Model</title>
<link>https://arxiv.org/abs/2503.01203</link>
<guid>https://arxiv.org/abs/2503.01203</guid>
<content:encoded><![CDATA[
arXiv:2503.01203v2 Announce Type: replace 
Abstract: Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interval Regression: A Comparative Study with Proposed Models</title>
<link>https://arxiv.org/abs/2503.02011</link>
<guid>https://arxiv.org/abs/2503.02011</guid>
<content:encoded><![CDATA[
arXiv:2503.02011v2 Announce Type: replace 
Abstract: Regression models are essential for a wide range of real-world applications. However, in practice, target values are not always precisely known; instead, they may be represented as intervals of acceptable values. This challenge has led to the development of Interval Regression models. In this study, we provide a comprehensive review of existing Interval Regression models and introduce alternative models for comparative analysis. Experiments are conducted on both real-world and synthetic datasets to offer a broad perspective on model performance. The results demonstrate that no single model is universally optimal, highlighting the importance of selecting the most suitable model for each specific scenario.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Text Classification via Transferable Representations</title>
<link>https://arxiv.org/abs/2503.07691</link>
<guid>https://arxiv.org/abs/2503.07691</guid>
<content:encoded><![CDATA[
arXiv:2503.07691v2 Announce Type: replace 
Abstract: Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g., women and men) remains an open challenge. We propose an approach that extends the use of the Wasserstein Dependency Measure for learning unbiased neural text classifiers. Given the challenge of distinguishing fair from unfair information in a text encoder, we draw inspiration from adversarial training by inducing independence between representations learned for the target label and those for a sensitive attribute. We further show that Domain Adaptation can be efficiently leveraged to remove the need for access to the sensitive attributes in the dataset we cure. We provide both theoretical and empirical evidence that our approach is well-founded.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation</title>
<link>https://arxiv.org/abs/2503.14572</link>
<guid>https://arxiv.org/abs/2503.14572</guid>
<content:encoded><![CDATA[
arXiv:2503.14572v3 Announce Type: replace 
Abstract: The capacity of foundation models allows for their application to new, unseen tasks. The adaptation to such tasks is called transfer learning. An efficient transfer learning method that circumvents parameter optimization is imprinting. The conceptual differences between studies on imprinting form the basis of our systematic investigation. In this work, we propose the general \texttt{IMPRINT} framework, identifying three main components: generation, normalization, and aggregation. Through the lens of this framework, we conduct an in-depth analysis and a comparison of the existing methods. Our findings reveal the benefits of representing novel data with multiple proxies in the generation step and show the importance of proper normalization. Beyond an extensive analytical grounding, our framework enables us to propose a novel variant of imprinting which outperforms previous work on transfer learning tasks by 4\%. This variant determines proxies through clustering motivated by the neural collapse phenomenon -- a connection that we draw for the first time. We publicly release our code at https://github.com/DATEXIS/IMPRINT.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trustworthy By Design Classification Model for Building Energy Retrofit Decision Support</title>
<link>https://arxiv.org/abs/2504.06055</link>
<guid>https://arxiv.org/abs/2504.06055</guid>
<content:encoded><![CDATA[
arXiv:2504.06055v2 Announce Type: replace 
Abstract: Improving energy efficiency in residential buildings is critical to combating climate change and reducing greenhouse gas emissions. Retrofitting existing buildings, which contribute a significant share of energy use, is therefore a key priority, especially in regions with outdated building stock. Artificial Intelligence (AI) and Machine Learning (ML) can automate retrofit decision-making and find retrofit strategies. However, their use faces challenges of data availability, model transparency, and compliance with national and EU AI regulations including the AI act, ethics guidelines and the ALTAI. This paper presents a trustworthy-by-design ML-based decision support framework that recommends energy efficiency strategies for residential buildings using minimal user-accessible inputs. The framework merges Conditional Tabular Generative Adversarial Networks (CTGAN) to augment limited and imbalanced data with a neural network-based multi-label classifier that predicts potential combinations of retrofit actions. To support explanation and trustworthiness, an Explainable AI (XAI) layer using SHapley Additive exPlanations (SHAP) clarifies the rationale behind recommendations and guides feature engineering. Two case studies validate performance and generalization: the first leveraging a well-established, large EPC dataset for England and Wales; the second using a small, imbalanced post-retrofit dataset from Latvia (RETROFIT-LAT). Results show that the framework can handle diverse data conditions and improve performance up to 53% compared to the baseline. Overall, the proposed framework provides a feasible, interpretable, and trustworthy AI system for building retrofit decision support through assured performance, usability, and transparency to aid stakeholders in prioritizing effective energy investments and support regulation-compliant, data-driven innovation in sustainable energy transition.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Conformal Prediction Under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2505.15721</link>
<guid>https://arxiv.org/abs/2505.15721</guid>
<content:encoded><![CDATA[
arXiv:2505.15721v2 Announce Type: replace 
Abstract: Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IF-GUIDE: Influence Function-Guided Detoxification of LLMs</title>
<link>https://arxiv.org/abs/2506.01790</link>
<guid>https://arxiv.org/abs/2506.01790</guid>
<content:encoded><![CDATA[
arXiv:2506.01790v3 Announce Type: replace 
Abstract: We study how training data contributes to the emergence of toxic behaviors in large language models. Most prior work on reducing model toxicity adopts reactive approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a proactive approach, IF-GUIDE, that leverages influence functions to identify and suppress harmful tokens in the training data. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-GUIDE does not rely on human-preference data, which is typically required by existing alignment methods. In our evaluation, we demonstrate that IF-GUIDE substantially reduces both explicit and implicit toxicity-by up to 10$\times$ compared to uncensored models, and up to 3$\times$ compared to baseline alignment methods such as DPO and RAD-across both pre-training and fine-tuning scenarios. IF-GUIDE is computationally efficient: a billion-parameter model is not necessary for computing influence scores; a million-parameter model-with 7.5$\times$ fewer parameters-can effectively serve as a proxy for identifying harmful data. Our code is publicly available at: https://github.com/ztcoalson/IF-Guide
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.07413</link>
<guid>https://arxiv.org/abs/2506.07413</guid>
<content:encoded><![CDATA[
arXiv:2506.07413v3 Announce Type: replace 
Abstract: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v3 Announce Type: replace 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance</title>
<link>https://arxiv.org/abs/2506.15305</link>
<guid>https://arxiv.org/abs/2506.15305</guid>
<content:encoded><![CDATA[
arXiv:2506.15305v3 Announce Type: replace 
Abstract: The rapid expansion of cross-border e-commerce (CBEC) has created significant opportunities for small- and medium-sized sellers, yet financing remains a critical challenge due to their limited credit histories. Third-party logistics (3PL)-led supply chain finance (SCF) has emerged as a promising solution, leveraging in-transit inventory as collateral. We propose an advanced credit risk management framework tailored for 3PL-led SCF, addressing the dual challenges of credit risk assessment and loan size determination. Specifically, we leverage conditional generative modeling of sales distributions through Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for risk measures estimation. We propose a unified framework that enables flexible estimation of multiple risk measures while introducing a functional risk measure formulation that systematically captures the relationship between these risk measures and varying loan levels, supported by theoretical guarantees. To capture complex covariate interactions in e-commerce sales data, we integrate QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on synthetic and real-world data validate the efficacy of our model for credit risk assessment and loan size determination. This study explores the use of generative models in CBEC SCF risk management, illustrating their potential to strengthen credit assessment and support financing for small- and medium-sized sellers.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Validity for Distance Functions and Internal Clustering Validity Indices in Correlation Space</title>
<link>https://arxiv.org/abs/2507.16497</link>
<guid>https://arxiv.org/abs/2507.16497</guid>
<content:encoded><![CDATA[
arXiv:2507.16497v2 Announce Type: replace 
Abstract: Internal clustering validity indices (ICVIs) assess clustering quality without ground truth labels. Comparative studies consistently find that no single ICVI outperforms others across datasets, leaving practitioners without principled ICVI selection. We argue that inconsistent ICVI performance arises because studies evaluate them based on matching human labels rather than measuring the quality of the discovered structure in the data, using datasets without formally quantifying the structure type and quality. Structure type refers to the mathematical organisation in data that clustering aims to discover. Validity theory requires a theoretical definition of clustering quality, which depends on structure type. We demonstrate this through the first validity assessment of clustering quality measures for correlation patterns, a structure type that arises from clustering time series by correlation relationships. We formalise 23 canonical correlation patterns as the theoretical optimal clustering and use synthetic data modelling this structure with controlled perturbations to evaluate validity across content, criterion, construct, and external validity. Our findings show that Silhouette Width Criterion (SWC) and Davies-Bouldin Index (DBI) are valid for correlation patterns, whilst Calinski-Harabasz (VRC) and Pakhira-Bandyopadhyay-Maulik (PBM) indices fail. Simple Lp norm distances achieve validity, whilst correlation-specific functions fail structural, criterion, and external validity. These results differ from previous studies where VRC and PBM performed well, demonstrating that validity depends on structure type. Our structure-type-specific validation method provides both practical guidance (quality thresholds SWC>0.9, DBI<0.15) and a methodological template for establishing validity for other structure types.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
arXiv:2508.04946v3 Announce Type: replace 
Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis</title>
<link>https://arxiv.org/abs/2508.10967</link>
<guid>https://arxiv.org/abs/2508.10967</guid>
<content:encoded><![CDATA[
arXiv:2508.10967v2 Announce Type: replace 
Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a given product molecule, which is a fundamental task in chemical synthesis. However, existing models rely on static pattern-matching paradigm, which limits their ability to perform effective logic decision-making, leading to black-box decision-making. Building on this, we propose Retro-Expert, an interpretable retrosynthesis framework that performs collaborative reasoning by combining the complementary reasoning strengths of Large Language Models and specialized models via reinforcement learning. It outputs natural language explanations grounded in chemical logic through three components: (1) specialized models analyze the product to construct high-quality chemical decision space, (2) LLM-driven critical reasoning to generate predictions and corresponding interpretable reasoning path, and (3) reinforcement learning optimizing interpretable decision policy. Experiments show that Retro-Expert not only surpasses both LLM-based and specialized models across different metrics but also provides expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.16560</link>
<guid>https://arxiv.org/abs/2508.16560</guid>
<content:encoded><![CDATA[
arXiv:2508.16560v3 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPA: An Information-Reconstructive Input Projection Framework for Efficient Foundation Model Adaptation</title>
<link>https://arxiv.org/abs/2509.04398</link>
<guid>https://arxiv.org/abs/2509.04398</guid>
<content:encoded><![CDATA[
arXiv:2509.04398v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly aims to reconstruct the original input within a reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen. Code available at https://github.com/valeoai/peft-ipa .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</title>
<link>https://arxiv.org/abs/2509.10809</link>
<guid>https://arxiv.org/abs/2509.10809</guid>
<content:encoded><![CDATA[
arXiv:2509.10809v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&amp;P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model's native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.16625</link>
<guid>https://arxiv.org/abs/2509.16625</guid>
<content:encoded><![CDATA[
arXiv:2509.16625v4 Announce Type: replace 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization</title>
<link>https://arxiv.org/abs/2509.22161</link>
<guid>https://arxiv.org/abs/2509.22161</guid>
<content:encoded><![CDATA[
arXiv:2509.22161v2 Announce Type: replace 
Abstract: Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a codebook), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation. Smoothed vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires two properties: (1) smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and (2) all codebook entries should be utilized, preventing code collapse. Existing methods typically address these desiderata separately. By contrast, the present study introduces a simple and intuitive regularization that promotes both simultaneously by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks, including discrete image autoencoding and contrastive speech representation learning, demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-marginal temporal Schr\"odinger Bridge Matching from unpaired data</title>
<link>https://arxiv.org/abs/2510.01894</link>
<guid>https://arxiv.org/abs/2510.01894</guid>
<content:encoded><![CDATA[
arXiv:2510.01894v2 Announce Type: replace 
Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose Multi-Marginal temporal Schr\"odinger Bridge Matching (MMtSBM) from unpaired data, extending the theoretical guarantees and empirical efficiency of Diffusion Schr\"odinger Bridge Matching (arXiv:2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real-world datasets such as transcriptomic trajectory inference in 100 dimensions, and, for the first time, recovers couplings and dynamics in very high-dimensional image settings. Our work establishes multi-marginal Schr\"odinger bridges as a practical and principled approach for recovering hidden dynamics from static data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives</title>
<link>https://arxiv.org/abs/2510.04996</link>
<guid>https://arxiv.org/abs/2510.04996</guid>
<content:encoded><![CDATA[
arXiv:2510.04996v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference</title>
<link>https://arxiv.org/abs/2510.09665</link>
<guid>https://arxiv.org/abs/2510.09665</guid>
<content:encoded><![CDATA[
arXiv:2510.09665v2 Announce Type: replace 
Abstract: KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</title>
<link>https://arxiv.org/abs/2510.11062</link>
<guid>https://arxiv.org/abs/2510.11062</guid>
<content:encoded><![CDATA[
arXiv:2510.11062v3 Announce Type: replace 
Abstract: Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.
  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Augmented Deep Learning for Downhole Depth Sensing and Field Validation</title>
<link>https://arxiv.org/abs/2511.00129</link>
<guid>https://arxiv.org/abs/2511.00129</guid>
<content:encoded><![CDATA[
arXiv:2511.00129v2 Announce Type: replace 
Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling</title>
<link>https://arxiv.org/abs/2511.05811</link>
<guid>https://arxiv.org/abs/2511.05811</guid>
<content:encoded><![CDATA[
arXiv:2511.05811v2 Announce Type: replace 
Abstract: Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis</title>
<link>https://arxiv.org/abs/2511.08887</link>
<guid>https://arxiv.org/abs/2511.08887</guid>
<content:encoded><![CDATA[
arXiv:2511.08887v3 Announce Type: replace 
Abstract: Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. In this work we propose FAST-CAD, a theoretically grounded framework that combines domain-adversarial training (DAT) with group distributionally robust optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. Our approach is built on domain adaptation and minimax fairness theory and provides convergence guarantees and fairness bounds. We curate a multimodal dataset covering 12 demographic subgroups defined by age, gender, and posture. FAST-CAD employs self-supervised encoders with adversarial domain discrimination to learn demographic-invariant representations, while Group-DRO optimizes worst-group risk to ensure robust performance across all subgroups. Extensive experiments show that our method achieves superior diagnostic performance while maintaining fairness across demographic groups, and our theoretical analysis supports the effectiveness of the unified DAT + Group-DRO framework. This work provides both practical advances and theoretical insights for fair medical AI systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks</title>
<link>https://arxiv.org/abs/2511.09114</link>
<guid>https://arxiv.org/abs/2511.09114</guid>
<content:encoded><![CDATA[
arXiv:2511.09114v2 Announce Type: replace 
Abstract: Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>