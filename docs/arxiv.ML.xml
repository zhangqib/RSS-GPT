<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>A self-driving lab for solution-processed electrochromic thin films</title>
<link>https://arxiv.org/abs/2512.05989</link>
<guid>https://arxiv.org/abs/2512.05989</guid>
<content:encoded><![CDATA[
<div> Keywords: electrochromic materials, solution-processed, self-driving laboratories, Bayesian optimization, process optimization<br><br>Summary:<br><br>This study explores the advancement of solution-processed electrochromic materials, which are crucial for developing energy-efficient smart windows and displays. The performance of these materials is highly dependent on their composition and the conditions under which they are processed. Electrochromic thin film electrodes require coatings that are smooth and free of defects to achieve optimal contrast between their bleached and colored states. Traditional optimization of spin-coated electrochromic thin layers is complex and time-consuming, presenting challenges in rapid development. To address this, the researchers implement self-driving laboratories that integrate automation with machine learning techniques. This system automates data acquisition, image processing, spectral analysis, and employs Bayesian optimization to systematically and efficiently navigate the parameter space. The approach significantly enhances throughput and directs the search towards optimal processing parameters more effectively than conventional methods. Furthermore, the framework is flexible and applicable to a broad range of solution-processed materials beyond electrochromics. Overall, the work demonstrates the powerful role of self-driving labs in accelerating materials discovery and optimizing fabrication processes, potentially transforming how new materials are developed for energy-saving technologies. <div>
arXiv:2512.05989v1 Announce Type: new 
Abstract: Solution-processed electrochromic materials offer high potential for energy-efficient smart windows and displays. Their performance varies with material choice and processing conditions. Electrochromic thin film electrodes require a smooth, defect-free coating for optimal contrast between bleached and colored states. The complexity of optimizing the spin-coated electrochromic thin layer poses challenges for rapid development. This study demonstrates the use of self-driving laboratories to accelerate the development of electrochromic coatings by coupling automation with machine learning. Our system combines automated data acquisition, image processing, spectral analysis, and Bayesian optimization to explore processing parameters efficiently. This approach not only increases throughput but also enables a pointed search for optimal processing parameters. The approach can be applied to various solution-processed materials, highlighting the potential of self-driving labs in enhancing materials discovery and process optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Amortized Inference: A Topological Unification of Search, Closure, and Structure</title>
<link>https://arxiv.org/abs/2512.05990</link>
<guid>https://arxiv.org/abs/2512.05990</guid>
<content:encoded><![CDATA[
<div> Keywords: Memory-Amortized Inference, Homological Parity Principle, Topological Cycle Closure, Wake-Sleep algorithm, Cognitive Phase Transitions

<br><br>Summary:  
1. The paper introduces Memory-Amortized Inference (MAI), a theoretical framework combining learning and memory into a unified geometric substrate informed by algebraic topology.  
2. It proposes the Homological Parity Principle, distinguishing stable content (even-dimensional homology, \(H_{even}\)) from dynamic context (odd-dimensional homology, \(H_{odd}\)) in cognition.  
3. Cognition is modeled as a topological trinity transformation: Search (complex recursive searching), Closure (topological cycle closure), and Structure (stable knowledge encoding).  
4. The work connects complex NPSPACE search (via Savitch’s Theorem) to tractable P-time dynamic programming lookups, showing how cognitive systems convert slow, recursive reasoning into fast intuitive retrieval.  
5. The process parallels a generalized Wake-Sleep algorithm that alternates between optimizing dynamic inference flows (\(H_{odd}\), wake phase) and consolidating knowledge into stable structures (\(H_{even}\), sleep phase), explaining fast vs. slow thinking.  
6. The framework provides a rigorous, topological foundation for cognition and suggests new post-Turing computational architectures leveraging topological resonance for efficient inference and learning. <div>
arXiv:2512.05990v1 Announce Type: new 
Abstract: Contemporary ML separates the static structure of parameters from the dynamic flow of inference, yielding systems that lack the sample efficiency and thermodynamic frugality of biological cognition. In this theoretical work, we propose \textbf{Memory-Amortized Inference (MAI)}, a formal framework rooted in algebraic topology that unifies learning and memory as phase transitions of a single geometric substrate. Central to our theory is the \textbf{Homological Parity Principle}, which posits a fundamental dichotomy: even-dimensional homology ($H_{even}$) physically instantiates stable \textbf{Content} (stable scaffolds or ``what''), while odd-dimensional homology ($H_{odd}$) instantiates dynamic \textbf{Context} (dynamic flows or ``where''). We derive the logical flow of MAI as a topological trinity transformation: \textbf{Search $\to$ Closure $\to$ Structure}. Specifically, we demonstrate that cognition operates by converting high-complexity recursive search (modeled by \textit{Savitch's Theorem} in NPSPACE) into low-complexity lookup (modeled by \textit{Dynamic Programming} in P) via the mechanism of \textbf{Topological Cycle Closure}. We further show that this consolidation process is governed by a topological generalization of the Wake-Sleep algorithm, functioning as a coordinate descent that alternates between optimizing the $H_{odd}$ flow (inference/wake) and condensing persistent cycles into the $H_{even}$ scaffold (learning/sleep). This framework offers a rigorous explanation for the emergence of fast-thinking (intuition) from slow-thinking (reasoning) and provides a blueprint for post-Turing architectures that compute via topological resonance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning recognition and analysis of Volatile Organic Compounds based on experimental and synthetic infrared absorption spectra</title>
<link>https://arxiv.org/abs/2512.06059</link>
<guid>https://arxiv.org/abs/2512.06059</guid>
<content:encoded><![CDATA[
<div> Volatile Organic Compounds, Infrared Spectroscopy, Neural Networks, Data Augmentation, VOC Detection<br><br>Summary:<br><br>The article addresses the challenge of detecting volatile organic compounds (VOCs), which are harmful due to their ability to evaporate easily and impact human health. Infrared (IR) spectroscopy is used to detect VOCs at very low concentrations by analyzing their absorption spectra; however, the complexity of these spectra makes real-time recognition and quantification difficult. To overcome this, the authors collected an experimental dataset containing IR spectra of nine different VOC classes at various concentrations. Recognizing the limited size and diversity of this dataset, they employed conditional generative neural networks to produce synthetic spectra, augmenting the dataset and increasing its variety. This augmentation enabled the training of robust discriminative neural networks capable of accurately identifying the nine VOC classes and estimating their concentrations. The research demonstrates that such neural network models, trained on both real and synthetic data, can be effectively implemented in sensing devices for real-time VOC recognition and concentration analysis, potentially improving environmental monitoring and human health protection measures. <div>
arXiv:2512.06059v1 Announce Type: new 
Abstract: Volatile Organic Compounds (VOCs) are organic molecules that have low boiling points and therefore easily evaporate into the air. They pose significant risks to human health, making their accurate detection the crux of efforts to monitor and minimize exposure. Infrared (IR) spectroscopy enables the ultrasensitive detection at low-concentrations of VOCs in the atmosphere by measuring their IR absorption spectra. However, the complexity of the IR spectra limits the possibility to implement VOC recognition and quantification in real-time. While deep neural networks (NNs) are increasingly used for the recognition of complex data structures, they typically require massive datasets for the training phase. Here, we create an experimental VOC dataset for nine different classes of compounds at various concentrations, using their IR absorption spectra. To further increase the amount of spectra and their diversity in term of VOC concentration, we augment the experimental dataset with synthetic spectra created via conditional generative NNs. This allows us to train robust discriminative NNs, able to reliably identify the nine VOCs, as well as to precisely predict their concentrations. The trained NN is suitable to be incorporated into sensing devices for VOCs recognition and analysis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Privacy Isn't Synthetic: Hidden Data Leakage in Generative AI Models</title>
<link>https://arxiv.org/abs/2512.06062</link>
<guid>https://arxiv.org/abs/2512.06062</guid>
<content:encoded><![CDATA[
<div> Keywords: generative models, synthetic data, membership inference attack, data privacy, clustering  

<br><br>Summary:  
This paper investigates privacy risks in synthetic data generated by generative models, which are often used to share data while preserving privacy. The authors reveal that synthetic data can leak information about the original training data due to structural overlaps in the data manifold. They introduce a novel black-box membership inference attack that does not require access to the model's internal parameters or real data but relies on repeatedly querying the generative model to produce numerous synthetic samples. By applying unsupervised clustering, the attack identifies dense regions and analyzes cluster medoids and neighborhoods, which align with high-density regions where real training data resides. These clusters act as proxies to infer membership or reconstruct approximate records of the original data subjects. The experimental evaluation spans sensitive areas including healthcare and finance, demonstrating that membership leakage occurs even when differential privacy or noise-based protections are applied during model training. This exposes a subtle but critical attack vector focusing on distributional neighborhood inference rather than direct memorization of samples. The findings underscore the need for enhanced privacy guarantees in synthetic data generation pipelines. The authors also provide open-source implementation and evaluation code to promote further research on this emerging threat. <div>
arXiv:2512.06062v1 Announce Type: new 
Abstract: Generative models are increasingly used to produce privacy-preserving synthetic data as a safe alternative to sharing sensitive training datasets. However, we demonstrate that such synthetic releases can still leak information about the underlying training samples through structural overlap in the data manifold. We propose a black-box membership inference attack that exploits this vulnerability without requiring access to model internals or real data. The attacker repeatedly queries the generative model to obtain large numbers of synthetic samples, performs unsupervised clustering to identify dense regions of the synthetic distribution, and then analyzes cluster medoids and neighborhoods that correspond to high-density regions in the original training data. These neighborhoods act as proxies for training samples, enabling the adversary to infer membership or reconstruct approximate records. Our experiments across healthcare, finance, and other sensitive domains show that cluster overlap between real and synthetic data leads to measurable membership leakage-even when the generator is trained with differential privacy or other noise mechanisms. The results highlight an under-explored attack surface in synthetic data generation pipelines and call for stronger privacy guarantees that account for distributional neighborhood inference rather than sample-level memorization alone, underscoring its role in privacy-preserving data publishing. Implementation and evaluation code are publicly available at:github.com/Cluster-Medoid-Leakage-Attack.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaxWildfire: A GPU-Accelerated Wildfire Simulator for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06102</link>
<guid>https://arxiv.org/abs/2512.06102</guid>
<content:encoded><![CDATA[
<div> Artificial intelligence, reinforcement learning, wildfire management, JaxWildfire, GPU acceleration<br><br>Summary:<br><br>This paper addresses the challenge of using reinforcement learning (RL) to improve wildfire and natural hazard management, highlighting the critical bottleneck posed by the slow speed of existing wildfire simulators. The authors introduce JaxWildfire, a novel wildfire simulator that uses a probabilistic fire spread model based on cellular automata. Implemented in JAX, JaxWildfire leverages vectorized simulations via the vmap function, enabling high-throughput simulations on GPUs. This approach results in a dramatic 6 to 35 times speedup compared to existing wildfire simulation software. Additionally, JaxWildfire supports gradient-based optimization of simulator parameters, contributing to more efficient and precise model tuning. The paper demonstrates that JaxWildfire is suitable for training RL agents, which can learn effective wildfire suppression strategies within this accelerated simulation environment. Overall, this work represents an important advancement by addressing computational limitations and enabling the application of RL techniques to natural hazard management, particularly wildfire control, paving the way for improved decision-making and proactive interventions in complex, uncertain scenarios. <div>
arXiv:2512.06102v1 Announce Type: new 
Abstract: Artificial intelligence methods are increasingly being explored for managing wildfires and other natural hazards. In particular, reinforcement learning (RL) is a promising path towards improving outcomes in such uncertain decision-making scenarios and moving beyond reactive strategies. However, training RL agents requires many environment interactions, and the speed of existing wildfire simulators is a severely limiting factor. We introduce $\texttt{JaxWildfire}$, a simulator underpinned by a principled probabilistic fire spread model based on cellular automata. It is implemented in JAX and enables vectorized simulations using $\texttt{vmap}$, allowing high throughput of simulations on GPUs. We demonstrate that $\texttt{JaxWildfire}$ achieves 6-35x speedup over existing software and enables gradient-based optimization of simulator parameters. Furthermore, we show that $\texttt{JaxWildfire}$ can be used to train RL agents to learn wildfire suppression policies. Our work is an important step towards enabling the advancement of RL techniques for managing natural hazards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ARC-AGI Without Pretraining</title>
<link>https://arxiv.org/abs/2512.06104</link>
<guid>https://arxiv.org/abs/2512.06104</guid>
<content:encoded><![CDATA[
<div> Keywords: CompressARC, Minimum Description Length, ARC-AGI, zero pretraining, visual IQ puzzles<br><br>Summary:<br>1. The paper challenges the common notion that solving ARC-AGI-1 benchmark visual puzzles requires massive pretraining typically used in large language models (LLMs).<br>2. It introduces CompressARC, a novel 76,000-parameter model that operates without any pretraining and solves 20% of the ARC-AGI puzzles.<br>3. CompressARC uniquely solves puzzles by minimizing the description length (Minimum Description Length, MDL) of the target puzzle during inference, effectively training on the single test puzzle itself minus the solution.<br>4. This approach endows CompressARC with exceptional generalization abilities despite extremely limited data, a property not commonly observed in conventional deep learning methods.<br>5. The results suggest that MDL can be an alternative pathway to achieving intelligence beyond pretrained models, as CompressARC successfully solves diverse and creative visual IQ-style puzzles without relying on the provided ARC-AGI training set or extensive sample data. <div>
arXiv:2512.06104v1 Announce Type: new 
Abstract: Conventional wisdom in the age of LLMs dictates that solving IQ-test-like visual puzzles from the ARC-AGI-1 benchmark requires capabilities derived from massive pretraining. To counter this, we introduce CompressARC, a 76K parameter model without any pretraining that solves 20% of evaluation puzzles by minimizing the description length (MDL) of the target puzzle purely during inference time. The MDL endows CompressARC with extreme generalization abilities typically unheard of in deep learning. To our knowledge, CompressARC is the only deep learning method for ARC-AGI where training happens only on a single sample: the target inference puzzle itself, with the final solution information removed. Moreover, CompressARC does not train on the pre-provided ARC-AGI "training set". Under these extremely data-limited conditions, we do not ordinarily expect any puzzles to be solvable at all. Yet CompressARC still solves a diverse distribution of creative ARC-AGI puzzles, suggesting MDL to be an alternative feasible way to produce intelligence, besides conventional pretraining.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Prescriptive Framework for Determining Optimal Days for Short-Term Traffic Counts</title>
<link>https://arxiv.org/abs/2512.06111</link>
<guid>https://arxiv.org/abs/2512.06111</guid>
<content:encoded><![CDATA[
<div> Keywords: AADT, short-duration traffic counts, machine learning, traffic volume estimation, Texas DOT<br><br>Summary:<br><br>1. The Federal Highway Administration requires state Departments of Transportation to collect accurate Annual Average Daily Traffic (AADT) data, but many struggle with reliability, especially on unmonitored roads.<br>2. Continuous count stations provide precise traffic data but are costly and challenging to deploy widely, leading agencies to depend on short-duration traffic counts.<br>3. This study introduces a novel machine learning framework designed to identify the most optimal representative day(s) for short count data collection to enhance AADT prediction accuracy.<br>4. Using traffic volume data from Texas in 2022 and 2023, the proposed 'optimal day' approach iteratively selects informative days and is compared against a baseline representing common DOT practices without optimal day selection.<br>5. The study utilizes continuous count data to simulate 24-hour short counts and incorporates actual field short counts via a leave-one-out method for unbiased feature engineering across similar roads.<br>6. Results show the machine learning method outperforms the baseline, with the best day achieving significantly lower errors (RMSE: 7,871.15 vs. 11,185.00) and higher R² (0.9756 vs. 0.9499), indicating improved AADT estimation.<br>7. This approach offers an effective, lower-cost alternative for DOTs to improve traffic data accuracy, comply with monitoring standards, and reduce statewide traffic data collection expenses. <div>
arXiv:2512.06111v1 Announce Type: new 
Abstract: The Federal Highway Administration (FHWA) mandates that state Departments of Transportation (DOTs) collect reliable Annual Average Daily Traffic (AADT) data. However, many U.S. DOTs struggle to obtain accurate AADT, especially for unmonitored roads. While continuous count (CC) stations offer accurate traffic volume data, their implementation is expensive and difficult to deploy widely, compelling agencies to rely on short-duration traffic counts. This study proposes a machine learning framework, the first to our knowledge, to identify optimal representative days for conducting short count (SC) data collection to improve AADT prediction accuracy. Using 2022 and 2023 traffic volume data from the state of Texas, we compare two scenarios: an 'optimal day' approach that iteratively selects the most informative days for AADT estimation and a 'no optimal day' baseline reflecting current practice by most DOTs. To align with Texas DOT's traffic monitoring program, continuous count data were utilized to simulate the 24 hour short counts. The actual field short counts were used to enhance feature engineering through using a leave-one-out (LOO) technique to generate unbiased representative daily traffic features across similar road segments. Our proposed methodology outperforms the baseline across the top five days, with the best day (Day 186) achieving lower errors (RMSE: 7,871.15, MAE: 3,645.09, MAPE: 11.95%) and higher R^2 (0.9756) than the baseline (RMSE: 11,185.00, MAE: 5,118.57, MAPE: 14.42%, R^2: 0.9499). This research offers DOTs an alternative to conventional short-duration count practices, improving AADT estimation, supporting Highway Performance Monitoring System compliance, and reducing the operational costs of statewide traffic data collection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Koopman Machine for Interpretable Longitudinal Personalized Alzheimer's Disease Forecasting</title>
<link>https://arxiv.org/abs/2512.06134</link>
<guid>https://arxiv.org/abs/2512.06134</guid>
<content:encoded><![CDATA[
<div> Neural Koopman Machine, Alzheimer's disease, cognitive decline forecasting, multimodal data, hierarchical attention  

<br><br>Summary:  
This paper introduces the Neural Koopman Machine (NKM), a novel machine learning model designed to forecast individual cognitive decline in Alzheimer's disease (AD) by simultaneously predicting multiple cognitive scores. Inspired by dynamical systems and attention mechanisms, NKM integrates multimodal data including genetic, neuroimaging, proteomic, and demographic information. It uniquely incorporates analytical ($\alpha$) and biological ($\beta$) knowledge to guide feature grouping and control hierarchical attention, enabling the extraction of relevant patterns. Using a Fusion Group-Aware Hierarchical Attention mechanism within the Koopman operator framework, NKM converts complex nonlinear data trajectories into interpretable linear representations, enhancing model transparency. The model's efficacy was validated on the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, where it outperformed traditional machine learning and deep learning methods in predicting cognitive decline trajectories. NKM not only forecasts multiple cognitive scores at once but also quantifies the differential contributions of biomarkers to each cognitive outcome. Furthermore, it identifies brain regions most predictive of cognitive deterioration, facilitating biological interpretability. Overall, NKM represents an advancement in personalized and interpretable forecasting of AD progression by leveraging multimodal data within an explainable framework, shedding light on potential multimodal biological factors underlying Alzheimer's disease. <div>
arXiv:2512.06134v1 Announce Type: new 
Abstract: Early forecasting of individual cognitive decline in Alzheimer's disease (AD) is central to disease evaluation and management. Despite advances, it is as of yet challenging for existing methodological frameworks to integrate multimodal data for longitudinal personalized forecasting while maintaining interpretability. To address this gap, we present the Neural Koopman Machine (NKM), a new machine learning architecture inspired by dynamical systems and attention mechanisms, designed to forecast multiple cognitive scores simultaneously using multimodal genetic, neuroimaging, proteomic, and demographic data. NKM integrates analytical ($\alpha$) and biological ($\beta$) knowledge to guide feature grouping and control the hierarchical attention mechanisms to extract relevant patterns. By implementing Fusion Group-Aware Hierarchical Attention within the Koopman operator framework, NKM transforms complex nonlinear trajectories into interpretable linear representations. To demonstrate NKM's efficacy, we applied it to study the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset. Our results suggest that NKM consistently outperforms both traditional machine learning methods and deep learning models in forecasting trajectories of cognitive decline. Specifically, NKM (1) forecasts changes of multiple cognitive scores simultaneously, (2) quantifies differential biomarker contributions to predicting distinctive cognitive scores, and (3) identifies brain regions most predictive of cognitive deterioration. Together, NKM advances personalized, interpretable forecasting of future cognitive decline in AD using past multimodal data through an explainable, explicit system and reveals potential multimodal biological underpinnings of AD progression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>gp2Scale: A Class of Compactly-Supported Non-Stationary Kernels and Distributed Computing for Exact Gaussian Processes on 10 Million Data Points</title>
<link>https://arxiv.org/abs/2512.06143</link>
<guid>https://arxiv.org/abs/2512.06143</guid>
<content:encoded><![CDATA[
<div> Keywords: Gaussian processes, scalable methods, non-stationary kernels, sparse covariance, exact inference<br><br>Summary:  
The paper presents a novel methodology called gp2Scale that addresses the challenge of scaling exact Gaussian processes (GPs) to datasets exceeding 10 million points without relying on common approximations such as inducing points, kernel interpolation, or neighborhood-based methods. The approach leverages highly flexible, compactly supported, and non-stationary kernels to uncover inherent sparse structures within the covariance matrix. This sparsity is exploited effectively to perform efficient and exact computations of linear system solutions and log-determinants needed for model training. The method demonstrates competitive or superior approximation performance compared to state-of-the-art GP approximation techniques on several real-world datasets. Notably, gp2Scale maintains full fidelity to exact GP inference rather than trading accuracy for scalability. A key advantage is its agnosticism toward kernel design choices, noise models, mean functions, and input space types, supporting arbitrary customizations without compromising performance or flexibility. This positions gp2Scale as a versatile and powerful tool for modern GP applications where expressive, non-stationary kernels are increasingly important, overcoming the traditional trade-offs between computational speed, predictive accuracy, uncertainty quantification, and design flexibility. <div>
arXiv:2512.06143v1 Announce Type: new 
Abstract: Despite a large corpus of recent work on scaling up Gaussian processes, a stubborn trade-off between computational speed, prediction and uncertainty quantification accuracy, and customizability persists. This is because the vast majority of existing methodologies exploit various levels of approximations that lower accuracy and limit the flexibility of kernel and noise-model designs -- an unacceptable drawback at a time when expressive non-stationary kernels are on the rise in many fields. Here, we propose a methodology we term \emph{gp2Scale} that scales exact Gaussian processes to more than 10 million data points without relying on inducing points, kernel interpolation, or neighborhood-based approximations, and instead leveraging the existing capabilities of a GP: its kernel design. Highly flexible, compactly supported, and non-stationary kernels lead to the identification of naturally occurring sparse structure in the covariance matrix, which is then exploited for the calculations of the linear system solution and the log-determinant for training. We demonstrate our method's functionality on several real-world datasets and compare it with state-of-the-art approximation algorithms. Although we show superior approximation performance in many cases, the method's real power lies in its agnosticism toward arbitrary GP customizations -- core kernel design, noise, and mean functions -- and the type of input space, making it optimally suited for modern Gaussian process applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Invariant Graph Representations Through Redundant Information</title>
<link>https://arxiv.org/abs/2512.06154</link>
<guid>https://arxiv.org/abs/2512.06154</guid>
<content:encoded><![CDATA[
<div> Keywords: invariant graph representations, out-of-distribution generalization, Partial Information Decomposition, redundant information, spurious and causal subgraphs<br><br>Summary:<br><br>This paper addresses the challenge of learning invariant graph representations that perform well under out-of-distribution (OOD) generalization, which is difficult because learned features often contain spurious components. To overcome limitations of classical information-theoretic methods in invariant learning, the authors introduce Partial Information Decomposition (PID), a tool that decomposes information into unique, redundant, and synergistic parts, allowing precise identification of redundant information shared between spurious subgraphs and invariant subgraphs in graphs. They propose a novel multi-level optimization framework called Redundancy-guided Invariant Graph learning (RIG), which explicitly maximizes redundant information between causal and spurious graph components. RIG alternates between estimating a lower bound of this redundant information and maximizing it alongside other objectives to effectively isolate spurious from causal subgraphs. This isolation fosters better OOD generalization under various distribution shifts. Experimental results on both synthetic and real-world graph datasets demonstrate that RIG improves generalization capabilities compared to existing methods. Overall, the paper presents a principled and effective approach to learning robust invariant graph representations by leveraging PID to better understand and exploit redundant information in graph data. <div>
arXiv:2512.06154v1 Announce Type: new 
Abstract: Learning invariant graph representations for out-of-distribution (OOD) generalization remains challenging because the learned representations often retain spurious components. To address this challenge, this work introduces a new tool from information theory called Partial Information Decomposition (PID) that goes beyond classical information-theoretic measures. We identify limitations in existing approaches for invariant representation learning that solely rely on classical information-theoretic measures, motivating the need to precisely focus on redundant information about the target $Y$ shared between spurious subgraphs $G_s$ and invariant subgraphs $G_c$ obtained via PID. Next, we propose a new multi-level optimization framework that we call -- Redundancy-guided Invariant Graph learning (RIG) -- that maximizes redundant information while isolating spurious and causal subgraphs, enabling OOD generalization under diverse distribution shifts. Our approach relies on alternating between estimating a lower bound of redundant information (which itself requires an optimization) and maximizing it along with additional objectives. Experiments on both synthetic and real-world graph datasets demonstrate the generalization capabilities of our proposed RIG framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PMA-Diffusion: A Physics-guided Mask-Aware Diffusion Framework for TSE from Sparse Observations</title>
<link>https://arxiv.org/abs/2512.06183</link>
<guid>https://arxiv.org/abs/2512.06183</guid>
<content:encoded><![CDATA[
<div> Keywords: traffic state estimation, diffusion model, mask-aware training, physics-guided projection, sparse observations  

<br><br>Summary:  
This paper addresses the challenge of reconstructing high-resolution highway traffic speed fields from sparse and incomplete sensor data, which is crucial for Intelligent Transportation Systems. The authors propose PMA-Diffusion, a novel physics-guided mask-aware diffusion framework that effectively recovers unobserved traffic speeds. The method involves training a diffusion prior directly on sparsely observed speed fields using two specialized mask-aware training strategies named Single-Mask and Double-Mask, which improve learning from incomplete data. During inference, a physics-guided posterior sampler is introduced, combining reverse diffusion updates with observation projection and physics-based projection via adaptive anisotropic smoothing, ensuring physical consistency while reconstructing missing traffic speeds. The framework is empirically tested on the real-world I-24 MOTION dataset across different observation visibility ratios. Results demonstrate that PMA-Diffusion significantly outperforms existing baseline methods in reconstruction accuracy, even when only 5% of the traffic speeds are observed. Remarkably, the model trained with sparse data nearly matches the performance of models trained on fully observed speed fields, highlighting the efficiency and robustness of the approach. Overall, the integration of mask-aware diffusion priors with physics-guided posterior sampling offers a reliable, flexible, and practical solution to traffic state estimation under realistic sensing limitations. <div>
arXiv:2512.06183v1 Announce Type: new 
Abstract: High-resolution highway traffic state information is essential for Intelligent Transportation Systems, but typical traffic data acquired from loop detectors and probe vehicles are often too sparse and noisy to capture the detailed dynamics of traffic flow. We propose PMA-Diffusion, a physics-guided mask-aware diffusion framework that reconstructs unobserved highway speed fields from sparse, incomplete observations. Our approach trains a diffusion prior directly on sparsely observed speed fields using two mask-aware training strategies: Single-Mask and Double-Mask. At the inference phase, the physics-guided posterior sampler alternates reverse-diffusion updates, observation projection, and physics-guided projection based on adaptive anisotropic smoothing to reconstruct the missing speed fields. The proposed framework is tested on the I-24 MOTION dataset with varying visibility ratios. Even under severe sparsity, with only 5% visibility, PMA-Diffusion outperforms other baselines across three reconstruction error metrics. Furthermore, PMA-diffusion trained with sparse observation nearly matches the performance of the baseline model trained on fully observed speed fields. The results indicate that combining mask-aware diffusion priors with a physics-guided posterior sampler provides a reliable and flexible solution for traffic state estimation under realistic sensing sparsity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Should We Evaluate Data Deletion in Graph-Based ANN Indexes?</title>
<link>https://arxiv.org/abs/2512.06200</link>
<guid>https://arxiv.org/abs/2512.06200</guid>
<content:encoded><![CDATA[
<div> Keywords: Approximate Nearest Neighbor Search, data deletion, graph-based ANNS, evaluation framework, Deletion Control  

<br><br>Summary:  
This study addresses the challenge of data deletion in Approximate Nearest Neighbor Search (ANNS), particularly for dynamic datasets used in applications like Retrieval-Augmented Generation. It identifies a gap in existing research concerning comprehensive evaluation methodologies for data deletion in ANNS indexes. To fill this gap, the authors propose a novel experimental framework alongside detailed evaluation metrics focused on deletion efficiency, encompassing factors such as accuracy and query speed. The paper categorizes existing data deletion methods for graph-based ANNS into three distinct approaches, providing formal mathematical definitions for each. Utilizing this framework, the study evaluates the Hierarchical Navigable Small World (HNSW) algorithm — a leading ANNS method — to examine how different deletion strategies impact performance. Based on the findings, the authors introduce Deletion Control, an adaptive method designed to select the most appropriate deletion technique dynamically to maintain a required level of search accuracy. This contribution is significant for practical use cases requiring both efficient nearest neighbor search and data management in changing datasets. The proposed framework and Deletion Control method advance the state of the art by enabling systematic performance assessment and adaptive optimization in dynamic ANNS scenarios. <div>
arXiv:2512.06200v1 Announce Type: new 
Abstract: Approximate Nearest Neighbor Search (ANNS) has recently gained significant attention due to its many applications, such as Retrieval-Augmented Generation. Such applications require ANNS algorithms that support dynamic data, so the ANNS problem on dynamic data has attracted considerable interest. However, a comprehensive evaluation methodology for data deletion in ANNS has yet to be established. This study proposes an experimental framework and comprehensive evaluation metrics to assess the efficiency of data deletion for ANNS indexes under practical use cases. Specifically, we categorize data deletion methods in graph-based ANNS into three approaches and formalize them mathematically. The performance is assessed in terms of accuracy, query speed, and other relevant metrics. Finally, we apply the proposed evaluation framework to Hierarchical Navigable Small World, one of the state-of-the-art ANNS methods, to analyze the effects of data deletion, and propose Deletion Control, a method which dynamically selects the appropriate deletion method under a required search accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K2-V2: A 360-Open, Reasoning-Enhanced LLM</title>
<link>https://arxiv.org/abs/2512.06201</link>
<guid>https://arxiv.org/abs/2512.06201</guid>
<content:encoded><![CDATA[
<div> Keywords: K2-V2, open LLM, reasoning adaptation, training data release, model performance<br><br>Summary: K2-V2 is a newly introduced 360-open large language model (LLM) developed from scratch to serve as a robust base for reasoning adaptation along with capabilities in conversation and knowledge retrieval typical of general LLMs. It claims to be the strongest fully open model in its size category, outperforming competitors like Qwen2.5-72B and nearing the performance of Qwen3-235B. The model is carefully trained with infused domain knowledge, reasoning ability, extended context handling, and integrated tool use to explicitly prepare it for handling complex reasoning tasks. Its potential is demonstrated through simple supervised fine-tuning, setting a strong baseline indicative of considerable potential for further advanced alignment. To support ongoing improvements and open source production uses, the creators are releasing the full training history and comprehensive data composition. This transparency aims to maximize the effectiveness of continuous training. Additionally, the model weights and signature LLM360 artifacts—including complete training data—are made publicly available to empower the community with a reasoning-focused and capable foundation for further research and application development. <div>
arXiv:2512.06201v1 Announce Type: new 
Abstract: We introduce K2-V2, a 360-open LLM built from scratch as a superior base for reasoning adaptation, in addition to functions such as conversation and knowledge retrieval from general LLMs. It stands as the strongest fully open model, rivals open-weight leaders in its size class, outperforms Qwen2.5-72B and approaches the performance of Qwen3-235B. We actively infuse domain knowledge, reasoning, long-context, and tool use throughout the training process. This explicitly prepares the model for complex reasoning tasks. We demonstrate this potential using simple supervised fine-tuning, establishing a strong baseline that indicates significant headroom for advanced alignment. By releasing the full training history and data composition, we maximize the effectiveness of continuous training, a key open source production scenario. We release the model weights and signature LLM360 artifacts, such as complete training data, to empower the community with a capable, reasoning-centric foundation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantifying Memory Use in Reinforcement Learning with Temporal Range</title>
<link>https://arxiv.org/abs/2512.06204</link>
<guid>https://arxiv.org/abs/2512.06204</guid>
<content:encoded><![CDATA[
<div> Temporal Range, Reinforcement Learning, Memory Dependence, Sensitivity Analysis, Temporal Lag<br><br>Summary:<br><br>This paper introduces Temporal Range, a novel, model-agnostic metric designed to quantify how much a trained reinforcement learning (RL) policy relies on its past observations. Temporal Range measures first-order sensitivities of multiple vector outputs over a temporal window with respect to input sequences, summarizing these influences into a magnitude-weighted average lag. It is computed efficiently using reverse-mode automatic differentiation on Jacobian blocks linking outputs at later timesteps to earlier inputs. The metric is theoretically grounded by a set of natural axioms and performs well in linear settings. Experimentally, Temporal Range remains small in fully observed control tasks, scales appropriately with the ground-truth lag in synthetic sequence-copying tasks (Copy-$k$), and correlates with the minimal history window needed for near-optimal performance as shown through window ablation studies. The authors evaluate Temporal Range across various architectures including MLPs, RNNs, and state-space models (SSMs), as well as a compact Long Expressive Memory (LEM) policy, highlighting its utility as a proxy for task-level memory use. Overall, Temporal Range offers a practical, per-sequence measure of memory dependence that can help compare different agents and environments and assist in selecting the shortest sufficient context for decision-making in RL settings. <div>
arXiv:2512.06204v1 Announce Type: new 
Abstract: How much does a trained RL policy actually use its past observations? We propose \emph{Temporal Range}, a model-agnostic metric that treats first-order sensitivities of multiple vector outputs across a temporal window to the input sequence as a temporal influence profile and summarizes it by the magnitude-weighted average lag. Temporal Range is computed via reverse-mode automatic differentiation from the Jacobian blocks $\partial y_s/\partial x_t\in\mathbb{R}^{c\times d}$ averaged over final timesteps $s\in\{t+1,\dots,T\}$ and is well-characterized in the linear setting by a small set of natural axioms. Across diagnostic and control tasks (POPGym; flicker/occlusion; Copy-$k$) and architectures (MLPs, RNNs, SSMs), Temporal Range (i) remains small in fully observed control, (ii) scales with the task's ground-truth lag in Copy-$k$, and (iii) aligns with the minimum history window required for near-optimal return as confirmed by window ablations. We also report Temporal Range for a compact Long Expressive Memory (LEM) policy trained on the task, using it as a proxy readout of task-level memory. Our axiomatic treatment draws on recent work on range measures, specialized here to temporal lag and extended to vector-valued outputs in the RL setting. Temporal Range thus offers a practical per-sequence readout of memory dependence for comparing agents and environments and for selecting the shortest sufficient context.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Average-reward reinforcement learning in semi-Markov decision processes via relative value iteration</title>
<link>https://arxiv.org/abs/2512.06218</link>
<guid>https://arxiv.org/abs/2512.06218</guid>
<content:encoded><![CDATA[
<div> asynchronous stochastic approximation, semi-Markov decision processes, RVI Q-learning, average-reward optimality, convergence analysis

<br><br>Summary: This paper leverages recent advances in asynchronous stochastic approximation (SA) within the Borkar-Meyn framework to address reinforcement learning challenges in average-reward semi-Markov decision processes (SMDPs). The authors establish the almost sure convergence of an asynchronous SA variant of Schweitzer's classical relative value iteration (RVI) algorithm, specifically the RVI Q-learning algorithm, for finite-state, weakly communicating SMDPs. Their analysis demonstrates that the algorithm converges to a compact, connected subset of solutions for the average-reward optimality equation, with uniqueness of the solution guaranteed under additional constraints on step sizes and asynchrony. To fully exploit the SA framework, the paper introduces novel monotonicity conditions for accurate estimation of the optimal reward rate within RVI Q-learning, significantly broadening the class of applicable algorithmic scenarios. The authors also provide innovative arguments for the stability and convergence analysis, thereby overcoming limitations in existing frameworks. Overall, the work offers a rigorous convergence proof and an expanded theoretical foundation for using asynchronous RVI Q-learning methods in solving average-reward SMDPs in reinforcement learning contexts. <div>
arXiv:2512.06218v1 Announce Type: new 
Abstract: This paper applies the authors' recent results on asynchronous stochastic approximation (SA) in the Borkar-Meyn framework to reinforcement learning in average-reward semi-Markov decision processes (SMDPs). We establish the convergence of an asynchronous SA analogue of Schweitzer's classical relative value iteration algorithm, RVI Q-learning, for finite-space, weakly communicating SMDPs. In particular, we show that the algorithm converges almost surely to a compact, connected subset of solutions to the average-reward optimality equation, with convergence to a unique, sample path-dependent solution under additional stepsize and asynchrony conditions. Moreover, to make full use of the SA framework, we introduce new monotonicity conditions for estimating the optimal reward rate in RVI Q-learning. These conditions substantially expand the previously considered algorithmic framework and are addressed through novel arguments in the stability and convergence analysis of RVI Q-learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Back to Author Console Empowering GNNs for Domain Adaptation via Denoising Target Graph</title>
<link>https://arxiv.org/abs/2512.06236</link>
<guid>https://arxiv.org/abs/2512.06236</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Domain Adaptation, Node Classification, Graph Structure Shift, Edge Denoising<br><br>Summary:<br><br>1. This paper investigates the node classification task within the setting of graph domain adaptation, aiming to improve Graph Neural Networks (GNNs) performance on target graphs by utilizing both source and target graph structures alongside source labels.<br>2. It addresses the challenge of structural domain shifts that occur when graph data are gathered from different times or regions, which can lead to degraded GNN performance on the target domain.<br>3. The authors reveal that adding an auxiliary loss function focused on denoising edges of the target graph significantly enhances the generalization ability of GNNs for node classification.<br>4. Building on this finding, they introduce GraphDeT, a new framework incorporating the auxiliary edge denoising task into GNN training, targeting improved node classification under domain adaptation scenarios.<br>5. Theoretical analysis links this auxiliary task to tightening the graph generalization bound via -distance, which effectively constrains the learning process and results in better generalization.<br>6. Experimental evaluations demonstrate that GraphDeT outperforms existing baselines in managing domain shifts caused by both temporal and regional differences in graph data, confirming its effectiveness and robustness. <div>
arXiv:2512.06236v1 Announce Type: new 
Abstract: We explore the node classification task in the context of graph domain adaptation, which uses both source and target graph structures along with source labels to enhance the generalization capabilities of Graph Neural Networks (GNNs) on target graphs. Structure domain shifts frequently occur, especially when graph data are collected at different times or from varying areas, resulting in poor performance of GNNs on target graphs. Surprisingly, we find that simply incorporating an auxiliary loss function for denoising graph edges on target graphs can be extremely effective in enhancing GNN performance on target graphs. Based on this insight, we propose our framework, GraphDeT, a framework that integrates this auxiliary edge task into GNN training for node classification under domain adaptation. Our theoretical analysis connects this auxiliary edge task to the graph generalization bound with -distance, demonstrating such auxiliary task can imposes a constraint which tightens the bound and thereby improves generalization. The experimental results demonstrate superior performance compared to the existing baselines in handling both time and regional domain graph shifts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantization Blindspots: How Model Compression Breaks Backdoor Defenses</title>
<link>https://arxiv.org/abs/2512.06243</link>
<guid>https://arxiv.org/abs/2512.06243</guid>
<content:encoded><![CDATA[
<div> Backdoor attacks, quantization, backdoor defenses, INT8, neural networks<br><br>Summary:<br><br>1. The paper investigates the impact of model quantization on the effectiveness of existing backdoor attack defenses in neural networks. 2. It highlights that backdoor attacks embed malicious behavior that activates only on specific inputs while maintaining high accuracy on clean data, posing a critical threat to deployed machine learning systems. 3. The study evaluates five representative backdoor defenses across three model precision settings: full precision (FP32), INT8 dynamic quantization, and simulated INT4 precision on two vision datasets using the BadNet attack. 4. The results reveal that INT8 quantization drastically reduces the detection rates of all studied defenses to zero, while the backdoor attack success rate remains above 99%. 5. At INT4 precision, the defense performance becomes dataset-dependent; for example, Neural Cleanse effectively detects backdoors on GTSRB but fails on CIFAR-10, even though attacks remain highly successful (>90%). 6. This work uncovers a significant evaluation mismatch since most defenses are tested on full-precision models but real-world deployments use quantized models. 7. The authors emphasize the need to consider quantization robustness as a key factor in future backdoor defense evaluations and designs. <div>
arXiv:2512.06243v1 Announce Type: new 
Abstract: Backdoor attacks embed input-dependent malicious behavior into neural networks while preserving high clean accuracy, making them a persistent threat for deployed ML systems. At the same time, real-world deployments almost never serve full-precision models: post-training quantization to INT8 or lower precision is now standard practice for reducing memory and latency. This work asks a simple question: how do existing backdoor defenses behave under standard quantization pipelines? We conduct a systematic empirical study of five representative defenses across three precision settings (FP32, INT8 dynamic, INT4 simulated) and two standard vision benchmarks using a canonical BadNet attack. We observe that INT8 quantization reduces the detection rate of all evaluated defenses to 0% while leaving attack success rates above 99%. For INT4, we find a pronounced dataset dependence: Neural Cleanse remains effective on GTSRB but fails on CIFAR-10, even though backdoors continue to survive quantization with attack success rates above 90%. Our results expose a mismatch between how defenses are commonly evaluated (on FP32 models) and how models are actually deployed (in quantized form), and they highlight quantization robustness as a necessary axis in future evaluations and designs of backdoor defenses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Auto-exploration for online reinforcement learning</title>
<link>https://arxiv.org/abs/2512.06244</link>
<guid>https://arxiv.org/abs/2512.06244</guid>
<content:encoded><![CDATA[
<div> Exploration-exploitation dilemma, Reinforcement learning, Parameter-free, Sample complexity, Advantage gap function<br><br>Summary:<br><br>The paper addresses the fundamental challenge of balancing exploration and exploitation in reinforcement learning (RL). Existing RL algorithms for finite state and action discounted problems typically depend on assumptions requiring sufficient exploration across both state and action spaces. These assumptions lead to algorithms that are often non-implementable and exhibit sub-optimal performance due to reliance on problem-specific parameters. To overcome these issues, the authors introduce a novel class of methods termed "auto-exploration," which can automatically explore state and action spaces without needing prior knowledge of any problem-dependent parameters. Two variants of their algorithm are proposed: one designed for the tabular setting and another for linear function approximation. Both methods are backed by theoretical guarantees, achieving an $O(\epsilon^{-2})$ sample complexity to reach an $\epsilon$-accurate solution under broad, algorithm-independent assumptions about the existence of an exploring optimal policy. Importantly, this sample complexity bound is free from algorithm-dependent parameters, which are common in previous approaches and can be arbitrarily large. The proposed methods are also straightforward to implement, as they do not require direct estimation of unknown parameters. This progress is enabled through several new algorithmic techniques including dynamic mixing time, use of discounted state distributions for sampling, a robust gradient estimator, and the application of an advantage gap function to verify convergence. <div>
arXiv:2512.06244v1 Announce Type: new 
Abstract: The exploration-exploitation dilemma in reinforcement learning (RL) is a fundamental challenge to efficient RL algorithms. Existing algorithms for finite state and action discounted RL problems address this by assuming sufficient exploration over both state and action spaces. However, this yields non-implementable algorithms and sub-optimal performance. To resolve these limitations, we introduce a new class of methods with auto-exploration, or methods that automatically explore both state and action spaces in a parameter-free way, i.e.,~without a priori knowledge of problem-dependent parameters. We present two variants: one for the tabular setting and one for linear function approximation. Under algorithm-independent assumptions on the existence of an exploring optimal policy, both methods attain $O(\epsilon^{-2})$ sample complexity to solve to $\epsilon$ error. Crucially, these complexities are novel since they are void of algorithm-dependent parameters seen in prior works, which may be arbitrarily large. The methods are also simple to implement because they are parameter-free and do not directly estimate the unknown parameters. These feats are achieved by new algorithmic innovations for RL, including a dynamic mixing time, a discounted state distribution for sampling, a simple robust gradient estimator, and a recent advantage gap function to certify convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning When to Switch: Adaptive Policy Selection via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06250</link>
<guid>https://arxiv.org/abs/2512.06250</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, adaptive switching, maze navigation, Q-learning, multi-strategy agents<br><br>Summary:<br><br>This research addresses the challenge of switching between multiple strategies in autonomous agents tasked with complex navigation problems. It introduces a reinforcement learning method, specifically Q-learning, to adaptively learn switching thresholds between two distinct navigation policies: systematic exploration (coverage) and goal-directed pathfinding (convergence). Using maze navigation as a test scenario, the agent dynamically adjusts its switching behavior based on discretized states defined by coverage percentage and distance to the goal. Unlike fixed-threshold methods, the agent requires minimal prior knowledge, needing only maze size and target location without knowledge of wall positions or preset heuristics. The agent discretizes coverage into buckets ranging from 20% to 60% and learns which threshold to apply depending on observed progress. Experimental evaluation across 240 configurations involving multiple maze sizes, unique layouts, and agent variants reveals that adaptive threshold learning outperforms single-strategy and fixed-threshold baselines consistently. Performance improvements include 23-55% faster completion times, an 83% reduction in runtime variance, and a 71% enhancement in worst-case outcomes. Additionally, the learned behaviors generalize well to unseen maze layouts within the same size category. Notably, performance gains increase with maze complexity, underscoring the growing advantage of adaptive policy switching in large, complex environments. <div>
arXiv:2512.06250v1 Announce Type: new 
Abstract: Autonomous agents often require multiple strategies to solve complex tasks, but determining when to switch between strategies remains challenging. This research introduces a reinforcement learning technique to learn switching thresholds between two orthogonal navigation policies. Using maze navigation as a case study, this work demonstrates how an agent can dynamically transition between systematic exploration (coverage) and goal-directed pathfinding (convergence) to improve task performance. Unlike fixed-threshold approaches, the agent uses Q-learning to adapt switching behavior based on coverage percentage and distance to goal, requiring only minimal domain knowledge: maze dimensions and target location. The agent does not require prior knowledge of wall positions, optimal threshold values, or hand-crafted heuristics; instead, it discovers effective switching strategies dynamically during each run. The agent discretizes its state space into coverage and distance buckets, then adapts which coverage threshold (20-60\%) to apply based on observed progress signals. Experiments across 240 test configurations (4 maze sizes from 16$\times$16 to 128$\times$128 $\times$ 10 unique mazes $\times$ 6 agent variants) demonstrate that adaptive threshold learning outperforms both single-strategy agents and fixed 40\% threshold baselines. Results show 23-55\% improvements in completion time, 83\% reduction in runtime variance, and 71\% improvement in worst-case scenarios. The learned switching behavior generalizes within each size class to unseen wall configurations. Performance gains scale with problem complexity: 23\% improvement for 16$\times$16 mazes, 34\% for 32$\times$32, and 55\% for 64$\times$64, demonstrating that as the space of possible maze structures grows, the value of adaptive policy selection over fixed heuristics increases proportionally.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Without Time-Based Embodiment Resets in Soft-Actor Critic</title>
<link>https://arxiv.org/abs/2512.06252</link>
<guid>https://arxiv.org/abs/2512.06252</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Soft Actor-Critic, continuing tasks, embodiment resets, exploration<br><br>Summary:<br><br>This paper investigates the challenges of training reinforcement learning agents without relying on common task accessories such as episode terminations and embodiment resets. First, it introduces a continuing version of the Soft Actor-Critic (SAC) algorithm designed to work without episode boundaries. By applying slight modifications to the reward functions of existing tasks, the continuing SAC matches or surpasses episodic SAC performance and reduces sensitivity to the discount factor γ. Second, the study uses a modified Gym Reacher task to analyze why continuing SAC struggles without embodiment resets. The results indicate that embodiment resets facilitate better state space exploration, and removing them leads to poor exploration and slower or failed learning. Third, experiments on additional simulated tasks and a real-robot vision task demonstrate that increasing policy entropy when performance stagnates or worsens helps recover the lost performance caused by the absence of embodiment resets. Overall, this work highlights the importance of considering exploration mechanisms and policy entropy adjustments for robust reinforcement learning in continuous, reset-free environments, bringing training setups closer to real-world scenarios. <div>
arXiv:2512.06252v1 Announce Type: new 
Abstract: When creating new reinforcement learning tasks, practitioners often accelerate the learning process by incorporating into the task several accessory components, such as breaking the environment interaction into independent episodes and frequently resetting the environment. Although they can enable the learning of complex intelligent behaviors, such task accessories can result in unnatural task setups and hinder long-term performance in the real world. In this work, we explore the challenges of learning without episode terminations and robot embodiment resets using the Soft Actor-Critic (SAC) algorithm. To learn without terminations, we present a continuing version of the SAC algorithm and show that, with simple modifications to the reward functions of existing tasks, continuing SAC can perform as well as or better than episodic SAC while reducing the sensitivity of performance to the value of the discount rate $\gamma$. On a modified Gym Reacher task, we investigate possible explanations for the failure of continuing SAC when learning without embodiment resets. Our results suggest that embodiment resets help with exploration of the state space in the SAC algorithm, and removing embodiment resets can lead to poor exploration of the state space and failure of or significantly slower learning. Finally, on additional simulated tasks and a real-robot vision task, we show that increasing the entropy of the policy when performance trends worse or remains static is an effective intervention for recovering the performance lost due to not using embodiment resets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Networked Restless Multi-Arm Bandits with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06274</link>
<guid>https://arxiv.org/abs/2512.06274</guid>
<content:encoded><![CDATA[
<div> Restless Multi-Armed Bandits, networked RMAB, independent cascade model, submodularity, Q-learning<br><br>Summary:<br><br>This paper addresses the limitations of traditional Restless Multi-Armed Bandit (RMAB) models, which assume independence among arms and therefore fail to capture interactions between individuals common in real-world settings. To overcome this, the authors propose a Networked RMAB framework that integrates RMABs with the independent cascade model, enabling the modeling of network effects among arms. They formally define the Bellman equation for the Networked RMAB but recognize significant computational challenges due to the exponentially large state and action spaces. To tackle this, the paper establishes that the Bellman equation exhibits submodularity, allowing the use of a hill-climbing algorithm that guarantees a \(1 - \frac{1}{e}\) approximation in Bellman updates. Furthermore, they prove convergence of these approximate updates through a modified contraction analysis. Finally, the authors develop an efficient Q-learning algorithm adapted for the networked setting and validate their approach experimentally on real-world graph data. The results show that their network-aware Q-learning method outperforms traditional k-step look-ahead and network-blind baselines, underscoring the importance of accounting for network interactions in sequential decision-making problems. <div>
arXiv:2512.06274v1 Announce Type: new 
Abstract: Restless Multi-Armed Bandits (RMABs) are a powerful framework for sequential decision-making, widely applied in resource allocation and intervention optimization challenges in public health. However, traditional RMABs assume independence among arms, limiting their ability to account for interactions between individuals that can be common and significant in a real-world environment. This paper introduces Networked RMAB, a novel framework that integrates the RMAB model with the independent cascade model to capture interactions between arms in networked environments. We define the Bellman equation for networked RMAB and present its computational challenge due to exponentially large action and state spaces. To resolve the computational challenge, we establish the submodularity of Bellman equation and apply the hill-climbing algorithm to achieve a $1-\frac{1}{e}$ approximation guarantee in Bellman updates. Lastly, we prove that the approximate Bellman updates are guaranteed to converge by a modified contraction analysis. We experimentally verify these results by developing an efficient Q-learning algorithm tailored to the networked setting. Experimental results on real-world graph data demonstrate that our Q-learning approach outperforms both $k$-step look-ahead and network-blind approaches, highlighting the importance of capturing and leveraging network effects where they exist.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Theoretical Compression Bounds for Wide Multilayer Perceptrons</title>
<link>https://arxiv.org/abs/2512.06288</link>
<guid>https://arxiv.org/abs/2512.06288</guid>
<content:encoded><![CDATA[
<div> Pruning, Quantization, Neural Networks, Compression, Multilayer Perceptrons  

<br><br>Summary:  
This paper presents a rigorous theoretical justification for pruning and quantization techniques commonly used to compress large neural networks. The authors introduce a randomized greedy compression algorithm applied post-training, which enables the identification of pruned and quantized subnetworks within multilayer perceptrons (MLPs) that maintain competitive performance levels. They extend these theoretical results to structured pruning methodologies applicable to both MLPs and convolutional neural networks (CNNs), offering a unified framework for understanding pruning in wide networks. A notable feature of their analysis is the absence of assumptions regarding data distribution or characteristics, highlighting a fundamental tradeoff between network compressibility and the width of the neural network. The proposed algorithm shows similarities to the classical Optimal Brain Damage (OBD) method but incorporates randomness post-training, blending concepts from traditional pruning strategies with new theoretical insights. Overall, this work bridges the gap between empirical successes in model compression techniques and their mathematical underpinnings, specifically focusing on wide neural network architectures, thus providing a solid theoretical foundation that explains why pruning and quantization can be so effective in practice. <div>
arXiv:2512.06288v1 Announce Type: new 
Abstract: Pruning and quantization techniques have been broadly successful in reducing the number of parameters needed for large neural networks, yet theoretical justification for their empirical success falls short. We consider a randomized greedy compression algorithm for pruning and quantization post-training and use it to rigorously show the existence of pruned/quantized subnetworks of multilayer perceptrons (MLPs) with competitive performance. We further extend our results to structured pruning of MLPs and convolutional neural networks (CNNs), thus providing a unified analysis of pruning in wide networks. Our results are free of data assumptions, and showcase a tradeoff between compressibility and network width. The algorithm we consider bears some similarities with Optimal Brain Damage (OBD) and can be viewed as a post-training randomized version of it. The theoretical results we derive bridge the gap between theory and application for pruning/quantization, and provide a justification for the empirical success of compression in wide multilayer perceptrons.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Importance-aware Topic Modeling for Discovering Public Transit Risk from Noisy Social Media</title>
<link>https://arxiv.org/abs/2512.06293</link>
<guid>https://arxiv.org/abs/2512.06293</guid>
<content:encoded><![CDATA[
<div> Keywords: urban transit, social media, topic modeling, Poisson Deconvolution Factorization, influence-weighted graph<br><br>Summary:<br><br>This paper addresses the challenge faced by urban transit agencies of detecting emerging service risks such as crowding, delays, and safety issues via social media, where signals are typically sparse and obscured by routine chatter. The authors propose a novel approach that jointly models linguistic interactions and user influence by constructing an influence-weighted keyword co-occurrence graph from cleaned social media posts, ensuring more socially impactful posts contribute proportionally to evidence. Central to their framework is a Poisson Deconvolution Factorization (PDF) method that decomposes the graph into a low-rank topical structure and topic-localized residual interactions, yielding an interpretable topic–keyword basis with topic importance scores. They introduce a decorrelation regularizer to promote distinct, well-separated topics while employing a lightweight optimization procedure that guarantees stable convergence under nonnegativity and normalization constraints. The number of topics is optimally selected via a coherence-driven sweep that balances topic quality and distinctness. Experimental results on large-scale social streams demonstrate that their model achieves state-of-the-art performance in topic coherence and strong topic diversity compared to leading baselines. Additionally, the authors have made their code and dataset publicly available to support reproducibility and further research. <div>
arXiv:2512.06293v1 Announce Type: new 
Abstract: Urban transit agencies increasingly turn to social media to monitor emerging service risks such as crowding, delays, and safety incidents, yet the signals of concern are sparse, short, and easily drowned by routine chatter. We address this challenge by jointly modeling linguistic interactions and user influence. First, we construct an influence-weighted keyword co-occurrence graph from cleaned posts so that socially impactful posts contributes proportionally to the underlying evidence. The core of our framework is a Poisson Deconvolution Factorization (PDF) that decomposes this graph into a low-rank topical structure and topic-localized residual interactions, producing an interpretable topic--keyword basis together with topic importance scores. A decorrelation regularizer \emph{promotes} distinct topics, and a lightweight optimization procedure ensures stable convergence under nonnegativity and normalization constraints. Finally, the number of topics is selected through a coherence-driven sweep that evaluates the quality and distinctness of the learned topics. On large-scale social streams, the proposed model achieves state-of-the-art topic coherence and strong diversity compared with leading baselines. The code and dataset are publicly available at https://github.com/pangjunbiao/Topic-Modeling_ITS.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropic Confinement and Mode Connectivity in Overparameterized Neural Networks</title>
<link>https://arxiv.org/abs/2512.06297</link>
<guid>https://arxiv.org/abs/2512.06297</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, loss landscape, entropic barriers, curvature, optimization dynamics<br><br>Summary:<br><br>1. Modern neural networks' loss landscapes feature basins of attraction connected by low-loss paths, suggesting possible transitions between solutions.<br>2. Despite these connections, optimization algorithms tend to remain confined to a single convex basin, rarely exploring intermediate points along these pathways.<br>3. The paper resolves this paradox by identifying "entropic barriers" caused by the interaction between curvature variations along the paths and noise in the optimization dynamics.<br>4. Empirical findings indicate that curvature increases systematically away from minima, generating effective forces that push noisy optimization trajectories back toward the endpoints of the paths, even when the loss values are nearly constant.<br>5. These curvature-induced entropic barriers persist longer and influence solution localization in parameter space more strongly than energetic barriers.<br>6. Overall, the study highlights how curvature-induced entropic forces govern both the connectivity between solutions in the loss landscape and the confinement of optimization trajectories within basins in deep learning.<br>7. This understanding provides insights into why optimization dynamics in deep networks rarely cross low-loss connecting paths and remain localized despite apparent path connectivity. <div>
arXiv:2512.06297v1 Announce Type: new 
Abstract: Modern neural networks exhibit a striking property: basins of attraction in the loss landscape are often connected by low-loss paths, yet optimization dynamics generally remain confined to a single convex basin and rarely explore intermediate points. We resolve this paradox by identifying entropic barriers arising from the interplay between curvature variations along these paths and noise in optimization dynamics. Empirically, we find that curvature systematically rises away from minima, producing effective forces that bias noisy dynamics back toward the endpoints - even when the loss remains nearly flat. These barriers persist longer than energetic barriers, shaping the late-time localization of solutions in parameter space. Our results highlight the role of curvature-induced entropic forces in governing both connectivity and confinement in deep learning landscapes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chemistry Integrated Language Model using Hierarchical Molecular Representation for Polymer Informatics</title>
<link>https://arxiv.org/abs/2512.06301</link>
<guid>https://arxiv.org/abs/2512.06301</guid>
<content:encoded><![CDATA[
<div> Keywords: polymers, machine learning, molecular representation, property prediction, inverse design<br><br>Summary:<br><br>1. The paper addresses the challenge of applying machine learning (ML) to polymers, which have lagged behind inorganic compounds and small molecules mainly due to data scarcity. The authors argue that strategic molecular representations can help overcome this bottleneck.<br><br>2. They introduce CI-LLM (Chemically Informed Language Model), a novel framework that combines HAPPY (Hierarchically Abstracted rePeat unit of PolYmer) tokenization encoding chemical substructures with numerical descriptors, integrated within transformer architectures.<br><br>3. For forward property prediction, the authors develop De³BERTa, a descriptor-enriched encoder transformer model. This model achieves 3.5 times faster inference compared to conventional SMILES-based models while improving accuracy by 0.9-4.1 percent across four polymer property targets.<br><br>4. Importantly, De³BERTa provides interpretable insights into the structure-property relationship at the subgroup level, enhancing the explainability of ML predictions in polymer chemistry.<br><br>5. For inverse design, the study presents a GPT-based generator capable of producing polymers with user-defined properties. It achieves 100% scaffold retention and successfully performs multi-property optimization, including negative correlations between objectives.<br><br>This comprehensive framework demonstrates significant advances in both forward prediction and inverse design of polymers, illustrating how chemically informed representations can broaden ML applications in polymer science. <div>
arXiv:2512.06301v1 Announce Type: new 
Abstract: Machine learning has transformed material discovery for inorganic compounds and small molecules, yet polymers remain largely inaccessible to these methods. While data scarcity is often cited as the primary bottleneck, we demonstrate that strategic molecular representations can overcome this limitation. We introduce CI-LLM (Chemically Informed Language Model), a framework combining HAPPY (Hierarchically Abstracted rePeat unit of PolYmer), which encodes chemical substructures as tokens, with numerical descriptors within transformer architectures. For property prediction, De$^3$BERTa, our descriptor-enriched encoder, achieves 3.5x faster inference than SMILES-based models with improved accuracy ($R^2$ score gains of 0.9-4.1 percent across four properties), while providing interpretable structure-property insights at the subgroup level. For inverse design, our GPT-based generator produces polymers with targeted properties, achieving 100 percent scaffold retention and successful multi-property optimization for negatively correlated objectives. This comprehensive framework demonstrates both forward prediction and inverse design capabilities, showcasing how strategic molecular representation advances machine learning applications in polymer science.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multimodal Graph Neural Networks for Prognostic Modeling of Brain Network Reorganization</title>
<link>https://arxiv.org/abs/2512.06303</link>
<guid>https://arxiv.org/abs/2512.06303</guid>
<content:encoded><![CDATA[
<div> Keywords: multimodal graph neural network, brain network reorganization, fractional stochastic differential operators, attention mechanisms, prognostic biomarkers  

<br><br>Summary:  
This study presents a novel multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI data to model the spatiotemporal reorganization of brain networks. Brain regions are represented as nodes, while structural and functional connections form edges, creating longitudinal brain graphs for each individual. To capture temporal evolution and long-term dependencies in brain network dynamics, the model employs fractional stochastic differential operators embedded within graph-based recurrent networks, allowing it to account for stochastic fluctuations over time. The framework uses attention mechanisms to effectively fuse multimodal information and extract interpretable biomarkers, such as network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index that quantifies individual risk related to network instability and cognitive decline. Experiments conducted on longitudinal neuroimaging datasets demonstrate that the proposed method achieves high predictive accuracy while maintaining interpretability. Overall, the results illustrate the potential of mathematically rigorous, multimodal graph-based modeling to derive clinically meaningful biomarkers from existing neuroimaging data without the need for new data acquisition, contributing to improved prognosis and understanding of neurological disease progression. <div>
arXiv:2512.06303v1 Announce Type: new 
Abstract: Understanding the dynamic reorganization of brain networks is critical for predicting cognitive decline, neurological progression, and individual variability in clinical outcomes. This work proposes a multimodal graph neural network framework that integrates structural MRI, diffusion tensor imaging, and functional MRI to model spatiotemporal brain network reorganization. Brain regions are represented as nodes and structural and functional connectivity as edges, forming longitudinal brain graphs for each subject. Temporal evolution is captured via fractional stochastic differential operators embedded within graph-based recurrent networks, enabling the modeling of long-term dependencies and stochastic fluctuations in network dynamics. Attention mechanisms fuse multimodal information and generate interpretable biomarkers, including network energy entropy, graph curvature, fractional memory indices, and modality-specific attention scores. These biomarkers are combined into a composite prognostic index to quantify individual risk of network instability or cognitive decline. Experiments on longitudinal neuroimaging datasets demonstrate both predictive accuracy and interpretability. The results highlight the potential of mathematically rigorous, multimodal graph-based approaches for deriving clinically meaningful biomarkers from existing imaging data without requiring new data collection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretive Efficiency: Information-Geometric Foundations of Data Usefulness</title>
<link>https://arxiv.org/abs/2512.06341</link>
<guid>https://arxiv.org/abs/2512.06341</guid>
<content:encoded><![CDATA[
<div> Interpretability, machine learning, mutual information, Fisher geometry, representation design<br><br>Summary:<br><br>1. The paper introduces Interpretive Efficiency, a novel metric designed to quantify how effectively data supports an interpretive representation in trustworthy machine learning.  2. Interpretive Efficiency is defined as a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel, addressing the limitations of existing interpretability metrics.  3. The metric is rigorously grounded on five axioms: boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency, ensuring its theoretical soundness.  4. The authors connect Interpretive Efficiency to mutual information and derive a local Fisher-geometric expansion, providing a geometric perspective to analyze interpretability.  5. They further establish asymptotic and finite-sample guarantees for estimation using empirical-process tools, making the measure statistically reliable.  6. The experimental evaluation on controlled image and signal tasks demonstrates that the metric recovers known theoretical orderings, uncovers representational redundancy that accuracy metrics hide, and correlates with robustness.  7. The results suggest that Interpretive Efficiency is a practical, theory-backed diagnostic tool that can guide the design of interpretable representations in machine learning systems. <div>
arXiv:2512.06341v1 Announce Type: new 
Abstract: Interpretability is central to trustworthy machine learning, yet existing metrics rarely quantify how effectively data support an interpretive representation. We propose Interpretive Efficiency, a normalized, task-aware functional that measures the fraction of task-relevant information transmitted through an interpretive channel. The definition is grounded in five axioms ensuring boundedness, Blackwell-style monotonicity, data-processing stability, admissible invariance, and asymptotic consistency. We relate the functional to mutual information and derive a local Fisher-geometric expansion, then establish asymptotic and finite-sample estimation guarantees using standard empirical-process tools. Experiments on controlled image and signal tasks demonstrate that the measure recovers theoretical orderings, exposes representational redundancy masked by accuracy, and correlates with robustness, making it a practical, theory-backed diagnostic for representation design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Distance Distracts: Representation Distance Bias in BT-Loss for Reward Models</title>
<link>https://arxiv.org/abs/2512.06343</link>
<guid>https://arxiv.org/abs/2512.06343</guid>
<content:encoded><![CDATA[
<div> Keywords: Reward modeling, Bradley-Terry loss, representation distance, NormBT, Large Language Models<br><br>Summary:<br><br>1. Reward models play a pivotal role in aligning large language models (LLMs) within the reinforcement learning from human feedback (RLHF) framework, where the Bradley-Terry (BT) loss is the standard objective function.  
2. The BT loss gradient's norm depends on two components: the difference in predicted rewards between chosen and rejected responses (prediction error), and the representation distance between the pair in the final layer's output space.  
3. While the prediction error guides learning as intended, the representation distance can skew the magnitude of gradient updates, causing pairs with small representation distance to have weak updates even if misranked, and those with large distance to have disproportionately strong updates.  
4. This imbalance makes gradients from large-distance pairs dominate learning, overshadowing important fine-grained distinctions in small-distance pairs.  
5. To address this, the authors propose NormBT, an adaptive pair-wise normalization technique that mitigates representation-driven effects, focusing updates on prediction error. NormBT is a low-overhead, drop-in enhancement to BT loss. Experimental results show consistent reward model performance improvements across LLM architectures and datasets, notably achieving over 5% gains in the Reasoning category of RewardBench that features many small-distance pairs.  
6. This study identifies a fundamental limitation of the BT objective and offers a simple, effective correction to improve LLM reward modeling. <div>
arXiv:2512.06343v1 Announce Type: new 
Abstract: Reward models are central to Large Language Model (LLM) alignment within the framework of RLHF. The standard objective used in reward modeling is the Bradley-Terry (BT) loss, which learns from pairwise data consisting of a pair of chosen and rejected responses. In this work, we analyze the per-sample gradient of BT-loss and show that its norm scales with two distinct components: (1) the difference in predicted rewards between chosen and rejected responses, which reflects the prediction error, and critically, (2) representation distance between the pair measured in the output space of the final layer. While the first term captures the intended training signal, we show that the second term can significantly impact the update magnitude and misalign learning. Specifically, pairs with small representation distance often receive vanishingly weak updates, even when misranked, while pairs with large distance receive disproportionately strong updates. This leads to gradients from large-distance pairs to overshadow those from small-distance pairs, where fine-grained distinctions are especially important. To overcome this limitation, we propose NormBT, an adaptive pair-wise normalization scheme that balances representation-driven effects and focuses learning signals on prediction error. NormBT is a lightweight, drop-in integration to BT loss with negligible overhead. Across various LLM backbones and datasets, NormBT improves reward model performance consistently, with notable gains of over 5% on the Reasoning category of RewardBench, which contains numerous small-distance pairs. This work reveals a key limitation in the widely used BT objective and provides a simple, effective correction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero Generalization Error Theorem for Random Interpolators via Algebraic Geometry</title>
<link>https://arxiv.org/abs/2512.06347</link>
<guid>https://arxiv.org/abs/2512.06347</guid>
<content:encoded><![CDATA[
<div> Keywords: generalization error, interpolators, teacher-student framework, algebraic geometry, machine learning theory  

<br><br>Summary:  
This paper theoretically investigates the generalization ability of interpolators in machine learning models within a teacher-student setting. It shows that the generalization error of interpolators becomes zero once the training dataset size surpasses a model-dependent threshold. The work addresses a key open problem in understanding why large-scale models, such as deep neural networks, generalize well despite having the capacity to interpolate training data perfectly. Unlike prior studies focusing on stochastic gradient descent’s implicit bias towards good solutions, empirical evidence and this study suggest that the intrinsic properties of the model parameters are the main contributors to effective generalization. Specifically, the authors prove that, under the teacher-student framework, even randomly sampled interpolators (parameter sets that perfectly fit training data) generalize exactly to zero error when the sample number passes a threshold. This threshold is explicitly characterized by the geometric structure of the entire set of interpolators in the parameter space. The analysis employs algebraic geometry tools to rigorously describe and understand this geometric structure, providing new mathematical insights into why interpolating solutions generalize well, advancing the theoretical understanding of modern machine learning generalization phenomena. <div>
arXiv:2512.06347v1 Announce Type: new 
Abstract: We theoretically demonstrate that the generalization error of interpolators for machine learning models under teacher-student settings becomes 0 once the number of training samples exceeds a certain threshold. Understanding the high generalization ability of large-scale models such as deep neural networks (DNNs) remains one of the central open problems in machine learning theory. While recent theoretical studies have attributed this phenomenon to the implicit bias of stochastic gradient descent (SGD) toward well-generalizing solutions, empirical evidences indicate that it primarily stems from properties of the model itself. Specifically, even randomly sampled interpolators, which are parameters that achieve zero training error, have been observed to generalize effectively. In this study, under a teacher-student framework, we prove that the generalization error of randomly sampled interpolators becomes exactly zero once the number of training samples exceeds a threshold determined by the geometric structure of the interpolator set in parameter space. As a proof technique, we leverage tools from algebraic geometry to mathematically characterize this geometric structure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Upgraded Graph Reinforcement Learning for Carbon-Aware Job Scheduling in Smart Manufacturing</title>
<link>https://arxiv.org/abs/2512.06351</link>
<guid>https://arxiv.org/abs/2512.06351</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language model, graph reinforcement learning, flexible job shop scheduling, carbon-aware, smart manufacturing<br><br>Summary:<br><br>This paper introduces \textsc{Luca}, a novel framework that combines large language models (LLMs) with graph neural networks to enhance flexible job shop scheduling in smart manufacturing. The approach uses an innovative in-house prompting strategy to create fused embeddings that capture both structural and contextual information of the scheduling state. These embeddings feed into a deep reinforcement learning policy network designed to make real-time scheduling decisions optimized for two objectives: minimizing makespan and reducing carbon emissions. The framework incorporates a dual-objective reward function that balances energy efficiency with scheduling timeliness, addressing sustainability goals. Experimental validation was performed on both synthetic and public datasets, demonstrating consistent superiority over existing comparison algorithms. Specifically, on synthetic data, \textsc{Luca} reduced the average makespan by 4.1% and up to 12.2% compared to the best alternatives while keeping emission levels stable. On public benchmarks, it achieved additional improvements in both makespan and carbon emission metrics. Overall, \textsc{Luca} proves to be an effective and practical solution for dynamic, sustainable, and carbon-aware scheduling in smart manufacturing systems. <div>
arXiv:2512.06351v1 Announce Type: new 
Abstract: This paper presents \textsc{Luca}, a \underline{l}arge language model (LLM)-\underline{u}pgraded graph reinforcement learning framework for \underline{c}arbon-\underline{a}ware flexible job shop scheduling. \textsc{Luca} addresses the challenges of dynamic and sustainable scheduling in smart manufacturing systems by integrating a graph neural network and an LLM, guided by a carefully designed in-house prompting strategy, to produce a fused embedding that captures both structural characteristics and contextual semantics of the latest scheduling state. This expressive embedding is then processed by a deep reinforcement learning policy network, which generates real-time scheduling decisions optimized for both makespan and carbon emission objectives. To support sustainability goals, \textsc{Luca} incorporates a dual-objective reward function that encourages both energy efficiency and scheduling timeliness. Experimental results on both synthetic and public datasets demonstrate that \textsc{Luca} consistently outperforms comparison algorithms. For instance, on the synthetic dataset, it achieves an average of 4.1\% and up to 12.2\% lower makespan compared to the best-performing comparison algorithm while maintaining the same emission level. On public datasets, additional gains are observed for both makespan and emission. These results demonstrate that \textsc{Luca} is effective and practical for carbon-aware scheduling in smart manufacturing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DDFI: Diverse and Distribution-aware Missing Feature Imputation via Two-step Reconstruction</title>
<link>https://arxiv.org/abs/2512.06356</link>
<guid>https://arxiv.org/abs/2512.06356</guid>
<content:encoded><![CDATA[
<div> Keywords: incomplete node features, feature propagation, graph neural networks, masked autoencoder, inductive tasks<br><br>Summary:<br><br>Incomplete node features frequently occur in real-world graphs, such as partial privacy in web user attributes, which degrades Graph Neural Network (GNN) performance. Feature Propagation (FP) is commonly used for imputing missing node features but suffers from three main limitations: difficulty with graphs that are not fully connected, over-smoothing of imputed features, and being designed primarily for transductive tasks without addressing distribution shifts in inductive tasks. To overcome these challenges, the paper introduces DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that innovatively integrates FP with a graph-based Masked AutoEncoder (MAE). A key contribution is the Co-Label Linking (CLL) algorithm, which randomly connects training nodes sharing the same label to improve imputation on graphs with many disconnected components. At inference, DDFI employs a novel two-step representation generation process where FP-imputed features are further reconstructed by the MAE, reducing feature distribution shift and enhancing feature diversity in inductive scenarios. Additionally, the authors present a new real-world dataset named Sailing, derived from voyage records, containing naturally missing features, to evaluate imputation methods more effectively. Experiments on six public datasets and Sailing demonstrate that DDFI surpasses state-of-the-art approaches in both transductive and inductive settings, validating its effectiveness and robustness. <div>
arXiv:2512.06356v1 Announce Type: new 
Abstract: Incomplete node features are ubiquitous in real-world scenarios, e.g., the attributes of web users may be partly private, which causes the performance of Graph Neural Networks (GNNs) to decline significantly. Feature propagation (FP) is a well-known method that performs well for imputation of missing node features on graphs, but it still has the following three issues: 1) it struggles with graphs that are not fully connected, 2) imputed features face the over-smoothing problem, and 3) FP is tailored for transductive tasks, overlooking the feature distribution shift in inductive tasks. To address these challenges, we introduce DDFI, a Diverse and Distribution-aware Missing Feature Imputation method that combines feature propagation with a graph-based Masked AutoEncoder (MAE) in a nontrivial manner. It first designs a simple yet effective algorithm, namely Co-Label Linking (CLL), that randomly connects nodes in the training set with the same label to enhance the performance on graphs with numerous connected components. Then we develop a novel two-step representation generation process at the inference stage. Specifically, instead of directly using FP-imputed features as input during inference, DDFI further reconstructs the features through the whole MAE to reduce feature distribution shift in the inductive tasks and enhance the diversity of node features. Meanwhile, since existing feature imputation methods for graphs only evaluate by simulating the missing scenes with manually masking the features, we collect a new dataset called Sailing from the records of voyages that contains naturally missing features to help better evaluate the effectiveness. Extensive experiments conducted on six public datasets and Sailing show that DDFI outperforms the state-of-the-art methods under both transductive and inductive settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proportional integral derivative booster for neural networks-based time-series prediction: Case of water demand prediction</title>
<link>https://arxiv.org/abs/2512.06357</link>
<guid>https://arxiv.org/abs/2512.06357</guid>
<content:encoded><![CDATA[
<div> Multi-step prediction, time-series forecasting, PID controller, neural networks, periodic data<br><br>Summary:  
This article addresses the challenge of improving multi-step ahead prediction accuracy for periodic time-series data, which is crucial for decision-making in various industrial domains. It introduces a novel method inspired by the proportional-integral-derivative (PID) control approach designed to enhance neural network forecasting models without significantly increasing system complexity. The PID-based booster is applied at every prediction step to adjust forecasted values closer to the actual observed values, thereby refining accuracy. The water demand forecasting problem serves as a practical case study, utilizing two established deep neural network models to validate the effectiveness of the PID-inspired enhancement technique. Further generalization of the method is demonstrated by applying it to a neural network model predicting hourly energy consumption, showcasing its applicability to different types of periodic time-series problems. Comparative experiments between the original prediction models and those augmented with the PID-based booster highlight the method's superiority in achieving better forecasting accuracy while maintaining low computational complexity. Overall, the study presents a promising approach that integrates control theory principles with deep learning models to improve multi-step time-series predictions across diverse industrial contexts. <div>
arXiv:2512.06357v1 Announce Type: new 
Abstract: Multi-step time-series prediction is an essential supportive step for decision-makers in several industrial areas. Artificial intelligence techniques, which use a neural network component in various forms, have recently frequently been used to accomplish this step. However, the complexity of the neural network structure still stands up as a critical problem against prediction accuracy. In this paper, a method inspired by the proportional-integral-derivative (PID) control approach is investigated to enhance the performance of neural network models used for multi-step ahead prediction of periodic time-series information while maintaining a negligible impact on the complexity of the system. The PID-based method is applied to the predicted value at each time step to bring that value closer to the real value. The water demand forecasting problem is considered as a case study, where two deep neural network models from the literature are used to prove the effectiveness of the proposed boosting method. Furthermore, to prove the applicability of this PID-based booster to other types of periodic time-series prediction problems, it is applied to enhance the accuracy of a neural network model used for multi-step forecasting of hourly energy consumption. The comparison between the results of the original prediction models and the results after using the proposed technique demonstrates the superiority of the proposed method in terms of prediction accuracy and system complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Optimizers for Fast Gradient-Based Learning</title>
<link>https://arxiv.org/abs/2512.06370</link>
<guid>https://arxiv.org/abs/2512.06370</guid>
<content:encoded><![CDATA[
<div> Keywords: optimizer design, gradient-based learning, convex optimization, hyperparameter tuning, dynamic training  

<br><br>Summary:  
This paper establishes a theoretical framework for automating the design of optimizers used in gradient-based learning by focusing on maximizing the instantaneous decrease in loss, guided by the greedy principle. It conceptualizes optimizers as functions mapping loss gradient signals to parameter updates, transforming the design task into a series of convex optimization problems over the optimizer space. By solving these convex problems under various constraints, the framework not only reproduces many popular optimizers as closed-form solutions but also identifies their optimal hyperparameters tailored to specific problem settings. This approach provides a systematic and principled way to both design new optimizers and tune their hyperparameters based on the observed gradient statistics during training. Additionally, the paper highlights that this meta-optimization process can be performed dynamically and adaptively throughout training rather than only before or after, enabling optimizers to better respond to changing training conditions. Ultimately, this work paves the way for more automated, adaptive, and theoretically grounded approaches to optimizer design and hyperparameter optimization in machine learning. <div>
arXiv:2512.06370v1 Announce Type: new 
Abstract: We lay the theoretical foundation for automating optimizer design in gradient-based learning. Based on the greedy principle, we formulate the problem of designing optimizers as maximizing the instantaneous decrease in loss. By treating an optimizer as a function that translates loss gradient signals into parameter motions, the problem reduces to a family of convex optimization problems over the space of optimizers. Solving these problems under various constraints not only recovers a wide range of popular optimizers as closed-form solutions, but also produces the optimal hyperparameters of these optimizers with respect to the problems at hand. This enables a systematic approach to design optimizers and tune their hyperparameters according to the gradient statistics that are collected during the training process. Furthermore, this optimization of optimization can be performed dynamically during training.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLAX: Large-Scale, Distributed Reinforcement Learning for Large Language Models on TPUs</title>
<link>https://arxiv.org/abs/2512.06392</link>
<guid>https://arxiv.org/abs/2512.06392</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, TPU, Parameter Server, Dataset Curation<br><br>Summary:<br><br>1. The paper introduces RLAX, a scalable reinforcement learning (RL) framework designed to improve reasoning capabilities of large language models (LLMs) using TPUs. <br>2. RLAX employs a parameter-server architecture where a master trainer updates model weights on a central server, and multiple inference workers retrieve these weights to generate new rollouts, enabling efficient distributed training.<br>3. The framework incorporates a set of system techniques allowing scalable and preemptible RL training, supporting a variety of state-of-the-art RL algorithms.<br>4. To enhance training efficiency and model performance, the authors develop novel dataset curation and alignment methods.<br>5. Large-scale experiments show that RLAX significantly improves the QwQ-32B model’s pass@8 accuracy by 12.8% within approximately 12 hours and 48 minutes using 1024 TPU v5p cores, while maintaining robustness against training interruptions due to preemptions.<br><br>This work demonstrates a practical and powerful infrastructure for accelerating and scaling RL-based improvements on large language models leveraging TPU clusters, offering a pathway to faster convergence and higher quality in LLM tasks. <div>
arXiv:2512.06392v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as the de-facto paradigm for improving the reasoning capabilities of large language models (LLMs). We have developed RLAX, a scalable RL framework on TPUs. RLAX employs a parameter-server architecture. A master trainer periodically pushes updated model weights to the parameter server while a fleet of inference workers pull the latest weights and generates new rollouts. We introduce a suite of system techniques to enable scalable and preemptible RL for a diverse set of state-of-art RL algorithms. To accelerate convergence and improve model quality, we have devised new dataset curation and alignment techniques. Large-scale evaluations show that RLAX improves QwQ-32B's pass@8 accuracy by 12.8% in just 12 hours 48 minutes on 1024 v5p TPUs, while remaining robust to preemptions during training.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hankel-FNO: Fast Underwater Acoustic Charting Via Physics-Encoded Fourier Neural Operator</title>
<link>https://arxiv.org/abs/2512.06417</link>
<guid>https://arxiv.org/abs/2512.06417</guid>
<content:encoded><![CDATA[
<div> underwater acoustics, Fourier Neural Operator, Hankel-FNO, acoustic charting, sound propagation<br><br>Summary:<br><br>1. The article addresses the challenge of fast and accurate underwater acoustic charting, which is essential for tasks like sensor placement optimization and autonomous vehicle path planning. 2. Traditional numerical solvers provide accuracy but are computationally expensive and not suitable for large-scale or real-time applications. 3. Existing deep learning surrogate models accelerate computations but face limitations including fixed-resolution constraints and reliance on explicit partial differential equation formulations, limiting their generalizability across environments. 4. The authors propose Hankel-FNO, a model based on the Fourier Neural Operator that integrates knowledge of sound propagation and bathymetry for enhanced performance. 5. Hankel-FNO demonstrates superior speed compared to conventional solvers and achieves higher accuracy than data-driven alternatives, particularly in long-range acoustic predictions. 6. The model exhibits strong adaptability to varying environmental conditions and sound source configurations, requiring minimal fine-tuning to maintain performance. <div>
arXiv:2512.06417v1 Announce Type: new 
Abstract: Fast and accurate underwater acoustic charting is crucial for downstream tasks such as environment-aware sensor placement optimization and autonomous vehicle path planning. Conventional methods rely on computationally expensive while accurate numerical solvers, which are not scalable for large-scale or real-time applications. Although deep learning-based surrogate models can accelerate these computations, they often suffer from limitations such as fixed-resolution constraints or dependence on explicit partial differential equation formulations. These issues hinder their applicability and generalization across diverse environments. We propose Hankel-FNO, a Fourier Neural Operator (FNO)-based model for efficient and accurate acoustic charting. By incorporating sound propagation knowledge and bathymetry, our method has high accuracy while maintaining high computational speed. Results demonstrate that Hankel-FNO outperforms traditional solvers in speed and surpasses data-driven alternatives in accuracy, especially in long-range predictions. Experiments show the model's adaptability to diverse environments and sound source settings with minimal fine-tuning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A new initialisation to Control Gradients in Sinusoidal Neural network</title>
<link>https://arxiv.org/abs/2512.06427</link>
<guid>https://arxiv.org/abs/2512.06427</guid>
<content:encoded><![CDATA[
<div> Keywords: neural network initialization, SIREN, sinusoidal activation, gradient control, Neural Tangent Kernel<br><br>Summary: This paper addresses the challenge of proper initialization in neural networks, particularly those using sinusoidal activation functions such as SIREN, to mitigate gradient explosion and vanishing. The authors derive a new closed-form initialization scheme distinct from the original SIREN method by analyzing fixed points from the convergence of pre-activation distributions and the variance of Jacobian sequences. This approach simultaneously controls gradients and ensures pre-activations vanish appropriately, preventing the emergence of spurious frequencies that degrade model generalization. The impact of the new initialization on training dynamics is further investigated using the Neural Tangent Kernel framework, revealing its strong influence on learning behavior. The proposed initialization is benchmarked against the original SIREN initialization and other baseline methods across various tasks including function fitting, image reconstruction, and physics-informed neural networks. Empirically, the new method consistently outperforms state-of-the-art methods, demonstrating improved training stability, better gradient scaling with network depth, and superior generalization performance in reconstruction problems. Overall, this work provides a rigorous theoretical foundation and practical improvements for initializing sinusoidal-activated neural networks, contributing to enhanced training efficiency and model accuracy. <div>
arXiv:2512.06427v1 Announce Type: new 
Abstract: Proper initialisation strategy is of primary importance to mitigate gradient explosion or vanishing when training neural networks. Yet, the impact of initialisation parameters still lacks a precise theoretical understanding for several well-established architectures. Here, we propose a new initialisation for networks with sinusoidal activation functions such as \texttt{SIREN}, focusing on gradients control, their scaling with network depth, their impact on training and on generalization. To achieve this, we identify a closed-form expression for the initialisation of the parameters, differing from the original \texttt{SIREN} scheme. This expression is derived from fixed points obtained through the convergence of pre-activation distribution and the variance of Jacobian sequences. Controlling both gradients and targeting vanishing pre-activation helps preventing the emergence of inappropriate frequencies during estimation, thereby improving generalization. We further show that this initialisation strongly influences training dynamics through the Neural Tangent Kernel framework (NTK). Finally, we benchmark \texttt{SIREN} with the proposed initialisation against the original scheme and other baselines on function fitting and image reconstruction. The new initialisation consistently outperforms state-of-the-art methods across a wide range of reconstruction tasks, including those involving physics-informed neural networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural expressiveness for beyond importance model compression</title>
<link>https://arxiv.org/abs/2512.06440</link>
<guid>https://arxiv.org/abs/2512.06440</guid>
<content:encoded><![CDATA[
<div> Keywords: Neural Network Pruning, Expressiveness, Model Compression, Data-Agnostic Pruning, YOLOv8  

<br><br>Summary:  
1. The paper introduces a novel pruning criterion called "Expressiveness," focusing on a neuron's or group of neurons' ability to effectively redistribute informational resources based on the overlap of activations, rather than relying on weight importance.  
2. This expressiveness criterion is strongly correlated with the network’s initialization state, making it independent of the training state and offering a new perspective on the "When to Prune" question.  
3. The method can be approximated using arbitrary data or small representative datasets, enabling data-agnostic pruning strategies that broaden model compression applicability.  
4. A hybrid pruning strategy combining expressiveness and importance-based criteria shows complementary benefits and achieves up to 10 times higher parameter compression compared to weight-based methods, with about 1% average performance degradation.  
5. Using expressiveness pruning alone surpasses many state-of-the-art methods in compression efficiency. Specifically, on the YOLOv8 model for object detection on the COCO dataset, the approach reduced MACs by 46.1%, removed 55.4% of parameters, and improved mean Average Precision ($mAP_{50-95}$) by 3%. <div>
arXiv:2512.06440v1 Announce Type: new 
Abstract: Neural Network Pruning has been established as driving force in the exploration of memory and energy efficient solutions with high throughput both during training and at test time. In this paper, we introduce a novel criterion for model compression, named "Expressiveness". Unlike existing pruning methods that rely on the inherent "Importance" of neurons' and filters' weights, ``Expressiveness" emphasizes a neuron's or group of neurons ability to redistribute informational resources effectively, based on the overlap of activations. This characteristic is strongly correlated to a network's initialization state, establishing criterion autonomy from the learning state stateless and thus setting a new fundamental basis for the expansion of compression strategies in regards to the "When to Prune" question. We show that expressiveness is effectively approximated with arbitrary data or limited dataset's representative samples, making ground for the exploration of Data-Agnostic strategies. Our work also facilitates a "hybrid" formulation of expressiveness and importance-based pruning strategies, illustrating their complementary benefits and delivering up to 10x extra gains w.r.t. weight-based approaches in parameter compression ratios, with an average of 1% in performance degradation. We also show that employing expressiveness (independently) for pruning leads to an improvement over top-performing and foundational methods in terms of compression efficiency. Finally, on YOLOv8, we achieve a 46.1% MACs reduction by removing 55.4\% of the parameters, with an increase of 3% in the mean Absolute Precision ($mAP_{50-95}$) for object detection on COCO dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BitStopper: An Efficient Transformer Attention Accelerator via Stage-fusion and Early Termination</title>
<link>https://arxiv.org/abs/2512.06457</link>
<guid>https://arxiv.org/abs/2512.06457</guid>
<content:encoded><![CDATA[
<div> Dynamic sparsity, BitStopper, Transformer accelerator, Bit-serial enable stage fusion, Token selection<br><br>Summary:<br><br>This paper addresses the high computational and memory costs of self-attention in attention-based large language models by introducing BitStopper, a fine-grained algorithm-architecture co-design that eliminates the need for a sparsity predictor. First, the Bit-serial Enable Stage Fusion (BESF) mechanism reduces memory accesses by progressively terminating trivial tokens and merging the prediction stage with execution, effectively minimizing overhead. Second, the Lightweight and Adaptive Token Selection (LATS) strategy complements bit-level sparsity speculation to improve the selection of important tokens efficiently. Third, the Bit-level Asynchronous Processing (BAP) strategy enhances compute utilization by enabling on-demand, bit-grained memory fetching, optimizing hardware resource use. Finally, the architecture design translates these theoretical improvements into practical performance gains. Extensive evaluations show that BitStopper outperforms state-of-the-art Transformer accelerators, achieving 2.03x and 1.89x speedups compared to Sanger and SOFA, respectively. Additionally, it delivers 2.4x and 2.1x improvements in energy efficiency, demonstrating notable advancements in both speed and power consumption for accelerating sparse attention computations in large language models. <div>
arXiv:2512.06457v1 Announce Type: new 
Abstract: Attention-based large language models (LLMs) have transformed modern AI applications, but the quadratic cost of self-attention imposes significant compute and memory overhead. Dynamic sparsity (DS) attention mitigates this, yet its hardware efficiency is limited by the added prediction stage and the heavy memory traffic it entails. To address these limitations, this paper proposes BitStopper, a fine-grained algorithm-architecture co-design that operates without a sparsity predictor. First, a bit-serial enable stage fusion (BESF) mechanism is proposed to reuse and minimize the memory access by progressively terminating trivial tokens and merging the prediction stage into the execution stage. Second, a lightweight and adaptive token selection (LATS) strategy is developed to work in concert with the bit-level sparsity speculation. Third, a bit-level asynchronous processing (BAP) strategy is employed to improve compute utilization during the on-demand bit-grained memory fetching. Finally, an elaborate architecture is designed to translate the theoretical complexity reduction into practical performance improvement. Extensive evaluations demonstrate that, compared to state-of-the-art (SOTA) Transformer accelerators, BitStopper achieves 2.03x and 1.89x speedups over Sanger and SOFA, respectively, while delivering 2.4x and 2.1x improvements in energy efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Goal-Conditioned Reinforcement Learning Works: Relation to Dual Control</title>
<link>https://arxiv.org/abs/2512.06471</link>
<guid>https://arxiv.org/abs/2512.06471</guid>
<content:encoded><![CDATA[
<div> Goal-conditioned RL, optimal control, reward design, partial observability, dual control<br><br>Summary:<br><br>This paper addresses goal-conditioned reinforcement learning (RL), where the agent's objective is to maximize the probability of reaching specific target goal states. The authors analyze the goal-conditioned setting through the lens of optimal control theory, deriving an optimality gap that explains why classical reward formulations, often quadratic and dense, may underperform compared to goal-conditioned rewards. They highlight that goal-conditioned rewards are more effective in guiding agents toward desired outcomes. Further, the study extends to partially observed Markov decision processes (POMDPs), linking the challenge of state estimation with the probabilistic nature of the goal-conditioned reward. This connection naturally fits dual control problems, where simultaneous state estimation and control decisions are necessary. The paper validates the theoretical benefits by applying goal-conditioned policies to environments characterized by nonlinear dynamics and uncertainty. Both reinforcement learning algorithms and predictive control methods are used, demonstrating improved performance and robustness over classical approaches. Overall, the work provides a theoretical foundation and practical insights into why goal-conditioned RL can outperform traditional dense reward methods, especially in complex, uncertain, and partially observable settings. <div>
arXiv:2512.06471v1 Announce Type: new 
Abstract: Goal-conditioned reinforcement learning (RL) concerns the problem of training an agent to maximize the probability of reaching target goal states. This paper presents an analysis of the goal-conditioned setting based on optimal control. In particular, we derive an optimality gap between more classical, often quadratic, objectives and the goal-conditioned reward, elucidating the success of goal-conditioned RL and why classical ``dense'' rewards can falter. We then consider the partially observed Markov decision setting and connect state estimation to our probabilistic reward, further making the goal-conditioned reward well suited to dual control problems. The advantages of goal-conditioned policies are validated on nonlinear and uncertain environments using both RL and predictive control techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing LLMs Using Quantization for Mobile Execution</title>
<link>https://arxiv.org/abs/2512.06490</link>
<guid>https://arxiv.org/abs/2512.06490</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Post-Training Quantization, 4-bit quantization, GGUF format, mobile inference<br><br>Summary:<br><br>This paper addresses the challenge of deploying large language models (LLMs) on resource-constrained mobile devices by applying Post-Training Quantization (PTQ) techniques. Specifically, it uses 4-bit quantization through the BitsAndBytes library integrated with the Hugging Face Transformers framework to compress Meta’s Llama 3.2 3B model. The quantized model is further converted into the GGUF format using llama.cpp tools, optimized for mobile inference. This approach results in a substantial 68.66% reduction in model size, making it feasible to run the Llama 3.2 3B model efficiently on Android devices. Qualitative assessments demonstrate that the 4-bit quantized model retains its ability to perform inference tasks effectively despite the compression. The study highlights the practical deployment of the GGUF-formatted quantized model on Android via the Termux environment and the Ollama framework. Ultimately, the combination of 4-bit PTQ and mobile-optimized formats like GGUF offers a promising solution for balancing model compactness with inference performance, facilitating capable LLM deployment in mobile contexts. <div>
arXiv:2512.06490v1 Announce Type: new 
Abstract: Large Language Models (LLMs) offer powerful capabilities, but their significant size and computational requirements hinder deployment on resource-constrained mobile devices. This paper investigates Post-Training Quantization (PTQ) for compressing LLMs for mobile execution. We apply 4-bit PTQ using the BitsAndBytes library with the Hugging Face Transformers framework to Meta's Llama 3.2 3B model. The quantized model is converted to GGUF format using llama.cpp tools for optimized mobile inference. The PTQ workflow achieves a 68.66% reduction in model size through 4-bit quantization, enabling the Llama 3.2 3B model to run efficiently on an Android device. Qualitative validation shows that the 4-bit quantized model can perform inference tasks successfully. We demonstrate the feasibility of running the quantized GGUF model on an Android device using the Termux environment and the Ollama framework. PTQ, especially at 4-bit precision combined with mobile-optimized formats like GGUF, provides a practical pathway for deploying capable LLMs on mobile devices, balancing model size and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagnosis-based mortality prediction for intensive care unit patients via transfer learning</title>
<link>https://arxiv.org/abs/2512.06511</link>
<guid>https://arxiv.org/abs/2512.06511</guid>
<content:encoded><![CDATA[
<div> transfer learning, ICU mortality prediction, diagnostic heterogeneity, GLM, XGBoost<br><br>Summary:<br><br>This study addresses the challenge of varying underlying causes of critical illness in the intensive care unit (ICU) and the lack of prediction models that consider diagnostic heterogeneity. The authors evaluate transfer learning methods for diagnosis-specific mortality prediction using both Generalized Linear Models (GLM) and XGBoost approaches applied to the eICU Collaborative Research Database. The results demonstrate that transfer learning models consistently outperform those trained solely on diagnosis-specific data and models relying only on the conventional APACHE IVa severity-of-illness score. Furthermore, transfer learning models exhibit better calibration compared to models trained on pooled data from all diagnoses. The study also proposes that the Youden cutoff is a better decision threshold for binary classification tasks than the conventional 0.5 threshold. Importantly, the transfer learning approach maintains strong predictive performance across different cutoff criteria, indicating its robustness for clinical decision-making. Overall, this research highlights the potential of transfer learning to improve prognosis accuracy in ICU patients by accounting for diagnostic variability, offering a more tailored and reliable tool for mortality prediction in critical care settings. <div>
arXiv:2512.06511v1 Announce Type: new 
Abstract: In the intensive care unit, the underlying causes of critical illness vary substantially across diagnoses, yet prediction models accounting for diagnostic heterogeneity have not been systematically studied. To address the gap, we evaluate transfer learning approaches for diagnosis-specific mortality prediction and apply both GLM- and XGBoost-based models to the eICU Collaborative Research Database. Our results demonstrate that transfer learning consistently outperforms models trained only on diagnosis-specific data and those using a well-known ICU severity-of-illness score, i.e., APACHE IVa, alone, while also achieving better calibration than models trained on the pooled data. Our findings also suggest that the Youden cutoff is a more appropriate decision threshold than the conventional 0.5 for binary outcomes, and that transfer learning maintains consistently high predictive performance across various cutoff criteria.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical geometric deep learning enables scalable analysis of molecular dynamics</title>
<link>https://arxiv.org/abs/2512.06520</link>
<guid>https://arxiv.org/abs/2512.06520</guid>
<content:encoded><![CDATA[
<div> Keywords: molecular dynamics, graph neural networks, protein-nucleic acid complexes, long-range interactions, atomic detail  

<br><br>Summary:  
This paper addresses challenges in analyzing molecular dynamics (MD) simulations of complex biomolecular systems, especially those lacking established quantitative features. It highlights the potential of graph neural networks (GNNs), which use message passing between nodes representing spatially neighboring atoms, to eliminate manual feature engineering. However, applying GNNs to large biomolecular assemblies with more than a few hundred residues has been restricted by difficulties in capturing long-range interactions and the high memory and runtime demands of large graphs. The authors propose a novel method that aggregates local information efficiently, substantially reducing computational resources without losing atomic-level resolution. This innovation enables the analysis of extensive simulations of protein-nucleic acid complexes containing thousands of residues to be conducted on single GPUs within minutes. For moderately sized systems with hundreds of residues, the method not only accelerates the analysis but also enhances model performance and interpretability. Overall, the approach broadens the applicability of GNN-based analysis to larger biomolecular MD trajectories, facilitating rapid, detailed, and interpretable insights into complex biomolecular dynamics at atomic resolution. <div>
arXiv:2512.06520v1 Announce Type: new 
Abstract: Molecular dynamics simulations can generate atomically detailed trajectories of complex systems, but analyzing these dynamics can be challenging when systems lack well-established quantitative descriptors (features). Graph neural networks (GNNs) in which messages are passed between nodes that represent atoms that are spatial neighbors promise to obviate manual feature engineering, but the use of GNNs with biomolecular systems of more than a few hundred residues has been limited in the context of analyzing dynamics by both difficulties in capturing the details of long-range interactions with message passing and the memory and runtime requirements associated with large graphs. Here, we show how local information can be aggregated to reduce memory and runtime requirements without sacrificing atomic detail. We demonstrate that this approach opens the door to analyzing simulations of protein-nucleic acid complexes with thousands of residues on single GPUs within minutes. For systems with hundreds of residues, for which there are sufficient data to make quantitative comparisons, we show that the approach improves performance and interpretability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Token-level Supervision: Unlocking the Potential of Decoding-based Regression via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06533</link>
<guid>https://arxiv.org/abs/2512.06533</guid>
<content:encoded><![CDATA[
<div> Keywords: decoding-based regression, reinforcement learning, sequence generation, numerical prediction, Markov Decision Process  

<br><br>Summary:  
This paper addresses the challenge in decoding-based regression where traditional token-level objectives like cross-entropy do not align well with continuous numerical values, causing limitations in precision and generalization. The authors propose using Reinforcement Learning (RL) to overcome this issue by framing the generation process as a Markov Decision Process and applying sequence-level rewards to enforce global numerical coherence rather than local token-level constraints. Their method employs specific RL algorithms such as ReMax and GRPO to better capture the overall magnitude of target values. Extensive experiments on tasks including tabular regression and code metric regression demonstrate that their RL-enhanced approach consistently outperforms state-of-the-art token-level baselines and conventional regression heads. The results showcase significant improvements in both sampling efficiency and predictive precision. This work establishes that integrating sequence-level signals via reinforcement learning unlocks the potential of decoding-based regression, making it a robust and accurate paradigm for general numerical prediction tasks. The analysis verifies the advantages of sequence-level optimization over token-level penalties, promoting better understanding and future research directions in applying large language models for numerical regression. <div>
arXiv:2512.06533v1 Announce Type: new 
Abstract: Decoding-based regression, which reformulates regression as a sequence generation task, has emerged as a promising paradigm of applying large language models for numerical prediction. However, its progress is hindered by the misalignment between discrete token-level objectives (e.g., cross-entropy) and continuous numerical values. Existing approaches relying on token-level constraints often fail to capture the global magnitude of the target value, limiting their precision and generalization. In this paper, we propose to unlock the potential of decoding-based regression via Reinforcement Learning (RL). We formulate the generation process as a Markov Decision Process, utilizing sequence-level rewards to enforce global numerical coherence. Extensive experiments on tabular regression and code metric regression demonstrate that our method (specifically with ReMax and GRPO) consistently outperforms both state-of-the-art token-level baselines and traditional regression heads, showing the superiority of introducing sequence-level signals. Our analysis further reveals that RL significantly enhances sampling efficiency and predictive precision, establishing decoding-based regression as a robust and accurate paradigm for general-purpose numerical prediction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A-3PO: Accelerating Asynchronous LLM Training with Staleness-aware Proximal Policy Approximation</title>
<link>https://arxiv.org/abs/2512.06547</link>
<guid>https://arxiv.org/abs/2512.06547</guid>
<content:encoded><![CDATA[
<div> Keywords: Decoupled loss, reinforcement learning, proximal policy, importance weight, A-3PO  

<br><br>Summary:  
1. Decoupled loss is a reinforcement learning algorithm designed to address high data staleness in asynchronous RL settings.  
2. It improves the stability of coupled-loss algorithms like PPO and GRPO by introducing a proximal policy that separates off-policy corrections (importance weights) from the policy update mechanism (trust region).  
3. The proximal policy, however, requires an extra forward pass through the neural network at each training step, which becomes a computational bottleneck for training large language models.  
4. The authors propose A-3PO (APproximated Proximal Policy Optimization), an approach that approximates the proximal policy via simple interpolation, thus eliminating the need for the extra forward pass.  
5. A-3PO achieves a reduction in training time by 18% while maintaining comparable learning performance, effectively improving training efficiency without sacrificing stability.  
6. The method and example code are made publicly available for use and further research at the specified GitHub repository. <div>
arXiv:2512.06547v1 Announce Type: new 
Abstract: Decoupled loss has been a successful reinforcement learning (RL) algorithm to deal with the high data staleness under the asynchronous RL setting. Decoupled loss improves coupled-loss style of algorithms' (e.g., PPO, GRPO) learning stability by introducing a proximal policy to decouple the off-policy corrections (importance weight) from the controlling policy updates (trust region). However, the proximal policy requires an extra forward pass through the network at each training step, creating a computational bottleneck for large language models. We observe that since the proximal policy only serves as a trust region anchor between the behavior and target policies, we can approximate it through simple interpolation without explicit computation. We call this approach A-3PO (APproximated Proximal Policy Optimization). A-3PO eliminates this overhead, reducing training time by 18% while maintaining comparable performance. Code & off-the-shelf example are available at: https://github.com/inclusionAI/AReaL/blob/main/docs/algorithms/prox_approx.md
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Manifold Part 2: Neural Network Mathematics</title>
<link>https://arxiv.org/abs/2512.06563</link>
<guid>https://arxiv.org/abs/2512.06563</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, fixed points, manifold complexity, learning dynamics, federated systems<br><br>Summary:<br><br>This work formalizes neural networks as global systems shaped by stacked piecewise manifolds, employing fixed-point theory and boundary-conditioned iteration to describe their learning process. It posits that without fixed coordinates and operators, neural networks emerge as learnable numerical computations influenced by manifold complexity and high-order nonlinearities constrained by boundary conditions. Real-world data contributes significant challenges such as immense scope, scale, and minibatch fragmentation, leading to data complexity that impacts training. The dynamics of training further generate learning complexity through mechanisms including shifting node covers, curvature accumulation, and plasticity fluctuations. The paper argues that neural networks do not start with fixed points; rather, they iteratively construct them driven by residual connections, and stable fixed-point regions are essential for the emergence of learning capability. This framework explains the limitations of monolithic neural network models when confronted with geometric and data-induced plasticity. Finally, it advocates for designing architectures and federated systems that distribute manifold complexity across multiple elastic models, resulting in a coherent world-modeling framework that integrates geometry, algebra, fixed-point theory, and real-data complexity to better manage learning in complex environments. <div>
arXiv:2512.06563v1 Announce Type: new 
Abstract: This work develops the global equations of neural networks through stacked piecewise manifolds, fixed--point theory, and boundary--conditioned iteration. Once fixed coordinates and operators are removed, a neural network appears as a learnable numerical computation shaped by manifold complexity, high--order nonlinearity, and boundary conditions. Real--world data impose strong data complexity, near-infinite scope, scale, and minibatch fragmentation, while training dynamics produce learning complexity through shifting node covers, curvature accumulation, and the rise and decay of plasticity. These forces constrain learnability and explain why capability emerges only when fixed--point regions stabilize. Neural networks do not begin with fixed points; they construct them through residual--driven iteration. This perspective clarifies the limits of monolithic models under geometric and data--induced plasticity and motivates architectures and federated systems that distribute manifold complexity across many elastic models, forming a coherent world--modeling framework grounded in geometry, algebra, fixed points, and real--data complexity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QL-LSTM: A Parameter-Efficient LSTM for Stable Long-Sequence Modeling</title>
<link>https://arxiv.org/abs/2512.06582</link>
<guid>https://arxiv.org/abs/2512.06582</guid>
<content:encoded><![CDATA[
<div> Quantum-Leap LSTM, Parameter-Shared Unified Gating, Hierarchical Gated Recurrence, Additive Skip Connections, Sequence Modeling

<br><br>Summary:  
This paper introduces the Quantum-Leap LSTM (QL-LSTM), a novel recurrent neural network architecture aimed at overcoming two major limitations in standard LSTM and GRU models: redundant gate-specific parameters and the difficulty in retaining long-range temporal information. First, the Parameter-Shared Unified Gating (PSUG) mechanism consolidates all gate-specific transformations into a single shared weight matrix, resulting in approximately a 48% reduction in parameters while maintaining the full gating functionality vital for model performance. Second, the Hierarchical Gated Recurrence with Additive Skip Connections (HGR-ASC) component introduces a multiplication-free pathway designed to facilitate better long-range information flow throughout the sequence, thereby mitigating the degradation typically caused by the forget gate in traditional recurrent models. The authors evaluate QL-LSTM on the IMDB sentiment classification dataset with extended-length documents, comparing it against standard LSTM, GRU, and BiLSTM baselines. Results show that QL-LSTM achieves competitive accuracy levels despite having substantially fewer parameters. Despite these improvements in parameter efficiency and step-wise computational cost, the current QL-LSTM prototype does not provide wall-clock speed advantages due to the sequential processing nature of recurrent architectures, indicating a need for future kernel-level optimizations to realize practical runtime speedups. <div>
arXiv:2512.06582v1 Announce Type: new 
Abstract: Recurrent neural architectures such as LSTM and GRU remain widely used in sequence modeling, but they continue to face two core limitations: redundant gate-specific parameters and reduced ability to retain information across long temporal distances. This paper introduces the Quantum-Leap LSTM (QL-LSTM), a recurrent architecture designed to address both challenges through two independent components. The Parameter-Shared Unified Gating mechanism replaces all gate-specific transformations with a single shared weight matrix, reducing parameters by approximately 48 percent while preserving full gating behavior. The Hierarchical Gated Recurrence with Additive Skip Connections component adds a multiplication-free pathway that improves long-range information flow and reduces forget-gate degradation. We evaluate QL-LSTM on sentiment classification using the IMDB dataset with extended document lengths, comparing it to LSTM, GRU, and BiLSTM reference models. QL-LSTM achieves competitive accuracy while using substantially fewer parameters. Although the PSUG and HGR-ASC components are more efficient per time step, the current prototype remains limited by the inherent sequential nature of recurrent models and therefore does not yet yield wall-clock speed improvements without further kernel-level optimization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On fine-tuning Boltz-2 for protein-protein affinity prediction</title>
<link>https://arxiv.org/abs/2512.06592</link>
<guid>https://arxiv.org/abs/2512.06592</guid>
<content:encoded><![CDATA[
<div> Keywords: protein-protein binding affinity, Boltz-2, structure-based prediction, sequence-based models, molecular interactions<br><br>Summary:<br>1. The accurate prediction of protein-protein binding affinity is essential for understanding molecular interactions and advancing therapeutic design. <br>2. The study adapts Boltz-2, originally a state-of-the-art protein-ligand affinity predictor based on structural information, for protein-protein affinity regression tasks. <br>3. Evaluation was conducted on two datasets: TCR3d and PPB-affinity. Despite Boltz-2-PPI delivering high structural accuracy, it performed worse than sequence-based prediction models across both small and large data scenarios. <br>4. Combining the structural embeddings from Boltz-2-PPI with embeddings derived from sequence-based models resulted in complementary performance improvements, particularly when paired with weaker sequence-based models. This implies that sequence- and structure-based methods capture different, complementary signals. <br>5. The findings highlight inherent biases in training with structural data and suggest that current structure-based representations are not yet optimized or sufficiently primed for highly accurate protein-protein affinity prediction, encouraging further research in this area. <div>
arXiv:2512.06592v1 Announce Type: new 
Abstract: Accurate prediction of protein-protein binding affinity is vital for understanding molecular interactions and designing therapeutics. We adapt Boltz-2, a state-of-the-art structure-based protein-ligand affinity predictor, for protein-protein affinity regression and evaluate it on two datasets, TCR3d and PPB-affinity. Despite high structural accuracy, Boltz-2-PPI underperforms relative to sequence-based alternatives in both small- and larger-scale data regimes. Combining embeddings from Boltz-2-PPI with sequence-based embeddings yields complementary improvements, particularly for weaker sequence models, suggesting different signals are learned by sequence- and structure-based models. Our results echo known biases associated with training with structural data and suggest that current structure-based representations are not primed for performant affinity prediction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast and Effective Solution to the Problem of Look-ahead Bias in LLMs</title>
<link>https://arxiv.org/abs/2512.06607</link>
<guid>https://arxiv.org/abs/2512.06607</guid>
<content:encoded><![CDATA[
<div> finance, large language models, look-ahead bias, model fine-tuning, inference-time adjustment

<br><br>Summary: This paper addresses the challenge of applying large language models (LLMs) to predictive tasks in finance, where look-ahead bias arises due to models being trained on extensive time-series data. Traditional backtesting methods are problematic because retraining cutting-edge models from scratch with a specific knowledge cutoff is computationally expensive and time-consuming. To overcome this, the authors propose a novel, fast, and cost-efficient approach that modifies the model’s output probabilities (logits) during inference rather than retraining. Their method involves using two smaller, specialized models: one fine-tuned on the data that should be "forgotten" and another on data that should be retained. By leveraging these models to adjust the logits of a large base model, the approach effectively removes both direct (verbatim) and underlying (semantic) knowledge that could cause look-ahead bias. The method also corrects biases in predictions and outperforms previously established techniques aimed at mitigating similar issues. This innovation enables more reliable backtests in financial modeling without the prohibitive costs associated with retraining large-scale models. <div>
arXiv:2512.06607v1 Announce Type: new 
Abstract: Applying LLMs to predictive tasks in finance is challenging due to look-ahead bias resulting from their training on long time-series data. This precludes the backtests typically employed in finance since retraining frontier models from scratch with a specific knowledge cutoff is prohibitive. In this paper, we introduce a fast, effective, and low-cost alternative. Our method guides generation at inference time by adjusting the logits of a large base model using a pair of smaller, specialized models -- one fine-tuned on information to be forgotten and another on information to be retained. We demonstrate that our method effectively removes both verbatim and semantic knowledge, corrects biases, and outperforms prior methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Vector Quantization using Gaussian Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.06609</link>
<guid>https://arxiv.org/abs/2512.06609</guid>
<content:encoded><![CDATA[
<div> Vector quantized variational autoencoder, Gaussian VAE, Gaussian Quant, target divergence constraint, image compression<br><br>Summary:<br><br>This paper addresses the challenge of training Vector Quantized Variational Autoencoders (VQ-VAEs), which are discrete auto-encoders used to compress images into discrete tokens but are difficult to optimize due to discretization. The authors propose Gaussian Quant (GQ), a novel method that converts a Gaussian VAE into a VQ-VAE without the need for additional training by using a random Gaussian noise codebook and selecting the closest noise vector to the posterior mean. They provide theoretical guarantees showing that a sufficiently large codebook size (logarithm exceeding the bits-back coding rate) ensures minimal quantization error. To enhance practical effectiveness, the paper introduces the target divergence constraint (TDC), a heuristic training strategy for Gaussian VAEs that improves the performance of GQ. Empirical results demonstrate that GQ surpasses existing VQ-VAE variants, including VQGAN, FSQ, LFQ, and BSQ, across both UNet and ViT backbone architectures. Additionally, TDC shows improvements over previous Gaussian VAE discretization methods like TokenBridge. The authors also provide the source code publicly, enabling reproducibility and further research. This work contributes a simple yet effective framework for bridging Gaussian VAE and discrete VQ-VAE approaches for image compression. <div>
arXiv:2512.06609v1 Announce Type: new 
Abstract: Vector quantized variational autoencoder (VQ-VAE) is a discrete auto-encoder that compresses images into discrete tokens. It is difficult to train due to discretization. In this paper, we propose a simple yet effective technique, dubbed Gaussian Quant (GQ), that converts a Gaussian VAE with certain constraint into a VQ-VAE without training. GQ generates random Gaussian noise as a codebook and finds the closest noise to the posterior mean. Theoretically, we prove that when the logarithm of the codebook size exceeds the bits-back coding rate of the Gaussian VAE, a small quantization error is guaranteed. Practically, we propose a heuristic to train Gaussian VAE for effective GQ, named target divergence constraint (TDC). Empirically, we show that GQ outperforms previous VQ-VAEs, such as VQGAN, FSQ, LFQ, and BSQ, on both UNet and ViT architectures. Furthermore, TDC also improves upon previous Gaussian VAE discretization methods, such as TokenBridge. The source code is provided in https://github.com/tongdaxu/VQ-VAE-from-Gaussian-VAE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Temporal Convolutional Neural Networks for Cross-Sectional Equity Return Prediction: A Comparative Benchmark Study</title>
<link>https://arxiv.org/abs/2512.06630</link>
<guid>https://arxiv.org/abs/2512.06630</guid>
<content:encoded><![CDATA[
<div> Quantum machine learning, temporal convolutional neural network, stock market prediction, quantum convolution circuits, Sharpe ratio<br><br>Summary:<br><br>This paper addresses the challenges faced by classical forecasting models in stock market prediction, such as noisy inputs, regime shifts, and limited generalization. It proposes a Quantum Temporal Convolutional Neural Network (QTCNN) that integrates a classical temporal encoder with parameter-efficient quantum convolution circuits for improved cross-sectional equity return prediction. The temporal encoder extracts multi-scale sequential patterns from technical indicators, while the quantum circuits exploit superposition and entanglement to enhance feature representation and reduce overfitting. The authors benchmark QTCNN on the JPX Tokyo Stock Exchange dataset and evaluate its performance via long-short portfolio strategies using the out-of-sample Sharpe ratio. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by about 72%. This demonstrates that quantum-enhanced models, like QTCNN, have significant practical potential for robust and accurate decision-making in quantitative finance. The study highlights the synergy between classical temporal encoding and quantum processing, offering a novel approach to cope with complex, noisy financial environments. The results pave the way for further research and application of quantum machine learning in financial forecasting tasks. <div>
arXiv:2512.06630v1 Announce Type: new 
Abstract: Quantum machine learning offers a promising pathway for enhancing stock market prediction, particularly under complex, noisy, and highly dynamic financial environments. However, many classical forecasting models struggle with noisy input, regime shifts, and limited generalization capacity. To address these challenges, we propose a Quantum Temporal Convolutional Neural Network (QTCNN) that combines a classical temporal encoder with parameter-efficient quantum convolution circuits for cross-sectional equity return prediction. The temporal encoder extracts multi-scale patterns from sequential technical indicators, while the quantum processing leverages superposition and entanglement to enhance feature representation and suppress overfitting. We conduct a comprehensive benchmarking study on the JPX Tokyo Stock Exchange dataset and evaluate predictions through long-short portfolio construction using out-of-sample Sharpe ratio as the primary performance metric. QTCNN achieves a Sharpe ratio of 0.538, outperforming the best classical baseline by approximately 72\%. These results highlight the practical potential of quantum-enhanced forecasting model, QTCNN, for robust decision-making in quantitative finance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Data Characteristics on GNN Evaluation for Detecting Fake News</title>
<link>https://arxiv.org/abs/2512.06638</link>
<guid>https://arxiv.org/abs/2512.06638</guid>
<content:encoded><![CDATA[

arXiv:2512.06638v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are widely used for the detection of fake news by modeling the content and propagation structure of news articles on social media. We show that two of the most commonly used benchmark data sets - GossipCop and PolitiFact - are poorly suited to evaluating the utility of models that use propagation structure. Specifically, these data sets exhibit shallow, ego-like graph topologies that provide little or no ability to differentiate among modeling methods. We systematically benchmark five GNN architectures against a structure-agnostic multilayer perceptron (MLP) that uses the same node features. We show that MLPs match or closely trail the performance of GNNs, with performance gaps often within 1-2% and overlapping confidence intervals. To isolate the contribution of structure in these datasets, we conduct controlled experiments where node features are shuffled or edge structures randomized. We find that performance collapses under feature shuffling but remains stable under edge randomization. This suggests that structure plays a negligible role in these benchmarks. Structural analysis further reveals that over 75% of nodes are only one hop from the root, exhibiting minimal structural diversity. In contrast, on synthetic datasets where node features are noisy and structure is informative, GNNs significantly outperform MLPs. These findings provide strong evidence that widely used benchmarks do not meaningfully test the utility of modeling structural features, and they motivate the development of datasets with richer, more diverse graph topologies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Fraud Identification and Interpretability Study for Listed Companies Based on Convolutional Neural Network</title>
<link>https://arxiv.org/abs/2512.06648</link>
<guid>https://arxiv.org/abs/2512.06648</guid>
<content:encoded><![CDATA[

arXiv:2512.06648v1 Announce Type: new 
Abstract: Since the emergence of joint-stock companies, financial fraud by listed firms has repeatedly undermined capital markets. Fraud is difficult to detect because of covert tactics and the high labor and time costs of audits. Traditional statistical models are interpretable but struggle with nonlinear feature interactions, while machine learning models are powerful but often opaque. In addition, most existing methods judge fraud only for the current year based on current year data, limiting timeliness.
  This paper proposes a financial fraud detection framework for Chinese A-share listed companies based on convolutional neural networks (CNNs). We design a feature engineering scheme that transforms firm-year panel data into image like representations, enabling the CNN to capture cross-sectional and temporal patterns and to predict fraud in advance. Experiments show that the CNN outperforms logistic regression and LightGBM in accuracy, robustness, and early-warning performance, and that proper tuning of the classification threshold is crucial in high-risk settings.
  To address interpretability, we analyze the model along the dimensions of entity, feature, and time using local explanation techniques. We find that solvency, ratio structure, governance structure, and internal control are general predictors of fraud, while environmental indicators matter mainly in high-pollution industries. Non-fraud firms share stable feature patterns, whereas fraud firms exhibit heterogeneous patterns concentrated in short time windows. A case study of Guanong Shares in 2022 shows that cash flow analysis, social responsibility, governance structure, and per-share indicators are the main drivers of the model's fraud prediction, consistent with the company's documented misconduct.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating Black Carbon Concentration from Urban Traffic Using Vision-Based Machine Learning</title>
<link>https://arxiv.org/abs/2512.06649</link>
<guid>https://arxiv.org/abs/2512.06649</guid>
<content:encoded><![CDATA[

arXiv:2512.06649v1 Announce Type: new 
Abstract: Black carbon (BC) emissions in urban areas are primarily driven by traffic, with hotspots near major roads disproportionately affecting marginalized communities. Because BC monitoring is typically performed using costly and specialized instruments. there is little to no available data on BC from local traffic sources that could help inform policy interventions targeting local factors. By contrast, traffic monitoring systems are widely deployed in cities around the world, highlighting the imbalance between what we know about traffic conditions and what do not know about their environmental consequences. To bridge this gap, we propose a machine learning-driven system that extracts visual information from traffic video to capture vehicles behaviors and conditions. Combining these features with weather data, our model estimates BC at street level, achieving an R-squared value of 0.72 and RMSE of 129.42 ng/m3 (nanogram per cubic meter). From a sustainability perspective, this work leverages resources already supported by urban infrastructure and established modeling techniques to generate information relevant to traffic emission. Obtaining BC concentration data provides actionable insights to support pollution reduction, urban planning, public health, and environmental justice at the local municipal level.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Test-Time Training for Predicting Need for Invasive Mechanical Ventilation in Multi-Center Cohorts</title>
<link>https://arxiv.org/abs/2512.06652</link>
<guid>https://arxiv.org/abs/2512.06652</guid>
<content:encoded><![CDATA[

arXiv:2512.06652v1 Announce Type: new 
Abstract: Accurate prediction of the need for invasive mechanical ventilation (IMV) in intensive care units (ICUs) patients is crucial for timely interventions and resource allocation. However, variability in patient populations, clinical practices, and electronic health record (EHR) systems across institutions introduces domain shifts that degrade the generalization performance of predictive models during deployment. Test-Time Training (TTT) has emerged as a promising approach to mitigate such shifts by adapting models dynamically during inference without requiring labeled target-domain data. In this work, we introduce Adaptive Test-Time Training (AdaTTT), an enhanced TTT framework tailored for EHR-based IMV prediction in ICU settings. We begin by deriving information-theoretic bounds on the test-time prediction error and demonstrate that it is constrained by the uncertainty between the main and auxiliary tasks. To enhance their alignment, we introduce a self-supervised learning framework with pretext tasks: reconstruction and masked feature modeling optimized through a dynamic masking strategy that emphasizes features critical to the main task. Additionally, to improve robustness against domain shifts, we incorporate prototype learning and employ Partial Optimal Transport (POT) for flexible, partial feature alignment while maintaining clinically meaningful patient representations. Experiments across multi-center ICU cohorts demonstrate competitive classification performance on different test-time adaptation benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GSAE: Graph-Regularized Sparse Autoencoders for Robust LLM Safety Steering</title>
<link>https://arxiv.org/abs/2512.06655</link>
<guid>https://arxiv.org/abs/2512.06655</guid>
<content:encoded><![CDATA[

arXiv:2512.06655v1 Announce Type: new 
Abstract: Large language models (LLMs) face critical safety challenges, as they can be manipulated to generate harmful content through adversarial prompts and jailbreak attacks. Many defenses are typically either black-box guardrails that filter outputs, or internals-based methods that steer hidden activations by operationalizing safety as a single latent feature or dimension. While effective for simple concepts, this assumption is limiting, as recent evidence shows that abstract concepts such as refusal and temporality are distributed across multiple features rather than isolated in one. To address this limitation, we introduce Graph-Regularized Sparse Autoencoders (GSAEs), which extends SAEs with a Laplacian smoothness penalty on the neuron co-activation graph. Unlike standard SAEs that assign each concept to a single latent feature, GSAEs recover smooth, distributed safety representations as coherent patterns spanning multiple features. We empirically demonstrate that GSAE enables effective runtime safety steering, assembling features into a weighted set of safety-relevant directions and controlling them with a two-stage gating mechanism that activates interventions only when harmful prompts or continuations are detected during generation. This approach enforces refusals adaptively while preserving utility on benign queries. Across safety and QA benchmarks, GSAE steering achieves an average 82% selective refusal rate, substantially outperforming standard SAE steering (42%), while maintaining strong task accuracy (70% on TriviaQA, 65% on TruthfulQA, 74% on GSM8K). Robustness experiments further show generalization across LLaMA-3, Mistral, Qwen, and Phi families and resilience against jailbreak attacks (GCG, AutoDAN), consistently maintaining >= 90% refusal of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Robustness: A New Approach to Evaluating Feature Attribution Methods</title>
<link>https://arxiv.org/abs/2512.06665</link>
<guid>https://arxiv.org/abs/2512.06665</guid>
<content:encoded><![CDATA[

arXiv:2512.06665v1 Announce Type: new 
Abstract: This paper studies the robustness of feature attribution methods for deep neural networks. It challenges the current notion of attributional robustness that largely ignores the difference in the model's outputs and introduces a new way of evaluating the robustness of attribution methods. Specifically, we propose a new definition of similar inputs, a new robustness metric, and a novel method based on generative adversarial networks to generate these inputs. In addition, we present a comprehensive evaluation with existing metrics and state-of-the-art attribution methods. Our findings highlight the need for a more objective metric that reveals the weaknesses of an attribution method rather than that of the neural network, thus providing a more accurate evaluation of the robustness of attribution methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Meta-Learning Gap: Combining Hydra and Quant for Large-Scale Time Series Classification</title>
<link>https://arxiv.org/abs/2512.06666</link>
<guid>https://arxiv.org/abs/2512.06666</guid>
<content:encoded><![CDATA[

arXiv:2512.06666v1 Announce Type: new 
Abstract: Time series classification faces a fundamental trade-off between accuracy and computational efficiency. While comprehensive ensembles like HIVE-COTE 2.0 achieve state-of-the-art accuracy, their 340-hour training time on the UCR benchmark renders them impractical for large-scale datasets. We investigate whether targeted combinations of two efficient algorithms from complementary paradigms can capture ensemble benefits while maintaining computational feasibility. Combining Hydra (competing convolutional kernels) and Quant (hierarchical interval quantiles) across six ensemble configurations, we evaluate performance on 10 large-scale MONSTER datasets (7,898 to 1,168,774 training instances). Our strongest configuration improves mean accuracy from 0.829 to 0.836, succeeding on 7 of 10 datasets. However, prediction-combination ensembles capture only 11% of theoretical oracle potential, revealing a substantial meta-learning optimization gap. Feature-concatenation approaches exceeded oracle bounds by learning novel decision boundaries, while prediction-level complementarity shows moderate correlation with ensemble gains. The central finding: the challenge has shifted from ensuring algorithms are different to learning how to combine them effectively. Current meta-learning strategies struggle to exploit the complementarity that oracle analysis confirms exists. Improved combination strategies could potentially double or triple ensemble gains across diverse time series classification applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GradientSpace: Unsupervised Data Clustering for Improved Instruction Tuning</title>
<link>https://arxiv.org/abs/2512.06678</link>
<guid>https://arxiv.org/abs/2512.06678</guid>
<content:encoded><![CDATA[

arXiv:2512.06678v1 Announce Type: new 
Abstract: Instruction tuning is one of the key steps required for adapting large language models (LLMs) to a broad spectrum of downstream applications. However, this procedure is difficult because real-world datasets are rarely homogeneous; they consist of a mixture of diverse information, causing gradient interference, where conflicting gradients pull the model in opposing directions, degrading performance. A common strategy to mitigate this issue is to group data based on semantic or embedding similarity. However, this fails to capture how data influences model parameters during learning. While recent works have attempted to cluster gradients directly, they randomly project gradients into lower dimensions to manage memory, which leads to accuracy loss. Moreover, these methods rely on expert ensembles which necessitates multiple inference passes and expensive on-the-fly gradient computations during inference. To address these limitations, we propose GradientSpace, a framework that clusters samples directly in full-dimensional gradient space. We introduce an online SVD-based algorithm that operates on LoRA gradients to identify latent skills without the infeasible cost of storing all sample gradients. Each cluster is used to train a specialized LoRA expert along with a lightweight router trained to select the best expert during inference. We show that routing to a single, appropriate expert outperforms expert ensembles used in prior work, while significantly reducing inference latency. Our experiments across mathematical reasoning, code generation, finance, and creative writing tasks demonstrate that GradientSpace leads to coherent expert specialization and consistent accuracy gains over state-of-the-art clustering methods and finetuning techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>State Diversity Matters in Offline Behavior Distillation</title>
<link>https://arxiv.org/abs/2512.06692</link>
<guid>https://arxiv.org/abs/2512.06692</guid>
<content:encoded><![CDATA[

arXiv:2512.06692v1 Announce Type: new 
Abstract: Offline Behavior Distillation (OBD), which condenses massive offline RL data into a compact synthetic behavioral dataset, offers a promising approach for efficient policy training and can be applied across various downstream RL tasks. In this paper, we uncover a misalignment between original and distilled datasets, observing that a high-quality original dataset does not necessarily yield a superior synthetic dataset. Through an empirical analysis of policy performance under varying levels of training loss, we show that datasets with greater state diversity outperforms those with higher state quality when training loss is substantial, as is often the case in OBD, whereas the relationship reverses under minimal loss, which contributes to the misalignment. By associating state quality and diversity in reducing pivotal and surrounding error, respectively, our theoretical analysis establishes that surrounding error plays a more crucial role in policy performance when pivotal error is large, thereby highlighting the importance of state diversity in OBD scenario. Furthermore, we propose a novel yet simple algorithm, state density weighted (SDW) OBD, which emphasizes state diversity by weighting the distillation objective using the reciprocal of state density, thereby distilling a more diverse state information into synthetic data. Extensive experiments across multiple D4RL datasets confirm that SDW significantly enhances OBD performance when the original dataset exhibits limited state diversity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Barren plateaus in quantum denoising diffusion probabilistic models</title>
<link>https://arxiv.org/abs/2512.06695</link>
<guid>https://arxiv.org/abs/2512.06695</guid>
<content:encoded><![CDATA[

arXiv:2512.06695v1 Announce Type: new 
Abstract: Quantum generative models leverage quantum superposition and entanglement to enhance learning efficiency for both classical and quantum data. The quantum denoising diffusion probabilistic model (QuDDPM), inspired by its classical counterpart, has been proposed as a promising framework for quantum generative learning. QuDDPM is capable of efficiently learning and generating quantum data, and it demonstrates excellent performance in learning correlated quantum noise models, quantum many-body phases, and the topological structure of quantum data. However, we show that barren plateaus emerge in QuDDPMs due to the use of 2-design states as the input for the denoising process, which severely undermines the performance of QuDDPM. Through theoretical analysis and experimental validation, we confirm the presence of barren plateaus in the original QuDDPM. To address this issue, we introduce an improved QuDDPM that utilizes a distribution maintaining a certain distance from the Haar distribution, ensuring better trainability. Experimental results demonstrate that our approach effectively mitigates the barren plateau problem and generates samples with higher quality, paving the way for scalable and efficient quantum generative learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pathway to $O(\sqrt{d})$ Complexity bound under Wasserstein metric of flow-based models</title>
<link>https://arxiv.org/abs/2512.06702</link>
<guid>https://arxiv.org/abs/2512.06702</guid>
<content:encoded><![CDATA[

arXiv:2512.06702v1 Announce Type: new 
Abstract: We provide attainable analytical tools to estimate the error of flow-based generative models under the Wasserstein metric and to establish the optimal sampling iteration complexity bound with respect to dimension as $O(\sqrt{d})$. We show this error can be explicitly controlled by two parts: the Lipschitzness of the push-forward maps of the backward flow which scales independently of the dimension; and a local discretization error scales $O(\sqrt{d})$ in terms of dimension. The former one is related to the existence of Lipschitz changes of variables induced by the (heat) flow. The latter one consists of the regularity of the score function in both spatial and temporal directions.
  These assumptions are valid in the flow-based generative model associated with the F\"{o}llmer process and $1$-rectified flow under the Gaussian tail assumption. As a consequence, we show that the sampling iteration complexity grows linearly with the square root of the trace of the covariance operator, which is related to the invariant distribution of the forward process.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Multimodal RUL Framework for Remaining Useful Life Estimation with Layer-wise Explanations</title>
<link>https://arxiv.org/abs/2512.06708</link>
<guid>https://arxiv.org/abs/2512.06708</guid>
<content:encoded><![CDATA[

arXiv:2512.06708v1 Announce Type: new 
Abstract: Estimating the Remaining Useful Life (RUL) of mechanical systems is pivotal in Prognostics and Health Management (PHM). Rolling-element bearings are among the most frequent causes of machinery failure, highlighting the need for robust RUL estimation methods. Existing approaches often suffer from poor generalization, lack of robustness, high data demands, and limited interpretability. This paper proposes a novel multimodal-RUL framework that jointly leverages image representations (ImR) and time-frequency representations (TFR) of multichannel, nonstationary vibration signals. The architecture comprises three branches: (1) an ImR branch and (2) a TFR branch, both employing multiple dilated convolutional blocks with residual connections to extract spatial degradation features; and (3) a fusion branch that concatenates these features and feeds them into an LSTM to model temporal degradation patterns. A multi-head attention mechanism subsequently emphasizes salient features, followed by linear layers for final RUL regression. To enable effective multimodal learning, vibration signals are converted into ImR via the Bresenham line algorithm and into TFR using Continuous Wavelet Transform. We also introduce multimodal Layer-wise Relevance Propagation (multimodal-LRP), a tailored explainability technique that significantly enhances model transparency. The approach is validated on the XJTU-SY and PRONOSTIA benchmark datasets. Results show that our method matches or surpasses state-of-the-art baselines under both seen and unseen operating conditions, while requiring ~28 % less training data on XJTU-SY and ~48 % less on PRONOSTIA. The model exhibits strong noise resilience, and multimodal-LRP visualizations confirm the interpretability and trustworthiness of predictions, making the framework highly suitable for real-world industrial deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Novel Deep Neural Network Architecture for Real-Time Water Demand Forecasting</title>
<link>https://arxiv.org/abs/2512.06714</link>
<guid>https://arxiv.org/abs/2512.06714</guid>
<content:encoded><![CDATA[

arXiv:2512.06714v1 Announce Type: new 
Abstract: Short-term water demand forecasting (StWDF) is the foundation stone in the derivation of an optimal plan for controlling water supply systems. Deep learning (DL) approaches provide the most accurate solutions for this purpose. However, they suffer from complexity problem due to the massive number of parameters, in addition to the high forecasting error at the extreme points. In this work, an effective method to alleviate the error at these points is proposed. It is based on extending the data by inserting virtual data within the actual data to relieve the nonlinearity around them. To our knowledge, this is the first work that considers the problem related to the extreme points. Moreover, the water demand forecasting model proposed in this work is a novel DL model with relatively low complexity. The basic model uses the gated recurrent unit (GRU) to handle the sequential relationship in the historical demand data, while an unsupervised classification method, K-means, is introduced for the creation of new features to enhance the prediction accuracy with less number of parameters. Real data obtained from two different water plants in China are used to train and verify the model proposed. The prediction results and the comparison with the state-of-the-art illustrate that the method proposed reduces the complexity of the model six times of what achieved in the literature while conserving the same accuracy. Furthermore, it is found that extending the data set significantly reduces the error by about 30%. However, it increases the training time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Motor Behavior Using Deep Learning and Reservoir Computing</title>
<link>https://arxiv.org/abs/2512.06725</link>
<guid>https://arxiv.org/abs/2512.06725</guid>
<content:encoded><![CDATA[

arXiv:2512.06725v1 Announce Type: new 
Abstract: We present a novel approach to EEG decoding for non-invasive brain machine interfaces (BMIs), with a focus on motor-behavior classification. While conventional convolutional architectures such as EEGNet and DeepConvNet are effective in capturing local spatial patterns, they are markedly less suited for modeling long-range temporal dependencies and nonlinear dynamics. To address this limitation, we integrate an Echo State Network (ESN), a prominent paradigm in reservoir computing into the decoding pipeline. ESNs construct a high-dimensional, sparsely connected recurrent reservoir that excels at tracking temporal dynamics, thereby complementing the spatial representational power of CNNs. Evaluated on a skateboard-trick EEG dataset preprocessed via the PREP pipeline and implemented in MNE-Python, our ESNNet achieves 83.2% within-subject and 51.3% LOSO accuracies, surpassing widely used CNN-based baselines. Code is available at https://github.com/Yutiankunkun/Motion-Decoding-Using-Biosignals
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KV-CAR: KV Cache Compression using Autoencoders and KV Reuse in Large Language Models</title>
<link>https://arxiv.org/abs/2512.06727</link>
<guid>https://arxiv.org/abs/2512.06727</guid>
<content:encoded><![CDATA[

arXiv:2512.06727v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) scale in size and context length, the memory requirements of the key value (KV) cache have emerged as a major bottleneck during autoregressive decoding. The KV cache grows with sequence length and embedding dimension, often exceeding the memory footprint of the model itself and limiting achievable batch sizes and context windows. To address this challenge, we present KV CAR, a unified and architecture agnostic framework that significantly reduces KV cache storage while maintaining model fidelity. KV CAR combines two complementary techniques. First, a lightweight autoencoder learns compact representations of key and value tensors along the embedding dimension, compressing them before they are stored in the KV cache and restoring them upon retrieval. Second, a similarity driven reuse mechanism identifies opportunities to reuse KV tensors of specific attention heads across adjacent layers. Together, these methods reduce the dimensional and structural redundancy in KV tensors without requiring changes to the transformer architecture. Evaluations on GPT 2 and TinyLLaMA models across Wikitext, C4, PIQA, and Winogrande datasets demonstrate that KV CAR achieves up to 47.85 percent KV cache memory reduction with minimal impact on perplexity and zero shot accuracy. System level measurements on an NVIDIA A40 GPU show that the reduced KV footprint directly translates into longer sequence lengths and larger batch sizes during inference. These results highlight the effectiveness of KV CAR in enabling memory efficient LLM inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Interpretability of AR-SSVEP-Based Motor Intention Recognition via CNN-BiLSTM and SHAP Analysis on EEG Data</title>
<link>https://arxiv.org/abs/2512.06730</link>
<guid>https://arxiv.org/abs/2512.06730</guid>
<content:encoded><![CDATA[

arXiv:2512.06730v1 Announce Type: new 
Abstract: Patients with motor dysfunction show low subjective engagement in rehabilitation training. Traditional SSVEP-based brain-computer interface (BCI) systems rely heavily on external visual stimulus equipment, limiting their practicality in real-world settings. This study proposes an augmented reality steady-state visually evoked potential (AR-SSVEP) system to address the lack of patient initiative and the high workload on therapists. Firstly, we design four HoloLens 2-based EEG classes and collect EEG data from seven healthy subjects for analysis. Secondly, we build upon the conventional CNN-BiLSTM architecture by integrating a multi-head attention mechanism (MACNN-BiLSTM). We extract ten temporal-spectral EEG features and feed them into a CNN to learn high-level representations. Then, we use BiLSTM to model sequential dependencies and apply a multi-head attention mechanism to highlight motor-intention-related patterns. Finally, the SHAP (SHapley Additive exPlanations) method is applied to visualize EEG feature contributions to the neural network's decision-making process, enhancing the model's interpretability. These findings enhance real-time motor intention recognition and support recovery in patients with motor impairments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arc Gradient Descent: A Mathematically Derived Reformulation of Gradient Descent with Phase-Aware, User-Controlled Step Dynamics</title>
<link>https://arxiv.org/abs/2512.06737</link>
<guid>https://arxiv.org/abs/2512.06737</guid>
<content:encoded><![CDATA[

arXiv:2512.06737v1 Announce Type: new 
Abstract: The paper presents the formulation, implementation, and evaluation of the ArcGD optimiser. The evaluation is conducted initially on a non-convex benchmark function and subsequently on a real-world ML dataset. The initial comparative study using the Adam optimiser is conducted on a stochastic variant of the highly non-convex and notoriously challenging Rosenbrock function, renowned for its narrow, curved valley, across dimensions ranging from 2D to 1000D and an extreme case of 50,000D. Two configurations were evaluated to eliminate learning-rate bias: (i) both using ArcGD's effective learning rate and (ii) both using Adam's default learning rate. ArcGD consistently outperformed Adam under the first setting and, although slower under the second, achieved super ior final solutions in most cases. In the second evaluation, ArcGD is evaluated against state-of-the-art optimizers (Adam, AdamW, Lion, SGD) on the CIFAR-10 image classification dataset across 8 diverse MLP architectures ranging from 1 to 5 hidden layers. ArcGD achieved the highest average test accuracy (50.7%) at 20,000 iterations, outperforming AdamW (46.6%), Adam (46.8%), SGD (49.6%), and Lion (43.4%), winning or tying on 6 of 8 architectures. Notably, while Adam and AdamW showed strong early convergence at 5,000 iterations, but regressed with extended training, whereas ArcGD continued improving, demonstrating generalization and resistance to overfitting without requiring early stopping tuning. Strong performance on geometric stress tests and standard deep-learning benchmarks indicates broad applicability, highlighting the need for further exploration. Moreover, it is also shown that a variant of ArcGD can be interpreted as a special case of the Lion optimiser, highlighting connections between the inherent mechanisms of such optimisation methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Scale Protein Structure Modelling with Geometric Graph U-Nets</title>
<link>https://arxiv.org/abs/2512.06752</link>
<guid>https://arxiv.org/abs/2512.06752</guid>
<content:encoded><![CDATA[

arXiv:2512.06752v1 Announce Type: new 
Abstract: Geometric Graph Neural Networks (GNNs) and Transformers have become state-of-the-art for learning from 3D protein structures. However, their reliance on message passing prevents them from capturing the hierarchical interactions that govern protein function, such as global domains and long-range allosteric regulation. In this work, we argue that the network architecture itself should mirror this biological hierarchy. We introduce Geometric Graph U-Nets, a new class of models that learn multi-scale representations by recursively coarsening and refining the protein graph. We prove that this hierarchical design can theoretically more expressive than standard Geometric GNNs. Empirically, on the task of protein fold classification, Geometric U-Nets substantially outperform invariant and equivariant baselines, demonstrating their ability to learn the global structural patterns that define protein folds. Our work provides a principled foundation for designing geometric deep learning architectures that can learn the multi-scale structure of biomolecules.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Analysis for Bandit Learning in Matching Markets with Serial Dictatorship</title>
<link>https://arxiv.org/abs/2512.06758</link>
<guid>https://arxiv.org/abs/2512.06758</guid>
<content:encoded><![CDATA[

arXiv:2512.06758v1 Announce Type: new 
Abstract: The problem of two-sided matching markets is well-studied in computer science and economics, owing to its diverse applications across numerous domains. Since market participants are usually uncertain about their preferences in various online matching platforms, an emerging line of research is dedicated to the online setting where one-side participants (players) learn their unknown preferences through multiple rounds of interactions with the other side (arms). Sankararaman et al. provide an $\Omega\left( \frac{N\log(T)}{\Delta^2} + \frac{K\log(T)}{\Delta} \right)$ regret lower bound for this problem under serial dictatorship assumption, where $N$ is the number of players, $K (\geq N)$ is the number of arms, $\Delta$ is the minimum reward gap across players and arms, and $T$ is the time horizon. Serial dictatorship assumes arms have the same preferences, which is common in reality when one side participants have a unified evaluation standard. Recently, the work of Kong and Li proposes the ET-GS algorithm and achieves an $O\left( \frac{K\log(T)}{\Delta^2} \right)$ regret upper bound, which is the best upper bound attained so far. Nonetheless, a gap between the lower and upper bounds, ranging from $N$ to $K$, persists. It remains unclear whether the lower bound or the upper bound needs to be improved. In this paper, we propose a multi-level successive selection algorithm that obtains an $O\left( \frac{N\log(T)}{\Delta^2} + \frac{K\log(T)}{\Delta} \right)$ regret bound when the market satisfies serial dictatorship. To the best of our knowledge, we are the first to propose an algorithm that matches the lower bound in the problem of matching markets with bandits.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Over-smoothing beyond Dirichlet energy</title>
<link>https://arxiv.org/abs/2512.06782</link>
<guid>https://arxiv.org/abs/2512.06782</guid>
<content:encoded><![CDATA[

arXiv:2512.06782v1 Announce Type: new 
Abstract: While Dirichlet energy serves as a prevalent metric for quantifying over-smoothing, it is inherently restricted to capturing first-order feature derivatives. To address this limitation, we propose a generalized family of node similarity measures based on the energy of higher-order feature derivatives. Through a rigorous theoretical analysis of the relationships among these measures, we establish the decay rates of Dirichlet energy under both continuous heat diffusion and discrete aggregation operators. Furthermore, our analysis reveals an intrinsic connection between the over-smoothing decay rate and the spectral gap of the graph Laplacian. Finally, empirical results demonstrate that attention-based Graph Neural Networks (GNNs) suffer from over-smoothing when evaluated under these proposed metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Angular Regularization for Positive-Unlabeled Learning on the Hypersphere</title>
<link>https://arxiv.org/abs/2512.06785</link>
<guid>https://arxiv.org/abs/2512.06785</guid>
<content:encoded><![CDATA[

arXiv:2512.06785v1 Announce Type: new 
Abstract: Positive-Unlabeled (PU) learning addresses classification problems where only a subset of positive examples is labeled and the remaining data is unlabeled, making explicit negative supervision unavailable. Existing PU methods often rely on negative-risk estimation or pseudo-labeling, which either require strong distributional assumptions or can collapse in high-dimensional settings. We propose AngularPU, a novel PU framework that operates on the unit hypersphere using cosine similarity and angular margin. In our formulation, the positive class is represented by a learnable prototype vector, and classification reduces to thresholding the cosine similarity between an embedding and this prototype-eliminating the need for explicit negative modeling. To counteract the tendency of unlabeled embeddings to cluster near the positive prototype, we introduce an angular regularizer that encourages dispersion of the unlabeled set over the hypersphere, improving separation. We provide theoretical guarantees on the Bayes-optimality of the angular decision rule, consistency of the learned prototype, and the effect of the regularizer on the unlabeled distribution. Experiments on benchmark datasets demonstrate that AngularPU achieves competitive or superior performance compared to state-of-the-art PU methods, particularly in settings with scarce positives and high-dimensional embeddings, while offering geometric interpretability and scalability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Small-Gain Nash: Certified Contraction to Nash Equilibria in Differentiable Games</title>
<link>https://arxiv.org/abs/2512.06791</link>
<guid>https://arxiv.org/abs/2512.06791</guid>
<content:encoded><![CDATA[

arXiv:2512.06791v1 Announce Type: new 
Abstract: Classical convergence guarantees for gradient-based learning in games require the pseudo-gradient to be (strongly) monotone in Euclidean geometry as shown by rosen(1965), a condition that often fails even in simple games with strong cross-player couplings. We introduce Small-Gain Nash (SGN), a block small-gain condition in a custom block-weighted geometry. SGN converts local curvature and cross-player Lipschitz coupling bounds into a tractable certificate of contraction. It constructs a weighted block metric in which the pseudo-gradient becomes strongly monotone on any region where these bounds hold, even when it is non-monotone in the Euclidean sense. The continuous flow is exponentially contracting in this designed geometry, and projected Euler and RK4 discretizations converge under explicit step-size bounds derived from the SGN margin and a local Lipschitz constant. Our analysis reveals a certified ``timescale band'', a non-asymptotic, metric-based certificate that plays a TTUR-like role: rather than forcing asymptotic timescale separation via vanishing, unequal step sizes, SGN identifies a finite band of relative metric weights for which a single-step-size dynamics is provably contractive. We validate the framework on quadratic games where Euclidean monotonicity analysis fails to predict convergence, but SGN successfully certifies it, and extend the construction to mirror/Fisher geometries for entropy-regularized policy gradient in Markov games. The result is an offline certification pipeline that estimates curvature, coupling, and Lipschitz parameters on compact regions, optimizes block weights to enlarge the SGN margin, and returns a structural, computable convergence certificate consisting of a metric, contraction rate, and safe step-sizes for non-monotone games.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partial Inverse Design of High-Performance Concrete Using Cooperative Neural Networks for Constraint-Aware Mix Generation</title>
<link>https://arxiv.org/abs/2512.06813</link>
<guid>https://arxiv.org/abs/2512.06813</guid>
<content:encoded><![CDATA[

arXiv:2512.06813v1 Announce Type: new 
Abstract: High-performance concrete offers exceptional strength and durability but requires complex mix designs involving many interdependent variables and practical constraints. While data-driven methods have advanced predictive modeling for forward design, inverse design, which focuses on determining mix compositions that achieve target performance, remains limited, particularly in design situations where some mix variables are fixed by constraints and only the remaining variables must be determined. This study proposes a cooperative neural network framework for the partial inverse design of high-performance concrete. The framework combines two coupled neural network models, an imputation model that infers the undetermined variables and a surrogate model that predicts compressive strength. Through cooperative learning, the model generates valid and performance-consistent mix designs in a single forward pass while accommodating different constraint combinations without retraining. Its performance is compared with both probabilistic and generative approaches, including Bayesian inference based on a Gaussian process surrogate and autoencoder-based models. Evaluated on a benchmark dataset, the proposed model achieves stable and higher R-squared values of 0.87-0.92 and reduces mean squared error by an average of 50 percent compared with autoencoder baselines and by an average of 70 percent compared with Bayesian inference. The results demonstrate that the cooperative neural network provides an accurate, robust, and computationally efficient foundation for constraint-aware, data-driven mix proportioning in concrete engineering.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Factorization-based Bearing Fault Diagnosis</title>
<link>https://arxiv.org/abs/2512.06837</link>
<guid>https://arxiv.org/abs/2512.06837</guid>
<content:encoded><![CDATA[

arXiv:2512.06837v1 Announce Type: new 
Abstract: This paper studies the key problems of bearing fault diagnosis of high-speed train. As the core component of the train operation system, the health of bearings is directly related to the safety of train operation. The traditional diagnostic methods are facing the challenge of insufficient diagnostic accuracy under complex conditions. To solve these problems, we propose a novel Neural Factorization-based Classification (NFC) framework for bearing fault diagnosis. It is built on two core idea: 1) Embedding vibration time series into multiple mode-wise latent feature vectors to capture diverse fault-related patterns; 2) Leveraging neural factorization principles to fuse these vectors into a unified vibration representation. This design enables effective mining of complex latent fault characteristics from raw time-series data. We further instantiate the framework with two models CP-NFC and Tucker-NFC based on CP and Tucker fusion schemes, respectively. Experimental results show that both models achieve superior diagnostic performance compared with traditional machine learning methods. The comparative analysis provides valuable empirical evidence and practical guidance for selecting effective diagnostic strategies in high-speed train bearing monitoring.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Know your Trajectory -- Trustworthy Reinforcement Learning deployment through Importance-Based Trajectory Analysis</title>
<link>https://arxiv.org/abs/2512.06917</link>
<guid>https://arxiv.org/abs/2512.06917</guid>
<content:encoded><![CDATA[

arXiv:2512.06917v1 Announce Type: new 
Abstract: As Reinforcement Learning (RL) agents are increasingly deployed in real-world applications, ensuring their behavior is transparent and trustworthy is paramount. A key component of trust is explainability, yet much of the work in Explainable RL (XRL) focuses on local, single-step decisions. This paper addresses the critical need for explaining an agent's long-term behavior through trajectory-level analysis. We introduce a novel framework that ranks entire trajectories by defining and aggregating a new state-importance metric. This metric combines the classic Q-value difference with a "radical term" that captures the agent's affinity to reach its goal, providing a more nuanced measure of state criticality. We demonstrate that our method successfully identifies optimal trajectories from a heterogeneous collection of agent experiences. Furthermore, by generating counterfactual rollouts from critical states within these trajectories, we show that the agent's chosen path is robustly superior to alternatives, thereby providing a powerful "Why this, and not that?" explanation. Our experiments in standard OpenAI Gym environments validate that our proposed importance metric is more effective at identifying optimal behaviors compared to classic approaches, offering a significant step towards trustworthy autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parent-Guided Semantic Reward Model (PGSRM): Embedding-Based Reward Functions for Reinforcement Learning of Transformer Language Models</title>
<link>https://arxiv.org/abs/2512.06920</link>
<guid>https://arxiv.org/abs/2512.06920</guid>
<content:encoded><![CDATA[

arXiv:2512.06920v1 Announce Type: new 
Abstract: We introduce the Parent-Guided Semantic Reward Model (PGSRM), a lightweight reward framework for reinforcement learning (RL) of transformer language models. PGSRM replaces binary correctness signals, human preference data, and trained reward models with a simple signal: cosine similarity between a parent model's reference output embedding and a child model's generated output for the same input. This yields a dense, semantically meaningful reward with no human annotation or additional model training. We apply PGSRM on five language tasks and find that it produces smoother reward improvement and more stable PPO dynamics than a binary reward baseline, suggesting that embedding-based semantic rewards are a practical alternative to RLHF-style reward modeling for parent-guided alignment in smaller transformer models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Phishing Detection with Transformer-Based Semantic Features</title>
<link>https://arxiv.org/abs/2512.06925</link>
<guid>https://arxiv.org/abs/2512.06925</guid>
<content:encoded><![CDATA[

arXiv:2512.06925v1 Announce Type: new 
Abstract: Phishing is a cybercrime in which individuals are deceived into revealing personal information, often resulting in financial loss. These attacks commonly occur through fraudulent messages, misleading advertisements, and compromised legitimate websites. This study proposes a Quantile Regression Deep Q-Network (QR-DQN) approach that integrates RoBERTa semantic embeddings with handcrafted lexical features to enhance phishing detection while accounting for uncertainties. Unlike traditional DQN methods that estimate single scalar Q-values, QR-DQN leverages quantile regression to model the distribution of returns, improving stability and generalization on unseen phishing data. A diverse dataset of 105,000 URLs was curated from PhishTank, OpenPhish, Cloudflare, and other sources, and the model was evaluated using an 80/20 train-test split. The QR-DQN framework achieved a test accuracy of 99.86%, precision of 99.75%, recall of 99.96%, and F1-score of 99.85%, demonstrating high effectiveness. Compared to standard DQN with lexical features, the hybrid QR-DQN with lexical and semantic features reduced the generalization gap from 1.66% to 0.04%, indicating significant improvement in robustness. Five-fold cross-validation confirmed model reliability, yielding a mean accuracy of 99.90% with a standard deviation of 0.04%. These results suggest that the proposed hybrid approach effectively identifies phishing threats, adapts to evolving attack strategies, and generalizes well to unseen data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the Sensitivity of BiLSTM Forecasting Models to Sequence Length and Input Noise</title>
<link>https://arxiv.org/abs/2512.06926</link>
<guid>https://arxiv.org/abs/2512.06926</guid>
<content:encoded><![CDATA[

arXiv:2512.06926v1 Announce Type: new 
Abstract: Deep learning (DL) models, a specialized class of multilayer neural networks, have become central to time-series forecasting in critical domains such as environmental monitoring and the Internet of Things (IoT). Among these, Bidirectional Long Short-Term Memory (BiLSTM) architectures are particularly effective in capturing complex temporal dependencies. However, the robustness and generalization of such models are highly sensitive to input data characteristics - an aspect that remains underexplored in existing literature. This study presents a systematic empirical analysis of two key data-centric factors: input sequence length and additive noise. To support this investigation, a modular and reproducible forecasting pipeline is developed, incorporating standardized preprocessing, sequence generation, model training, validation, and evaluation. Controlled experiments are conducted on three real-world datasets with varying sampling frequencies to assess BiLSTM performance under different input conditions. The results yield three key findings: (1) longer input sequences significantly increase the risk of overfitting and data leakage, particularly in data-constrained environments; (2) additive noise consistently degrades predictive accuracy across sampling frequencies; and (3) the simultaneous presence of both factors results in the most substantial decline in model stability. While datasets with higher observation frequencies exhibit greater robustness, they remain vulnerable when both input challenges are present. These findings highlight important limitations in current DL-based forecasting pipelines and underscore the need for data-aware design strategies. This work contributes to a deeper understanding of DL model behavior in dynamic time-series environments and provides practical insights for developing more reliable and generalizable forecasting systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Normalization Mamba with Multi Scale Trend Decomposition and Patch MoE Encoding</title>
<link>https://arxiv.org/abs/2512.06929</link>
<guid>https://arxiv.org/abs/2512.06929</guid>
<content:encoded><![CDATA[

arXiv:2512.06929v1 Announce Type: new 
Abstract: Time series forecasting in real world environments faces significant challenges non stationarity, multi scale temporal patterns, and distributional shifts that degrade model stability and accuracy. This study propose AdaMamba, a unified forecasting architecture that integrates adaptive normalization, multi scale trend extraction, and contextual sequence modeling to address these challenges. AdaMamba begins with an Adaptive Normalization Block that removes non stationary components through multi scale convolutional trend extraction and channel wise recalibration, enabling consistent detrending and variance stabilization. The normalized sequence is then processed by a Context Encoder that combines patch wise embeddings, positional encoding, and a Mamba enhanced Transformer layer with a mixture of experts feed forward module, allowing efficient modeling of both long range dependencies and local temporal dynamics. A lightweight prediction head generates multi horizon forecasts, and a denormalization mechanism reconstructs outputs by reintegrating local trends to ensure robustness under varying temporal conditions. AdaMamba provides strong representational capacity with modular extensibility, supporting deterministic prediction and compatibility with probabilistic extensions. Its design effectively mitigates covariate shift and enhances predictive reliability across heterogeneous datasets. Experimental evaluations demonstrate that AdaMamba's combination of adaptive normalization and expert augmented contextual modeling yields consistent improvements in stability and accuracy over conventional Transformer based baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden Leaks in Time Series Forecasting: How Data Leakage Affects LSTM Evaluation Across Configurations and Validation Strategies</title>
<link>https://arxiv.org/abs/2512.06932</link>
<guid>https://arxiv.org/abs/2512.06932</guid>
<content:encoded><![CDATA[

arXiv:2512.06932v1 Announce Type: new 
Abstract: Deep learning models, particularly Long Short-Term Memory (LSTM) networks, are widely used in time series forecasting due to their ability to capture complex temporal dependencies. However, evaluation integrity is often compromised by data leakage, a methodological flaw in which input-output sequences are constructed before dataset partitioning, allowing future information to unintentionally influence training. This study investigates the impact of data leakage on performance, focusing on how validation design mediates leakage sensitivity. Three widely used validation techniques (2-way split, 3-way split, and 10-fold cross-validation) are evaluated under both leaky (pre-split sequence generation) and clean conditions, with the latter mitigating leakage risk by enforcing temporal separation during data splitting prior to sequence construction. The effect of leakage is assessed using RMSE Gain, which measures the relative increase in RMSE caused by leakage, computed as the percentage difference between leaky and clean setups. Empirical results show that 10-fold cross-validation exhibits RMSE Gain values of up to 20.5% at extended lag steps. In contrast, 2-way and 3-way splits demonstrate greater robustness, typically maintaining RMSE Gain below 5% across diverse configurations. Moreover, input window size and lag step significantly influence leakage sensitivity: smaller windows and longer lags increase the risk of leakage, whereas larger windows help reduce it. These findings underscore the need for configuration-aware, leakage-resistant evaluation pipelines to ensure reliable performance estimation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unifying Human-Centered AI Fairness Framework</title>
<link>https://arxiv.org/abs/2512.06944</link>
<guid>https://arxiv.org/abs/2512.06944</guid>
<content:encoded><![CDATA[

arXiv:2512.06944v1 Announce Type: new 
Abstract: The increasing use of Artificial Intelligence (AI) in critical societal domains has amplified concerns about fairness, particularly regarding unequal treatment across sensitive attributes such as race, gender, and socioeconomic status. While there has been substantial work on ensuring AI fairness, navigating trade-offs between competing notions of fairness as well as predictive accuracy remains challenging, creating barriers to the practical deployment of fair AI systems. To address this, we introduce a unifying human-centered fairness framework that systematically covers eight distinct fairness metrics, formed by combining individual and group fairness, infra-marginal and intersectional assumptions, and outcome-based and equality-of-opportunity (EOO) perspectives. This structure allows stakeholders to align fairness interventions with their values and contextual considerations. The framework uses a consistent and easy-to-understand formulation for all metrics to reduce the learning curve for non-experts. Rather than privileging a single fairness notion, the framework enables stakeholders to assign weights across multiple fairness objectives, reflecting their priorities and facilitating multi-stakeholder compromises. We apply this approach to four real-world datasets: the UCI Adult census dataset for income prediction, the COMPAS dataset for criminal recidivism, the German Credit dataset for credit risk assessment, and the MEPS dataset for healthcare utilization. We show that adjusting weights reveals nuanced trade-offs between different fairness metrics. Finally, through case studies in judicial decision-making and healthcare, we demonstrate how the framework can inform practical and value-sensitive deployment of fair AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing BFGS and OGR for Second-Order Optimization</title>
<link>https://arxiv.org/abs/2512.06969</link>
<guid>https://arxiv.org/abs/2512.06969</guid>
<content:encoded><![CDATA[

arXiv:2512.06969v1 Announce Type: new 
Abstract: Estimating the Hessian matrix, especially for neural network training, is a challenging problem due to high dimensionality and cost. In this work, we compare the classical Sherman-Morrison update used in the popular BFGS method (Broy-den-Fletcher-Goldfarb-Shanno), which maintains a positive definite Hessian approximation under a convexity assumption, with a novel approach called Online Gradient Regression (OGR). OGR performs regression of gradients against positions using an exponential moving average to estimate second derivatives online, without requiring Hessian inversion. Unlike BFGS, OGR allows estimation of a general (not necessarily positive definite) Hessian and can thus handle non-convex structures. We evaluate both methods across standard test functions and demonstrate that OGR achieves faster convergence and improved loss, particularly in non-convex settings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction with Expert Advice under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2512.06971</link>
<guid>https://arxiv.org/abs/2512.06971</guid>
<content:encoded><![CDATA[

arXiv:2512.06971v1 Announce Type: new 
Abstract: We study the classic problem of prediction with expert advice under the constraint of local differential privacy (LDP). In this context, we first show that a classical algorithm naturally satisfies LDP and then design two new algorithms that improve it: RW-AdaBatch and RW-Meta. For RW-AdaBatch, we exploit the limited-switching behavior induced by LDP to provide a novel form of privacy amplification that grows stronger on easier data, analogous to the shuffle model in offline learning. Drawing on the theory of random walks, we prove that this improvement carries essentially no utility cost. For RW-Meta, we develop a general method for privately selecting between experts that are themselves non-trivial learning algorithms, and we show that in the context of LDP this carries no extra privacy cost. In contrast, prior work has only considered data-independent experts. We also derive formal regret bounds that scale inversely with the degree of independence between experts. Our analysis is supplemented by evaluation on real-world data reported by hospitals during the COVID-19 pandemic; RW-Meta outperforms both the classical baseline and a state-of-the-art \textit{central} DP algorithm by 1.5-3$\times$ on the task of predicting which hospital will report the highest density of COVID patients each week.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-Driven Composite Neural Architecture Search for Multi-Source RL State Encoding</title>
<link>https://arxiv.org/abs/2512.06982</link>
<guid>https://arxiv.org/abs/2512.06982</guid>
<content:encoded><![CDATA[

arXiv:2512.06982v1 Announce Type: new 
Abstract: Designing state encoders for reinforcement learning (RL) with multiple information sources -- such as sensor measurements, time-series signals, image observations, and textual instructions -- remains underexplored and often requires manual design. We formalize this challenge as a problem of composite neural architecture search (NAS), where multiple source-specific modules and a fusion module are jointly optimized. Existing NAS methods overlook useful side information from the intermediate outputs of these modules -- such as their representation quality -- limiting sample efficiency in multi-source RL settings. To address this, we propose an LLM-driven NAS pipeline that leverages language-model priors and intermediate-output signals to guide sample-efficient search for high-performing composite state encoders. On a mixed-autonomy traffic control task, our approach discovers higher-performing architectures with fewer candidate evaluations than traditional NAS baselines and the LLM-based GENIUS framework.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OXtal: An All-Atom Diffusion Model for Organic Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.06987</link>
<guid>https://arxiv.org/abs/2512.06987</guid>
<content:encoded><![CDATA[

arXiv:2512.06987v1 Announce Type: new 
Abstract: Accurately predicting experimentally-realizable 3D molecular crystal structures from their 2D chemical graphs is a long-standing open challenge in computational chemistry called crystal structure prediction (CSP). Efficiently solving this problem has implications ranging from pharmaceuticals to organic semiconductors, as crystal packing directly governs the physical and chemical properties of organic solids. In this paper, we introduce OXtal, a large-scale 100M parameter all-atom diffusion model that directly learns the conditional joint distribution over intramolecular conformations and periodic packing. To efficiently scale OXtal, we abandon explicit equivariant architectures imposing inductive bias arising from crystal symmetries in favor of data augmentation strategies. We further propose a novel crystallization-inspired lattice-free training scheme, Stoichiometric Stochastic Shell Sampling ($S^4$), that efficiently captures long-range interactions while sidestepping explicit lattice parametrization -- thus enabling more scalable architectural choices at all-atom resolution. By leveraging a large dataset of 600K experimentally validated crystal structures (including rigid and flexible molecules, co-crystals, and solvates), OXtal achieves orders-of-magnitude improvements over prior ab initio machine learning CSP methods, while remaining orders of magnitude cheaper than traditional quantum-chemical approaches. Specifically, OXtal recovers experimental structures with conformer $\text{RMSD}_1<0.5$ {\AA} and attains over 80\% packing similarity rate, demonstrating its ability to model both thermodynamic and kinetic regularities of molecular crystallization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flash Multi-Head Feed-Forward Network</title>
<link>https://arxiv.org/abs/2512.06989</link>
<guid>https://arxiv.org/abs/2512.06989</guid>
<content:encoded><![CDATA[

arXiv:2512.06989v1 Announce Type: new 
Abstract: We explore Multi-Head FFN (MH-FFN) as a replacement of FFN in the Transformer architecture, motivated by the structural similarity between single-head attention and FFN. While multi-head mechanisms enhance expressivity in attention, naively applying them to FFNs faces two challenges: memory consumption scaling with the head count, and an imbalanced ratio between the growing intermediate size and the fixed head dimension as models scale, which degrades scalability and expressive power. To address these challenges, we propose Flash Multi-Head FFN (FlashMHF), with two key innovations: an I/O-aware fused kernel computing outputs online in SRAM akin to FlashAttention, and a design using dynamically weighted parallel sub-networks to maintain a balanced ratio between intermediate and head dimensions. Validated on models from 128M to 1.3B parameters, FlashMHF consistently improves perplexity and downstream task accuracy over SwiGLU FFNs, while reducing peak memory usage by 3-5x and accelerating inference by up to 1.08x. Our work establishes the multi-head design as a superior architectural principle for FFNs, presenting FlashMHF as a powerful, efficient, and scalable alternative to FFNs in Transformers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward Reliable Machine Unlearning: Theory, Algorithms, and Evaluation</title>
<link>https://arxiv.org/abs/2512.06993</link>
<guid>https://arxiv.org/abs/2512.06993</guid>
<content:encoded><![CDATA[

arXiv:2512.06993v1 Announce Type: new 
Abstract: We propose new methodologies for both unlearning random set of samples and class unlearning and show that they outperform existing methods. The main driver of our unlearning methods is the similarity of predictions to a retrained model on both the forget and remain samples. We introduce Adversarial Machine UNlearning (AMUN), which surpasses prior state-of-the-art methods for image classification based on SOTA MIA scores. AMUN lowers the model's confidence on forget samples by fine-tuning on their corresponding adversarial examples. Through theoretical analysis, we identify factors governing AMUN's performance, including smoothness. To facilitate training of smooth models with a controlled Lipschitz constant, we propose FastClip, a scalable method that performs layer-wise spectral-norm clipping of affine layers. In a separate study, we show that increased smoothness naturally improves adversarial example transfer, thereby supporting the second factor above.
  Following the same principles for class unlearning, we show that existing methods fail in replicating a retrained model's behavior by introducing a nearest-neighbor membership inference attack (MIA-NN) that uses the probabilities assigned to neighboring classes to detect unlearned samples and demonstrate the vulnerability of such methods. We then propose a fine-tuning objective that mitigates this leakage by approximating, for forget-class inputs, the distribution over remaining classes that a model retrained from scratch would produce. To construct this approximation, we estimate inter-class similarity and tilt the target model's distribution accordingly. The resulting Tilted ReWeighting(TRW) distribution serves as the desired target during fine-tuning. Across multiple benchmarks, TRW matches or surpasses existing unlearning methods on prior metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Always Keep Your Promises: DynamicLRP, A Model-Agnostic Solution To Layer-Wise Relevance Propagation</title>
<link>https://arxiv.org/abs/2512.07010</link>
<guid>https://arxiv.org/abs/2512.07010</guid>
<content:encoded><![CDATA[

arXiv:2512.07010v1 Announce Type: new 
Abstract: Layer-wise Relevance Propagation (LRP) provides principled attribution for neural networks through conservation properties and foundations in Deep Taylor Decomposition. However, existing implementations operate at the module level, requiring architecture-specific propagation rules and modifications. These limit the generality of target model and sustainability of implementations as architectures evolve. We introduce DynamicLRP, a model-agnostic LRP framework operating at the tensor operation level. By decomposing attribution to individual operations within computation graphs and introducing a novel mechanism for deferred activation resolution, named the Promise System, our approach achieves true architecture agnosticity while maintaining LRP's theoretical guarantees. This design operates independently of backpropagation machinery, enabling operation on arbitrary computation graphs without model modification and side-by-side execution with gradient backpropagation. Being based on computation graphs, this method is theoretically extensible to other deep learning libraries that support auto-differentiation. We demonstrate faithfulness matching or exceeding specialized implementations (1.77 vs 1.69 ABPC on VGG, equivalent performance on ViT, 93.70\% and 95.06\% top-1 attribution accuracy for explaining RoBERTa-large and Flan-T5-large answers on SQuADv2, respectively) while maintaining practical efficiency on models with hundreds of millions of parameters. We achieved 99.92\% node coverage across 31,465 computation graph nodes from 15 diverse architectures, including state-space models (Mamba), audio transformers (Whisper), and multimodal systems (DePlot) without any model-specific code with rules for 47 fundamental operations implemented. Our operation-level decomposition and Promise System establish a sustainable, extensible foundation for LRP across evolving architectures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Block Sparse Flash Attention</title>
<link>https://arxiv.org/abs/2512.07011</link>
<guid>https://arxiv.org/abs/2512.07011</guid>
<content:encoded><![CDATA[

arXiv:2512.07011v1 Announce Type: new 
Abstract: Modern large language models increasingly require long contexts for reasoning and multi-document tasks, but attention's quadratic complexity creates a severe computational bottleneck. We present Block-Sparse FlashAttention (BSFA), a drop-in replacement that accelerates long-context inference while preserving model quality. Unlike methods that predict importance before computing scores, BSFA computes exact query-key similarities to select the top-k most important value blocks for each query. By comparing per-block maximum scores against calibrated thresholds, we skip approximately 50% of the computation and memory transfers for pruned blocks. Our training-free approach requires only a one-time threshold calibration on a small dataset to learn the per-layer and per-head attention score distributions. We provide a CUDA kernel implementation that can be used as a drop-in replacement for FlashAttention. On Llama-3.1-8B, BSFA achieves up to 1.10x speedup on real-world reasoning benchmarks and up to 1.24x for needle-in-a-haystack retrieval tasks while maintaining above 99% baseline accuracy, with certain configurations even improving accuracy by focusing on the most relevant content, substantially outperforming existing sparse attention methods. The implementation is available at https://github.com/Danielohayon/Block-Sparse-Flash-Attention
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transferring Clinical Knowledge into ECGs Representation</title>
<link>https://arxiv.org/abs/2512.07021</link>
<guid>https://arxiv.org/abs/2512.07021</guid>
<content:encoded><![CDATA[

arXiv:2512.07021v1 Announce Type: new 
Abstract: Deep learning models have shown high accuracy in classifying electrocardiograms (ECGs), but their black box nature hinders clinical adoption due to a lack of trust and interpretability. To address this, we propose a novel three-stage training paradigm that transfers knowledge from multimodal clinical data (laboratory exams, vitals, biometrics) into a powerful, yet unimodal, ECG encoder. We employ a self-supervised, joint-embedding pre-training stage to create an ECG representation that is enriched with contextual clinical information, while only requiring the ECG signal at inference time. Furthermore, as an indirect way to explain the model's output we train it to also predict associated laboratory abnormalities directly from the ECG embedding. Evaluated on the MIMIC-IV-ECG dataset, our model outperforms a standard signal-only baseline in multi-label diagnosis classification and successfully bridges a substantial portion of the performance gap to a fully multimodal model that requires all data at inference. Our work demonstrates a practical and effective method for creating more accurate and trustworthy ECG classification models. By converting abstract predictions into physiologically grounded \emph{explanations}, our approach offers a promising path toward the safer integration of AI into clinical workflows.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transformation of Biological Networks into Images via Semantic Cartography for Visual Interpretation and Scalable Deep Analysis</title>
<link>https://arxiv.org/abs/2512.07040</link>
<guid>https://arxiv.org/abs/2512.07040</guid>
<content:encoded><![CDATA[

arXiv:2512.07040v1 Announce Type: new 
Abstract: Complex biological networks are fundamental to biomedical science, capturing interactions among molecules, cells, genes, and tissues. Deciphering these networks is critical for understanding health and disease, yet their scale and complexity represent a daunting challenge for current computational methods. Traditional biological network analysis methods, including deep learning approaches, while powerful, face inherent challenges such as limited scalability, oversmoothing long-range dependencies, difficulty in multimodal integration, expressivity bounds, and poor interpretability. We present Graph2Image, a framework that transforms large biological networks into sets of two-dimensional images by spatially arranging representative network nodes on a 2D grid. This transformation decouples the nodes as images, enabling the use of convolutional neural networks (CNNs) with global receptive fields and multi-scale pyramids, thus overcoming limitations of existing biological network analysis methods in scalability, memory efficiency, and long-range context capture. Graph2Image also facilitates seamless integration with other imaging and omics modalities and enhances interpretability through direct visualization of node-associated images. When applied to several large-scale biological network datasets, Graph2Image improved classification accuracy by up to 67.2% over existing methods and provided interpretable visualizations that revealed biologically coherent patterns. It also allows analysis of very large biological networks (nodes > 1 billion) on a personal computer. Graph2Image thus provides a scalable, interpretable, and multimodal-ready approach for biological network analysis, offering new opportunities for disease diagnosis and the study of complex biological systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning on Molecular Graphs: A Systematic Investigation of Masking Design</title>
<link>https://arxiv.org/abs/2512.07064</link>
<guid>https://arxiv.org/abs/2512.07064</guid>
<content:encoded><![CDATA[

arXiv:2512.07064v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) plays a central role in molecular representation learning. Yet, many recent innovations in masking-based pretraining are introduced as heuristics and lack principled evaluation, obscuring which design choices are genuinely effective. This work cast the entire pretrain-finetune workflow into a unified probabilistic framework, enabling a transparent comparison and deeper understanding of masking strategies. Building on this formalism, we conduct a controlled study of three core design dimensions: masking distribution, prediction target, and encoder architecture, under rigorously controlled settings. We further employ information-theoretic measures to assess the informativeness of pretraining signals and connect them to empirically benchmarked downstream performance. Our findings reveal a surprising insight: sophisticated masking distributions offer no consistent benefit over uniform sampling for common node-level prediction tasks. Instead, the choice of prediction target and its synergy with the encoder architecture are far more critical. Specifically, shifting to semantically richer targets yields substantial downstream improvements, particularly when paired with expressive Graph Transformer encoders. These insights offer practical guidance for developing more effective SSL methods for molecular graphs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Procrustean Bed for AI-Driven Retrosynthesis: A Unified Framework for Reproducible Evaluation</title>
<link>https://arxiv.org/abs/2512.07079</link>
<guid>https://arxiv.org/abs/2512.07079</guid>
<content:encoded><![CDATA[

arXiv:2512.07079v1 Announce Type: new 
Abstract: Progress in computer-aided synthesis planning (CASP) is obscured by the lack of standardized evaluation infrastructure and the reliance on metrics that prioritize topological completion over chemical validity. We introduce RetroCast, a unified evaluation suite that standardizes heterogeneous model outputs into a common schema to enable statistically rigorous, apples-to-apples comparison. The framework includes a reproducible benchmarking pipeline with stratified sampling and bootstrapped confidence intervals, accompanied by SynthArena, an interactive platform for qualitative route inspection. We utilize this infrastructure to evaluate leading search-based and sequence-based algorithms on a new suite of standardized benchmarks. Our analysis reveals a divergence between "solvability" (stock-termination rate) and route quality; high solvability scores often mask chemical invalidity or fail to correlate with the reproduction of experimental ground truths. Furthermore, we identify a "complexity cliff" in which search-based methods, despite high solvability rates, exhibit a sharp performance decay in reconstructing long-range synthetic plans compared to sequence-based approaches. We release the full framework, benchmark definitions, and a standardized database of model predictions to support transparent and reproducible development in the field.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACE: A Generalizable Drift Detector for Streaming Data-Driven Optimization</title>
<link>https://arxiv.org/abs/2512.07082</link>
<guid>https://arxiv.org/abs/2512.07082</guid>
<content:encoded><![CDATA[

arXiv:2512.07082v1 Announce Type: new 
Abstract: Many optimization tasks involve streaming data with unknown concept drifts, posing a significant challenge as Streaming Data-Driven Optimization (SDDO). Existing methods, while leveraging surrogate model approximation and historical knowledge transfer, are often under restrictive assumptions such as fixed drift intervals and fully environmental observability, limiting their adaptability to diverse dynamic environments. We propose TRACE, a TRAnsferable C}oncept-drift Estimator that effectively detects distributional changes in streaming data with varying time scales. TRACE leverages a principled tokenization strategy to extract statistical features from data streams and models drift patterns using attention-based sequence learning, enabling accurate detection on unseen datasets and highlighting the transferability of learned drift patterns. Further, we showcase TRACE's plug-and-play nature by integrating it into a streaming optimizer, facilitating adaptive optimization under unknown drifts. Comprehensive experimental results on diverse benchmarks demonstrate the superior generalization, robustness, and effectiveness of our approach in SDDO scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Persona: Disentangling Personality from Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07092</link>
<guid>https://arxiv.org/abs/2512.07092</guid>
<content:encoded><![CDATA[

arXiv:2512.07092v1 Announce Type: new 
Abstract: Background: The deployment of personalized Large Language Models (LLMs) is currently constrained by the stability-plasticity dilemma. Prevailing alignment methods, such as Supervised Fine-Tuning (SFT), rely on stochastic weight updates that often incur an "alignment tax" -- degrading general reasoning capabilities.
  Methods: We propose the Soul Engine, a framework based on the Linear Representation Hypothesis, which posits that personality traits exist as orthogonal linear subspaces. We introduce SoulBench, a dataset constructed via dynamic contextual sampling. Using a dual-head architecture on a frozen Qwen-2.5 base, we extract disentangled personality vectors without modifying the backbone weights.
  Results: Our experiments demonstrate three breakthroughs. First, High-Precision Profiling: The model achieves a Mean Squared Error (MSE) of 0.011 against psychological ground truth. Second, Geometric Orthogonality: T-SNE visualization confirms that personality manifolds are distinct and continuous, allowing for "Zero-Shot Personality Injection" that maintains original model intelligence. Third, Deterministic Steering: We achieve robust control over behavior via vector arithmetic, validated through extensive ablation studies.
  Conclusion: This work challenges the necessity of fine-tuning for personalization. By transitioning from probabilistic prompting to deterministic latent intervention, we provide a mathematically rigorous foundation for safe, controllable AI personalization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual Refinement Cycle Learning: Unsupervised Text Classification of Mamba and Community Detection on Text Attributed Graph</title>
<link>https://arxiv.org/abs/2512.07100</link>
<guid>https://arxiv.org/abs/2512.07100</guid>
<content:encoded><![CDATA[

arXiv:2512.07100v1 Announce Type: new 
Abstract: Pretrained language models offer strong text understanding capabilities but remain difficult to deploy in real-world text-attributed networks due to their heavy dependence on labeled data. Meanwhile, community detection methods typically ignore textual semantics, limiting their usefulness in downstream applications such as content organization, recommendation, and risk monitoring. To overcome these limitations, we present Dual Refinement Cycle Learning (DRCL), a fully unsupervised framework designed for practical scenarios where no labels or category definitions are available.
  DRCL integrates structural and semantic information through a warm-start initialization and a bidirectional refinement cycle between a GCN-based Community Detection Module (GCN-CDM) and a Text Semantic Modeling Module (TSMM). The two modules iteratively exchange pseudo-labels, allowing semantic cues to enhance structural clustering and structural patterns to guide text representation learning without manual supervision.
  Across several text-attributed graph datasets, DRCL consistently improves the structural and semantic quality of discovered communities. Moreover, a Mamba-based classifier trained solely from DRCL's community signals achieves accuracy comparable to supervised models, demonstrating its potential for deployment in large-scale systems where labeled data are scarce or costly.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOAM: Blocked State Folding for Memory-Efficient LLM Training</title>
<link>https://arxiv.org/abs/2512.07112</link>
<guid>https://arxiv.org/abs/2512.07112</guid>
<content:encoded><![CDATA[

arXiv:2512.07112v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated remarkable performance due to their large parameter counts and extensive training data. However, their scale leads to significant memory bottlenecks during training, especially when using memory-intensive optimizers like Adam. Existing memory-efficient approaches often rely on techniques such as singular value decomposition (SVD), projections, or weight freezing, which can introduce substantial computational overhead, require additional memory for projections, or degrade model performance. In this paper, we propose Folded Optimizer with Approximate Moment (FOAM), a method that compresses optimizer states by computing block-wise gradient means and incorporates a residual correction to recover lost information. Theoretically, FOAM achieves convergence rates equivalent to vanilla Adam under standard non-convex optimization settings. Empirically, FOAM reduces total training memory by approximately 50\%, eliminates up to 90\% of optimizer state memory overhead, and accelerates convergence. Furthermore, FOAM is compatible with other memory-efficient optimizers, delivering performance and throughput that match or surpass both full-rank and existing memory-efficient baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PlantBiMoE: A Bidirectional Foundation Model with SparseMoE for Plant Genomes</title>
<link>https://arxiv.org/abs/2512.07113</link>
<guid>https://arxiv.org/abs/2512.07113</guid>
<content:encoded><![CDATA[

arXiv:2512.07113v1 Announce Type: new 
Abstract: Understanding the underlying linguistic rules of plant genomes remains a fundamental challenge in computational biology. Recent advances including AgroNT and PDLLMs have made notable progress although, they suffer from excessive parameter size and limited ability to model the bidirectional nature of DNA strands respectively. To address these limitations, we propose PlantBiMoE, a lightweight and expressive plant genome language model that integrates bidirectional Mamba and a Sparse Mixture-of-Experts (SparseMoE) framework. The bidirectional Mamba enables the model to effectively capture structural dependencies across both the forward and reverse DNA strands, while SparseMoE significantly reduces the number of active parameters, improving computational efficiency without sacrificing modeling capacity. We evaluated and tested our model on the Modified Plants Genome Benchmark (MPGB), an enhanced genomic benchmark, which consolidates 31 datasets across 11 representative tasks, with input sequence lengths ranging from 50 to 6,000 bp. Experimental results demonstrate that PlantBiMoE achieves the best performance on 20 out of 31 datasets and the average best when comparing with existing models. In summary, all above results demonstrate that our model can effectively represent plant genomic sequences, serving as a robust computational tool for diverse genomic tasks, while making substantive contributions to plant genomics, gene editing, and synthetic biology. The code is available at: https://github.com/HUST-Keep-Lin/PlantBiMoE
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Winning the Lottery by Preserving Network Training Dynamics with Concrete Ticket Search</title>
<link>https://arxiv.org/abs/2512.07142</link>
<guid>https://arxiv.org/abs/2512.07142</guid>
<content:encoded><![CDATA[

arXiv:2512.07142v1 Announce Type: new 
Abstract: The Lottery Ticket Hypothesis asserts the existence of highly sparse, trainable subnetworks ('winning tickets') within dense, randomly initialized neural networks. However, state-of-the-art methods of drawing these tickets, like Lottery Ticket Rewinding (LTR), are computationally prohibitive, while more efficient saliency-based Pruning-at-Initialization (PaI) techniques suffer from a significant accuracy-sparsity trade-off and fail basic sanity checks. In this work, we argue that PaI's reliance on first-order saliency metrics, which ignore inter-weight dependencies, contributes substantially to this performance gap, especially in the sparse regime. To address this, we introduce Concrete Ticket Search (CTS), an algorithm that frames subnetwork discovery as a holistic combinatorial optimization problem. By leveraging a Concrete relaxation of the discrete search space and a novel gradient balancing scheme (GRADBALANCE) to control sparsity, CTS efficiently identifies high-performing subnetworks near initialization without requiring sensitive hyperparameter tuning. Motivated by recent works on lottery ticket training dynamics, we further propose a knowledge distillation-inspired family of pruning objectives, finding that minimizing the reverse Kullback-Leibler divergence between sparse and dense network outputs (CTS-KL) is particularly effective. Experiments on varying image classification tasks show that CTS produces subnetworks that robustly pass sanity checks and achieve accuracy comparable to or exceeding LTR, while requiring only a small fraction of the computation. For example, on ResNet-20 on CIFAR10, it reaches 99.3% sparsity with 74.0% accuracy in 7.9 minutes, while LTR attains the same sparsity with 68.3% accuracy in 95.2 minutes. CTS's subnetworks outperform saliency-based methods across all sparsities, but its advantage over LTR is most pronounced in the highly sparse regime.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowLPS: Langevin-Proximal Sampling for Flow-based Inverse Problem Solvers</title>
<link>https://arxiv.org/abs/2512.07150</link>
<guid>https://arxiv.org/abs/2512.07150</guid>
<content:encoded><![CDATA[

arXiv:2512.07150v1 Announce Type: new 
Abstract: Deep generative models have become powerful priors for solving inverse problems, and various training-free methods have been developed. However, when applied to latent flow models, existing methods often fail to converge to the posterior mode or suffer from manifold deviation within latent spaces. To mitigate this, here we introduce a novel training-free framework, FlowLPS, that solves inverse problems with pretrained flow models via a Langevin Proximal Sampling (LPS) strategy. Our method integrates Langevin dynamics for manifold-consistent exploration with proximal optimization for precise mode seeking, achieving a superior balance between reconstruction fidelity and perceptual quality across multiple inverse tasks on FFHQ and DIV2K, outperforming state of the art inverse solvers.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving the Throughput of Diffusion-based Large Language Models via a Training-Free Confidence-Aware Calibration</title>
<link>https://arxiv.org/abs/2512.07173</link>
<guid>https://arxiv.org/abs/2512.07173</guid>
<content:encoded><![CDATA[

arXiv:2512.07173v1 Announce Type: new 
Abstract: We present CadLLM, a training-free method to accelerate the inference throughput of diffusion-based LLMs (dLLMs). We first investigate the dynamic nature of token unmasking confidence across blocks and steps. Based on this observation, we present a lightweight adaptive approach that controls the generation block size, step size, and threshold based on the average confidence of unmasked tokens. We further reduce softmax overhead by dynamically leveraging a subset of the vocabulary to regulate sampling breadth. CadLLM is a plug-and-play, model-agnostic method compatible with KV-cache-based dLLMs. Extensive experiments on four popular tasks demonstrate that CadLLM yields up to 2.28x throughput improvement over the state-of-the-art baseline with competitive accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPACE: Noise Contrastive Estimation Stabilizes Self-Play Fine-Tuning for Large Language Models</title>
<link>https://arxiv.org/abs/2512.07175</link>
<guid>https://arxiv.org/abs/2512.07175</guid>
<content:encoded><![CDATA[

arXiv:2512.07175v1 Announce Type: new 
Abstract: Self-play fine-tuning has demonstrated promising abilities in adapting large language models (LLMs) to downstream tasks with limited real-world data. The basic principle is to iteratively refine the model with real samples and synthetic ones generated from itself. However, the existing methods primarily focus on the relative gaps between the rewards for two types of data, neglecting their absolute values. Through theoretical analysis, we identify that the gap-based methods suffer from unstable evolution, due to the potentially degenerated objectives. To address this limitation, we introduce a novel self-play fine-tuning method, namely Self-PlAy via Noise Contrastive Estimation (SPACE), which leverages noise contrastive estimation to capture the real-world data distribution. Specifically, SPACE treats synthetic samples as auxiliary components, and discriminates them from the real ones in a binary classification manner. As a result, SPACE independently optimizes the absolute reward values for each type of data, ensuring a consistently meaningful objective and thereby avoiding the instability issue. Theoretically, we show that the optimal solution of the objective in SPACE aligns with the underlying distribution of real-world data, and SPACE guarantees a provably stable convergence to the optimal distribution. Empirically, we show that SPACE significantly improves the performance of LLMs over various tasks, and outperforms supervised fine-tuning that employs much more real-world samples. Compared to gap-based self-play fine-tuning methods, SPACE exhibits remarkable superiority and stable evolution.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniDiff: A Unified Diffusion Framework for Multimodal Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07184</link>
<guid>https://arxiv.org/abs/2512.07184</guid>
<content:encoded><![CDATA[

arXiv:2512.07184v1 Announce Type: new 
Abstract: As multimodal data proliferates across diverse real-world applications, leveraging heterogeneous information such as texts and timestamps for accurate time series forecasting (TSF) has become a critical challenge. While diffusion models demonstrate exceptional performance in generation tasks, their application to TSF remains largely confined to modeling single-modality numerical sequences, overlooking the abundant cross-modal signals inherent in complex heterogeneous data. To address this gap, we propose UniDiff, a unified diffusion framework for multimodal time series forecasting. To process the numerical sequence, our framework first tokenizes the time series into patches, preserving local temporal dynamics by mapping each patch to an embedding space via a lightweight MLP. At its core lies a unified and parallel fusion module, where a single cross-attention mechanism adaptively weighs and integrates structural information from timestamps and semantic context from texts in one step, enabling a flexible and efficient interplay between modalities. Furthermore, we introduce a novel classifier-free guidance mechanism designed for multi-source conditioning, allowing for decoupled control over the guidance strength of textual and temporal information during inference, which significantly enhances model robustness. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed UniDiff model achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less is More: Non-uniform Road Segments are Efficient for Bus Arrival Prediction</title>
<link>https://arxiv.org/abs/2512.07200</link>
<guid>https://arxiv.org/abs/2512.07200</guid>
<content:encoded><![CDATA[

arXiv:2512.07200v1 Announce Type: new 
Abstract: In bus arrival time prediction, the process of organizing road infrastructure network data into homogeneous entities is known as segmentation. Segmenting a road network is widely recognized as the first and most critical step in developing an arrival time prediction system, particularly for auto-regressive-based approaches. Traditional methods typically employ a uniform segmentation strategy, which fails to account for varying physical constraints along roads, such as road conditions, intersections, and points of interest, thereby limiting prediction efficiency. In this paper, we propose a Reinforcement Learning (RL)-based approach to efficiently and adaptively learn non-uniform road segments for arrival time prediction. Our method decouples the prediction process into two stages: 1) Non-uniform road segments are extracted based on their impact scores using the proposed RL framework; and 2) A linear prediction model is applied to the selected segments to make predictions. This method ensures optimal segment selection while maintaining computational efficiency, offering a significant improvement over traditional uniform approaches. Furthermore, our experimental results suggest that the linear approach can even achieve better performance than more complex methods. Extensive experiments demonstrate the superiority of the proposed method, which not only enhances efficiency but also improves learning performance on large-scale benchmarks. The dataset and the code are publicly accessible at: https://github.com/pangjunbiao/Less-is-More.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Prior-Guided Federated Prompt Calibration</title>
<link>https://arxiv.org/abs/2512.07208</link>
<guid>https://arxiv.org/abs/2512.07208</guid>
<content:encoded><![CDATA[

arXiv:2512.07208v1 Announce Type: new 
Abstract: Federated Prompt Learning (FPL) offers a parameter-efficient solution for collaboratively training large models, but its performance is severely hindered by data heterogeneity, which causes locally trained prompts to become biased. Existing methods, focusing on aggregation or regularization, fail to address this root cause of local training bias. To this end, we propose Geometry-Guided Text Prompt Calibration (GGTPC), a novel framework that directly corrects this bias by providing clients with a global geometric prior. This prior, representing the shape of the global data distribution derived from the covariance matrix, is reconstructed on the server in a privacy-preserving manner. Clients then use a novel Geometry-Prior Calibration Layer (GPCL) to align their local feature distributions with this global prior during training. Extensive experiments show GGTPC's effectiveness. On the label-skewed CIFAR-100 dataset ($\beta$=0.1), it outperforms the state-of-the-art by 2.15\%. Under extreme skew ($\beta$=0.01), it improves upon the baseline by 9.17\%. Furthermore, as a plug-and-play module on the domain-skewed Office-Home dataset, it boosts FedAvg's performance by 4.60\%. These results demonstrate that GGTPC effectively mitigates data heterogeneity by correcting the fundamental local training bias, serving as a versatile module to enhance various FL algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pay Less Attention to Function Words for Free Robustness of Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07222</link>
<guid>https://arxiv.org/abs/2512.07222</guid>
<content:encoded><![CDATA[

arXiv:2512.07222v1 Announce Type: new 
Abstract: To address the trade-off between robustness and performance for robust VLM, we observe that function words could incur vulnerability of VLMs against cross-modal adversarial attacks, and propose Function-word De-Attention (FDA) accordingly to mitigate the impact of function words. Similar to differential amplifiers, our FDA calculates the original and the function-word cross-attention within attention heads, and differentially subtracts the latter from the former for more aligned and robust VLMs. Comprehensive experiments include 2 SOTA baselines under 6 different attacks on 2 downstream tasks, 3 datasets, and 3 models. Overall, our FDA yields an average 18/13/53% ASR drop with only 0.2/0.3/0.6% performance drops on the 3 tested models on retrieval, and a 90% ASR drop with a 0.3% performance gain on visual grounding. We demonstrate the scalability, generalization, and zero-shot performance of FDA experimentally, as well as in-depth ablation studies and analysis. Code will be made publicly at https://github.com/michaeltian108/FDA.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PINE: Pipeline for Important Node Exploration in Attributed Networks</title>
<link>https://arxiv.org/abs/2512.07244</link>
<guid>https://arxiv.org/abs/2512.07244</guid>
<content:encoded><![CDATA[

arXiv:2512.07244v1 Announce Type: new 
Abstract: A graph with semantically attributed nodes are a common data structure in a wide range of domains. It could be interlinked web data or citation networks of scientific publications. The essential problem for such a data type is to determine nodes that carry greater importance than all the others, a task that markedly enhances system monitoring and management. Traditional methods to identify important nodes in networks introduce centrality measures, such as node degree or more complex PageRank. However, they consider only the network structure, neglecting the rich node attributes. Recent methods adopt neural networks capable of handling node features, but they require supervision. This work addresses the identified gap--the absence of approaches that are both unsupervised and attribute-aware--by introducing a Pipeline for Important Node Exploration (PINE). At the core of the proposed framework is an attention-based graph model that incorporates node semantic features in the learning process of identifying the structural graph properties. The PINE's node importance scores leverage the obtained attention distribution. We demonstrate the superior performance of the proposed PINE method on various homogeneous and heterogeneous attributed networks. As an industry-implemented system, PINE tackles the real-world challenge of unsupervised identification of key entities within large-scale enterprise graphs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IFFair: Influence Function-driven Sample Reweighting for Fair Classification</title>
<link>https://arxiv.org/abs/2512.07249</link>
<guid>https://arxiv.org/abs/2512.07249</guid>
<content:encoded><![CDATA[

arXiv:2512.07249v1 Announce Type: new 
Abstract: Because machine learning has significantly improved efficiency and convenience in the society, it's increasingly used to assist or replace human decision-making. However, the data-based pattern makes related algorithms learn and even exacerbate potential bias in samples, resulting in discriminatory decisions against certain unprivileged groups, depriving them of the rights to equal treatment, thus damaging the social well-being and hindering the development of related applications. Therefore, we propose a pre-processing method IFFair based on the influence function. Compared with other fairness optimization approaches, IFFair only uses the influence disparity of training samples on different groups as a guidance to dynamically adjust the sample weights during training without modifying the network structure, data features and decision boundaries. To evaluate the validity of IFFair, we conduct experiments on multiple real-world datasets and metrics. The experimental results show that our approach mitigates bias of multiple accepted metrics in the classification setting, including demographic parity, equalized odds, equality of opportunity and error rate parity without conflicts. It also demonstrates that IFFair achieves better trade-off between multiple utility and fairness metrics compared with previous pre-processing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIT-Graph: State Integrated Tool Graph for Multi-Turn Agents</title>
<link>https://arxiv.org/abs/2512.07287</link>
<guid>https://arxiv.org/abs/2512.07287</guid>
<content:encoded><![CDATA[

arXiv:2512.07287v1 Announce Type: new 
Abstract: Despite impressive advances in agent systems, multi-turn tool-use scenarios remain challenging. It is mainly because intent is clarified progressively and the environment evolves with each tool call. While reusing past experience is natural, current LLM agents either treat entire trajectories or pre-defined subtasks as indivisible units, or solely exploit tool-to-tool dependencies, hindering adaptation as states and information evolve across turns. In this paper, we propose a State Integrated Tool Graph (SIT-Graph), which enhances multi-turn tool use by exploiting partially overlapping experience. Inspired by human decision-making that integrates episodic and procedural memory, SIT-Graph captures both compact state representations (episodic-like fragments) and tool-to-tool dependencies (procedural-like routines) from historical trajectories. Specifically, we first build a tool graph from accumulated tool-use sequences, and then augment each edge with a compact state summary of the dialog and tool history that may shape the next action. At inference time, SIT-Graph enables a human-like balance between episodic recall and procedural execution: when the next decision requires recalling prior context, the agent retrieves the state summaries stored on relevant edges and uses them to guide its next action; when the step is routine, it follows high-confidence tool dependencies without explicit recall. Experiments across multiple stateful multi-turn tool-use benchmarks show that SIT-Graph consistently outperforms strong memory- and graph-based baselines, delivering more robust tool selection and more effective experience transfer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Relationship-Aware Transformer for Tabular Data</title>
<link>https://arxiv.org/abs/2512.07310</link>
<guid>https://arxiv.org/abs/2512.07310</guid>
<content:encoded><![CDATA[

arXiv:2512.07310v1 Announce Type: new 
Abstract: Deep learning models for tabular data typically do not allow for imposing a graph of external dependencies between samples, which can be useful for accounting for relatedness in tasks such as treatment effect estimation. Graph neural networks only consider adjacent nodes, making them difficult to apply to sparse graphs. This paper proposes several solutions based on a modified attention mechanism, which accounts for possible relationships between data points by adding a term to the attention matrix. Our models are compared with each other and the gradient boosting decision trees in a regression task on synthetic and real-world datasets, as well as in a treatment effect estimation task on the IHDP dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning-Augmented Ski Rental with Discrete Distributions: A Bayesian Approach</title>
<link>https://arxiv.org/abs/2512.07313</link>
<guid>https://arxiv.org/abs/2512.07313</guid>
<content:encoded><![CDATA[

arXiv:2512.07313v1 Announce Type: new 
Abstract: We revisit the classic ski rental problem through the lens of Bayesian decision-making and machine-learned predictions. While traditional algorithms minimize worst-case cost without assumptions, and recent learning-augmented approaches leverage noisy forecasts with robustness guarantees, our work unifies these perspectives. We propose a discrete Bayesian framework that maintains exact posterior distributions over the time horizon, enabling principled uncertainty quantification and seamless incorporation of expert priors. Our algorithm achieves prior-dependent competitive guarantees and gracefully interpolates between worst-case and fully-informed settings. Our extensive experimental evaluation demonstrates superior empirical performance across diverse scenarios, achieving near-optimal results under accurate priors while maintaining robust worst-case guarantees. This framework naturally extends to incorporate multiple predictions, non-uniform priors, and contextual information, highlighting the practical advantages of Bayesian reasoning in online decision problems with imperfect predictions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Local-Curvature-Aware Knowledge Graph Embedding: An Extended Ricci Flow Approach</title>
<link>https://arxiv.org/abs/2512.07332</link>
<guid>https://arxiv.org/abs/2512.07332</guid>
<content:encoded><![CDATA[

arXiv:2512.07332v1 Announce Type: new 
Abstract: Knowledge graph embedding (KGE) relies on the geometry of the embedding space to encode semantic and structural relations. Existing methods place all entities on one homogeneous manifold, Euclidean, spherical, hyperbolic, or their product/multi-curvature variants, to model linear, symmetric, or hierarchical patterns. Yet a predefined, homogeneous manifold cannot accommodate the sharply varying curvature that real-world graphs exhibit across local regions. Since this geometry is imposed a priori, any mismatch with the knowledge graph's local curvatures will distort distances between entities and hurt the expressiveness of the resulting KGE. To rectify this, we propose RicciKGE to have the KGE loss gradient coupled with local curvatures in an extended Ricci flow such that entity embeddings co-evolve dynamically with the underlying manifold geometry towards mutual adaptation. Theoretically, when the coupling coefficient is bounded and properly selected, we rigorously prove that i) all the edge-wise curvatures decay exponentially, meaning that the manifold is driven toward the Euclidean flatness; and ii) the KGE distances strictly converge to a global optimum, which indicates that geometric flattening and embedding optimization are promoting each other. Experimental improvements on link prediction and node classification benchmarks demonstrate RicciKGE's effectiveness in adapting to heterogeneous knowledge graph structures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recover-to-Forget: Gradient Reconstruction from LoRA for Efficient LLM Unlearning</title>
<link>https://arxiv.org/abs/2512.07374</link>
<guid>https://arxiv.org/abs/2512.07374</guid>
<content:encoded><![CDATA[

arXiv:2512.07374v1 Announce Type: new 
Abstract: Unlearning in large foundation models (e.g., LLMs) is essential for enabling dynamic knowledge updates, enforcing data deletion rights, and correcting model behavior. However, existing unlearning methods often require full-model fine-tuning or access to the original training data, which limits their scalability and practicality. In this work, we introduce Recover-to-Forget (R2F), a novel framework for efficient unlearning in LLMs based on reconstructing full-model gradient directions from low-rank LoRA adapter updates. Rather than performing backpropagation through the full model, we compute gradients with respect to LoRA parameters using multiple paraphrased prompts and train a gradient decoder to approximate the corresponding full-model gradients. To ensure applicability to larger or black-box models, the decoder is trained on a proxy model and transferred to target models. We provide a theoretical analysis of cross-model generalization and demonstrate that our method achieves effective unlearning while preserving general model performance. Experimental results demonstrate that R2F offers a scalable and lightweight alternative for unlearning in pretrained LLMs without requiring full retraining or access to internal parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNE: Efficient LLM Unlearning via LoRA Fine-Tuning with Negative Examples</title>
<link>https://arxiv.org/abs/2512.07375</link>
<guid>https://arxiv.org/abs/2512.07375</guid>
<content:encoded><![CDATA[

arXiv:2512.07375v1 Announce Type: new 
Abstract: Large language models (LLMs) possess vast knowledge acquired from extensive training corpora, but they often cannot remove specific pieces of information when needed, which makes it hard to handle privacy, bias mitigation, and knowledge correction. Traditional model unlearning approaches require computationally expensive fine-tuning or direct weight editing, making them impractical for real-world deployment. In this work, we introduce LoRA-based Unlearning with Negative Examples (LUNE), a lightweight framework that performs negative-only unlearning by updating only low-rank adapters while freezing the backbone, thereby localizing edits and avoiding disruptive global changes. Leveraging Low-Rank Adaptation (LoRA), LUNE targets intermediate representations to suppress (or replace) requested knowledge with an order-of-magnitude lower compute and memory than full fine-tuning or direct weight editing. Extensive experiments on multiple factual unlearning tasks show that LUNE: (I) achieves effectiveness comparable to full fine-tuning and memory-editing methods, and (II) reduces computational cost by about an order of magnitude.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Reliable Test-Time Adaptation: Style Invariance as a Correctness Likelihood</title>
<link>https://arxiv.org/abs/2512.07390</link>
<guid>https://arxiv.org/abs/2512.07390</guid>
<content:encoded><![CDATA[

arXiv:2512.07390v1 Announce Type: new 
Abstract: Test-time adaptation (TTA) enables efficient adaptation of deployed models, yet it often leads to poorly calibrated predictive uncertainty - a critical issue in high-stakes domains such as autonomous driving, finance, and healthcare. Existing calibration methods typically assume fixed models or static distributions, resulting in degraded performance under real-world, dynamic test conditions. To address these challenges, we introduce Style Invariance as a Correctness Likelihood (SICL), a framework that leverages style-invariance for robust uncertainty estimation. SICL estimates instance-wise correctness likelihood by measuring prediction consistency across style-altered variants, requiring only the model's forward pass. This makes it a plug-and-play, backpropagation-free calibration module compatible with any TTA method. Comprehensive evaluations across four baselines, five TTA methods, and two realistic scenarios with three model architecture demonstrate that SICL reduces calibration error by an average of 13 percentage points compared to conventional calibration approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Empirical Results for Adjusting Truncated Backpropagation Through Time while Training Neural Audio Effects</title>
<link>https://arxiv.org/abs/2512.07393</link>
<guid>https://arxiv.org/abs/2512.07393</guid>
<content:encoded><![CDATA[

arXiv:2512.07393v1 Announce Type: new 
Abstract: This paper investigates the optimization of Truncated Backpropagation Through Time (TBPTT) for training neural networks in digital audio effect modeling, with a focus on dynamic range compression. The study evaluates key TBPTT hyperparameters -- sequence number, batch size, and sequence length -- and their influence on model performance. Using a convolutional-recurrent architecture, we conduct extensive experiments across datasets with and without conditionning by user controls. Results demonstrate that carefully tuning these parameters enhances model accuracy and training stability, while also reducing computational demands. Objective evaluations confirm improved performance with optimized settings, while subjective listening tests indicate that the revised TBPTT configuration maintains high perceptual quality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Asymptotic analysis of shallow and deep forgetting in replay with Neural Collapse</title>
<link>https://arxiv.org/abs/2512.07400</link>
<guid>https://arxiv.org/abs/2512.07400</guid>
<content:encoded><![CDATA[

arXiv:2512.07400v1 Announce Type: new 
Abstract: A persistent paradox in continual learning (CL) is that neural networks often retain linearly separable representations of past tasks even when their output predictions fail. We formalize this distinction as the gap between deep feature-space and shallow classifier-level forgetting. We reveal a critical asymmetry in Experience Replay: while minimal buffers successfully anchor feature geometry and prevent deep forgetting, mitigating shallow forgetting typically requires substantially larger buffer capacities. To explain this, we extend the Neural Collapse framework to the sequential setting. We characterize deep forgetting as a geometric drift toward out-of-distribution subspaces and prove that any non-zero replay fraction asymptotically guarantees the retention of linear separability. Conversely, we identify that the "strong collapse" induced by small buffers leads to rank-deficient covariances and inflated class means, effectively blinding the classifier to true population boundaries. By unifying CL with out-of-distribution detection, our work challenges the prevailing reliance on large buffers, suggesting that explicitly correcting these statistical artifacts could unlock robust performance with minimal replay.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Tuning of Parameterized Traffic Controllers via Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07417</link>
<guid>https://arxiv.org/abs/2512.07417</guid>
<content:encoded><![CDATA[

arXiv:2512.07417v1 Announce Type: new 
Abstract: Effective traffic control is essential for mitigating congestion in transportation networks. Conventional traffic management strategies, including route guidance, ramp metering, and traffic signal control, often rely on state feedback controllers, used for their simplicity and reactivity; however, they lack the adaptability required to cope with complex and time-varying traffic dynamics. This paper proposes a multi-agent reinforcement learning framework in which each agent adaptively tunes the parameters of a state feedback traffic controller, combining the reactivity of state feedback controllers with the adaptability of reinforcement learning. By tuning parameters at a lower frequency rather than directly determining control actions at a high frequency, the reinforcement learning agents achieve improved training efficiency while maintaining adaptability to varying traffic conditions. The multi-agent structure further enhances system robustness, as local controllers can operate independently in the event of partial failures. The proposed framework is evaluated on a simulated multi-class transportation network under varying traffic conditions. Results show that the proposed multi-agent framework outperforms the no control and fixed-parameter state feedback control cases, while performing on par with the single-agent RL-based adaptive state feedback control, with a much better resilience to partial failures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revolutionizing Mixed Precision Quantization: Towards Training-free Automatic Proxy Discovery via Large Language Models</title>
<link>https://arxiv.org/abs/2512.07419</link>
<guid>https://arxiv.org/abs/2512.07419</guid>
<content:encoded><![CDATA[

arXiv:2512.07419v1 Announce Type: new 
Abstract: Mixed-Precision Quantization (MPQ) liberates the Deep Neural Networks (DNNs) from the Out-Of-Memory (OOM) bottleneck, which garnered increasing research attention. However, conventional methods either searched from costly differentiable optimization, which is neither efficient nor flexible, or learned a quantized DNN from the proxy (i.e., HAWQ) manually designed by human experts, which is labor-intensive and requires huge expert knowledge. Can we design a proxy without involving any human experts and training? In this paper, we provide an affirmative answer by proposing a novel Large Language Models (LLMs)-driven Training-free Automatic Proxy (dubbed TAP) discovery framework, which reforms the design paradigm of MPQ by utilizing LLMs to find superior TAP tailored for MPQ, automatically. In addition, to bridge the gap between black-box LLMs and the tough MPQ task, we ingeniously propose simple Direct Policy Optimization (DPO) based reinforcement learning to enhance LLMs' reasoning by optimizing prompts, which can construct a positive feedback loop between the LLM and the MPQ task, enabling LLMs to generate better TAP in the next evolution. Extensive experiments on mainstream benchmarks demonstrate that TAP achieves state-of-the-art performance. Finally, we truly believe that our TAP will significantly contribute to the MPQ community by providing a new perspective on LLM-driven design algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MIDG: Mixture of Invariant Experts with knowledge injection for Domain Generalization in Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.07430</link>
<guid>https://arxiv.org/abs/2512.07430</guid>
<content:encoded><![CDATA[

arXiv:2512.07430v1 Announce Type: new 
Abstract: Existing methods in domain generalization for Multimodal Sentiment Analysis (MSA) often overlook inter-modal synergies during invariant features extraction, which prevents the accurate capture of the rich semantic information within multimodal data. Additionally, while knowledge injection techniques have been explored in MSA, they often suffer from fragmented cross-modal knowledge, overlooking specific representations that exist beyond the confines of unimodal. To address these limitations, we propose a novel MSA framework designed for domain generalization. Firstly, the framework incorporates a Mixture of Invariant Experts model to extract domain-invariant features, thereby enhancing the model's capacity to learn synergistic relationships between modalities. Secondly, we design a Cross-Modal Adapter to augment the semantic richness of multimodal representations through cross-modal knowledge injection. Extensive domain experiments conducted on three datasets demonstrate that the proposed MIDG achieves superior performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Bias in Graph Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2512.07433</link>
<guid>https://arxiv.org/abs/2512.07433</guid>
<content:encoded><![CDATA[

arXiv:2512.07433v1 Announce Type: new 
Abstract: Graph hyperdimensional computing (HDC) has emerged as a promising paradigm for cognitive tasks, emulating brain-like computation with high-dimensional vectors known as hypervectors. While HDC offers robustness and efficiency on graph-structured data, its fairness implications remain largely unexplored. In this paper, we study fairness in graph HDC, where biases in data representation and decision rules can lead to unequal treatment of different groups. We show how hypervector encoding and similarity-based classification can propagate or even amplify such biases, and we propose a fairness-aware training framework, FairGHDC, to mitigate them. FairGHDC introduces a bias correction term, derived from a gap-based demographic-parity regularizer, and converts it into a scalar fairness factor that scales the update of the class hypervector for the ground-truth label. This enables debiasing directly in the hypervector space without modifying the graph encoder or requiring backpropagation. Experimental results on six benchmark datasets demonstrate that FairGHDC substantially reduces demographic-parity and equal-opportunity gaps while maintaining accuracy comparable to standard GNNs and fairness-aware GNNs. At the same time, FairGHDC preserves the computational advantages of HDC, achieving up to about one order of magnitude ($\approx 10\times$) speedup in training time on GPU compared to GNN and fairness-aware GNN baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KAN-Dreamer: Benchmarking Kolmogorov-Arnold Networks as Function Approximators in World Models</title>
<link>https://arxiv.org/abs/2512.07437</link>
<guid>https://arxiv.org/abs/2512.07437</guid>
<content:encoded><![CDATA[

arXiv:2512.07437v1 Announce Type: new 
Abstract: DreamerV3 is a state-of-the-art online model-based reinforcement learning (MBRL) algorithm known for remarkable sample efficiency. Concurrently, Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to Multi-Layer Perceptrons (MLPs), offering superior parameter efficiency and interpretability. To mitigate KANs' computational overhead, variants like FastKAN leverage Radial Basis Functions (RBFs) to accelerate inference. In this work, we investigate integrating KAN architectures into the DreamerV3 framework. We introduce KAN-Dreamer, replacing specific MLP and convolutional components of DreamerV3 with KAN and FastKAN layers. To ensure efficiency within the JAX-based World Model, we implement a tailored, fully vectorized version with simplified grid management. We structure our investigation into three subsystems: Visual Perception, Latent Prediction, and Behavior Learning. Empirical evaluations on the DeepMind Control Suite (walker_walk) analyze sample efficiency, training time, and asymptotic performance. Experimental results demonstrate that utilizing our adapted FastKAN as a drop-in replacement for the Reward and Continue predictors yields performance on par with the original MLP-based architecture, maintaining parity in both sample efficiency and training speed. This report serves as a preliminary study for future developments in KAN-based world models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forget and Explain: Transparent Verification of GNN Unlearning</title>
<link>https://arxiv.org/abs/2512.07450</link>
<guid>https://arxiv.org/abs/2512.07450</guid>
<content:encoded><![CDATA[

arXiv:2512.07450v1 Announce Type: new 
Abstract: Graph neural networks (GNNs) are increasingly used to model complex patterns in graph-structured data. However, enabling them to "forget" designated information remains challenging, especially under privacy regulations such as the GDPR. Existing unlearning methods largely optimize for efficiency and scalability, yet they offer little transparency, and the black-box nature of GNNs makes it difficult to verify whether forgetting has truly occurred. We propose an explainability-driven verifier for GNN unlearning that snapshots the model before and after deletion, using attribution shifts and localized structural changes (for example, graph edit distance) as transparent evidence. The verifier uses five explainability metrics: residual attribution, heatmap shift, explainability score deviation, graph edit distance, and a diagnostic graph rule shift. We evaluate two backbones (GCN, GAT) and four unlearning strategies (Retrain, GraphEditor, GNNDelete, IDEA) across five benchmarks (Cora, Citeseer, Pubmed, Coauthor-CS, Coauthor-Physics). Results show that Retrain and GNNDelete achieve near-complete forgetting, GraphEditor provides partial erasure, and IDEA leaves residual signals. These explanation deltas provide the primary, human-readable evidence of forgetting; we also report membership-inference ROC-AUC as a complementary, graph-wide privacy signal.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Algorithms for Combined Regularized Support Vector Machines: Application in Music Genre Classification</title>
<link>https://arxiv.org/abs/2512.07463</link>
<guid>https://arxiv.org/abs/2512.07463</guid>
<content:encoded><![CDATA[

arXiv:2512.07463v1 Announce Type: new 
Abstract: In the era of rapid development of artificial intelligence, its applications span across diverse fields, relying heavily on effective data processing and model optimization. Combined Regularized Support Vector Machines (CR-SVMs) can effectively handle the structural information among data features, but there is a lack of efficient algorithms in distributed-stored big data. To address this issue, we propose a unified optimization framework based on consensus structure. This framework is not only applicable to various loss functions and combined regularization terms but can also be effectively extended to non-convex regularization terms, showing strong scalability. Based on this framework, we develop a distributed parallel alternating direction method of multipliers (ADMM) algorithm to efficiently compute CR-SVMs when data is stored in a distributed manner. To ensure the convergence of the algorithm, we also introduce the Gaussian back-substitution method. Meanwhile, for the integrity of the paper, we introduce a new model, the sparse group lasso support vector machine (SGL-SVM), and apply it to music information retrieval. Theoretical analysis confirms that the computational complexity of the proposed algorithm is not affected by different regularization terms and loss functions, highlighting the universality of the parallel algorithm. Experiments on synthetic and free music archiv datasets demonstrate the reliability, stability, and efficiency of the algorithm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Materium: An Autoregressive Approach for Material Generation</title>
<link>https://arxiv.org/abs/2512.07486</link>
<guid>https://arxiv.org/abs/2512.07486</guid>
<content:encoded><![CDATA[

arXiv:2512.07486v1 Announce Type: new 
Abstract: We present Materium: an autoregressive transformer for generating crystal structures that converts 3D material representations into token sequences. These sequences include elements with oxidation states, fractional coordinates and lattice parameters. Unlike diffusion approaches, which refine atomic positions iteratively through many denoising steps, Materium places atoms at precise fractional coordinates, enabling fast, scalable generation. With this design, the model can be trained in a few hours on a single GPU and generate samples much faster on GPUs and CPUs than diffusion-based approaches. The model was trained and evaluated using multiple properties as conditions, including fundamental properties, such as density and space group, as well as more practical targets, such as band gap and magnetic density. In both single and combined conditions, the model performs consistently well, producing candidates that align with the requested inputs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Low-Tubal-Rank Tensor Estimation via Alternating Preconditioned Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07490</link>
<guid>https://arxiv.org/abs/2512.07490</guid>
<content:encoded><![CDATA[

arXiv:2512.07490v1 Announce Type: new 
Abstract: The problem of low-tubal-rank tensor estimation is a fundamental task with wide applications across high-dimensional signal processing, machine learning, and image science. Traditional approaches tackle such a problem by performing tensor singular value decomposition, which is computationally expensive and becomes infeasible for large-scale tensors. Recent approaches address this issue by factorizing the tensor into two smaller factor tensors and solving the resulting problem using gradient descent. However, this kind of approach requires an accurate estimate of the tensor rank, and when the rank is overestimated, the convergence of gradient descent and its variants slows down significantly or even diverges. To address this problem, we propose an Alternating Preconditioned Gradient Descent (APGD) algorithm, which accelerates convergence in the over-parameterized setting by adding a preconditioning term to the original gradient and updating these two factors alternately. Based on certain geometric assumptions on the objective function, we establish linear convergence guarantees for more general low-tubal-rank tensor estimation problems. Then we further analyze the specific cases of low-tubal-rank tensor factorization and low-tubal-rank tensor recovery. Our theoretical results show that APGD achieves linear convergence even under over-parameterization, and the convergence rate is independent of the tensor condition number. Extensive simulations on synthetic data are carried out to validate our theoretical assertions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring possible vector systems for faster training of neural networks with preconfigured latent spaces</title>
<link>https://arxiv.org/abs/2512.07509</link>
<guid>https://arxiv.org/abs/2512.07509</guid>
<content:encoded><![CDATA[

arXiv:2512.07509v1 Announce Type: new 
Abstract: The overall neural network (NN) performance is closely related to the properties of its embedding distribution in latent space (LS). It has recently been shown that predefined vector systems, specifically An root system vectors, can be used as targets for latent space configurations (LSC) to ensure the desired LS structure. One of the main LSC advantage is the possibility of training classifier NNs without classification layers, which facilitates training NNs on datasets with extremely large numbers of classes. This paper provides a more general overview of possible vector systems for NN training along with their properties and methods for vector system construction. These systems are used to configure LS of encoders and visual transformers to significantly speed up ImageNet-1K and 50k-600k classes LSC training. It is also shown that using the minimum number of LS dimensions for a specific number of classes results in faster convergence. The latter has potential advantages for reducing the size of vector databases used to store NN embeddings.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning: Progress and Prospects</title>
<link>https://arxiv.org/abs/2512.07519</link>
<guid>https://arxiv.org/abs/2512.07519</guid>
<content:encoded><![CDATA[

arXiv:2512.07519v1 Announce Type: new 
Abstract: This Inaugural Lecture was given at Royal Holloway University of London in 1996. It covers an introduction to machine learning and describes various theoretical advances and practical projects in the field. The Lecture here is presented in its original format, but a few remarks have been added in 2025 to reflect recent developments, and the list of references has been updated to enhance the convenience and accuracy for readers.
  When did machine learning start? Maybe a good starting point is 1949, when Claude Shannon proposed a learning algorithm for chess-playing programs. Or maybe we should go back to the 1930s when Ronald Fisher developed discriminant analysis - a type of learning where the problem is to construct a decision rule that separates two types of vectors. Or could it be the 18th century when David Hume discussed the idea of induction? Or the 14th century, when William of Ockham formulated the principle of "simplicity" known as "Ockham's razor" (Ockham, by the way, is a small village not far from Royal Holloway). Or it may be that, like almost everything else in Western civilisation and culture, the origin of these ideas lies in the Mediterranean. After all, it was Aristotle who said that "we learn some things only by doing things".
  The field of machine learning has been greatly influenced by other disciplines and the subject is in itself not a very homogeneous discipline, but includes separate, overlapping subfields. There are many parallel lines of research in ML: inductive learning, neural networks, clustering, and theories of learning. They are all part of the more general field of machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Based Reinforcement Learning Under Confounding</title>
<link>https://arxiv.org/abs/2512.07528</link>
<guid>https://arxiv.org/abs/2512.07528</guid>
<content:encoded><![CDATA[

arXiv:2512.07528v1 Announce Type: new 
Abstract: We investigate model-based reinforcement learning in contextual Markov decision processes (C-MDPs) in which the context is unobserved and induces confounding in the offline dataset. In such settings, conventional model-learning methods are fundamentally inconsistent, as the transition and reward mechanisms generated under a behavioral policy do not correspond to the interventional quantities required for evaluating a state-based policy. To address this issue, we adapt a proximal off-policy evaluation approach that identifies the confounded reward expectation using only observable state-action-reward trajectories under mild invertibility conditions on proxy variables. When combined with a behavior-averaged transition model, this construction yields a surrogate MDP whose Bellman operator is well defined and consistent for state-based policies, and which integrates seamlessly with the maximum causal entropy (MaxCausalEnt) model-learning framework. The proposed formulation enables principled model learning and planning in confounded environments where contextual information is unobserved, unavailable, or impractical to collect.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FRWKV:Frequency-Domain Linear Attention for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07539</link>
<guid>https://arxiv.org/abs/2512.07539</guid>
<content:encoded><![CDATA[

arXiv:2512.07539v1 Announce Type: new 
Abstract: Traditional Transformers face a major bottleneck in long-sequence time series forecasting due to their quadratic complexity $(\mathcal{O}(T^2))$ and their limited ability to effectively exploit frequency-domain information. Inspired by RWKV's $\mathcal{O}(T)$ linear attention and frequency-domain modeling, we propose FRWKV, a frequency-domain linear-attention framework that overcomes these limitations. Our model integrates linear attention mechanisms with frequency-domain analysis, achieving $\mathcal{O}(T)$ computational complexity in the attention path while exploiting spectral information to enhance temporal feature representations for scalable long-sequence modeling. Across eight real-world datasets, FRWKV achieves a first-place average rank. Our ablation studies confirm the critical roles of both the linear attention and frequency-encoder components. This work demonstrates the powerful synergy between linear attention and frequency analysis, establishing a new paradigm for scalable time series modeling. Code is available at this repository: https://github.com/yangqingyuan-byte/FRWKV.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RRAEDy: Adaptive Latent Linearization of Nonlinear Dynamical Systems</title>
<link>https://arxiv.org/abs/2512.07542</link>
<guid>https://arxiv.org/abs/2512.07542</guid>
<content:encoded><![CDATA[

arXiv:2512.07542v1 Announce Type: new 
Abstract: Most existing latent-space models for dynamical systems require fixing the latent dimension in advance, they rely on complex loss balancing to approximate linear dynamics, and they don't regularize the latent variables. We introduce RRAEDy, a model that removes these limitations by discovering the appropriate latent dimension, while enforcing both regularized and linearized dynamics in the latent space. Built upon Rank-Reduction Autoencoders (RRAEs), RRAEDy automatically rank and prune latent variables through their singular values while learning a latent Dynamic Mode Decomposition (DMD) operator that governs their temporal progression. This structure-free yet linearly constrained formulation enables the model to learn stable and low-dimensional dynamics without auxiliary losses or manual tuning. We provide theoretical analysis demonstrating the stability of the learned operator and showcase the generality of our model by proposing an extension that handles parametric ODEs. Experiments on canonical benchmarks, including the Van der Pol oscillator, Burgers' equation, 2D Navier-Stokes, and Rotating Gaussians, show that RRAEDy achieves accurate and robust predictions. Our code is open-source and available at https://github.com/JadM133/RRAEDy. We also provide a video summarizing the main results at https://youtu.be/ox70mSSMGrM.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReLaX: Reasoning with Latent Exploration for Large Reasoning Models</title>
<link>https://arxiv.org/abs/2512.07558</link>
<guid>https://arxiv.org/abs/2512.07558</guid>
<content:encoded><![CDATA[

arXiv:2512.07558v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated remarkable potential in enhancing the reasoning capability of Large Reasoning Models (LRMs). However, RLVR often leads to entropy collapse, resulting in premature policy convergence and performance saturation. While manipulating token-level entropy has proven effective for promoting policy exploration, we argue that the latent dynamics underlying token generation encode a far richer computational structure for steering policy optimization toward a more effective exploration-exploitation tradeoff. To enable tractable analysis and intervention of the latent dynamics of LRMs, we leverage Koopman operator theory to obtain a linearized representation of their hidden-state dynamics. This enables us to introduce Dynamic Spectral Dispersion (DSD), a new metric to quantify the heterogeneity of the model's latent dynamics, serving as a direct indicator of policy exploration. Building upon these foundations, we propose Reasoning with Latent eXploration (ReLaX), a paradigm that explicitly incorporates latent dynamics to regulate exploration and exploitation during policy optimization. Comprehensive experiments across a wide range of multimodal and text-only reasoning benchmarks show that ReLaX significantly mitigates premature convergence and consistently achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Weighted Contrastive Learning for Anomaly-Aware Time-Series Forecasting</title>
<link>https://arxiv.org/abs/2512.07569</link>
<guid>https://arxiv.org/abs/2512.07569</guid>
<content:encoded><![CDATA[

arXiv:2512.07569v1 Announce Type: new 
Abstract: Reliable forecasting of multivariate time series under anomalous conditions is crucial in applications such as ATM cash logistics, where sudden demand shifts can disrupt operations. Modern deep forecasters achieve high accuracy on normal data but often fail when distribution shifts occur. We propose Weighted Contrastive Adaptation (WECA), a Weighted contrastive objective that aligns normal and anomaly-augmented representations, preserving anomaly-relevant information while maintaining consistency under benign variations. Evaluations on a nationwide ATM transaction dataset with domain-informed anomaly injection show that WECA improves SMAPE on anomaly-affected data by 6.1 percentage points compared to a normally trained baseline, with negligible degradation on normal data. These results demonstrate that WECA enhances forecasting reliability under anomalies without sacrificing performance during regular operations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time Series Foundation Models for Process Model Forecasting</title>
<link>https://arxiv.org/abs/2512.07624</link>
<guid>https://arxiv.org/abs/2512.07624</guid>
<content:encoded><![CDATA[

arXiv:2512.07624v1 Announce Type: new 
Abstract: Process Model Forecasting (PMF) aims to predict how the control-flow structure of a process evolves over time by modeling the temporal dynamics of directly-follows (DF) relations, complementing predictive process monitoring that focuses on single-case prefixes. Prior benchmarks show that machine learning and deep learning models provide only modest gains over statistical baselines, mainly due to the sparsity and heterogeneity of the DF time series. We investigate Time Series Foundation Models (TSFMs), large pre-trained models for generic time series, as an alternative for PMF. Using DF time series derived from real-life event logs, we compare zero-shot use of TSFMs, without additional training, with fine-tuned variants adapted on PMF-specific data. TSFMs generally achieve lower forecasting errors (MAE and RMSE) than traditional and specialized models trained from scratch on the same logs, indicating effective transfer of temporal structure from non-process domains. While fine-tuning can further improve accuracy, the gains are often small and may disappear on smaller or more complex datasets, so zero-shot use remains a strong default. Our study highlights the generalization capability and data efficiency of TSFMs for process-related time series and, to the best of our knowledge, provides the first systematic evaluation of temporal foundation models for PMF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Mathematical Theory of Top-$k$ Sparse Attention via Total Variation Distance</title>
<link>https://arxiv.org/abs/2512.07647</link>
<guid>https://arxiv.org/abs/2512.07647</guid>
<content:encoded><![CDATA[

arXiv:2512.07647v1 Announce Type: new 
Abstract: We develop a unified mathematical framework for certified Top-$k$ attention truncation that quantifies approximation error at both the distribution and output levels. For a single attention distribution $P$ and its Top-$k$ truncation $\hat P$, we show that the total-variation distance coincides with the discarded softmax tail mass and satisfies $\mathrm{TV}(P,\hat P)=1-e^{-\mathrm{KL}(\hat P\Vert P)}$, yielding sharp Top-$k$-specific bounds in place of generic inequalities. From this we derive non-asymptotic deterministic bounds -- from a single boundary gap through multi-gap and blockwise variants -- that control $\mathrm{TV}(P,\hat P)$ using only the ordered logits. Using an exact head-tail decomposition, we prove that the output error factorizes as $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2=\tau\|\mu_{\mathrm{tail}}-\mu_{\mathrm{head}}\|_2$ with $\tau=\mathrm{TV}(P,\hat P)$, yielding a new head-tail diameter bound $\|\mathrm{Attn}(q,K,V)-\mathrm{Attn}_k(q,K,V)\|_2\le\tau\,\mathrm{diam}_{H,T}$ and refinements linking the error to $\mathrm{Var}_P(V)$. Under an i.i.d. Gaussian score model $s_i\sim\mathcal N(\mu,\sigma^2)$ we derive closed-form tail masses and an asymptotic rule for the minimal $k_\varepsilon$ ensuring $\mathrm{TV}(P,\hat P)\le\varepsilon$, namely $k_\varepsilon/n\approx\Phi_c(\sigma+\Phi^{-1}(\varepsilon))$. Experiments on bert-base-uncased and synthetic logits confirm the predicted scaling of $k_\varepsilon/n$ and show that certified Top-$k$ can reduce scored keys by 2-4$\times$ on average while meeting the prescribed total-variation budget.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Depth-Wise Activation Steering for Honest Language Models</title>
<link>https://arxiv.org/abs/2512.07667</link>
<guid>https://arxiv.org/abs/2512.07667</guid>
<content:encoded><![CDATA[

arXiv:2512.07667v1 Announce Type: new 
Abstract: Large language models sometimes assert falsehoods despite internally representing the correct answer, failures of honesty rather than accuracy, which undermines auditability and safety. Existing approaches largely optimize factual correctness or depend on retraining and brittle single-layer edits, offering limited leverage over truthful reporting. We present a training-free activation steering method that weights steering strength across network depth using a Gaussian schedule. On the MASK benchmark, which separates honesty from knowledge, we evaluate seven models spanning the LLaMA, Qwen, and Mistral families and find that Gaussian scheduling improves honesty over no-steering and single-layer baselines in six of seven models. Equal-budget ablations on LLaMA-3.1-8B-Instruct and Qwen-2.5-7B-Instruct show the Gaussian schedule outperforms random, uniform, and box-filter depth allocations, indicating that how intervention is distributed across depth materially affects outcomes beyond total strength. The method is simple, model-agnostic, requires no finetuning, and provides a low-cost control knob for eliciting truthful reporting from models' existing capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Bootstrap Perspective on Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2512.07676</link>
<guid>https://arxiv.org/abs/2512.07676</guid>
<content:encoded><![CDATA[

arXiv:2512.07676v1 Announce Type: new 
Abstract: Machine learning models trained with \emph{stochastic} gradient descent (SGD) can generalize better than those trained with deterministic gradient descent (GD). In this work, we study SGD's impact on generalization through the lens of the statistical bootstrap: SGD uses gradient variability under batch sampling as a proxy for solution variability under the randomness of the data collection process. We use empirical results and theoretical analysis to substantiate this claim. In idealized experiments on empirical risk minimization, we show that SGD is drawn to parameter choices that are robust under resampling and thus avoids spurious solutions even if they lie in wider and deeper minima of the training loss. We prove rigorously that by implicitly regularizing the trace of the gradient covariance matrix, SGD controls the algorithmic variability. This regularization leads to solutions that are less sensitive to sampling noise, thereby improving generalization. Numerical experiments on neural network training show that explicitly incorporating the estimate of the algorithmic variability as a regularizer improves test performance. This fact supports our claim that bootstrap estimation underpins SGD's generalization advantages.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context and Few-Shots Learning for Forecasting Time Series Data based on Large Language Models</title>
<link>https://arxiv.org/abs/2512.07705</link>
<guid>https://arxiv.org/abs/2512.07705</guid>
<content:encoded><![CDATA[

arXiv:2512.07705v1 Announce Type: new 
Abstract: Existing data-driven approaches in modeling and predicting time series data include ARIMA (Autoregressive Integrated Moving Average), Transformer-based models, LSTM (Long Short-Term Memory) and TCN (Temporal Convolutional Network). These approaches, and in particular deep learning-based models such as LSTM and TCN, have shown great results in predicting time series data. With the advancement of leveraging pre-trained foundation models such as Large Language Models (LLMs) and more notably Google's recent foundation model for time series data, {\it TimesFM} (Time Series Foundation Model), it is of interest to investigate whether these foundation models have the capability of outperforming existing modeling approaches in analyzing and predicting time series data.
  This paper investigates the performance of using LLM models for time series data prediction. We investigate the in-context learning methodology in the training of LLM models that are specific to the underlying application domain. More specifically, the paper explores training LLMs through in-context, zero-shot and few-shot learning and forecasting time series data with OpenAI {\tt o4-mini} and Gemini 2.5 Flash Lite, as well as the recent Google's Transformer-based TimesFM, a time series-specific foundation model, along with two deep learning models, namely TCN and LSTM networks. The findings indicate that TimesFM has the best overall performance with the lowest RMSE value (0.3023) and the competitive inference time (266 seconds). Furthermore, OpenAI's o4-mini also exhibits a good performance based on Zero Shot learning.
  These findings highlight pre-trained time series foundation models as a promising direction for real-time forecasting, enabling accurate and scalable deployment with minimal model adaptation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Delayed-Full Charging Through Transformer-Based Real-Time-to-Departure Modeling for EV Battery Longevity</title>
<link>https://arxiv.org/abs/2512.07723</link>
<guid>https://arxiv.org/abs/2512.07723</guid>
<content:encoded><![CDATA[

arXiv:2512.07723v1 Announce Type: new 
Abstract: Electric vehicles (EVs) are key to sustainable mobility, yet their lithium-ion batteries (LIBs) degrade more rapidly under prolonged high states of charge (SOC). This can be mitigated by delaying full charging \ours until just before departure, which requires accurate prediction of user departure times. In this work, we propose Transformer-based real-time-to-event (TTE) model for accurate EV departure prediction. Our approach represents each day as a TTE sequence by discretizing time into grid-based tokens. Unlike previous methods primarily dependent on temporal dependency from historical patterns, our method leverages streaming contextual information to predict departures. Evaluation on a real-world study involving 93 users and passive smartphone data demonstrates that our method effectively captures irregular departure patterns within individual routines, outperforming baseline models. These results highlight the potential for practical deployment of the \ours algorithm and its contribution to sustainable transportation systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A multimodal Bayesian Network for symptom-level depression and anxiety prediction from voice and speech data</title>
<link>https://arxiv.org/abs/2512.07741</link>
<guid>https://arxiv.org/abs/2512.07741</guid>
<content:encoded><![CDATA[

arXiv:2512.07741v1 Announce Type: new 
Abstract: During psychiatric assessment, clinicians observe not only what patients report, but important nonverbal signs such as tone, speech rate, fluency, responsiveness, and body language. Weighing and integrating these different information sources is a challenging task and a good candidate for support by intelligence-driven tools - however this is yet to be realized in the clinic. Here, we argue that several important barriers to adoption can be addressed using Bayesian network modelling. To demonstrate this, we evaluate a model for depression and anxiety symptom prediction from voice and speech features in large-scale datasets (30,135 unique speakers). Alongside performance for conditions and symptoms (for depression, anxiety ROC-AUC=0.842,0.831 ECE=0.018,0.015; core individual symptom ROC-AUC>0.74), we assess demographic fairness and investigate integration across and redundancy between different input modality types. Clinical usefulness metrics and acceptability to mental health service users are explored. When provided with sufficiently rich and large-scale multimodal data streams and specified to represent common mental conditions at the symptom rather than disorder level, such models are a principled approach for building robust assessment support tools: providing clinically-relevant outputs in a transparent and explainable format that is directly amenable to expert clinical supervision.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Formalized Hopfield Networks and Boltzmann Machines</title>
<link>https://arxiv.org/abs/2512.07766</link>
<guid>https://arxiv.org/abs/2512.07766</guid>
<content:encoded><![CDATA[

arXiv:2512.07766v1 Announce Type: new 
Abstract: Neural networks are widely used, yet their analysis and verification remain challenging. In this work, we present a Lean 4 formalization of neural networks, covering both deterministic and stochastic models. We first formalize Hopfield networks, recurrent networks that store patterns as stable states. We prove convergence and the correctness of Hebbian learning, a training rule that updates network parameters to encode patterns, here limited to the case of pairwise-orthogonal patterns. We then consider stochastic networks, where updates are probabilistic and convergence is to a stationary distribution. As a canonical example, we formalize the dynamics of Boltzmann machines and prove their ergodicity, showing convergence to a unique stationary distribution using a new formalization of the Perron-Frobenius theorem.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GatedFWA: Linear Flash Windowed Attention with Gated Associative Memory</title>
<link>https://arxiv.org/abs/2512.07782</link>
<guid>https://arxiv.org/abs/2512.07782</guid>
<content:encoded><![CDATA[

arXiv:2512.07782v1 Announce Type: new 
Abstract: Modern autoregressive models rely on attention, yet the Softmax full attention in Transformers scales quadratically with sequence length. Sliding Window Attention (SWA) achieves linear-time encoding/decoding by constraining the attention pattern, but under an \textit{Associative Memory} interpretation, its difference-style update renders the training objective effectively \emph{unbounded}. In contrast, Softmax attention normalizes updates, leading to \emph{memory shrinkage and gradient vanishing}. We propose GatedFWA: a Memory-\underline{Gated} (\underline{F}lash) \underline{W}indowed \underline{A}ttention mechanism that preserves SWAs efficiency while stabilizing memory updates and making gradient flow controllable. In essence, GatedFWA accumulate a per-token/head gate into a decay bias added to the attention logits, acting as a learnable contraction in the memory recurrence. We implement a fused one-pass gate preprocessing and a FlashAttention-compatible kernel that injects the gate under a sliding mask, ensuring I/O efficiency and numerical stability. On language modelling benchmarks, GatedFWA delivers competitive throughput with negligible overhead and better use of global context, and it integrates cleanly with token compression/selection methods such as NSA and generalizes to various autoregressive domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Group Representational Position Encoding</title>
<link>https://arxiv.org/abs/2512.07805</link>
<guid>https://arxiv.org/abs/2512.07805</guid>
<content:encoded><![CDATA[

arXiv:2512.07805v1 Announce Type: new 
Abstract: We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,\omega\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: https://github.com/model-architectures/GRAPE.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Long-Range Benefits of Next-Token Prediction</title>
<link>https://arxiv.org/abs/2512.07818</link>
<guid>https://arxiv.org/abs/2512.07818</guid>
<content:encoded><![CDATA[

arXiv:2512.07818v1 Announce Type: new 
Abstract: Why do modern language models, trained to do well on next-word prediction, appear to generate coherent documents and capture long-range structure? Here we show that next-token prediction is provably powerful for learning longer-range structure, even with common neural network architectures. Specifically, we prove that optimizing next-token prediction over a Recurrent Neural Network (RNN) yields a model that closely approximates the training distribution: for held-out documents sampled from the training distribution, no algorithm of bounded description length limited to examining the next $k$ tokens, for any $k$, can distinguish between $k$ consecutive tokens of such documents and $k$ tokens generated by the learned language model following the same prefix. We provide polynomial bounds (in $k$, independent of the document length) on the model size needed to achieve such $k$-token indistinguishability, offering a complexity-theoretic explanation for the long-range coherence observed in practice.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Adoption and Usage of AI Agents: Early Evidence from Perplexity</title>
<link>https://arxiv.org/abs/2512.07828</link>
<guid>https://arxiv.org/abs/2512.07828</guid>
<content:encoded><![CDATA[

arXiv:2512.07828v1 Announce Type: new 
Abstract: This paper presents the first large-scale field study of the adoption, usage intensity, and use cases of general-purpose AI agents operating in open-world web environments. Our analysis centers on Comet, an AI-powered browser developed by Perplexity, and its integrated agent, Comet Assistant. Drawing on hundreds of millions of anonymized user interactions, we address three fundamental questions: Who is using AI agents? How intensively are they using them? And what are they using them for? Our findings reveal substantial heterogeneity in adoption and usage across user segments. Earlier adopters, users in countries with higher GDP per capita and educational attainment, and individuals working in digital or knowledge-intensive sectors -- such as digital technology, academia, finance, marketing, and entrepreneurship -- are more likely to adopt or actively use the agent. To systematically characterize the substance of agent usage, we introduce a hierarchical agentic taxonomy that organizes use cases across three levels: topic, subtopic, and task. The two largest topics, Productivity & Workflow and Learning & Research, account for 57% of all agentic queries, while the two largest subtopics, Courses and Shopping for Goods, make up 22%. The top 10 out of 90 tasks represent 55% of queries. Personal use constitutes 55% of queries, while professional and educational contexts comprise 30% and 16%, respectively. In the short term, use cases exhibit strong stickiness, but over time users tend to shift toward more cognitively oriented topics. The diffusion of increasingly capable AI agents carries important implications for researchers, businesses, policymakers, and educators, inviting new lines of inquiry into this rapidly emerging class of AI capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Knowledge-Based Language Model: Deducing Grammatical Knowledge in a Multi-Agent Language Acquisition Simulation</title>
<link>https://arxiv.org/abs/2512.02195</link>
<guid>https://arxiv.org/abs/2512.02195</guid>
<content:encoded><![CDATA[

arXiv:2512.02195v1 Announce Type: cross 
Abstract: This paper presents an initial study performed by the MODOMA system. The MODOMA is a computational multi-agent laboratory environment for unsupervised language acquisition experiments such that acquisition is based on the interaction between two language models, an adult and a child agent. Although this framework employs statistical as well as rule-based procedures, the result of language acquisition is a knowledge-based language model, which can be used to generate and parse new utterances of the target language. This system is fully parametrized and researchers can control all aspects of the experiments while the results of language acquisition, that is, the acquired grammatical knowledge, are explicitly represented and can be consulted. Thus, this system introduces novel possibilities for conducting computational language acquisition experiments. The experiments presented by this paper demonstrate that functional and content categories can be acquired and represented by the daughter agent based on training and test data containing different amounts of exemplars generated by the adult agent. Interestingly, similar patterns, which are well-established for human-generated data, are also found for these machine-generated data. As the procedures resulted in the successful acquisition of discrete grammatical categories by the child agent, these experiments substantiate the validity of the MODOMA approach to modelling language acquisition.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intrusion Detection on Resource-Constrained IoT Devices with Hardware-Aware ML and DL</title>
<link>https://arxiv.org/abs/2512.02272</link>
<guid>https://arxiv.org/abs/2512.02272</guid>
<content:encoded><![CDATA[

arXiv:2512.02272v1 Announce Type: cross 
Abstract: This paper proposes a hardware-aware intrusion detection system (IDS) for Internet of Things (IoT) and Industrial IoT (IIoT) networks; it targets scenarios where classification is essential for fast, privacy-preserving, and resource-efficient threat detection. The goal is to optimize both tree-based machine learning (ML) models and compact deep neural networks (DNNs) within strict edge-device constraints. This allows for a fair comparison and reveals trade-offs between model families. We apply constrained grid search for tree-based classifiers and hardware-aware neural architecture search (HW-NAS) for 1D convolutional neural networks (1D-CNNs). Evaluation on the Edge-IIoTset benchmark shows that selected models meet tight flash, RAM, and compute limits: LightGBM achieves 95.3% accuracy using 75 KB flash and 1.2 K operations, while the HW-NAS-optimized CNN reaches 97.2% with 190 KB flash and 840 K floating-point operations (FLOPs). We deploy the full pipeline on a Raspberry Pi 3 B Plus, confirming that tree-based models operate within 30 ms and that CNNs remain suitable when accuracy outweighs latency. These results highlight the practicality of hardware-constrained model design for real-time IDS at the edge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics Enhanced Deep Surrogates for the Phonon Boltzmann Transport Equation</title>
<link>https://arxiv.org/abs/2512.05976</link>
<guid>https://arxiv.org/abs/2512.05976</guid>
<content:encoded><![CDATA[

arXiv:2512.05976v1 Announce Type: cross 
Abstract: Designing materials with controlled heat flow at the nano-scale is central to advances in microelectronics, thermoelectrics, and energy-conversion technologies. At these scales, phonon transport follows the Boltzmann Transport Equation (BTE), which captures non-diffusive (ballistic) effects but is too costly to solve repeatedly in inverse-design loops. Existing surrogate approaches trade speed for accuracy: fast macroscopic solvers can overestimate conductivities by hundreds of percent, while recent data-driven operator learners often require thousands of high-fidelity simulations. This creates a need for a fast, data-efficient surrogate that remains reliable across ballistic and diffusive regimes. We introduce a Physics-Enhanced Deep Surrogate (PEDS) that combines a differentiable Fourier solver with a neural generator and couples it with uncertainty-driven active learning. The Fourier solver acts as a physical inductive bias, while the network learns geometry-dependent corrections and a mixing coefficient that interpolates between macroscopic and nano-scale behavior. PEDS reduces training-data requirements by up to 70% compared with purely data-driven baselines, achieves roughly 5% fractional error with only 300 high-fidelity BTE simulations, and enables efficient design of porous geometries spanning 12-85 W m$^{-1}$ K$^{-1}$ with average design errors of 4%. The learned mixing parameter recovers the ballistic-diffusive transition and improves out of distribution robustness. These results show that embedding simple, differentiable low-fidelity physics can dramatically increase surrogate data-efficiency and interpretability, making repeated PDE-constrained optimization practical for nano-scale thermal-materials design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Accelerating Materials Discovery: Learning a Universal Representation of Chemical Processes for Cross-Domain Property Prediction</title>
<link>https://arxiv.org/abs/2512.05979</link>
<guid>https://arxiv.org/abs/2512.05979</guid>
<content:encoded><![CDATA[

arXiv:2512.05979v1 Announce Type: cross 
Abstract: Experimental validation of chemical processes is slow and costly, limiting exploration in materials discovery. Machine learning can prioritize promising candidates, but existing data in patents and literature is heterogeneous and difficult to use. We introduce a universal directed-tree process-graph representation that unifies unstructured text, molecular structures, and numeric measurements into a single machine-readable format. To learn from this structured data, we developed a multi-modal graph neural network with a property-conditioned attention mechanism. Trained on approximately 700,000 process graphs from nearly 9,000 diverse documents, our model learns semantically rich embeddings that generalize across domains. When fine-tuned on compact, domain-specific datasets, the pretrained model achieves strong performance, demonstrating that universal process representations learned at scale transfer effectively to specialized prediction tasks with minimal additional data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VG3T: Visual Geometry Grounded Gaussian Transformer</title>
<link>https://arxiv.org/abs/2512.05988</link>
<guid>https://arxiv.org/abs/2512.05988</guid>
<content:encoded><![CDATA[

arXiv:2512.05988v1 Announce Type: cross 
Abstract: Generating a coherent 3D scene representation from multi-view images is a fundamental yet challenging task. Existing methods often struggle with multi-view fusion, leading to fragmented 3D representations and sub-optimal performance. To address this, we introduce VG3T, a novel multi-view feed-forward network that predicts a 3D semantic occupancy via a 3D Gaussian representation. Unlike prior methods that infer Gaussians from single-view images, our model directly predicts a set of semantically attributed Gaussians in a joint, multi-view fashion. This novel approach overcomes the fragmentation and inconsistency inherent in view-by-view processing, offering a unified paradigm to represent both geometry and semantics. We also introduce two key components, Grid-Based Sampling and Positional Refinement, to mitigate the distance-dependent density bias common in pixel-aligned Gaussian initialization methods. Our VG3T shows a notable 1.7%p improvement in mIoU while using 46% fewer primitives than the previous state-of-the-art on the nuScenes benchmark, highlighting its superior efficiency and performance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Going All-In on LLM Accuracy: Fake Prediction Markets, Real Confidence Signals</title>
<link>https://arxiv.org/abs/2512.05998</link>
<guid>https://arxiv.org/abs/2512.05998</guid>
<content:encoded><![CDATA[

arXiv:2512.05998v1 Announce Type: cross 
Abstract: Large language models are increasingly used to evaluate other models, yet these judgments typically lack any representation of confidence. This pilot study tests whether framing an evaluation task as a betting game (a fictional prediction market with its own LLM currency) improves forecasting accuracy and surfaces calibrated confidence signals. We generated 100 math and logic questions with verifiable answers. Six Baseline models (three current-generation, three prior-generation) answered all items. Three Predictor models then forecasted, for each question-baseline pair, if the baseline would answer correctly. Each predictor completed matched runs in two conditions: Control (simple correct/incorrect predictions) and Incentive (predictions plus wagers of 1-100,000 LLMCoin under even odds, starting from a 1,000,000 LLMCoin bankroll). Across 5,400 predictions per condition, Incentive runs showed modestly higher accuracy (81.5% vs. 79.1%, p = .089, d = 0.86) and significantly faster learning across rounds (12.0 vs. 2.9 percentage-point improvement from Round 1 to Round 4, p = .011). Most notably, stake size tracked confidence. "Whale" bets of 40,000+ coins were correct ~99% of the time, while small bets (<1,000 coins) showed only ~74% accuracy. The key finding is not that fictional money makes models smarter; accuracy gains were modest and did not reach statistical significance (p = .089) in this pilot. Rather, the betting mechanic created a legible confidence signal absent from binary yes/no outputs. This suggests that simple financial framing may help transform LLMs into risk-aware forecasters, making their internal beliefs visible and usable. The protocol offers a foundation for future work for meta-evaluation systems and what may become LLM-to-LLM prediction markets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-resolution Physics-Aware Recurrent Convolutional Neural Network for Complex Flows</title>
<link>https://arxiv.org/abs/2512.06031</link>
<guid>https://arxiv.org/abs/2512.06031</guid>
<content:encoded><![CDATA[

arXiv:2512.06031v1 Announce Type: cross 
Abstract: We present MRPARCv2, Multi-resolution Physics-Aware Recurrent Convolutional Neural Network, designed to model complex flows by embedding the structure of advection-diffusion-reaction equations and leveraging a multi-resolution architecture. MRPARCv2 introduces hierarchical discretization and cross-resolution feature communication to improve the accuracy and efficiency of flow simulations. We evaluate the model on a challenging 2D turbulent radiative layer dataset from The Well multi-physics benchmark repository and demonstrate significant improvements when compared to the single resolution baseline model, in both Variance Scaled Root Mean Squared Error and physics-driven metrics, including turbulent kinetic energy spectra and mass-temperature distributions. Despite having 30% fewer trainable parameters, MRPARCv2 outperforms its predecessor by up to 50% in roll-out prediction error and 86% in spectral error. A preliminary study on uncertainty quantification was performed, and we also analyzed the model's performance under different levels of abstractions of the flow, specifically on sampling subsets of field variables. We find that the absence of physical constraints on the equation of state (EOS) in the network architecture leads to degraded accuracy. A variable substitution experiment confirms that this issue persists regardless of which physical quantity is predicted directly. Our findings highlight the advantages of multi-resolution inductive bias for capturing multi-scale flow dynamics and suggest the need for future PIML models to embed EOS knowledge to enhance physical fidelity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-Loop Robotic Manipulation of Transparent Substrates for Self-Driving Laboratories using Deep Learning Micro-Error Correction</title>
<link>https://arxiv.org/abs/2512.06038</link>
<guid>https://arxiv.org/abs/2512.06038</guid>
<content:encoded><![CDATA[

arXiv:2512.06038v1 Announce Type: cross 
Abstract: Self-driving laboratories (SDLs) have accelerated the throughput and automation capabilities for discovering and improving chemistries and materials. Although these SDLs have automated many of the steps required to conduct chemical and materials experiments, a commonly overlooked step in the automation pipeline is the handling and reloading of substrates used to transfer or deposit materials onto for downstream characterization. Here, we develop a closed-loop method of Automated Substrate Handling and Exchange (ASHE) using robotics, dual-actuated dispensers, and deep learning-driven computer vision to detect and correct errors in the manipulation of fragile and transparent substrates for SDLs. Using ASHE, we demonstrate a 98.5% first-time placement accuracy across 130 independent trials of reloading transparent glass substrates into an SDL, where only two substrate misplacements occurred and were successfully detected as errors and automatically corrected. Through the development of more accurate and reliable methods for handling various types of substrates, we move toward an improvement in the automation capabilities of self-driving laboratories, furthering the acceleration of novel chemical and materials discoveries.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Road of Adaptive AI for Precision in Cybersecurity</title>
<link>https://arxiv.org/abs/2512.06048</link>
<guid>https://arxiv.org/abs/2512.06048</guid>
<content:encoded><![CDATA[

arXiv:2512.06048v1 Announce Type: cross 
Abstract: Cybersecurity's evolving complexity presents unique challenges and opportunities for AI research and practice. This paper shares key lessons and insights from designing, building, and operating production-grade GenAI pipelines in cybersecurity, with a focus on the continual adaptation required to keep pace with ever-shifting knowledge bases, tooling, and threats. Our goal is to provide an actionable perspective for AI practitioners and industry stakeholders navigating the frontier of GenAI for cybersecurity, with particular attention to how different adaptation mechanisms complement each other in end-to-end systems. We present practical guidance derived from real-world deployments, propose best practices for leveraging retrieval- and model-level adaptation, and highlight open research directions for making GenAI more robust, precise, and auditable in cyber defense.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unifying Entropy Regularization in Optimal Control: From and Back to Classical Objectives via Iterated Soft Policies and Path Integral Solutions</title>
<link>https://arxiv.org/abs/2512.06109</link>
<guid>https://arxiv.org/abs/2512.06109</guid>
<content:encoded><![CDATA[

arXiv:2512.06109v1 Announce Type: cross 
Abstract: This paper develops a unified perspective on several stochastic optimal control formulations through the lens of Kullback-Leibler regularization. We propose a central problem that separates the KL penalties on policies and transitions, assigning them independent weights, thereby generalizing the standard trajectory-level KL-regularization commonly used in probabilistic and KL-regularized control. This generalized formulation acts as a generative structure allowing to recover various control problems. These include the classical Stochastic Optimal Control (SOC), Risk-Sensitive Optimal Control (RSOC), and their policy-based KL-regularized counterparts. The latter we refer to as soft-policy SOC and RSOC, facilitating alternative problems with tractable solutions. Beyond serving as regularized variants, we show that these soft-policy formulations majorize the original SOC and RSOC problem. This means that the regularized solution can be iterated to retrieve the original solution. Furthermore, we identify a structurally synchronized case of the risk-seeking soft-policy RSOC formulation, wherein the policy and transition KL-regularization weights coincide. Remarkably, this specific setting gives rise to several powerful properties such as a linear Bellman equation, path integral solution, and, compositionality, thereby extending these computationally favourable properties to a broad class of control problems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hardware Software Optimizations for Fast Model Recovery on Reconfigurable Architectures</title>
<link>https://arxiv.org/abs/2512.06113</link>
<guid>https://arxiv.org/abs/2512.06113</guid>
<content:encoded><![CDATA[

arXiv:2512.06113v1 Announce Type: cross 
Abstract: Model Recovery (MR) is a core primitive for physical AI and real-time digital twins, but GPUs often execute MR inefficiently due to iterative dependencies, kernel-launch overheads, underutilized memory bandwidth, and high data-movement latency. We present MERINDA, an FPGA-accelerated MR framework that restructures computation as a streaming dataflow pipeline. MERINDA exploits on-chip locality through BRAM tiling, fixed-point kernels, and the concurrent use of LUT fabric and carry-chain adders to expose fine-grained spatial parallelism while minimizing off-chip traffic. This hardware-aware formulation removes synchronization bottlenecks and sustains high throughput across the iterative updates in MR. On representative MR workloads, MERINDA delivers up to 6.3x fewer cycles than an FPGA-based LTC baseline, enabling real-time performance for time-critical physical systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep learning for autism detection using clinical notes: A comparison of transfer learning for a transparent and black-box approach</title>
<link>https://arxiv.org/abs/2512.06161</link>
<guid>https://arxiv.org/abs/2512.06161</guid>
<content:encoded><![CDATA[

arXiv:2512.06161v1 Announce Type: cross 
Abstract: Autism spectrum disorder (ASD) is a complex neurodevelopmental condition whose rising prevalence places increasing demands on a lengthy diagnostic process. Machine learning (ML) has shown promise in automating ASD diagnosis, but most existing models operate as black boxes and are typically trained on a single dataset, limiting their generalizability. In this study, we introduce a transparent and interpretable ML approach that leverages BioBERT, a state-of-the-art language model, to analyze unstructured clinical text. The model is trained to label descriptions of behaviors and map them to diagnostic criteria, which are then used to assign a final label (ASD or not). We evaluate transfer learning, the ability to transfer knowledge to new data, using two distinct real-world datasets. We trained on datasets sequentially and mixed together and compared the performance of the best models and their ability to transfer to new data. We also created a black-box approach and repeated this transfer process for comparison. Our transparent model demonstrated robust performance, with the mixed-data training strategy yielding the best results (97 % sensitivity, 98 % specificity). Sequential training across datasets led to a slight drop in performance, highlighting the importance of training data order. The black-box model performed worse (90 % sensitivity, 96 % specificity) when trained sequentially or with mixed data. Overall, our transparent approach outperformed the black-box approach. Mixing datasets during training resulted in slightly better performance and should be the preferred approach when practically possible. This work paves the way for more trustworthy, generalizable, and clinically actionable AI tools in neurodevelopmental diagnostics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Lux thresholds: a systematic pipeline for classifying biologically relevant light contexts from wearable data</title>
<link>https://arxiv.org/abs/2512.06181</link>
<guid>https://arxiv.org/abs/2512.06181</guid>
<content:encoded><![CDATA[

arXiv:2512.06181v1 Announce Type: cross 
Abstract: Background: Wearable spectrometers enable field quantification of biologically relevant light, yet reproducible pipelines for contextual classification remain under-specified.
  Objective: To establish and validate a subject-wise evaluated, reproducible pipeline and actionable design rules for classifying natural vs. artificial light from wearable spectral data.
  Methods: We analysed ActLumus recordings from 26 participants, each monitored for at least 7 days at 10-second sampling, paired with daily exposure diaries. The pipeline fixes the sequence: domain selection, log-base-10 transform, L2 normalisation excluding total intensity (to avoid brightness shortcuts), hour-level medoid aggregation, sine/cosine hour encoding, and MLP classifier, evaluated under participant-wise cross-validation.
  Results: The proposed sequence consistently achieved high performance on the primary task, with representative configurations reaching AUC = 0.938 (accuracy 88%) for natural vs. artificial classification on the held-out subject split. In contrast, indoor vs. outdoor classification remained at feasibility level due to spectral overlap and class imbalance (best AUC approximately 0.75; majority-class collapse without contextual sensors). Threshold baselines were insufficient on our data, supporting the need for spectral-temporal modelling beyond illuminance cut-offs.
  Conclusions: We provide a reproducible, auditable baseline pipeline and design rules for contextual light classification under subject-wise generalisation. All code, configuration files, and derived artefacts will be openly archived (GitHub + Zenodo DOI) to support reuse and benchmarking.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Zero-Shot Prediction of Color Trajectories in Food Drying</title>
<link>https://arxiv.org/abs/2512.06190</link>
<guid>https://arxiv.org/abs/2512.06190</guid>
<content:encoded><![CDATA[

arXiv:2512.06190v1 Announce Type: cross 
Abstract: Food drying is widely used to reduce moisture content, ensure safety, and extend shelf life. Color evolution of food samples is an important indicator of product quality in food drying. Although existing studies have examined color changes under different drying conditions, current approaches primarily rely on low-dimensional color features and cannot fully capture the complex, dynamic color trajectories of food samples. Moreover, existing modeling approaches lack the ability to generalize to unseen process conditions. To address these limitations, we develop a novel multi-modal color-trajectory prediction method that integrates high-dimensional temporal color information with drying process parameters to enable accurate and data-efficient color trajectory prediction. Under unseen drying conditions, the model attains RMSEs of 2.12 for cookie drying and 1.29 for apple drying, reducing errors by over 90% compared with baseline models. These experimental results demonstrate the model's superior accuracy, robustness, and broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On measuring grounding and generalizing grounding problems</title>
<link>https://arxiv.org/abs/2512.06205</link>
<guid>https://arxiv.org/abs/2512.06205</guid>
<content:encoded><![CDATA[

arXiv:2512.06205v1 Announce Type: cross 
Abstract: The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024: Efficient and Robust Aggregation Methods for Federated Learning</title>
<link>https://arxiv.org/abs/2512.06206</link>
<guid>https://arxiv.org/abs/2512.06206</guid>
<content:encoded><![CDATA[

arXiv:2512.06206v1 Announce Type: cross 
Abstract: We present the design and results of the MICCAI Federated Tumor Segmentation (FeTS) Challenge 2024, which focuses on federated learning (FL) for glioma sub-region segmentation in multi-parametric MRI and evaluates new weight aggregation methods aimed at improving robustness and efficiency. Six participating teams were evaluated using a standardized FL setup and a multi-institutional dataset derived from the BraTS glioma benchmark, consisting of 1,251 training cases, 219 validation cases, and 570 hidden test cases with segmentations for enhancing tumor (ET), tumor core (TC), and whole tumor (WT). Teams were ranked using a cumulative scoring system that considered both segmentation performance, measured by Dice Similarity Coefficient (DSC) and the 95th percentile Hausdorff Distance (HD95), and communication efficiency assessed through the convergence score. A PID-controller-based method achieved the top overall ranking, obtaining mean DSC values of 0.733, 0.761, and 0.751 for ET, TC, and WT, respectively, with corresponding HD95 values of 33.922 mm, 33.623 mm, and 32.309 mm, while also demonstrating the highest communication efficiency with a convergence score of 0.764. These findings advance the state of federated learning for medical imaging, surpassing top-performing methods from previous challenge iterations and highlighting PID controllers as effective mechanisms for stabilizing and optimizing weight aggregation in FL. The challenge code is available at https://github.com/FeTS-AI/Challenge.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SparsePixels: Efficient Convolution for Sparse Data on FPGAs</title>
<link>https://arxiv.org/abs/2512.06208</link>
<guid>https://arxiv.org/abs/2512.06208</guid>
<content:encoded><![CDATA[

arXiv:2512.06208v1 Announce Type: cross 
Abstract: Inference of standard CNNs on FPGAs often incurs high latency and a long initiation interval due to the deep nested loops required to densely convolve every input pixel regardless of its feature value, especially when the image size is large. However, in some image data, input features can be spatially sparse, and semantic information may occupy only a small fraction of the input pixels. In this case most computation would be wasted on empty regions. In this work, we introduce SparsePixels, a framework for efficient convolution for spatially sparse image data on FPGAs, targeting fast inference applications in constrained environments with latency requirements of microseconds or below. Our approach implements a special class of CNNs that selectively retain and compute on a small subset of pixels that are active while ignoring the rest. We show that, for example, in a neutrino physics dataset for identifying neutrino interactions in LArTPC images that have around 4k input pixels but are naturally very sparse, a standard CNN with a compact size of 4k parameters incurs an inference latency of 48.665 $\mu$s on an FPGA, whereas a sparse CNN of the same base architecture computing on less than 1% of the input pixels results in a $\times 73$ inference speedup to 0.665 $\mu$s, with resource utilization well within on-chip budgets, trading only a small percent-level performance loss. At least one-order-of magnitude speedups with comparable performance are also demonstrated in similar datasets with sparse image patterns. This work aims to benefit future algorithm developments for fast and efficient data readout in modern experiments such as the trigger and data acquisition systems at the CERN Large Hadron Collider. For easy adoption, we have developed a library to support building sparse CNNs with quantization-aware training, as well as an HLS implementation for FPGA deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forests of Uncertaint(r)ees: Using tree-based ensembles to estimate probability distributions of future conflict</title>
<link>https://arxiv.org/abs/2512.06210</link>
<guid>https://arxiv.org/abs/2512.06210</guid>
<content:encoded><![CDATA[

arXiv:2512.06210v1 Announce Type: cross 
Abstract: Predictions of fatalities from violent conflict on the PRIO-GRID-month (pgm) level are characterized by high levels of uncertainty, limiting their usefulness in practical applications. We discuss the two main sources of uncertainty for this prediction task, the nature of violent conflict and data limitations, embedding this in the wider literature on uncertainty quantification in machine learning. We develop a strategy to quantify uncertainty in conflict forecasting, shifting from traditional point predictions to full predictive distributions. Our approach compares and combines multiple tree-based classifiers and distributional regressors in a custom auto-ML setup, estimating distributions for each pgm individually. We also test the integration of regional models in spatial ensembles as a potential avenue to reduce uncertainty. The models are able to consistently outperform a suite of benchmarks derived from conflict history in predictions up to one year in advance, with performance driven by regions where conflict was observed. With our evaluation, we emphasize the need to understand how a metric behaves for a given prediction problem, in our case characterized by extremely high zero-inflatedness. While not resulting in better predictions, the integration of smaller models does not decrease performance for this prediction task, opening avenues to integrate data sources with less spatial coverage in the future.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Broader View on Clustering under Cluster-Aware Norm Objectives</title>
<link>https://arxiv.org/abs/2512.06211</link>
<guid>https://arxiv.org/abs/2512.06211</guid>
<content:encoded><![CDATA[

arXiv:2512.06211v1 Announce Type: cross 
Abstract: We revisit the $(f,g)$-clustering problem that we introduced in a recent work [SODA'25], and which subsumes fundamental clustering problems such as $k$-Center, $k$-Median, Min-Sum of Radii, and Min-Load $k$-Clustering. This problem assigns each of the $k$ clusters a cost determined by the monotone, symmetric norm $f$ applied to the vector distances in the cluster, and aims at minimizing the norm $g$ applied to the vector of cluster costs. Previously, we focused on certain special cases for which we designed constant-factor approximation algorithms. Our bounds for more general settings left, however, large gaps to the known bounds for the basic problems they capture.
  In this work, we provide a clearer picture of the approximability of these more general settings. First, we design an $O(\log^2 n)$-approximation algorithm for $(f, L_{1})$-clustering for any $f$. This improves upon our previous $\widetilde{O}(\sqrt{n})$-approximation. Second, we provide an $O(k)$-approximation for the general $(f,g)$-clustering problem, which improves upon our previous $\widetilde{O}(\sqrt{kn})$-approximation algorithm and matches the best-known upper bound for Min-Load $k$-Clustering.
  We then design an approximation algorithm for $(f,g)$-clustering that interpolates, up to polylog factors, between the best known bounds for $k$-Center, $k$-Median, Min-Sum of Radii, Min-Load $k$-Clustering, (Top, $L_{1}$)-clustering, and $(L_{\infty},g)$-clustering based on a newly defined parameter of $f$ and $g$.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Data Enrichment using Confidence-Aware Fine-Grained Debate among Open-Source LLMs for Mental Health and Online Safety</title>
<link>https://arxiv.org/abs/2512.06227</link>
<guid>https://arxiv.org/abs/2512.06227</guid>
<content:encoded><![CDATA[

arXiv:2512.06227v1 Announce Type: cross 
Abstract: Real-world indicators are important for improving natural language processing (NLP) tasks such as life events for mental health analysis and risky behaviour for online safety, yet labelling such information in NLP training datasets is often costly and/or difficult given the dynamic nature of such events. This paper compares several LLM-based data enrichment methods and introduces a novel Confidence-Aware Fine-Grained Debate (CFD) framework in which multiple LLM agents simulate human annotators and exchange fine-grained evidence to reach consensus. We describe two new expert-annotated datasets, a mental health Reddit wellbeing dataset and an online safety Facebook sharenting risk dataset. Our CFD framework achieves the most robust data enrichment performance compared to a range of baselines and we show that this type of data enrichment consistently improves downstream tasks. Enriched features incorporated via debate transcripts yield the largest gains, outperforming the non-enriched baseline by 10.1% for the online safety task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opinion: Learning Intuitive Physics May Require More than Visual Data</title>
<link>https://arxiv.org/abs/2512.06232</link>
<guid>https://arxiv.org/abs/2512.06232</guid>
<content:encoded><![CDATA[

arXiv:2512.06232v1 Announce Type: cross 
Abstract: Humans expertly navigate the world by building rich internal models founded on an intuitive understanding of physics. Meanwhile, despite training on vast quantities of internet video data, state-of-the-art deep learning models still fall short of human-level performance on intuitive physics benchmarks. This work investigates whether data distribution, rather than volume, is the key to learning these principles. We pretrain a Video Joint Embedding Predictive Architecture (V-JEPA) model on SAYCam, a developmentally realistic, egocentric video dataset partially capturing three children's everyday visual experiences. We find that training on this dataset, which represents 0.01% of the data volume used to train SOTA models, does not lead to significant performance improvements on the IntPhys2 benchmark. Our results suggest that merely training on a developmentally realistic dataset is insufficient for current architectures to learn representations that support intuitive physics. We conclude that varying visual data volume and distribution alone may not be sufficient for building systems with artificial intuitive physics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Who Will Top the Charts? Multimodal Music Popularity Prediction via Adaptive Fusion of Modality Experts and Temporal Engagement Modeling</title>
<link>https://arxiv.org/abs/2512.06259</link>
<guid>https://arxiv.org/abs/2512.06259</guid>
<content:encoded><![CDATA[

arXiv:2512.06259v1 Announce Type: cross 
Abstract: Predicting a song's commercial success prior to its release remains an open and critical research challenge for the music industry. Early prediction of music popularity informs strategic decisions, creative planning, and marketing. Existing methods suffer from four limitations:(i) temporal dynamics in audio and lyrics are averaged away; (ii) lyrics are represented as a bag of words, disregarding compositional structure and affective semantics; (iii) artist- and song-level historical performance is ignored; and (iv) multimodal fusion approaches rely on simple feature concatenation, resulting in poorly aligned shared representations. To address these limitations, we introduce GAMENet, an end-to-end multimodal deep learning architecture for music popularity prediction. GAMENet integrates modality-specific experts for audio, lyrics, and social metadata through an adaptive gating mechanism. We use audio features from Music4AllOnion processed via OnionEnsembleAENet, a network of autoencoders designed for robust feature extraction; lyric embeddings derived through a large language model pipeline; and newly introduced Career Trajectory Dynamics (CTD) features that capture multi-year artist career momentum and song-level trajectory statistics. Using the Music4All dataset (113k tracks), previously explored in MIR tasks but not popularity prediction, GAMENet achieves a 12% improvement in R^2 over direct multimodal feature concatenation. Spotify audio descriptors alone yield an R^2 of 0.13. Integrating aggregate CTD features increases this to 0.69, with an additional 7% gain from temporal CTD features. We further validate robustness using the SpotGenTrack Popularity Dataset (100k tracks), achieving a 16% improvement over the previous baseline. Extensive ablations confirm the model's effectiveness and the distinct contribution of each modality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Strongly Convex Simulation Optimization: Optimize then Predict with Inexact Solutions</title>
<link>https://arxiv.org/abs/2512.06270</link>
<guid>https://arxiv.org/abs/2512.06270</guid>
<content:encoded><![CDATA[

arXiv:2512.06270v1 Announce Type: cross 
Abstract: In this work, we study contextual strongly convex simulation optimization and adopt an "optimize then predict" (OTP) approach for real-time decision making. In the offline stage, simulation optimization is conducted across a set of covariates to approximate the optimal-solution function; in the online stage, decisions are obtained by evaluating this approximation at the observed covariate. The central theoretical challenge is to understand how the inexactness of solutions generated by simulation-optimization algorithms affects the optimality gap, which is overlooked in existing studies. To address this, we develop a unified analysis framework that explicitly accounts for both solution bias and variance. Using Polyak-Ruppert averaging SGD as an illustrative simulation-optimization algorithm, we analyze the optimality gap of OTP under four representative smoothing techniques: $k$ nearest neighbor, kernel smoothing, linear regression, and kernel ridge regression. We establish convergence rates, derive the optimal allocation of the computational budget $\Gamma$ between the number of design covariates and the per-covariate simulation effort, and demonstrate the convergence rate can approximately achieve $\Gamma^{-1}$ under appropriate smoothing technique and sample-allocation rule. Finally, through a numerical study, we validate the theoretical findings and demonstrate the effectiveness and practical value of the proposed approach.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability</title>
<link>https://arxiv.org/abs/2512.06294</link>
<guid>https://arxiv.org/abs/2512.06294</guid>
<content:encoded><![CDATA[

arXiv:2512.06294v1 Announce Type: cross 
Abstract: Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling Spatio-temporal Extremes via Conditional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2512.06348</link>
<guid>https://arxiv.org/abs/2512.06348</guid>
<content:encoded><![CDATA[

arXiv:2512.06348v1 Announce Type: cross 
Abstract: Extreme weather events are widely studied in fields such as agriculture, ecology, and meteorology. The spatio-temporal co-occurrence of extreme events can strengthen or weaken under changing climate conditions. In this paper, we propose a novel approach to model spatio-temporal extremes by integrating climate indices via a conditional variational autoencoder (cXVAE). A convolutional neural network (CNN) is embedded in the decoder to convolve climatological indices with the spatial dependence within the latent space, thereby allowing the decoder to be dependent on the climate variables. There are three main contributions here. First, we demonstrate through extensive simulations that the proposed conditional XVAE accurately emulates spatial fields and recovers spatially and temporally varying extremal dependence with very low computational cost post training. Second, we provide a simple, scalable approach to detecting condition-driven shifts and whether the dependence structure is invariant to the conditioning variable. Third, when dependence is found to be condition-sensitive, the conditional XVAE supports counterfactual experiments allowing intervention on the climate covariate and propagating the associated change through the learned decoder to quantify differences in joint tail risk, co-occurrence ranges, and return metrics. To demonstrate the practical utility and performance of the model in real-world scenarios, we apply our method to analyze the monthly maximum Fire Weather Index (FWI) over eastern Australia from 2014 to 2024 conditioned on the El Ni\~{n}o/Southern Oscillation (ENSO) index.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Web Technologies Security in the AI Era: A Survey of CDN-Enhanced Defenses</title>
<link>https://arxiv.org/abs/2512.06390</link>
<guid>https://arxiv.org/abs/2512.06390</guid>
<content:encoded><![CDATA[

arXiv:2512.06390v1 Announce Type: cross 
Abstract: The modern web stack, which is dominated by browser-based applications and API-first backends, now operates under an adversarial equilibrium where automated, AI-assisted attacks evolve continuously. Content Delivery Networks (CDNs) and edge computing place programmable defenses closest to users and bots, making them natural enforcement points for machine-learning (ML) driven inspection, throttling, and isolation. This survey synthesizes the landscape of AI-enhanced defenses deployed at the edge: (i) anomaly- and behavior-based Web Application Firewalls (WAFs) within broader Web Application and API Protection (WAAP), (ii) adaptive DDoS detection and mitigation, (iii) bot management that resists human-mimicry, and (iv) API discovery, positive security modeling, and encrypted-traffic anomaly analysis. We add a systematic survey method, a threat taxonomy mapped to edge-observable signals, evaluation metrics, deployment playbooks, and governance guidance. We conclude with a research agenda spanning XAI, adversarial robustness, and autonomous multi-agent defense. Our findings indicate that edge-centric AI measurably improves time-to-detect and time-to-mitigate while reducing data movement and enhancing compliance, yet introduces new risks around model abuse, poisoning, and governance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More for Multi-Step Logical Reasoning of LLM Generalisation Under Rule Removal, Paraphrasing, and Compression</title>
<link>https://arxiv.org/abs/2512.06393</link>
<guid>https://arxiv.org/abs/2512.06393</guid>
<content:encoded><![CDATA[

arXiv:2512.06393v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel across many natural language tasks, yet their generalisation to structural perturbations in logical contexts remains poorly understood. We introduce a controlled evaluation framework that probes reasoning reliability through four targeted stress tests: (1) rule deletion, removing either redundant or essential rules from a multi-step inference chain; (2) contradictory evidence injection; (3) logic-preserving rewrites generated through several families of equivalence laws (contrapositive, double negation, implication, De Morgan, identity, and commutativity); and (4) multi-law equivalence stacking that introduces 2-5 simultaneous logical transformations.
  Across three representative model families: BERT, Qwen2, and LLaMA-like models. Our experiments reveal a strikingly consistent pattern: all models achieve perfect accuracy on the base tasks and remain fully generalise to redundant rule deletion and all equivalence-based rewrites (single or multi-law), but fail sharply under essential rule deletion (dropping to 25% accuracy) and collapse completely in the presence of explicit contradictions (0% accuracy). These results demonstrate that LLMs possess stable invariance to semantic-preserving logical transformations, yet remain fundamentally brittle to missing or conflicting evidence. Our framework provides a clean diagnostic tool for isolating such reasoning failure modes and highlights persistent gaps in the logical generalisation abilities of current LLMs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UncertaintyZoo: A Unified Toolkit for Quantifying Predictive Uncertainty in Deep Learning Systems</title>
<link>https://arxiv.org/abs/2512.06406</link>
<guid>https://arxiv.org/abs/2512.06406</guid>
<content:encoded><![CDATA[

arXiv:2512.06406v1 Announce Type: cross 
Abstract: Large language models(LLMs) are increasingly expanding their real-world applications across domains, e.g., question answering, autonomous driving, and automatic software development. Despite this achievement, LLMs, as data-driven systems, often make incorrect predictions, which can lead to potential losses in safety-critical scenarios. To address this issue and measure the confidence of model outputs, multiple uncertainty quantification(UQ) criteria have been proposed. However, even though important, there are limited tools to integrate these methods, hindering the practical usage of UQ methods and future research in this domain. To bridge this gap, in this paper, we introduce UncertaintyZoo, a unified toolkit that integrates 29 uncertainty quantification methods, covering five major categories under a standardized interface. Using UncertaintyZoo, we evaluate the usefulness of existing uncertainty quantification methods under the code vulnerability detection task on CodeBERT and ChatGLM3 models. The results demonstrate that UncertaintyZoo effectively reveals prediction uncertainty. The tool with a demonstration video is available on the project site https://github.com/Paddingbuta/UncertaintyZoo.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Training Dynamics in Scale-wise Autoregressive Generation</title>
<link>https://arxiv.org/abs/2512.06421</link>
<guid>https://arxiv.org/abs/2512.06421</guid>
<content:encoded><![CDATA[

arXiv:2512.06421v1 Announce Type: cross 
Abstract: Recent advances in autoregressive (AR) generative models have produced increasingly powerful systems for media synthesis. Among them, next-scale prediction has emerged as a popular paradigm, where models generate images in a coarse-to-fine manner. However, scale-wise AR models suffer from exposure bias, which undermines generation quality. We identify two primary causes of this issue: (1) train-test mismatch, where the model must rely on its own imperfect predictions during inference, and (2) imbalance in scale-wise learning difficulty, where certain scales exhibit disproportionately higher optimization complexity. Through a comprehensive analysis of training dynamics, we propose Self-Autoregressive Refinement (SAR) to address these limitations. SAR introduces a Stagger-Scale Rollout (SSR) mechanism that performs lightweight autoregressive rollouts to expose the model to its own intermediate predictions, thereby aligning train-test patterns, and a complementary Contrastive Student-Forcing Loss (CSFL) that provides adequate supervision for self-generated contexts to ensure stable training. Experimental results show that applying SAR to pretrained AR models consistently improves generation quality with minimal computational overhead. For instance, SAR yields a 5.2% FID reduction on FlexVAR-d16 trained on ImageNet 256 within 10 epochs (5 hours on 32xA100 GPUs). Given its efficiency, scalability, and effectiveness, we expect SAR to serve as a reliable post-training method for visual autoregressive generation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automated Deep Learning Estimation of Anthropometric Measurements for Preparticipation Cardiovascular Screening</title>
<link>https://arxiv.org/abs/2512.06434</link>
<guid>https://arxiv.org/abs/2512.06434</guid>
<content:encoded><![CDATA[

arXiv:2512.06434v1 Announce Type: cross 
Abstract: Preparticipation cardiovascular examination (PPCE) aims to prevent sudden cardiac death (SCD) by identifying athletes with structural or electrical cardiac abnormalities. Anthropometric measurements, such as waist circumference, limb lengths, and torso proportions to detect Marfan syndrome, can indicate elevated cardiovascular risk. Traditional manual methods are labor-intensive, operator-dependent, and challenging to scale. We present a fully automated deep-learning approach to estimate five key anthropometric measurements from 2D synthetic human body images. Using a dataset of 100,000 images derived from 3D body meshes, we trained and evaluated VGG19, ResNet50, and DenseNet121 with fully connected layers for regression. All models achieved sub-centimeter accuracy, with ResNet50 performing best, achieving a mean MAE of 0.668 cm across all measurements. Our results demonstrate that deep learning can deliver accurate anthropometric data at scale, offering a practical tool to complement athlete screening protocols. Future work will validate the models on real-world images to extend applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Canonical Tail Dependence for Soft Extremal Clustering of Multichannel Brain Signals</title>
<link>https://arxiv.org/abs/2512.06435</link>
<guid>https://arxiv.org/abs/2512.06435</guid>
<content:encoded><![CDATA[

arXiv:2512.06435v1 Announce Type: cross 
Abstract: We develop a novel characterization of extremal dependence between two cortical regions of the brain when its signals display extremely large amplitudes. We show that connectivity in the tails of the distribution reveals unique features of extreme events (e.g., seizures) that can help to identify their occurrence. Numerous studies have established that connectivity-based features are effective for discriminating brain states. Here, we demonstrate the advantage of the proposed approach: that tail connectivity provides additional discriminatory power, enabling more accurate identification of extreme-related events and improved seizure risk management. Common approaches in tail dependence modeling use pairwise summary measures or parametric models. However, these approaches do not identify channels that drive the maximal tail dependence between two groups of signals -- an information that is useful when analyzing electroencephalography of epileptic patients where specific channels are responsible for seizure occurrences. A familiar approach in traditional signal processing is canonical correlation, which we extend to the tails to develop a visualization of extremal channel-contributions. Through the tail pairwise dependence matrix (TPDM), we develop a computationally-efficient estimator for our canonical tail dependence measure. Our method is then used for accurate frequency-based soft clustering of neonates, distinguishing those with seizures from those without.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PRIMRose: Insights into the Per-Residue Energy Metrics of Proteins with Double InDel Mutations using Deep Learning</title>
<link>https://arxiv.org/abs/2512.06496</link>
<guid>https://arxiv.org/abs/2512.06496</guid>
<content:encoded><![CDATA[

arXiv:2512.06496v1 Announce Type: cross 
Abstract: Understanding how protein mutations affect protein structure is essential for advancements in computational biology and bioinformatics. We introduce PRIMRose, a novel approach that predicts energy values for each residue given a mutated protein sequence. Unlike previous models that assess global energy shifts, our method analyzes the localized energetic impact of double amino acid insertions or deletions (InDels) at the individual residue level, enabling residue-specific insights into structural and functional disruption. We implement a Convolutional Neural Network architecture to predict the energy changes of each residue in a protein mutation. We train our model on datasets constructed from nine proteins, grouped into three categories: one set with exhaustive double InDel mutations, another with approximately 145k randomly sampled double InDel mutations, and a third with approximately 80k randomly sampled double InDel mutations. Our model achieves high predictive accuracy across a range of energy metrics as calculated by the Rosetta molecular modeling suite and reveals localized patterns that influence model performance, such as solvent accessibility and secondary structure context. This per-residue analysis offers new insights into the mutational tolerance of specific regions within proteins and provides higher interpretable and biologically meaningful predictions of InDels' effects.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On The Role of K-Space Acquisition in MRI Reconstruction Domain-Generalization</title>
<link>https://arxiv.org/abs/2512.06530</link>
<guid>https://arxiv.org/abs/2512.06530</guid>
<content:encoded><![CDATA[

arXiv:2512.06530v1 Announce Type: cross 
Abstract: Recent work has established learned k-space acquisition patterns as a promising direction for improving reconstruction quality in accelerated Magnetic Resonance Imaging (MRI). Despite encouraging results, most existing research focuses on acquisition patterns optimized for a single dataset or modality, with limited consideration of their transferability across imaging domains. In this work, we demonstrate that the benefits of learned k-space sampling can extend beyond the training domain, enabling superior reconstruction performance under domain shifts. Our study presents two main contributions. First, through systematic evaluation across datasets and acquisition paradigms, we show that models trained with learned sampling patterns exhibitimproved generalization under cross-domain settings. Second, we propose a novel method that enhances domain robustness by introducing acquisition uncertainty during training-stochastically perturbing k-space trajectories to simulate variability across scanners and imaging conditions. Our results highlight the importance of treating kspace trajectory design not merely as an acceleration mechanism, but as an active degree of freedom for improving domain generalization in MRI reconstruction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images</title>
<link>https://arxiv.org/abs/2512.06531</link>
<guid>https://arxiv.org/abs/2512.06531</guid>
<content:encoded><![CDATA[

arXiv:2512.06531v1 Announce Type: cross 
Abstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximate Multiplier Induced Error Propagation in Deep Neural Networks</title>
<link>https://arxiv.org/abs/2512.06537</link>
<guid>https://arxiv.org/abs/2512.06537</guid>
<content:encoded><![CDATA[

arXiv:2512.06537v1 Announce Type: cross 
Abstract: Deep Neural Networks (DNNs) rely heavily on dense arithmetic operations, motivating the use of Approximate Multipliers (AxMs) to reduce energy consumption in hardware accelerators. However, a rigorous mathematical characterization of how AxMs error distributions influence DNN accuracy remains underdeveloped. This work presents an analytical framework that connects the statistical error moments of an AxM to the induced distortion in General Matrix Multiplication (GEMM). Using the Frobenius norm of the resulting error matrix, we derive a closed form expression for practical DNN dimensions that demonstrates the distortion is predominantly governed by the multiplier mean error (bias). To evaluate this model in realistic settings, we incorporate controlled error injection into GEMM and convolution layers and examine its effect on ImageNet scale networks. The predicted distortion correlates strongly with the observed accuracy degradation, and an error configurable AxM case study implemented on an FPGA further confirms the analytical trends. By providing a lightweight alternative to behavioral or hardware level simulations, this framework enables rapid estimation of AxM impact on DNN inference quality.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Latent Variable Framework for Scaling Laws in Large Language Models</title>
<link>https://arxiv.org/abs/2512.06553</link>
<guid>https://arxiv.org/abs/2512.06553</guid>
<content:encoded><![CDATA[

arXiv:2512.06553v1 Announce Type: cross 
Abstract: We propose a statistical framework built on latent variable modeling for scaling laws of large language models (LLMs). Our work is motivated by the rapid emergence of numerous new LLM families with distinct architectures and training strategies, evaluated on an increasing number of benchmarks. This heterogeneity makes a single global scaling curve inadequate for capturing how performance varies across families and benchmarks. To address this, we propose a latent variable modeling framework in which each LLM family is associated with a latent variable that captures the common underlying features in that family. An LLM's performance on different benchmarks is then driven by its latent skills, which are jointly determined by the latent variable and the model's own observable features. We develop an estimation procedure for this latent variable model and establish its statistical properties. We also design efficient numerical algorithms that support estimation and various downstream tasks. Empirically, we evaluate the approach on 12 widely used benchmarks from the Open LLM Leaderboard (v1/v2).
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proof of Concept for Mammography Classification with Enhanced Compactness and Separability Modules</title>
<link>https://arxiv.org/abs/2512.06575</link>
<guid>https://arxiv.org/abs/2512.06575</guid>
<content:encoded><![CDATA[

arXiv:2512.06575v1 Announce Type: cross 
Abstract: This study presents a validation and extension of a recent methodological framework for medical image classification. While an improved ConvNeXt Tiny architecture, integrating Global Average and Max Pooling fusion (GAGM), lightweight channel attention (SEVector), and Feature Smoothing Loss (FSL), demonstrated promising results on Alzheimer MRI under CPU friendly conditions, our work investigates its transposability to mammography classification. Using a Kaggle dataset that consolidates INbreast, MIAS, and DDSM mammography collections, we compare a baseline CNN, ConvNeXt Tiny, and InceptionV3 backbones enriched with GAGM and SEVector modules. Results confirm the effectiveness of GAGM and SEVector in enhancing feature discriminability and reducing false negatives, particularly for malignant cases. In our experiments, however, the Feature Smoothing Loss did not yield measurable improvements under mammography classification conditions, suggesting that its effectiveness may depend on specific architectural and computational assumptions. Beyond validation, our contribution extends the original framework through multi metric evaluation (macro F1, per class recall variance, ROC/AUC), feature interpretability analysis (Grad CAM), and the development of an interactive dashboard for clinical exploration. As a perspective, we highlight the need to explore alternative approaches to improve intra class compactness and inter class separability, with the specific goal of enhancing the distinction between malignant and benign cases in mammography classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Nonlinear Denoising Score Matching for Enhanced Learning of Structured Distributions</title>
<link>https://arxiv.org/abs/2512.06615</link>
<guid>https://arxiv.org/abs/2512.06615</guid>
<content:encoded><![CDATA[

arXiv:2512.06615v1 Announce Type: cross 
Abstract: We present latent nonlinear denoising score matching (LNDSM), a novel training objective for score-based generative models that integrates nonlinear forward dynamics with the VAE-based latent SGM framework. This combination is achieved by reformulating the cross-entropy term using the approximate Gaussian transition induced by the Euler-Maruyama scheme. To ensure numerical stability, we identify and remove two zero-mean but variance exploding terms arising from small time steps. Experiments on variants of the MNIST dataset demonstrate that the proposed method achieves faster synthesis and enhanced learning of inherently structured distributions. Compared to benchmark structure-agnostic latent SGMs, LNDSM consistently attains superior sample quality and variability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Hedge Swaptions</title>
<link>https://arxiv.org/abs/2512.06639</link>
<guid>https://arxiv.org/abs/2512.06639</guid>
<content:encoded><![CDATA[

arXiv:2512.06639v1 Announce Type: cross 
Abstract: This paper investigates the deep hedging framework, based on reinforcement learning (RL), for the dynamic hedging of swaptions, contrasting its performance with traditional sensitivity-based rho-hedging. We design agents under three distinct objective functions (mean squared error, downside risk, and Conditional Value-at-Risk) to capture alternative risk preferences and evaluate how these objectives shape hedging styles. Relying on a three-factor arbitrage-free dynamic Nelson-Siegel model for our simulation experiments, our findings show that near-optimal hedging effectiveness is achieved when using two swaps as hedging instruments. Deep hedging strategies dynamically adapt the hedging portfolio's exposure to risk factors across states of the market. In our experiments, their out-performance over rho-hedging strategies persists even in the presence some of model misspecification. These results highlight RL's potential to deliver more efficient and resilient swaption hedging strategies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Masked Autoencoder Pretraining on Strong-Lensing Images for Joint Dark-Matter Model Classification and Super-Resolution</title>
<link>https://arxiv.org/abs/2512.06642</link>
<guid>https://arxiv.org/abs/2512.06642</guid>
<content:encoded><![CDATA[

arXiv:2512.06642v1 Announce Type: cross 
Abstract: Strong gravitational lensing can reveal the influence of dark-matter substructure in galaxies, but analyzing these effects from noisy, low-resolution images poses a significant challenge. In this work, we propose a masked autoencoder (MAE) pretraining strategy on simulated strong-lensing images from the DeepLense ML4SCI benchmark to learn generalizable representations for two downstream tasks: (i) classifying the underlying dark matter model (cold dark matter, axion-like, or no substructure) and (ii) enhancing low-resolution lensed images via super-resolution. We pretrain a Vision Transformer encoder using a masked image modeling objective, then fine-tune the encoder separately for each task. Our results show that MAE pretraining, when combined with appropriate mask ratio tuning, yields a shared encoder that matches or exceeds a ViT trained from scratch. Specifically, at a 90% mask ratio, the fine-tuned classifier achieves macro AUC of 0.968 and accuracy of 88.65%, compared to the scratch baseline (AUC 0.957, accuracy 82.46%). For super-resolution (16x16 to 64x64), the MAE-pretrained model reconstructs images with PSNR ~33 dB and SSIM 0.961, modestly improving over scratch training. We ablate the MAE mask ratio, revealing a consistent trade-off: higher mask ratios improve classification but slightly degrade reconstruction fidelity. Our findings demonstrate that MAE pretraining on physics-rich simulations provides a flexible, reusable encoder for multiple strong-lensing analysis tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedDSR: Federated Deep Supervision and Regularization Towards Autonomous Driving</title>
<link>https://arxiv.org/abs/2512.06676</link>
<guid>https://arxiv.org/abs/2512.06676</guid>
<content:encoded><![CDATA[

arXiv:2512.06676v1 Announce Type: cross 
Abstract: Federated Learning (FL) enables collaborative training of autonomous driving (AD) models across distributed vehicles while preserving data privacy. However, FL encounters critical challenges such as poor generalization and slow convergence due to non-independent and identically distributed (non-IID) data from diverse driving environments. To overcome these obstacles, we introduce Federated Deep Supervision and Regularization (FedDSR), a paradigm that incorporates multi-access intermediate layer supervision and regularization within federated AD system. Specifically, FedDSR comprises following integral strategies: (I) to select multiple intermediate layers based on predefined architecture-agnostic standards. (II) to compute mutual information (MI) and negative entropy (NE) on those selected layers to serve as intermediate loss and regularizer. These terms are integrated into the output-layer loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. (III) to aggregate models from vehicles trained based on aforementioned rules of (I) and (II) to generate the global model on central server. By guiding and penalizing the learning of feature representations at intermediate stages, FedDSR enhances the model generalization and accelerates model convergence for federated AD. We then take the semantic segmentation task as an example to assess FedDSR and apply FedDSR to multiple model architectures and FL algorithms. Extensive experiments demonstrate that FedDSR achieves up to 8.93% improvement in mIoU and 28.57% reduction in training rounds, compared to other FL baselines, making it highly suitable for practical deployment in federated AD ecosystems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predictive Modeling of I/O Performance for Machine Learning Training Pipelines: A Data-Driven Approach to Storage Optimization</title>
<link>https://arxiv.org/abs/2512.06699</link>
<guid>https://arxiv.org/abs/2512.06699</guid>
<content:encoded><![CDATA[

arXiv:2512.06699v1 Announce Type: cross 
Abstract: Modern machine learning training is increasingly bottlenecked by data I/O rather than compute. GPUs often sit idle at below 50% utilization waiting for data. This paper presents a machine learning approach to predict I/O performance and recommend optimal storage configurations for ML training pipelines. We collected 141 observations through systematic benchmarking across different storage backends (NVMe SSD, network-attached storage, in-memory filesystems), data formats, and access patterns, covering both low-level I/O operations and full training pipelines. After evaluating seven regression models and three classification approaches, XGBoost achieved the best performance with R-squared of 0.991, predicting I/O throughput within 11.8% error on average. Feature importance analysis revealed that throughput metrics and batch size are the primary performance drivers. This data-driven approach can reduce configuration time from days of trial-and-error to minutes of predictive recommendation. The methodology is reproducible and extensible to other resource management problems in ML systems. Code and data are available at https://github.com/knkarthik01/gpu_storage_ml_project
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Becoming Experienced Judges: Selective Test-Time Learning for Evaluators</title>
<link>https://arxiv.org/abs/2512.06751</link>
<guid>https://arxiv.org/abs/2512.06751</guid>
<content:encoded><![CDATA[

arXiv:2512.06751v1 Announce Type: cross 
Abstract: Automatic evaluation with large language models, commonly known as LLM-as-a-judge, is now standard across reasoning and alignment tasks. Despite evaluating many samples in deployment, these evaluators typically (i) treat each case independently, missing the opportunity to accumulate experience, and (ii) rely on a single fixed prompt for all cases, neglecting the need for sample-specific evaluation criteria. We introduce Learning While Evaluating (LWE), a framework that allows evaluators to improve sequentially at inference time without requiring training or validation sets. LWE maintains an evolving meta-prompt that (i) produces sample-specific evaluation instructions and (ii) refines itself through self-generated feedback. Furthermore, we propose Selective LWE, which updates the meta-prompt only on self-inconsistent cases, focusing computation where it matters most. This selective approach retains the benefits of sequential learning while being far more cost-effective. Across two pairwise comparison benchmarks, Selective LWE outperforms strong baselines, empirically demonstrating that evaluators can improve during sequential testing with a simple selective update, learning most from the cases they struggle with.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ADAM Optimization with Adaptive Batch Selection</title>
<link>https://arxiv.org/abs/2512.06795</link>
<guid>https://arxiv.org/abs/2512.06795</guid>
<content:encoded><![CDATA[

arXiv:2512.06795v1 Announce Type: cross 
Abstract: Adam is a widely used optimizer in neural network training due to its adaptive learning rate. However, because different data samples influence model updates to varying degrees, treating them equally can lead to inefficient convergence. To address this, a prior work proposed adapting the sampling distribution using a bandit framework to select samples adaptively. While promising, the bandit-based variant of Adam suffers from limited theoretical guarantees. In this paper, we introduce Adam with Combinatorial Bandit Sampling (AdamCB), which integrates combinatorial bandit techniques into Adam to resolve these issues. AdamCB is able to fully utilize feedback from multiple samples at once, enhancing both theoretical guarantees and practical performance. Our regret analysis shows that AdamCB achieves faster convergence than Adam-based methods including the previous bandit-based variant. Numerical experiments demonstrate that AdamCB consistently outperforms existing methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal and Diffusion Transports in Machine Learning</title>
<link>https://arxiv.org/abs/2512.06797</link>
<guid>https://arxiv.org/abs/2512.06797</guid>
<content:encoded><![CDATA[

arXiv:2512.06797v1 Announce Type: cross 
Abstract: Several problems in machine learning are naturally expressed as the design and analysis of time-evolving probability distributions. This includes sampling via diffusion methods, optimizing the weights of neural networks, and analyzing the evolution of token distributions across layers of large language models. While the targeted applications differ (samples, weights, tokens), their mathematical descriptions share a common structure. A key idea is to switch from the Eulerian representation of densities to their Lagrangian counterpart through vector fields that advect particles. This dual view introduces challenges, notably the non-uniqueness of Lagrangian vector fields, but also opportunities to craft density evolutions and flows with favorable properties in terms of regularity, stability, and computational tractability. This survey presents an overview of these methods, with emphasis on two complementary approaches: diffusion methods, which rely on stochastic interpolation processes and underpin modern generative AI, and optimal transport, which defines interpolation by minimizing displacement cost. We illustrate how both approaches appear in applications ranging from sampling, neural network optimization, to modeling the dynamics of transformers for large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Aware Attention LSTM Autoencoder for Early Fault Diagnosis of Battery Systems</title>
<link>https://arxiv.org/abs/2512.06809</link>
<guid>https://arxiv.org/abs/2512.06809</guid>
<content:encoded><![CDATA[

arXiv:2512.06809v1 Announce Type: cross 
Abstract: Battery safety is paramount for electric vehicles. Early fault diagnosis remains a challenge due to the subtle nature of anomalies and the interference of dynamic operating noise. Existing data-driven methods often suffer from "physical blindness" leading to missed detections or false alarms. To address this, we propose a Physics-Aware Attention LSTM Autoencoder (PA-ALSTM-AE). This novel framework explicitly integrates battery aging laws (mileage) into the deep learning pipeline through a multi-stage fusion mechanism. Specifically, an adaptive physical feature construction module selects mileage-sensitive features, and a physics-guided latent fusion module dynamically calibrates the memory cells of the LSTM based on the aging state. Extensive experiments on the large-scale Vloong real-world dataset demonstrate that the proposed method significantly outperforms state-of-the-art baselines. Notably, it improves the recall rate of early faults by over 3 times while maintaining high precision, offering a robust solution for industrial battery management systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RMAdapter: Reconstruction-based Multi-Modal Adapter for Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.06811</link>
<guid>https://arxiv.org/abs/2512.06811</guid>
<content:encoded><![CDATA[

arXiv:2512.06811v1 Announce Type: cross 
Abstract: Pre-trained Vision-Language Models (VLMs), \textit{e.g.} CLIP, have become essential tools in multimodal transfer learning. However, fine-tuning VLMs in few-shot scenarios poses significant challenges in balancing task-specific adaptation and generalization in the obtained model. Meanwhile, current researches have predominantly focused on prompt-based adaptation methods, leaving adapter-based approaches underexplored and revealing notable performance gaps. To address these challenges, we introduce a novel Reconstruction-based Multimodal Adapter (RMAdapter), which leverages a dual-branch architecture. Unlike conventional single-branch adapters, RMAdapter consists of: (1) an adaptation branch that injects task-specific knowledge through parameter-efficient fine-tuning, and (2) a reconstruction branch that preserves general knowledge by reconstructing latent space features back into the original feature space. This design facilitates a dynamic balance between general and task-specific knowledge. Importantly, although RMAdapter introduces an additional reconstruction branch, it is carefully optimized to remain lightweight. By computing reconstruction loss locally at each layer and sharing projection modules, the overall computational overhead is kept minimal. A consistency constraint is also incorporated to better regulate the trade-off between discriminability and generalization. We comprehensively evaluate the effectiveness of RMAdapter on three representative tasks: generalization to new categories, generalization to new target datasets, and domain generalization. Without relying on data augmentation or duplicate prompt designs, our RMAdapter consistently outperforms state-of-the-art approaches across all evaluation metrics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hide-and-Seek Attribution: Weakly Supervised Segmentation of Vertebral Metastases in CT</title>
<link>https://arxiv.org/abs/2512.06849</link>
<guid>https://arxiv.org/abs/2512.06849</guid>
<content:encoded><![CDATA[

arXiv:2512.06849v1 Announce Type: cross 
Abstract: Accurate segmentation of vertebral metastasis in CT is clinically important yet difficult to scale, as voxel-level annotations are scarce and both lytic and blastic lesions often resemble benign degenerative changes. We introduce a weakly supervised method trained solely on vertebra-level healthy/malignant labels, without any lesion masks. The method combines a Diffusion Autoencoder (DAE) that produces a classifier-guided healthy edit of each vertebra with pixel-wise difference maps that propose candidate lesion regions. To determine which regions truly reflect malignancy, we introduce Hide-and-Seek Attribution: each candidate is revealed in turn while all others are hidden, the edited image is projected back to the data manifold by the DAE, and a latent-space classifier quantifies the isolated malignant contribution of that component. High-scoring regions form the final lytic or blastic segmentation. On held-out radiologist annotations, we achieve strong blastic/lytic performance despite no mask supervision (F1: 0.91/0.85; Dice: 0.87/0.78), exceeding baselines (F1: 0.79/0.67; Dice: 0.74/0.55). These results show that vertebra-level labels can be transformed into reliable lesion masks, demonstrating that generative editing combined with selective occlusion supports accurate weakly supervised segmentation in CT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Less Is More, but Where? Dynamic Token Compression via LLM-Guided Keyframe Prior</title>
<link>https://arxiv.org/abs/2512.06866</link>
<guid>https://arxiv.org/abs/2512.06866</guid>
<content:encoded><![CDATA[

arXiv:2512.06866v1 Announce Type: cross 
Abstract: Recent advances in Video Large Language Models (VLLMs) have achieved remarkable video understanding capabilities, yet face critical efficiency bottlenecks due to quadratic computational growth with lengthy visual token sequences of long videos. While existing keyframe sampling methods can improve temporal modeling efficiency, additional computational cost is introduced before feature encoding, and the binary frame selection paradigm is found suboptimal. Therefore, in this work, we propose Dynamic Token compression via LLM-guided Keyframe prior (DyToK), a training-free paradigm that enables dynamic token compression by harnessing VLLMs' inherent attention mechanisms. Our analysis reveals that VLLM attention layers naturally encoding query-conditioned keyframe priors, by which DyToK dynamically adjusts per-frame token retention ratios, prioritizing semantically rich frames while suppressing redundancies. Extensive experiments demonstrate that DyToK achieves state-of-the-art efficiency-accuracy tradeoffs. DyToK shows plug-and-play compatibility with existing compression methods, such as VisionZip and FastV, attaining 4.3x faster inference while preserving accuracy across multiple VLLMs, such as LLaVA-OneVision and Qwen2.5-VL. Code is available at https://github.com/yu-lin-li/DyToK .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MINES: Explainable Anomaly Detection through Web API Invariant Inference</title>
<link>https://arxiv.org/abs/2512.06906</link>
<guid>https://arxiv.org/abs/2512.06906</guid>
<content:encoded><![CDATA[

arXiv:2512.06906v1 Announce Type: cross 
Abstract: Detecting the anomalies of web applications, important infrastructures for running modern companies and governments, is crucial for providing reliable web services. Many modern web applications operate on web APIs (e.g., RESTful, SOAP, and WebSockets), their exposure invites intended attacks or unintended illegal visits, causing abnormal system behaviors. However, such anomalies can share very similar logs with normal logs, missing crucial information (which could be in database) for log discrimination. Further, log instances can be also noisy, which can further mislead the state-of-the-art log learning solutions to learn spurious correlation, resulting superficial models and rules for anomaly detection. In this work, we propose MINES which infers explainable API invariants for anomaly detection from the schema level instead of detailed raw log instances, which can (1) significantly discriminate noise in logs to identify precise normalities and (2) detect abnormal behaviors beyond the instrumented logs. Technically, MINES (1) converts API signatures into table schema to enhance the original database shema; and (2) infers the potential database constraints on the enhanced database schema to capture the potential relationships between APIs and database tables. MINES uses LLM for extracting potential relationship based on two given table structures; and use normal log instances to reject and accept LLM-generated invariants. Finally, MINES translates the inferred constraints into invariants to generate Python code for verifying the runtime logs. We extensively evaluate MINES on web-tamper attacks on the benchmarks of TrainTicket, NiceFish, Gitea, Mastodon, and NextCloud against baselines such as LogRobust, LogFormer, and WebNorm. The results show that MINES achieves high recall for the anomalies while introducing almost zero false positives, indicating a new state-of-the-art.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Navigation for Surface Vehicles in Vortical Flow Fields</title>
<link>https://arxiv.org/abs/2512.06912</link>
<guid>https://arxiv.org/abs/2512.06912</guid>
<content:encoded><![CDATA[

arXiv:2512.06912v1 Announce Type: cross 
Abstract: For centuries, khalasi have skillfully harnessed ocean currents to navigate vast waters with minimal effort. Emulating this intuition in autonomous systems remains a significant challenge, particularly for Autonomous Surface Vehicles tasked with long duration missions under strict energy budgets. In this work, we present a learning-based approach for energy-efficient surface vehicle navigation in vortical flow fields, where partial observability often undermines traditional path-planning methods. We present an end to end reinforcement learning framework based on Soft Actor Critic that learns flow-aware navigation policies using only local velocity measurements. Through extensive evaluation across diverse and dynamically rich scenarios, our method demonstrates substantial energy savings and robust generalization to previously unseen flow conditions, offering a promising path toward long term autonomy in ocean environments. The navigation paths generated by our proposed approach show an improvement in energy conservation 30 to 50 percent compared to the existing state of the art techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetric Aggregation of Conformity Scores for Efficient Uncertainty Sets</title>
<link>https://arxiv.org/abs/2512.06945</link>
<guid>https://arxiv.org/abs/2512.06945</guid>
<content:encoded><![CDATA[

arXiv:2512.06945v1 Announce Type: cross 
Abstract: Access to multiple predictive models trained for the same task, whether in regression or classification, is increasingly common in many applications. Aggregating their predictive uncertainties to produce reliable and efficient uncertainty quantification is therefore a critical but still underexplored challenge, especially within the framework of conformal prediction (CP). While CP methods can generate individual prediction sets from each model, combining them into a single, more informative set remains a challenging problem. To address this, we propose SACP (Symmetric Aggregated Conformal Prediction), a novel method that aggregates nonconformity scores from multiple predictors. SACP transforms these scores into e-values and combines them using any symmetric aggregation function. This flexible design enables a robust, data-driven framework for selecting aggregation strategies that yield sharper prediction sets. We also provide theoretical insights that help justify the validity and performance of the SACP approach. Extensive experiments on diverse datasets show that SACP consistently improves efficiency and often outperforms state-of-the-art model aggregation baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PARIS: Pruning Algorithm via the Representer theorem for Imbalanced Scenarios</title>
<link>https://arxiv.org/abs/2512.06950</link>
<guid>https://arxiv.org/abs/2512.06950</guid>
<content:encoded><![CDATA[

arXiv:2512.06950v1 Announce Type: cross 
Abstract: The challenge of \textbf{imbalanced regression} arises when standard Empirical Risk Minimization (ERM) biases models toward high-frequency regions of the data distribution, causing severe degradation on rare but high-impact ``tail'' events. Existing strategies uch as loss re-weighting or synthetic over-sampling often introduce noise, distort the underlying distribution, or add substantial algorithmic complexity.
  We introduce \textbf{PARIS} (Pruning Algorithm via the Representer theorem for Imbalanced Scenarios), a principled framework that mitigates imbalance by \emph{optimizing the training set itself}. PARIS leverages the representer theorem for neural networks to compute a \textbf{closed-form representer deletion residual}, which quantifies the exact change in validation loss caused by removing a single training point \emph{without retraining}. Combined with an efficient Cholesky rank-one downdating scheme, PARIS performs fast, iterative pruning that eliminates uninformative or performance-degrading samples.
  We use a real-world space weather example, where PARIS reduces the training set by up to 75\% while preserving or improving overall RMSE, outperforming re-weighting, synthetic oversampling, and boosting baselines. Our results demonstrate that representer-guided dataset pruning is a powerful, interpretable, and computationally efficient approach to rare-event regression.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Task adaptation of Vision-Language-Action model: 1st Place Solution for the 2025 BEHAVIOR Challenge</title>
<link>https://arxiv.org/abs/2512.06951</link>
<guid>https://arxiv.org/abs/2512.06951</guid>
<content:encoded><![CDATA[

arXiv:2512.06951v1 Announce Type: cross 
Abstract: We present a vision-action policy that won 1st place in the 2025 BEHAVIOR Challenge - a large-scale benchmark featuring 50 diverse long-horizon household tasks in photo-realistic simulation, requiring bimanual manipulation, navigation, and context-aware decision making.
  Building on the Pi0.5 architecture, we introduce several innovations. Our primary contribution is correlated noise for flow matching, which improves training efficiency and enables correlation-aware inpainting for smooth action sequences. We also apply learnable mixed-layer attention and System 2 stage tracking for ambiguity resolution. Training employs multi-sample flow matching to reduce variance, while inference uses action compression and challenge-specific correction rules.
  Our approach achieves 26% q-score across all 50 tasks on both public and private leaderboards.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical analysis of Inverse Entropy-regularized Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.06956</link>
<guid>https://arxiv.org/abs/2512.06956</guid>
<content:encoded><![CDATA[

arXiv:2512.06956v1 Announce Type: cross 
Abstract: Inverse reinforcement learning aims to infer the reward function that explains expert behavior observed through trajectories of state--action pairs. A long-standing difficulty in classical IRL is the non-uniqueness of the recovered reward: many reward functions can induce the same optimal policy, rendering the inverse problem ill-posed. In this paper, we develop a statistical framework for Inverse Entropy-regularized Reinforcement Learning that resolves this ambiguity by combining entropy regularization with a least-squares reconstruction of the reward from the soft Bellman residual. This combination yields a unique and well-defined so-called least-squares reward consistent with the expert policy. We model the expert demonstrations as a Markov chain with the invariant distribution defined by an unknown expert policy $\pi^\star$ and estimate the policy by a penalized maximum-likelihood procedure over a class of conditional distributions on the action space. We establish high-probability bounds for the excess Kullback--Leibler divergence between the estimated policy and the expert policy, accounting for statistical complexity through covering numbers of the policy class. These results lead to non-asymptotic minimax optimal convergence rates for the least-squares reward function, revealing the interplay between smoothing (entropy regularization), model complexity, and sample size. Our analysis bridges the gap between behavior cloning, inverse reinforcement learning, and modern statistical learning theory.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Conditional Independence Differential Graphs From Time-Dependent Data</title>
<link>https://arxiv.org/abs/2512.06960</link>
<guid>https://arxiv.org/abs/2512.06960</guid>
<content:encoded><![CDATA[

arXiv:2512.06960v1 Announce Type: cross 
Abstract: Estimation of differences in conditional independence graphs (CIGs) of two time series Gaussian graphical models (TSGGMs) is investigated where the two TSGGMs are known to have similar structure. The TSGGM structure is encoded in the inverse power spectral density (IPSD) of the time series. In several existing works, one is interested in estimating the difference in two precision matrices to characterize underlying changes in conditional dependencies of two sets of data consisting of independent and identically distributed (i.i.d.) observations. In this paper we consider estimation of the difference in two IPSDs to characterize the underlying changes in conditional dependencies of two sets of time-dependent data. Our approach accounts for data time dependencies unlike past work. We analyze a penalized D-trace loss function approach in the frequency domain for differential graph learning, using Wirtinger calculus. We consider both convex (group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. An alternating direction method of multipliers (ADMM) algorithm is presented to optimize the objective function. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm) and graph recovery. Both synthetic and real data examples are presented in support of the proposed approaches. In synthetic data examples, our log-sum-penalized differential time-series graph estimator significantly outperformed our lasso based differential time-series graph estimator which, in turn, significantly outperformed an existing lasso-penalized i.i.d. modeling approach, with $F_1$ score as the performance metric.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Learning of Feasibility-Aware Signal Temporal Logic and BarrierNet for Robust and Correct Control</title>
<link>https://arxiv.org/abs/2512.06973</link>
<guid>https://arxiv.org/abs/2512.06973</guid>
<content:encoded><![CDATA[

arXiv:2512.06973v1 Announce Type: cross 
Abstract: Control Barrier Functions (CBFs) have emerged as a powerful tool for enforcing safety in optimization-based controllers, and their integration with Signal Temporal Logic (STL) has enabled the specification-driven synthesis of complex robotic behaviors. However, existing CBF-STL approaches typically rely on fixed hyperparameters and myopic, per-time step optimization, which can lead to overly conservative behavior, infeasibility near tight input limits, and difficulty satisfying long-horizon STL tasks. To address these limitations, we propose a feasibility-aware learning framework that embeds trainable, time-varying High Order Control Barrier Functions (HOCBFs) into a differentiable Quadratic Program (dQP). Our approach provides a systematic procedure for constructing time-varying HOCBF constraints for a broad fragment of STL and introduces a unified robustness measure that jointly captures STL satisfaction, QP feasibility, and control-bound compliance. Three neural networks-InitNet, RefNet, and an extended BarrierNet-collaborate to generate reference inputs and adapt constraint-related hyperparameters automatically over time and across initial conditions, reducing conservativeness while maximizing robustness. The resulting controller achieves STL satisfaction with strictly feasible dQPs and requires no manual tuning. Simulation results demonstrate that the proposed framework maintains high STL robustness under tight input bounds and significantly outperforms fixed-parameter and non-adaptive baselines in complex environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Guided Diffusion Priors for Multi-Slice Reconstruction in Scientific Imaging</title>
<link>https://arxiv.org/abs/2512.06977</link>
<guid>https://arxiv.org/abs/2512.06977</guid>
<content:encoded><![CDATA[

arXiv:2512.06977v1 Announce Type: cross 
Abstract: Accurate multi-slice reconstruction from limited measurement data is crucial to speed up the acquisition process in medical and scientific imaging. However, it remains challenging due to the ill-posed nature of the problem and the high computational and memory demands. We propose a framework that addresses these challenges by integrating partitioned diffusion priors with physics-based constraints. By doing so, we substantially reduce memory usage per GPU while preserving high reconstruction quality, outperforming both physics-only and full multi-slice reconstruction baselines for different modalities, namely Magnetic Resonance Imaging (MRI) and four-dimensional Scanning Transmission Electron Microscopy (4D-STEM). Additionally, we show that the proposed method improves in-distribution accuracy as well as strong generalization to out-of-distribution datasets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Selective Masking based Self-Supervised Learning for Image Semantic Segmentation</title>
<link>https://arxiv.org/abs/2512.06981</link>
<guid>https://arxiv.org/abs/2512.06981</guid>
<content:encoded><![CDATA[

arXiv:2512.06981v1 Announce Type: cross 
Abstract: This paper proposes a novel self-supervised learning method for semantic segmentation using selective masking image reconstruction as the pretraining task. Our proposed method replaces the random masking augmentation used in most masked image modelling pretraining methods. The proposed selective masking method selectively masks image patches with the highest reconstruction loss by breaking the image reconstruction pretraining into iterative steps to leverage the trained model's knowledge. We show on two general datasets (Pascal VOC and Cityscapes) and two weed segmentation datasets (Nassar 2020 and Sugarbeets 2016) that our proposed selective masking method outperforms the traditional random masking method and supervised ImageNet pretraining on downstream segmentation accuracy by 2.9% for general datasets and 2.5% for weed segmentation datasets. Furthermore, we found that our selective masking method significantly improves accuracy for the lowest-performing classes. Lastly, we show that using the same pretraining and downstream dataset yields the best result for low-budget self-supervised pretraining. Our proposed Selective Masking Image Reconstruction method provides an effective and practical solution to improve end-to-end semantic segmentation workflows, especially for scenarios that require limited model capacity to meet inference speed and computational resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Memory: A comparison of memory mechanisms in world models</title>
<link>https://arxiv.org/abs/2512.06983</link>
<guid>https://arxiv.org/abs/2512.06983</guid>
<content:encoded><![CDATA[

arXiv:2512.06983v1 Announce Type: cross 
Abstract: World models enable agents to plan within imagined environments by predicting future states conditioned on past observations and actions. However, their ability to plan over long horizons is limited by the effective memory span of the backbone architecture. This limitation leads to perceptual drift in long rollouts, hindering the model's capacity to perform loop closures within imagined trajectories. In this work, we investigate the effective memory span of transformer-based world models through an analysis of several memory augmentation mechanisms. We introduce a taxonomy that distinguishes between memory encoding and memory injection mechanisms, motivating their roles in extending the world model's memory through the lens of residual stream dynamics. Using a state recall evaluation task, we measure the memory recall of each mechanism and analyze its respective trade-offs. Our findings show that memory mechanisms improve the effective memory span in vision transformers and provide a path to completing loop closures within a world model's imagination.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing video analytics inference pipelines: a case study</title>
<link>https://arxiv.org/abs/2512.07009</link>
<guid>https://arxiv.org/abs/2512.07009</guid>
<content:encoded><![CDATA[

arXiv:2512.07009v1 Announce Type: cross 
Abstract: Cost-effective and scalable video analytics are essential for precision livestock monitoring, where high-resolution footage and near-real-time monitoring needs from commercial farms generates substantial computational workloads. This paper presents a comprehensive case study on optimizing a poultry welfare monitoring system through system-level improvements across detection, tracking, clustering, and behavioral analysis modules. We introduce a set of optimizations, including multi-level parallelization, Optimizing code with substituting CPU code with GPU-accelerated code, vectorized clustering, and memory-efficient post-processing. Evaluated on real-world farm video footage, these changes deliver up to a 2x speedup across pipelines without compromising model accuracy. Our findings highlight practical strategies for building high-throughput, low-latency video inference systems that reduce infrastructure demands in agricultural and smart sensing deployments as well as other large-scale video analytics applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comprehensive Study of Supervised Machine Learning Models for Zero-Day Attack Detection: Analyzing Performance on Imbalanced Data</title>
<link>https://arxiv.org/abs/2512.07030</link>
<guid>https://arxiv.org/abs/2512.07030</guid>
<content:encoded><![CDATA[

arXiv:2512.07030v1 Announce Type: cross 
Abstract: Among the various types of cyberattacks, identifying zero-day attacks is problematic because they are unknown to security systems as their pattern and characteristics do not match known blacklisted attacks. There are many Machine Learning (ML) models designed to analyze and detect network attacks, especially using supervised models. However, these models are designed to classify samples (normal and attacks) based on the patterns they learn during the training phase, so they perform inefficiently on unseen attacks. This research addresses this issue by evaluating five different supervised models to assess their performance and execution time in predicting zero-day attacks and find out which model performs accurately and quickly. The goal is to improve the performance of these supervised models by not only proposing a framework that applies grid search, dimensionality reduction and oversampling methods to overcome the imbalance problem, but also comparing the effectiveness of oversampling on ml model metrics, in particular the accuracy. To emulate attack detection in real life, this research applies a highly imbalanced data set and only exposes the classifiers to zero-day attacks during the testing phase, so the models are not trained to flag the zero-day attacks. Our results show that Random Forest (RF) performs best under both oversampling and non-oversampling conditions, this increased effectiveness comes at the cost of longer processing times. Therefore, we selected XG Boost (XGB) as the top model due to its fast and highly accurate performance in detecting zero-day attacks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating and Preserving High-level Fidelity in Super-Resolution</title>
<link>https://arxiv.org/abs/2512.07037</link>
<guid>https://arxiv.org/abs/2512.07037</guid>
<content:encoded><![CDATA[

arXiv:2512.07037v1 Announce Type: cross 
Abstract: Recent image Super-Resolution (SR) models are achieving impressive effects in reconstructing details and delivering visually pleasant outputs. However, the overpowering generative ability can sometimes hallucinate and thus change the image content despite gaining high visual quality. This type of high-level change can be easily identified by humans yet not well-studied in existing low-level image quality metrics. In this paper, we establish the importance of measuring high-level fidelity for SR models as a complementary criterion to reveal the reliability of generative SR models. We construct the first annotated dataset with fidelity scores from different SR models, and evaluate how state-of-the-art (SOTA) SR models actually perform in preserving high-level fidelity. Based on the dataset, we then analyze how existing image quality metrics correlate with fidelity measurement, and further show that this high-level task can be better addressed by foundation models. Finally, by fine-tuning SR models based on our fidelity feedback, we show that both semantic fidelity and perceptual quality can be improved, demonstrating the potential value of our proposed criteria, both in model evaluation and optimization. We will release the dataset, code, and models upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ideal Attribution and Faithful Watermarks for Language Models</title>
<link>https://arxiv.org/abs/2512.07038</link>
<guid>https://arxiv.org/abs/2512.07038</guid>
<content:encoded><![CDATA[

arXiv:2512.07038v1 Announce Type: cross 
Abstract: We introduce ideal attribution mechanisms, a formal abstraction for reasoning about attribution decisions over strings. At the core of this abstraction lies the ledger, an append-only log of the prompt-response interaction history between a model and its user. Each mechanism produces deterministic decisions based on the ledger and an explicit selection criterion, making it well-suited to serve as a ground truth for attribution. We frame the design goal of watermarking schemes as faithful representation of ideal attribution mechanisms. This novel perspective brings conceptual clarity, replacing piecemeal probabilistic statements with a unified language for stating the guarantees of each scheme. It also enables precise reasoning about desiderata for future watermarking schemes, even when no current construction achieves them, since the ideal functionalities are specified first. In this way, the framework provides a roadmap that clarifies which guarantees are attainable in an idealized setting and worth pursuing in practice.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAUNet: A Lightweight UNet Variant with Deformable Convolutions and Parameter-Free Attention for Medical Image Segmentation</title>
<link>https://arxiv.org/abs/2512.07051</link>
<guid>https://arxiv.org/abs/2512.07051</guid>
<content:encoded><![CDATA[

arXiv:2512.07051v1 Announce Type: cross 
Abstract: Medical image segmentation plays a pivotal role in automated diagnostic and treatment planning systems. In this work, we present DAUNet, a novel lightweight UNet variant that integrates Deformable V2 Convolutions and Parameter-Free Attention (SimAM) to improve spatial adaptability and context-aware feature fusion without increasing model complexity. DAUNet's bottleneck employs dynamic deformable kernels to handle geometric variations, while the decoder and skip pathways are enhanced using SimAM attention modules for saliency-aware refinement. Extensive evaluations on two challenging datasets, FH-PS-AoP (fetal head and pubic symphysis ultrasound) and FUMPE (CT-based pulmonary embolism detection), demonstrate that DAUNet outperforms state-of-the-art models in Dice score, HD95, and ASD, while maintaining superior parameter efficiency. Ablation studies highlight the individual contributions of deformable convolutions and SimAM attention. DAUNet's robustness to missing context and low-contrast regions establishes its suitability for deployment in real-time and resource-constrained clinical environments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFIR-DETR: Frequency Domain Enhancement and Dynamic Feature Aggregation for Cross-Scene Small Object Detection</title>
<link>https://arxiv.org/abs/2512.07078</link>
<guid>https://arxiv.org/abs/2512.07078</guid>
<content:encoded><![CDATA[

arXiv:2512.07078v1 Announce Type: cross 
Abstract: Detecting small objects in UAV remote sensing images and identifying surface defects in industrial inspection remain difficult tasks. These applications face common obstacles: features are sparse and weak, backgrounds are cluttered, and object scales vary dramatically. Current transformer-based detectors, while powerful, struggle with three critical issues. First, features degrade severely as networks downsample progressively. Second, spatial convolutions cannot capture long-range dependencies effectively. Third, standard upsampling methods inflate feature maps unnecessarily.
  We introduce DFIR-DETR to tackle these problems through dynamic feature aggregation combined with frequency-domain processing. Our architecture builds on three novel components. The DCFA module uses dynamic K-sparse attention, cutting complexity from O(N2) down to O(NK), and employs spatial gated linear units for better nonlinear modeling. The DFPN module applies amplitude-normalized upsampling to prevent feature inflation and uses dual-path shuffle convolution to retain spatial details across scales. The FIRC3 module operates in the frequency domain, achieving global receptive fields without sacrificing efficiency.
  We tested our method extensively on NEU-DET and VisDrone datasets. Results show mAP50 scores of 92.9% and 51.6% respectively-both state-of-the-art. The model stays lightweight with just 11.7M parameters and 41.2 GFLOPs. Strong performance across two very different domains confirms that DFIR-DETR generalizes well and works effectively in resource-limited settings for cross-scene small object detection.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ThinkTrap: Denial-of-Service Attacks against Black-box LLM Services via Infinite Thinking</title>
<link>https://arxiv.org/abs/2512.07086</link>
<guid>https://arxiv.org/abs/2512.07086</guid>
<content:encoded><![CDATA[

arXiv:2512.07086v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become foundational components in a wide range of applications, including natural language understanding and generation, embodied intelligence, and scientific discovery. As their computational requirements continue to grow, these models are increasingly deployed as cloud-based services, allowing users to access powerful LLMs via the Internet. However, this deployment model introduces a new class of threat: denial-of-service (DoS) attacks via unbounded reasoning, where adversaries craft specially designed inputs that cause the model to enter excessively long or infinite generation loops. These attacks can exhaust backend compute resources, degrading or denying service to legitimate users. To mitigate such risks, many LLM providers adopt a closed-source, black-box setting to obscure model internals. In this paper, we propose ThinkTrap, a novel input-space optimization framework for DoS attacks against LLM services even in black-box environments. The core idea of ThinkTrap is to first map discrete tokens into a continuous embedding space, then undertake efficient black-box optimization in a low-dimensional subspace exploiting input sparsity. The goal of this optimization is to identify adversarial prompts that induce extended or non-terminating generation across several state-of-the-art LLMs, achieving DoS with minimal token overhead. We evaluate the proposed attack across multiple commercial, closed-source LLM services. Our results demonstrate that, even far under the restrictive request frequency limits commonly enforced by these platforms, typically capped at ten requests per minute (10 RPM), the attack can degrade service throughput to as low as 1% of its original capacity, and in some cases, induce complete service failure.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Neural Affinity Framework for Abstract Reasoning: Diagnosing the Compositional Gap in Transformer Architectures via Procedural Task Taxonomy</title>
<link>https://arxiv.org/abs/2512.07109</link>
<guid>https://arxiv.org/abs/2512.07109</guid>
<content:encoded><![CDATA[

arXiv:2512.07109v1 Announce Type: cross 
Abstract: Responding to Hodel et al.'s (2024) call for a formal definition of task relatedness in re-arc, we present the first 9-category taxonomy of all 400 tasks, validated at 97.5% accuracy via rule-based code analysis. We prove the taxonomy's visual coherence by training a CNN on raw grid pixels (95.24% accuracy on S3, 36.25% overall, 3.3x chance), then apply the taxonomy diagnostically to the original ARC-AGI-2 test set. Our curriculum analysis reveals 35.3% of tasks exhibit low neural affinity for Transformers--a distributional bias mirroring ARC-AGI-2. To probe this misalignment, we fine-tuned a 1.7M-parameter Transformer across 302 tasks, revealing a profound Compositional Gap: 210 of 302 tasks (69.5%) achieve >80% cell accuracy (local patterns) but <10% grid accuracy (global synthesis). This provides direct evidence for a Neural Affinity Ceiling Effect, where performance is bounded by architectural suitability, not curriculum. Applying our framework to Li et al.'s independent ViTARC study (400 specialists, 1M examples each) confirms its predictive power: Very Low affinity tasks achieve 51.9% versus 77.7% for High affinity (p<0.001), with a task at 0% despite massive data. The taxonomy enables precise diagnosis: low-affinity tasks (A2) hit hard ceilings, while high-affinity tasks (C1) reach 99.8%. These findings indicate that progress requires hybrid architectures with affinity-aligned modules. We release our validated taxonomy,
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chromatic Feature Vectors for 2-Trees: Exact Formulas for Partition Enumeration with Network Applications</title>
<link>https://arxiv.org/abs/2512.07120</link>
<guid>https://arxiv.org/abs/2512.07120</guid>
<content:encoded><![CDATA[

arXiv:2512.07120v1 Announce Type: cross 
Abstract: We establish closed-form enumeration formulas for chromatic feature vectors of 2-trees under the bichromatic triangle constraint. These efficiently computable structural features derive from constrained graph colorings where each triangle uses exactly two colors, forbidding monochromatic and rainbow triangles, a constraint arising in distributed systems where components avoid complete concentration or isolation. For theta graphs Theta_n, we prove r_k(Theta_n) = S(n-2, k-1) for k >= 3 (Stirling numbers of the second kind) and r_2(Theta_n) = 2^(n-2) + 1, computable in O(n) time. For fan graphs Phi_n, we establish r_2(Phi_n) = F_{n+1} (Fibonacci numbers) and derive explicit formulas r_k(Phi_n) = sum_{t=k-1}^{n-1} a_{n-1,t} * S(t, k-1) with efficiently computable binomial coefficients, achieving O(n^2) computation per component. Unlike classical chromatic polynomials, which assign identical features to all n-vertex 2-trees, bichromatic constraints provide informative structural features. While not complete graph invariants, these features capture meaningful structural properties through connections to Fibonacci polynomials, Bell numbers, and independent set enumeration. Applications include Byzantine fault tolerance in hierarchical networks, VM allocation in cloud computing, and secret-sharing protocols in distributed cryptography.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepSVM: Learning Stochastic Volatility Models with Physics-Informed Deep Operator Networks</title>
<link>https://arxiv.org/abs/2512.07162</link>
<guid>https://arxiv.org/abs/2512.07162</guid>
<content:encoded><![CDATA[

arXiv:2512.07162v1 Announce Type: cross 
Abstract: Real-time calibration of stochastic volatility models (SVMs) is computationally bottlenecked by the need to repeatedly solve coupled partial differential equations (PDEs). In this work, we propose DeepSVM, a physics-informed Deep Operator Network (PI-DeepONet) designed to learn the solution operator of the Heston model across its entire parameter space. Unlike standard data-driven deep learning (DL) approaches, DeepSVM requires no labelled training data. Rather, we employ a hard-constrained ansatz that enforces terminal payoffs and static no-arbitrage conditions by design. Furthermore, we use Residual-based Adaptive Refinement (RAR) to stabilize training in difficult regions subject to high gradients. Overall, DeepSVM achieves a final training loss of $10^{-5}$ and predicts highly accurate option prices across a range of typical market dynamics. While pricing accuracy is high, we find that the model's derivatives (Greeks) exhibit noise in the at-the-money (ATM) regime, highlighting the specific need for higher-order regularization in physics-informed operator learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JEPA as a Neural Tokenizer: Learning Robust Speech Representations with Density Adaptive Attention</title>
<link>https://arxiv.org/abs/2512.07168</link>
<guid>https://arxiv.org/abs/2512.07168</guid>
<content:encoded><![CDATA[

arXiv:2512.07168v1 Announce Type: cross 
Abstract: We introduce a two-stage self-supervised framework that combines the Joint-Embedding Predictive Architecture (JEPA) with a Density Adaptive Attention Mechanism (DAAM) for learning robust speech representations. Stage~1 uses JEPA with DAAM to learn semantic audio features via masked prediction in latent space, fully decoupled from waveform reconstruction. Stage~2 leverages these representations for efficient tokenization using Finite Scalar Quantization (FSQ) and a mixed-radix packing scheme, followed by high-fidelity waveform reconstruction with a HiFi-GAN decoder. By integrating Gaussian mixture-based density-adaptive gating into the JEPA encoder, the model performs adaptive temporal feature selection and discovers hierarchical speech structure at a low frame rate of 2.5~Hz. The resulting tokens (47.5 tokens/sec) provide a reversible, highly compressed, and language-model-friendly representation that is competitive with, and often more efficient than, existing neural audio codecs.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContextualSHAP : Enhancing SHAP Explanations Through Contextual Language Generation</title>
<link>https://arxiv.org/abs/2512.07178</link>
<guid>https://arxiv.org/abs/2512.07178</guid>
<content:encoded><![CDATA[

arXiv:2512.07178v1 Announce Type: cross 
Abstract: Explainable Artificial Intelligence (XAI) has become an increasingly important area of research, particularly as machine learning models are deployed in high-stakes domains. Among various XAI approaches, SHAP (SHapley Additive exPlanations) has gained prominence due to its ability to provide both global and local explanations across different machine learning models. While SHAP effectively visualizes feature importance, it often lacks contextual explanations that are meaningful for end-users, especially those without technical backgrounds. To address this gap, we propose a Python package that extends SHAP by integrating it with a large language model (LLM), specifically OpenAI's GPT, to generate contextualized textual explanations. This integration is guided by user-defined parameters (such as feature aliases, descriptions, and additional background) to tailor the explanation to both the model context and the user perspective. We hypothesize that this enhancement can improve the perceived understandability of SHAP explanations. To evaluate the effectiveness of the proposed package, we applied it in a healthcare-related case study and conducted user evaluations involving real end-users. The results, based on Likert-scale surveys and follow-up interviews, indicate that the generated explanations were perceived as more understandable and contextually appropriate compared to visual-only outputs. While the findings are preliminary, they suggest that combining visualization with contextualized text may support more user-friendly and trustworthy model explanations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding Diffusion Models via Code Execution</title>
<link>https://arxiv.org/abs/2512.07201</link>
<guid>https://arxiv.org/abs/2512.07201</guid>
<content:encoded><![CDATA[

arXiv:2512.07201v1 Announce Type: cross 
Abstract: Diffusion models have achieved remarkable performance in generative modeling, yet their theoretical foundations are often intricate, and the gap between mathematical formulations in papers and practical open-source implementations can be difficult to bridge. Existing tutorials primarily focus on deriving equations, offering limited guidance on how diffusion models actually operate in code. To address this, we present a concise implementation of approximately 300 lines that explains diffusion models from a code-execution perspective. Our minimal example preserves the essential components -- including forward diffusion, reverse sampling, the noise-prediction network, and the training loop -- while removing unnecessary engineering details. This technical report aims to provide researchers with a clear, implementation-first understanding of how diffusion models work in practice and how code and theory correspond. Our code and pre-trained models are available at: https://github.com/disanda/GM/tree/main/DDPM-DDIM-ClassifierFree.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoLugano: A Deep Learning Framework for Fully Automated Lymphoma Segmentation and Lugano Staging on FDG-PET/CT</title>
<link>https://arxiv.org/abs/2512.07206</link>
<guid>https://arxiv.org/abs/2512.07206</guid>
<content:encoded><![CDATA[

arXiv:2512.07206v1 Announce Type: cross 
Abstract: Purpose: To develop a fully automated deep learning system, AutoLugano, for end-to-end lymphoma classification by performing lesion segmentation, anatomical localization, and automated Lugano staging from baseline FDG-PET/CT scans. Methods: The AutoLugano system processes baseline FDG-PET/CT scans through three sequential modules:(1) Anatomy-Informed Lesion Segmentation, a 3D nnU-Net model, trained on multi-channel inputs, performs automated lesion detection (2) Atlas-based Anatomical Localization, which leverages the TotalSegmentator toolkit to map segmented lesions to 21 predefined lymph node regions using deterministic anatomical rules; and (3) Automated Lugano Staging, where the spatial distribution of involved regions is translated into Lugano stages and therapeutic groups (Limited vs. Advanced Stage).The system was trained on the public autoPET dataset (n=1,007) and externally validated on an independent cohort of 67 patients. Performance was assessed using accuracy, sensitivity, specificity, F1-scorefor regional involvement detection and staging agreement. Results: On the external validation set, the proposed model demonstrated robust performance, achieving an overall accuracy of 88.31%, sensitivity of 74.47%, Specificity of 94.21% and an F1-score of 80.80% for regional involvement detection,outperforming baseline models. Most notably, for the critical clinical task of therapeutic stratification (Limited vs. Advanced Stage), the system achieved a high accuracy of 85.07%, with a specificity of 90.48% and a sensitivity of 82.61%.Conclusion: AutoLugano represents the first fully automated, end-to-end pipeline that translates a single baseline FDG-PET/CT scan into a complete Lugano stage. This study demonstrates its strong potential to assist in initial staging, treatment stratification, and supporting clinical decision-making.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coherent Audio-Visual Editing via Conditional Audio Generation Following Video Edits</title>
<link>https://arxiv.org/abs/2512.07209</link>
<guid>https://arxiv.org/abs/2512.07209</guid>
<content:encoded><![CDATA[

arXiv:2512.07209v1 Announce Type: cross 
Abstract: We introduce a novel pipeline for joint audio-visual editing that enhances the coherence between edited video and its accompanying audio. Our approach first applies state-of-the-art video editing techniques to produce the target video, then performs audio editing to align with the visual changes. To achieve this, we present a new video-to-audio generation model that conditions on the source audio, target video, and a text prompt. We extend the model architecture to incorporate conditional audio input and propose a data augmentation strategy that improves training efficiency. Furthermore, our model dynamically adjusts the influence of the source audio based on the complexity of the edits, preserving the original audio structure where possible. Experimental results demonstrate that our method outperforms existing approaches in maintaining audio-visual alignment and content integrity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample from What You See: Visuomotor Policy Learning via Diffusion Bridge with Observation-Embedded Stochastic Differential Equation</title>
<link>https://arxiv.org/abs/2512.07212</link>
<guid>https://arxiv.org/abs/2512.07212</guid>
<content:encoded><![CDATA[

arXiv:2512.07212v1 Announce Type: cross 
Abstract: Imitation learning with diffusion models has advanced robotic control by capturing multi-modal action distributions. However, existing approaches typically treat observations as high-level conditioning inputs to the denoising network, rather than integrating them into the stochastic dynamics of the diffusion process itself. As a result, sampling must begin from random Gaussian noise, weakening the coupling between perception and control and often yielding suboptimal performance. We introduce BridgePolicy, a generative visuomotor policy that explicitly embeds observations within the stochastic differential equation via a diffusion-bridge formulation. By constructing an observation-informed trajectory, BridgePolicy enables sampling to start from a rich, informative prior rather than random noise, substantially improving precision and reliability in control. A key challenge is that classical diffusion bridges connect distributions with matched dimensionality, whereas robotic observations are heterogeneous and multi-modal and do not naturally align with the action space. To address this, we design a multi-modal fusion module and a semantic aligner that unify visual and state inputs and align observation and action representations, making the bridge applicable to heterogeneous robot data. Extensive experiments across 52 simulation tasks on three benchmarks and five real-world tasks demonstrate that BridgePolicy consistently outperforms state-of-the-art generative policies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUSE: A Simple Yet Effective Multimodal Search-Based Framework for Lifelong User Interest Modeling</title>
<link>https://arxiv.org/abs/2512.07216</link>
<guid>https://arxiv.org/abs/2512.07216</guid>
<content:encoded><![CDATA[

arXiv:2512.07216v1 Announce Type: cross 
Abstract: Lifelong user interest modeling is crucial for industrial recommender systems, yet existing approaches rely predominantly on ID-based features, suffering from poor generalization on long-tail items and limited semantic expressiveness. While recent work explores multimodal representations for behavior retrieval in the General Search Unit (GSU), they often neglect multimodal integration in the fine-grained modeling stage -- the Exact Search Unit (ESU). In this work, we present a systematic analysis of how to effectively leverage multimodal signals across both stages of the two-stage lifelong modeling framework. Our key insight is that simplicity suffices in the GSU: lightweight cosine similarity with high-quality multimodal embeddings outperforms complex retrieval mechanisms. In contrast, the ESU demands richer multimodal sequence modeling and effective ID-multimodal fusion to unlock its full potential. Guided by these principles, we propose MUSE, a simple yet effective multimodal search-based framework. MUSE has been deployed in Taobao display advertising system, enabling 100K-length user behavior sequence modeling and delivering significant gains in top-line metrics with negligible online latency overhead. To foster community research, we share industrial deployment practices and open-source the first large-scale dataset featuring ultra-long behavior sequences paired with high-quality multimodal embeddings. Our code and data is available at https://taobao-mm.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeSTR: A Neuro-Symbolic Abductive Framework for Temporal Reasoning in Large Language Models</title>
<link>https://arxiv.org/abs/2512.07218</link>
<guid>https://arxiv.org/abs/2512.07218</guid>
<content:encoded><![CDATA[

arXiv:2512.07218v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, temporal reasoning, particularly under complex temporal constraints, remains a major challenge. To this end, existing approaches have explored symbolic methods, which encode temporal structure explicitly, and reflective mechanisms, which revise reasoning errors through multi-step inference. Nonetheless, symbolic approaches often underutilize the reasoning capabilities of LLMs, while reflective methods typically lack structured temporal representations, which can result in inconsistent or hallucinated reasoning. As a result, even when the correct temporal context is available, LLMs may still misinterpret or misapply time-related information, leading to incomplete or inaccurate answers. To address these limitations, in this work, we propose Neuro-Symbolic Temporal Reasoning (NeSTR), a novel framework that integrates structured symbolic representations with hybrid reflective reasoning to enhance the temporal sensitivity of LLM inference. NeSTR preserves explicit temporal relations through symbolic encoding, enforces logical consistency via verification, and corrects flawed inferences using abductive reflection. Extensive experiments on diverse temporal question answering benchmarks demonstrate that NeSTR achieves superior zero-shot performance and consistently improves temporal reasoning without any fine-tuning, showcasing the advantage of neuro-symbolic integration in enhancing temporal understanding in large language models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Clinical Interpretability of Deep Learning Segmentation Through Shapley-Derived Agreement and Uncertainty Metrics</title>
<link>https://arxiv.org/abs/2512.07224</link>
<guid>https://arxiv.org/abs/2512.07224</guid>
<content:encoded><![CDATA[

arXiv:2512.07224v1 Announce Type: cross 
Abstract: Segmentation is the identification of anatomical regions of interest, such as organs, tissue, and lesions, serving as a fundamental task in computer-aided diagnosis in medical imaging. Although deep learning models have achieved remarkable performance in medical image segmentation, the need for explainability remains critical for ensuring their acceptance and integration in clinical practice, despite the growing research attention in this area. Our approach explored the use of contrast-level Shapley values, a systematic perturbation of model inputs to assess feature importance. While other studies have investigated gradient-based techniques through identifying influential regions in imaging inputs, Shapley values offer a broader, clinically aligned approach, explaining how model performance is fairly attributed to certain imaging contrasts over others. Using the BraTS 2024 dataset, we generated rankings for Shapley values for four MRI contrasts across four model architectures. Two metrics were proposed from the Shapley ranking: agreement between model and ``clinician" imaging ranking, and uncertainty quantified through Shapley ranking variance across cross-validation folds. Higher-performing cases (Dice \textgreater0.6) showed significantly greater agreement with clinical rankings. Increased Shapley ranking variance correlated with decreased performance (U-Net: $r=-0.581$). These metrics provide clinically interpretable proxies for model reliability, helping clinicians better understand state-of-the-art segmentation models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Robust Protective Perturbation against DeepFake Face Swapping</title>
<link>https://arxiv.org/abs/2512.07228</link>
<guid>https://arxiv.org/abs/2512.07228</guid>
<content:encoded><![CDATA[

arXiv:2512.07228v1 Announce Type: cross 
Abstract: DeepFake face swapping enables highly realistic identity forgeries, posing serious privacy and security risks. A common defence embeds invisible perturbations into images, but these are fragile and often destroyed by basic transformations such as compression or resizing. In this paper, we first conduct a systematic analysis of 30 transformations across six categories and show that protection robustness is highly sensitive to the choice of training transformations, making the standard Expectation over Transformation (EOT) with uniform sampling fundamentally suboptimal. Motivated by this, we propose Expectation Over Learned distribution of Transformation (EOLT), the framework to treat transformation distribution as a learnable component rather than a fixed design choice. Specifically, EOLT employs a policy network that learns to automatically prioritize critical transformations and adaptively generate instance-specific perturbations via reinforcement learning, enabling explicit modeling of defensive bottlenecks while maintaining broad transferability. Extensive experiments demonstrate that our method achieves substantial improvements over state-of-the-art approaches, with 26% higher average robustness and up to 30% gains on challenging transformation categories.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdLift: Lifting Adversarial Perturbations to Safeguard 3D Gaussian Splatting Assets Against Instruction-Driven Editing</title>
<link>https://arxiv.org/abs/2512.07247</link>
<guid>https://arxiv.org/abs/2512.07247</guid>
<content:encoded><![CDATA[

arXiv:2512.07247v1 Announce Type: cross 
Abstract: Recent studies have extended diffusion-based instruction-driven 2D image editing pipelines to 3D Gaussian Splatting (3DGS), enabling faithful manipulation of 3DGS assets and greatly advancing 3DGS content creation. However, it also exposes these assets to serious risks of unauthorized editing and malicious tampering. Although imperceptible adversarial perturbations against diffusion models have proven effective for protecting 2D images, applying them to 3DGS encounters two major challenges: view-generalizable protection and balancing invisibility with protection capability. In this work, we propose the first editing safeguard for 3DGS, termed AdLift, which prevents instruction-driven editing across arbitrary views and dimensions by lifting strictly bounded 2D adversarial perturbations into 3D Gaussian-represented safeguard. To ensure both adversarial perturbations effectiveness and invisibility, these safeguard Gaussians are progressively optimized across training views using a tailored Lifted PGD, which first conducts gradient truncation during back-propagation from the editing model at the rendered image and applies projected gradients to strictly constrain the image-level perturbation. Then, the resulting perturbation is backpropagated to the safeguard Gaussian parameters via an image-to-Gaussian fitting operation. We alternate between gradient truncation and image-to-Gaussian fitting, yielding consistent adversarial-based protection performance across different viewpoints and generalizes to novel views. Empirically, qualitative and quantitative results demonstrate that AdLift effectively protects against state-of-the-art instruction-driven 2D image and 3DGS editing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-negative DAG Learning from Time-Series Data</title>
<link>https://arxiv.org/abs/2512.07267</link>
<guid>https://arxiv.org/abs/2512.07267</guid>
<content:encoded><![CDATA[

arXiv:2512.07267v1 Announce Type: cross 
Abstract: This work aims to learn the directed acyclic graph (DAG) that captures the instantaneous dependencies underlying a multivariate time series. The observed data follow a linear structural vector autoregressive model (SVARM) with both instantaneous and time-lagged dependencies, where the instantaneous structure is modeled by a DAG to reflect potential causal relationships. While recent continuous relaxation approaches impose acyclicity through smooth constraint functions involving powers of the adjacency matrix, they lead to non-convex optimization problems that are challenging to solve. In contrast, we assume that the underlying DAG has only non-negative edge weights, and leverage this additional structure to impose acyclicity via a convex constraint. This enables us to cast the problem of non-negative DAG recovery from multivariate time-series data as a convex optimization problem in abstract form, which we solve using the method of multipliers. Crucially, the convex formulation guarantees global optimality of the solution. Finally, we assess the performance of the proposed method on synthetic time-series data, where it outperforms existing alternatives.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A graph generation pipeline for critical infrastructures based on heuristics, images and depth data</title>
<link>https://arxiv.org/abs/2512.07269</link>
<guid>https://arxiv.org/abs/2512.07269</guid>
<content:encoded><![CDATA[

arXiv:2512.07269v1 Announce Type: cross 
Abstract: Virtual representations of physical critical infrastructures, such as water or energy plants, are used for simulations and digital twins to ensure resilience and continuity of their services. These models usually require 3D point clouds from laser scanners that are expensive to acquire and require specialist knowledge to use. In this article, we present a graph generation pipeline based on photogrammetry. The pipeline detects relevant objects and predicts their relation using RGB images and depth data generated by a stereo camera. This more cost-effective approach uses deep learning for object detection and instance segmentation of the objects, and employs user-defined heuristics or rules to infer their relations. Results of two hydraulic systems show that this strategy can produce graphs close to the ground truth while its flexibility allows the method to be tailored to specific applications and its transparency qualifies it to be used in the high stakes decision-making that is required for critical infrastructures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifiable Deep Quantitative Group Testing</title>
<link>https://arxiv.org/abs/2512.07279</link>
<guid>https://arxiv.org/abs/2512.07279</guid>
<content:encoded><![CDATA[

arXiv:2512.07279v1 Announce Type: cross 
Abstract: We present a neural network-based framework for solving the quantitative group testing (QGT) problem that achieves both high decoding accuracy and structural verifiability. In QGT, the objective is to identify a small subset of defective items among $N$ candidates using only $M \ll N$ pooled tests, each reporting the number of defectives in the tested subset. We train a multi-layer perceptron to map noisy measurement vectors to binary defect indicators, achieving accurate and robust recovery even under sparse, bounded perturbations. Beyond accuracy, we show that the trained network implicitly learns the underlying pooling structure that links items to tests, allowing this structure to be recovered directly from the network's Jacobian. This indicates that the model does not merely memorize training patterns but internalizes the true combinatorial relationships governing QGT. Our findings reveal that standard feedforward architectures can learn verifiable inverse mappings in structured combinatorial recovery problems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Equivariant Diffusion for Crystal Structure Prediction</title>
<link>https://arxiv.org/abs/2512.07289</link>
<guid>https://arxiv.org/abs/2512.07289</guid>
<content:encoded><![CDATA[

arXiv:2512.07289v1 Announce Type: cross 
Abstract: In addressing the challenge of Crystal Structure Prediction (CSP), symmetry-aware deep learning models, particularly diffusion models, have been extensively studied, which treat CSP as a conditional generation task. However, ensuring permutation, rotation, and periodic translation equivariance during diffusion process remains incompletely addressed. In this work, we propose EquiCSP, a novel equivariant diffusion-based generative model. We not only address the overlooked issue of lattice permutation equivariance in existing models, but also develop a unique noising algorithm that rigorously maintains periodic translation equivariance throughout both training and inference processes. Our experiments indicate that EquiCSP significantly surpasses existing models in terms of generating accurate structures and demonstrates faster convergence during the training process.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exact Synthetic Populations for Scalable Societal and Market Modeling</title>
<link>https://arxiv.org/abs/2512.07306</link>
<guid>https://arxiv.org/abs/2512.07306</guid>
<content:encoded><![CDATA[

arXiv:2512.07306v1 Announce Type: cross 
Abstract: We introduce a constraint-programming framework for generating synthetic populations that reproduce target statistics with high precision while enforcing full individual consistency. Unlike data-driven approaches that infer distributions from samples, our method directly encodes aggregated statistics and structural relations, enabling exact control of demographic profiles without requiring any microdata. We validate the approach on official demographic sources and study the impact of distributional deviations on downstream analyses. This work is conducted within the Pollitics project developed by Emotia, where synthetic populations can be queried through large language models to model societal behaviors, explore market and policy scenarios, and provide reproducible decision-grade insights without personal data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>M-STAR: Multi-Scale Spatiotemporal Autoregression for Human Mobility Modeling</title>
<link>https://arxiv.org/abs/2512.07314</link>
<guid>https://arxiv.org/abs/2512.07314</guid>
<content:encoded><![CDATA[

arXiv:2512.07314v1 Announce Type: cross 
Abstract: Modeling human mobility is vital for extensive applications such as transportation planning and epidemic modeling. With the rise of the Artificial Intelligence Generated Content (AIGC) paradigm, recent works explore synthetic trajectory generation using autoregressive and diffusion models. While these methods show promise for generating single-day trajectories, they remain limited by inefficiencies in long-term generation (e.g., weekly trajectories) and a lack of explicit spatiotemporal multi-scale modeling. This study proposes Multi-Scale Spatio-Temporal AutoRegression (M-STAR), a new framework that generates long-term trajectories through a coarse-to-fine spatiotemporal prediction process. M-STAR combines a Multi-scale Spatiotemporal Tokenizer that encodes hierarchical mobility patterns with a Transformer-based decoder for next-scale autoregressive prediction. Experiments on two real-world datasets show that M-STAR outperforms existing methods in fidelity and significantly improves generation speed. The data and codes are available at https://github.com/YuxiaoLuo0013/M-STAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Two-dimensional RMSD projections for reaction path visualization and validation</title>
<link>https://arxiv.org/abs/2512.07329</link>
<guid>https://arxiv.org/abs/2512.07329</guid>
<content:encoded><![CDATA[

arXiv:2512.07329v1 Announce Type: cross 
Abstract: Transition state or minimum energy path finding methods constitute a routine component of the computational chemistry toolkit. Standard analysis involves trajectories conventionally plotted in terms of the relative energy to the initial state against a cumulative displacement variable, or the image number. These dimensional reductions obscure structural rearrangements in high dimensions and may often be trajectory dependent. This precludes the ability to compare optimization trajectories of different methods beyond the number of calculations, time taken, and final saddle geometry. We present a method mapping trajectories onto a two-dimension surface defined by a permutation corrected root mean square deviation from the reactant and product configurations. Energy is represented as an interpolated color-mapped surface constructed from all optimization steps using radial basis functions. This representation highlights optimization trajectories, identifies endpoint basins, and diagnoses convergence concerns invisible in one-dimensional profiles. We validate the framework on a cycloaddition reaction, showing that a machine-learned potential saddle and density functional theory reference lie on comparable energy contours despite geometric displacements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning in an expectation-maximisation framework for nowcasting</title>
<link>https://arxiv.org/abs/2512.07335</link>
<guid>https://arxiv.org/abs/2512.07335</guid>
<content:encoded><![CDATA[

arXiv:2512.07335v1 Announce Type: cross 
Abstract: Decision making often occurs in the presence of incomplete information, leading to the under- or overestimation of risk. Leveraging the observable information to learn the complete information is called nowcasting. In practice, incomplete information is often a consequence of reporting or observation delays. In this paper, we propose an expectation-maximisation (EM) framework for nowcasting that uses machine learning techniques to model both the occurrence as well as the reporting process of events. We allow for the inclusion of covariate information specific to the occurrence and reporting periods as well as characteristics related to the entity for which events occurred. We demonstrate how the maximisation step and the information flow between EM iterations can be tailored to leverage the predictive power of neural networks and (extreme) gradient boosting machines (XGBoost). With simulation experiments, we show that we can effectively model both the occurrence and reporting of events when dealing with high-dimensional covariate information. In the presence of non-linear effects, we show that our methodology outperforms existing EM-based nowcasting frameworks that use generalised linear models in the maximisation step. Finally, we apply the framework to the reporting of Argentinian Covid-19 cases, where the XGBoost-based approach again is most performant.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PrivORL: Differentially Private Synthetic Dataset for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.07342</link>
<guid>https://arxiv.org/abs/2512.07342</guid>
<content:encoded><![CDATA[

arXiv:2512.07342v1 Announce Type: cross 
Abstract: Recently, offline reinforcement learning (RL) has become a popular RL paradigm. In offline RL, data providers share pre-collected datasets -- either as individual transitions or sequences of transitions forming trajectories -- to enable the training of RL models (also called agents) without direct interaction with the environments. Offline RL saves interactions with environments compared to traditional RL, and has been effective in critical areas, such as navigation tasks. Meanwhile, concerns about privacy leakage from offline RL datasets have emerged.
  To safeguard private information in offline RL datasets, we propose the first differential privacy (DP) offline dataset synthesis method, PrivORL, which leverages a diffusion model and diffusion transformer to synthesize transitions and trajectories, respectively, under DP. The synthetic dataset can then be securely released for downstream analysis and research. PrivORL adopts the popular approach of pre-training a synthesizer on public datasets, and then fine-tuning on sensitive datasets using DP Stochastic Gradient Descent (DP-SGD). Additionally, PrivORL introduces curiosity-driven pre-training, which uses feedback from the curiosity module to diversify the synthetic dataset and thus can generate diverse synthetic transitions and trajectories that closely resemble the sensitive dataset. Extensive experiments on five sensitive offline RL datasets show that our method achieves better utility and fidelity in both DP transition and trajectory synthesis compared to baselines. The replication package is available at the GitHub repository.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Geometric Unification of Concept Learning with Concept Cones</title>
<link>https://arxiv.org/abs/2512.07355</link>
<guid>https://arxiv.org/abs/2512.07355</guid>
<content:encoded><![CDATA[

arXiv:2512.07355v1 Announce Type: cross 
Abstract: Two traditions of interpretability have evolved side by side but seldom spoken to each other: Concept Bottleneck Models (CBMs), which prescribe what a concept should be, and Sparse Autoencoders (SAEs), which discover what concepts emerge. While CBMs use supervision to align activations with human-labeled concepts, SAEs rely on sparse coding to uncover emergent ones. We show that both paradigms instantiate the same geometric structure: each learns a set of linear directions in activation space whose nonnegative combinations form a concept cone. Supervised and unsupervised methods thus differ not in kind but in how they select this cone. Building on this view, we propose an operational bridge between the two paradigms. CBMs provide human-defined reference geometries, while SAEs can be evaluated by how well their learned cones approximate or contain those of CBMs. This containment framework yields quantitative metrics linking inductive biases -- such as SAE type, sparsity, or expansion ratio -- to emergence of plausible\footnote{We adopt the terminology of \citet{jacovi2020towards}, who distinguish between faithful explanations (accurately reflecting model computations) and plausible explanations (aligning with human intuition and domain knowledge). CBM concepts are plausible by construction -- selected or annotated by humans -- though not necessarily faithful to the true latent factors that organise the data manifold.} concepts. Using these metrics, we uncover a ``sweet spot'' in both sparsity and expansion factor that maximizes both geometric and semantic alignment with CBM concepts. Overall, our work unifies supervised and unsupervised concept discovery through a shared geometric framework, providing principled metrics to measure SAE progress and assess how well discovered concept align with plausible human concepts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do LLMs Trust the Code They Write?</title>
<link>https://arxiv.org/abs/2512.07404</link>
<guid>https://arxiv.org/abs/2512.07404</guid>
<content:encoded><![CDATA[

arXiv:2512.07404v1 Announce Type: cross 
Abstract: Despite the effectiveness of large language models (LLMs) for code generation, they often output incorrect code. One reason is that model output probabilities are often not well-correlated with correctness, and reflect only the final output of the generation process. Inspired by findings that LLMs internally encode concepts like truthfulness, this paper explores if LLMs similarly represent code correctness. Specifically, we identify a correctness representation inside LLMs by contrasting the hidden states between pairs of correct and incorrect code for the same programming tasks. By experimenting on four LLMs, we show that exploiting this extracted correctness representation outperforms standard log-likelihood ranking, as well as verbalized model confidence. Furthermore, we explore how this internal correctness signal can be used to select higher-quality code samples, without requiring test execution. Ultimately, this work demonstrates how leveraging internal representations can enhance code generation systems and make LLMs more reliable, thus improving confidence in automatically generated code.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Microseismic event classification with a lightweight Fourier Neural Operator model</title>
<link>https://arxiv.org/abs/2512.07425</link>
<guid>https://arxiv.org/abs/2512.07425</guid>
<content:encoded><![CDATA[

arXiv:2512.07425v1 Announce Type: cross 
Abstract: Real-time monitoring of induced seismicity is crucial for mitigating operational hazards, relying on the rapid and accurate classification of microseismic events from continuous data streams. However, while many deep learning models excel at this task, their high computational requirements often limit their practical application in real-time monitoring systems. To address this limitation, a lightweight model based on the Fourier Neural Operator (FNO) is proposed for microseismic event classification, leveraging its inherent resolution-invariance and computational efficiency for waveform processing. In the STanford EArthquake Dataset (STEAD), a global and large-scale database of seismic waveforms, the FNO-based model demonstrates high effectiveness for trigger classification, with an F1 score of 95% even in the scenario of data sparsity in training. The new FNO model greatly decreases the computer power needed relative to current deep learning models without sacrificing the classification success rate measured by the F1 score. A test on a real microseismic dataset shows a classification success rate with an F1 score of 98%, outperforming many traditional deep-learning techniques. A combination of high success rate and low computational power indicates that the FNO model can serve as a methodology of choice for real-time monitoring of microseismicity for induced seismicity. The method saves computational resources and facilitates both post-processing and real-time seismic processing suitable for the implementation of traffic light systems to prevent undesired induced seismicity.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimized Machine Learning Methods for Studying the Thermodynamic Behavior of Complex Spin Systems</title>
<link>https://arxiv.org/abs/2512.07458</link>
<guid>https://arxiv.org/abs/2512.07458</guid>
<content:encoded><![CDATA[

arXiv:2512.07458v1 Announce Type: cross 
Abstract: This paper presents a systematic study of the application of convolutional neural networks (CNNs) as an efficient and versatile tool for the analysis of critical and low-temperature phase states in spin system models. The problem of calculating the dependence of the average energy on the spatial distribution of exchange integrals for the Edwards-Anderson model on a square lattice with frustrated interactions is considered. We further construct a single convolutional classifier of phase states of the ferromagnetic Ising model on square, triangular, honeycomb, and kagome lattices, trained on configurations generated by the Swendsen-Wang cluster algorithm. Computed temperature profiles of the averaged posterior probability of the high-temperature phase form clear S-shaped curves that intersect in the vicinity of the theoretical critical temperatures and allow one to determine the critical temperature for the kagome lattice without additional retraining. It is shown that convolutional models substantially reduce the root-mean-square error (RMSE) compared with fully connected architectures and efficiently capture complex correlations between thermodynamic characteristics and the structure of magnetic correlated systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding LLM Agent Behaviours via Game Theory: Strategy Recognition, Biases and Multi-Agent Dynamics</title>
<link>https://arxiv.org/abs/2512.07462</link>
<guid>https://arxiv.org/abs/2512.07462</guid>
<content:encoded><![CDATA[

arXiv:2512.07462v1 Announce Type: cross 
Abstract: As Large Language Models (LLMs) increasingly operate as autonomous decision-makers in interactive and multi-agent systems and human societies, understanding their strategic behaviour has profound implications for safety, coordination, and the design of AI-driven social and economic infrastructures. Assessing such behaviour requires methods that capture not only what LLMs output, but the underlying intentions that guide their decisions. In this work, we extend the FAIRGAME framework to systematically evaluate LLM behaviour in repeated social dilemmas through two complementary advances: a payoff-scaled Prisoners Dilemma isolating sensitivity to incentive magnitude, and an integrated multi-agent Public Goods Game with dynamic payoffs and multi-agent histories. These environments reveal consistent behavioural signatures across models and languages, including incentive-sensitive cooperation, cross-linguistic divergence and end-game alignment toward defection. To interpret these patterns, we train traditional supervised classification models on canonical repeated-game strategies and apply them to FAIRGAME trajectories, showing that LLMs exhibit systematic, model- and language-dependent behavioural intentions, with linguistic framing at times exerting effects as strong as architectural differences. Together, these findings provide a unified methodological foundation for auditing LLMs as strategic agents and reveal systematic cooperation biases with direct implications for AI governance, collective decision-making, and the design of safe multi-agent systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Affordance Field Intervention: Enabling VLAs to Escape Memory Traps in Robotic Manipulation</title>
<link>https://arxiv.org/abs/2512.07472</link>
<guid>https://arxiv.org/abs/2512.07472</guid>
<content:encoded><![CDATA[

arXiv:2512.07472v1 Announce Type: cross 
Abstract: Vision-Language-Action (VLA) models have shown great performance in robotic manipulation by mapping visual observations and language instructions directly to actions. However, they remain brittle under distribution shifts: when test scenarios change, VLAs often reproduce memorized trajectories instead of adapting to the updated scene, which is a failure mode we refer to as the "Memory Trap". This limitation stems from the end-to-end design, which lacks explicit 3D spatial reasoning and prevents reliable identification of actionable regions in unfamiliar environments. To compensate for this missing spatial understanding, 3D Spatial Affordance Fields (SAFs) can provide a geometric representation that highlights where interactions are physically feasible, offering explicit cues about regions the robot should approach or avoid. We therefore introduce Affordance Field Intervention (AFI), a lightweight hybrid framework that uses SAFs as an on-demand plug-in to guide VLA behavior. Our system detects memory traps through proprioception, repositions the robot to recent high-affordance regions, and proposes affordance-driven waypoints that anchor VLA-generated actions. A SAF-based scorer then selects trajectories with the highest cumulative affordance. Extensive experiments demonstrate that our method achieves an average improvement of 23.5% across different VLA backbones ($\pi_{0}$ and $\pi_{0.5}$) under out-of-distribution scenarios on real-world robotic platforms, and 20.2% on the LIBERO-Pro benchmark, validating its effectiveness in enhancing VLA robustness to distribution shifts.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation</title>
<link>https://arxiv.org/abs/2512.07540</link>
<guid>https://arxiv.org/abs/2512.07540</guid>
<content:encoded><![CDATA[

arXiv:2512.07540v1 Announce Type: cross 
Abstract: Error Span Detection (ESD) is a subtask of automatic machine translation evaluation that localizes error spans in translations and labels their severity. State-of-the-art generative ESD methods typically decode using Maximum a Posteriori (MAP), assuming that model-estimated probabilities are perfectly correlated with similarity to human annotation. However, we observed that annotations dissimilar to the human annotation could achieve a higher model likelihood than the human annotation. We address this issue by applying Minimum Bayes Risk (MBR) decoding to generative ESD models. Specifically, we employ sentence- and span-level similarity metrics as utility functions to select candidate hypotheses based on their approximate similarity to the human annotation. Extensive experimental results show that our MBR decoding outperforms the MAP baseline at the system, sentence, and span-levels. Furthermore, to mitigate the computational cost of MBR decoding, we demonstrate that applying MBR distillation enables a standard greedy model to match MBR decoding performance, effectively eliminating the inference-time latency bottleneck.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Dimensional Change Point Detection using Graph Spanning Ratio</title>
<link>https://arxiv.org/abs/2512.07541</link>
<guid>https://arxiv.org/abs/2512.07541</guid>
<content:encoded><![CDATA[

arXiv:2512.07541v1 Announce Type: cross 
Abstract: Inspired by graph-based methodologies, we introduce a novel graph-spanning algorithm designed to identify changes in both offline and online data across low to high dimensions. This versatile approach is applicable to Euclidean and graph-structured data with unknown distributions, while maintaining control over error probabilities. Theoretically, we demonstrate that the algorithm achieves high detection power when the magnitude of the change surpasses the lower bound of the minimax separation rate, which scales on the order of $\sqrt{nd}$. Our method outperforms other techniques in terms of accuracy for both Gaussian and non-Gaussian data. Notably, it maintains strong detection power even with small observation windows, making it particularly effective for online environments where timely and precise change detection is critical.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On Conditional Independence Graph Learning From Multi-Attribute Gaussian Dependent Time Series</title>
<link>https://arxiv.org/abs/2512.07557</link>
<guid>https://arxiv.org/abs/2512.07557</guid>
<content:encoded><![CDATA[

arXiv:2512.07557v1 Announce Type: cross 
Abstract: Estimation of the conditional independence graph (CIG) of high-dimensional multivariate Gaussian time series from multi-attribute data is considered. Existing methods for graph estimation for such data are based on single-attribute models where one associates a scalar time series with each node. In multi-attribute graphical models, each node represents a random vector or vector time series. In this paper we provide a unified theoretical analysis of multi-attribute graph learning for dependent time series using a penalized log-likelihood objective function formulated in the frequency domain using the discrete Fourier transform of the time-domain data. We consider both convex (sparse-group lasso) and non-convex (log-sum and SCAD group penalties) penalty/regularization functions. We establish sufficient conditions in a high-dimensional setting for consistency (convergence of the inverse power spectral density to true value in the Frobenius norm), local convexity when using non-convex penalties, and graph recovery. We do not impose any incoherence or irrepresentability condition for our convergence results. We also empirically investigate selection of the tuning parameters based on the Bayesian information criterion, and illustrate our approach using numerical examples utilizing both synthetic and real data.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Toward More Reliable Artificial Intelligence: Reducing Hallucinations in Vision-Language Models</title>
<link>https://arxiv.org/abs/2512.07564</link>
<guid>https://arxiv.org/abs/2512.07564</guid>
<content:encoded><![CDATA[

arXiv:2512.07564v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) frequently generate hallucinated content plausible but incorrect claims about image content. We propose a training-free self-correction framework enabling VLMs to iteratively refine responses through uncertainty-guided visual re-attention. Our method combines multidimensional uncertainty quantification (token entropy, attention dispersion, semantic consistency, claim confidence) with attention-guided cropping of under-explored regions. Operating entirely with frozen, pretrained VLMs, our framework requires no gradient updates. We validate our approach on the POPE and MMHAL BENCH benchmarks using the Qwen2.5-VL-7B [23] architecture. Experimental results demonstrate that our method reduces hallucination rates by 9.8 percentage points compared to the baseline, while improving object existence accuracy by 4.7 points on adversarial splits. Furthermore, qualitative analysis confirms that uncertainty-guided re-attention successfully grounds corrections in visual evidence where standard decoding fails. We validate our approach on Qwen2.5-VL-7B [23], with plans to extend validation across diverse architectures in future versions. We release our code and methodology to facilitate future research in trustworthy multimodal systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>$\phi$-test: Global Feature Selection and Inference for Shapley Additive Explanations</title>
<link>https://arxiv.org/abs/2512.07578</link>
<guid>https://arxiv.org/abs/2512.07578</guid>
<content:encoded><![CDATA[

arXiv:2512.07578v1 Announce Type: cross 
Abstract: We propose $\phi$-test, a global feature-selection and significance procedure for black-box predictors that combines Shapley attributions with selective inference. Given a trained model and an evaluation dataset, $\phi$-test performs SHAP-guided screening and fits a linear surrogate on the screened features via a selection rule with a tractable selective-inference form. For each retained feature, it outputs a Shapley-based global score, a surrogate coefficient, and post-selection $p$-values and confidence intervals in a global feature-importance table. Experiments on real tabular regression tasks with tree-based and neural backbones suggest that $\phi$-test can retain much of the predictive ability of the original model while using only a few features and producing feature sets that remain fairly stable across resamples and backbone classes. In these settings, $\phi$-test acts as a practical global explanation layer linking Shapley-based importance summaries with classical statistical inference.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative Analysis and Parametric Tuning of PPO, GRPO, and DAPO for LLM Reasoning Enhancement</title>
<link>https://arxiv.org/abs/2512.07611</link>
<guid>https://arxiv.org/abs/2512.07611</guid>
<content:encoded><![CDATA[

arXiv:2512.07611v1 Announce Type: cross 
Abstract: This study presents a systematic comparison of three Reinforcement Learning (RL) algorithms (PPO, GRPO, and DAPO) for improving complex reasoning in large language models (LLMs). Our main contribution is a controlled transfer-learning evaluation: models are first fine-tuned on the specialized Countdown Game and then assessed on a suite of general-purpose reasoning benchmarks. Across all tasks, RL-trained models outperform their corresponding base models, although the degree of improvement differs by benchmark.
  Our parametric analysis offers practical guidance for RL-based LLM training. Increasing the group size in GRPO and DAPO leads to more stable training dynamics and higher accuracy, while the impact of the KL-penalty coefficient is non-monotonic. Additionally, we find that the Dynamic Sampling (DS) component in DAPO does not improve performance; in fact, the best overall results are achieved with DAPO when DS is disabled.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCMind-2.1-Kaiyuan-2B Technical Report</title>
<link>https://arxiv.org/abs/2512.07612</link>
<guid>https://arxiv.org/abs/2512.07612</guid>
<content:encoded><![CDATA[

arXiv:2512.07612v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) has resulted in a significant knowledge gap between the open-source community and industry, primarily because the latter relies on closed-source, high-quality data and training recipes. To address this, we introduce PCMind-2.1-Kaiyuan-2B, a fully open-source 2-billion-parameter model focused on improving training efficiency and effectiveness under resource constraints. Our methodology includes three key innovations: a Quantile Data Benchmarking method for systematically comparing heterogeneous open-source datasets and providing insights on data mixing strategies; a Strategic Selective Repetition scheme within a multi-phase paradigm to effectively leverage sparse, high-quality data; and a Multi-Domain Curriculum Training policy that orders samples by quality. Supported by a highly optimized data preprocessing pipeline and architectural modifications for FP16 stability, Kaiyuan-2B achieves performance competitive with state-of-the-art fully open-source models, demonstrating practical and scalable solutions for resource-limited pretraining. We release all assets (including model weights, data, and code) under Apache 2.0 license at https://huggingface.co/thu-pacman/PCMind-2.1-Kaiyuan-2B.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Agent Capability Problem: Predicting Solvability Through Information-Theoretic Bounds</title>
<link>https://arxiv.org/abs/2512.07631</link>
<guid>https://arxiv.org/abs/2512.07631</guid>
<content:encoded><![CDATA[

arXiv:2512.07631v1 Announce Type: cross 
Abstract: When should an autonomous agent commit resources to a task? We introduce the Agent Capability Problem (ACP), a framework for predicting whether an agent can solve a problem under resource constraints. Rather than relying on empirical heuristics, ACP frames problem-solving as information acquisition: an agent requires $\Itotal$ bits to identify a solution and gains $\Istep$ bits per action at cost $\Cstep$, yielding an effective cost $\Ceff = (\Itotal/\Istep), \Cstep$ that predicts resource requirements before search. We prove that $\Ceff$ lower-bounds expected cost and provide tight probabilistic upper bounds. Experimental validation shows that ACP predictions closely track actual agent performance, consistently bounding search effort while improving efficiency over greedy and random strategies. The framework generalizes across LLM-based and agentic workflows, linking principles from active learning, Bayesian optimization, and reinforcement learning through a unified information-theoretic lens. \
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Test-time Scaling via Prediction Merging on Large-Scale Recommendation</title>
<link>https://arxiv.org/abs/2512.07650</link>
<guid>https://arxiv.org/abs/2512.07650</guid>
<content:encoded><![CDATA[

arXiv:2512.07650v1 Announce Type: cross 
Abstract: Inspired by the success of language models (LM), scaling up deep learning recommendation systems (DLRS) has become a recent trend in the community. All previous methods tend to scale up the model parameters during training time. However, how to efficiently utilize and scale up computational resources during test time remains underexplored, which can prove to be a scaling-efficient approach and bring orthogonal improvements in LM domains. The key point in applying test-time scaling to DLRS lies in effectively generating diverse yet meaningful outputs for the same instance. We propose two ways: One is to explore the heterogeneity of different model architectures. The other is to utilize the randomness of model initialization under a homogeneous architecture. The evaluation is conducted across eight models, including both classic and SOTA models, on three benchmarks. Sufficient evidence proves the effectiveness of both solutions. We further prove that under the same inference budget, test-time scaling can outperform parameter scaling. Our test-time scaling can also be seamlessly accelerated with the increase in parallel servers when deployed online, without affecting the inference time on the user side. Code is available.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delay-Aware Diffusion Policy: Bridging the Observation-Execution Gap in Dynamic Tasks</title>
<link>https://arxiv.org/abs/2512.07697</link>
<guid>https://arxiv.org/abs/2512.07697</guid>
<content:encoded><![CDATA[

arXiv:2512.07697v1 Announce Type: cross 
Abstract: As a robot senses and selects actions, the world keeps changing. This inference delay creates a gap of tens to hundreds of milliseconds between the observed state and the state at execution. In this work, we take the natural generalization from zero delay to measured delay during training and inference. We introduce Delay-Aware Diffusion Policy (DA-DP), a framework for explicitly incorporating inference delays into policy learning. DA-DP corrects zero-delay trajectories to their delay-compensated counterparts, and augments the policy with delay conditioning. We empirically validate DA-DP on a variety of tasks, robots, and delays and find its success rate more robust to delay than delay-unaware methods. DA-DP is architecture agnostic and transfers beyond diffusion policies, offering a general pattern for delay-aware imitation learning. More broadly, DA-DP encourages evaluation protocols that report performance as a function of measured latency, not just task difficulty.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PVeRA: Probabilistic Vector-Based Random Matrix Adaptation</title>
<link>https://arxiv.org/abs/2512.07703</link>
<guid>https://arxiv.org/abs/2512.07703</guid>
<content:encoded><![CDATA[

arXiv:2512.07703v1 Announce Type: cross 
Abstract: Large foundation models have emerged in the last years and are pushing performance boundaries for a variety of tasks. Training or even finetuning such models demands vast datasets and computational resources, which are often scarce and costly. Adaptation methods provide a computationally efficient solution to address these limitations by allowing such models to be finetuned on small amounts of data and computing power. This is achieved by appending new trainable modules to frozen backbones with only a fraction of the trainable parameters and fitting only these modules on novel tasks. Recently, the VeRA adapter was shown to excel in parameter-efficient adaptations by utilizing a pair of frozen random low-rank matrices shared across all layers. In this paper, we propose PVeRA, a probabilistic version of the VeRA adapter, which modifies the low-rank matrices of VeRA in a probabilistic manner. This modification naturally allows handling inherent ambiguities in the input and allows for different sampling configurations during training and testing. A comprehensive evaluation was performed on the VTAB-1k benchmark and seven adapters, with PVeRA outperforming VeRA and other adapters. Our code for training models with PVeRA and benchmarking all adapters is available https://github.com/leofillioux/pvera.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Each Prompt Matters: Scaling Reinforcement Learning Without Wasting Rollouts on Hundred-Billion-Scale MoE</title>
<link>https://arxiv.org/abs/2512.07710</link>
<guid>https://arxiv.org/abs/2512.07710</guid>
<content:encoded><![CDATA[

arXiv:2512.07710v1 Announce Type: cross 
Abstract: We present CompassMax-V3-Thinking, a hundred-billion-scale MoE reasoning model trained with a new RL framework built on one principle: each prompt must matter. Scaling RL to this size exposes critical inefficiencies-zero-variance prompts that waste rollouts, unstable importance sampling over long horizons, advantage inversion from standard reward models, and systemic bottlenecks in rollout processing. To overcome these challenges, we introduce several unified innovations: (1) Multi-Stage Zero-Variance Elimination, which filters out non-informative prompts and stabilizes group-based policy optimization (e.g. GRPO) by removing wasted rollouts; (2) ESPO, an entropy-adaptive optimization method that balances token-level and sequence-level importance sampling to maintain stable learning dynamics; (3) a Router Replay strategy that aligns training-time MoE router decisions with inference-time behavior to mitigate train-infer discrepancies, coupled with a reward model adjustment to prevent advantage inversion; (4) a high-throughput RL system with FP8-precision rollouts, overlapped reward computation, and length-aware scheduling to eliminate performance bottlenecks. Together, these contributions form a cohesive pipeline that makes RL on hundred-billion-scale MoE models stable and efficient. The resulting model delivers strong performance across both internal and public evaluations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A scalable and real-time neural decoder for topological quantum codes</title>
<link>https://arxiv.org/abs/2512.07737</link>
<guid>https://arxiv.org/abs/2512.07737</guid>
<content:encoded><![CDATA[

arXiv:2512.07737v1 Announce Type: cross 
Abstract: Fault-tolerant quantum computing will require error rates far below those achievable with physical qubits. Quantum error correction (QEC) bridges this gap, but depends on decoders being simultaneously fast, accurate, and scalable. This combination of requirements has not yet been met by a machine-learning decoder, nor by any decoder for promising resource-efficient codes such as the colour code. Here we introduce AlphaQubit 2, a neural-network decoder that achieves near-optimal logical error rates for both surface and colour codes at large scales under realistic noise. For the colour code, it is orders of magnitude faster than other high-accuracy decoders. For the surface code, we demonstrate real-time decoding faster than 1 microsecond per cycle up to distance 11 on current commercial accelerators with better accuracy than leading real-time decoders. These results support the practical application of a wider class of promising QEC codes, and establish a credible path towards high-accuracy, real-time neural decoding at the scales required for fault-tolerant quantum computation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Neural Networks for Source Inversion and Parameters Estimation in Atmospheric Dispersion</title>
<link>https://arxiv.org/abs/2512.07755</link>
<guid>https://arxiv.org/abs/2512.07755</guid>
<content:encoded><![CDATA[

arXiv:2512.07755v1 Announce Type: cross 
Abstract: Recent studies have shown the success of deep learning in solving forward and inverse problems in engineering and scientific computing domains, such as physics-informed neural networks (PINNs). In the fields of atmospheric science and environmental monitoring, estimating emission source locations is a central task that further relies on multiple model parameters that dictate velocity profiles and diffusion parameters. Estimating these parameters at the same time as emission sources from scarce data is a difficult task. In this work, we achieve this by leveraging the flexibility and generality of PINNs. We use a weighted adaptive method based on the neural tangent kernels to solve a source inversion problem with parameter estimation on the 2D and 3D advection-diffusion equations with unknown velocity and diffusion coefficients that may vary in space and time. Our proposed weighted adaptive method is presented as an extension of PINNs for forward PDE problems to a highly ill-posed source inversion and parameter estimation problem. The key idea behind our methodology is to attempt the joint recovery of the solution, the sources along with the unknown parameters, thereby using the underlying partial differential equation as a constraint that couples multiple unknown functional parameters, leading to more efficient use of the limited information in the measurements. We present various numerical experiments, using different types of measurements that model practical engineering systems, to show that our proposed method is indeed successful and robust to additional noise in the measurements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RL-MTJail: Reinforcement Learning for Automated Black-Box Multi-Turn Jailbreaking of Large Language Models</title>
<link>https://arxiv.org/abs/2512.07761</link>
<guid>https://arxiv.org/abs/2512.07761</guid>
<content:encoded><![CDATA[

arXiv:2512.07761v1 Announce Type: cross 
Abstract: Large language models are vulnerable to jailbreak attacks, threatening their safe deployment in real-world applications. This paper studies black-box multi-turn jailbreaks, aiming to train attacker LLMs to elicit harmful content from black-box models through a sequence of prompt-output interactions. Existing approaches typically rely on single turn optimization, which is insufficient for learning long-term attack strategies. To bridge this gap, we formulate the problem as a multi-turn reinforcement learning task, directly optimizing the harmfulness of the final-turn output as the outcome reward. To mitigate sparse supervision and promote long-term attack strategies, we propose two heuristic process rewards: (1) controlling the harmfulness of intermediate outputs to prevent triggering the black-box model's rejection mechanisms, and (2) maintaining the semantic relevance of intermediate outputs to avoid drifting into irrelevant content. Experimental results on multiple benchmarks show consistently improved attack success rates across multiple models, highlighting the effectiveness of our approach. The code is available at https://github.com/xxiqiao/RL-MTJail. Warning: This paper contains examples of harmful content.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-informed Online Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.07770</link>
<guid>https://arxiv.org/abs/2512.07770</guid>
<content:encoded><![CDATA[

arXiv:2512.07770v1 Announce Type: cross 
Abstract: Conformal prediction provides a pivotal and flexible technique for uncertainty quantification by constructing prediction sets with a predefined coverage rate. Many online conformal prediction methods have been developed to address data distribution shifts in fully adversarial environments, resulting in overly conservative prediction sets. We propose Conformal Optimistic Prediction (COP), an online conformal prediction algorithm incorporating underlying data pattern into the update rule. Through estimated cumulative distribution function of non-conformity scores, COP produces tighter prediction sets when predictable pattern exists, while retaining valid coverage guarantees even when estimates are inaccurate. We establish a joint bound on coverage and regret, which further confirms the validity of our approach. We also prove that COP achieves distribution-free, finite-sample coverage under arbitrary learning rates and can converge when scores are $i.i.d.$. The experimental results also show that COP can achieve valid coverage and construct shorter prediction intervals than other baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReasonBENCH: Benchmarking the (In)Stability of LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.07795</link>
<guid>https://arxiv.org/abs/2512.07795</guid>
<content:encoded><![CDATA[

arXiv:2512.07795v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly deployed in settings where reasoning, such as multi-step problem solving and chain-of-thought, is essential. Yet, current evaluation practices overwhelmingly report single-run accuracy while ignoring the intrinsic uncertainty that naturally arises from stochastic decoding. This omission creates a blind spot because practitioners cannot reliably assess whether a method's reported performance is stable, reproducible, or cost-consistent. We introduce ReasonBENCH, the first benchmark designed to quantify the underlying instability in LLM reasoning. ReasonBENCH provides (i) a modular evaluation library that standardizes reasoning frameworks, models, and tasks, (ii) a multi-run protocol that reports statistically reliable metrics for both quality and cost, and (iii) a public leaderboard to encourage variance-aware reporting. Across tasks from different domains, we find that the vast majority of reasoning strategies and models exhibit high instability. Notably, even strategies with similar average performance can display confidence intervals up to four times wider, and the top-performing methods often incur higher and less stable costs. Such instability compromises reproducibility across runs and, consequently, the reliability of reported performance. To better understand these dynamics, we further analyze the impact of prompts, model families, and scale on the trade-off between solve rate and stability. Our results highlight reproducibility as a critical dimension for reliable LLM reasoning and provide a foundation for future reasoning methods and uncertainty quantification techniques. ReasonBENCH is publicly available at https://github.com/au-clan/ReasonBench .
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Collaborative Causal Sensemaking: Closing the Complementarity Gap in Human-AI Decision Support</title>
<link>https://arxiv.org/abs/2512.07801</link>
<guid>https://arxiv.org/abs/2512.07801</guid>
<content:encoded><![CDATA[

arXiv:2512.07801v1 Announce Type: cross 
Abstract: LLM-based agents are rapidly being plugged into expert decision-support, yet in messy, high-stakes settings they rarely make the team smarter: human-AI teams often underperform the best individual, experts oscillate between verification loops and over-reliance, and the promised complementarity does not materialise. We argue this is not just a matter of accuracy, but a fundamental gap in how we conceive AI assistance: expert decisions are made through collaborative cognitive processes where mental models, goals, and constraints are continually co-constructed, tested, and revised between human and AI. We propose Collaborative Causal Sensemaking (CCS) as a research agenda and organizing framework for decision-support agents: systems designed as partners in cognitive work, maintaining evolving models of how particular experts reason, helping articulate and revise goals, co-constructing and stress-testing causal hypotheses, and learning from the outcomes of joint decisions so that both human and agent improve over time. We sketch challenges around training ecologies that make collaborative thinking instrumentally valuable, representations and interaction protocols for co-authored models, and evaluation centred on trust and complementarity. These directions can reframe MAS research around agents that participate in collaborative sensemaking and act as AI teammates that think with their human partners.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LUNA: LUT-Based Neural Architecture for Fast and Low-Cost Qubit Readout</title>
<link>https://arxiv.org/abs/2512.07808</link>
<guid>https://arxiv.org/abs/2512.07808</guid>
<content:encoded><![CDATA[

arXiv:2512.07808v1 Announce Type: cross 
Abstract: Qubit readout is a critical operation in quantum computing systems, which maps the analog response of qubits into discrete classical states. Deep neural networks (DNNs) have recently emerged as a promising solution to improve readout accuracy . Prior hardware implementations of DNN-based readout are resource-intensive and suffer from high inference latency, limiting their practical use in low-latency decoding and quantum error correction (QEC) loops. This paper proposes LUNA, a fast and efficient superconducting qubit readout accelerator that combines low-cost integrator-based preprocessing with Look-Up Table (LUT) based neural networks for classification. The architecture uses simple integrators for dimensionality reduction with minimal hardware overhead, and employs LogicNets (DNNs synthesized into LUT logic) to drastically reduce resource usage while enabling ultra-low-latency inference. We integrate this with a differential evolution based exploration and optimization framework to identify high-quality design points. Our results show up to a 10.95x reduction in area and 30% lower latency with little to no loss in fidelity compared to the state-of-the-art. LUNA enables scalable, low-footprint, and high-speed qubit readout, supporting the development of larger and more reliable quantum computing systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph-Based Learning of Spectro-Topographical EEG Representations with Gradient Alignment for Brain-Computer Interfaces</title>
<link>https://arxiv.org/abs/2512.07820</link>
<guid>https://arxiv.org/abs/2512.07820</guid>
<content:encoded><![CDATA[

arXiv:2512.07820v1 Announce Type: cross 
Abstract: We present a novel graph-based learning of EEG representations with gradient alignment (GEEGA) that leverages multi-domain information to learn EEG representations for brain-computer interfaces. Our model leverages graph convolutional networks to fuse embeddings from frequency-based topographical maps and time-frequency spectrograms, capturing inter-domain relationships. GEEGA addresses the challenge of achieving high inter-class separability, which arises from the temporally dynamic and subject-sensitive nature of EEG signals by incorporating the center loss and pairwise difference loss. Additionally, GEEGA incorporates a gradient alignment strategy to resolve conflicts between gradients from different domains and the fused embeddings, ensuring that discrepancies, where gradients point in conflicting directions, are aligned toward a unified optimization direction. We validate the efficacy of our method through extensive experiments on three publicly available EEG datasets: BCI-2a, CL-Drive and CLARE. Comprehensive ablation studies further highlight the impact of various components of our model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Multi-Layered Honeynet Architecture for Threat Behavior Analysis via Deep Learning</title>
<link>https://arxiv.org/abs/2512.07827</link>
<guid>https://arxiv.org/abs/2512.07827</guid>
<content:encoded><![CDATA[

arXiv:2512.07827v1 Announce Type: cross 
Abstract: The escalating sophistication and variety of cyber threats have rendered static honeypots inadequate, necessitating adaptive, intelligence-driven deception. In this work, ADLAH is introduced: an Adaptive Deep Learning Anomaly Detection Honeynet designed to maximize high-fidelity threat intelligence while minimizing cost through autonomous orchestration of infrastructure. The principal contribution is offered as an end-to-end architectural blueprint and vision for an AI-driven deception platform. Feasibility is evidenced by a functional prototype of the central decision mechanism, in which a reinforcement learning (RL) agent determines, in real time, when sessions should be escalated from low-interaction sensor nodes to dynamically provisioned, high-interaction honeypots. Because sufficient live data were unavailable, field-scale validation is not claimed; instead, design trade-offs and limitations are detailed, and a rigorous roadmap toward empirical evaluation at scale is provided. Beyond selective escalation and anomaly detection, the architecture pursues automated extraction, clustering, and versioning of bot attack chains, a core capability motivated by the empirical observation that exposed services are dominated by automated traffic. Together, these elements delineate a practical path toward cost-efficient capture of high-value adversary behavior, systematic bot versioning, and the production of actionable threat intelligence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do Generalisation Results Generalise?</title>
<link>https://arxiv.org/abs/2512.07832</link>
<guid>https://arxiv.org/abs/2512.07832</guid>
<content:encoded><![CDATA[

arXiv:2512.07832v1 Announce Type: cross 
Abstract: A large language model's (LLM's) out-of-distribution (OOD) generalisation ability is crucial to its deployment. Previous work assessing LLMs' generalisation performance, however, typically focuses on a single out-of-distribution dataset. This approach may fail to precisely evaluate the capabilities of the model, as the data shifts encountered once a model is deployed are much more diverse. In this work, we investigate whether OOD generalisation results generalise. More specifically, we evaluate a model's performance across multiple OOD testsets throughout a finetuning run; we then evaluate the partial correlation of performances across these testsets, regressing out in-domain performance. This allows us to assess how correlated are generalisation performances once in-domain performance is controlled for. Analysing OLMo2 and OPT, we observe no overarching trend in generalisation results: the existence of a positive or negative correlation between any two OOD testsets depends strongly on the specific choice of model analysed.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Relational Visual Similarity</title>
<link>https://arxiv.org/abs/2512.07833</link>
<guid>https://arxiv.org/abs/2512.07833</guid>
<content:encoded><![CDATA[

arXiv:2512.07833v1 Announce Type: cross 
Abstract: Humans do not just see attribute similarity -- we also see relational similarity. An apple is like a peach because both are reddish fruit, but the Earth is also like a peach: its crust, mantle, and core correspond to the peach's skin, flesh, and pit. This ability to perceive and recognize relational similarity, is arguable by cognitive scientist to be what distinguishes humans from other species. Yet, all widely used visual similarity metrics today (e.g., LPIPS, CLIP, DINO) focus solely on perceptual attribute similarity and fail to capture the rich, often surprising relational similarities that humans perceive. How can we go beyond the visible content of an image to capture its relational properties? How can we bring images with the same relational logic closer together in representation space? To answer these questions, we first formulate relational image similarity as a measurable problem: two images are relationally similar when their internal relations or functions among visual elements correspond, even if their visual attributes differ. We then curate 114k image-caption dataset in which the captions are anonymized -- describing the underlying relational logic of the scene rather than its surface content. Using this dataset, we finetune a Vision-Language model to measure the relational similarity between images. This model serves as the first step toward connecting images by their underlying relational structure rather than their visible appearance. Our study shows that while relational similarity has a lot of real-world applications, existing image similarity models fail to capture it -- revealing a critical gap in visual computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Optimal Approximation Factor in Density Estimation</title>
<link>https://arxiv.org/abs/1902.05876</link>
<guid>https://arxiv.org/abs/1902.05876</guid>
<content:encoded><![CDATA[

arXiv:1902.05876v4 Announce Type: replace 
Abstract: Consider the following problem: given two arbitrary densities $q_1,q_2$ and a sample-access to an unknown target density $p$, find which of the $q_i$'s is closer to $p$ in total variation.
  A remarkable result due to Yatracos shows that this problem is tractable in the following sense: there exists an algorithm that uses $O(\epsilon^{-2})$ samples from $p$ and outputs~$q_i$ such that with high probability, $TV(q_i,p) \leq 3\cdot\mathsf{opt} + \epsilon$, where $\mathsf{opt}= \min\{TV(q_1,p),TV(q_2,p)\}$. Moreover, this result extends to any finite class of densities $\mathcal{Q}$: there exists an algorithm that outputs the best density in $\mathcal{Q}$ up to a multiplicative approximation factor of 3.
  We complement and extend this result by showing that: (i) the factor 3 can not be improved if one restricts the algorithm to output a density from $\mathcal{Q}$, and (ii) if one allows the algorithm to output arbitrary densities (e.g.\ a mixture of densities from $\mathcal{Q}$), then the approximation factor can be reduced to 2, which is optimal. In particular this demonstrates an advantage of improper learning over proper in this setup.
  We develop two approaches to achieve the optimal approximation factor of 2: an adaptive one and a static one. Both approaches are based on a geometric point of view of the problem and rely on estimating surrogate metrics to the total variation. Our sample complexity bounds exploit techniques from {\it Adaptive Data Analysis}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Attacking All Tasks at Once Using Adversarial Examples in Multi-Task Learning</title>
<link>https://arxiv.org/abs/2305.12066</link>
<guid>https://arxiv.org/abs/2305.12066</guid>
<content:encoded><![CDATA[

arXiv:2305.12066v4 Announce Type: replace 
Abstract: Visual content understanding frequently relies on multi-task models to extract robust representations of a single visual input for multiple downstream tasks. However, in comparison to extensively studied single-task models, the adversarial robustness of multi-task models has received significantly less attention and many questions remain unclear: 1) How robust are multi-task models to single task adversarial attacks, 2) Can adversarial attacks be designed to simultaneously attack all tasks in a multi-task model, and 3) How does parameter sharing across tasks affect multi-task model robustness to adversarial attacks? This paper aims to answer these questions through careful analysis and rigorous experimentation. First, we analyze the inherent drawbacks of two commonly-used adaptations of single-task white-box attacks in attacking multi-task models. We then propose a novel attack framework, Dynamic Gradient Balancing Attack (DGBA). Our framework poses the problem of attacking all tasks in a multi-task model as an optimization problem that can be efficiently solved through integer linear programming. Extensive evaluation on two popular MTL benchmarks, NYUv2 and Tiny-Taxonomy, demonstrates the effectiveness of DGBA compared to baselines in attacking both clean and adversarially trained multi-task models. Our results also reveal a fundamental trade-off between improving task accuracy via parameter sharing across tasks and undermining model robustness due to increased attack transferability from parameter sharing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unified Perspective for Loss-Oriented Imbalanced Learning via Localization</title>
<link>https://arxiv.org/abs/2310.04752</link>
<guid>https://arxiv.org/abs/2310.04752</guid>
<content:encoded><![CDATA[

arXiv:2310.04752v2 Announce Type: replace 
Abstract: Due to the inherent imbalance in real-world datasets, na\"ive Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden Minima in Two-Layer ReLU Networks</title>
<link>https://arxiv.org/abs/2312.16819</link>
<guid>https://arxiv.org/abs/2312.16819</guid>
<content:encoded><![CDATA[

arXiv:2312.16819v3 Announce Type: replace 
Abstract: We consider the optimization problem arising from fitting two-layer ReLU networks with $d$ inputs under the square loss, where labels are generated by a target network. Two infinite families of spurious minima have recently been identified: one whose loss vanishes as $d \to \infty$, and another whose loss remains bounded away from zero. The latter are nevertheless avoided by vanilla SGD, and thus hidden, motivating the search for analytic properties distinguishing the two types. Perhaps surprisingly, the Hessian spectra of hidden and non-hidden minima agree up to terms of order $O(d^{-1/2})$, providing limited explanatory power. Consequently, our analysis of hidden minima proceeds instead via curves along which the loss is minimized or maximized. The main result is that arcs emanating from hidden minima differ, characteristically, by their structure and symmetry, precisely on account of the $O(d^{-1/2})$-eigenvalue terms absent from previous analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ensemble Learning of Machine Learning Force Fields</title>
<link>https://arxiv.org/abs/2403.17507</link>
<guid>https://arxiv.org/abs/2403.17507</guid>
<content:encoded><![CDATA[

arXiv:2403.17507v2 Announce Type: replace 
Abstract: Machine learning force fields (MLFFs) are a promising approach to balance the accuracy of quantum mechanics with the efficiency of classical potentials, yet selecting an optimal model amid increasingly diverse architectures that delivers reliable force predictions and stable simulations remains a core pratical challenge. Here we introduce EL-MLFFs, an ensemble learning framework that uses a stacking methodology to integrate predictions from diverse base MLFFs. Our approach constructs a graph representation where a graph neural network (GNN) acts as a meta-model to refine the initial force predictions. We present two meta-model architectures: a computationally efficient direct fitting model and a physically-principled conservative model that ensures energy conservation. The framework is evaluated on a diverse range of systems, including single molecules (methane), surface chemistry (methanol/Cu(100)), molecular dynamics benchmarks (MD17), and the MatPES materials dataset. Results show that EL-MLFFs improves predictive accuracy across these domains. For molecular systems, it reduces force errors and improves the simulation stability compared to base models. For materials, the method yields lower formation energy errors on the WBM test set. The EL- MLFFs framework offers a systematic approach to address challenges of model selection and the accuracy-stability trade-off in molecular and materials simulations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SDT-GNN: Streaming-based Distributed Training Framework for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2404.02300</link>
<guid>https://arxiv.org/abs/2404.02300</guid>
<content:encoded><![CDATA[

arXiv:2404.02300v2 Announce Type: replace 
Abstract: Recently, distributed GNN training frameworks, such as DistDGL and PyG, have been developed to enable training GNN models on large graphs by leveraging multiple GPUs in a distributed manner. Despite these advances, their memory requirements are still excessively high, thereby hindering GNN training on large graphs using commodity workstations. In this paper, we propose SDT-GNN, a streaming-based distributed GNN training framework. Unlike the existing frameworks that load the entire graph in memory, it takes a stream of edges as input for graph partitioning to reduce the memory requirement for partitioning. It also enables distributed GNN training even when the aggregated memory size of GPUs is smaller than the size of the graph and feature data. Furthermore, to improve the quality of partitioning, we propose SPRING, a novel streaming partitioning algorithm for distributed GNN training. We demonstrate the effectiveness and efficiency of SDT-GNN on seven large public datasets. SDT-GNN has up to 95% less memory footprint than DistDGL and PyG without sacrificing the prediction accuracy. SPRING also outperforms state-of-the-art streaming partitioning algorithms significantly.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covariate-Elaborated Robust Partial Information Transfer with Conditional Spike-and-Slab Prior</title>
<link>https://arxiv.org/abs/2404.03764</link>
<guid>https://arxiv.org/abs/2404.03764</guid>
<content:encoded><![CDATA[

arXiv:2404.03764v3 Announce Type: replace 
Abstract: The popularity of transfer learning stems from the fact that it can borrow information from useful auxiliary datasets. Existing statistical transfer learning methods usually adopt a global similarity measure between the source data and the target data, which may lead to inefficiency when only partial information is shared. In this paper, we propose a novel Bayesian transfer learning method named ``CONCERT'' to allow robust partial information transfer for high-dimensional data analysis. A conditional spike-and-slab prior is introduced in the joint distribution of target and source parameters for information transfer. By incorporating covariate-specific priors, we can characterize partial similarities and integrate source information collaboratively to improve the performance on the target. In contrast to existing work, the CONCERT is a one-step procedure which achieves variable selection and information transfer simultaneously. We establish variable selection consistency, as well as estimation and prediction error bounds for CONCERT. Our theory demonstrates the covariate-specific benefit of transfer learning. To ensure the scalability of the algorithm, we adopt the variational Bayes framework to facilitate implementation. Extensive experiments and two real data applications showcase the validity and advantages of CONCERT over existing cutting-edge transfer learning methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Diffusion Models for Time Series and Spatio-Temporal Data</title>
<link>https://arxiv.org/abs/2404.18886</link>
<guid>https://arxiv.org/abs/2404.18886</guid>
<content:encoded><![CDATA[

arXiv:2404.18886v5 Announce Type: replace 
Abstract: Diffusion models have been widely used in time series and spatio-temporal data, enhancing generative, inferential, and downstream capabilities. These models are applied across diverse fields such as healthcare, recommendation, climate, energy, audio, and traffic. By separating applications for time series and spatio-temporal data, we offer a structured perspective on model category, task type, data modality, and practical application domain. This study aims to provide a solid foundation for researchers and practitioners, inspiring future innovations that tackle traditional challenges and foster novel solutions in diffusion model-based data mining tasks and applications. For more detailed information, we have open-sourced a repository at https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast training and sampling of Restricted Boltzmann Machines</title>
<link>https://arxiv.org/abs/2405.15376</link>
<guid>https://arxiv.org/abs/2405.15376</guid>
<content:encoded><![CDATA[

arXiv:2405.15376v3 Announce Type: replace 
Abstract: Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting insights from data, but their training is hindered by the slow mixing of Markov Chain Monte Carlo (MCMC) processes, especially with highly structured datasets. In this study, we build on recent theoretical advances in RBM training and focus on the stepwise encoding of data patterns into singular vectors of the coupling matrix, significantly reducing the cost of generating new samples and evaluating the quality of the model, as well as the training cost in highly clustered datasets. The learning process is analogous to the thermodynamic continuous phase transitions observed in ferromagnetic models, where new modes in the probability measure emerge in a continuous manner. We leverage the continuous transitions in the training process to define a smooth annealing trajectory that enables reliable and computationally efficient log-likelihood estimates. This approach enables online assessment during training and introduces a novel sampling strategy called Parallel Trajectory Tempering (PTT) that outperforms previously optimized MCMC methods. To mitigate the critical slowdown effect in the early stages of training, we propose a pre-training phase. In this phase, the principal components are encoded into a low-rank RBM through a convex optimization process, facilitating efficient static Monte Carlo sampling and accurate computation of the partition function. Our results demonstrate that this pre-training strategy allows RBMs to efficiently handle highly structured datasets where conventional methods fail. Additionally, our log-likelihood estimation outperforms computationally intensive approaches in controlled scenarios, while the PTT algorithm significantly accelerates MCMC processes compared to conventional methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimeAutoDiff: A Unified Framework for Generation, Imputation, Forecasting, and Time-Varying Metadata Conditioning of Heterogeneous Time Series Tabular Data</title>
<link>https://arxiv.org/abs/2406.16028</link>
<guid>https://arxiv.org/abs/2406.16028</guid>
<content:encoded><![CDATA[

arXiv:2406.16028v3 Announce Type: replace 
Abstract: We present TimeAutoDiff, a unified latent-diffusion framework for four fundamental time-series tasks: unconditional generation, missing-data imputation, forecasting, and time-varying-metadata conditional generation. The model natively supports heterogeneous features including continuous, binary, and categorical variables. We unify all tasks using a masked-modeling strategy in which a binary mask specifies which time-series cells are observed and which must be generated. TimeAutoDiff combines a lightweight variational autoencoder, which maps mixed-type features into a continuous latent sequence, with a diffusion model that learns temporal dynamics in this latent space. Two architectural choices provide strong speed and scalability benefits. The diffusion model samples an entire latent trajectory at once rather than denoising one timestep at a time, greatly reducing reverse-diffusion calls. In addition, the VAE compresses along the feature axis, enabling efficient modeling of wide tables in a low-dimensional latent space. Empirical evaluation shows that TimeAutoDiff matches or surpasses strong baselines in synthetic sequence fidelity and consistently improves imputation and forecasting performance. Metadata conditioning enables realistic scenario exploration, allowing users to edit metadata sequences and produce coherent counterfactual trajectories that preserve cross-feature dependencies. Ablation studies highlight the importance of the VAE's feature encoding and key components of the denoiser. A distance-to-closest-record audit further indicates that the model generalizes without excessive memorization. Code is available at https://github.com/namjoonsuh/TimeAutoDiff
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Model Performance Under Worst-case Subpopulations</title>
<link>https://arxiv.org/abs/2407.01316</link>
<guid>https://arxiv.org/abs/2407.01316</guid>
<content:encoded><![CDATA[

arXiv:2407.01316v2 Announce Type: replace 
Abstract: The performance of ML models degrades when the training population is different from that seen under operation. Towards assessing distributional robustness, we study the worst-case performance of a model over all subpopulations of a given size, defined with respect to core attributes Z. This notion of robustness can consider arbitrary (continuous) attributes Z, and automatically accounts for complex intersectionality in disadvantaged groups. We develop a scalable yet principled two-stage estimation procedure that can evaluate the robustness of state-of-the-art models. We prove that our procedure enjoys several finite-sample convergence guarantees, including dimension-free convergence. Instead of overly conservative notions based on Rademacher complexities, our evaluation error depends on the dimension of Z only through the out-of-sample error in estimating the performance conditional on Z. On real datasets, we demonstrate that our method certifies the robustness of a model and prevents deployment of unreliable models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mastering AI: Big Data, Deep Learning, and the Evolution of Large Language Models -- AutoML from Basics to State-of-the-Art Techniques</title>
<link>https://arxiv.org/abs/2410.09596</link>
<guid>https://arxiv.org/abs/2410.09596</guid>
<content:encoded><![CDATA[

arXiv:2410.09596v3 Announce Type: replace 
Abstract: A comprehensive guide to Automated Machine Learning (AutoML) is presented, covering fundamental principles, practical implementations, and future trends. The paper is structured to assist both beginners and experienced practitioners, with detailed discussions on popular AutoML tools such as TPOT, AutoGluon, and Auto-Keras. Emerging topics like Neural Architecture Search (NAS) and AutoML's applications in deep learning are also addressed. It is anticipated that this work will contribute to ongoing research and development in the field of AI and machine learning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tidal Current Speed Forecasting Model based on Multi-Periodicity Learning</title>
<link>https://arxiv.org/abs/2410.09718</link>
<guid>https://arxiv.org/abs/2410.09718</guid>
<content:encoded><![CDATA[

arXiv:2410.09718v3 Announce Type: replace 
Abstract: Tidal energy is one of the key components in increasing the penetration of renewable energy. High tidal energy penetration into the electrical grid depends on accurate tidal current speed forecasting. Model inaccuracies hinder forecast accuracy. Previous research primarily used physical models to forecast tidal current speed, yet tidal current variations influenced by the orbital periods of celestial bodies make accurate physical modeling challenging. Research on the multi-periodicity of tides is crucial for forecasting tidal current speed. We propose the Wavelet-Enhanced Convolutional Network to learn multi-periodicity. The framework embeds intra-period and inter-period variations of one-dimensional tidal current data into the rows and columns, respectively, of a two-dimensional tensor. Then, the two-dimensional variations of the sequence can be processed by convolutional kernels. We integrate a time-frequency analysis method into the framework to further address local periodic features. Additionally, to enhance the framework's stability, we optimize the framework's hyperparameters with the Tree-structured Parzen Estimator. The proposed framework captures multi-periodic dependencies in tidal current data. Numerical results show a 10-step average Mean Absolute Error of 0.025, with at least a 1.18% error reduction compared to other baselines. Further ablation studies show a 1.4% reduction in Mean Absolute Percentage Error on the data with artificially added periodic fluctuations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FIT-GNN: Faster Inference Time for GNNs that 'FIT' in Memory Using Coarsening</title>
<link>https://arxiv.org/abs/2410.15001</link>
<guid>https://arxiv.org/abs/2410.15001</guid>
<content:encoded><![CDATA[

arXiv:2410.15001v4 Announce Type: replace 
Abstract: Scalability of Graph Neural Networks (GNNs) remains a significant challenge. To tackle this, methods like coarsening, condensation, and computation trees are used to train on a smaller graph, resulting in faster computation. Nonetheless, prior research has not adequately addressed the computational costs during the inference phase. This paper presents a novel approach to improve the scalability of GNNs by reducing computational burden during the inference phase using graph coarsening. We demonstrate two different methods -- Extra Nodes and Cluster Nodes. Our study extends the application of graph coarsening for graph-level tasks, including graph classification and graph regression. We conduct extensive experiments on multiple benchmark datasets to evaluate the performance of our approach. Our results show that the proposed method achieves orders of magnitude improvements in single-node inference time compared to traditional approaches. Furthermore, it significantly reduces memory consumption for node and graph classification and regression tasks, enabling efficient training and inference on low-resource devices where conventional methods are impractical. Notably, these computational advantages are achieved while maintaining competitive performance relative to baseline models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeqProFT: Sequence-only Protein Property Prediction with LoRA Finetuning</title>
<link>https://arxiv.org/abs/2411.11530</link>
<guid>https://arxiv.org/abs/2411.11530</guid>
<content:encoded><![CDATA[

arXiv:2411.11530v2 Announce Type: replace 
Abstract: Protein language models (PLMs) have demonstrated remarkable capabilities in learning relationships between protein sequences and functions. However, finetuning these large models requires substantial computational resources, often with suboptimal task-specific results. This study investigates how parameter-efficient finetuning via LoRA can enhance protein property prediction while significantly reducing computational demands. By applying LoRA to ESM-2 and ESM-C models of varying sizes and evaluating 10 diverse protein property prediction tasks, we demonstrate that smaller models with LoRA adaptation can match or exceed the performance of larger models without adaptation. Additionally, we integrate contact map information through a multi-head attention mechanism, improving model comprehension of structural features. Our systematic analysis reveals that LoRA finetuning enables faster convergence, better performance, and more efficient resource utilization, providing practical guidance for protein research applications in resource-constrained environments. The code is available at https://github.com/jiankliu/SeqProFT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inversely Learning Transferable Rewards via Abstracted States</title>
<link>https://arxiv.org/abs/2501.01669</link>
<guid>https://arxiv.org/abs/2501.01669</guid>
<content:encoded><![CDATA[

arXiv:2501.01669v3 Announce Type: replace 
Abstract: Inverse reinforcement learning (IRL) has progressed significantly toward accurately learning the underlying rewards in both discrete and continuous domains from behavior data. The next advance is to learn {\em intrinsic} preferences in ways that produce useful behavior in settings or tasks which are different but aligned with the observed ones. In the context of robotic applications, this helps integrate robots into processing lines involving new tasks (with shared intrinsic preferences) without programming from scratch. We introduce a method to inversely learn an abstract reward function from behavior trajectories in two or more differing instances of a domain. The abstract reward function is then used to learn task behavior in another separate instance of the domain. This step offers evidence of its transferability and validates its correctness. We evaluate the method on trajectories in tasks from multiple domains in OpenAI's Gym testbed and AssistiveGym and show that the learned abstract reward functions can successfully learn task behaviors in instances of the respective domains, which have not been seen previously.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act</title>
<link>https://arxiv.org/abs/2501.12962</link>
<guid>https://arxiv.org/abs/2501.12962</guid>
<content:encoded><![CDATA[

arXiv:2501.12962v4 Announce Type: replace 
Abstract: What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for high-risk systems, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First, a necessary high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second, an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.) Most non-discrimination regulations target only high-risk AI systems. (2.) The regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are partly inconsistent and raise questions of computational feasibility. (3.) Finally, we consider the possible (future) interaction of classical EU non-discrimination law and the AI Act regulations. We recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online-BLS: An Accurate and Efficient Online Broad Learning System for Data Stream Classification</title>
<link>https://arxiv.org/abs/2501.16932</link>
<guid>https://arxiv.org/abs/2501.16932</guid>
<content:encoded><![CDATA[

arXiv:2501.16932v2 Announce Type: replace 
Abstract: The state-of-the-art online learning models generally conduct a single online gradient descent when a new sample arrives and thus suffer from suboptimal model weights. To this end, we introduce an online broad learning system framework with closed-form solutions for each online update. Different from employing existing incremental broad learning algorithms for online learning tasks, which tend to incur degraded accuracy and expensive online update overhead, we design an effective weight estimation algorithm and an efficient online updating strategy to remedy the above two deficiencies, respectively. Specifically, an effective weight estimation algorithm is first developed by replacing notorious matrix inverse operations with Cholesky decomposition and forward-backward substitution to improve model accuracy. Second, we devise an efficient online updating strategy that dramatically reduces online update time. Theoretical analysis exhibits the splendid error bound and low time complexity of our model. The most popular test-then-training evaluation experiments on various real-world datasets prove its superiority and efficiency. Furthermore, our framework is naturally extended to data stream scenarios with concept drift and exceeds state-of-the-art baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flatten Graphs as Sequences: Transformers are Scalable Graph Generators</title>
<link>https://arxiv.org/abs/2502.02216</link>
<guid>https://arxiv.org/abs/2502.02216</guid>
<content:encoded><![CDATA[

arXiv:2502.02216v3 Announce Type: replace 
Abstract: We introduce AutoGraph, a scalable autoregressive model for attributed graph generation using decoder-only transformers. By flattening graphs into random sequences of tokens through a reversible process, AutoGraph enables modeling graphs as sequences without relying on additional node features that are expensive to compute, in contrast to diffusion-based approaches. This results in sampling complexity and sequence lengths that scale optimally linearly with the number of edges, making it scalable and efficient for large, sparse graphs. A key success factor of AutoGraph is that its sequence prefixes represent induced subgraphs, creating a direct link to sub-sentences in language modeling. Empirically, AutoGraph achieves state-of-the-art performance on synthetic and molecular benchmarks, with up to 100x faster generation and 3x faster training than leading diffusion models. It also supports substructure-conditioned generation without fine-tuning and shows promising transferability, bridging language modeling and graph generation to lay the groundwork for graph foundation models. Our code is available at https://github.com/BorgwardtLab/AutoGraph.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EasySpec: Layer-Parallel Speculative Decoding for Efficient Multi-GPU Utilization</title>
<link>https://arxiv.org/abs/2502.02493</link>
<guid>https://arxiv.org/abs/2502.02493</guid>
<content:encoded><![CDATA[

arXiv:2502.02493v2 Announce Type: replace 
Abstract: Speculative decoding is an effective and lossless method for Large Language Model (LLM) inference acceleration. It employs a smaller model to generate a draft token sequence, which is then verified by the original base model. In multi-GPU systems, inference latency can be further reduced through tensor parallelism (TP), while the optimal TP size of the draft model is typically smaller than that of the base model, leading to GPU idling during the drafting stage. We observe that such inefficiency stems from the sequential execution of layers, which is seemingly natural but actually unnecessary. Therefore, we propose EasySpec, a layer-parallel speculation strategy that optimizes the efficiency of multi-GPU utilization. EasySpec breaks the inter-layer data dependencies in the draft model, enabling multiple layers to run simultaneously across multiple devices as 'fuzzy' speculation. After each drafting-and-verification iteration, the draft model's key-value cache is calibrated in a single forward pass, preventing long-term fuzzy-error accumulation at minimal additional latency. EasySpec is a training-free and plug-in method. We evaluated EasySpec on several mainstream open-source LLMs, using smaller versions of models from the same series as drafters. The results demonstrate that EasySpec can achieve a peak speedup of 4.17x compared to vanilla decoding, while preserving the original distributions of the base LLMs. Specifically, the drafting stage can be accelerated by up to 1.62x with a maximum speculation accuracy drop of only 7%. The code is available at https://github.com/Yize-Wu/EasySpec.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stein Discrepancy for Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2502.03587</link>
<guid>https://arxiv.org/abs/2502.03587</guid>
<content:encoded><![CDATA[

arXiv:2502.03587v4 Announce Type: replace 
Abstract: Unsupervised domain adaptation (UDA) aims to improve model performance on an unlabeled target domain using a related, labeled source domain. A common approach aligns source and target feature distributions by minimizing a distance between them, often using symmetric measures such as maximum mean discrepancy (MMD). However, these methods struggle when target data is scarce. We propose a novel UDA framework that leverages Stein discrepancy, an asymmetric measure that depends on the target distribution only through its score function, making it particularly suitable for low-data target regimes. Our proposed method has kernelized and adversarial forms and supports flexible modeling of the target distribution via Gaussian, GMM, or VAE models. We derive a generalization bound on the target error and a convergence rate for the empirical Stein discrepancy in the two-sample setting. Empirically, our method consistently outperforms prior UDA approaches under limited target data across multiple benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Countering Overfitting with Counterfactual Examples</title>
<link>https://arxiv.org/abs/2502.09193</link>
<guid>https://arxiv.org/abs/2502.09193</guid>
<content:encoded><![CDATA[

arXiv:2502.09193v2 Announce Type: replace 
Abstract: Overfitting is a well-known issue in machine learning that occurs when a model struggles to generalize its predictions to new, unseen data beyond the scope of its training set. Traditional techniques to mitigate overfitting include early stopping, data augmentation, and regularization. In this work, we demonstrate that the degree of overfitting of a trained model is correlated with the ability to generate counterfactual examples. The higher the overfitting, the easier it will be to find a valid counterfactual example for a randomly chosen input data point. Therefore, we introduce CF-Reg, a novel regularization term in the training loss that controls overfitting by ensuring enough margin between each instance and its corresponding counterfactual. Experiments conducted across multiple datasets and models show that our counterfactual regularizer generally outperforms existing regularization techniques.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExLLM: Experience-Enhanced LLM Optimization for Molecular Design and Beyond</title>
<link>https://arxiv.org/abs/2502.12845</link>
<guid>https://arxiv.org/abs/2502.12845</guid>
<content:encoded><![CDATA[

arXiv:2502.12845v5 Announce Type: replace 
Abstract: Molecular design involves an enormous and irregular search space, where traditional optimizers such as Bayesian optimization, genetic algorithms, and generative models struggle to leverage expert knowledge or handle complex feedback. Recently, LLMs have been used as optimizers, achieving promising results on benchmarks such as PMO. However, existing approaches rely only on prompting or extra training, without mechanisms to handle complex feedback or maintain scalable memory. In particular, the common practice of appending or summarizing experiences at every query leads to redundancy, degraded exploration, and ultimately poor final outcomes under large-scale iterative search. We introduce ExLLM (Experience-Enhanced LLM optimization), an LLM-as-optimizer framework with three components: (1) a compact, evolving experience snippet tailored to large discrete spaces that distills non-redundant cues and improves convergence at low cost; (2) a simple yet effective k-offspring scheme that widens exploration per call and reduces orchestration cost; and (3) a lightweight feedback adapter that normalizes objectives for selection while formatting constraints and expert hints for iteration. ExLLM sets new state-of-the-art results on PMO and generalizes strongly in our setup, it sets records on circle packing and stellarator design, and yields consistent gains across additional domains requiring only a task-description template and evaluation functions to transfer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DFDT: Dynamic Fast Decision Tree for IoT Data Stream Mining on Edge Devices</title>
<link>https://arxiv.org/abs/2502.14011</link>
<guid>https://arxiv.org/abs/2502.14011</guid>
<content:encoded><![CDATA[

arXiv:2502.14011v2 Announce Type: replace 
Abstract: The Internet of Things generates massive data streams, with edge computing emerging as a key enabler for online IoT applications and 5G networks. Edge solutions facilitate real-time machine learning inference, but also require continuous adaptation to concept drifts. While extensions of the Very Fast Decision Tree (VFDT) remain state-of-the-art for tabular stream mining, their unregulated growth limits efficiency, particularly in ensemble settings where post-pruning at the individual tree level is seldom applied. This paper presents DFDT, a novel memory-constrained algorithm for online learning. DFDT employs activity-aware pre-pruning, dynamically adjusting splitting criteria based on leaf node activity: low-activity nodes are deactivated to conserve resources, moderately active nodes split under stricter conditions, and highly active nodes leverage a skipping mechanism for accelerated growth. Additionally, adaptive grace periods and tie thresholds allow DFDT to modulate splitting decisions based on observed data variability, enhancing the accuracy-memory-runtime trade-off while minimizing the need for hyperparameter tuning. An ablation study reveals three DFDT variants suited to different resource profiles. Fully compatible with existing ensemble frameworks, DFDT provides a drop-in alternative to standard VFDT-based learners.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improvement Towards Pareto Optimality: Mitigating Preference Conflicts in Multi-Objective Alignment</title>
<link>https://arxiv.org/abs/2502.14354</link>
<guid>https://arxiv.org/abs/2502.14354</guid>
<content:encoded><![CDATA[

arXiv:2502.14354v3 Announce Type: replace 
Abstract: Multi-Objective Alignment (MOA) aims to align LLMs' responses with multiple human preference objectives, with Direct Preference Optimization (DPO) emerging as a prominent approach. However, we find that DPO-based MOA approaches suffer from widespread preference conflicts in the data, where different objectives favor different responses. This results in conflicting optimization directions, hindering the optimization on the Pareto Front. To address this, we propose to construct Pareto-optimal responses to resolve preference conflicts. To efficiently obtain and utilize such responses, we propose a self-improving DPO framework that enables LLMs to self-generate and select Pareto-optimal responses for self-supervised preference alignment. Extensive experiments on two datasets demonstrate the superior Pareto Front achieved by our framework compared to various baselines. Code is available at https://github.com/zyttt-coder/SIPO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Global-Decision-Focused Neural ODEs for Proactive Grid Resilience Management</title>
<link>https://arxiv.org/abs/2502.18321</link>
<guid>https://arxiv.org/abs/2502.18321</guid>
<content:encoded><![CDATA[

arXiv:2502.18321v3 Announce Type: replace 
Abstract: Extreme hazard events such as wildfires and hurricanes increasingly threaten power systems, causing widespread outages and disrupting critical services. Recently, predict-then-optimize approaches have gained traction in grid operations, where system functionality forecasts are first generated and then used as inputs for downstream decision-making. However, this two-stage method often results in a misalignment between prediction and optimization objectives, leading to suboptimal resource allocation. To address this, we propose predict-all-then-optimize-globally (PATOG), a framework that integrates outage prediction with globally optimized interventions. At its core, our global-decision-focused (GDF) neural ODE model captures outage dynamics while optimizing resilience strategies in a decision-aware manner. Unlike conventional methods, our approach ensures spatially and temporally coherent decision-making, improving both predictive accuracy and operational efficiency. Experiments on synthetic and real-world datasets demonstrate significant improvements in outage prediction consistency and grid resilience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIMRL: Physics-Informed Multi-Scale Recurrent Learning for Spatiotemporal Prediction</title>
<link>https://arxiv.org/abs/2503.10253</link>
<guid>https://arxiv.org/abs/2503.10253</guid>
<content:encoded><![CDATA[

arXiv:2503.10253v3 Announce Type: replace 
Abstract: Simulation of spatiotemporal systems governed by partial differential equations is widely applied in fields such as biology, chemistry, aerospace dynamics, and meteorology. Traditional numerical methods incur high computational costs due to the requirement of small time steps for accurate predictions. While machine learning has reduced these costs, long-term predictions remain challenged by error accumulation, particularly in scenarios with insufficient data or varying time scales, where stability and accuracy are compromised. Existing methods often neglect the effective utilization of multi-scale data, leading to suboptimal robustness in predictions. To address these issues, we propose a novel multi-scale learning framework, namely, the Physics-Informed Multi-Scale Recurrent Learning (PIMRL), to effectively leverage multi-scale data for spatiotemporal dynamics prediction. The PIMRL framework comprises two modules: the micro-scale module embeds physical knowledge into neural networks via pretraining, and the macro-scale module adopts a data-driven approach to learn the temporal evolution of physics in the latent space. Experimental results demonstrate that the PIMRL framework consistently achieves state-of-the-art performance across five benchmark datasets ranging from one to three dimensions, showing average improvements of over 9\% in both RMSE and MAE evaluation metrics, with maximum enhancements reaching up to 80%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantization-Free Autoregressive Action Transformer</title>
<link>https://arxiv.org/abs/2503.14259</link>
<guid>https://arxiv.org/abs/2503.14259</guid>
<content:encoded><![CDATA[

arXiv:2503.14259v3 Announce Type: replace 
Abstract: Current transformer-based imitation learning approaches introduce discrete action representations and train an autoregressive transformer decoder on the resulting latent code. However, the initial quantization breaks the continuous structure of the action space thereby limiting the capabilities of the generative model. We propose a quantization-free method instead that leverages Generative Infinite-Vocabulary Transformers (GIVT) as a direct, continuous policy parametrization for autoregressive transformers. This simplifies the imitation learning pipeline while achieving state-of-the-art performance on a variety of popular simulated robotics tasks. We enhance our policy roll-outs by carefully studying sampling algorithms, further improving the results.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Physics-Informed Meta-Learning Framework for the Continuous Solution of Parametric PDEs on Arbitrary Geometries</title>
<link>https://arxiv.org/abs/2504.02459</link>
<guid>https://arxiv.org/abs/2504.02459</guid>
<content:encoded><![CDATA[

arXiv:2504.02459v2 Announce Type: replace 
Abstract: In this work, we introduce implicit Finite Operator Learning (iFOL) for the continuous and parametric solution of partial differential equations (PDEs) on arbitrary geometries. We propose a physics-informed encoder-decoder network to establish the mapping between continuous parameter and solution spaces. The decoder constructs the parametric solution field by leveraging an implicit neural field network conditioned on a latent or feature code. Instance-specific codes are derived through a PDE encoding process based on the second-order meta-learning technique. In training and inference, a physics-informed loss function is minimized during the PDE encoding and decoding. iFOL expresses the loss function in an energy or weighted residual form and evaluates it using discrete residuals derived from standard numerical PDE methods. This approach results in the backpropagation of discrete residuals during both training and inference.
  iFOL features several key properties: (1) its unique loss formulation eliminates the need for the conventional encode-process-decode pipeline previously used in operator learning with conditional neural fields for PDEs; (2) it not only provides accurate parametric and continuous fields but also delivers solution-to-parameter gradients without requiring additional loss terms or sensitivity analysis; (3) it can effectively capture sharp discontinuities in the solution; and (4) it removes constraints on the geometry and mesh, making it applicable to arbitrary geometries and spatial sampling (zero-shot super-resolution capability). We critically assess these features and analyze the network's ability to generalize to unseen samples across both stationary and transient PDEs. The overall performance of the proposed method is promising, demonstrating its applicability to a range of challenging problems in computational mechanics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Process Reward Models That Think</title>
<link>https://arxiv.org/abs/2504.16828</link>
<guid>https://arxiv.org/abs/2504.16828</guid>
<content:encoded><![CDATA[

arXiv:2504.16828v5 Announce Type: replace 
Abstract: Step-by-step verifiers -- also known as process reward models (PRMs) -- are a key ingredient for test-time scaling. PRMs require step-level supervision, making them expensive to train. This work aims to build data-efficient PRMs as verbalized step-wise reward models that verify every step in the solution by generating a verification chain-of-thought (CoT). We propose ThinkPRM, a long CoT verifier fine-tuned on orders of magnitude fewer process labels than those required by discriminative PRMs. Our approach capitalizes on the inherent reasoning abilities of long CoT models, and outperforms LLM-as-a-Judge and discriminative verifiers -- using only 1% of the process labels in PRM800K -- across several challenging benchmarks. Specifically, ThinkPRM beats the baselines on ProcessBench, MATH-500, and AIME '24 under best-of-N selection and reward-guided search. In an out-of-domain evaluation on a subset of GPQA-Diamond and LiveCodeBench, our PRM surpasses discriminative verifiers trained on the full PRM800K by 8% and 4.5%, respectively. Lastly, under the same token budget, ThinkPRM scales up verification compute more effectively compared to LLM-as-a-Judge, outperforming it by 7.2% on a subset of ProcessBench. Our work highlights the value of generative, long CoT PRMs that can scale test-time compute for verification while requiring minimal supervision for training. Our code, data, and models are released at https://github.com/mukhal/thinkprm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Repetition Makes Perfect: Recurrent Graph Neural Networks Match Message-Passing Limit</title>
<link>https://arxiv.org/abs/2505.00291</link>
<guid>https://arxiv.org/abs/2505.00291</guid>
<content:encoded><![CDATA[

arXiv:2505.00291v3 Announce Type: replace 
Abstract: We precisely characterize the expressivity of computable Recurrent Graph Neural Networks (recurrent GNNs). We prove that recurrent GNNs with finite-precision parameters, sum aggregation, and ReLU activation, can compute any graph algorithm that respects the natural message-passing invariance induced by the Color Refinement (or Weisfeiler-Leman) algorithm. While it is well known that the expressive power of GNNs is limited by this invariance [Morris et al., AAAI 2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually match this limit. This is in contrast to non-recurrent GNNs, which have the power of Weisfeiler-Leman only in a very weak, "non-uniform", sense where each graph size requires a different GNN to compute with. Our construction introduces only a polynomial overhead in both time and space.
  Furthermore, we show that by incorporating random initialization, for connected graphs recurrent GNNs can express all graph algorithms. In particular, any polynomial-time graph algorithm can be emulated on connected graphs in polynomial time by a recurrent GNN with random initialization.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Certifying Stability of Reinforcement Learning Policies using Generalized Lyapunov Functions</title>
<link>https://arxiv.org/abs/2505.10947</link>
<guid>https://arxiv.org/abs/2505.10947</guid>
<content:encoded><![CDATA[

arXiv:2505.10947v3 Announce Type: replace 
Abstract: Establishing stability certificates for closed-loop systems under reinforcement learning (RL) policies is essential to move beyond empirical performance and offer guarantees of system behavior. Classical Lyapunov methods require a strict stepwise decrease in the Lyapunov function but such certificates are difficult to construct for learned policies. The RL value function is a natural candidate but it is not well understood how it can be adapted for this purpose. To gain intuition, we first study the linear quadratic regulator (LQR) problem and make two key observations. First, a Lyapunov function can be obtained from the value function of an LQR policy by augmenting it with a residual term related to the system dynamics and stage cost. Second, the classical Lyapunov decrease requirement can be relaxed to a generalized Lyapunov condition requiring only decrease on average over multiple time steps. Using this intuition, we consider the nonlinear setting and formulate an approach to learn generalized Lyapunov functions by augmenting RL value functions with neural network residual terms. Our approach successfully certifies the stability of RL policies trained on Gymnasium and DeepMind Control benchmarks. We also extend our method to jointly train neural controllers and stability certificates using a multi-step Lyapunov loss, resulting in larger certified inner approximations of the region of attraction compared to the classical Lyapunov approach. Overall, our formulation enables stability certification for a broad class of systems with learned policies by making certificates easier to construct, thereby bridging classical control theory and modern learning-based methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Time-Series Foundation Models Deployment-Ready? A Systematic Study of Adversarial Robustness Across Domains</title>
<link>https://arxiv.org/abs/2505.19397</link>
<guid>https://arxiv.org/abs/2505.19397</guid>
<content:encoded><![CDATA[

arXiv:2505.19397v2 Announce Type: replace 
Abstract: Time-Series Foundation Models (TSFMs) are rapidly transitioning from research prototypes to core components of critical decision-making systems, driven by their impressive zero-shot forecasting capabilities. However, as their deployment surges, a critical blind spot remains: their fragility under adversarial attacks. This lack of scrutiny poses severe risks, particularly as TSFMs enter high-stakes environments vulnerable to manipulation. We present a systematic, diagnostic study arguing that for TSFMs, robustness is not merely a secondary metric but a prerequisite for trustworthy deployment comparable to accuracy. Our evaluation framework, explicitly tailored to the unique constraints of time series, incorporates normalized, sparsity-aware perturbation budgets and unified scale-invariant metrics across white-box and black-box settings. Across six representative TSFMs, we demonstrate that current architectures are alarmingly brittle: even small perturbations can reliably steer forecasts toward specific failure modes, such as trend flips and malicious drifts. We uncover TSFM-specific vulnerability patterns, including horizon-proximal brittleness, increased susceptibility with longer context windows, and weak cross-model transfer that points to model-specific failure modes rather than generic distortions. Finally, we show that simple adversarial fine-tuning offers a cost-effective path to substantial robustness gains, even with out-of-domain data. This work bridges the gap between TSFM capabilities and safety constraints, offering essential guidance for hardening the next generation of forecasting systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[

arXiv:2505.20561v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as rethinking and error correction, as a form of in-context exploration. However, the Markovian policy obtained from conventional RL training does not give rise to reflective exploration behaviors since the policy depends on the history only through the state and therefore has no incentive to enrich identical states with additional context. Instead, RL exploration is only useful during training to learn the optimal policy in a trial-and-error manner. Therefore, it remains unclear whether reflective reasoning will emerge during RL, or why it is beneficial. To remedy this, we recast reflective exploration within a Bayesian RL framework, which optimizes the expected return under a posterior distribution over Markov decision processes induced by the training data. This Bayesian formulation admits uncertainty-adaptive policies that, through belief updates, naturally incentivize information-gathering actions and induce self-reflection behaviors. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms conventional RL approaches, achieving superior test-time performance and token efficiency. Our code is available at https://github.com/shenao-zhang/BARL.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning where to learn: Training data distribution optimization for scientific machine learning</title>
<link>https://arxiv.org/abs/2505.21626</link>
<guid>https://arxiv.org/abs/2505.21626</guid>
<content:encoded><![CDATA[

arXiv:2505.21626v3 Announce Type: replace 
Abstract: In scientific machine learning, models are routinely deployed with parameter values or boundary conditions far from those used in training. This paper studies the learning-where-to-learn problem of designing a training data distribution that minimizes average prediction error across a family of deployment regimes. A theoretical analysis shows how the training distribution shapes deployment accuracy. This motivates two adaptive algorithms based on bilevel or alternating optimization in the space of probability measures. Discretized implementations using parametric distribution classes or nonparametric particle-based gradient flows deliver optimized training distributions that outperform nonadaptive designs. Once trained, the resulting models exhibit improved sample complexity and robustness to distribution shift. This framework unlocks the potential of principled data acquisition for learning functions and solution operators of partial differential equations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMs Judging LLMs: A Simplex Perspective</title>
<link>https://arxiv.org/abs/2505.21972</link>
<guid>https://arxiv.org/abs/2505.21972</guid>
<content:encoded><![CDATA[

arXiv:2505.21972v2 Announce Type: replace 
Abstract: Given the challenge of automatically evaluating free-form outputs from large language models (LLMs), an increasingly common solution is to use LLMs themselves as the judging mechanism, without any gold-standard scores. Implicitly, this practice accounts for only sampling variability (aleatoric uncertainty) and ignores uncertainty about judge quality (epistemic uncertainty). While this is justified if judges are perfectly accurate, it is unclear when such an approach is theoretically valid and practically robust. We study these questions for the task of ranking LLM candidates from a novel geometric perspective: for $M$-level scoring systems, both LLM judges and candidates can be represented as points on an $(M-1)$-dimensional probability simplex, where geometric concepts (e.g., triangle areas) correspond to key ranking concepts. This perspective yields intuitive theoretical conditions and visual proofs for when rankings are identifiable; for instance, we provide a formal basis for the ``folk wisdom'' that LLM judges are more effective for two-level scoring ($M=2$) than multi-level scoring ($M>2$). Leveraging the simplex, we design geometric Bayesian priors that encode epistemic uncertainty about judge quality and vary the priors to conduct sensitivity analyses. Experiments on LLM benchmarks show that rankings based solely on LLM judges are robust in many but not all datasets, underscoring both their widespread success and the need for caution. Our Bayesian method achieves substantially higher coverage rates than existing procedures, highlighting the importance of modeling epistemic uncertainty.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stepsize anything: A unified learning rate schedule for budgeted-iteration training</title>
<link>https://arxiv.org/abs/2505.24452</link>
<guid>https://arxiv.org/abs/2505.24452</guid>
<content:encoded><![CDATA[

arXiv:2505.24452v4 Announce Type: replace 
Abstract: The expanding computational costs and limited resources underscore the critical need for budgeted-iteration training, which aims to achieve optimal learning within predetermined iteration budgets. While learning rate schedules fundamentally govern the performance of different networks and tasks, particularly in budgeted-iteration scenarios, their design remains largely heuristic, lacking theoretical foundations. In addition, the optimal learning rate schedule requires extensive trial-and-error selection, making the training process inefficient. In this work, we propose the Unified Budget-Aware (UBA) schedule, a theoretically grounded learning rate schedule that consistently outperforms commonly-used schedules among diverse architectures and tasks under different constrained training budgets. First, we bridge the gap by constructing a novel training budget-aware optimization framework, which explicitly accounts for the robustness to landscape curvature variations. From this framework, we derive the UBA schedule, controlled by a single hyper-parameter \varphi that provides a trade-off between flexibility and simplicity, eliminating the need for per-network numerical optimization. Moreover, we establish a theoretical connection between \varphi and the condition number, adding interpretation and justification to our approach. Besides, we prove the convergence for different values of \varphi. We offer practical guidelines for its selection via theoretical analysis and empirical results. Extensive experimental results show that UBA consistently surpasses the commonly-used schedules across diverse vision and language tasks, spanning network architectures (e.g., ResNet, OLMo) and scales, under different training-iteration budgets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlearning Inversion Attacks for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2506.00808</link>
<guid>https://arxiv.org/abs/2506.00808</guid>
<content:encoded><![CDATA[

arXiv:2506.00808v2 Announce Type: replace 
Abstract: Graph unlearning methods aim to efficiently remove the impact of sensitive data from trained GNNs without full retraining, assuming that deleted information cannot be recovered. In this work, we challenge this assumption by introducing the graph unlearning inversion attack: given only black-box access to an unlearned GNN and partial graph knowledge, can an adversary reconstruct the removed edges? We identify two key challenges: varying probability-similarity thresholds for unlearned versus retained edges, and the difficulty of locating unlearned edge endpoints, and address them with TrendAttack. First, we derive and exploit the confidence pitfall, a theoretical and empirical pattern showing that nodes adjacent to unlearned edges exhibit a large drop in model confidence. Second, we design an adaptive prediction mechanism that applies different similarity thresholds to unlearned and other membership edges. Our framework flexibly integrates existing membership inference techniques and extends them with trend features. Experiments on four real-world datasets demonstrate that TrendAttack significantly outperforms state-of-the-art GNN membership inference baselines, exposing a critical privacy vulnerability in current graph unlearning methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safety-Aware Reinforcement Learning for Control via Risk-Sensitive Action-Value Iteration and Quantile Regression</title>
<link>https://arxiv.org/abs/2506.06954</link>
<guid>https://arxiv.org/abs/2506.06954</guid>
<content:encoded><![CDATA[

arXiv:2506.06954v2 Announce Type: replace 
Abstract: Mainstream approximate action-value iteration reinforcement learning (RL) algorithms suffer from overestimation bias, leading to suboptimal policies in high-variance stochastic environments. Quantile-based action-value iteration methods reduce this bias by learning a distribution of the expected cost-to-go using quantile regression. However, ensuring that the learned policy satisfies safety constraints remains a challenge when these constraints are not explicitly integrated into the RL framework. Existing methods often require complex neural architectures or manual tradeoffs due to combined cost functions. To address this, we propose a risk-regularized quantile-based algorithm integrating Conditional Value-at-Risk (CVaR) to enforce safety without complex architectures. We also provide theoretical guarantees on the contraction properties of the risk-sensitive distributional Bellman operator in Wasserstein space, ensuring convergence to a unique cost distribution. Simulations of a mobile robot in a dynamic reach-avoid task show that our approach leads to more goal successes, fewer collisions, and better safety-performance trade-offs than risk-neutral methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi-supervised Graph Anomaly Detection via Robust Homophily Learning</title>
<link>https://arxiv.org/abs/2506.15448</link>
<guid>https://arxiv.org/abs/2506.15448</guid>
<content:encoded><![CDATA[

arXiv:2506.15448v2 Announce Type: replace 
Abstract: Semi-supervised graph anomaly detection (GAD) utilizes a small set of labeled normal nodes to identify abnormal nodes from a large set of unlabeled nodes in a graph. Current methods in this line posit that 1) normal nodes share a similar level of homophily and 2) the labeled normal nodes can well represent the homophily patterns in the normal class. However, this assumption often does not hold well since normal nodes in a graph can exhibit diverse homophily in real-world GAD datasets. In this paper, we propose RHO, namely Robust Homophily Learning, to adaptively learn such homophily patterns. RHO consists of two novel modules, adaptive frequency response filters (AdaFreq) and graph normality alignment (GNA). AdaFreq learns a set of adaptive spectral filters that capture different frequency components of the labeled normal nodes with varying homophily in the channel-wise and cross-channel views of node attributes. GNA is introduced to enforce consistency between the channel-wise and cross-channel homophily representations to robustify the normality learned by the filters in the two views. Experiments on eight real-world GAD datasets show that RHO can effectively learn varying, often under-represented, homophily in the small normal node set and substantially outperforms state-of-the-art competing methods. Code is available at https://github.com/mala-lab/RHO.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Sample is Enough to Make Conformal Prediction Robust</title>
<link>https://arxiv.org/abs/2506.16553</link>
<guid>https://arxiv.org/abs/2506.16553</guid>
<content:encoded><![CDATA[

arXiv:2506.16553v2 Announce Type: replace 
Abstract: For any black-box model, conformal prediction (CP) returns prediction sets guaranteed to include the true label with high adjustable probability. Robust CP (RCP) extends the guarantee to the worst case noise up to a pre-defined magnitude. For RCP, a well-established approach is to use randomized smoothing since it is applicable to any black-box model and provides smaller sets compared to deterministic methods. However, smoothing-based robustness requires many model forward passes per each input which is computationally expensive. We show that conformal prediction attains some robustness even with a single forward pass on a randomly perturbed input. Using any binary certificate we propose a single sample robust CP (RCP1). Our approach returns robust sets with smaller average set size compared to SOTA methods which use many (e.g. 100) passes per input. Our key insight is to certify the conformal procedure itself rather than individual conformity scores. Our approach is agnostic to the task (classification and regression). We further extend our approach to smoothing-based robust conformal risk control.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Classical Hybrid Quantized Neural Network</title>
<link>https://arxiv.org/abs/2506.18240</link>
<guid>https://arxiv.org/abs/2506.18240</guid>
<content:encoded><![CDATA[

arXiv:2506.18240v4 Announce Type: replace 
Abstract: In this work, we introduce a novel Quadratic Binary Optimization (QBO) framework for training a quantized neural network. The framework enables the use of arbitrary activation and loss functions through spline interpolation, while Forward Interval Propagation addresses the nonlinearities and the multi-layered, composite structure of neural networks via discretizing activation functions into linear subintervals. This preserves the universal approximation properties of neural networks while allowing complex nonlinear functions accessible to quantum solvers, broadening their applicability in artificial intelligence. Theoretically, we derive an upper bound on the approximation error and the number of Ising spins required by deriving the sample complexity of the empirical risk minimization problem from an optimization perspective. A key challenge in solving the associated large-scale Quadratic Constrained Binary Optimization (QCBO) model is the presence of numerous constraints. To overcome this, we adopt the Quantum Conditional Gradient Descent (QCGD) algorithm, which solves QCBO directly on quantum hardware. We establish the convergence of QCGD under a quantum oracle subject to randomness, bounded variance, and limited coefficient precision, and further provide an upper bound on the Time-To-Solution. To enhance scalability, we further incorporate a decomposed copositive optimization scheme that replaces the monolithic lifted model with sample-wise subproblems. This decomposition substantially reduces the quantum resource requirements and enables efficient low-bit neural network training. We further propose the usage of QCGD and Quantum Progressive Hedging (QPH) algorithm to efficiently solve the decomposed problem.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Granger Causality in Neural Networks: Can Prediction Alone Reveal Structure?</title>
<link>https://arxiv.org/abs/2506.20347</link>
<guid>https://arxiv.org/abs/2506.20347</guid>
<content:encoded><![CDATA[

arXiv:2506.20347v2 Announce Type: replace 
Abstract: Granger Causality (GC) offers an elegant statistical framework to study the association between multivariate time series data. Vector autoregressive models (VAR) are simple and easy to fit, but have limited application because of their inherent inability to capture more complex (e.g., non-linear) associations. Numerous attempts have already been made in the literature that exploit the functional approximation power of deep neural networks (DNNs) for GC. However, these methods treat GC as a variable selection problem. We present a novel paradigm for investigating the learned GC from a single neural network used for joint modeling of all components of multivariate time series data, which is essentially linked with prediction and assessing the distribution shift in residuals. A deep learning model, with proper regularization, may learn the true GC structure when jointly used for all components of the time series when there is sufficient training data. We propose to uncover the learned GC structure by comparing the model uncertainty or distribution of the residuals when the past of everything is used as compared to the one where a specific time series component is dropped from the model. We also compare the effect of input layer dropout on the ability of a neural network to learn GC. We show that a well-regularized model can learn the true GC structure from the data without explicitly adding terms in the loss function that guide the model to select variables or perform sparse regression under specific settings. We also provide a comparison of deep learning architectures such as CNN, LSTM and transformer models on their ability to discover Granger Causality. The numerical experiments demonstrate that, compared to sparse regression models, a simple joint model is a strong baseline for learning the true GC which has the advantage that it does not require tuning of many extra hyper-parameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge</title>
<link>https://arxiv.org/abs/2507.09202</link>
<guid>https://arxiv.org/abs/2507.09202</guid>
<content:encoded><![CDATA[

arXiv:2507.09202v2 Announce Type: replace 
Abstract: Artificial intelligence (AI)-driven models have the potential to revolutionize weather forecasting, but still rely on initial conditions generated by costly Numerical Weather Prediction (NWP) systems. Although recent end-to-end forecasting models attempt to bypass NWP systems, these methods lack scalable assimilation of new types of observational data. Here, we introduce XiChen, an observation-scalable fully AI-driven global weather forecasting system, wherein the entire pipeline, from Data Assimilation (DA) to medium-range forecasting, can be accomplished within only 15 seconds. XiChen is built upon a foundation model that is pre-trained for weather forecasting and subsequently fine-tuned to serve as both observation operators and DA models, thereby enabling the scalable assimilation of conventional and raw satellite observations. Furthermore, the integration of Four-Dimensional Variational (4DVar) knowledge ensures XiChen to achieve DA and medium-range forecasting accuracy comparable to operational NWP systems, with skillful forecasting lead time beyond 8.75 days. A key feature of XiChen is its ability to maintain physical balance constraints during DA, enabling observed variables to correct unobserved ones effectively. In single-point perturbation DA experiments, XiChen exhibits flow-dependent characteristics similar to those of traditional 4DVar systems. These results demonstrate that XiChen holds strong potential for fully AI-driven weather forecasting independent of NWP systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Simple Approximate Bayesian Inference Neural Surrogate for Stochastic Petri Net Models</title>
<link>https://arxiv.org/abs/2507.10714</link>
<guid>https://arxiv.org/abs/2507.10714</guid>
<content:encoded><![CDATA[

arXiv:2507.10714v2 Announce Type: replace 
Abstract: Stochastic Petri Nets (SPNs) are an increasingly popular tool of choice for modeling discrete-event dynamics in areas such as epidemiology and systems biology, yet their parameter estimation remains challenging in general and in particular when transition rates depend on external covariates and explicit likelihoods are unavailable. We introduce a neural-surrogate (neural-network-based approximation of the posterior distribution) framework that predicts the coefficients of known covariate-dependent rate functions directly from noisy, partially observed token trajectories. Our model employs a lightweight 1D Convolutional Residual Network trained end-to-end on Gillespie-simulated SPN realizations, learning to invert system dynamics under realistic conditions of event dropout. During inference, Monte Carlo dropout provides calibrated uncertainty bounds together with point estimates. On synthetic SPNs with $10\%$ missing events, our surrogate recovers rate-function coefficients with an $RMSE = 0.043$ and substantially runs faster than traditional Bayesian approaches. These results demonstrate that data-driven, likelihood-free surrogates can enable accurate, robust, and real-time parameter recovery in complex, partially observed discrete-event systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Resilient Safety-driven Unlearning for Diffusion Models against Downstream Fine-tuning</title>
<link>https://arxiv.org/abs/2507.16302</link>
<guid>https://arxiv.org/abs/2507.16302</guid>
<content:encoded><![CDATA[

arXiv:2507.16302v2 Announce Type: replace 
Abstract: Text-to-image (T2I) diffusion models have achieved impressive image generation quality and are increasingly fine-tuned for personalized applications. However, these models often inherit unsafe behaviors from toxic pretraining data, raising growing safety concerns. While recent safety-driven unlearning methods have made promising progress in suppressing model toxicity, they are found to be fragile to downstream fine-tuning, as we reveal that state-of-the-art methods largely fail to retain their effectiveness even when fine-tuned on entirely benign datasets. To mitigate this problem, in this paper, we propose ResAlign, a safety-driven unlearning framework with enhanced resilience against downstream fine-tuning. By modeling downstream fine-tuning as an implicit optimization problem with a Moreau envelope-based reformulation, ResAlign enables efficient gradient estimation to minimize the recovery of harmful behaviors. Additionally, a meta-learning strategy is proposed to simulate a diverse distribution of fine-tuning scenarios to improve generalization. Extensive experiments across a wide range of datasets, fine-tuning methods, and configurations demonstrate that ResAlign consistently outperforms prior unlearning approaches in retaining safety, while effectively preserving benign generation capability. Our code and pretrained models are publicly available at https://github.com/AntigoneRandy/ResAlign.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look the Other Way: Designing 'Positive' Molecules with Negative Data via Task Arithmetic</title>
<link>https://arxiv.org/abs/2507.17876</link>
<guid>https://arxiv.org/abs/2507.17876</guid>
<content:encoded><![CDATA[

arXiv:2507.17876v2 Announce Type: replace 
Abstract: The scarcity of molecules with desirable properties (i.e., `positive' molecules) is an inherent bottleneck for generative molecule design. To sidestep such obstacle, here we propose molecular task arithmetic: training a model on diverse and abundant negative examples to learn 'property directions' - without accessing any positively labeled data - and moving models in the opposite property directions to generate positive molecules. When analyzed on 33 design experiments with distinct molecular entities (small molecules, proteins), model architectures, and scales, molecular task arithmetic generated more diverse and successful designs than models trained on positive molecules in general. Moreover, we employed molecular task arithmetic in dual-objective and few-shot design tasks. We find that molecular task arithmetic can consistently increase the diversity of designs while maintaining desirable complex design properties, such as good docking scores to a protein. With its simplicity, data efficiency, and performance, molecular task arithmetic bears the potential to become the de facto transfer learning strategy for de novo molecule design.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density Operator Expectation Maximization</title>
<link>https://arxiv.org/abs/2507.22786</link>
<guid>https://arxiv.org/abs/2507.22786</guid>
<content:encoded><![CDATA[

arXiv:2507.22786v2 Announce Type: replace 
Abstract: Machine learning with density operators, the mathematical foundation of quantum mechanics, is gaining prominence with rapid advances in quantum computing. Generative models based on density operators cannot yet handle tasks that are routinely handled by probabilistic models. The progress of latent variable models, a broad and influential class of probabilistic unsupervised models, was driven by the Expectation-Maximization framework. Deriving such a framework for density operators is challenging due to the non-commutativity of operators. To tackle this challenge, an inequality arising from the monotonicity of relative entropy is demonstrated to serve as an evidence lower bound for density operators. A minorant-maximization perspective on this bound leads to Density Operator Expectation Maximization (DO-EM), a general framework for training latent variable models defined through density operators. Through an information-geometric argument, the Expectation step in DO-EM is shown to be the Petz recovery map. The DO-EM algorithm is applied to Quantum Restricted Boltzmann Machines, adapting Contrastive Divergence to approximate the Maximization step gradient. Quantum interleaved Deep Boltzmann Machines and Quantum Gaussian-Bernoulli Restricted Boltzmann Machines, new models introduced in this work, outperform their probabilistic counterparts on generative tasks when trained with similar computational resources and identical hyperparameters.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Day-Ahead Energy Trading with Proximal Policy Optimization and Blockchain</title>
<link>https://arxiv.org/abs/2508.01888</link>
<guid>https://arxiv.org/abs/2508.01888</guid>
<content:encoded><![CDATA[

arXiv:2508.01888v2 Announce Type: replace 
Abstract: The increasing penetration of renewable energy sources in day-ahead energy markets introduces challenges in balancing supply and demand, ensuring grid resilience, and maintaining trust in decentralized trading systems. This paper proposes a novel framework that integrates the Proximal Policy Optimization (PPO) algorithm, a state-of-the-art reinforcement learning method, with blockchain technology to optimize automated trading strategies for prosumers in day-ahead energy markets. We introduce a comprehensive framework that employs RL agent for multi-objective energy optimization and blockchain for tamper-proof data and transaction management. Simulations using real-world data from the Electricity Reliability Council of Texas (ERCOT) demonstrate the effectiveness of our approach. The RL agent achieves demand-supply balancing within 2\% and maintains near-optimal supply costs for the majority of the operating hours. Moreover, it generates robust battery storage policies capable of handling variability in solar and wind generation. All decisions are recorded on an Algorand-based blockchain, ensuring transparency, auditability, and security - key enablers for trustworthy multi-agent energy trading. Our contributions include a novel system architecture, curriculum learning for robust agent development, and actionable policy insights for practical deployment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Large-Scale Pre-trained Models for Automated Ad Bidding Optimization</title>
<link>https://arxiv.org/abs/2508.02002</link>
<guid>https://arxiv.org/abs/2508.02002</guid>
<content:encoded><![CDATA[

arXiv:2508.02002v2 Announce Type: replace 
Abstract: Modern auto-bidding systems are required to balance overall performance with diverse advertiser goals and real-world constraints, reflecting the dynamic and evolving needs of the industry. Recent advances in conditional generative models, such as transformers and diffusers, have enabled direct trajectory generation tailored to advertiser preferences, offering a promising alternative to traditional Markov Decision Process-based methods. However, these generative methods face significant challenges, such as the distribution shift between offline and online environments, limited exploration of the action space, and the necessity to meet constraints like marginal Cost-per-Mille (CPM) and Return on Investment (ROI). To tackle these challenges, we propose GRAD (Generative Reward-driven Ad-bidding with Mixture-of-Experts), a scalable foundation model for auto-bidding that combines an Action-Mixture-of-Experts module for diverse bidding action exploration with the Value Estimator of Causal Transformer for constraint-aware optimization. Extensive offline and online experiments demonstrate that GRAD significantly enhances platform revenue, highlighting its effectiveness in addressing the evolving and diverse requirements of modern advertisers. Furthermore, GRAD has been implemented in multiple marketing scenarios at Meituan, one of the world's largest online food delivery platforms, leading to a 2.18% increase in Gross Merchandise Value (GMV) and 10.68% increase in ROI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DP-LLM: Runtime Model Adaptation with Dynamic Layer-wise Precision Assignment</title>
<link>https://arxiv.org/abs/2508.06041</link>
<guid>https://arxiv.org/abs/2508.06041</guid>
<content:encoded><![CDATA[

arXiv:2508.06041v4 Announce Type: replace 
Abstract: How can we effectively handle queries for on-device large language models (LLMs) with varying runtime constraints, such as latency and accuracy? Multi-scale quantization addresses this challenge by enabling memory-efficient runtime model adaptation of LLMs through the overlaying of multiple model variants quantized to different bitwidths. Meanwhile, an important question still remains open-ended: how can models be properly configured to match a target precision or latency? While mixed-precision offers a promising solution, we take this further by leveraging the key observation that the sensitivity of each layer dynamically changes across decoding steps. Building on this insight, we introduce DP-LLM, a novel mechanism that dynamically assigns precision to each layer based on input values. Experimental results across multiple models and benchmarks demonstrate that DP-LLM achieves a superior performance-latency trade-off, outperforming prior approaches.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Architecture-Aware Generalization Bounds for Temporal Networks: Theory and Fair Comparison Methodology</title>
<link>https://arxiv.org/abs/2508.06066</link>
<guid>https://arxiv.org/abs/2508.06066</guid>
<content:encoded><![CDATA[

arXiv:2508.06066v2 Announce Type: replace 
Abstract: Deep temporal architectures such as TCNs achieve strong predictive performance on sequential data, yet theoretical understanding of their generalization remains limited. We address this gap through three contributions: introducing an evaluation methodology for temporal models, revealing surprising empirical phenomena about temporal dependence, and the first architecture-aware theoretical framework for dependent sequences.
  Fair-Comparison Methodology. We introduce evaluation protocols that fix effective sample size $N_{\text{eff}}$ to isolate temporal structure effects from information content.
  Empirical Findings. Applying this method reveals that under $N_{\text{eff}} = 2000$, strongly dependent sequences ($\rho = 0.8$) exhibit approx' $76\%$ smaller generalization gaps than weakly dependent ones ($\rho = 0.2$), challenging the conventional view that dependence universally impedes learning. However, observed convergence rates ($N_{\text{eff}}^{-1.21}$ to $N_{\text{eff}}^{-0.89}$) significantly exceed theoretical worst-case predictions ($N^{-0.5}$), revealing that temporal architectures exploit problem structure in ways current theory does not capture.
  Lastly, we develop the first architecture-aware generalization bounds for deep temporal models on exponentially $\beta$-mixing sequences. By embedding Golowich et al.'s i.i.d. class bound within a novel blocking scheme that partitions $N$ samples into approx' $B \approx N/\log N$ quasi-independent blocks, we establish polynomial sample complexity under convex Lipschitz losses. The framework achieves $\sqrt{D}$ depth scaling alongside the product of layer-wise norms $R = \prod_{\ell=1}^{D} M^{(\ell)}$, avoiding exponential dependence. While these bounds are conservative, they prove learnability and identify architectural scaling laws, providing worst-case baselines that highlight where future theory must improve.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo</title>
<link>https://arxiv.org/abs/2508.07631</link>
<guid>https://arxiv.org/abs/2508.07631</guid>
<content:encoded><![CDATA[

arXiv:2508.07631v3 Announce Type: replace 
Abstract: We study the problem of posterior sampling in the context of score based generative models. We have a trained score network for a prior $p(x)$, a measurement model $p(y|x)$, and are tasked with sampling from the posterior $p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case) under well-accepted computational hardness assumptions. Despite this, popular algorithms for tasks such as image super-resolution, stylization, and reconstruction enjoy empirical success. Rather than establishing distributional assumptions or restricted settings under which exact posterior sampling is tractable, we view this as a more general "tilting" problem of biasing a distribution towards a measurement. Under minimal assumptions, we show that one can tractably sample from a distribution that is simultaneously close to the posterior of a noised prior in KL divergence and the true posterior in Fisher divergence. Intuitively, this combination ensures that the resulting sample is consistent with both the measurement and the prior. To the best of our knowledge these are the first formal results for (approximate) posterior sampling in polynomial time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP</title>
<link>https://arxiv.org/abs/2508.08005</link>
<guid>https://arxiv.org/abs/2508.08005</guid>
<content:encoded><![CDATA[

arXiv:2508.08005v3 Announce Type: replace 
Abstract: The Maximum Clique Problem (MCP) is a foundational NP-hard problem with wide-ranging applications, yet no single algorithm consistently outperforms all others across diverse graph instances. This underscores the critical need for instance-aware algorithm selection, a domain that remains largely unexplored for the MCP. To address this gap, we propose a novel learning-based framework that integrates both traditional machine learning and graph neural networks. We first construct a benchmark dataset by executing four state-of-the-art exact MCP solvers on a diverse collection of graphs and extracting their structural features. An evaluation of conventional classifiers establishes Random Forest as a strong baseline and reveals that connectivity and topological features are key predictors of performance. Building on these insights, we develop GAT-MLP, a dual-channel model that combines a Graph Attention Network (GAT) to encode local graph structure with a Multilayer Perceptron (MLP) to model global features. Extensive experiments demonstrate that GAT-MLP achieves superior and consistent performance, significantly outperforming all baseline methods. Our results highlight the effectiveness of the dual-channel architecture and the promise of graph neural networks for combinatorial algorithm selection, achieving 90.43% accuracy in choosing the optimal solver. Code and models are available at: https://anonymous.4open.science/r/GAT-MLP-7E5F.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent</title>
<link>https://arxiv.org/abs/2508.08222</link>
<guid>https://arxiv.org/abs/2508.08222</guid>
<content:encoded><![CDATA[

arXiv:2508.08222v2 Announce Type: replace 
Abstract: Transformers have demonstrated remarkable capabilities in multi-step reasoning tasks. However, understandings of the underlying mechanisms by which they acquire these abilities through training remain limited, particularly from a theoretical standpoint. This work investigates how transformers learn to solve symbolic multi-step reasoning problems through chain-of-thought processes, focusing on path-finding in trees. We analyze two intertwined tasks: a backward reasoning task, where the model outputs a path from a goal node to the root, and a more complex forward reasoning task, where the model implements two-stage reasoning by first identifying the goal-to-root path and then reversing it to produce the root-to-goal path. Our theoretical analysis, grounded in the dynamics of gradient descent, shows that trained one-layer transformers can provably solve both tasks with generalization guarantees to unseen trees. In particular, our multi-phase training dynamics for forward reasoning elucidate how different attention heads learn to specialize and coordinate autonomously to solve the two subtasks in a single autoregressive path. These results provide a mechanistic explanation of how trained transformers can implement sequential algorithmic procedures. Moreover, they offer insights into the emergence of reasoning abilities, suggesting that when tasks are structured to take intermediate chain-of-thought steps, even shallow multi-head transformers can effectively solve problems that would otherwise require deeper architectures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Jointly Computation- and Communication-Efficient Distributed Learning</title>
<link>https://arxiv.org/abs/2508.15509</link>
<guid>https://arxiv.org/abs/2508.15509</guid>
<content:encoded><![CDATA[

arXiv:2508.15509v2 Announce Type: replace 
Abstract: We address distributed learning problems over undirected networks. Specifically, we focus on designing a novel ADMM-based algorithm that is jointly computation- and communication-efficient. Our design guarantees computational efficiency by allowing agents to use stochastic gradients during local training. Moreover, communication efficiency is achieved as follows: i) the agents perform multiple training epochs between communication rounds, and ii) compressed transmissions are used. We prove exact linear convergence of the algorithm in the strongly convex setting. We corroborate our theoretical results by numerical comparisons with state of the art techniques on a classification task.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditionally adaptive augmented Lagrangian method for physics-informed learning of forward and inverse problems</title>
<link>https://arxiv.org/abs/2508.15695</link>
<guid>https://arxiv.org/abs/2508.15695</guid>
<content:encoded><![CDATA[

arXiv:2508.15695v2 Announce Type: replace 
Abstract: We present several key advances to the Physics and Equality Constrained Artificial Neural Networks (PECANN) framework, substantially improving its capacity to solve challenging partial differential equations (PDEs). Our enhancements broaden the framework's applicability and improve efficiency. First, we generalize the Augmented Lagrangian Method (ALM) to support multiple, independent penalty parameters for enforcing heterogeneous constraints. Second, we introduce a constraint aggregation technique to address inefficiencies associated with point-wise enforcement. Third, we incorporate a single Fourier feature mapping to capture highly oscillatory solutions with multi-scale features, where alternative methods often require multiple mappings or costlier architectures. Fourth, a novel time-windowing strategy enables seamless long-time evolution without relying on discrete time models. Fifth, and critically, we propose a conditionally adaptive penalty update (CAPU) strategy for ALM that accelerates the growth of Lagrange multipliers for constraints with larger violations, while enabling coordinated updates of multiple penalty parameters. CAPU accelerates the growth of Lagrange multipliers for selectively challenging constraints, enhancing constraint enforcement during training. We demonstrate the effectiveness of PECANN-CAPU across diverse problems, including the transonic rarefaction problem, reversible scalar advection by a vortex, high-wavenumber Helmholtz and Poisson's equations, and inverse heat source identification. The framework achieves competitive accuracy across all cases when compared with established methods and recent approaches based on Kolmogorov-Arnold networks. Collectively, these advances improve the robustness, computational efficiency, and applicability of PECANN to demanding problems in scientific computing.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ONG: Orthogonal Natural Gradient Descent</title>
<link>https://arxiv.org/abs/2508.17169</link>
<guid>https://arxiv.org/abs/2508.17169</guid>
<content:encoded><![CDATA[

arXiv:2508.17169v3 Announce Type: replace 
Abstract: Orthogonal Gradient Descent (OGD) has emerged as a powerful method for continual learning. However, its Euclidean projections do not leverage the underlying information-geometric structure of the problem, which can lead to suboptimal convergence in learning tasks. To address this, we propose incorporating the natural gradient into OGD and present \textbf{ONG (Orthogonal Natural Gradient Descent)}. ONG preconditions each new task-specific gradient with an efficient EKFAC approximation of the inverse Fisher information matrix, yielding updates that follow the steepest descent direction under a Riemannian metric. To preserve performance on previously learned tasks, ONG projects these natural gradients onto the orthogonal complement of prior tasks' natural gradients. We provide an initial theoretical justification for this procedure, introduce the Orthogonal Natural Gradient Descent (ONG) algorithm, and present preliminary results on the Permuted and Rotated MNIST benchmarks. Our preliminary results, however, indicate that a naive combination of natural gradients and orthogonal projections has potential issues. This finding has motivated continued future work focused on robustly reconciling these geometric perspectives to develop a continual learning method, establishing a more rigorous theoretical foundation with formal convergence guarantees, and extending empirical validation to large-scale continual learning benchmarks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</title>
<link>https://arxiv.org/abs/2508.20549</link>
<guid>https://arxiv.org/abs/2508.20549</guid>
<content:encoded><![CDATA[

arXiv:2508.20549v2 Announce Type: replace 
Abstract: The application of Vision-Language Models (VLMs) in medicine is critically hampered by the scarcity of high-quality, expert-annotated data. Supervised Fine-Tuning (SFT) on existing datasets often leads to poor generalization on unseen modalities and tasks, while Reinforcement Learning (RL), a promising alternative, is stymied by the lack of reliable reward signals in this data-scarce domain. To break this impasse, we introduce Generative Reward Learning for Medical Reasoning (MedGR$^2$), a novel framework that creates a self-improving virtuous cycle. MedGR$^2$ co-develops a data generator and a reward model, enabling the automated, continuous creation of high-quality, multi-modal medical data that serves as both a superior training source for SFT and RL. Our experiments demonstrate that SFT with MedGR$^2$-produced data already surpasses baselines trained on large-scale, human-curated datasets. Crucially, when leveraging this data for RL via Group Relative Policy Optimization (GRPO), our model achieves state-of-the-art cross-modality and cross-task generalization, significantly outperforming specialized RL-based methods. Furthermore, our compact model, empowered by MedGR$^2$, achieves performance competitive with foundation models possessing over 10 times more parameters. MedGR$^2$ presents a new paradigm for data-efficient learning in high-stakes domains, transforming the problem from data scarcity to data generation and unlocking the full potential of RL for building truly generalizable medical AI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>JaGuard: Jamming Correction of GNSS Deviation with Deep Temporal Graphs</title>
<link>https://arxiv.org/abs/2509.14000</link>
<guid>https://arxiv.org/abs/2509.14000</guid>
<content:encoded><![CDATA[

arXiv:2509.14000v2 Announce Type: replace 
Abstract: Global Navigation Satellite Systems (GNSS) are increasingly exposed to intentional jamming, threatening reliability when accurate positioning and timing are most critical. We address this problem by formulating interference mitigation as a dynamic graph regression task and propose JaGuard, a receiver-centric temporal graph neural network that estimates and corrects latitude and longitude errors. At each 1 Hz epoch, the satellite-receiver scene is represented as a heterogeneous star graph with time-varying satellite attributes such as SNR, azimuth and elevation. A single-layer HeteroGCLSTM fuses one-hop spatial context with short-term temporal dynamics to produce a 2D deviation estimate.
  We evaluate JaGuard on data collected from two commercial receivers under controlled conducted jamming using three jammer types (CW, 3xCW, FM) and six power levels from -45 to -70 dBm, each repeated 50 times across pre-jam, jam, and recovery phases. JaGuard outperforms strong multivariate baselines (TSMixer, uniform CNN, Seq2Point) in all conditions. Under severe jamming at -45 dBm, it achieves 3.64-7.74 cm MAE, improving to 1.59-1.90 cm for -60 to -70 dBm. On mixed-mode datasets, it attains 3.78 cm MAE on GP01 and 4.25 cm on U-blox 10. With only 10 percent of the training data, JaGuard remains ahead, reaching about 20 cm MAE compared to 36-42 cm for the baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evidential Physics-Informed Neural Networks for Scientific Discovery</title>
<link>https://arxiv.org/abs/2509.14568</link>
<guid>https://arxiv.org/abs/2509.14568</guid>
<content:encoded><![CDATA[

arXiv:2509.14568v3 Announce Type: replace 
Abstract: We present the fundamental theory and implementation guidelines underlying Evidential Physics-Informed Neural Network (E-PINN) -- a novel class of uncertainty-aware PINN. It leverages the marginal distribution loss function of evidential deep learning for estimating uncertainty of outputs, and infers unknown parameters of the PDE via a learned posterior distribution. Validating our model on two illustrative case studies -- the 1D Poisson equation with a Gaussian source and the 2D Fisher-KPP equation, we found that E-PINN generated empirical coverage probabilities that were calibrated significantly better than Bayesian PINN and Deep Ensemble methods. To demonstrate real-world applicability, we also present a brief case study on applying E-PINN to analyze clinical glucose-insulin datasets that have featured in medical research on diabetes pathophysiology.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Multi-Query Paradox in Zeroth-Order Optimization</title>
<link>https://arxiv.org/abs/2509.15552</link>
<guid>https://arxiv.org/abs/2509.15552</guid>
<content:encoded><![CDATA[

arXiv:2509.15552v3 Announce Type: replace 
Abstract: Zeroth-order (ZO) optimization provides a powerful framework for problems where explicit gradients are unavailable and have to be approximated using only queries to function value. The prevalent single-query approach is simple, but suffers from high estimation variance, motivating a multi-query paradigm to improves estimation accuracy. This, however, creates a critical trade-off: under a fixed budget of queries (i.e. cost), queries per iteration and the total number of optimization iterations are inversely proportional to one another. How to best allocate this budget is a fundamental, under-explored question.
  This work systematically resolves this query allocation problem. We analyze two aggregation methods: the de facto simple averaging (ZO-Avg), and a new Projection Alignment method (ZO-Align) we derive from local surrogate minimization. By deriving convergence rates for both methods that make the dependence on the number of queries explicit across strongly convex, convex, non-convex, and stochastic settings, we uncover a stark dichotomy: For ZO-Avg, we prove that using more than one query per iteration is always query-inefficient, rendering the single-query approach optimal. On the contrary, ZO-Align generally performs better with more queries per iteration, resulting in a full-subspace estimation as the optimal approach. Thus, our work clarifies that the multi-query problem boils down to a choice not about an intermediate query size, but between two classic algorithms, a choice dictated entirely by the aggregation method used. These theoretical findings are also consistently validated by extensive experiments.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Staying on the Manifold: Geometry-Aware Noise Injection</title>
<link>https://arxiv.org/abs/2509.20201</link>
<guid>https://arxiv.org/abs/2509.20201</guid>
<content:encoded><![CDATA[

arXiv:2509.20201v2 Announce Type: replace 
Abstract: It has been shown that perturbing the input during training implicitly regularises the gradient of the learnt function, leading to smoother models and enhancing generalisation. However, previous research mostly considered the addition of ambient noise in the input space, without considering the underlying structure of the data. In this work, we propose several strategies of adding geometry-aware input noise that accounts for the lower dimensional manifold the input space inhabits. We start by projecting ambient Gaussian noise onto the tangent space of the manifold. In a second step, the noise sample is mapped on the manifold via the associated geodesic curve. We also consider Brownian motion noise, which moves in random steps along the manifold. We show that geometry-aware noise leads to improved generalisation and robustness to hyperparameter selection on highly curved manifolds, while performing at least as well as training without noise on simpler manifolds. Our proposed framework extends to data manifolds approximated by generative models and we observe similar trends on the MNIST digits dataset.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Task Reasoning LLM Agents for Multi-turn Task Planning via Single-turn Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.20616</link>
<guid>https://arxiv.org/abs/2509.20616</guid>
<content:encoded><![CDATA[

arXiv:2509.20616v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in knowledge acquisition, reasoning, and tool use, making them promising candidates for autonomous agent applications. However, training LLM agents for complex multi-turn task planning faces significant challenges, including sparse episode-wise rewards, credit assignment across long horizons, and the computational overhead of reinforcement learning in multi-turn interaction settings. To this end, this paper introduces a novel approach that transforms multi-turn task planning into single-turn task reasoning problems, enabling efficient policy optimization through Group Relative Policy Optimization (GRPO) with dense and verifiable reward from expert trajectories. Our theoretical analysis shows that GRPO improvement on single-turn task reasoning results in a lower bound of the multi-turn success probability under the minimal turns, as well as the generalization to subtasks with shorter horizons. Experimental evaluation on the complex task planning benchmark demonstrates that our 1.5B parameter model trained with single-turn GRPO achieves superior performance compared to larger baseline models up to 14B parameters, with success rates of 70% for long-horizon planning tasks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closed-form $\ell_r$ norm scaling with data for overparameterized linear regression and diagonal linear networks under $\ell_p$ bias</title>
<link>https://arxiv.org/abs/2509.21181</link>
<guid>https://arxiv.org/abs/2509.21181</guid>
<content:encoded><![CDATA[

arXiv:2509.21181v3 Announce Type: replace 
Abstract: For overparameterized linear regression with isotropic Gaussian design and minimum-$\ell_p$ interpolator $p\in(1,2]$, we give a unified, high-probability characterization for the scaling of the family of parameter norms $ \\{ \lVert \widehat{w_p} \rVert_r \\}_{r \in [1,p]} $ with sample size.
  We solve this basic, but unresolved question through a simple dual-ray analysis, which reveals a competition between a signal *spike* and a *bulk* of null coordinates in $X^\top Y$, yielding closed-form predictions for (i) a data-dependent transition $n_\star$ (the "elbow"), and (ii) a universal threshold $r_\star=2(p-1)$ that separates $\lVert \widehat{w_p} \rVert_r$'s which plateau from those that continue to grow with an explicit exponent.
  This unified solution resolves the scaling of *all* $\ell_r$ norms within the family $r\in [1,p]$ under $\ell_p$-biased interpolation, and explains in one picture which norms saturate and which increase as $n$ grows.
  We then study diagonal linear networks (DLNs) trained by gradient descent. By calibrating the initialization scale $\alpha$ to an effective $p_{\mathrm{eff}}(\alpha)$ via the DLN separable potential, we show empirically that DLNs inherit the same elbow/threshold laws, providing a predictive bridge between explicit and implicit bias.
  Given that many generalization proxies depend on $\lVert \widehat {w_p} \rVert_r$, our results suggest that their predictive power will depend sensitively on which $l_r$ norm is used.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.22601</link>
<guid>https://arxiv.org/abs/2509.22601</guid>
<content:encoded><![CDATA[

arXiv:2509.22601v4 Announce Type: replace 
Abstract: Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent's own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL, where a replay buffer stores good experience for off-policy update, by gradually steering the policy entropy across stages. Specifically, the proposed curriculum scheduling harmonizes intrinsic reward shaping and self-imitation to 1) expedite exploration via frequent tool interactions at the beginning, and 2) strengthen exploitation of successful tactics upon convergence towards familiarity with the environment. We also combine bag-of-tricks of industrial RL optimizations for a strong baseline Dr.BoT to demonstrate our effectiveness. In ALFWorld and WebShop, SPEAR increases the success rates of GRPO/GiGPO/Dr.BoT by up to 16.1%/5.1%/8.6% and 20.7%/11.8%/13.9%, respectively. In AIME24 and AIME25, SPEAR boosts Dr.BoT by up to 3.8% and 6.1%, respectively. Such gains incur only 10%-25% extra theoretical complexity and negligible runtime overhead in practice, demonstrating the plug-and-play scalability of SPEAR.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuON: A Linear-time Alternative to Orthogonal Momentum Updates</title>
<link>https://arxiv.org/abs/2509.24320</link>
<guid>https://arxiv.org/abs/2509.24320</guid>
<content:encoded><![CDATA[

arXiv:2509.24320v3 Announce Type: replace 
Abstract: Orthogonal momentum gradient updates have emerged to overcome the limitations of vector-based optimizers like Adam. The vector-based optimizer Adam suffers from high memory costs and ill-conditioned momentum gradient updates. However, traditional Orthogonal momentum approaches, such as SVD/QR decomposition, suffer from high computational and memory costs and underperform compared to well-tuned SGD with momentum.Recent advances, such as Muon, improve efficiency by applying momentum before orthogonalization and approximate orthogonal matrices via Newton-Schulz iterations, which gives better GPU utilization, active high TFLOPS, and reduces memory usage by up to 3x. Nevertheless, Muon(Vanilla) suffers from exploding attention logits and has cubic computation complexity. In this paper we , deep dive into orthogonal momentum gradient updates to find the main properties that help Muon to achieve remarkable performance.We propose \textbf{AuON} (Alternative Unit-norm momentum updates by Normalized nonlinear scaling), a linear-time optimizer that achieves strong performance without approximate orthogonal matrices, while preserving structural alignment and reconditioning ill-posed updates. AuON has an automatic (\textbf{"emergency brake"}) to handle exploding attention logits.. We further introduce a hybrid variant (\textbf{ Hybrid-AuON})that applies the linear transformations with Newton-Schulz iterations which out performs Muon in the language modeling tasks. Code is available at: https://github.com/ryyzn9/AuON
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Inductive Biases for Voltage Prediction in Distribution Grids</title>
<link>https://arxiv.org/abs/2509.25158</link>
<guid>https://arxiv.org/abs/2509.25158</guid>
<content:encoded><![CDATA[

arXiv:2509.25158v2 Announce Type: replace 
Abstract: Voltage prediction in distribution grids is a critical yet difficult task for maintaining power system stability. Machine learning approaches, particularly Graph Neural Networks (GNNs), offer significant speedups but suffer from poor generalization when trained on limited or incomplete data. In this work, we systematically investigate the role of inductive biases in improving a model's ability to reliably learn power flow. Specifically, we evaluate three physics-informed strategies: (i) power-flow-constrained loss functions, (ii) complex-valued neural networks, and (iii) residual-based task reformulation. Using the ENGAGE dataset, which spans multiple low- and medium-voltage grid configurations, we conduct controlled experiments to isolate the effect of each inductive bias and assess both standard predictive performance and out-of-distribution generalization. Our study provides practical insights into which model assumptions most effectively guide learning for reliable and efficient voltage prediction in modern distribution networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eyes-on-Me: Scalable RAG Poisoning through Transferable Attention-Steering Attractors</title>
<link>https://arxiv.org/abs/2510.00586</link>
<guid>https://arxiv.org/abs/2510.00586</guid>
<content:encoded><![CDATA[

arXiv:2510.00586v2 Announce Type: replace 
Abstract: Existing data poisoning attacks on retrieval-augmented generation (RAG) systems scale poorly because they require costly optimization of poisoned documents for each target phrase. We introduce Eyes-on-Me, a modular attack that decomposes an adversarial document into reusable Attention Attractors and Focus Regions. Attractors are optimized to direct attention to the Focus Region. Attackers can then insert semantic baits for the retriever or malicious instructions for the generator, adapting to new targets at near zero cost. This is achieved by steering a small subset of attention heads that we empirically identify as strongly correlated with attack success. Across 18 end-to-end RAG settings (3 datasets $\times$ 2 retrievers $\times$ 3 generators), Eyes-on-Me raises average attack success rates from 21.9 to 57.8 (+35.9 points, 2.6$\times$ over prior work). A single optimized attractor transfers to unseen black box retrievers and generators without retraining. Our findings establish a scalable paradigm for RAG data poisoning and show that modular, reusable components pose a practical threat to modern AI systems. They also reveal a strong link between attention concentration and model outputs, informing interpretability research.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.01132</link>
<guid>https://arxiv.org/abs/2510.01132</guid>
<content:encoded><![CDATA[

arXiv:2510.01132v2 Announce Type: replace 
Abstract: We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RAMAC: Multimodal Risk-Aware Offline Reinforcement Learning and the Role of Behavior Regularization</title>
<link>https://arxiv.org/abs/2510.02695</link>
<guid>https://arxiv.org/abs/2510.02695</guid>
<content:encoded><![CDATA[

arXiv:2510.02695v2 Announce Type: replace 
Abstract: In safety-critical domains where online data collection is infeasible, offline reinforcement learning (RL) offers an attractive alternative but only if policies deliver high returns without incurring catastrophic lower-tail risk. Prior work on risk-averse offline RL achieves safety at the cost of value or model-based pessimism, and restricted policy classes that limit policy expressiveness, whereas diffusion/flow-based expressive generative policies trained with a behavioral-cloning (BC) objective have been used only in risk-neutral settings. Here, we address this gap by introducing the \textbf{Risk-Aware Multimodal Actor-Critic (RAMAC)}, which couples an expressive generative actor with a distributional critic and, to our knowledge, is the first model-free approach that learns \emph{risk-aware expressive generative policies}. RAMAC differentiates a composite objective that adds a Conditional Value-at-Risk (CVaR) term to a BC loss, achieving risk-sensitive learning in complex multimodal scenarios. Since out-of-distribution (OOD) actions are a major driver of catastrophic failures in offline RL, we further analyze OOD behavior under prior-anchored perturbation schemes from recent BC-regularized risk-averse offline RL. This clarifies why a behavior-regularized objective that directly constrains the expressive generative policy to the dataset support provides an effective, risk-agnostic mechanism for suppressing OOD actions in modern expressive policies. We instantiate RAMAC with a diffusion-based actor, using it both to illustrate the analysis in a 2-D risky bandit and to deploy OOD-action detectors on Stochastic-D4RL benchmarks, empirically validating our insights. Across these tasks, we observe consistent gains in $\mathrm{CVaR}_{0.1}$ while maintaining strong returns. Our implementation is available at GitHub: https://github.com/KaiFukazawa/RAMAC.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>General Exploratory Bonus for Optimistic Exploration in RLHF</title>
<link>https://arxiv.org/abs/2510.03269</link>
<guid>https://arxiv.org/abs/2510.03269</guid>
<content:encoded><![CDATA[

arXiv:2510.03269v3 Announce Type: replace 
Abstract: Optimistic exploration is central to improving sample efficiency in reinforcement learning with human feedback, yet existing exploratory bonus methods to incentivize exploration often fail to realize optimism. We provide a theoretical analysis showing that current formulations, under KL or $\alpha$-divergence regularization, unintentionally bias exploration toward high-probability regions of the reference model, thereby reinforcing conservative behavior instead of promoting discovery of uncertain regions. To address this pitfall, we introduce the General Exploratory Bonus (GEB), a novel theoretical framework that provably satisfies the optimism principle. GEB counteracts divergence-induced bias via reference-dependent reward regulation and unifies prior heuristic bonuses as special cases, while extending naturally across the full $\alpha$-divergence family. Empirically, GEB consistently outperforms baselines on alignment tasks across multiple divergence settings and large language model backbones. These results demonstrate that GEB offers both a principled and practical solution for optimistic exploration in RLHF.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trustworthy Retrosynthesis: Eliminating Hallucinations with a Diverse Ensemble of Reaction Scorers</title>
<link>https://arxiv.org/abs/2510.10645</link>
<guid>https://arxiv.org/abs/2510.10645</guid>
<content:encoded><![CDATA[

arXiv:2510.10645v3 Announce Type: replace 
Abstract: Retrosynthesis is one of the domains transformed by the rise of generative models, and it is one where the problem of nonsensical or erroneous outputs (hallucinations) is particularly insidious: reliable assessment of synthetic plans is time-consuming, with automatic methods lacking. In this work, we present RetroTrim, a retrosynthesis system that successfully avoids nonsensical plans on a set of challenging drug-like targets. Compared to common baselines in the field, our system is not only the sole method that succeeds in filtering out hallucinated reactions, but it also results in the highest number of high-quality paths overall. The key insight behind RetroTrim is the combination of diverse reaction scoring strategies, based on machine learning models and existing chemical databases. We show that our scoring strategies capture different classes of hallucinations by analyzing them on a dataset of labeled retrosynthetic intermediates. This approach formed the basis of our winning solution to the Standard Industries \$1 million Retrosynthesis Challenge. To measure the performance of retrosynthesis systems, we propose a novel evaluation protocol for reactions and synthetic paths based on a structured review by expert chemists. Using this protocol, we compare systems on a set of 32 novel targets, curated to reflect recent trends in drug structures. While the insights behind our methodology are broadly applicable to retrosynthesis, our focus is on targets in the drug-like domain. By releasing our benchmark targets and the details of our evaluation protocol, we hope to inspire further research into reliable retrosynthesis.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training</title>
<link>https://arxiv.org/abs/2510.14614</link>
<guid>https://arxiv.org/abs/2510.14614</guid>
<content:encoded><![CDATA[

arXiv:2510.14614v2 Announce Type: replace 
Abstract: As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline. Codes are available at: https://github.com/CASL-KU/FAL
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Observations to Parameters: Detecting Changepoint in Nonlinear Dynamics with Simulation-based Inference</title>
<link>https://arxiv.org/abs/2510.17933</link>
<guid>https://arxiv.org/abs/2510.17933</guid>
<content:encoded><![CDATA[

arXiv:2510.17933v2 Announce Type: replace 
Abstract: Detecting regime shifts in chaotic time series is hard because observation-space signals are entangled with intrinsic variability. We propose Parameter--Space Changepoint Detection (Param--CPD), a two--stage framework that first amortizes Bayesian inference of governing parameters with a neural posterior estimator trained by simulation-based inference, and then applies a standard CPD algorithm to the resulting parameter trajectory. On Lorenz--63 with piecewise-constant parameters, Param--CPD improves F1, reduces localization error, and lowers false positives compared to observation--space baselines. We further verify identifiability and calibration of the inferred posteriors on stationary trajectories, explaining why parameter space offers a cleaner detection signal. Robustness analyses over tolerance, window length, and noise indicate consistent gains. Our results show that operating in a physically interpretable parameter space enables accurate and interpretable changepoint detection in nonlinear dynamical systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QiMeng-SALV: Signal-Aware Learning for Verilog Code Generation</title>
<link>https://arxiv.org/abs/2510.19296</link>
<guid>https://arxiv.org/abs/2510.19296</guid>
<content:encoded><![CDATA[

arXiv:2510.19296v4 Announce Type: replace 
Abstract: The remarkable progress of Large Language Models (LLMs) presents promising opportunities for Verilog code generation which is significantly important for automated circuit design. The lacking of meaningful functional rewards hinders the preference optimization based on Reinforcement Learning (RL) for producing functionally correct Verilog code. In this paper, we propose Signal-Aware Learning for Verilog code generation (QiMeng-SALV) by leveraging code segments of functionally correct output signal to optimize RL training. Considering Verilog code specifies the structural interconnection of hardware gates and wires so that different output signals are independent, the key insight of QiMeng-SALV is to extract verified signal-aware implementations in partially incorrect modules, so as to enhance the extraction of meaningful functional rewards. Roughly, we verify the functional correctness of signals in generated module by comparing with that of reference module in the training data. Then abstract syntax tree (AST) is employed to identify signal-aware code segments which can provide meaningful functional rewards from erroneous modules. Finally, we introduce signal-aware DPO which is optimized on the correct signal-level code segments, thereby preventing noise and interference from incorrect signals. The proposed QiMeng-SALV underscores the paradigm shift from conventional module-level to fine-grained signal-level optimization in Verilog code generation, addressing the issue of insufficient functional rewards. Experiments demonstrate that our method achieves state-of-the-art performance on VerilogEval and RTLLM, with a 7B parameter model matching the performance of the DeepSeek v3 671B model and significantly outperforming the leading open-source model CodeV trained on the same dataset. Our code is available at https://github.com/QiMeng-IPRC/QiMeng-SALV.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-diffusion for Solving Inverse Problems</title>
<link>https://arxiv.org/abs/2510.21417</link>
<guid>https://arxiv.org/abs/2510.21417</guid>
<content:encoded><![CDATA[

arXiv:2510.21417v2 Announce Type: replace 
Abstract: We propose self-diffusion, a novel framework for solving inverse problems without relying on pretrained generative models. Traditional diffusion-based approaches require training a model on a clean dataset to learn to reverse the forward noising process. This model is then used to sample clean solutions -- corresponding to posterior sampling from a Bayesian perspective -- that are consistent with the observed data under a specific task. In contrast, self-diffusion introduces a self-contained iterative process that alternates between noising and denoising steps to progressively refine its estimate of the solution. At each step of self-diffusion, noise is added to the current estimate, and a self-denoiser, which is a single untrained convolutional network randomly initialized from scratch, is continuously trained for certain iterations via a data fidelity loss to predict the solution from the noisy estimate. Essentially, self-diffusion exploits the spectral bias of neural networks and modulates it through a scheduled noise process. Without relying on pretrained score functions or external denoisers, this approach still remains adaptive to arbitrary forward operators and noisy observations, making it highly flexible and broadly applicable. We demonstrate the effectiveness of our approach on a variety of linear inverse problems, showing that self-diffusion achieves competitive or superior performance compared to other methods.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>K-DAREK: Distance Aware Error for Kurkova Kolmogorov Networks</title>
<link>https://arxiv.org/abs/2510.22021</link>
<guid>https://arxiv.org/abs/2510.22021</guid>
<content:encoded><![CDATA[

arXiv:2510.22021v2 Announce Type: replace 
Abstract: Neural networks are powerful parametric function approximators, while Gaussian processes (GPs) are nonparametric probabilistic models that place distributions over functions via kernel-defined correlations but become computationally expensive for large-scale problems. Kolmogorov-Arnold networks (KANs), semi-parametric neural architectures, model complex functions efficiently using spline layers. Kurkova Kolmogorov-Arnold networks (KKANs) extend KANs by replacing the early spline layers with multi-layer perceptrons that map inputs into higher-dimensional spaces before applying spline-based transformations, which yield more stable training and provide robust architectures for system modeling. By enhancing the KKAN architecture, we develop a novel learning algorithm, distance-aware error for Kurkova-Kolmogorov networks (K-DAREK), for efficient and interpretable function approximation with uncertainty quantification. Our approach establishes robust error bounds that are distance-aware; this means they reflect the proximity of a test point to its nearest training points. In safe control case studies, we demonstrate that K-DAREK is about four times faster and ten times more computationally efficient than Ensemble of KANs, 8.6 times more scalable than GP as data size increases, and 7.2% safer than our previous work distance-aware error for Kolmogorov networks (DAREK). Moreover, on real data (e.g., Real Estate Valuation), K-DAREK's error bound achieves zero coverage violations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FLEX: Continuous Agent Evolution via Forward Learning from Experience</title>
<link>https://arxiv.org/abs/2511.06449</link>
<guid>https://arxiv.org/abs/2511.06449</guid>
<content:encoded><![CDATA[

arXiv:2511.06449v2 Announce Type: replace 
Abstract: Autonomous agents driven by Large Language Models (LLMs) have revolutionized reasoning and problem-solving but remain static after training, unable to grow with experience as intelligent beings do during deployment. We introduce Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that enables LLM agents to continuously evolve through accumulated experience. Specifically, FLEX cultivates scalable and inheritable evolution by constructing a structured experience library through continual reflection on successes and failures during interaction with the environment. FLEX delivers substantial improvements on mathematical reasoning, chemical retrosynthesis, and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14% on ProteinGym). We further identify a clear scaling law of experiential growth and the phenomenon of experience inheritance across agents, marking a step toward scalable and inheritable continuous agent evolution. Project Page: https://flex-gensi-thuair.github.io.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HN-MVTS: HyperNetwork-based Multivariate Time Series Forecasting</title>
<link>https://arxiv.org/abs/2511.08340</link>
<guid>https://arxiv.org/abs/2511.08340</guid>
<content:encoded><![CDATA[

arXiv:2511.08340v2 Announce Type: replace 
Abstract: Accurate forecasting of multivariate time series data remains a formidable challenge, particularly due to the growing complexity of temporal dependencies in real-world scenarios. While neural network-based models have achieved notable success in this domain, complex channel-dependent models often suffer from performance degradation compared to channel-independent models that do not consider the relationship between components but provide high robustness due to small capacity. In this work, we propose HN-MVTS, a novel architecture that integrates a hypernetwork-based generative prior with an arbitrary neural network forecasting model. The input of this hypernetwork is a learnable embedding matrix of time series components. To restrict the number of new parameters, the hypernetwork learns to generate the weights of the last layer of the target forecasting networks, serving as a data-adaptive regularizer that improves generalization and long-range predictive accuracy. The hypernetwork is used only during the training, so it does not increase the inference time compared to the base forecasting model. Extensive experiments on eight benchmark datasets demonstrate that application of HN-MVTS to the state-of-the-art models (DLinear, PatchTST, TSMixer, etc.) typically improves their performance. Our findings suggest that hypernetwork-driven parameterization offers a promising direction for enhancing existing forecasting techniques in complex scenarios.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Fusion-Enhanced Decision Transformer for Stable Cross-Domain Generalization</title>
<link>https://arxiv.org/abs/2511.09173</link>
<guid>https://arxiv.org/abs/2511.09173</guid>
<content:encoded><![CDATA[

arXiv:2511.09173v2 Announce Type: replace 
Abstract: Cross-domain shifts present a significant challenge for decision transformer (DT) policies. Existing cross-domain policy adaptation methods typically rely on a single simple filtering criterion to select source trajectory fragments and stitch them together. They match either state structure or action feasibility. However, the selected fragments still have poor stitchability: state structures can misalign, the return-to-go (RTG) becomes incomparable when the reward or horizon changes, and actions may jump at trajectory junctions. As a result, RTG tokens lose continuity, which compromises DT's inference ability. To tackle these challenges, we propose Data Fusion-Enhanced Decision Transformer (DFDT), a compact pipeline that restores stitchability. Particularly, DFDT fuses scarce target data with selectively trusted source fragments via a two-level data filter, maximum mean discrepancy (MMD) mismatch for state-structure alignment, and optimal transport (OT) deviation for action feasibility. It then trains on a feasibility-weighted fusion distribution. Furthermore, DFDT replaces RTG tokens with advantage-conditioned tokens, which improves the continuity of the semantics in the token sequence. It also applies a $Q$-guided regularizer to suppress junction value and action jumps. Theoretically, we provide bounds that tie state value and policy performance gaps to the MMD-mismatch and OT-deviation measures, and show that the bounds tighten as these two measures shrink. We show that DFDT improves return and stability over strong offline RL and sequence-model baselines across gravity, kinematic, and morphology shifts on D4RL-style control tasks, and further corroborate these gains with token-stitching and sequence-semantics stability analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlowCast: Advancing Precipitation Nowcasting with Conditional Flow Matching</title>
<link>https://arxiv.org/abs/2511.09731</link>
<guid>https://arxiv.org/abs/2511.09731</guid>
<content:encoded><![CDATA[

arXiv:2511.09731v2 Announce Type: replace 
Abstract: Radar-based precipitation nowcasting, the task of forecasting short-term precipitation fields from previous radar images, is a critical problem for flood risk management and decision-making. While deep learning has substantially advanced this field, two challenges remain fundamental: the uncertainty of atmospheric dynamics and the efficient modeling of high-dimensional data. Diffusion models have shown strong promise by producing sharp, reliable forecasts, but their iterative sampling process is computationally prohibitive for time-critical applications. We introduce FlowCast, the first end-to-end probabilistic model leveraging Conditional Flow Matching (CFM) as a direct noise-to-data generative framework for precipitation nowcasting. Unlike hybrid approaches, FlowCast learns a direct noise-to-data mapping in a compressed latent space, enabling rapid, high-fidelity sample generation. Our experiments demonstrate that FlowCast establishes a new state-of-the-art in probabilistic performance while also exceeding deterministic baselines in predictive accuracy. A direct comparison further reveals the CFM objective is both more accurate and significantly more efficient than a diffusion objective on the same architecture, maintaining high performance with significantly fewer sampling steps. This work positions CFM as a powerful and practical alternative for high-dimensional spatiotemporal forecasting.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LAD-BNet: Lag-Aware Dual-Branch Networks for Real-Time Energy Forecasting on Edge Devices</title>
<link>https://arxiv.org/abs/2511.10680</link>
<guid>https://arxiv.org/abs/2511.10680</guid>
<content:encoded><![CDATA[

arXiv:2511.10680v2 Announce Type: replace 
Abstract: Real-time energy forecasting on edge devices represents a major challenge for smart grid optimization and intelligent buildings. We present LAD-BNet (Lag-Aware Dual-Branch Network), an innovative neural architecture optimized for edge inference with Google Coral TPU. Our hybrid approach combines a branch dedicated to explicit exploitation of temporal lags with a Temporal Convolutional Network (TCN) featuring dilated convolutions, enabling simultaneous capture of short and long-term dependencies. Tested on real energy consumption data with 10-minute temporal resolution, LAD-BNet achieves 14.49% MAPE at 1-hour horizon with only 18ms inference time on Edge TPU, representing an 8-12 x acceleration compared to CPU. The multi-scale architecture enables predictions up to 12 hours with controlled performance degradation. Our model demonstrates a 2.39% improvement over LSTM baselines and 3.04% over pure TCN architectures, while maintaining a 180MB memory footprint suitable for embedded device constraints. These results pave the way for industrial applications in real-time energy optimization, demand management, and operational planning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mask the Redundancy: Evolving Masking Representation Learning for Multivariate Time-Series Clustering</title>
<link>https://arxiv.org/abs/2511.17008</link>
<guid>https://arxiv.org/abs/2511.17008</guid>
<content:encoded><![CDATA[

arXiv:2511.17008v2 Announce Type: replace 
Abstract: Multivariate Time-Series (MTS) clustering discovers intrinsic grouping patterns of temporal data samples. Although time-series provide rich discriminative information, they also contain substantial redundancy, such as steady-state machine operation records and zero-output periods of solar power generation. Such redundancy diminishes the attention given to discriminative timestamps in representation learning, thus leading to performance bottlenecks in MTS clustering. Masking has been widely adopted to enhance the MTS representation, where temporal reconstruction tasks are designed to capture critical information from MTS. However, most existing masking strategies appear to be standalone preprocessing steps, isolated from the learning process, which hinders dynamic adaptation to the importance of clustering-critical timestamps. Accordingly, this paper proposes the Evolving-masked MTS Clustering (EMTC) method, whose model architecture comprises Importance-aware Variate-wise Masking (IVM) and Multi-Endogenous Views (MEV) generation modules. IVM adaptively guides the model in learning more discriminative representations for clustering, while the reconstruction and cluster-guided contrastive learning pathways enhance and connect the representation learning to clustering tasks. Extensive experiments on 15 benchmark datasets demonstrate the superiority of EMTC over eight SOTA methods, where the EMTC achieves an average improvement of 4.85% in F1-Score over the strongest baselines.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subgoal Graph-Augmented Planning for LLM-Guided Open-World Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.20993</link>
<guid>https://arxiv.org/abs/2511.20993</guid>
<content:encoded><![CDATA[

arXiv:2511.20993v2 Announce Type: replace 
Abstract: Large language models (LLMs) offer strong high-level planning capabilities for reinforcement learning (RL) by decomposing tasks into subgoals. However, their practical utility is limited by poor planning-execution alignment, which reflects a critical gap between abstract plans and actionable, environment-compatible behaviors. This misalignment arises from two interrelated limitations: (1) LLMs often produce subgoals that are semantically plausible but infeasible or irrelevant in the target environment due to insufficient grounding in environment-specific knowledge, and (2) single-LLM planning conflates generation with self-verification, resulting in overconfident yet unreliable subgoals that frequently fail during execution. To address these challenges, we propose Subgoal Graph-Augmented Actor-Critic-Refiner (SGA-ACR), a framework that integrates an environment-specific subgoal graph and structured entity knowledge with a multi-LLM planning pipeline that explicitly separates generation, critique, and refinement to produce executable and verifiable subgoals. A subgoal tracker further monitors execution progress, provides auxiliary rewards, and adaptively updates the subgoal graph to maintain alignment between plans and actions. Experimental results on 22 diverse tasks in the open-world game "Crafter" demonstrate the effectiveness of our proposed method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Does the Model Say What the Data Says? A Simple Heuristic for Model Data Alignment</title>
<link>https://arxiv.org/abs/2511.21931</link>
<guid>https://arxiv.org/abs/2511.21931</guid>
<content:encoded><![CDATA[

arXiv:2511.21931v2 Announce Type: replace 
Abstract: In this work, we propose a simple and computationally efficient framework for evaluating whether machine learning models align with the structure of the data they learn from; that is, whether the model says what the data says. Unlike existing interpretability methods that focus exclusively on explaining model behavior, our approach establishes a baseline derived directly from the data itself. Drawing inspiration from Rubin's Potential Outcomes Framework, we quantify how strongly each feature separates the two outcome groups in a binary classification task, moving beyond traditional descriptive statistics to estimate each feature's effect on the outcome. By comparing these data-derived feature rankings with model-based explanations, we provide practitioners with an interpretable and model-agnostic method for assessing model-data alignment.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Tucker Convolutional Network for Water Quality Analysis</title>
<link>https://arxiv.org/abs/2512.01465</link>
<guid>https://arxiv.org/abs/2512.01465</guid>
<content:encoded><![CDATA[

arXiv:2512.01465v2 Announce Type: replace 
Abstract: Water quality monitoring is a core component of ecological environmental protection. However, due to sensor failure or other inevitable factors, data missing often exists in long-term monitoring, posing great challenges in water quality analysis. This paper proposes a Neural Tucker Convolutional Network (NTCN) model for water quality data imputation, which features the following key components: a) Encode different mode entities into respective embedding vectors, and construct a Tucker interaction tensor by outer product operations to capture the complex mode-wise feature interactions; b) Use 3D convolution to extract fine-grained spatiotemporal features from the interaction tensor. Experiments on three real-world water quality datasets show that the proposed NTCN model outperforms several state-of-the-art imputation models in terms of accuracy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-time Air Pollution prediction model based on Spatiotemporal Big data</title>
<link>https://arxiv.org/abs/1805.00432</link>
<guid>https://arxiv.org/abs/1805.00432</guid>
<content:encoded><![CDATA[

arXiv:1805.00432v4 Announce Type: replace-cross 
Abstract: Air pollution is one of the most concerns for urban areas. Many countries have constructed monitoring stations to hourly collect pollution values. Recently, there is a research in Daegu city, Korea for real-time air quality monitoring via sensors installed on taxis running across the whole city. The collected data is huge (1-second interval) and in both Spatial and Temporal format. In this paper, based on this spatiotemporal Big data, we propose a real-time air pollution prediction model based on Convolutional Neural Network (CNN) algorithm for image-like Spatial distribution of air pollution. Regarding to Temporal information in the data, we introduce a combination of a Long Short-Term Memory (LSTM) unit for time series data and a Neural Network model for other air pollution impact factors such as weather conditions to build a hybrid prediction model. This model is simple in architecture but still brings good prediction ability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Supply Chain Interdependencies for Stock Return Prediction: A Full-State Graph Convolutional LSTM</title>
<link>https://arxiv.org/abs/2303.09406</link>
<guid>https://arxiv.org/abs/2303.09406</guid>
<content:encoded><![CDATA[

arXiv:2303.09406v2 Announce Type: replace-cross 
Abstract: Stock return prediction is fundamental to financial decision-making, yet traditional time series models fail to capture the complex interdependencies between companies in modern markets. We propose the Full-State Graph Convolutional LSTM (FS-GCLSTM), a novel temporal graph neural network that incorporates value-chain relationships to enhance stock return forecasting. Our approach features two key innovations: First, we represent inter-firm dependencies through value-chain networks, where nodes correspond to companies and edges capture supplier-customer relationships, enabling the model to leverage information beyond historical price data. Second, FS-GCLSTM applies graph convolutions to all LSTM components - current input features, previous hidden states, and cell states - ensuring that spatial information from the value-chain network influences every aspect of the temporal update mechanism. We evaluate FS-GCLSTM on Eurostoxx 600 and S&amp;P 500 datasets using LSEG value-chain data. While not achieving the lowest traditional prediction errors, FS-GCLSTM consistently delivers superior portfolio performance, attaining the highest annualized returns, Sharpe ratios, and Sortino ratios across both markets. Performance gains are more pronounced in the denser Eurostoxx 600 network, and robustness tests confirm stability across different input sequence lengths, demonstrating the practical value of integrating value-chain data with temporal graph neural networks.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhyloLM : Inferring the Phylogeny of Large Language Models and Predicting their Performances in Benchmarks</title>
<link>https://arxiv.org/abs/2404.04671</link>
<guid>https://arxiv.org/abs/2404.04671</guid>
<content:encoded><![CDATA[

arXiv:2404.04671v5 Announce Type: replace-cross 
Abstract: This paper introduces PhyloLM, a method adapting phylogenetic algorithms to Large Language Models (LLMs) to explore whether and how they relate to each other and to predict their performance characteristics. Our method calculates a phylogenetic distance metric based on the similarity of LLMs' output. The resulting metric is then used to construct dendrograms, which satisfactorily capture known relationships across a set of 111 open-source and 45 closed models. Furthermore, our phylogenetic distance predicts performance in standard benchmarks, thus demonstrating its functional validity and paving the way for a time and cost-effective estimation of LLM capabilities. To sum up, by translating population genetic concepts to machine learning, we propose and validate a tool to evaluate LLM development, relationships and capabilities, even in the absence of transparent training information.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSP-GNN: Learning to Track via Bilevel Optimization</title>
<link>https://arxiv.org/abs/2407.04308</link>
<guid>https://arxiv.org/abs/2407.04308</guid>
<content:encoded><![CDATA[

arXiv:2407.04308v3 Announce Type: replace-cross 
Abstract: We propose a graph-based tracking formulation for multi-object tracking (MOT) where target detections contain kinematic information and re-identification features (attributes). Our method applies a successive shortest paths (SSP) algorithm to a tracking graph defined over a batch of frames. The edge costs in this tracking graph are computed via a message-passing network, a graph neural network (GNN) variant. The parameters of the GNN, and hence, the tracker, are learned end-to-end on a training set of example ground-truth tracks and detections. Specifically, learning takes the form of bilevel optimization guided by our novel loss function. We evaluate our algorithm on simulated scenarios to understand its sensitivity to scenario aspects and model hyperparameters. Across varied scenario complexities, our method compares favorably to a strong baseline.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Alpha-VI DeepONet: A prior-robust variational Bayesian approach for enhancing DeepONets with uncertainty quantification</title>
<link>https://arxiv.org/abs/2408.00681</link>
<guid>https://arxiv.org/abs/2408.00681</guid>
<content:encoded><![CDATA[

arXiv:2408.00681v2 Announce Type: replace-cross 
Abstract: We introduce a novel deep operator network (DeepONet) framework that incorporates generalised variational inference (GVI) using R\'enyi's $\alpha$-divergence to learn complex operators while quantifying uncertainty. By incorporating Bayesian neural networks as the building blocks for the branch and trunk networks, our framework endows DeepONet with uncertainty quantification. The use of R\'enyi's $\alpha$-divergence, instead of the Kullback-Leibler divergence (KLD), commonly used in standard variational inference, mitigates issues related to prior misspecification that are prevalent in Variational Bayesian DeepONets. This approach offers enhanced flexibility and robustness. We demonstrate that modifying the variational objective function yields superior results in terms of minimising the mean squared error and improving the negative log-likelihood on the test set. Our framework's efficacy is validated across various mechanical systems, where it outperforms both deterministic and standard KLD-based VI DeepONets in predictive accuracy and uncertainty quantification. The hyperparameter $\alpha$, which controls the degree of robustness, can be tuned to optimise performance for specific problems. We apply this approach to a range of mechanics problems, including gravity pendulum, advection-diffusion, and diffusion-reaction systems. Our findings underscore the potential of $\alpha$-VI DeepONet to advance the field of data-driven operator learning and its applications in engineering and scientific domains.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Handy Appetizer</title>
<link>https://arxiv.org/abs/2409.17120</link>
<guid>https://arxiv.org/abs/2409.17120</guid>
<content:encoded><![CDATA[

arXiv:2409.17120v2 Announce Type: replace-cross 
Abstract: This book explores the role of Artificial Intelligence (AI), Machine Learning (ML), and Deep Learning (DL) in driving the progress of big data analytics and management. The book focuses on simplifying the complex mathematical concepts behind deep learning, offering intuitive visualizations and practical case studies to help readers understand how neural networks and technologies like Convolutional Neural Networks (CNNs) work. It introduces several classic models and technologies such as Transformers, GPT, ResNet, BERT, and YOLO, highlighting their applications in fields like natural language processing, image recognition, and autonomous driving. The book also emphasizes the importance of pre-trained models and how they can enhance model performance and accuracy, with instructions on how to apply these models in various real-world scenarios. Additionally, it provides an overview of key big data management technologies like SQL and NoSQL databases, as well as distributed computing frameworks such as Apache Hadoop and Spark, explaining their importance in managing and processing vast amounts of data. Ultimately, the book underscores the value of mastering deep learning and big data management skills as critical tools for the future workforce, making it an essential resource for both beginners and experienced professionals.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning and Machine Learning, Advancing Big Data Analytics and Management: Unveiling AI's Potential Through Tools, Techniques, and Applications</title>
<link>https://arxiv.org/abs/2410.01268</link>
<guid>https://arxiv.org/abs/2410.01268</guid>
<content:encoded><![CDATA[

arXiv:2410.01268v3 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI), machine learning, and deep learning have become transformative forces in big data analytics and management, enabling groundbreaking advancements across diverse industries. This article delves into the foundational concepts and cutting-edge developments in these fields, with a particular focus on large language models (LLMs) and their role in natural language processing, multimodal reasoning, and autonomous decision-making. Highlighting tools such as ChatGPT, Claude, and Gemini, the discussion explores their applications in data analysis, model design, and optimization.
  The integration of advanced algorithms like neural networks, reinforcement learning, and generative models has enhanced the capabilities of AI systems to process, visualize, and interpret complex datasets. Additionally, the emergence of technologies like edge computing and automated machine learning (AutoML) democratizes access to AI, empowering users across skill levels to engage with intelligent systems. This work also underscores the importance of ethical considerations, transparency, and fairness in the deployment of AI technologies, paving the way for responsible innovation.
  Through practical insights into hardware configurations, software environments, and real-world applications, this article serves as a comprehensive resource for researchers and practitioners. By bridging theoretical underpinnings with actionable strategies, it showcases the potential of AI and LLMs to revolutionize big data management and drive meaningful advancements across domains such as healthcare, finance, and autonomous systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs</title>
<link>https://arxiv.org/abs/2410.02799</link>
<guid>https://arxiv.org/abs/2410.02799</guid>
<content:encoded><![CDATA[

arXiv:2410.02799v2 Announce Type: replace-cross 
Abstract: Kidney exchange programs have substantially increased transplantation rates but also raise critical concerns about fairness in organ allocation. We propose a novel framework leveraging Data Envelopment Analysis (DEA) to evaluate multiple dimensions of fairness-Priority, Access, and Outcome-within a unified model. This approach captures complexities often missed in single-metric analyses. Using data from the United Network for Organ Sharing, we separately quantify fairness across these dimensions: Priority fairness through waitlist durations, Access fairness via the Living Kidney Donor Profile Index (LKDPI) scores, and Outcome fairness based on graft lifespan. We then apply our conditional DEA model with covariate adjustment to demonstrate significant disparities in kidney allocation efficiency across ethnic groups. To quantify uncertainty, we employ conformal prediction within a novel Reference Frontier Mapping (RFM) framework, yielding group-conditional prediction intervals with finite-sample coverage guarantees. Our findings show notable differences in efficiency distributions between ethnic groups. Our study provides a rigorous framework for evaluating fairness in complex resource allocation systems with resource scarcity and mutual compatibility constraints.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Taggants: Dataset Ownership Verification via Harmless Targeted Data Poisoning</title>
<link>https://arxiv.org/abs/2410.09101</link>
<guid>https://arxiv.org/abs/2410.09101</guid>
<content:encoded><![CDATA[

arXiv:2410.09101v2 Announce Type: replace-cross 
Abstract: Dataset ownership verification, the process of determining if a dataset is used in a model's training data, is necessary for detecting unauthorized data usage and data contamination. Existing approaches, such as backdoor watermarking, rely on inducing a detectable behavior into the trained model on a part of the data distribution. However, these approaches have limitations, as they can be harmful to the model's performances or require unpractical access to the model's internals. Most importantly, previous approaches lack guarantee against false positives. This paper introduces data taggants, a novel non-backdoor dataset ownership verification technique. Our method uses pairs of out-of-distribution samples and random labels as secret keys, and leverages clean-label targeted data poisoning to subtly alter a dataset, so that models trained on it respond to the key samples with the corresponding key labels. The keys are built as to allow for statistical certificates with black-box access only to the model. We validate our approach through comprehensive and realistic experiments on ImageNet1k using ViT and ResNet models with state-of-the-art training recipes. Our findings demonstrate that data taggants can reliably detect models trained on the protected dataset with high confidence, without compromising validation accuracy, and show their superiority over backdoor watermarking. We demonstrate the stealthiness and robustness of our method against various defense mechanisms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Can GNNs Learn Link Heuristics? A Concise Review and Evaluation of Link Prediction Methods</title>
<link>https://arxiv.org/abs/2411.14711</link>
<guid>https://arxiv.org/abs/2411.14711</guid>
<content:encoded><![CDATA[

arXiv:2411.14711v2 Announce Type: replace-cross 
Abstract: This paper explores the ability of Graph Neural Networks (GNNs) in learning various forms of information for link prediction, alongside a brief review of existing link prediction methods. Our analysis reveals that GNNs cannot effectively learn structural information related to the number of common neighbors between two nodes, primarily due to the nature of set-based pooling of the neighborhood aggregation scheme. Also, our extensive experiments indicate that trainable node embeddings can improve the performance of GNN-based link prediction models. Importantly, we observe that the denser the graph, the greater such the improvement. We attribute this to the characteristics of node embeddings, where the link state of each link sample could be encoded into the embeddings of nodes that are involved in the neighborhood aggregation of the two nodes in that link sample. In denser graphs, every node could have more opportunities to attend the neighborhood aggregation of other nodes and encode states of more link samples to its embedding, thus learning better node embeddings for link prediction. Lastly, we demonstrate that the insights gained from our research carry important implications in identifying the limitations of existing link prediction methods, which could guide the future development of more robust algorithms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-ICE: An Inner Interpretable Framework for Image Classification via Bi-directional Interactions between Concept and Input Embeddings</title>
<link>https://arxiv.org/abs/2411.18645</link>
<guid>https://arxiv.org/abs/2411.18645</guid>
<content:encoded><![CDATA[

arXiv:2411.18645v2 Announce Type: replace-cross 
Abstract: Inner interpretability is a promising field aiming to uncover the internal mechanisms of AI systems through scalable, automated methods. While significant research has been conducted on large language models, limited attention has been paid to applying inner interpretability to large-scale image tasks, focusing primarily on architectural and functional levels to visualize learned concepts. In this paper, we first present a conceptual framework that supports inner interpretability and multilevel analysis for large-scale image classification tasks. Specifically, we introduce the Bi-directional Interaction between Concept and Input Embeddings (Bi-ICE) module, which facilitates interpretability across the computational, algorithmic, and implementation levels. This module enhances transparency by generating predictions based on human-understandable concepts, quantifying their contributions, and localizing them within the inputs. Finally, we showcase enhanced transparency in image classification, measuring concept contributions, and pinpointing their locations within the inputs. Our approach highlights algorithmic interpretability by demonstrating the process of concept learning and its convergence.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning Neutrino-Nucleus Cross Sections</title>
<link>https://arxiv.org/abs/2412.16303</link>
<guid>https://arxiv.org/abs/2412.16303</guid>
<content:encoded><![CDATA[

arXiv:2412.16303v3 Announce Type: replace-cross 
Abstract: Neutrino-nucleus scattering cross sections are critical theoretical inputs for long-baseline neutrino oscillation experiments. However, robust modeling of these cross sections remains challenging. For a simple but physically motivated toy model of the DUNE experiment, we demonstrate that an accurate neural-network model of the cross section -- leveraging only Standard-Model symmetries -- can be learned from near-detector data. We perform a neutrino oscillation analysis with simulated far-detector events, finding that oscillation analysis results enabled by our data-driven cross-section model approach the theoretical limit achievable with perfect prior knowledge of the cross section. We further quantify the effects of flux shape and detector resolution uncertainties as well as systematics from cross-section mismodeling. This proof-of-principle study highlights the potential of future neutrino near-detector datasets and data-driven cross-section models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to explain grokking</title>
<link>https://arxiv.org/abs/2412.18624</link>
<guid>https://arxiv.org/abs/2412.18624</guid>
<content:encoded><![CDATA[

arXiv:2412.18624v4 Announce Type: replace-cross 
Abstract: Explanation of grokking (delayed generalization) in learning is given by modeling grokking by the stochastic gradient Langevin dynamics (Brownian motion) and applying the ideas of thermodynamics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Particle Algorithm for Mean-Field Variational Inference</title>
<link>https://arxiv.org/abs/2412.20385</link>
<guid>https://arxiv.org/abs/2412.20385</guid>
<content:encoded><![CDATA[

arXiv:2412.20385v3 Announce Type: replace-cross 
Abstract: Variational inference is a fast and scalable alternative to Markov chain Monte Carlo and has been widely applied to posterior inference tasks in statistics and machine learning. A traditional approach for implementing mean-field variational inference (MFVI) is coordinate ascent variational inference (CAVI), which relies crucially on parametric assumptions on complete conditionals. We introduce a novel particle-based algorithm for MFVI, named PArticle VI (PAVI), for nonparametric mean-field approximation. We obtain non-asymptotic error bounds for our algorithm. To our knowledge, this is the first end-to-end guarantee for particle-based MFVI.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiTAR: Diffusion Transformer Autoregressive Modeling for Speech Generation</title>
<link>https://arxiv.org/abs/2502.03930</link>
<guid>https://arxiv.org/abs/2502.03930</guid>
<content:encoded><![CDATA[

arXiv:2502.03930v4 Announce Type: replace-cross 
Abstract: Several recent studies have attempted to autoregressively generate continuous speech representations without discrete speech tokens by combining diffusion and autoregressive models, yet they often face challenges with excessive computational loads or suboptimal outcomes. In this work, we propose Diffusion Transformer Autoregressive Modeling (DiTAR), a patch-based autoregressive framework combining a language model with a diffusion transformer. This approach significantly enhances the efficacy of autoregressive models for continuous tokens and reduces computational demands. DiTAR utilizes a divide-and-conquer strategy for patch generation, where the language model processes aggregated patch embeddings and the diffusion transformer subsequently generates the next patch based on the output of the language model. For inference, we propose defining temperature as the time point of introducing noise during the reverse diffusion ODE to balance diversity and determinism. We also show in the extensive scaling analysis that DiTAR has superb scalability. In zero-shot speech generation, DiTAR achieves state-of-the-art performance in robustness, speaker similarity, and naturalness.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Singular: Revealing the Value of Multiple Generations in Benchmark Evaluation</title>
<link>https://arxiv.org/abs/2502.08943</link>
<guid>https://arxiv.org/abs/2502.08943</guid>
<content:encoded><![CDATA[

arXiv:2502.08943v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated significant utility in real-world applications, exhibiting impressive capabilities in natural language processing and understanding. Benchmark evaluations are crucial for assessing the capabilities of LLMs as they can provide a comprehensive assessment of their strengths and weaknesses. However, current evaluation methods often overlook the inherent randomness of LLMs by employing deterministic generation strategies or relying on a single random sample, resulting in unaccounted sampling variance and unreliable benchmark score estimates. In this paper, we propose a hierarchical statistical model that provides a more comprehensive representation of the benchmarking process by incorporating both benchmark characteristics and LLM randomness. We show that leveraging multiple generations improves the accuracy of estimating the benchmark score and reduces variance. Multiple generations also allow us to define $\mathbb P\left(\text{correct}\right)$, a prompt-level difficulty score based on correct ratios, providing fine-grained insights into individual prompts. Additionally, we create a data map that visualizes difficulty and semantics of prompts, enabling error detection and quality control in benchmark construction.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graceful forgetting: Memory as a process</title>
<link>https://arxiv.org/abs/2502.11105</link>
<guid>https://arxiv.org/abs/2502.11105</guid>
<content:encoded><![CDATA[

arXiv:2502.11105v4 Announce Type: replace-cross 
Abstract: A rational framework is proposed to explain how we accommodate unbounded sensory input within bounded memory. Memory is stored as statistics organized into structures that are repeatedly summarized and compressed to make room for new input. Repeated summarization requires an intensive ongoing process guided by heuristics that help optimize the memory for future needs. Sensory input is rapidly encoded as simple statistics that are progressively elaborated into more abstract constructs. This framework differs from previous accounts of memory by reliance on statistics as a representation of memory, the use of heuristics to guide the choice of statistics at each summarization step, and the hypothesis of a process that is complex and expensive. The framework is intended as an aid to make sense of our extensive knowledge of memory, and bring us closer to an understanding of memory in functional and mechanistic terms.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>T1-PILOT: Optimized Trajectories for T1 Mapping Acceleration</title>
<link>https://arxiv.org/abs/2502.20333</link>
<guid>https://arxiv.org/abs/2502.20333</guid>
<content:encoded><![CDATA[

arXiv:2502.20333v2 Announce Type: replace-cross 
Abstract: Cardiac T1 mapping provides critical quantitative insights into myocardial tissue composition, enabling the assessment of pathologies such as fibrosis, inflammation, and edema. However, the inherently dynamic nature of the heart imposes strict limits on acquisition times, making high-resolution T1 mapping a persistent challenge. Compressed sensing (CS) approaches have reduced scan durations by undersampling k-space and reconstructing images from partial data, and recent studies show that jointly optimizing the undersampling patterns with the reconstruction network can substantially improve performance. Still, most current T1 mapping pipelines rely on static, hand-crafted masks that do not exploit the full acceleration and accuracy potential. In this work, we introduce T1-PILOT: an end-to-end method that explicitly incorporates the T1 signal relaxation model into the sampling-reconstruction framework to guide the learning of non-Cartesian trajectories, crossframe alignment, and T1 decay estimation. Through extensive experiments on the CMRxRecon dataset, T1-PILOT significantly outperforms several baseline strategies (including learned single-mask and fixed radial or golden-angle sampling schemes), achieving higher T1 map fidelity at greater acceleration factors. In particular, we observe consistent gains in PSNR and VIF relative to existing methods, along with marked improvements in delineating finer myocardial structures. Our results highlight that optimizing sampling trajectories in tandem with the physical relaxation model leads to both enhanced quantitative accuracy and reduced acquisition times.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid Normalization</title>
<link>https://arxiv.org/abs/2503.04598</link>
<guid>https://arxiv.org/abs/2503.04598</guid>
<content:encoded><![CDATA[

arXiv:2503.04598v4 Announce Type: replace-cross 
Abstract: Transformers have become the de facto architecture for a wide range of machine learning tasks, particularly in large language models (LLMs). Despite their remarkable performance, many challenges remain in training deep transformer networks, especially regarding the position of the layer normalization. While Pre-Norm structures facilitate more stable training owing to their stronger identity path, they often lead to suboptimal performance compared to Post-Norm. In this paper, we propose $\textbf{HybridNorm}$, a simple yet effective hybrid normalization strategy that integrates the advantages of both Pre-Norm and Post-Norm. Specifically, HybridNorm employs QKV normalization within the attention mechanism and Post-Norm in the feed-forward network (FFN) of each transformer block. We provide both theoretical insights and empirical evidence to demonstrate that HybridNorm improves the gradient flow and the model robustness. Extensive experiments on large-scale transformer models, including both dense and sparse variants, show that HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches across multiple benchmarks. These findings highlight the potential of HybridNorm as a more stable and effective technique for improving the training and performance of deep transformer models. Code is available at https://github.com/BryceZhuo/HybridNorm.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Thermodynamic bounds on energy use in quasi-static Deep Neural Networks</title>
<link>https://arxiv.org/abs/2503.09980</link>
<guid>https://arxiv.org/abs/2503.09980</guid>
<content:encoded><![CDATA[

arXiv:2503.09980v3 Announce Type: replace-cross 
Abstract: The rapid growth of deep neural networks (DNNs) has brought increasing attention to their energy use during training and inference. Here, we establish the thermodynamic bounds on energy consumption in quasi-static analog DNNs by mapping modern feedforward architectures onto a physical free-energy functional. This framework provides a direct statistical-mechanical interpretation of quasi-static DNNs. As a result, inference can proceed in a thermodynamically reversible manner, with vanishing minimal energy cost, in contrast to the Landauer limit that constrains digital hardware. Importantly, inference corresponds to relaxation to a unique free-energy minimum with F_{\min}=0, allowing all constraints to be satisfied without residual stress. By comparison, training overconstrains the system: simultaneous clamping of inputs and outputs generates stresses that propagate backward through the architecture, reproducing the rules of backpropagation. Parameter annealing then relaxes these stresses, providing a purely physical route to learning without an explicit loss function. We further derive a universal lower bound on training energy, E< 2NDkT, which scales with both the number of trainable parameters and the dataset size.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024)</title>
<link>https://arxiv.org/abs/2503.14937</link>
<guid>https://arxiv.org/abs/2503.14937</guid>
<content:encoded><![CDATA[

arXiv:2503.14937v2 Announce Type: replace-cross 
Abstract: Proceedings of the 3rd Italian Conference on Big Data and Data Science (ITADATA2024), held in Pisa, Italy, September 17-19, 2024.
  The Italian Conference on Big Data and Data Science (ITADATA2024) is the annual event supported by the CINI Big Data National Laboratory and ISTI CNR that aims to put together Italian researchers and professionals from academia, industry, government, and public administration working in the field of big data and data science, as well as related fields (e.g., security and privacy, HPC, Cloud).
  ITADATA2024 covered research on all theoretical and practical aspects of Big Data and data science including data governance, data processing, data analysis, data reporting, data protection, as well as experimental studies and lessons learned. In particular, ITADATA2024 focused on
  - Data spaces
  - Data processing life cycle
  - Machine learning and Large Language Models
  - Applications of big data and data science in healthcare, finance, industry 5.0, and beyond
  - Data science for social network analysis
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhanced Spatiotemporal Consistency for Image-to-LiDAR Data Pretraining</title>
<link>https://arxiv.org/abs/2503.19912</link>
<guid>https://arxiv.org/abs/2503.19912</guid>
<content:encoded><![CDATA[

arXiv:2503.19912v2 Announce Type: replace-cross 
Abstract: LiDAR representation learning has emerged as a promising approach to reducing reliance on costly and labor-intensive human annotations. While existing methods primarily focus on spatial alignment between LiDAR and camera sensors, they often overlook the temporal dynamics critical for capturing motion and scene continuity in driving scenarios. To address this limitation, we propose SuperFlow++, a novel framework that integrates spatiotemporal cues in both pretraining and downstream tasks using consecutive LiDAR-camera pairs. SuperFlow++ introduces four key components: (1) a view consistency alignment module to unify semantic information across camera views, (2) a dense-to-sparse consistency regularization mechanism to enhance feature robustness across varying point cloud densities, (3) a flow-based contrastive learning approach that models temporal relationships for improved scene understanding, and (4) a temporal voting strategy that propagates semantic information across LiDAR scans to improve prediction consistency. Extensive evaluations on 11 heterogeneous LiDAR datasets demonstrate that SuperFlow++ outperforms state-of-the-art methods across diverse tasks and driving conditions. Furthermore, by scaling both 2D and 3D backbones during pretraining, we uncover emergent properties that provide deeper insights into developing scalable 3D foundation models. With strong generalizability and computational efficiency, SuperFlow++ establishes a new benchmark for data-efficient LiDAR-based perception in autonomous driving. The code is publicly available at https://github.com/Xiangxu-0103/SuperFlow
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation</title>
<link>https://arxiv.org/abs/2503.22370</link>
<guid>https://arxiv.org/abs/2503.22370</guid>
<content:encoded><![CDATA[

arXiv:2503.22370v5 Announce Type: replace-cross 
Abstract: We introduce the sequential multi-object robotic grasp sampling algorithm SeqGrasp that can robustly synthesize stable grasps on diverse objects using the robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp to construct the large-scale Allegro Hand sequential grasping dataset SeqDataset and use it for training the diffusion-based sequential grasp generator SeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against the state-of-the-art non-sequential multi-object grasp generation method MultiGrasp in simulation and on a real robot. The experimental results demonstrate that SeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate than MultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster at generating grasps than SeqGrasp and MultiGrasp. Project page: https://yulihn.github.io/SeqGrasp/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Enhanced Ensemble Filters</title>
<link>https://arxiv.org/abs/2504.17836</link>
<guid>https://arxiv.org/abs/2504.17836</guid>
<content:encoded><![CDATA[

arXiv:2504.17836v3 Announce Type: replace-cross 
Abstract: The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state-observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. Here this shortcoming is addressed by using machine learning to map the joint predicted state and observation to the updated state estimate. The derivation of methods from a mean field formulation of the true filtering distribution suggests a single parametrization of the algorithm that can be deployed at different ensemble sizes. And we use a mean field formulation of the ensemble Kalman filter as an inductive bias for our architecture.
  To develop this perspective, in which the mean-field limit of the algorithm and finite interacting ensemble particle approximations share a common set of parameters, a novel form of neural operator is introduced, taking probability distributions as input: a measure neural mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-field limit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root-mean-square-error performance relative to leading methods in filtering the Lorenz '96 and Kuramoto-Sivashinsky models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Machine Learning for Multivariate Angular Simulation</title>
<link>https://arxiv.org/abs/2504.21505</link>
<guid>https://arxiv.org/abs/2504.21505</guid>
<content:encoded><![CDATA[

arXiv:2504.21505v2 Announce Type: replace-cross 
Abstract: With the recent development of new geometric and angular-radial frameworks for multivariate extremes, reliably simulating from angular variables in moderate-to-high dimensions is of increasing importance. Empirical approaches have the benefit of simplicity, and work reasonably well in low dimensions, but as the number of variables increases, they can lack the required flexibility and scalability. Classical parametric models for angular variables, such as the von Mises--Fisher distribution (vMF), provide an alternative. Exploiting finite mixtures of vMF distributions increases their flexibility, but there are cases where, without letting the number of mixture components grow considerably, a mixture model with a fixed number of components is not sufficient to capture the intricate features that can arise in data. Owing to their flexibility, generative deep learning methods are able to capture complex data structures; they therefore have the potential to be useful in the simulation of multivariate angular variables. In this paper, we introduce a range of deep learning approaches for this task, including generative adversarial networks, normalizing flows and flow matching. We assess their performance via a range of metrics, and make comparisons to the more classical approach of using a finite mixture of vMF distributions. The methods are also applied to a metocean data set, with diagnostics indicating strong performance, demonstrating the applicability of such techniques to real-world, complex data structures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter estimation for land-surface models using Neural Physics</title>
<link>https://arxiv.org/abs/2505.02979</link>
<guid>https://arxiv.org/abs/2505.02979</guid>
<content:encoded><![CDATA[

arXiv:2505.02979v2 Announce Type: replace-cross 
Abstract: The Neural Physics approach is used to determine the parameters of a simple land-surface model using PyTorch's backpropagation engine to carry out the optimisation. In order to test the inverse model, a synthetic dataset is created by running the model in forward mode with known parameter values to create soil temperature time series that can be used as observations for the inverse model. We show that it is not possible to obtain a reliable parameter estimation using a time series of soil temperature observed at a single depth. Using measurements at two depths, reliable parameter estimates can be obtained although it is not possible to differentiate between latent and sensible heat fluxes. We apply the inverse model to urban flux tower data in Phoenix, United States, and show that the thermal conductivity, volumetric heat capacity and the combined sensible-latent heat transfer coefficient can be reliably estimated using an observed value for the effective surface albedo. The resulting model accurately predicts the outgoing longwave radiation, conductive soil fluxes and the combined sensible-latent heat fluxes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Feature Space of a Qubit Coupled to an Arbitrary Bath</title>
<link>https://arxiv.org/abs/2505.03397</link>
<guid>https://arxiv.org/abs/2505.03397</guid>
<content:encoded><![CDATA[

arXiv:2505.03397v4 Announce Type: replace-cross 
Abstract: Qubit control protocols have traditionally leveraged a characterisation of the qubit-bath coupling via its power spectral density. Previous work proposed the inference of noise operators that characterise the influence of a classical bath using a grey-box approach that combines deep neural networks with physics-encoded layers. This overall structure is complex and poses challenges in scaling and real-time operations. Here, we show that no expensive neural networks are needed and that this noise operator description admits an efficient parameterisation. We refer to the resulting parameter space as the \textit{quantum feature space} of the qubit dynamics resulting from the coupled bath. We show that the Euclidean distance defined over the quantum feature space provides an effective method for classifying noise processes in the presence of a given set of controls. Using the quantum feature space as the input space for a simple machine learning algorithm (random forest, in this case), we demonstrate that it can effectively classify the stationarity and the broad class of noise processes perturbing a qubit. Finally, we explore how control pulse parameters map to the quantum feature space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations</title>
<link>https://arxiv.org/abs/2505.06774</link>
<guid>https://arxiv.org/abs/2505.06774</guid>
<content:encoded><![CDATA[

arXiv:2505.06774v2 Announce Type: replace-cross 
Abstract: In this paper, we present a framework for modeling quantum recurrent neural networks (RNNs) and their enhanced version, long short-term memory (LSTM) networks using the core ideas presented by Linden et al. (2009), where the entangling and disentangling power of unitary transformations is investigated. In particular, we interpret entangling and disentangling power as information retention and forgetting mechanisms in LSTMs. Thus, entanglement emerges as a key component of the optimization (training) process. We believe that, by leveraging prior knowledge of the entangling power of unitaries, the proposed quantum-classical framework can guide the design of better-parameterized quantum circuits for various real-world applications.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating the robustness of adversarial defenses in malware detection systems</title>
<link>https://arxiv.org/abs/2505.09342</link>
<guid>https://arxiv.org/abs/2505.09342</guid>
<content:encoded><![CDATA[

arXiv:2505.09342v2 Announce Type: replace-cross 
Abstract: Machine learning is a key tool for Android malware detection, effectively identifying malicious patterns in apps. However, ML-based detectors are vulnerable to evasion attacks, where small, crafted changes bypass detection. Despite progress in adversarial defenses, the lack of comprehensive evaluation frameworks in binary-constrained domains limits understanding of their robustness. We introduce two key contributions. First, Prioritized Binary Rounding, a technique to convert continuous perturbations into binary feature spaces while preserving high attack success and low perturbation size. Second, the sigma-binary attack, a novel adversarial method for binary domains, designed to achieve attack goals with minimal feature changes. Experiments on the Malscan dataset show that sigma-binary outperforms existing attacks and exposes key vulnerabilities in state-of-the-art defenses. Defenses equipped with adversary detectors, such as KDE, DLA, DNN+, and ICNN, exhibit significant brittleness, with attack success rates exceeding 90% using fewer than 10 feature modifications and reaching 100% with just 20. Adversarially trained defenses, including AT-rFGSM-k, AT-MaxMA, improves robustness under small budgets but remains vulnerable to unrestricted perturbations, with attack success rates of 99.45% and 96.62%, respectively. Although PAD-SMA demonstrates strong robustness against state-of-the-art gradient-based adversarial attacks by maintaining an attack success rate below 16.55%, the sigma-binary attack significantly outperforms these methods, achieving a 94.56% success rate under unrestricted perturbations. These findings highlight the critical need for precise method like sigma-binary to expose hidden vulnerabilities in existing defenses and support the development of more resilient malware detection systems.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Is PRM Necessary? Problem-Solving RL Implicitly Induces PRM Capability in LLMs</title>
<link>https://arxiv.org/abs/2505.11227</link>
<guid>https://arxiv.org/abs/2505.11227</guid>
<content:encoded><![CDATA[

arXiv:2505.11227v2 Announce Type: replace-cross 
Abstract: The development of reasoning capabilities represents a critical frontier in large language models (LLMs) research, where reinforcement learning (RL) and process reward models (PRMs) have emerged as predominant methodological frameworks. Contrary to conventional wisdom, empirical evidence from DeepSeek-R1 demonstrates that pure RL training focused on mathematical problem-solving can progressively enhance reasoning abilities without PRM integration, challenging the perceived necessity of process supervision. In this study, we conduct a systematic investigation of the relationship between RL training and PRM capabilities. Our findings demonstrate that problem-solving proficiency and process supervision capabilities represent complementary dimensions of reasoning that co-evolve synergistically during pure RL training. In particular, current PRMs underperform simple baselines like majority voting when applied to state-of-the-art models such as DeepSeek-R1 and QwQ-32B. To address this limitation, we propose Self-PRM, an introspective framework in which models autonomously evaluate and rerank their generated solutions through self-reward mechanisms. Although Self-PRM consistently improves the accuracy of the benchmark (particularly with larger sample sizes), analysis exposes persistent challenges: The approach exhibits low precision (<10\%) on difficult problems, frequently misclassifying flawed solutions as valid. These analyses underscore the need for continued RL scaling to improve reward alignment and introspective accuracy. Overall, our findings suggest that PRM may not be essential for enhancing complex reasoning, as pure RL not only improves problem-solving skills but also inherently fosters robust PRM capabilities. We hope these findings provide actionable insights for building more reliable and self-aware complex reasoning models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mind The Gap: Quantifying Mechanistic Gaps in Algorithmic Reasoning via Neural Compilation</title>
<link>https://arxiv.org/abs/2505.18623</link>
<guid>https://arxiv.org/abs/2505.18623</guid>
<content:encoded><![CDATA[

arXiv:2505.18623v2 Announce Type: replace-cross 
Abstract: This paper aims to understand how neural networks learn algorithmic reasoning by addressing two questions: How faithful are learned algorithms when they are effective, and why do neural networks fail to learn effective algorithms otherwise? To answer these questions, we use neural compilation, a technique that directly encodes a source algorithm into neural network parameters, enabling the network to compute the algorithm exactly. This enables comparison between compiled and conventionally learned parameters, intermediate vectors, and behaviors. This investigation is crucial for developing neural networks that robustly learn complexalgorithms from data. Our analysis focuses on graph neural networks (GNNs), which are naturally aligned with algorithmic reasoning tasks, specifically our choices of BFS, DFS, and Bellman-Ford, which cover the spectrum of effective, faithful, and ineffective learned algorithms. Commonly, learning algorithmic reasoning is framed as induction over synthetic data, where a parameterized model is trained on inputs, traces, and outputs produced by an underlying ground truth algorithm. In contrast, we introduce a neural compilation method for GNNs, which sets network parameters analytically, bypassing training. Focusing on GNNs leverages their alignment with algorithmic reasoning, extensive algorithmic induction literature, and the novel application of neural compilation to GNNs. Overall, this paper aims to characterize expressability-trainability gaps - a fundamental shortcoming in learning algorithmic reasoning. We hypothesize that inductive learning is most effective for parallel algorithms contained within the computational class \texttt{NC}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large Language Models Miss the Multi-Agent Mark</title>
<link>https://arxiv.org/abs/2505.21298</link>
<guid>https://arxiv.org/abs/2505.21298</guid>
<content:encoded><![CDATA[

arXiv:2505.21298v4 Announce Type: replace-cross 
Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images</title>
<link>https://arxiv.org/abs/2506.06389</link>
<guid>https://arxiv.org/abs/2506.06389</guid>
<content:encoded><![CDATA[

arXiv:2506.06389v2 Announce Type: replace-cross 
Abstract: Deep learning models have shown remarkable success in dermatological image analysis, offering potential for automated skin disease diagnosis. Previously, convolutional neural network(CNN) based architectures have achieved immense popularity and success in computer vision (CV) based task like skin image recognition, generation and video analysis. But with the emergence of transformer based models, CV tasks are now are nowadays carrying out using these models. Vision Transformers (ViTs) is such a transformer-based models that have shown success in computer vision. It uses self-attention mechanisms to achieve state-of-the-art performance across various tasks. However, their reliance on global attention mechanisms makes them susceptible to adversarial perturbations. This paper aims to investigate the susceptibility of ViTs for medical images to adversarial watermarking-a method that adds so-called imperceptible perturbations in order to fool models. By generating adversarial watermarks through Projected Gradient Descent (PGD), we examine the transferability of such attacks to CNNs and analyze the performance defense mechanism -- adversarial training. Results indicate that while performance is not compromised for clean images, ViTs certainly become much more vulnerable to adversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless, adversarial training raises it up to 90.0%.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Radial Attention: $O(n\log n)$ Sparse Attention with Energy Decay for Long Video Generation</title>
<link>https://arxiv.org/abs/2506.19852</link>
<guid>https://arxiv.org/abs/2506.19852</guid>
<content:encoded><![CDATA[

arXiv:2506.19852v2 Announce Type: replace-cross 
Abstract: Recent advances in diffusion models have enabled high-quality video generation, but the additional temporal dimension significantly increases computational costs, making training and inference on long videos prohibitively expensive. In this paper, we identify a phenomenon we term Spatiotemporal Energy Decay in video diffusion models: post-softmax attention scores diminish as spatial and temporal distance between tokens increase, akin to the physical decay of signal or waves over space and time in nature. Motivated by this, we propose Radial Attention, a scalable sparse attention mechanism with $\mathcal{O}(n \log n)$ complexity that translates energy decay into exponentially decaying compute density, which is significantly more efficient than standard $\mathcal{O}(n^2)$ dense attention and more expressive than linear attention. Specifically, Radial Attention employs a simple, static attention mask where each token attends to spatially nearby tokens, with the attention window size shrinking with temporal distance. Moreover, it allows pre-trained video diffusion models to extend their generation length with efficient LoRA-based fine-tuning. Extensive experiments show that Radial Attention maintains video quality across Wan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9$\times$ speedup over the original dense attention. With minimal tuning, it enables video generation up to 4$\times$ longer while reducing training costs by up to 4.4$\times$ compared to direct fine-tuning and accelerating inference by up to 3.7$\times$ compared to dense attention inference. Code is released at \href{https://github.com/mit-han-lab/radial-attention}{https://github.com/mit-han-lab/radial-attention}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Not to Detect Prompt Injections with an LLM</title>
<link>https://arxiv.org/abs/2507.05630</link>
<guid>https://arxiv.org/abs/2507.05630</guid>
<content:encoded><![CDATA[

arXiv:2507.05630v3 Announce Type: replace-cross 
Abstract: LLM-integrated applications and agents are vulnerable to prompt injection attacks, where adversaries embed malicious instructions within seemingly benign input data to manipulate the LLM's intended behavior. Recent defenses based on known-answer detection (KAD) scheme have reported near-perfect performance by observing an LLM's output to classify input data as clean or contaminated. KAD attempts to repurpose the very susceptibility to prompt injection as a defensive mechanism. We formally characterize the KAD scheme and uncover a structural vulnerability that invalidates its core security premise. To exploit this fundamental vulnerability, we methodically design an adaptive attack, DataFlip. It consistently evades KAD defenses, achieving detection rates as low as $0\%$ while reliably inducing malicious behavior with a success rate of $91\%$, all without requiring white-box access to the LLM or any optimization procedures.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fredholm Neural Networks for forward and inverse problems in elliptic PDEs</title>
<link>https://arxiv.org/abs/2507.06038</link>
<guid>https://arxiv.org/abs/2507.06038</guid>
<content:encoded><![CDATA[

arXiv:2507.06038v3 Announce Type: replace-cross 
Abstract: Building on our previous work introducing Fredholm Neural Networks (Fredholm NNs/ FNNs) for solving integral equations, we extend the framework to tackle forward and inverse problems for linear and semi-linear elliptic partial differential equations. The proposed scheme consists of a deep neural network (DNN) which is designed to represent the iterative process of fixed-point iterations for the solution of elliptic PDEs using the boundary integral method within the framework of potential theory. The number of layers, weights, biases and hyperparameters are computed in an explainable manner based on the iterative scheme, and we therefore refer to this as the Potential Fredholm Neural Network (PFNN). We show that this approach ensures both accuracy and explainability, achieving small errors in the interior of the domain, and near machine-precision on the boundary. We provide a constructive proof for the consistency of the scheme and provide explicit error bounds for both the interior and boundary of the domain, reflected in the layers of the PFNN. These error bounds depend on the approximation of the boundary function and the integral discretization scheme, both of which directly correspond to components of the Fredholm NN architecture. In this way, we provide an explainable scheme that explicitly respects the boundary conditions. We assess the performance of the proposed scheme for the solution of both the forward and inverse problem for linear and semi-linear elliptic PDEs in two dimensions.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Probabilistic Approximate Optimization Algorithm</title>
<link>https://arxiv.org/abs/2507.07420</link>
<guid>https://arxiv.org/abs/2507.07420</guid>
<content:encoded><![CDATA[

arXiv:2507.07420v3 Announce Type: replace-cross 
Abstract: We introduce a generalized \textit{Probabilistic Approximate Optimization Algorithm (PAOA)}, a classical variational Monte Carlo framework that extends and formalizes prior work by Weitz \textit{et al.}~\cite{Combes_2023}, enabling parameterized and fast sampling on present-day Ising machines and probabilistic computers. PAOA operates by iteratively modifying the couplings of a network of binary stochastic units, guided by cost evaluations from independent samples. We establish a direct correspondence between derivative-free updates and the gradient of the full Markov flow over the exponentially large state space, showing that PAOA admits a principled variational formulation. Simulated annealing emerges as a limiting case under constrained parameterizations, and we implement this regime on an FPGA-based probabilistic computer with on-chip annealing to solve large 3D spin-glass problems. Benchmarking PAOA against QAOA on the canonical 26-spin Sherrington-Kirkpatrick model with matched parameters reveals superior performance for PAOA. We show that PAOA naturally extends simulated annealing by optimizing multiple temperature profiles, leading to improved performance over SA on heavy-tailed problems such as SK-L\'evy.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stochastic Approximation with Block Coordinate Optimal Stepsizes</title>
<link>https://arxiv.org/abs/2507.08963</link>
<guid>https://arxiv.org/abs/2507.08963</guid>
<content:encoded><![CDATA[

arXiv:2507.08963v2 Announce Type: replace-cross 
Abstract: We consider stochastic approximation with block-coordinate stepsizes and propose adaptive stepsize rules that aim to minimize the expected distance from the next iterate to an (unknown) target point. These stepsize rules employ online estimates of the second moment of the search direction along each block coordinate. The popular Adam algorithm can be interpreted as a variant with a specific estimator. By leveraging a simple conditional estimator, we derive a new method that obtains competitive performance against Adam but requires less memory and fewer hyper-parameters. We prove that this family of methods converges almost surely to a small neighborhood of the target point, and the radius of the neighborhood depends on the bias and variance of the second-moment estimator. Our analysis relies on a simple aiming condition that assumes neither convexity nor smoothness, thus has broad applicability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Chain of Thought Monitorability: A New and Fragile Opportunity for AI Safety</title>
<link>https://arxiv.org/abs/2507.11473</link>
<guid>https://arxiv.org/abs/2507.11473</guid>
<content:encoded><![CDATA[

arXiv:2507.11473v2 Announce Type: replace-cross 
Abstract: AI systems that "think" in human language offer a unique opportunity for AI safety: we can monitor their chains of thought (CoT) for the intent to misbehave. Like all other known AI oversight methods, CoT monitoring is imperfect and allows some misbehavior to go unnoticed. Nevertheless, it shows promise and we recommend further research into CoT monitorability and investment in CoT monitoring alongside existing safety methods. Because CoT monitorability may be fragile, we recommend that frontier model developers consider the impact of development decisions on CoT monitorability.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iwin Transformer: Hierarchical Vision Transformer using Interleaved Windows</title>
<link>https://arxiv.org/abs/2507.18405</link>
<guid>https://arxiv.org/abs/2507.18405</guid>
<content:encoded><![CDATA[

arXiv:2507.18405v2 Announce Type: replace-cross 
Abstract: We introduce Iwin Transformer, a novel position-embedding-free hierarchical vision transformer, which can be fine-tuned directly from low to high resolution, through the collaboration of innovative interleaved window attention and depthwise separable convolution. This approach uses attention to connect distant tokens and applies convolution to link neighboring tokens, enabling global information exchange within a single module, overcoming Swin Transformer's limitation of requiring two consecutive blocks to approximate global attention. Extensive experiments on visual benchmarks demonstrate that Iwin Transformer exhibits strong competitiveness in tasks such as image classification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and video action recognition. We also validate the effectiveness of the core component in Iwin as a standalone module that can seamlessly replace the self-attention module in class-conditional image generation. The concepts and methods introduced by the Iwin Transformer have the potential to inspire future research, like Iwin 3D Attention in video generation. The code and models are available at https://github.com/cominder/Iwin-Transformer.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DRWKV: Focusing on Object Edges for Low-Light Image Enhancement</title>
<link>https://arxiv.org/abs/2507.18594</link>
<guid>https://arxiv.org/abs/2507.18594</guid>
<content:encoded><![CDATA[

arXiv:2507.18594v3 Announce Type: replace-cross 
Abstract: Low-light image enhancement remains a challenging task, particularly in preserving object edge continuity and fine structural details under extreme illumination degradation. In this paper, we propose a novel model, DRWKV (Detailed Receptance Weighted Key Value), which integrates our proposed Global Edge Retinex (GER) theory, enabling effective decoupling of illumination and edge structures for enhanced edge fidelity. Secondly, we introduce Evolving WKV Attention, a spiral-scanning mechanism that captures spatial edge continuity and models irregular structures more effectively. Thirdly, we design the Bilateral Spectrum Aligner (Bi-SAB) and a tailored MS2-Loss to jointly align luminance and chrominance features, improving visual naturalness and mitigating artifacts. Extensive experiments on five LLIE benchmarks demonstrate that DRWKV achieves leading performance in PSNR, SSIM, and NIQE while maintaining low computational complexity. Furthermore, DRWKV enhances downstream performance in low-light multi-object tracking tasks, validating its generalization capabilities.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2507.23334</link>
<guid>https://arxiv.org/abs/2507.23334</guid>
<content:encoded><![CDATA[

arXiv:2507.23334v2 Announce Type: replace-cross 
Abstract: Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A General Approach to Visualizing Uncertainty in Statistical Graphics</title>
<link>https://arxiv.org/abs/2508.00937</link>
<guid>https://arxiv.org/abs/2508.00937</guid>
<content:encoded><![CDATA[

arXiv:2508.00937v2 Announce Type: replace-cross 
Abstract: We present a general approach to visualizing uncertainty in static 2-D statistical graphics. If we treat a visualization as a function of its underlying quantities, uncertainty in those quantities induces a distribution over images. We show how to aggregate these images into a single visualization that represents the uncertainty. The approach can be viewed as a generalization of sample-based approaches that use overlay. Notably, standard representations, such as confidence intervals and bands, emerge with their usual coverage guarantees without being explicitly quantified or visualized. As a proof of concept, we implement our approach in the IID setting using resampling, provided as an open-source Python library. Because the approach operates directly on images, the user needs only to supply the data and the code for visualizing the quantities of interest without uncertainty. Through several examples, we show how both familiar and novel forms of uncertainty visualization can be created. The implementation is not only a practical validation of the underlying theory but also an immediately usable tool that can complement existing uncertainty-visualization libraries.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI Research and Deployment</title>
<link>https://arxiv.org/abs/2508.02292</link>
<guid>https://arxiv.org/abs/2508.02292</guid>
<content:encoded><![CDATA[

arXiv:2508.02292v2 Announce Type: replace-cross 
Abstract: Financial AI holds great promise for transforming modern finance, with the potential to support a wide range of tasks such as market forecasting, portfolio management, quantitative trading, and automated analysis. However, existing platforms remain limited in task coverage, lack robust multimodal data integration, and offer insufficient support for the training and deployment of large language models (LLMs). In response to these limitations, we present FinWorld, an all-in-one open-source platform that provides end-to-end support for the entire financial AI workflow, from data acquisition to experimentation and deployment. FinWorld distinguishes itself through native integration of heterogeneous financial data, unified support for diverse AI paradigms, and advanced agent automation, enabling seamless development and deployment. Leveraging data from 2 representative markets, 4 stock pools, and over 800 million financial data points, we conduct comprehensive experiments on 4 key financial AI tasks. These experiments systematically evaluate deep learning and reinforcement learning algorithms, with particular emphasis on RL-based finetuning for LLMs and LLM Agents. The empirical results demonstrate that FinWorld significantly enhances reproducibility, supports transparent benchmarking, and streamlines deployment, thereby providing a strong foundation for future research and real-world applications. Code is available at Github~\footnote{https://github.com/DVampire/FinWorld}.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling User Preferences as Distributions for Optimal Transport-based Cross-domain Recommendation under Non-overlapping Settings</title>
<link>https://arxiv.org/abs/2508.16210</link>
<guid>https://arxiv.org/abs/2508.16210</guid>
<content:encoded><![CDATA[

arXiv:2508.16210v2 Announce Type: replace-cross 
Abstract: Cross-domain recommender (CDR) systems aim to transfer knowledge from data-rich domains to data-sparse ones, alleviating sparsity and cold-start issues present in conventional single-domain recommenders. However, many CDR approaches rely on overlapping users or items to establish explicit cross-domain connections, which is unrealistic in practice. Moreover, most methods represent user preferences as fixed discrete vectors, limiting their ability to capture the fine-grained and multi-aspect nature of user interests. To address these limitations, we propose DUP-OT (Distributional User Preferences with Optimal Transport), a novel framework for non-overlapping CDR. DUP-OT consists of three stages: (1) a shared preprocessing module that extracts review-based embeddings using a unified sentence encoder and autoencoder; (2) a user preference modeling module that represents each user's interests as a Gaussian Mixture Model (GMM) over item embeddings; and (3) an optimal-transport-based alignment module that matches Gaussian components across domains, enabling effective preference transfer for target-domain rating prediction. Experiments on Amazon Review datasets demonstrate that DUP-OT mitigates domain discrepancy and significantly outperforms state-of-the-art baselines under strictly non-overlapping training settings, with user correspondence revealed only for inference-time evaluation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An exact multiple-time-step variational formulation for the committor and the transition rate</title>
<link>https://arxiv.org/abs/2509.03539</link>
<guid>https://arxiv.org/abs/2509.03539</guid>
<content:encoded><![CDATA[

arXiv:2509.03539v2 Announce Type: replace-cross 
Abstract: For a transition between two stable states, the committor is the probability that the dynamics leads to one stable state before the other. It can be estimated from trajectory data by minimizing an expression for the transition rate that depends on a lag time. We show that an existing such expression is minimized by the exact committor only when the lag time is a single time step, resulting in a biased estimate in practical applications. We introduce an alternative expression that is minimized by the exact committor at any lag time. The key idea is that, when trajectories enter the stable states, the times that they enter (stopping times) must be used for estimating the committor and transition rate instead of the lag time. Numerical tests on benchmark systems demonstrate that our committor and transition rate estimates are much less sensitive to the choice of lag time. We show how further accuracy for the transition rate can be achieved by combining results from two lag times. We also relate the transition rate expression to a variational approach for kinetic statistics based on the mean-squared residual and discuss further numerical considerations with the aid of a decomposition of the error into dynamic modes.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling to Multimodal and Multichannel Heart Sound Classification with Synthetic and Augmented Biosignals</title>
<link>https://arxiv.org/abs/2509.11606</link>
<guid>https://arxiv.org/abs/2509.11606</guid>
<content:encoded><![CDATA[

arXiv:2509.11606v3 Announce Type: replace-cross 
Abstract: Cardiovascular diseases (CVDs) are the leading cause of death worldwide, accounting for approximately 17.9 million deaths each year. Early detection is critical, creating a demand for accurate and inexpensive pre-screening methods. Deep learning has recently been applied to classify abnormal heart sounds indicative of CVDs using synchronised phonocardiogram (PCG) and electrocardiogram (ECG) signals, as well as multichannel PCG (mPCG). However, state-of-the-art architectures remain underutilised due to the limited availability of synchronised and multichannel datasets. Augmented datasets and pre-trained models provide a pathway to overcome these limitations, enabling transformer-based architectures to be trained effectively. This work combines traditional signal processing with denoising diffusion models, WaveGrad and DiffWave, to create an augmented dataset to fine-tune a Wav2Vec 2.0-based classifier on multimodal and multichannel heart sound datasets. The approach achieves state-of-the-art performance. On the Computing in Cardiology (CinC) 2016 dataset of single channel PCG, accuracy, unweighted average recall (UAR), sensitivity, specificity and Matthew's correlation coefficient (MCC) reach 92.48%, 93.05%, 93.63%, 92.48%, 94.93% and 0.8283, respectively. Using the synchronised PCG and ECG signals of the training-a dataset from CinC, 93.14%, 92.21%, 94.35%, 90.10%, 95.12% and 0.8380 are achieved for accuracy, UAR, sensitivity, specificity and MCC, respectively. Using a wearable vest dataset consisting of mPCG data, the model achieves 77.13% accuracy, 74.25% UAR, 86.47% sensitivity, 62.04% specificity, and 0.5082 MCC. These results demonstrate the effectiveness of transformer-based models for CVD detection when supported by augmented datasets, highlighting their potential to advance multimodal and multichannel heart sound classification.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SADA: Safe and Adaptive Aggregation of Multiple Black-Box Predictions in Semi-Supervised Learning</title>
<link>https://arxiv.org/abs/2509.21707</link>
<guid>https://arxiv.org/abs/2509.21707</guid>
<content:encoded><![CDATA[

arXiv:2509.21707v2 Announce Type: replace-cross 
Abstract: Semi-supervised learning (SSL) arises in practice when labeled data are scarce or expensive to obtain, while large quantities of unlabeled data are readily available.
  With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions of uncertain quality for both inference and prediction tasks. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through small-scale simulations and two real-data analyses with distinct scientific goals. A user-friendly R package, sada, is provided to facilitate practical implementation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inferring Cosmological Parameters with Evidential Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2509.24327</link>
<guid>https://arxiv.org/abs/2509.24327</guid>
<content:encoded><![CDATA[

arXiv:2509.24327v2 Announce Type: replace-cross 
Abstract: We examine the use of a novel variant of Physics-Informed Neural Networks to predict cosmological parameters from recent supernovae and baryon acoustic oscillations (BAO) datasets. Our machine learning framework generates uncertainty estimates for target variables and the inferred unknown parameters of the underlying PDE descriptions. Built upon a hybrid of the principles of Evidential Deep Learning, Physics-Informed Neural Networks, Bayesian Neural Networks and Gaussian Processes, our model enables learning of the posterior distribution of the unknown PDE parameters through standard gradient-descent based training. We apply our model to an up-to-date BAO dataset (Bousis et al. 2024) calibrated with the CMB-inferred sound horizon, and the Pantheon$+$ Sne Ia distances (Scolnic et al. 2018), examining the relative effectiveness and mutual consistency among the standard $\Lambda$CDM, $w$CDM and $\Lambda_s$CDM models. Unlike previous results arising from the standard approach of minimizing an appropriate $\chi^2$ function, the posterior distributions for parameters in various models trained purely on Pantheon$+$ data were found to be largely contained within the $2\sigma$ contours of their counterparts trained on BAO data. Their posterior medians for $h_0$ were within about $2\sigma$ of one another, indicating that our machine learning-guided approach provides a different measure of the Hubble tension.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MetaChest: Generalized few-shot learning of pathologies from chest X-rays</title>
<link>https://arxiv.org/abs/2509.25590</link>
<guid>https://arxiv.org/abs/2509.25590</guid>
<content:encoded><![CDATA[

arXiv:2509.25590v2 Announce Type: replace-cross 
Abstract: The limited availability of annotated data presents a major challenge for applying deep learning methods to medical image analysis. Few-shot learning methods aim to recognize new classes from only a small number of labeled examples. These methods are typically studied under the standard few-shot learning setting, where all classes in a task are new. However, medical applications such as pathology classification from chest X-rays often require learning new classes while simultaneously leveraging knowledge of previously known ones, a scenario more closely aligned with generalized few-shot classification. Despite its practical relevance, few-shot learning has been scarcely studied in this context. In this work, we present MetaChest, a large-scale dataset of 479,215 chest X-rays collected from four public databases. MetaChest includes a meta-set partition specifically designed for standard few-shot classification, as well as an algorithm for generating multi-label episodes. We conduct extensive experiments evaluating both a standard transfer learning approach and an extension of ProtoNet across a wide range of few-shot multi-label classification tasks. Our results demonstrate that increasing the number of classes per episode and the number of training examples per class improves classification performance. Notably, the transfer learning approach consistently outperforms the ProtoNet extension, despite not being tailored for few-shot learning. We also show that higher-resolution images improve accuracy at the cost of additional computation, while efficient model architectures achieve comparable performance to larger models with significantly reduced resource requirements.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCPO: Proportionate Credit Policy Optimization for Aligning Image Generation Models</title>
<link>https://arxiv.org/abs/2509.25774</link>
<guid>https://arxiv.org/abs/2509.25774</guid>
<content:encoded><![CDATA[

arXiv:2509.25774v2 Announce Type: replace-cross 
Abstract: While reinforcement learning has advanced the alignment of text-to-image (T2I) models, state-of-the-art policy gradient methods are still hampered by training instability and high variance, hindering convergence speed and compromising image quality. Our analysis identifies a key cause of this instability: disproportionate credit assignment, in which the mathematical structure of the generative sampler produces volatile and non-proportional feedback across timesteps. To address this, we introduce Proportionate Credit Policy Optimization (PCPO), a framework that enforces proportional credit assignment through a stable objective reformulation and a principled reweighting of timesteps. This correction stabilizes the training process, leading to significantly accelerated convergence and superior image quality. The improvement in quality is a direct result of mitigating model collapse, a common failure mode in recursive training. PCPO substantially outperforms existing policy gradient baselines on all fronts, including the state-of-the-art DanceGRPO. Code is available at https://github.com/jaylee2000/pcpo/.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning from the electronic structure of molecules across the periodic table</title>
<link>https://arxiv.org/abs/2510.00224</link>
<guid>https://arxiv.org/abs/2510.00224</guid>
<content:encoded><![CDATA[

arXiv:2510.00224v2 Announce Type: replace-cross 
Abstract: Machine-Learned Interatomic Potentials (MLIPs) require vast amounts of atomic structure data to learn forces and energies, and their performance continues to improve with training set size. Meanwhile, the even greater quantities of accompanying data in the Hamiltonian matrix H behind these datasets has so far gone unused for this purpose. Here, we provide a recipe for integrating the orbital interaction data within H towards training pipelines for atomic-level properties. We first introduce HELM ("Hamiltonian-trained Electronic-structure Learning for Molecules"), a state-of-the-art Hamiltonian prediction model which bridges the gap between Hamiltonian prediction and universal MLIPs by scaling to H of structures with 100+ atoms, high elemental diversity, and large basis sets including diffuse functions. To accompany HELM, we release a curated Hamiltonian matrix dataset, 'OMol_CSH_58k', with unprecedented elemental diversity (58 elements), molecular size (up to 150 atoms), and basis set (def2-TZVPD). Finally, we introduce 'Hamiltonian pretraining' as a method to extract meaningful descriptors of atomic environments even from a limited number atomic structures, and repurpose this shared embedding space to improve performance on energy-prediction in low-data regimes. Our results highlight the use of electronic interactions as a rich and transferable data source for representing chemical space.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees</title>
<link>https://arxiv.org/abs/2510.01268</link>
<guid>https://arxiv.org/abs/2510.01268</guid>
<content:encoded><![CDATA[

arXiv:2510.01268v4 Announce Type: replace-cross 
Abstract: We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 37\%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Hedging Under Non-Convexity: Limitations and a Case for AlphaZero</title>
<link>https://arxiv.org/abs/2510.01874</link>
<guid>https://arxiv.org/abs/2510.01874</guid>
<content:encoded><![CDATA[

arXiv:2510.01874v2 Announce Type: replace-cross 
Abstract: This paper examines replication portfolio construction in incomplete markets - a key problem in financial engineering with applications in pricing, hedging, balance sheet management, and energy storage planning. We model this as a two-player game between an investor and the market, where the investor makes strategic bets on future states while the market reveals outcomes. Inspired by the success of Monte Carlo Tree Search in stochastic games, we introduce an AlphaZero-based system and compare its performance to deep hedging - a widely used industry method based on gradient descent. Through theoretical analysis and experiments, we show that deep hedging struggles in environments where the optimal action-value function is not subject to convexity constraints - such as those involving non-convex transaction costs, capital constraints, or regulatory limitations - converging to local optima. We construct specific market environments to highlight these limitations and demonstrate that AlphaZero consistently finds near-optimal replication strategies. On the theoretical side, we establish a connection between deep hedging and convex optimization, suggesting that its effectiveness is contingent on convexity assumptions. Our experiments further suggest that AlphaZero is more sample-efficient - an important advantage in data-scarce, overfitting-prone derivative markets.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>mini-vec2vec: Scaling Universal Geometry Alignment with Linear Transformations</title>
<link>https://arxiv.org/abs/2510.02348</link>
<guid>https://arxiv.org/abs/2510.02348</guid>
<content:encoded><![CDATA[

arXiv:2510.02348v3 Announce Type: replace-cross 
Abstract: We build upon vec2vec, a procedure designed to align text embedding spaces without parallel data. vec2vec finds a near-perfect alignment, but it is expensive and unstable. We present mini-vec2vec, a simple and efficient alternative that requires substantially lower computational cost and is highly robust. Moreover, the learned mapping is a linear transformation. Our method consists of three main stages: a tentative matching of pseudo-parallel embedding vectors, transformation fitting, and iterative refinement. Our linear alternative exceeds the original instantiation of vec2vec by orders of magnitude in efficiency, while matching or exceeding their results. The method's stability and interpretable algorithmic steps facilitate scaling and unlock new opportunities for adoption in new domains and fields.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Monitoring: A General Framework with an Application to Non-life Insurance Pricing</title>
<link>https://arxiv.org/abs/2510.04556</link>
<guid>https://arxiv.org/abs/2510.04556</guid>
<content:encoded><![CDATA[

arXiv:2510.04556v2 Announce Type: replace-cross 
Abstract: Maintaining the predictive performance of pricing models is challenging when insurance portfolios and data-generating mechanisms evolve over time. Focusing on non-life insurance, we adopt the concept-drift terminology from machine learning and distinguish virtual drift from real concept drift in an actuarial setting. Methodologically, we (i) formalize deviance loss and Murphy's score decomposition to assess global and local auto-calibration; (ii) study the Gini score as a rank-based performance measure, derive its asymptotic distribution, and develop a consistent bootstrap estimator of its asymptotic variance; and (iii) combine these results into a statistically grounded, model-agnostic monitoring framework that integrates a Gini-based ranking drift test with global and local auto-calibration tests. An application to a modified motor insurance portfolio with controlled concept-drift scenarios illustrates how the framework guides decisions on refitting or recalibrating pricing models.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Biophysically-Conditioned Generative Framework for 3D Brain Tumor MRI Synthesis</title>
<link>https://arxiv.org/abs/2510.09365</link>
<guid>https://arxiv.org/abs/2510.09365</guid>
<content:encoded><![CDATA[

arXiv:2510.09365v2 Announce Type: replace-cross 
Abstract: Magnetic resonance imaging (MRI) inpainting supports numerous clinical and research applications. We introduce the first generative model that conditions on voxel-level, continuous tumor concentrations to synthesize high-fidelity brain tumor MRIs. For the BraTS 2025 Inpainting Challenge, we adapt this architecture to the complementary task of healthy tissue restoration by setting the tumor concentrations to zero. Our latent diffusion model conditioned on both tissue segmentations and the tumor concentrations generates 3D spatially coherent and anatomically consistent images for both tumor synthesis and healthy tissue inpainting. For healthy inpainting, we achieve a PSNR of 18.5, and for tumor inpainting, we achieve 17.4. Our code is available at: https://github.com/valentin-biller/ldm.git
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Learning Is Provably Bayesian Inference: A Generalization Theory for Meta-Learning</title>
<link>https://arxiv.org/abs/2510.10981</link>
<guid>https://arxiv.org/abs/2510.10981</guid>
<content:encoded><![CDATA[

arXiv:2510.10981v2 Announce Type: replace-cross 
Abstract: This paper develops a finite-sample statistical theory for in-context learning (ICL), analyzed within a meta-learning framework that accommodates mixtures of diverse task types. We introduce a principled risk decomposition that separates the total ICL risk into two orthogonal components: Bayes Gap and Posterior Variance. The Bayes Gap quantifies how well the trained model approximates the Bayes-optimal in-context predictor. For a uniform-attention Transformer, we derive a non-asymptotic upper bound on this gap, which explicitly clarifies the dependence on the number of pretraining prompts and their context length. The Posterior Variance is a model-independent risk representing the intrinsic task uncertainty. Our key finding is that this term is determined solely by the difficulty of the true underlying task, while the uncertainty arising from the task mixture vanishes exponentially fast with only a few in-context examples. Together, these results provide a unified view of ICL: the Transformer selects the optimal meta-algorithm during pretraining and rapidly converges to the optimal algorithm for the true task at test time.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pretraining in Actor-Critic Reinforcement Learning for Robot Locomotion</title>
<link>https://arxiv.org/abs/2510.12363</link>
<guid>https://arxiv.org/abs/2510.12363</guid>
<content:encoded><![CDATA[

arXiv:2510.12363v2 Announce Type: replace-cross 
Abstract: The pretraining-finetuning paradigm has facilitated numerous transformative advancements in artificial intelligence research in recent years. However, in the domain of reinforcement learning (RL) for robot locomotion, individual skills are often learned from scratch despite the high likelihood that some generalizable knowledge is shared across all task-specific policies belonging to the same robot embodiment. This work aims to define a paradigm for pretraining neural network models that encapsulate such knowledge and can subsequently serve as a basis for warm-starting the RL process in classic actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin with a task-agnostic exploration-based data collection algorithm to gather diverse, dynamic transition data, which is then used to train a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning. The pretrained weights are then loaded into both the actor and critic networks to warm-start the policy optimization of actual tasks. We systematically validated our proposed method with 9 distinct robot locomotion RL environments comprising 3 different robot embodiments, showing significant benefits of this initialization strategy. Our proposed approach on average improves sample efficiency by 36.9% and task performance by 7.3% compared to random initialization. We further present key ablation studies and empirical analyses that shed light on the mechanisms behind the effectiveness of this method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HPC-Driven Modeling with ML-Based Surrogates for Magnon-Photon Dynamics in Hybrid Quantum Systems</title>
<link>https://arxiv.org/abs/2510.22221</link>
<guid>https://arxiv.org/abs/2510.22221</guid>
<content:encoded><![CDATA[

arXiv:2510.22221v2 Announce Type: replace-cross 
Abstract: Simulating hybrid magnonic quantum systems remains a challenge due to the large disparity between the timescales of the two systems. We present a massively parallel GPU-based simulation framework that enables fully coupled, large-scale modeling of on-chip magnon-photon circuits. Our approach resolves the dynamic interaction between ferromagnetic and electromagnetic fields with high spatiotemporal fidelity. To accelerate design workflows, we develop a physics-informed machine learning surrogate trained on the simulation data, reducing computational cost while maintaining accuracy. This combined approach reveals real-time energy exchange dynamics and reproduces key phenomena such as anti-crossing behavior and the suppression of ferromagnetic resonance under strong electromagnetic fields. By addressing the multiscale and multiphysics challenges in magnon-photon modeling, our framework enables scalable simulation and rapid prototyping of next-generation quantum and spintronic devices.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smaller Models, Smarter Rewards: A Two-Sided Approach to Process and Outcome Rewards</title>
<link>https://arxiv.org/abs/2510.23083</link>
<guid>https://arxiv.org/abs/2510.23083</guid>
<content:encoded><![CDATA[

arXiv:2510.23083v2 Announce Type: replace-cross 
Abstract: Generating high-quality code remains a challenge for Large Language Models (LLMs). For the evolution of reasoning models on this task, reward models are a necessary intermediate step. These models judge outcomes or intermediate steps. Decoder-only transformer models can be turned into reward models by introducing a regression layer and supervised fine-tuning. While it is known that reflection capabilities generally increase with the size of a model, we want to investigate whether state-of-the-art small language models like the Phi-4 family can be turned into usable reward models blending the consideration of process rewards and outcome rewards.
  Targeting this goal, we construct a dataset of code samples with correctness labels derived from the APPS coding challenge benchmark. We then train a value-head model to estimate the success probability of intermediate outputs. Our evaluation shows that small LLMs are capable of serving as effective reward models or code evaluation critics, successfully identifying correct solutions among multiple candidates. Using this critic, we achieve over a 20% improvement in the search capability of the most accurate code out of multiple generations.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MotionStream: Real-Time Video Generation with Interactive Motion Controls</title>
<link>https://arxiv.org/abs/2511.01266</link>
<guid>https://arxiv.org/abs/2511.01266</guid>
<content:encoded><![CDATA[

arXiv:2511.01266v2 Announce Type: replace-cross 
Abstract: Current motion-conditioned video generation methods suffer from prohibitive latency (minutes per video) and non-causal processing that prevents real-time interaction. We present MotionStream, enabling sub-second latency with up to 29 FPS streaming generation on a single GPU. Our approach begins by augmenting a text-to-video model with motion control, which generates high-quality videos that adhere to the global text prompt and local motion guidance, but does not perform inference on the fly. As such, we distill this bidirectional teacher into a causal student through Self Forcing with Distribution Matching Distillation, enabling real-time streaming inference. Several key challenges arise when generating videos of long, potentially infinite time-horizons -- (1) bridging the domain gap from training on finite length and extrapolating to infinite horizons, (2) sustaining high quality by preventing error accumulation, and (3) maintaining fast inference, without incurring growth in computational cost due to increasing context windows. A key to our approach is introducing carefully designed sliding-window causal attention, combined with attention sinks. By incorporating self-rollout with attention sinks and KV cache rolling during training, we properly simulate inference-time extrapolations with a fixed context window, enabling constant-speed generation of arbitrarily long videos. Our models achieve state-of-the-art results in motion following and video quality while being two orders of magnitude faster, uniquely enabling infinite-length streaming. With MotionStream, users can paint trajectories, control cameras, or transfer motion, and see results unfold in real-time, delivering a truly interactive experience.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BondBERT: What we learn when assigning sentiment in the bond market</title>
<link>https://arxiv.org/abs/2511.01869</link>
<guid>https://arxiv.org/abs/2511.01869</guid>
<content:encoded><![CDATA[

arXiv:2511.01869v2 Announce Type: replace-cross 
Abstract: Bond markets respond differently to macroeconomic news compared to equity markets, yet most sentiment models are trained primarily on general financial or equity news data. However, bond prices often move in the opposite direction to economic optimism, making general or equity-based sentiment tools potentially misleading. We introduce BondBERT, a transformer-based language model fine-tuned on bond-specific news. BondBERT can act as the perception and reasoning component of a financial decision-support agent, providing sentiment signals that integrate with forecasting models. We propose a generalisable framework for adapting transformers to low-volatility, domain-inverse sentiment tasks by compiling and cleaning 30,000 UK bond market articles (2018-2025). BondBERT's sentiment predictions are compared against FinBERT, FinGPT, and Instruct-FinGPT using event-based correlation, up/down accuracy analyses, and LSTM forecasting across ten UK sovereign bonds. We find that BondBERT consistently produces positive correlations with bond returns, and achieves higher alignment and forecasting accuracy than the three baseline models. These results demonstrate that domain-specific sentiment adaptation better captures fixed income dynamics, bridging a gap between NLP advances and bond market analytics.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EvoMem: Improving Multi-Agent Planning with Dual-Evolving Memory</title>
<link>https://arxiv.org/abs/2511.01912</link>
<guid>https://arxiv.org/abs/2511.01912</guid>
<content:encoded><![CDATA[

arXiv:2511.01912v2 Announce Type: replace-cross 
Abstract: Planning has been a cornerstone of artificial intelligence for solving complex problems, and recent progress in LLM-based multi-agent frameworks have begun to extend this capability. However, the role of human-like memory within these frameworks remains largely unexplored. Understanding how agents coordinate through memory is critical for natural language planning, where iterative reasoning, constraint tracking, and error correction drive the success. Inspired by working memory model in cognitive psychology, we present EvoMem, a multi-agent framework built on a dual-evolving memory mechanism. The framework consists of three agents (Constraint Extractor, Verifier, and Actor) and two memory modules: Constraint Memory (CMem), which evolves across queries by storing task-specific rules and constraints while remains fixed within a query, and Query-feedback Memory (QMem), which evolves within a query by accumulating feedback across iterations for solution refinement. Both memory modules are reset at the end of each query session. Evaluations on trip planning, meeting planning, and calendar scheduling show consistent performance improvements, highlighting the effectiveness of EvoMem. This success underscores the importance of memory in enhancing multi-agent planning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DeepPAAC: A New Deep Galerkin Method for Principal-Agent Problems</title>
<link>https://arxiv.org/abs/2511.04309</link>
<guid>https://arxiv.org/abs/2511.04309</guid>
<content:encoded><![CDATA[

arXiv:2511.04309v2 Announce Type: replace-cross 
Abstract: We consider numerical resolution of principal-agent (PA) problems in continuous time. We formulate a generic PA model with continuous and lump payments and a multi-dimensional strategy of the agent. To tackle the resulting Hamilton-Jacobi-Bellman equation with an implicit Hamiltonian we develop a novel deep learning method: the Deep Principal-Agent Actor Critic (DeepPAAC) Actor-Critic algorithm. DeepPAAC is able to handle multi-dimensional states and controls, as well as constraints. We investigate the role of the neural network architecture, training designs, loss functions, etc. on the convergence of the solver, presenting five different case studies.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>High-Performance Variance-Covariance Matrix Construction Using an Uncentered Gram Formulation</title>
<link>https://arxiv.org/abs/2511.08223</link>
<guid>https://arxiv.org/abs/2511.08223</guid>
<content:encoded><![CDATA[

arXiv:2511.08223v2 Announce Type: replace-cross 
Abstract: Reichel (2025) defined the bariance as a pairwise-difference measure that can be rewritten in linear time using only scalar sums. We extend this idea to the covariance matrix by showing that the standard matrix expression involving the uncentered Gram matrix and a correction term is algebraically identical to the pairwise-difference definition while avoiding explicit centering. The computation then reduces to one outer product of dimension p-by-p and a single subtraction. Benchmarks in Python show clear runtime gains, especially when BLAS optimizations are absent. Optionally faster Gram-matrix routines such as RXTX (Rybin et al., 2025) further reduce overall cost.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SIGMMA: Hierarchical Graph-Based Multi-Scale Multi-modal Contrastive Alignment of Histopathology Image and Spatial Transcriptome</title>
<link>https://arxiv.org/abs/2511.15464</link>
<guid>https://arxiv.org/abs/2511.15464</guid>
<content:encoded><![CDATA[

arXiv:2511.15464v2 Announce Type: replace-cross 
Abstract: Recent advances in computational pathology have leveraged vision-language models to learn joint representations of Hematoxylin and Eosin (HE) images with spatial transcriptomic (ST) profiles. However, existing approaches typically align HE tiles with their corresponding ST profiles at a single scale, overlooking fine-grained cellular structures and their spatial organization. To address this, we propose Sigmma, a multi-modal contrastive alignment framework for learning hierarchical representations of HE images and spatial transcriptome profiles across multiple scales. Sigmma introduces multi-scale contrastive alignment, ensuring that representations learned at different scales remain coherent across modalities. Furthermore, by representing cell interactions as a graph and integrating inter- and intra-subgraph relationships, our approach effectively captures cell-cell interactions, ranging from fine to coarse, within the tissue microenvironment. We demonstrate that Sigmm learns representations that better capture cross-modal correspondences, leading to an improvement of avg. 9.78\% in the gene-expression prediction task and avg. 26.93\% in the cross-modal retrieval task across datasets. We further show that it learns meaningful multi-tissue organization in downstream analyses.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation</title>
<link>https://arxiv.org/abs/2511.16543</link>
<guid>https://arxiv.org/abs/2511.16543</guid>
<content:encoded><![CDATA[

arXiv:2511.16543v2 Announce Type: replace-cross 
Abstract: The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Inspired by knowledge distillation, Prism leverages a powerful, instruction-following teacher LLM (FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. Our extensive experiments on benchmark datasets reveal a key finding: the distillation process not only transfers knowledge but also acts as a noise filter. Our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, demonstrating an emergent ability to correct hallucinations present in the teacher's outputs. While achieving a 24x speedup and a 10x reduction in memory consumption, our analysis validates that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality, and perhaps more importantly, trustworthy explainable recommendation.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Impact of Off-Policy Training Data on Probe Generalisation</title>
<link>https://arxiv.org/abs/2511.17408</link>
<guid>https://arxiv.org/abs/2511.17408</guid>
<content:encoded><![CDATA[

arXiv:2511.17408v2 Announce Type: replace-cross 
Abstract: Probing has emerged as a promising method for monitoring large language models (LLMs), enabling inference-time detection of concerning behaviours such as deception and sycophancy. However, natural examples of many behaviours are rare, forcing researchers to rely on synthetic or off-policy LLM responses for training probes. We systematically evaluate how the use of synthetic and off-policy data influences probe generalisation across eight distinct LLM behaviours. Testing linear and attention probes across multiple LLMs, we find that the response generation strategy can significantly affect probe performance, though the magnitude of this effect greatly varies by behaviour. We find that successful generalisation from off-policy responses to incentivised responses (e.g. those where the behaviour is advantageous) is predictive of successful generalisation to on-policy data. Leveraging this result, we predict that Deception and Sandbagging probes may fail to generalise from off-policy to on-policy data when used in real monitoring scenarios. We also find that shifts in the training data domain cause even larger performance degradation than off-to-on-policy shift, with different-domain test scores being consistently lower than the same-domain ones. In the absence of on-policy data, using same-domain off-policy data appears to yield more reliable probes than using on-policy data from a different domain. Still, we emphasise the need for methods that can better handle distribution shifts in LLM monitoring.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Latent Collaboration in Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2511.20639</link>
<guid>https://arxiv.org/abs/2511.20639</guid>
<content:encoded><![CDATA[

arXiv:2511.20639v2 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) extend large language models (LLMs) from independent single-model reasoning to coordinative system-level intelligence. While existing LLM agents depend on text-based mediation for reasoning and communication, we take a step forward by enabling models to collaborate directly within the continuous latent space. We introduce LatentMAS, an end-to-end training-free framework that enables pure latent collaboration among LLM agents. In LatentMAS, each agent first performs auto-regressive latent thoughts generation through last-layer hidden embeddings. A shared latent working memory then preserves and transfers each agent's internal representations, ensuring lossless information exchange. We provide theoretical analyses establishing that LatentMAS attains higher expressiveness and lossless information preservation with substantially lower complexity than vanilla text-based MAS. In addition, empirical evaluations across 9 comprehensive benchmarks spanning math and science reasoning, commonsense understanding, and code generation show that LatentMAS consistently outperforms strong single-model and text-based MAS baselines, achieving up to 14.6% higher accuracy, reducing output token usage by 70.8%-83.7%, and providing 4x-4.3x faster end-to-end inference. These results demonstrate that our new latent collaboration framework enhances system-level reasoning quality while offering substantial efficiency gains without any additional training. Code and data are fully open-sourced at https://github.com/Gen-Verse/LatentMAS.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Diffusion Priors for Text-to-Image Generation via Optimization-based Visual Inversion</title>
<link>https://arxiv.org/abs/2511.20821</link>
<guid>https://arxiv.org/abs/2511.20821</guid>
<content:encoded><![CDATA[

arXiv:2511.20821v2 Announce Type: replace-cross 
Abstract: Diffusion models have established the state-of-the-art in text-to-image generation, but their performance often relies on a diffusion prior network to translate text embeddings into the visual manifold for easier decoding. These priors are computationally expensive and require extensive training on massive datasets. In this work, we challenge the necessity of a trained prior at all by employing Optimization-based Visual Inversion (OVI), a training-free and data-free alternative, to replace the need for a prior. OVI initializes a latent visual representation from random pseudo-tokens and iteratively optimizes it to maximize the cosine similarity with input textual prompt embedding. We further propose two novel constraints, a Mahalanobis-based and a Nearest-Neighbor loss, to regularize the OVI optimization process toward the distribution of realistic images. Our experiments, conducted on Kandinsky 2.2, show that OVI can serve as an alternative to traditional priors. More importantly, our analysis reveals a critical flaw in current evaluation benchmarks like T2I-CompBench++, where simply using the text embedding as a prior achieves surprisingly high scores, despite lower perceptual quality. Our constrained OVI methods improve visual fidelity over this baseline, with the Nearest-Neighbor approach proving particularly effective, achieving quantitative scores comparable to or higher than the state-of-the-art data-efficient prior, indicating that the idea merits further investigation. The code will be publicly available upon acceptance.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Two Proxy Methods for Causal Identification</title>
<link>https://arxiv.org/abs/2512.00175</link>
<guid>https://arxiv.org/abs/2512.00175</guid>
<content:encoded><![CDATA[

arXiv:2512.00175v2 Announce Type: replace-cross 
Abstract: Identifying causal effects in the presence of unmeasured variables is a fundamental challenge in causal inference, for which proxy variable methods have emerged as a powerful solution. We contrast two major approaches in this framework: (1) bridge equation methods, which leverage solutions to integral equations to recover causal targets, and (2) array decomposition methods, which recover latent factors composing counterfactual quantities by exploiting unique determination of eigenspaces. We compare the model restrictions underlying these two approaches and provide insight into implications of the underlying assumptions, clarifying the scope of applicability for each method.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MM-ACT: Learn from Multimodal Parallel Generation to Act</title>
<link>https://arxiv.org/abs/2512.00975</link>
<guid>https://arxiv.org/abs/2512.00975</guid>
<content:encoded><![CDATA[

arXiv:2512.00975v2 Announce Type: replace-cross 
Abstract: A generalist robotic policy needs both semantic understanding for task planning and the ability to interact with the environment through predictive capabilities. To tackle this, we present MM-ACT, a unified Vision-Language-Action (VLA) model that integrates text, image, and action in shared token space and performs generation across all three modalities. MM-ACT adopts a re-mask parallel decoding strategy for text and image generation, and employs a one-step parallel decoding strategy for action generation to improve efficiency. We introduce Context-Shared Multimodal Learning, a unified training paradigm that supervises generation in all three modalities from a shared context, enhancing action generation through cross-modal learning. Experiments were conducted on the LIBERO simulation and Franka real-robot setups as well as RoboTwin2.0 to assess in-domain and out-of-domain performances respectively. Our approach achieves a success rate of 96.3% on LIBERO, 72.0% across three tasks of real Franka, and 52.38% across eight bimanual tasks of RoboTwin2.0 with an additional gain of 9.25% from cross-modal learning. We release our codes, models and data at https://github.com/HHYHRHY/MM-ACT.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Path Collaborative Reasoning via Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01485</link>
<guid>https://arxiv.org/abs/2512.01485</guid>
<content:encoded><![CDATA[

arXiv:2512.01485v2 Announce Type: replace-cross 
Abstract: Chain-of-Thought (CoT) reasoning has significantly advanced the problem-solving capabilities of Large Language Models (LLMs), yet conventional CoT often exhibits internal determinism during decoding, limiting exploration of plausible alternatives. Recent methods attempt to address this by generating soft abstract tokens to enable reasoning in a continuous semantic space. However, we find that such approaches remain constrained by the greedy nature of autoregressive decoding, which fundamentally isolates the model from alternative reasoning possibilities. In this work, we propose Multi-Path Perception Policy Optimization (M3PO), a novel reinforcement learning framework that explicitly injects collective insights into the reasoning process. M3PO leverages parallel policy rollouts as naturally diverse reasoning sources and integrates cross-path interactions into policy updates through a lightweight collaborative mechanism. This design allows each trajectory to refine its reasoning with peer feedback, thereby cultivating more reliable multi-step reasoning patterns. Empirical results show that M3PO achieves state-of-the-art performance on both knowledge- and reasoning-intensive benchmarks. Models trained with M3PO maintain interpretability and inference efficiency, underscoring the promise of multi-path collaborative learning for robust reasoning.
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Much Ado About Noising: Dispelling the Myths of Generative Robotic Control</title>
<link>https://arxiv.org/abs/2512.01809</link>
<guid>https://arxiv.org/abs/2512.01809</guid>
<content:encoded><![CDATA[

arXiv:2512.01809v2 Announce Type: replace-cross 
Abstract: Generative models, like flows and diffusions, have recently emerged as popular and efficacious policy parameterizations in robotics. There has been much speculation as to the factors underlying their successes, ranging from capturing multi-modal action distribution to expressing more complex behaviors. In this work, we perform a comprehensive evaluation of popular generative control policies (GCPs) on common behavior cloning (BC) benchmarks. We find that GCPs do not owe their success to their ability to capture multi-modality or to express more complex observation-to-action mappings. Instead, we find that their advantage stems from iterative computation, as long as intermediate steps are supervised during training and this supervision is paired with a suitable level of stochasticity. As a validation of our findings, we show that a minimum iterative policy (MIP), a lightweight two-step regression-based policy, essentially matches the performance of flow GCPs, and often outperforms distilled shortcut models. Our results suggest that the distribution-fitting component of GCPs is less salient than commonly believed, and point toward new design spaces focusing solely on control performance. Project page: https://simchowitzlabpublic.github.io/much-ado-about-noising-project/
]]></content:encoded>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
<pubDate>Tue, 09 Dec 2025 00:00:00 -0500</pubDate>
</item>

<item>
<title>Morphling: Fast, Fused, and Flexible GNN Training at Scale</title>
<link>https://arxiv.org/abs/2512.01678</link>
<guid>https://arxiv.org/abs/2512.01678</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, code synthesis, performance optimization, sparsity-aware execution, distributed training  

<br /><br />Summary:  
Graph Neural Networks (GNNs) face significant hardware challenges due to the combination of irregular graph traversals and dense matrix computations, which existing frameworks like PyTorch Geometric (PyG) and Deep Graph Library (DGL) do not efficiently handle. Morphling is introduced as a domain-specific code synthesizer that compiles high-level GNN specifications into optimized, backend-specialized implementations for OpenMP, CUDA, and MPI. It leverages a library of architecture-aware primitives customized for each execution environment to enhance performance. Additionally, Morphling integrates a runtime sparsity-aware execution engine that dynamically selects between dense or sparse computation paths based on input feature statistics, effectively reducing unnecessary operations on zero-valued data. The effectiveness of Morphling was validated on eleven diverse real-world datasets covering various graph types, feature sizes, and sparsity levels. Results demonstrate that Morphling improves per-epoch training throughput by approximately 20× on CPUs, 19× on GPUs, and 6× in distributed settings compared to PyG and DGL, with speedups peaking at 66×. Moreover, Morphling’s memory-efficient layouts reduce peak memory usage by up to 15×, enabling training of large-scale GNNs on commodity hardware. This work highlights that specialized, architecture-aware code generation is a viable and scalable solution for efficient GNN execution across different parallel and distributed platforms. <div>
arXiv:2512.01678v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) present a fundamental hardware challenge by fusing irregular, memory-bound graph traversals with regular, compute-intensive dense matrix operations. While frameworks such as PyTorch Geometric (PyG) and Deep Graph Library (DGL) prioritize high-level usability, they fail to address these divergent execution characteristics. As a result, they rely on generic kernels that suffer from poor cache locality, excessive memory movement, and substantial intermediate allocations. To address these limitations, we present Morphling, a domain-specific code synthesizer designed to bridge this gap. Morphling compiles high-level GNN specifications into portable, backend-specialized implementations targeting OpenMP, CUDA, and MPI. It achieves this by instantiating a library of optimized, architecture-aware primitives tailored to each execution environment. Morphling also incorporates a runtime sparsity-aware execution engine that dynamically selects dense or sparse execution paths using input feature statistics, reducing unnecessary computation on zero-valued entries. We evaluate Morphling on eleven real-world datasets spanning diverse graph structures, feature dimensionalities, and sparsity regimes. Morphling improves per-epoch training throughput by an average of 20X on CPUs, 19X on GPUs, and 6X in distributed settings over PyG and DGL, with peak speedups reaching 66X. Morphling's memory-efficient layouts further reduce peak memory consumption by up to 15X, enabling large-scale GNN training on commodity hardware. These findings demonstrate that specialized, architecture-aware code synthesis provides an effective and scalable path toward high-performance GNN execution across diverse parallel and distributed platforms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advanced Unsupervised Learning: A Comprehensive Overview of Multi-View Clustering Techniques</title>
<link>https://arxiv.org/abs/2512.05169</link>
<guid>https://arxiv.org/abs/2512.05169</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view clustering, unsupervised learning, data integration, scalability, deep learning<br /><br />Summary:<br /><br />This article addresses the challenges faced by machine learning, including computational limits, single-view algorithm constraints, and processing complex datasets from multiple domains or perspectives. Multi-view clustering (MVC) is presented as a robust unsupervised learning approach that overcomes limitations of single-view methods by providing enriched data representation and practical utility despite increased complexity. The survey contributes by first categorizing MVC methods into distinct groups such as co-training, co-regularization, subspace approaches, deep learning, kernel-based techniques, anchor-based methods, and graph-based strategies. Second, it offers an in-depth analysis of each category's strengths, weaknesses, and challenges like scalability and handling incomplete data. Third, the paper discusses future trends, interdisciplinary applications, and directions for MVC research advancements. The work is thorough, reviewing over 140 key publications, comparing integration strategies—early fusion, late fusion, and joint learning—and exploring use cases in healthcare, multimedia, and social network analysis. Overall, this survey fills existing gaps in MVC research and provides valuable insights to guide future development in the field. <div>
arXiv:2512.05169v1 Announce Type: new 
Abstract: Machine learning techniques face numerous challenges to achieve optimal performance. These include computational constraints, the limitations of single-view learning algorithms and the complexity of processing large datasets from different domains, sources or views. In this context, multi-view clustering (MVC), a class of unsupervised multi-view learning, emerges as a powerful approach to overcome these challenges. MVC compensates for the shortcomings of single-view methods and provides a richer data representation and effective solutions for a variety of unsupervised learning tasks. In contrast to traditional single-view approaches, the semantically rich nature of multi-view data increases its practical utility despite its inherent complexity. This survey makes a threefold contribution: (1) a systematic categorization of multi-view clustering methods into well-defined groups, including co-training, co-regularization, subspace, deep learning, kernel-based, anchor-based, and graph-based strategies; (2) an in-depth analysis of their respective strengths, weaknesses, and practical challenges, such as scalability and incomplete data; and (3) a forward-looking discussion of emerging trends, interdisciplinary applications, and future directions in MVC research. This study represents an extensive workload, encompassing the review of over 140 foundational and recent publications, the development of comparative insights on integration strategies such as early fusion, late fusion, and joint learning, and the structured investigation of practical use cases in the areas of healthcare, multimedia, and social network analysis. By integrating these efforts, this work aims to fill existing gaps in MVC research and provide actionable insights for the advancement of the field.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Coefficient of Variation Masking: A Volatility-Aware Strategy for EHR Foundation Models</title>
<link>https://arxiv.org/abs/2512.05216</link>
<guid>https://arxiv.org/abs/2512.05216</guid>
<content:encoded><![CDATA[
<div> Masked autoencoders, electronic health records, volatility, coefficient of variation masking, clinical representations<br /><br />Summary:<br /><br />This paper addresses the challenge of learning meaningful representations from electronic health records (EHR) using masked autoencoders (MAEs). Current MAE approaches use uniform random masking which assumes equal predictability across all features. However, the authors highlight that laboratory tests differ significantly in volatility; stable biomarkers like sodium contrast with volatile ones like lactate, which are harder to model but clinically important as they often indicate acute conditions. To tackle this, they propose a novel volatility-aware pretraining strategy called Coefficient of Variation Masking (CV-Masking), which adjusts masking probabilities based on each feature's intrinsic variability. This approach aligns with clinical workflows by employing a value-only masking objective. Experimental results on a large set of laboratory tests demonstrate that CV-Masking improves reconstruction accuracy, enhances downstream predictive performance, and speeds up model convergence. Overall, the method yields more robust, clinically relevant EHR representations, potentially aiding diverse clinical tasks and improving patient care by better capturing complex temporal patterns in volatile biomarkers. <div>
arXiv:2512.05216v1 Announce Type: new 
Abstract: Masked autoencoders (MAEs) are increasingly applied to electronic health records (EHR) for learning general-purpose representations that support diverse clinical tasks. However, existing approaches typically rely on uniform random masking, implicitly assuming all features are equally predictable. In reality, laboratory tests exhibit substantial heterogeneity in volatility: some biomarkers (e.g., sodium) remain stable, while others (e.g., lactate) fluctuate considerably and are more difficult to model. Clinically, volatile biomarkers often signal acute pathophysiology and require more sophisticated modeling to capture their complex temporal patterns. We propose a volatility-aware pretraining strategy, Coefficient of Variation Masking (CV-Masking), that adaptively adjusts masking probabilities according to the intrinsic variability of each feature. Combined with a value-only masking objective aligned with clinical workflows, CV-Masking yields systematic improvements over random and variance-based strategies. Experiments on a large panel of laboratory tests show that CV-Masking enhances reconstruction, improves downstream predictive performance, and accelerates convergence, producing more robust and clinically meaningful EHR representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Tokenization for Clinical Time Series: When Less is More</title>
<link>https://arxiv.org/abs/2512.05217</link>
<guid>https://arxiv.org/abs/2512.05217</guid>
<content:encoded><![CDATA[
<div> Tokenization, clinical time series, transformer, pretrained embeddings, prediction tasks<br /><br />Summary:  
This study systematically evaluates various tokenization strategies for clinical time series modeling using transformer-based architectures, emphasizing their impact on electronic health record (EHR) data analysis. The research is conducted through controlled ablations on four clinical prediction tasks from the MIMIC-IV dataset, highlighting the task-dependent nature of feature importance. Key findings indicate that explicit time encodings do not provide a statistically significant benefit across the evaluated tasks. Additionally, the influence of value features varies by task, being important for mortality prediction but less so for readmission prediction, indicating that code sequences themselves can be sufficiently predictive. The study also demonstrates that frozen pretrained code encoders outperform trainable versions significantly while using fewer parameters, offering computational efficiency without sacrificing accuracy. Larger clinical encoders contribute consistent improvements across tasks, particularly when combined with frozen embeddings that reduce computational overhead. Overall, the research promotes fair and controlled comparisons of tokenization methods and suggests that simpler, parameter-efficient models often achieve strong performance. However, it concludes that the best tokenization approach depends on the specific clinical prediction task being addressed. <div>
arXiv:2512.05217v1 Announce Type: new 
Abstract: Tokenization strategies shape how models process electronic health records, yet fair comparisons of their effectiveness remain limited. We present a systematic evaluation of tokenization approaches for clinical time series modeling using transformer-based architectures, revealing task-dependent and sometimes counterintuitive findings about temporal and value feature importance. Through controlled ablations across four clinical prediction tasks on MIMIC-IV, we demonstrate that explicit time encodings provide no consistent statistically significant benefit for the evaluated downstream tasks. Value features show task-dependent importance, affecting mortality prediction but not readmission, suggesting code sequences alone can carry sufficient predictive signal. We further show that frozen pretrained code encoders dramatically outperform their trainable counterparts while requiring dramatically fewer parameters. Larger clinical encoders provide consistent improvements across tasks, benefiting from frozen embeddings that eliminate computational overhead. Our controlled evaluation enables fairer tokenization comparisons and demonstrates that simpler, parameter-efficient approaches can, in many cases, achieve strong performance, though the optimal tokenization strategy remains task-dependent.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating the Antigenic Data Bottleneck: Semi-supervised Learning with Protein Language Models for Influenza A Surveillance</title>
<link>https://arxiv.org/abs/2512.05222</link>
<guid>https://arxiv.org/abs/2512.05222</guid>
<content:encoded><![CDATA[
<div> Influenza A, antigenicity, Protein Language Models, Semi-Supervised Learning, haemagglutinin<br /><br />Summary:<br /><br />Influenza A viruses (IAVs) rapidly evolve antigenically, necessitating frequent vaccine updates. Traditional haemagglutination inhibition (HI) assays to measure antigenicity are labor-intensive and unscalable, creating a bottleneck as genomic data far exceed available labeled phenotypic data. To address this, the study investigates combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) to maintain predictive accuracy when labeled data are limited. Two SSL methods—Self-training and Label Spreading—were evaluated against fully supervised baselines using four PLM embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation simulated low-label scenarios (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). Results showed SSL consistently improved performance under label scarcity, with Self-training using ProtVec yielding the largest relative gains. ESM-2 embeddings were especially robust, achieving F1 scores over 0.82 even with just 25% labeled data, indicating effective capture of key antigenic features. Prediction accuracy was high for H1N1 and H9N2, whereas the hypervariable H3N2 remained challenging but saw mitigated performance drops with SSL. These findings highlight that integrating PLMs and SSL can overcome antigenicity labeling shortages and better leverage unlabeled surveillance sequences, aiding rapid variant prioritization and timely vaccine strain selection. <div>
arXiv:2512.05222v1 Announce Type: new 
Abstract: Influenza A viruses (IAVs) evolve antigenically at a pace that requires frequent vaccine updates, yet the haemagglutination inhibition (HI) assays used to quantify antigenicity are labor-intensive and unscalable. As a result, genomic data vastly outpace available phenotypic labels, limiting the effectiveness of traditional supervised models. We hypothesize that combining pre-trained Protein Language Models (PLMs) with Semi-Supervised Learning (SSL) can retain high predictive accuracy even when labeled data are scarce. We evaluated two SSL strategies, Self-training and Label Spreading, against fully supervised baselines using four PLM-derived embeddings (ESM-2, ProtVec, ProtT5, ProtBert) applied to haemagglutinin (HA) sequences. A nested cross-validation framework simulated low-label regimes (25%, 50%, 75%, and 100% label availability) across four IAV subtypes (H1N1, H3N2, H5N1, H9N2). SSL consistently improved performance under label scarcity. Self-training with ProtVec produced the largest relative gains, showing that SSL can compensate for lower-resolution representations. ESM-2 remained highly robust, achieving F1 scores above 0.82 with only 25% labeled data, indicating that its embeddings capture key antigenic determinants. While H1N1 and H9N2 were predicted with high accuracy, the hypervariable H3N2 subtype remained challenging, although SSL mitigated the performance decline. These findings demonstrate that integrating PLMs with SSL can address the antigenicity labeling bottleneck and enable more effective use of unlabeled surveillance sequences, supporting rapid variant prioritization and timely vaccine strain selection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variance Matters: Improving Domain Adaptation via Stratified Sampling</title>
<link>https://arxiv.org/abs/2512.05226</link>
<guid>https://arxiv.org/abs/2512.05226</guid>
<content:encoded><![CDATA[
<div> Variance reduction, domain adaptation, stratified sampling, maximum mean discrepancy, correlation alignment<br /><br />Summary:<br /><br />1. The paper addresses the challenge of domain shift in machine learning by focusing on unsupervised domain adaptation (UDA), which aims to reduce domain discrepancy without labeled target data.<br /><br />2. A key problem identified is the high variance in discrepancy estimates during stochastic training, which can limit theoretical and practical benefits of UDA.<br /><br />3. The authors propose VaRDASS, a novel variance reduction method based on stratified sampling tailored specifically for UDA tasks.<br /><br />4. The approach is developed for two widely used domain discrepancy metrics: correlation alignment and maximum mean discrepancy (MMD). The paper derives specialized stratification objectives to reduce variance in these measures.<br /><br />5. Theoretical contributions include both expected and worst-case error bounds, with a proof that the proposed stratification objective for MMD is theoretically optimal in terms of minimizing variance under certain conditions.<br /><br />6. To implement the method, a practical optimization algorithm inspired by k-means clustering is proposed and analyzed.<br /><br />7. Experimental results on three domain shift datasets demonstrate that VaRDASS improves the accuracy of discrepancy estimation and enhances the performance on target domains compared to baseline methods. <div>
arXiv:2512.05226v1 Announce Type: new 
Abstract: Domain shift remains a key challenge in deploying machine learning models to the real world. Unsupervised domain adaptation (UDA) aims to address this by minimising domain discrepancy during training, but the discrepancy estimates suffer from high variance in stochastic settings, which can stifle the theoretical benefits of the method. This paper proposes Variance-Reduced Domain Adaptation via Stratified Sampling (VaRDASS), the first specialised stochastic variance reduction technique for UDA. We consider two specific discrepancy measures -- correlation alignment and the maximum mean discrepancy (MMD) -- and derive ad hoc stratification objectives for these terms. We then present expected and worst-case error bounds, and prove that our proposed objective for the MMD is theoretically optimal (i.e., minimises the variance) under certain assumptions. Finally, a practical k-means style optimisation algorithm is introduced and analysed. Experiments on three domain shift datasets demonstrate improved discrepancy estimation accuracy and target domain performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAR-FL: A Communication Efficient Peer-to-Peer Federated Learning System</title>
<link>https://arxiv.org/abs/2512.05234</link>
<guid>https://arxiv.org/abs/2512.05234</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Peer-to-Peer, Communication Complexity, Distributed Machine Learning, Network Churn  

<br /><br />Summary:  
This article addresses the challenges of Federated Learning (FL) in next-generation wireless systems, focusing on efficiency and robustness in environments with wireless-connected peers and network churn. It identifies the limitations of existing Peer-to-Peer (P2P) FL methods, particularly their high communication complexity, which hinders scalability. To overcome these issues, the authors propose MAR-FL, a novel P2P FL system that implements iterative group-based aggregation to significantly reduce communication overhead. MAR-FL achieves a communication cost scaling of O(N log N), which is a substantial improvement compared to the O(N²) complexity found in previous approaches. This advancement ensures that the system remains effective as the number of peers increases in each aggregation round. Additionally, MAR-FL demonstrates robustness in handling unreliable or unstable FL clients, making it suitable for real-world deployments where client availability is unpredictable. The system also supports integration with private computing techniques, facilitating privacy-preserving distributed learning. Overall, MAR-FL presents a scalable, efficient, and resilient solution for P2P Federated Learning under the constraints of wireless network environments and dynamic client participation. <div>
arXiv:2512.05234v1 Announce Type: new 
Abstract: The convergence of next-generation wireless systems and distributed Machine Learning (ML) demands Federated Learning (FL) methods that remain efficient and robust with wireless connected peers and under network churn. Peer-to-peer (P2P) FL removes the bottleneck of a central coordinator, but existing approaches suffer from excessive communication complexity, limiting their scalability in practice. We introduce MAR-FL, a novel P2P FL system that leverages iterative group-based aggregation to substantially reduce communication overhead while retaining resilience to churn. MAR-FL achieves communication costs that scale as O(N log N), contrasting with the O(N^2) complexity of previously existing baselines, and thereby maintains effectiveness especially as the number of peers in an aggregation round grows. The system is robust towards unreliable FL clients and can integrate private computing.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Edged Weisfeiler-Lehman Algorithm</title>
<link>https://arxiv.org/abs/2512.05238</link>
<guid>https://arxiv.org/abs/2512.05238</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Neural Networks, Weisfeiler-Lehman algorithm, Edge features, Graph classification, Edged Graph Isomorphism Network<br /><br />Summary:<br /><br />1. The paper addresses limitations in classical graph learning approaches, particularly the propagation-aggregation methodology used by many Graph Neural Networks (GNNs), which update node representations by recursively aggregating information from neighbor nodes and itself.<br /><br />2. The authors highlight the Weisfeiler-Lehman (1-WL) algorithm, a traditional graph isomorphism test based on color refinement of nodes and their neighbors, noting its inability to incorporate edge features or labels.<br /><br />3. To improve upon this, they introduce the Edged-WL algorithm (E-WL), an extension of the original 1-WL designed to incorporate and exploit edge features in graph data.<br /><br />4. Building on E-WL, the paper proposes the Edged Graph Isomorphism Network (EGIN), a novel GNN model developed to utilize edge features more effectively during graph learning, addressing a common weak point in existing GNN architectures.<br /><br />5. The authors validate their approach on 12 benchmark graph datasets that include edge features, demonstrating that EGIN consistently outperforms state-of-the-art baseline models in graph classification tasks, thus showing the effectiveness of leveraging edge information in graph learning. <div>
arXiv:2512.05238v1 Announce Type: new 
Abstract: As a classical approach on graph learning, the propagation-aggregation methodology is widely exploited by many of Graph Neural Networks (GNNs), wherein the representation of a node is updated by aggregating representations from itself and neighbor nodes recursively. Similar to the propagation-aggregation methodology, the Weisfeiler-Lehman (1-WL) algorithm tests isomorphism through color refinement according to color representations of a node and its neighbor nodes. However, 1-WL does not leverage any edge features (labels), presenting a potential improvement on exploiting edge features in some fields. To address this limitation, we proposed a novel Edged-WL algorithm (E-WL) which extends the original 1-WL algorithm to incorporate edge features. Building upon the E-WL algorithm, we also introduce an Edged Graph Isomorphism Network (EGIN) model for further exploiting edge features, which addresses one key drawback in many GNNs that do not utilize any edge features of graph data. We evaluated the performance of proposed models using 12 edge-featured benchmark graph datasets and compared them with some state-of-the-art baseline models. Experimental results indicate that our proposed EGIN models, in general, demonstrate superior performance in graph learning on graph classification tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging quantum and classical computing for partial differential equations through multifidelity machine learning</title>
<link>https://arxiv.org/abs/2512.05241</link>
<guid>https://arxiv.org/abs/2512.05241</guid>
<content:encoded><![CDATA[
<div> Keywords: quantum algorithms, partial differential equations, multifidelity learning, neural architecture, quantum lattice Boltzmann methods<br /><br />Summary:<br /><br />Quantum algorithms aiming to solve partial differential equations (PDEs) are currently limited by near-term quantum hardware constraints, such as low qubit counts restricting spatial resolution and finite circuit depth hindering accurate long-time simulations. These limitations result in low-fidelity quantum PDE solutions despite their theoretical potential for speedup. To address this, the paper introduces a multifidelity learning framework that leverages sparse classical data to enhance coarse quantum solutions into high-fidelity results. The framework first trains a low-fidelity surrogate model from abundant quantum solver outputs, then applies a multifidelity neural network architecture that combines linear and nonlinear correction mappings. This approach is demonstrated on benchmark nonlinear PDE problems, including the viscous Burgers equation and incompressible Navier-Stokes equations, solved with quantum lattice Boltzmann methods. The framework not only corrects coarse quantum predictions but also achieves reliable temporal extrapolation beyond the classical training period. By reducing the dependence on expensive high-fidelity simulations, the method provides predictions competitive with classical accuracy. Overall, this strategy bridges the gap between hardware-limited quantum simulations and real-world application demands, paving the way for practical deployment of near-term quantum computing in scientific computing and advancing algorithm development for computational physics. <div>
arXiv:2512.05241v1 Announce Type: new 
Abstract: Quantum algorithms for partial differential equations (PDEs) face severe practical constraints on near-term hardware: limited qubit counts restrict spatial resolution to coarse grids, while circuit depth limitations prevent accurate long-time integration. These hardware bottlenecks confine quantum PDE solvers to low-fidelity regimes despite their theoretical potential for computational speedup. We introduce a multifidelity learning framework that corrects coarse quantum solutions to high-fidelity accuracy using sparse classical training data, facilitating the path toward practical quantum utility for scientific computing. The approach trains a low-fidelity surrogate on abundant quantum solver outputs, then learns correction mappings through a multifidelity neural architecture that balances linear and nonlinear transformations. Demonstrated on benchmark nonlinear PDEs including viscous Burgers equation and incompressible Navier-Stokes flows via quantum lattice Boltzmann methods, the framework successfully corrects coarse quantum predictions and achieves temporal extrapolation well beyond the classical training window. This strategy illustrates how one can reduce expensive high-fidelity simulation requirements while producing predictions that are competitive with classical accuracy. By bridging the gap between hardware-limited quantum simulations and application requirements, this work establishes a pathway for extracting computational value from current quantum devices in real-world scientific applications, advancing both algorithm development and practical deployment of near-term quantum computing for computational physics.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When unlearning is free: leveraging low influence points to reduce computational costs</title>
<link>https://arxiv.org/abs/2512.05254</link>
<guid>https://arxiv.org/abs/2512.05254</guid>
<content:encoded><![CDATA[
<div> Data privacy, machine learning, unlearning, influence functions, computational savings<br /><br />Summary:<br /><br />1. With increasing concerns over data privacy in machine learning, the ability to unlearn or selectively remove specific data points from trained models has become critical. 2. Current state-of-the-art unlearning methods generally treat all data points in the forget set equally, without considering their individual impact on the model. 3. This paper challenges that uniform approach by investigating whether data points with negligible influence on model behavior need to be removed at all. 4. Through a comparative analysis of influence functions on tasks in natural language processing and computer vision, the authors identify subsets of training data that minimally affect model outputs. 5. Building on this discovery, the authors propose an efficient unlearning framework that first reduces the forget dataset by excluding low-impact points before performing unlearning. 6. By doing so, the method achieves significant computational savings—up to approximately 50%—on real-world datasets, making the unlearning process more practical and resource-efficient without compromising privacy goals. <div>
arXiv:2512.05254v1 Announce Type: new 
Abstract: As concerns around data privacy in machine learning grow, the ability to unlearn, or remove, specific data points from trained models becomes increasingly important. While state of the art unlearning methods have emerged in response, they typically treat all points in the forget set equally. In this work, we challenge this approach by asking whether points that have a negligible impact on the model's learning need to be removed. Through a comparative analysis of influence functions across language and vision tasks, we identify subsets of training data with negligible impact on model outputs. Leveraging this insight, we propose an efficient unlearning framework that reduces the size of datasets before unlearning leading to significant computational savings (up to approximately 50 percent) on real world empirical examples.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DMAGT: Unveiling miRNA-Drug Associations by Integrating SMILES and RNA Sequence Structures through Graph Transformer Models</title>
<link>https://arxiv.org/abs/2512.05287</link>
<guid>https://arxiv.org/abs/2512.05287</guid>
<content:encoded><![CDATA[
<div> Keywords: miRNA, drug discovery, graph neural network, transformer, drug-miRNA association prediction<br /><br />Summary:<br />1. MicroRNAs (miRNAs) play a critical role in gene regulation and have emerged as promising targets in pharmacology for drug development. Traditional experimental methods to explore drug-miRNA associations are limited by time and cost.<br />2. To address these limitations, the study proposes DMAGT, a novel machine learning model based on a multi-layer transformer-based graph neural network designed to predict drug-miRNA associations effectively.<br />3. DMAGT converts drug-miRNA associations into graph structures and uses Word2Vec embeddings to represent features of drug molecular structures and miRNA base structures, enabling deep relational learning through a graph transformer model.<br />4. The model was evaluated on three publicly available drug-miRNA association datasets: ncDR, RNAInter, and SM2miR, achieving a high predictive performance with an area under the curve (AUC) reaching 95.24±0.05%. DMAGT outperformed other state-of-the-art approaches in similar predictive tasks.<br />5. Practical validation was conducted by focusing on two drugs, 5-Fluorouracil and Oxaliplatin, where 14 out of the top 20 predicted drug-miRNA associations were confirmed experimentally, demonstrating DMAGT’s robustness and reliability.<br />6. Overall, DMAGT offers an efficient and stable computational approach for accelerating miRNA-targeted drug discovery, potentially reducing experimental costs and time significantly. <div>
arXiv:2512.05287v1 Announce Type: new 
Abstract: MiRNAs, due to their role in gene regulation, have paved a new pathway for pharmacology, focusing on drug development that targets miRNAs. However, traditional wet lab experiments are limited by efficiency and cost constraints, making it difficult to extensively explore potential associations between developed drugs and target miRNAs. Therefore, we have designed a novel machine learning model based on a multi-layer transformer-based graph neural network, DMAGT, specifically for predicting associations between drugs and miRNAs. This model transforms drug-miRNA associations into graphs, employs Word2Vec for embedding features of drug molecular structures and miRNA base structures, and leverages a graph transformer model to learn from embedded features and relational structures, ultimately predicting associations between drugs and miRNAs. To evaluate DMAGT, we tested its performance on three datasets composed of drug-miRNA associations: ncDR, RNAInter, and SM2miR, achieving up to AUC of $95.24\pm0.05$. DMAGT demonstrated superior performance in comparative experiments tackling similar challenges. To validate its practical efficacy, we specifically focused on two drugs, namely 5-Fluorouracil and Oxaliplatin. Of the 20 potential drug-miRNA associations identified as the most likely, 14 were successfully validated. The above experiments demonstrate that DMAGT has an excellent performance and stability in predicting drug-miRNA associations, providing a new shortcut for miRNA drug development.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Interpretability and Optimization: Provably Attribution-Weighted Actor-Critic in Reproducing Kernel Hilbert Spaces</title>
<link>https://arxiv.org/abs/2512.05291</link>
<guid>https://arxiv.org/abs/2512.05291</guid>
<content:encoded><![CDATA[
<div> Keywords: Actor-Critic, RKHS, SHAP, Reinforcement Learning, Interpretability<br /><br />Summary:<br /><br />This paper introduces RSA2C, an advanced Actor-Critic (AC) algorithm designed to improve interpretability in reinforcement learning by incorporating state attributions. It addresses a key limitation in existing explainable RL methods which treat all state features equally, ignoring the different impacts each state dimension has on rewards. RSA2C employs a kernelized, two-timescale actor-critic framework where the Actor is modeled in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic operate in scalar RKHSs. The components utilize sparsified dictionaries with the Actor and Advantage Critic sharing one, while the Value Critic maintains its own. State attributions are derived from the Value Critic using RKHS–SHAP, which leverages kernel mean embeddings to calculate on-manifold and off-manifold expectations. These attributions are transformed into Mahalanobis-gated weights, which modulate the Actor’s gradient updates and Advantage Critic’s targets, enhancing both training guidance and interpretability. The paper provides a theoretical global, non-asymptotic convergence bound under state perturbations, demonstrating the method’s stability and efficiency. Empirical evaluations on three continuous-control benchmarks confirm that RSA2C delivers improved learning efficiency, stable convergence, and interpretable state importance, advancing explainability in RL. <div>
arXiv:2512.05291v1 Announce Type: new 
Abstract: Actor-critic (AC) methods are a cornerstone of reinforcement learning (RL) but offer limited interpretability. Current explainable RL methods seldom use state attributions to assist training. Rather, they treat all state features equally, thereby neglecting the heterogeneous impacts of individual state dimensions on the reward. We propose RKHS--SHAP-based Advanced Actor--Critic (RSA2C), an attribution-aware, kernelized, two-timescale AC algorithm, including Actor, Value Critic, and Advantage Critic. The Actor is instantiated in a vector-valued reproducing kernel Hilbert space (RKHS) with a Mahalanobis-weighted operator-valued kernel, while the Value Critic and Advantage Critic reside in scalar RKHSs. These RKHS-enhanced components use sparsified dictionaries: the Value Critic maintains its own dictionary, while the Actor and Advantage Critic share one. State attributions, computed from the Value Critic via RKHS--SHAP (kernel mean embedding for on-manifold expectations and conditional mean embedding for off-manifold expectations), are converted into Mahalanobis-gated weights that modulate Actor gradients and Advantage Critic targets. Theoretically, we derive a global, non-asymptotic convergence bound under state perturbations, showing stability through the perturbation-error term and efficiency through the convergence-error term. Empirical results on three standard continuous-control environments show that our algorithm achieves efficiency, stability, and interpretability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CFO: Learning Continuous-Time PDE Dynamics via Flow-Matched Neural Operators</title>
<link>https://arxiv.org/abs/2512.05297</link>
<guid>https://arxiv.org/abs/2512.05297</guid>
<content:encoded><![CDATA[
<div> Neural operators, PDEs, continuous-time dynamics, flow matching, temporal resolution invariance  

<br /><br />Summary:  
This paper introduces the Continuous Flow Operator (CFO), a novel framework designed to learn continuous-time dynamics of time-dependent partial differential equations (PDEs) without the computational overhead typical of neural ODE methods. CFO repurposes flow matching to learn the PDE right-hand side directly, avoiding costly backpropagation through ODE solvers. The method fits temporal splines to the given trajectory data and uses finite-difference approximations of time derivatives at spline knots to create probability paths, whose velocities approximate true PDE dynamics. A neural operator is then trained to predict these analytic velocity fields via flow matching. CFO is inherently invariant to time resolution, allowing training on irregular, non-uniform temporal samples and permitting inference queries at arbitrary temporal resolutions through ODE integration. Evaluated on four benchmark problems—the Lorenz system, 1D Burgers equation, 2D diffusion-reaction, and 2D shallow water equations—CFO demonstrates superior long-horizon stability and remarkable data efficiency. Remarkably, CFO trained on only 25% of irregular subsampled time points outperforms autoregressive baselines trained on full datasets, yielding relative error reductions up to 87%. Despite requiring numerical integration during inference, CFO remains computationally efficient, using only half the function evaluations compared to baselines while uniquely supporting reverse-time inference and flexible temporal querying. <div>
arXiv:2512.05297v1 Announce Type: new 
Abstract: Neural operator surrogates for time-dependent partial differential equations (PDEs) conventionally employ autoregressive prediction schemes, which accumulate error over long rollouts and require uniform temporal discretization. We introduce the Continuous Flow Operator (CFO), a framework that learns continuous-time PDE dynamics without the computational burden of standard continuous approaches, e.g., neural ODE. The key insight is repurposing flow matching to directly learn the right-hand side of PDEs without backpropagating through ODE solvers. CFO fits temporal splines to trajectory data, using finite-difference estimates of time derivatives at knots to construct probability paths whose velocities closely approximate the true PDE dynamics. A neural operator is then trained via flow matching to predict these analytic velocity fields. This approach is inherently time-resolution invariant: training accepts trajectories sampled on arbitrary, non-uniform time grids while inference queries solutions at any temporal resolution through ODE integration. Across four benchmarks (Lorenz, 1D Burgers, 2D diffusion-reaction, 2D shallow water), CFO demonstrates superior long-horizon stability and remarkable data efficiency. CFO trained on only 25% of irregularly subsampled time points outperforms autoregressive baselines trained on complete data, with relative error reductions up to 87%. Despite requiring numerical integration at inference, CFO achieves competitive efficiency, outperforming autoregressive baselines using only 50% of their function evaluations, while uniquely enabling reverse-time inference and arbitrary temporal querying.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Scientific Machine Learning using Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)</title>
<link>https://arxiv.org/abs/2512.05306</link>
<guid>https://arxiv.org/abs/2512.05306</guid>
<content:encoded><![CDATA[
<div> Kolmogorov-Arnold Networks, Sparse Variational Gaussian Process, Uncertainty Quantification, Bayesian Inference, Scientific Machine Learning<br /><br />Summary:<br /><br />This article introduces Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs), a novel framework that enhances interpretability and uncertainty quantification in neural network models. The authors integrate sparse variational Gaussian process inference with the Kolmogorov-Arnold network topology, which allows scalable Bayesian inference with computational complexity that is quasi-linear relative to sample size. A key contribution is the use of analytic moment matching to efficiently propagate uncertainty through deep additive structures while preserving the model’s interpretability. The paper emphasizes the ability of SVGP KANs to distinguish between aleatoric (inherent noise) and epistemic (model) uncertainty, which is critical in scientific applications. This is demonstrated through three example studies: calibrating heteroscedastic measurement noise in fluid flow reconstruction, quantifying confidence degradation in multi-step forecasts of advection-diffusion processes, and detecting out-of-distribution inputs in convolutional autoencoders. The results indicate that SVGP KANs offer significant advantages for uncertainty-aware learning, making them well-suited for scientific machine learning tasks that require principled uncertainty quantification. <div>
arXiv:2512.05306v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks have emerged as interpretable alternatives to traditional multi-layer perceptrons. However, standard implementations lack principled uncertainty quantification capabilities essential for many scientific applications. We present a framework integrating sparse variational Gaussian process inference with the Kolmogorov-Arnold topology, enabling scalable Bayesian inference with computational complexity quasi-linear in sample size. Through analytic moment matching, we propagate uncertainty through deep additive structures while maintaining interpretability. We use three example studies to demonstrate the framework's ability to distinguish aleatoric from epistemic uncertainty: calibration of heteroscedastic measurement noise in fluid flow reconstruction, quantification of prediction confidence degradation in multi-step forecasting of advection-diffusion dynamics, and out-of-distribution detection in convolutional autoencoders. These results suggest Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KANs) is a promising architecture for uncertainty-aware learning in scientific machine learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Erosion of LLM Signatures: Can We Still Distinguish Human and LLM-Generated Scientific Ideas After Iterative Paraphrasing?</title>
<link>https://arxiv.org/abs/2512.05311</link>
<guid>https://arxiv.org/abs/2512.05311</guid>
<content:encoded><![CDATA[
<div> Keywords: LLM-generated ideas, human vs LLM detection, paraphrasing, source attribution, research context<br /><br />Summary:<br /><br />This paper addresses the challenge of distinguishing between scientific ideas generated by large language models (LLMs) and those produced by humans, an area that has been less explored compared to detecting LLM-generated text. The authors systematically evaluate state-of-the-art machine learning models in their ability to attribute the source of ideas, especially after multiple rounds of paraphrasing. Their results show a significant decline in detection accuracy—on average 25.4%—after five successive paraphrasing iterations, indicating that paraphrasing substantially obscures the origin of ideas. Furthermore, integrating the research problem as a contextual cue into the detection models improves performance by up to 2.97%, demonstrating the value of domain-specific context in attribution tasks. The study also finds that detection algorithms struggle the most when ideas are paraphrased into a simplified, non-expert style, which contributes the greatest challenge in preserving identifiable LLM characteristics. Overall, the work highlights the limitations of current detection methods in distinguishing human from LLM-generated scientific ideas after textual transformation and suggests directions for improving detection by leveraging contextual information and understanding stylistic shifts. <div>
arXiv:2512.05311v1 Announce Type: new 
Abstract: With the increasing reliance on LLMs as research agents, distinguishing between LLM and human-generated ideas has become crucial for understanding the cognitive nuances of LLMs' research capabilities. While detecting LLM-generated text has been extensively studied, distinguishing human vs LLM-generated scientific idea remains an unexplored area. In this work, we systematically evaluate the ability of state-of-the-art (SOTA) machine learning models to differentiate between human and LLM-generated ideas, particularly after successive paraphrasing stages. Our findings highlight the challenges SOTA models face in source attribution, with detection performance declining by an average of 25.4\% after five consecutive paraphrasing stages. Additionally, we demonstrate that incorporating the research problem as contextual information improves detection performance by up to 2.97%. Notably, our analysis reveals that detection algorithms struggle significantly when ideas are paraphrased into a simplified, non-expert style, contributing the most to the erosion of distinguishable LLM signatures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Deep Deterministic Policy Gradients on Continuous Control Tasks with Decoupled Prioritized Experience Replay</title>
<link>https://arxiv.org/abs/2512.05320</link>
<guid>https://arxiv.org/abs/2512.05320</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep Deterministic Policy Gradient, Actor-Critic, Experience Replay, Decoupled Prioritized Experience Replay, Continuous Control  

<br /><br />Summary:  
Background: Deep Deterministic Policy Gradient (DDPG) algorithms use Actor-Critic architectures where both networks are trained on the same replayed transition batches, despite having different learning objectives and update dynamics. This raises questions about the optimality of uniform transition usage.  
Objectives: The study aims to enhance DDPG performance by separating the transition batches used for training the Actor and the Critic, designing an experience replay mechanism that provides tailored learning signals.  
Methods: The authors propose Decoupled Prioritized Experience Replay (DPER), enabling independent sampling of transition batches for the Actor and Critic. DPER is compatible with any off-policy deep reinforcement learning algorithm focused on continuous control and is combined with the Twin Delayed DDPG algorithm for evaluation.  
Results: Experimental evaluation on several OpenAI Gym MuJoCo continuous control tasks demonstrates that DPER outperforms traditional experience replay methods, such as vanilla and prioritized experience replay.  
Conclusions: Decoupling the replay experience for Actor and Critic networks improves training dynamics and final policy performance. DPER offers a general and effective mechanism to boost a wide range of actor-critic off-policy reinforcement learning algorithms. <div>
arXiv:2512.05320v1 Announce Type: new 
Abstract: Background: Deep Deterministic Policy Gradient-based reinforcement learning algorithms utilize Actor-Critic architectures, where both networks are typically trained using identical batches of replayed transitions. However, the learning objectives and update dynamics of the Actor and Critic differ, raising concerns about whether uniform transition usage is optimal.
  Objectives: We aim to improve the performance of deep deterministic policy gradient algorithms by decoupling the transition batches used to train the Actor and the Critic. Our goal is to design an experience replay mechanism that provides appropriate learning signals to each component by using separate, tailored batches.
  Methods: We introduce Decoupled Prioritized Experience Replay (DPER), a novel approach that allows independent sampling of transition batches for the Actor and the Critic. DPER can be integrated into any off-policy deep reinforcement learning algorithm that operates in continuous control domains. We combine DPER with the state-of-the-art Twin Delayed DDPG algorithm and evaluate its performance across standard continuous control benchmarks.
  Results: DPER outperforms conventional experience replay strategies such as vanilla experience replay and prioritized experience replay in multiple MuJoCo tasks from the OpenAI Gym suite.
  Conclusions: Our findings show that decoupling experience replay for Actor and Critic networks can enhance training dynamics and final policy quality. DPER offers a generalizable mechanism that enhances performance for a wide class of actor-critic off-policy reinforcement learning algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robustness Test for AI Forecasting of Hurricane Florence Using FourCastNetv2 and Random Perturbations of the Initial Condition</title>
<link>https://arxiv.org/abs/2512.05323</link>
<guid>https://arxiv.org/abs/2512.05323</guid>
<content:encoded><![CDATA[
<div> Keywords: AI weather forecasting, FourCastNetv2, hurricane robustness, noise sensitivity, storm intensity  

<br /><br />Summary:  
This paper evaluates the robustness of NVIDIA’s AI weather forecasting model FourCastNetv2 (FCNv2) against input noise and uncertainty, focusing on hurricane predictions. Two main experiments are conducted: first, the initial conditions of Hurricane Florence from the ERA5 dataset are perturbed by injecting various levels of Gaussian noise to observe effects on predicted trajectories and storm intensity. Second, FCNv2 is started with fully random initial conditions to study its behavior with nonsensical inputs. Results show that FCNv2 can accurately preserve hurricane features under low to moderate noise levels, maintaining the storm’s general trajectory and structure even at high noise, though positional accuracy declines. The model consistently underestimates storm intensity and persistence across all noise levels. When initialized with random data, FCNv2 quickly produces smooth, cohesive forecasts, indicating a tendency towards stable, smoothed outputs regardless of input plausibility. The methodology presented is straightforward and can be applied to other AI-driven weather forecasting models for robustness assessment. These insights are important for evaluating model reliability, particularly for extreme weather event forecasting under uncertain or noisy conditions. <div>
arXiv:2512.05323v1 Announce Type: new 
Abstract: Understanding the robustness of a weather forecasting model with respect to input noise or different uncertainties is important in assessing its output reliability, particularly for extreme weather events like hurricanes. In this paper, we test sensitivity and robustness of an artificial intelligence (AI) weather forecasting model: NVIDIAs FourCastNetv2 (FCNv2). We conduct two experiments designed to assess model output under different levels of injected noise in the models initial condition. First, we perturb the initial condition of Hurricane Florence from the European Centre for Medium-Range Weather Forecasts (ECMWF) Reanalysis v5 (ERA5) dataset (September 13-16, 2018) with varying amounts of Gaussian noise and examine the impact on predicted trajectories and forecasted storm intensity. Second, we start FCNv2 with fully random initial conditions and observe how the model responds to nonsensical inputs. Our results indicate that FCNv2 accurately preserves hurricane features under low to moderate noise injection. Even under high levels of noise, the model maintains the general storm trajectory and structure, although positional accuracy begins to degrade. FCNv2 consistently underestimates storm intensity and persistence across all levels of injected noise. With full random initial conditions, the model generates smooth and cohesive forecasts after a few timesteps, implying the models tendency towards stable, smoothed outputs. Our approach is simple and portable to other data-driven AI weather forecasting models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Convex Federated Optimization under Cost-Aware Client Selection</title>
<link>https://arxiv.org/abs/2512.05327</link>
<guid>https://arxiv.org/abs/2512.05327</guid>
<content:encoded><![CDATA[
<div> Keywords: federated optimization, client-selection strategies, communication complexity, variance-reduced gradient, RG-SAGA

<br /><br />Summary:  
This article addresses the challenge of comparing federated optimization algorithms that use different client-selection strategies, noting that existing evaluation metrics do not account for varying communication costs. To remedy this, the authors propose a novel model that quantifies both communication and local computation complexities associated with several commonly used client-selection methods, explicitly linking each strategy to its respective cost. Within this new framework, they introduce an innovative algorithm designed for non-convex federated optimization, achieving the best-known communication and computational efficiency to date. The algorithm builds upon the inexact composite gradient method, enhanced by a specially constructed gradient estimator inspired by SAGA, a variance-reduced estimator known for its ability to leverage functional similarity. The authors derive a new variance bound revealing how SAGA exploits functional similarities among clients. Furthermore, they present the Recursive-Gradient (RG) technique, a general approach to improve error bounds of conditionally unbiased gradient estimators, including SAGA and SVRG. Applying this technique to SAGA yields the RG-SAGA gradient estimator, which demonstrates an improved error bound compared to the original, potentially offering superior convergence properties in federated optimization scenarios. <div>
arXiv:2512.05327v1 Announce Type: new 
Abstract: Different federated optimization algorithms typically employ distinct client-selection strategies: some methods communicate only with a randomly sampled subset of clients at each round, while others need to periodically communicate with all clients or use a hybrid scheme that combines both strategies. However, existing metrics for comparing optimization methods typically do not distinguish between these strategies, which often incur different communication costs in practice. To address this disparity, we introduce a simple and natural model of federated optimization that quantifies communication and local computation complexities. This new model allows for several commonly used client-selection strategies and explicitly associates each with a distinct cost. Within this setting, we propose a new algorithm that achieves the best-known communication and local complexities among existing federated optimization methods for non-convex optimization. This algorithm is based on the inexact composite gradient method with a carefully constructed gradient estimator and a special procedure for solving the auxiliary subproblem at each iteration. The gradient estimator is based on SAGA, a popular variance-reduced gradient estimator. We first derive a new variance bound for it, showing that SAGA can exploit functional similarity. We then introduce the Recursive-Gradient technique as a general way to potentially improve the error bound of a given conditionally unbiased gradient estimator, including both SAGA and SVRG. By applying this technique to SAGA, we obtain a new estimator, RG-SAGA, which has an improved error bound compared to the original one.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PathFinder: MCTS and LLM Feedback-based Path Selection for Multi-Hop Question Answering</title>
<link>https://arxiv.org/abs/2512.05336</link>
<guid>https://arxiv.org/abs/2512.05336</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-hop question answering, Large Language Models, Monte Carlo Tree Search, training data quality, sub-query reformulation

<br /><br />Summary: Multi-hop question answering (QA) requires language models to perform multi-step reasoning to arrive at correct answers, which remains a challenging task. Existing systems utilize Large Language Models (LLMs) to decompose questions and reason through multiple steps involving analysis, retrieval, and inference. However, training-based methods often face issues such as hallucinations by LLMs and incorrect reasoning paths that degrade performance. To address these problems, the authors propose PATHFINDER, a novel approach designed to enhance multi-hop QA systems. PATHFINDER leverages Monte Carlo Tree Search (MCTS) to generate training path traces that simulate reasoning steps more effectively. It also improves the quality of training data by filtering out erroneous and overly long traces using sub-answer recall metrics and validation by an LLM functioning as a judge. Additionally, the approach includes reformulation of sub-queries to manage cases where initial retrieval attempts fail, aiming to increase retrieval success. Experimental results show that PATHFINDER outperforms existing methods on public benchmark datasets, demonstrating improved accuracy and reasoning reliability. This combination of MCTS trace generation, data quality filtering, and sub-query reformulation establishes PATHFINDER as a promising solution for advancing multi-hop QA capabilities. <div>
arXiv:2512.05336v1 Announce Type: new 
Abstract: Multi-hop question answering is a challenging task in which language models must reason over multiple steps to reach the correct answer. With the help of Large Language Models and their reasoning capabilities, existing systems are able to think and decompose an input question over multiple steps to analyze, retrieve, and reason. However, training-based approaches for this problem still suffer from LLM hallucinations and incorrect reasoning paths that hinder performance. Hence, we propose PATHFINDER, an approach that: (i) uses Monte Carlo Tree Search to generate training path traces, (ii) improves training data quality by filtering erroneous and lengthy traces using sub-answer recall and LLM-as-a-judge verification, and (iii) reformulates sub-queries to handle failed retrieval cases. By following these steps, we demonstrate that PATHFINDER improves the performance of multi-hop QA over public benchmark datasets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interaction Tensor Shap</title>
<link>https://arxiv.org/abs/2512.05338</link>
<guid>https://arxiv.org/abs/2512.05338</guid>
<content:encoded><![CDATA[
<div> Keywords: Shapley interaction, Tensor Train, explainable AI, Interaction Tensor SHAP, high-dimensional models<br /><br />Summary:<br /><br />1. The paper addresses the challenge of understanding feature interactions in deep, high-dimensional machine learning models where existing Shapley-based methods either lack scalability or cannot handle higher order interactions efficiently.<br /><br />2. Current methods like the Shapley Taylor Interaction Index (STII) require exponential subset enumeration, making high order interaction computation infeasible, while tensor-based approaches such as Marginal SHAP Tensor (MST) are limited to first order effects.<br /><br />3. The authors propose Interaction Tensor SHAP (IT SHAP), which represents high order Shapley interactions exactly as tensor network contractions, specifically utilizing Tensor Train (TT) decompositions to achieve polynomial time complexity and polylogarithmic depth.<br /><br />4. IT SHAP reformulates the STII as a contraction between a Value Tensor and a Weight Tensor, both structured under TT format with polynomial ranks, thereby reducing exponential complexity (Theta(4^n)) to highly parallelizable NC2 time.<br /><br />5. This novel method provides a unified, axiomatic, and computationally tractable approach to capture both main effects and higher order interactions, paving the way for scalable interaction-aware explainable AI especially applicable to large, combinatorially complex black box models. <div>
arXiv:2512.05338v1 Announce Type: new 
Abstract: Machine learning models have grown increasingly deep and high dimensional, making it difficult to understand how individual and combined features influence their predictions. While Shapley value based methods provide principled feature attributions, existing formulations cannot tractably evaluate higher order interactions: the Shapley Taylor Interaction Index (STII) requires exponential scale enumeration of subsets, and current tensor based approaches such as the Marginal SHAP Tensor (MST) are restricted to first order effects. The central problem is that no existing framework simultaneously preserves the axiomatic exactness of STII and avoids the exponential computational blow up inherent to high order discrete derivatives. Here we show that high order Shapley interactions can be represented exactly as tensor network contractions, enabling polynomial time and polylog depth computation under Tensor Train (TT) structure. We introduce Interaction Tensor SHAP (IT SHAP), which reformulates STII as the contraction of a Value Tensor and a Weight Tensor, and assume a finite state TT representation of the Weight Tensor with polynomial TT ranks. Under TT structured model and distribution tensors, we show that IT SHAP reduces the exponential complex Theta(4^n) of STII to NC2 parallel time. These results demonstrate that IT SHAP provides a unified, axiomatic, and computationally tractable formulation of main effects and higher order interactions in high dimensional models. This framework establishes a foundation for scalable interaction aware explainable AI, with implications for large black box models whose combinatorial structure has previously rendered interaction analysis infeasible.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Taxonomy-Adaptive Moderation Model with Robust Guardrails for Large Language Models</title>
<link>https://arxiv.org/abs/2512.05339</link>
<guid>https://arxiv.org/abs/2512.05339</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Safety Alignment, Instruction Fine-tuning, Moderation Pipeline, Safety Benchmark<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) often receive safety alignment after training but can still generate harmful outputs, necessitating robust safeguards that address both inputs and outputs.<br /><br />2. The paper presents Roblox Guard 1.0, an advanced instruction fine-tuned LLM developed to improve LLM safety by implementing comprehensive input-output moderation through a pipeline of multiple LLMs.<br /><br />3. Roblox Guard 1.0 is built upon the Llama-3.1-8B-Instruct architecture and is specifically fine-tuned to generalize well to unseen safety taxonomies, showing strong performance on various out-of-domain safety evaluation benchmarks.<br /><br />4. The instruction fine-tuning methodology combines synthetic and open-source safety datasets, enhanced by chain-of-thought (CoT) rationales and input inversion techniques to improve contextual understanding and decision-making.<br /><br />5. Additionally, the authors introduce RobloxGuard-Eval, a novel benchmark with an extensible safety taxonomy designed to systematically evaluate the effectiveness of LLM guardrails and moderation systems. <div>
arXiv:2512.05339v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are typically aligned for safety during the post-training phase; however, they may still generate inappropriate outputs that could potentially pose risks to users. This challenge underscores the need for robust safeguards that operate across both model inputs and outputs. In this work, we introduce Roblox Guard 1.0, a state-of-the-art instruction fine-tuned LLM designed to enhance the safety of LLM systems through comprehensive input-output moderation, using a pipeline of LLMs to enhance moderation capability. Built on the Llama-3.1-8B-Instruct backbone, our model is instruction fine-tuned to generalize across previously unseen safety taxonomies and demonstrates strong performance on out-of-domain safety benchmarks. The instruction fine-tuning process uses a mix of synthetic and open-source safety datasets, augmented with chain-of-thought (CoT) rationales and input inversion to enhance contextual understanding and decision making. To support systematic evaluation, we also release RobloxGuard-Eval, a new benchmark featuring an extensible safety taxonomy to assess the effectiveness of LLM guardrails and moderation frameworks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Forgetting Builds Reliability: LLM Unlearning for Reliable Hardware Code Generation</title>
<link>https://arxiv.org/abs/2512.05341</link>
<guid>https://arxiv.org/abs/2512.05341</guid>
<content:encoded><![CDATA[
<div> Large Language Models, hardware design, unlearning framework, code generation, register-transfer level

<br /><br />Summary: This paper addresses the challenge of ensuring reliability in Large Language Models (LLMs) used for automated digital hardware code generation by proposing a novel unlearning framework. The framework integrates a syntax-preserving unlearning strategy that maintains the structural integrity of hardware description code during the forgetting process. In addition, it employs a fine-grained floor-aware selective loss mechanism that allows for precise, efficient removal of problematic or sensitive knowledge from the model without harming overall performance. This combined approach enables the model to forget proprietary intellectual property, contaminated benchmarks, and unsafe coding behaviors embedded during training. Experimental results demonstrate that this method can handle forget sets up to three times larger than existing approaches, often needing only a single training epoch to effectively unlearn the undesirable content. Importantly, the framework preserves both the syntactic correctness and the functional integrity of register-transfer level (RTL) code generated by the LLM, ensuring that hardware design quality is not compromised. This work thus represents a significant step towards making LLM-assisted hardware design more reliable, secure, and practical in industrial environments. <div>
arXiv:2512.05341v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown strong potential in accelerating digital hardware design through automated code generation. Yet, ensuring their reliability remains a critical challenge, as existing LLMs trained on massive heterogeneous datasets often exhibit problematic memorization of proprietary intellectual property (IP), contaminated benchmarks, and unsafe coding patterns. To mitigate these risks, we propose a novel unlearning framework tailored for LLM-based hardware code generation. Our method combines (i) a syntax-preserving unlearning strategy that safeguards the structural integrity of hardware code during forgetting, and (ii) a fine-grained floor-aware selective loss that enables precise and efficient removal of problematic knowledge. This integration achieves effective unlearning without degrading LLM code generation capabilities. Extensive experiments show that our framework supports forget sets up to 3x larger, typically requiring only a single training epoch, while preserving both syntactic correctness and functional integrity of register-transfer level (RTL) codes. Our work paves an avenue towards reliable LLM-assisted hardware design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Dimensionality Prediction in Hybrid Metal Halides via Feature Engineering and Class-Imbalance Mitigation</title>
<link>https://arxiv.org/abs/2512.05367</link>
<guid>https://arxiv.org/abs/2512.05367</guid>
<content:encoded><![CDATA[
<div> Hybrid metal halides, dimensionality prediction, machine learning, class imbalance, SMOTE<br /><br />Summary:<br /><br />1. The article introduces a machine learning framework designed to predict the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites.<br /><br />2. The dataset initially consisted of 494 HMH structures categorized into four dimensionality classes: 0D, 1D, 2D, and 3D, with a significant imbalance across these classes.<br /><br />3. To address the class imbalance challenge, the authors applied the Synthetic Minority Oversampling Technique (SMOTE), augmenting the dataset to 1336 samples.<br /><br />4. The framework incorporates chemically-informed, interaction-based feature engineering combined with a multi-stage workflow that includes feature selection, model stacking, and performance optimization.<br /><br />5. The method demonstrated substantial improvements in predicting underrepresented classes, achieving robust F1-scores and reliable cross-validation performance across all dimensionality categories. <div>
arXiv:2512.05367v1 Announce Type: new 
Abstract: We present a machine learning framework for predicting the structural dimensionality of hybrid metal halides (HMHs), including organic-inorganic perovskites, using a combination of chemically-informed feature engineering and advanced class-imbalance handling techniques. The dataset, consisting of 494 HMH structures, is highly imbalanced across dimensionality classes (0D, 1D, 2D, 3D), posing significant challenges to predictive modeling. This dataset was later augmented to 1336 via the Synthetic Minority Oversampling Technique (SMOTE) to mitigate the effects of the class imbalance. We developed interaction-based descriptors and integrated them into a multi-stage workflow that combines feature selection, model stacking, and performance optimization to improve dimensionality prediction accuracy. Our approach significantly improves F1-scores for underrepresented classes, achieving robust cross-validation performance across all dimensionalities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Rationalization for Robust Causal Effect Estimation</title>
<link>https://arxiv.org/abs/2512.05373</link>
<guid>https://arxiv.org/abs/2512.05373</guid>
<content:encoded><![CDATA[
<div> Keywords: causal inference, text data, positivity assumption, token selection, treatment effect estimation<br /><br />Summary:<br /><br />This paper addresses challenges in using high-dimensional text data for causal inference, especially for adjusting confounding variables in treatment effect estimation. The authors highlight how the positivity assumption—requiring sufficient overlap in treatment assignment across confounder values—is often violated when large textual data is transformed into feature spaces. This violation results from redundant or spurious textual features that increase dimensionality, causing extreme propensity scores, unstable weights, and inflated variance in causal effect estimates. To overcome these issues, the authors propose Confounding-Aware Token Rationalization (CATR), a novel framework that selects a sparse and necessary subset of tokens. CATR uses a residual-independence diagnostic to ensure that the selected tokens preserve sufficient confounding information for unconfoundedness. By removing irrelevant textual information while retaining essential confounder signals, CATR effectively mitigates positivity violations observed at the feature level and stabilizes downstream causal effect estimators. Experimental results on both synthetic datasets and a real-world medical dataset (MIMIC-III) show that CATR produces more accurate, stable, and interpretable causal effect estimates compared to existing methods, demonstrating its potential for practical applications in natural language processing-based causal inference. <div>
arXiv:2512.05373v1 Announce Type: new 
Abstract: Recent advances in natural language processing have enabled the increasing use of text data in causal inference, particularly for adjusting confounding factors in treatment effect estimation. Although high-dimensional text can encode rich contextual information, it also poses unique challenges for causal identification and estimation. In particular, the positivity assumption, which requires sufficient treatment overlap across confounder values, is often violated at the observational level, when massive text is represented in feature spaces. Redundant or spurious textual features inflate dimensionality, producing extreme propensity scores, unstable weights, and inflated variance in effect estimates. We address these challenges with Confounding-Aware Token Rationalization (CATR), a framework that selects a sparse necessary subset of tokens using a residual-independence diagnostic designed to preserve confounding information sufficient for unconfoundedness. By discarding irrelevant texts while retaining key signals, CATR mitigates observational-level positivity violations and stabilizes downstream causal effect estimators. Experiments on synthetic data and a real-world study using the MIMIC-III database demonstrate that CATR yields more accurate, stable, and interpretable causal effect estimates than existing baselines.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>China Regional 3km Downscaling Based on Residual Corrective Diffusion Model</title>
<link>https://arxiv.org/abs/2512.05377</link>
<guid>https://arxiv.org/abs/2512.05377</guid>
<content:encoded><![CDATA[
<div> Keywords: statistical downscaling, diffusion models, weather forecasting, high-resolution, deep learning<br /><br />Summary:  
This paper addresses the challenge of efficiently producing high-resolution weather forecasts through statistical downscaling, which builds statistical relationships between low- and high-resolution historical data using models. The authors focus on a diffusion-based downscaling framework called CorrDiff, enhancing and extending it by considering a region nearly 20 times larger than in the original work. Unlike the previous version that targeted only surface variables, this study includes high-level atmospheric variables across six pressure levels. A global residual connection is introduced to improve the model's accuracy. The approach generates 3 km resolution forecasts for the China region by downscaling 25 km global grid forecasts from CMA-GFS, an operational global model by the China Meteorological Administration, and SFF, a data-driven deep learning weather model using Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional numerical model, is used as the baseline for comparison. Experimental results show that CorrDiff downscaled forecasts outperform CMA-MESO's direct forecasts in terms of mean absolute error (MAE) for target variables. Additionally, radar composite reflectivity predictions exhibit that CorrDiff, as a generative model, captures fine-scale details more realistically compared to deterministic regression approaches, suggesting improved realism in small-scale weather feature generation. <div>
arXiv:2512.05377v1 Announce Type: new 
Abstract: A fundamental challenge in numerical weather prediction is to efficiently produce high-resolution forecasts. A common solution is applying downscaling methods, which include dynamical downscaling and statistical downscaling, to the outputs of global models. This work focuses on statistical downscaling, which establishes statistical relationships between low-resolution and high-resolution historical data using statistical models. Deep learning has emerged as a powerful tool for this task, giving rise to various high-performance super-resolution models, which can be directly applied for downscaling, such as diffusion models and Generative Adversarial Networks. This work relies on a diffusion-based downscaling framework named CorrDiff. In contrast to the original work of CorrDiff, the region considered in this work is nearly 20 times larger, and we not only consider surface variables as in the original work, but also encounter high-level variables (six pressure levels) as target downscaling variables. In addition, a global residual connection is added to improve accuracy. In order to generate the 3km forecasts for the China region, we apply our trained models to the 25km global grid forecasts of CMA-GFS, an operational global model of the China Meteorological Administration (CMA), and SFF, a data-driven deep learning-based weather model developed from Spherical Fourier Neural Operators (SFNO). CMA-MESO, a high-resolution regional model, is chosen as the baseline model. The experimental results demonstrate that the forecasts downscaled by our method generally outperform the direct forecasts of CMA-MESO in terms of MAE for the target variables. Our forecasts of radar composite reflectivity show that CorrDiff, as a generative model, can generate fine-scale details that lead to more realistic predictions compared to the corresponding deterministic regression models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalization Beyond Benchmarks: Evaluating Learnable Protein-Ligand Scoring Functions on Unseen Targets</title>
<link>https://arxiv.org/abs/2512.05386</link>
<guid>https://arxiv.org/abs/2512.05386</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, protein-ligand scoring, generalization, self-supervised pretraining, evaluation benchmarks  

<br /><br />Summary:  
This work addresses the challenge of ensuring that machine-learned protein-ligand scoring functions reliably generalize to novel protein targets, a critical factor in molecular design. The authors evaluate the generalization performance of state-of-the-art scoring functions using dataset splits designed to mimic real-world scenarios where few known protein structures and experimental affinity measurements exist for the targets. Their analysis reveals that commonly used benchmarks are insufficient as they do not accurately reflect the true difficulties in generalizing to new targets. To mitigate this gap, they explore the potential of large-scale self-supervised pretraining and provide initial evidence suggesting it can enhance generalization capabilities. Additionally, they examine simpler approaches that utilize limited test-target data to boost scoring function performance, highlighting practical strategies for improvement. The findings emphasize the need for developing more rigorous evaluation protocols tailored to realistic settings in order to better assess scoring function robustness. Ultimately, the study offers important insights and guidance for designing protein-ligand scoring functions with improved predictive power on previously unseen protein targets, advancing the reliability of machine learning tools in molecular discovery. <div>
arXiv:2512.05386v1 Announce Type: new 
Abstract: As machine learning becomes increasingly central to molecular design, it is vital to ensure the reliability of learnable protein-ligand scoring functions on novel protein targets. While many scoring functions perform well on standard benchmarks, their ability to generalize beyond training data remains a significant challenge. In this work, we evaluate the generalization capability of state-of-the-art scoring functions on dataset splits that simulate evaluation on targets with a limited number of known structures and experimental affinity measurements. Our analysis reveals that the commonly used benchmarks do not reflect the true challenge of generalizing to novel targets. We also investigate whether large-scale self-supervised pretraining can bridge this generalization gap and we provide preliminary evidence of its potential. Furthermore, we probe the efficacy of simple methods that leverage limited test-target data to improve scoring function performance. Our findings underscore the need for more rigorous evaluation protocols and offer practical guidance for designing scoring functions with predictive power extending to novel protein targets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Smart Timing for Mining: A Deep Learning Framework for Bitcoin Hardware ROI Prediction</title>
<link>https://arxiv.org/abs/2512.05402</link>
<guid>https://arxiv.org/abs/2512.05402</guid>
<content:encoded><![CDATA[
<div> Keywords: Bitcoin mining, ASIC hardware, ROI prediction, Transformer model, time series classification<br /><br />Summary:<br /><br />This paper addresses the challenge of optimally timing the acquisition of Bitcoin mining hardware, particularly ASIC machines, in response to volatile market conditions, fast technological changes, and cyclical revenue patterns. The authors identify a gap in existing research and tools that do not offer guidance on when to purchase mining hardware for profitable outcomes. To fill this gap, they formulate the hardware buying decision as a time series classification problem, categorizing returns on investment (ROI) within one year as profitable (ROI ≥ 1), marginal (0 < ROI < 1), or unprofitable (ROI ≤ 0). The study introduces MineROI-Net, a Transformer-based neural network architecture designed to capture multi-scale temporal patterns in mining profitability data. Evaluated on historical data from 20 different ASIC miners released over the period 2015–2024, MineROI-Net outperforms baseline models including LSTM and TSLANet, achieving an accuracy of 83.7% and a macro F1-score of 83.1%. Most notably, the model attains high precision: 93.6% in detecting unprofitable periods and 98.5% for profitable ones, effectively minimizing costly misclassifications. The work presents MineROI-Net as a practical, open-source tool for miners and investors, enabling more data-driven and financially informed hardware acquisition decisions in a capital-intensive industry. The model and code are publicly available at https://github.com/AMAAI-Lab/MineROI-Net. <div>
arXiv:2512.05402v1 Announce Type: new 
Abstract: Bitcoin mining hardware acquisition requires strategic timing due to volatile markets, rapid technological obsolescence, and protocol-driven revenue cycles. Despite mining's evolution into a capital-intensive industry, there is little guidance on when to purchase new Application-Specific Integrated Circuit (ASIC) hardware, and no prior computational frameworks address this decision problem. We address this gap by formulating hardware acquisition as a time series classification task, predicting whether purchasing ASIC machines yields profitable (Return on Investment (ROI) >= 1), marginal (0 < ROI < 1), or unprofitable (ROI <= 0) returns within one year. We propose MineROI-Net, an open source Transformer-based architecture designed to capture multi-scale temporal patterns in mining profitability. Evaluated on data from 20 ASIC miners released between 2015 and 2024 across diverse market regimes, MineROI-Net outperforms LSTM-based and TSLANet baselines, achieving 83.7% accuracy and 83.1% macro F1-score. The model demonstrates strong economic relevance, achieving 93.6% precision in detecting unprofitable periods and 98.5% precision for profitable ones, while avoiding misclassification of profitable scenarios as unprofitable and vice versa. These results indicate that MineROI-Net offers a practical, data-driven tool for timing mining hardware acquisitions, potentially reducing financial risk in capital-intensive mining operations. The model is available through: https://github.com/AMAAI-Lab/MineROI-Net.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RevoNAD: Reflective Evolutionary Exploration for Neural Architecture Design</title>
<link>https://arxiv.org/abs/2512.05403</link>
<guid>https://arxiv.org/abs/2512.05403</guid>
<content:encoded><![CDATA[
<div> Neural Architecture Design, Large Language Models, Evolutionary Algorithms, Multi-expert Consensus, Pareto Optimization<br /><br />Summary: Recent advancements in leveraging large language models (LLMs) have enabled Neural Architecture Design (NAD) systems to generate novel architectures beyond manually predefined search spaces. However, current LLM-driven generation approaches face challenges due to the discrete and non-differentiable nature of token-level design loops, which limits smooth feedback integration and causes issues such as mode collapse and infeasible designs. To address these challenges, the paper introduces RevoNAD, a reflective evolutionary orchestrator that effectively combines LLM-based reasoning with feedback-driven architectural search. RevoNAD implements a Multi-round Multi-expert Consensus mechanism to convert isolated design rules into meaningful architectural clues, enhancing design reliability. It also uses Adaptive Reflective Exploration, dynamically adjusting exploration based on reward variance to balance exploration and refinement of architectures. Additionally, RevoNAD employs Pareto-guided Evolutionary Selection to promote architectures that optimize multiple objectives simultaneously, including accuracy, efficiency, latency, confidence, and structural diversity. Extensive experiments on datasets such as CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape demonstrate that RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate its practical reliability and deployability in neural architecture design tasks. <div>
arXiv:2512.05403v1 Announce Type: new 
Abstract: Recent progress in leveraging large language models (LLMs) has enabled Neural Architecture Design (NAD) systems to generate new architecture not limited from manually predefined search space. Nevertheless, LLM-driven generation remains challenging: the token-level design loop is discrete and non-differentiable, preventing feedback from smoothly guiding architectural improvement. These methods, in turn, commonly suffer from mode collapse into redundant structures or drift toward infeasible designs when constructive reasoning is not well grounded. We introduce RevoNAD, a reflective evolutionary orchestrator that effectively bridges LLM-based reasoning with feedback-aligned architectural search. First, RevoNAD presents a Multi-round Multi-expert Consensus to transfer isolated design rules into meaningful architectural clues. Then, Adaptive Reflective Exploration adjusts the degree of exploration leveraging reward variance; it explores when feedback is uncertain and refines when stability is reached. Finally, Pareto-guided Evolutionary Selection effectively promotes architectures that jointly optimize accuracy, efficiency, latency, confidence, and structural diversity. Across CIFAR10, CIFAR100, ImageNet16-120, COCO-5K, and Cityscape, RevoNAD achieves state-of-the-art performance. Ablation and transfer studies further validate the effectiveness of RevoNAD in allowing practically reliable, and deployable neural architecture design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sepsis Prediction Using Graph Convolutional Networks over Patient-Feature-Value Triplets</title>
<link>https://arxiv.org/abs/2512.05416</link>
<guid>https://arxiv.org/abs/2512.05416</guid>
<content:encoded><![CDATA[
<div> Sepsis, Electronic Health Records, Graph Convolutional Network, Patient Embeddings, Early Warning<br /><br />Summary:<br /><br />1. Sepsis remains a significant cause of morbidity and mortality in intensive care units, with early detection challenged by complex and heterogeneous EHR data.  
2. The study introduces Triplet-GCN, a novel single-branch graph convolutional network designed to represent each patient encounter as patient-feature-value triplets, forming a bipartite graph between patients and features.  
3. Preprocessing includes median imputation and standardization for numeric data, effect coding for binary data, and mode imputation with low-dimensional embeddings for rare categorical features, while patient nodes are initialized with summary statistics. The edge values retain actual measurements to preserve contextual information about features.  
4. The model learns patient embeddings via graph convolution followed by a lightweight multilayer perceptron and is evaluated on a retrospective, multi-center Chinese cohort of 648 patients with a 70/30 train-test split from three tertiary hospitals.  
5. Triplet-GCN demonstrates superior performance compared to traditional tabular models (KNN, SVM, XGBoost, Random Forest) in terms of discrimination, balanced error metrics, and sensitivity-specificity trade-offs, suggesting that encoding EHR as triplets and leveraging graph structures enhances sepsis early warning and risk stratification. <div>
arXiv:2512.05416v1 Announce Type: new 
Abstract: In the intensive care setting, sepsis continues to be a major contributor to patient illness and death; however, its timely detection is hindered by the complex, sparse, and heterogeneous nature of electronic health record (EHR) data. We propose Triplet-GCN, a single-branch graph convolutional model that represents each encounter as patient-feature-value triplets, constructs a bipartite EHR graph, and learns patient embeddings via a Graph Convolutional Network (GCN) followed by a lightweight multilayer perceptron (MLP). The pipeline applies type-specific preprocessing -- median imputation and standardization for numeric variables, effect coding for binary features, and mode imputation with low-dimensional embeddings for rare categorical attributes -- and initializes patient nodes with summary statistics, while retaining measurement values on edges to preserve "who measured what and by how much". In a retrospective, multi-center Chinese cohort (N = 648; 70/30 train-test split) drawn from three tertiary hospitals, Triplet-GCN consistently outperforms strong tabular baselines (KNN, SVM, XGBoost, Random Forest) across discrimination and balanced error metrics, yielding a more favorable sensitivity-specificity trade-off and improved overall utility for early warning. These findings indicate that encoding EHR as triplets and propagating information over a patient-feature graph produce more informative patient representations than feature-independent models, offering a simple, end-to-end blueprint for deployable sepsis risk stratification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TS-HINT: Enhancing Semiconductor Time Series Regression Using Attention Hints From Large Language Model Reasoning</title>
<link>https://arxiv.org/abs/2512.05419</link>
<guid>https://arxiv.org/abs/2512.05419</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Foundation Model, chemical mechanical polishing, material removal rate, few-shot learning, attention mechanism<br /><br />Summary:<br /><br />This paper addresses the limitations of existing data-driven approaches for estimating the material removal rate (MRR) in semiconductor manufacturing processes like chemical mechanical polishing (CMP), which traditionally rely on static feature extraction from time series data, ignoring temporal dynamics. Recognizing the need for models effective in low-data environments, the authors propose TS-Hint, a novel Time Series Foundation Model (TSFM) framework. TS-Hint incorporates chain-of-thought reasoning to enhance learning by providing attention hints during training, which leverage both attention mechanism data and saliency information. This approach enables the model to focus more effectively on relevant temporal features within multivariate time series data. Experimentally, TS-Hint demonstrates superior performance, particularly in few-shot learning scenarios where data availability is limited, showcasing its ability to learn directly from complex time series inputs without the extensive data typically required. These findings suggest TS-Hint’s potential as a powerful tool for dynamic process monitoring and control in semiconductor manufacturing, overcoming previous constraints by embedding temporal understanding and attention-guided learning in the framework. <div>
arXiv:2512.05419v1 Announce Type: new 
Abstract: Existing data-driven methods rely on the extraction of static features from time series to approximate the material removal rate (MRR) of semiconductor manufacturing processes such as chemical mechanical polishing (CMP). However, this leads to a loss of temporal dynamics. Moreover, these methods require a large amount of data for effective training. In this paper, we propose TS-Hint, a Time Series Foundation Model (TSFM) framework, integrated with chain-of-thought reasoning which provides attention hints during training based on attention mechanism data and saliency data. Experimental results demonstrate the effectiveness of our model in limited data settings via few-shot learning and can learn directly from multivariate time series features.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IdealTSF: Can Non-Ideal Data Contribute to Enhancing the Performance of Time Series Forecasting Models?</title>
<link>https://arxiv.org/abs/2512.05442</link>
<guid>https://arxiv.org/abs/2512.05442</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, deep learning, negative samples, IdealTSF framework, adversarial optimization  

<br /><br />Summary:  
This paper addresses the challenges of time series forecasting, particularly focusing on issues caused by missing values and anomalies in sequential data. Traditional methods have emphasized extracting features from sequences or utilizing suboptimal data as positive samples for knowledge transfer. However, the authors propose that leveraging these non-ideal negative samples can significantly improve event prediction. To this end, they introduce the IdealTSF framework, which uniquely incorporates both ideal positive and negative samples into the forecasting process. The framework operates through three stages: pretraining, training, and optimization. Initially, the model undergoes pretraining by learning from negative sample data to extract meaningful knowledge. During the training phase, sequence data are transformed into ideal positive samples to enhance model robustness. Furthermore, the framework integrates a negative optimization mechanism that employs adversarial disturbances, helping the model better handle noisy or low-quality data. Experimental results demonstrate that incorporating negative samples greatly enhances the performance of the basic attention architecture used in time series forecasting. Overall, IdealTSF is especially advantageous in applications where data quality is compromised, providing a more effective solution for forecasting tasks under challenging data conditions. <div>
arXiv:2512.05442v1 Announce Type: new 
Abstract: Deep learning has shown strong performance in time series forecasting tasks. However, issues such as missing values and anomalies in sequential data hinder its further development in prediction tasks. Previous research has primarily focused on extracting feature information from sequence data or addressing these suboptimal data as positive samples for knowledge transfer. A more effective approach would be to leverage these non-ideal negative samples to enhance event prediction. In response, this study highlights the advantages of non-ideal negative samples and proposes the IdealTSF framework, which integrates both ideal positive and negative samples for time series forecasting. IdealTSF consists of three progressive steps: pretraining, training, and optimization. It first pretrains the model by extracting knowledge from negative sample data, then transforms the sequence data into ideal positive samples during training. Additionally, a negative optimization mechanism with adversarial disturbances is applied. Extensive experiments demonstrate that negative sample data unlocks significant potential within the basic attention architecture for time series forecasting. Therefore, IdealTSF is particularly well-suited for applications with noisy samples or low-quality data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Ensemble Learning Balances Accuracy and Overfitting: A Bias-Variance Perspective on Tabular Data</title>
<link>https://arxiv.org/abs/2512.05469</link>
<guid>https://arxiv.org/abs/2512.05469</guid>
<content:encoded><![CDATA[
<div> Keywords: ensemble models, generalization gap, tabular classification, overfitting, dataset complexity indicators<br /><br />Summary:<br /><br />This study investigates how ensemble models balance accuracy and overfitting across four tabular classification datasets: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. It uses repeated stratified cross-validation with statistical significance testing to compare linear models, a single decision tree, and nine ensemble methods. The results indicate that ensembles maintain high accuracy without large generalization gaps mainly by reducing variance through averaging or controlled boosting. For nearly linear, clean datasets, linear models already generalize well, so ensembles add little improvement. However, on datasets with significant nonlinear patterns, tree-based ensembles enhance test accuracy by 5 to 7 percentage points while keeping generalization gaps under 3%. On noisy or highly imbalanced datasets, ensembles stay competitive but need proper regularization to avoid fitting noise or majority class biases. The study introduces simple dataset complexity measures—such as linearity score, Fisher ratio, and noise estimate—to explain when ensembles are effective at variance control. Overall, these findings offer practical guidance for selecting models in real-world tabular machine learning, clarifying conditions under which ensembles provide the best trade-off between accuracy and overfitting. <div>
arXiv:2512.05469v1 Announce Type: new 
Abstract: Ensemble models often achieve higher accuracy than single learners, but their ability to maintain small generalization gaps is not always well understood. This study examines how ensembles balance accuracy and overfitting across four tabular classification tasks: Breast Cancer, Heart Disease, Pima Diabetes, and Credit Card Fraud. Using repeated stratified cross validation with statistical significance testing, we compare linear models, a single decision tree, and nine ensemble methods. The results show that ensembles can reach high accuracy without large gaps by reducing variance through averaging or controlled boosting. On nearly linear and clean data, linear models already generalize well and ensembles offer little additional benefit. On datasets with meaningful nonlinear structure, tree based ensembles increase test accuracy by 5 to 7 points while keeping gaps below 3 percent. On noisy or highly imbalanced datasets, ensembles remain competitive but require regularization to avoid fitting noise or majority class patterns. We also compute simple dataset complexity indicators, such as linearity score, Fisher ratio, and noise estimate, which explain when ensembles are likely to control variance effectively. Overall, the study provides a clear view of how and when ensembles maintain high accuracy while keeping overfitting low, offering practical guidance for model selection in real world tabular applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PERM EQ x GRAPH EQ: Equivariant Neural Networks for Quantum Molecular Learning</title>
<link>https://arxiv.org/abs/2512.05475</link>
<guid>https://arxiv.org/abs/2512.05475</guid>
<content:encoded><![CDATA[
<div> Keywords: Geometric Quantum Machine Learning, molecular geometry, equivariance, permutational symmetry, graph embedding<br /><br />Summary:<br /><br />1. The study compares the performance of various Geometric Quantum Machine Learning (QML) models arranged by molecular geometry hierarchy.<br />2. Two molecular datasets are analyzed: a simple linear molecule (LiH) and a trigonal pyramidal molecule (NH3), focusing on their accuracy and generalizability.<br />3. A classical equivariant model is used as a baseline to benchmark the quantum models.<br />4. The paper investigates QML models with different symmetry considerations—no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance—to understand their impact on performance.<br />5. Results indicate that graph embedding of features enhances the trainability of geometric datasets.<br />6. Among the models tested, permutational symmetric embedding is identified as the most generalizable QML approach for learning geometric molecular data.<br />7. The study reveals that molecular geometry plays a crucial role in determining the suitable model for generalizability in quantum machine learning tasks related to molecules. <div>
arXiv:2512.05475v1 Announce Type: new 
Abstract: In hierarchal order of molecular geometry, we compare the performances of Geometric Quantum Machine Learning models. Two molecular datasets are considered: the simplistic linear shaped LiH-molecule and the trigonal pyramidal molecule NH3. Both accuracy and generalizability metrics are considered. A classical equivariant model is used as a baseline for the performance comparison. The comparative performance of Quantum Machine Learning models with no symmetry equivariance, rotational and permutational equivariance, and graph embedded permutational equivariance is investigated. The performance differentials and the molecular geometry in question reveals the criteria for choice of models for generalizability. Graph embedding of features is shown to be an effective pathway to greater trainability for geometric datasets. Permutational symmetric embedding is found to be the most generalizable quantum Machine Learning model for geometric learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Turbulence Regression</title>
<link>https://arxiv.org/abs/2512.05483</link>
<guid>https://arxiv.org/abs/2512.05483</guid>
<content:encoded><![CDATA[
<div> Keywords: Air turbulence, NeuTucker, Tucker decomposition, wind field data, spatio-temporal interactions<br /><br />Summary:<br /><br />Air turbulence is characterized by irregular and disordered airflow caused by abrupt changes in velocity, pressure, or direction, particularly difficult to predict at low altitudes due to complex influencing factors. Traditional turbulence prediction methods relying solely on wind profile radar data face significant challenges in accurately capturing turbulence states. To address this, the paper introduces a NeuTucker (NeuTucF) decomposition model that processes discretized three-dimensional wind field data. The model is designed to handle continuous yet sparse data by discretizing continuous inputs, making them compatible with the NeuTucker framework. Central to this approach is the construction of a four-dimensional Tucker interaction tensor, which captures all possible spatial and temporal interactions across different elevations and the three components of wind speed. This low-rank Tucker decomposition model, based on a Tucker neural network, effectively extracts latent interactions within the 3D wind field data. When applied to real-world datasets with missing observations, the discretized NeuTucF model outperforms several common regression models in estimating missing values, demonstrating superior predictive accuracy and robustness in turbulence modeling under limited observational conditions. <div>
arXiv:2512.05483v1 Announce Type: new 
Abstract: Air turbulence refers to the disordered and irregular motion state generated by drastic changes in velocity, pressure, or direction during airflow. Various complex factors lead to intricate low-altitude turbulence outcomes. Under current observational conditions, especially when using only wind profile radar data, traditional methods struggle to accurately predict turbulence states. Therefore, this paper introduces a NeuTucker decomposition model utilizing discretized data. Designed for continuous yet sparse three-dimensional wind field data, it constructs a low-rank Tucker decomposition model based on a Tucker neural network to capture the latent interactions within the three-dimensional wind field data. Therefore, two core ideas are proposed here: 1) Discretizing continuous input data to adapt to models like NeuTucF that require discrete data inputs. 2) Constructing a four-dimensional Tucker interaction tensor to represent all possible spatio-temporal interactions among different elevations and three-dimensional wind speeds. In estimating missing observations in real datasets, this discretized NeuTucF model demonstrates superior performance compared to various common regression models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRASP: Graph Reasoning Agents for Systems Pharmacology with Human-in-the-Loop</title>
<link>https://arxiv.org/abs/2512.05502</link>
<guid>https://arxiv.org/abs/2512.05502</guid>
<content:encoded><![CDATA[
<div> Quantitative Systems Pharmacology, QSP modeling, graph reasoning, human-in-the-loop, biological knowledge graphs<br /><br />Summary:  
The article introduces GRASP, a novel multi-agent, graph-reasoning framework designed to improve Quantitative Systems Pharmacology (QSP) modeling, which is critical for drug development but traditionally time-intensive. GRASP encodes QSP models as typed biological knowledge graphs and translates them into executable MATLAB/SimBiology code, ensuring preservation of units, mass balance, and physiological constraints. It operates via a two-phase workflow: "Understanding," which reconstructs graphs from legacy code, and "Action," which enables constraint-checked, language-driven modifications using a state machine that iteratively validates model changes. The system employs breadth-first search (BFS) parameter alignment around new entities to identify dependent quantities and suggest biologically plausible defaults, supporting automatic execution and diagnostics until convergence. In head-to-head comparisons judged by a large language model (LLM), GRASP surpasses subject matter expert-guided chain-of-thought (CoT) and tree-of-thought (ToT) baselines in biological plausibility, mathematical correctness, structural fidelity, and code quality, scoring approximately 9–10 out of 10 versus 5–7 for baselines. BFS alignment achieves an F1 score of 0.95 in discovery of dependencies, unit consistency, and value ranges. This approach demonstrates that graph-structured, agent-driven workflows with human interaction can accelerate and rigorously support QSP model development, allowing domain experts to use natural language specifications without compromising biomedical fidelity. <div>
arXiv:2512.05502v1 Announce Type: new 
Abstract: Quantitative Systems Pharmacology (QSP) modeling is essential for drug development but it requires significant time investment that limits the throughput of domain experts. We present \textbf{GRASP} -- a multi-agent, graph-reasoning framework with a human-in-the-loop conversational interface -- that encodes QSP models as typed biological knowledge graphs and compiles them to executable MATLAB/SimBiology code while preserving units, mass balance, and physiological constraints. A two-phase workflow -- \textsc{Understanding} (graph reconstruction of legacy code) and \textsc{Action} (constraint-checked, language-driven modification) -- is orchestrated by a state machine with iterative validation. GRASP performs breadth-first parameter-alignment around new entities to surface dependent quantities and propose biologically plausible defaults, and it runs automatic execution/diagnostics until convergence. In head-to-head evaluations using LLM-as-judge, GRASP outperforms SME-guided CoT and ToT baselines across biological plausibility, mathematical correctness, structural fidelity, and code quality (\(\approx\)9--10/10 vs.\ 5--7/10). BFS alignment achieves F1 = 0.95 for dependency discovery, units, and range. These results demonstrate that graph-structured, agentic workflows can make QSP model development both accessible and rigorous, enabling domain experts to specify mechanisms in natural language without sacrificing biomedical fidelity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credal and Interval Deep Evidential Classifications</title>
<link>https://arxiv.org/abs/2512.05526</link>
<guid>https://arxiv.org/abs/2512.05526</guid>
<content:encoded><![CDATA[
<div> Uncertainty Quantification, Evidential Deep Learning, Credal Sets, Epistemic Uncertainty, Out-of-Distribution Detection<br /><br />Summary:  
This paper addresses the critical challenge of Uncertainty Quantification (UQ) in Artificial Intelligence classification tasks by proposing two novel methods: Credal Deep Evidential Classification (CDEC) and Interval Deep Evidential Classification (IDEC). Both techniques utilize advanced theoretical constructs—credal sets and intervals of evidential predictive distributions—to manage and represent uncertainty, specifically targeting epistemic (reducible) and aleatoric (irreducible) uncertainties. Unlike traditional models, CDEC and IDEC can abstain from making classifications when uncertainty exceeds defined thresholds, enhancing reliability by flagging excessive uncertainty. When uncertainties are within acceptable bounds, these methods deliver sets of probable labels accompanied by strong probabilistic guarantees, improving decision robustness. The models are trained through standard backpropagation with a specialized loss function based on evidential theory, effectively overcoming limitations from prior evidential deep learning approaches. Extensive experimentation on datasets like MNIST, CIFAR-10, CIFAR-100, and their related out-of-distribution shifts (e.g., F-MNIST, SVHN, TinyImageNet) demonstrates that CDEC and IDEC not only maintain competitive accuracy but also achieve state-of-the-art performance in out-of-distribution detection driven by epistemic and total uncertainty. Furthermore, an ablation study on ensemble size reveals that CDEC provides stable uncertainty estimates even with a relatively small ensemble, underlining efficiency alongside accuracy. <div>
arXiv:2512.05526v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) presents a pivotal challenge in the field of Artificial Intelligence (AI), profoundly impacting decision-making, risk assessment and model reliability. In this paper, we introduce Credal and Interval Deep Evidential Classifications (CDEC and IDEC, respectively) as novel approaches to address UQ in classification tasks. CDEC and IDEC leverage a credal set (closed and convex set of probabilities) and an interval of evidential predictive distributions, respectively, allowing us to avoid overfitting to the training data and to systematically assess both epistemic (reducible) and aleatoric (irreducible) uncertainties. When those surpass acceptable thresholds, CDEC and IDEC have the capability to abstain from classification and flag an excess of epistemic or aleatoric uncertainty, as relevant. Conversely, within acceptable uncertainty bounds, CDEC and IDEC provide a collection of labels with robust probabilistic guarantees. CDEC and IDEC are trained using standard backpropagation and a loss function that draws from the theory of evidence. They overcome the shortcomings of previous efforts, and extend the current evidential deep learning literature. Through extensive experiments on MNIST, CIFAR-10 and CIFAR-100, together with their natural OoD shifts (F-MNIST/K-MNIST, SVHN/Intel, TinyImageNet), we show that CDEC and IDEC achieve competitive predictive accuracy, state-of-the-art OoD detection under epistemic and total uncertainty, and tight, well-calibrated prediction regions that expand reliably under distribution shift. An ablation over ensemble size further demonstrates that CDEC attains stable uncertainty estimates with only a small ensemble.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IDK-S: Incremental Distributional Kernel for Streaming Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.05531</link>
<guid>https://arxiv.org/abs/2512.05531</guid>
<content:encoded><![CDATA[
<div> Keywords: anomaly detection, data streams, incremental update, kernel mean embedding, Isolation Distributional Kernel<br /><br />Summary:<br />1. The paper introduces $\mathcal{IDK}$-$\mathcal{S}$, a novel Incremental Distributional Kernel designed specifically for streaming anomaly detection tasks that must adapt to evolving data distributions in real time.<br />2. It builds upon the Isolation Distributional Kernel, a previously developed offline anomaly detector known for outperforming classic methods such as Isolation Forest and Local Outlier Factor by leveraging a data-dependent kernel.<br />3. The main innovation lies in an efficient incremental update mechanism that allows the model to update dynamically without the computational expense of fully retraining the model after every data change.<br />4. This incremental approach significantly reduces computational overhead and accelerates processing times, maintaining detection accuracy at a statistically equivalent level compared to full retraining.<br />5. Extensive experiments conducted on thirteen benchmark datasets validate the proposed method, demonstrating that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior accuracy and is frequently an order of magnitude faster than current state-of-the-art streaming anomaly detection methods. <div>
arXiv:2512.05531v1 Announce Type: new 
Abstract: Anomaly detection on data streams presents significant challenges, requiring methods to maintain high detection accuracy among evolving distributions while ensuring real-time efficiency. Here we introduce $\mathcal{IDK}$-$\mathcal{S}$, a novel $\mathbf{I}$ncremental $\mathbf{D}$istributional $\mathbf{K}$ernel for $\mathbf{S}$treaming anomaly detection that effectively addresses these challenges by creating a new dynamic representation in the kernel mean embedding framework. The superiority of $\mathcal{IDK}$-$\mathcal{S}$ is attributed to two key innovations. First, it inherits the strengths of the Isolation Distributional Kernel, an offline detector that has demonstrated significant performance advantages over foundational methods like Isolation Forest and Local Outlier Factor due to the use of a data-dependent kernel. Second, it adopts a lightweight incremental update mechanism that significantly reduces computational overhead compared to the naive baseline strategy of performing a full model retraining. This is achieved without compromising detection accuracy, a claim supported by its statistical equivalence to the full retrained model. Our extensive experiments on thirteen benchmarks demonstrate that $\mathcal{IDK}$-$\mathcal{S}$ achieves superior detection accuracy while operating substantially faster, in many cases by an order of magnitude, than existing state-of-the-art methods.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Theoretical Foundation of Sparse Dictionary Learning in Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.05534</link>
<guid>https://arxiv.org/abs/2512.05534</guid>
<content:encoded><![CDATA[
<div> Sparse dictionary learning, mechanistic interpretability, neural networks, optimization landscape, feature disentanglement<br /><br />Summary:<br /><br />This paper addresses the understanding of sparse dictionary learning (SDL) methods used in mechanistic interpretability of neural networks. 1) It highlights the importance of interpreting neural representations, which often encode multiple concepts in superposition, and reviews existing SDL methods such as sparse autoencoders, transcoders, and crosscoders that enforce sparsity to disentangle these features. 2) The authors identify a gap in theoretical understanding, as prior work largely focuses on sparse autoencoders with tied-weight constraints, leaving other SDL variants unexplored theoretically. 3) They propose the first unified theoretical framework that formulates multiple SDL approaches as instances of a single optimization problem, thus bridging various methods under one formalism. 4) Through rigorous analysis, the paper investigates the optimization landscape of SDL, providing explanations for several phenomena observed empirically, like feature absorption, dead neurons, and the neuron resampling technique. 5) To validate their theoretical insights, the authors also conduct controlled experiments, demonstrating consistency between theory and observed behaviors in SDL methods, ultimately advancing foundational understanding of sparse representation learning in neural networks. <div>
arXiv:2512.05534v1 Announce Type: new 
Abstract: As AI models achieve remarkable capabilities across diverse domains, understanding what representations they learn and how they process information has become increasingly important for both scientific progress and trustworthy deployment. Recent works in mechanistic interpretability have shown that neural networks represent meaningful concepts as directions in their representation spaces and often encode many concepts in superposition. Various sparse dictionary learning (SDL) methods, including sparse autoencoders, transcoders, and crosscoders, address this by training auxiliary models with sparsity constraints to disentangle these superposed concepts into interpretable features. These methods have demonstrated remarkable empirical success but have limited theoretical understanding. Existing theoretical work is limited to sparse autoencoders with tied-weight constraints, leaving the broader family of SDL methods without formal grounding. In this work, we develop the first unified theoretical framework considering SDL as one unified optimization problem. We demonstrate how diverse methods instantiate the theoretical framwork and provide rigorous analysis on the optimization landscape. We provide the first theoretical explanations for some empirically observed phenomena, including feature absorption, dead neurons, and the neuron resampling technique. We further design controlled experiments to validate our theoretical results.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SCoNE: Spherical Consistent Neighborhoods Ensemble for Effective and Efficient Multi-View Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.05540</link>
<guid>https://arxiv.org/abs/2512.05540</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-view anomaly detection, consistent neighborhoods, data-dependent properties, computational efficiency, SCoNE

<br /><br />Summary:  
The paper addresses the core challenge in multi-view anomaly detection, which is to represent local neighborhoods of normal data points consistently across multiple views. Existing methods represent neighborhoods independently in each view and then align them through a learning process, but these approaches face two key problems: (1) they struggle to capture consistent neighbors accurately, especially when neighbor densities vary across views, leading to poor detection performance; and (2) the learning process has a high computational cost of order $\mathcal{O}(N^2)$, making it impractical for large datasets. To overcome these limitations, the authors propose a novel approach called Spherical Consistent Neighborhoods Ensemble (SCoNE). SCoNE uniquely represents consistent neighborhoods directly using multi-view instances without relying on intermediate representations. Additionally, it models neighborhoods with data-dependent sizes—larger neighborhoods in sparse regions and smaller ones in dense regions—enabling better consistency across views without any learning step. As a result, SCoNE achieves a linear time complexity of $\mathcal{O}(N)$. Extensive empirical evaluations demonstrate that SCoNE not only outperforms existing methods in anomaly detection accuracy but also runs orders of magnitude faster on large-scale datasets, making it both effective and efficient. <div>
arXiv:2512.05540v1 Announce Type: new 
Abstract: The core problem in multi-view anomaly detection is to represent local neighborhoods of normal instances consistently across all views. Recent approaches consider a representation of local neighborhood in each view independently, and then capture the consistent neighbors across all views via a learning process. They suffer from two key issues. First, there is no guarantee that they can capture consistent neighbors well, especially when the same neighbors are in regions of varied densities in different views, resulting in inferior detection accuracy. Second, the learning process has a high computational cost of $\mathcal{O}(N^2)$, rendering them inapplicable for large datasets. To address these issues, we propose a novel method termed \textbf{S}pherical \textbf{C}onsistent \textbf{N}eighborhoods \textbf{E}nsemble (SCoNE). It has two unique features: (a) the consistent neighborhoods are represented with multi-view instances directly, requiring no intermediate representations as used in existing approaches; and (b) the neighborhoods have data-dependent properties, which lead to large neighborhoods in sparse regions and small neighborhoods in dense regions. The data-dependent properties enable local neighborhoods in different views to be represented well as consistent neighborhoods, without learning. This leads to $\mathcal{O}(N)$ time complexity. Empirical evaluations show that SCoNE has superior detection accuracy and runs orders-of-magnitude faster in large datasets than existing approaches.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RoBoN: Routed Online Best-of-n for Test-Time Scaling with Multiple LLMs</title>
<link>https://arxiv.org/abs/2512.05542</link>
<guid>https://arxiv.org/abs/2512.05542</guid>
<content:encoded><![CDATA[
<div> Best-of-n, LLM inference, multi-LLM routing, reward model, test-time scaling<br /><br />Summary:<br /><br />1. The paper introduces RoBoN (Routed Online Best-of-n), a novel method for test-time scaling in large language model (LLM) inference that leverages multiple LLMs instead of relying on a single model as in traditional best-of-n approaches. <br /><br />2. RoBoN sequentially routes generations across a suite of models based on scores derived from a reward model and an agreement signal on predicted responses, enabling dynamic and online model selection during inference without additional training. <br /><br />3. This method maintains computational parity with standard best-of-n by using the same overall compute budget and can work with any plug-in reward model. <br /><br />4. Extensive experiments on reasoning benchmarks such as MATH500, OlympiadBench, MinervaMath, GSM8K, and MMLU demonstrate that RoBoN consistently outperforms traditional best-of-n applied to individual models, achieving up to 3.4% absolute accuracy improvement. <br /><br />5. Furthermore, RoBoN surpasses a uniform multi-model portfolio baseline, highlighting the advantage of exploiting model diversity at inference time to enhance performance without additional training or fine-tuning. <div>
arXiv:2512.05542v1 Announce Type: new 
Abstract: Best-of-$n$ is a widely used test-time scaling approach for LLM inference. Yet despite evidence that LLMs exhibit complementary strengths across tasks, traditionally best-of-$n$ relies on a single model to generate responses. We propose RoBoN (Routed Online Best-of-$n$), a sequential multi-LLM alternative to the prevailing single-model best-of-$n$. Given a suite of models $\{m_i\}_{i=1}^M$, RoBoN sequentially routes generations one-by-one across models, based on scores computed using a reward model and an agreement signal on the predicted responses. This online routing requires no additional training, keeps compute parity, and works with any plug-in reward model. Across reasoning benchmarks (MATH500, OlympiadBench, MinervaMath, GSM8K, MMLU), RoBoN consistently outperforms standard best-of-$n$ applied to each individual model for larger $n$, with gains of up to 3.4\% in absolute accuracy, and also improves over a uniform multi-model portfolio baseline. Our results indicate that diversity across models can be exploited at inference to improve best-of-$n$ performance over any constituent model alone, providing a simple, training-free path to test-time scaling with multiple LLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Local Fidelity Through Sampling and Modeling Nonlinearity</title>
<link>https://arxiv.org/abs/2512.05556</link>
<guid>https://arxiv.org/abs/2512.05556</guid>
<content:encoded><![CDATA[
<div> Black-box models, explainability, LIME, MARS, N-ball sampling  

<br /><br />Summary:  
This paper addresses the limitations of the Local Interpretable Model-agnostic Explanation (LIME) method, which assumes linear local decision boundaries and thus struggles to capture non-linear relationships in model predictions. To overcome this, the authors propose using Multivariate Adaptive Regression Splines (MARS) to model non-linear local boundaries, enhancing the fidelity of explanations by better capturing the behavior of the reference black-box model. Furthermore, the study introduces the N-ball sampling technique, which samples directly from the target distribution rather than reweighting samples as done in LIME, leading to improved faithfulness in explanations. The proposed method is evaluated on three UCI datasets employing various classifiers and different kernel widths, demonstrating superior performance compared to existing baselines. Experimental results report an average 37% reduction in root mean square error, indicating a significant improvement in local fidelity and explanation accuracy. Overall, this approach provides more reliable and faithful local explanations for complex black-box machine learning models, which is crucial for their adoption in high-stakes decision-making scenarios. <div>
arXiv:2512.05556v1 Announce Type: new 
Abstract: With the increasing complexity of black-box machine learning models and their adoption in high-stakes areas, it is critical to provide explanations for their predictions. Local Interpretable Model-agnostic Explanation (LIME) is a widely used technique that explains the prediction of any classifier by learning an interpretable model locally around the predicted instance. However, it assumes that the local decision boundary is linear and fails to capture the non-linear relationships, leading to incorrect explanations. In this paper, we propose a novel method that can generate high-fidelity explanations. Multivariate adaptive regression splines (MARS) is used to model non-linear local boundaries that effectively captures the underlying behavior of the reference model, thereby enhancing the local fidelity of the explanation. Additionally, we utilize the N-ball sampling technique, which samples directly from the desired distribution instead of reweighting samples as done in LIME, further improving the faithfulness score. We evaluate our method on three UCI datasets across different classifiers and varying kernel widths. Experimental results show that our method yields more faithful explanations compared to baselines, achieving an average reduction of 37% in root mean square error, significantly improving local fidelity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein distance based semi-supervised manifold learning and application to GNSS multi-path detection</title>
<link>https://arxiv.org/abs/2512.05567</link>
<guid>https://arxiv.org/abs/2512.05567</guid>
<content:encoded><![CDATA[
<div> optimal transport, semi-supervised learning, Wasserstein distance, image classification, multipath interference detection<br /><br />Summary:<br /><br />1. This study proposes a semi-supervised learning approach based on optimal transport theory to address the challenge of learning from scarce labeled image data using deep convolutional networks. 2. The method leverages an implicit graph-based transductive framework where the similarity between image samples is quantified using the Wasserstein distance, a metric from optimal transport. 3. This distance metric is integrated into a label propagation mechanism that guides the learning process, enhancing the ability to generalize from limited labeled data. 4. The effectiveness of the proposed approach is validated on a real-world GNSS (Global Navigation Satellite System) application, specifically focusing on the detection of multipath interference, which is a significant issue in satellite signal interpretation. 5. Experiments conducted under various signal conditions demonstrate that by carefully selecting hyperparameters related to the degree of semi-supervision and sensitivity to the metric, the model achieves significantly improved classification accuracy compared to fully supervised training methods, highlighting the potential of the approach for practical applications dealing with limited labeled data. <div>
arXiv:2512.05567v1 Announce Type: new 
Abstract: The main objective of this study is to propose an optimal transport based semi-supervised approach to learn from scarce labelled image data using deep convolutional networks. The principle lies in implicit graph-based transductive semi-supervised learning where the similarity metric between image samples is the Wasserstein distance. This metric is used in the label propagation mechanism during learning. We apply and demonstrate the effectiveness of the method on a GNSS real life application. More specifically, we address the problem of multi-path interference detection. Experiments are conducted under various signal conditions. The results show that for specific choices of hyperparameters controlling the amount of semi-supervision and the level of sensitivity to the metric, the classification accuracy can be significantly improved over the fully supervised training method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy Ratio Clipping as a Soft Global Constraint for Stable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05591</link>
<guid>https://arxiv.org/abs/2512.05591</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, entropy ratio, policy stability, PPO-Clip, distribution shift<br /><br />Summary:<br /><br />This paper addresses instability issues in large language model post-training using reinforcement learning, particularly under the off-policy training paradigm that causes distribution shift and leads to unstable gradients and fluctuating policy entropy. The widely used PPO-Clip method partially mitigates these problems by clipping importance weights but fails to account for the overall global distributional shift, especially in actions not sampled during training. To overcome these limitations, the authors introduce a novel global metric— the entropy ratio between current and previous policies—that quantifies the relative change in policy exploration during updates. They propose Entropy Ratio Clipping (ERC), a new mechanism imposing bidirectional constraints on this entropy ratio, effectively stabilizing policy updates at the global distribution level and regulating shifts in the probability of unsampled actions. ERC is integrated into existing reinforcement learning algorithms DAPO and GPPO. Experimental results on multiple benchmark tasks demonstrate that ERC consistently enhances performance and training stability compared to baseline methods. This work highlights the importance of global distributional metrics in stabilizing reinforcement learning updates for language model fine-tuning. <div>
arXiv:2512.05591v1 Announce Type: new 
Abstract: Large language model post-training relies on reinforcement learning to improve model capability and alignment quality. However, the off-policy training paradigm introduces distribution shift, which often pushes the policy beyond the trust region, leading to training instabilities manifested as fluctuations in policy entropy and unstable gradients. Although PPO-Clip mitigates this issue through importance clipping, it still overlooks the global distributional shift of actions. To address these challenges, we propose using the entropy ratio between the current and previous policies as a new global metric that effectively quantifies the relative change in policy exploration throughout updates. Building on this metric, we introduce an \textbf{Entropy Ratio Clipping} (ERC) mechanism that imposes bidirectional constraints on the entropy ratio. This stabilizes policy updates at the global distribution level and compensates for the inability of PPO-clip to regulate probability shifts of un-sampled actions. We integrate ERC into both DAPO and GPPO reinforcement learning algorithms. Experiments across multiple benchmarks show that ERC consistently improves performance.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales</title>
<link>https://arxiv.org/abs/2512.05620</link>
<guid>https://arxiv.org/abs/2512.05620</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning optimizers, hyperparameter transfer, matrix-level preconditioning, scaling laws, language model training<br /><br />Summary:<br />1. This paper investigates the scaling behavior of advanced deep learning optimizers that use matrix-level preconditioning, such as Shampoo, SOAP, and Muon, which have shown promising speedups over AdamW in smaller-scale experiments but whose effectiveness at larger scale remains unclear.<br />2. The authors build on prior work like $\mu$P to analyze how the optimal learning rate and weight decay should scale with model width and depth across various optimizers, considering the impact of techniques including blocking and grafting.<br />3. They find that applying $\mu$P-based learning rate scaling improves hyperparameter transfer but still exhibits deviations at finite model widths, which can be mitigated using blocking strategies and explicit spectral normalization.<br />4. For compute-optimal scaling, they demonstrate that setting weight decay inversely proportional to model width ($1/\mathrm{width}$) performs nearly optimally across all studied optimizers.<br />5. Applying these refined scaling rules, Muon and Shampoo consistently outperform AdamW by approximately 1.4× and 1.3× speedups, respectively, when training Llama-architecture language models ranging from 190 million to 1.4 billion parameters; incorrect scaling quickly negates these gains.<br />6. The study concludes that robust and principled hyperparameter transfer is crucial for reliably comparing optimizer performance at scale under realistic tuning budgets. <div>
arXiv:2512.05620v1 Announce Type: new 
Abstract: Several recently introduced deep learning optimizers utilizing matrix-level preconditioning have shown promising speedups relative to the current dominant optimizer AdamW, particularly in relatively small-scale experiments. However, efforts to validate and replicate their successes have reported mixed results. To better understand the effectiveness of these optimizers at scale, in this work we investigate how to scale preconditioned optimizers via hyperparameter transfer, building on prior works such as $\mu$P. We study how the optimal learning rate and weight decay should scale with model width and depth for a wide range of optimizers, including Shampoo, SOAP, and Muon, accounting for the impact of commonly used techniques such as blocking and grafting. We find that scaling the learning rate according to $\mu$P improves transfer, but can still suffer from significant finite-width deviations that cause drifting optimal learning rates, which we show can be mitigated by blocking and explicit spectral normalization. For compute-optimal scaling, we find scaling independent weight decay as $1/\mathrm{width}$ is nearly optimal across optimizers. Applying these scaling rules, we show Muon and Shampoo consistently achieve $1.4\times$ and $1.3\times$ speedup over AdamW for training Llama-architecture language models of sizes ranging from $190$M to $1.4$B, whereas the speedup vanishes rapidly with scale under incorrect scaling. Based on these results and further ablations, we argue that studying optimal hyperparameter transfer is essential for reliably comparing optimizers at scale given a realistic tuning budget.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bounded Graph Clustering with Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.05623</link>
<guid>https://arxiv.org/abs/2512.05623</guid>
<content:encoded><![CDATA[
<div> Keywords: community detection, graph neural networks, clustering, flexible cluster control, machine learning  

<br /><br />Summary:  
This paper addresses a common challenge in community detection using graph neural networks (GNNs), where users typically need to specify the number of clusters beforehand, a requirement that can be impractical or inaccurate. Unlike classical algorithms that can sometimes infer the number of clusters from data, most GNN-based methods struggle to return the exact number of clusters, even when specified. To overcome this, the authors propose a novel framework that enables flexible control over the number of communities identified by GNNs during training. This method allows users to define a plausible range for the number of clusters rather than needing an exact count, improving adaptability. Moreover, if an exact number is desired, the framework can enforce this constraint and reliably produce the specified number of clusters. The approach thus provides both flexibility and precision in community detection tasks and enhances the practical usability of GNNs in scenarios where cluster number specification is challenging or uncertain. This work contributes a principled solution to a key limitation in GNN-based clustering models, making them more robust and user-friendly for varied applications. <div>
arXiv:2512.05623v1 Announce Type: new 
Abstract: In community detection, many methods require the user to specify the number of clusters in advance since an exhaustive search over all possible values is computationally infeasible. While some classical algorithms can infer this number directly from the data, this is typically not the case for graph neural networks (GNNs): even when a desired number of clusters is specified, standard GNN-based methods often fail to return the exact number due to the way they are designed. In this work, we address this limitation by introducing a flexible and principled way to control the number of communities discovered by GNNs. Rather than assuming the true number of clusters is known, we propose a framework that allows the user to specify a plausible range and enforce these bounds during training. However, if the user wants an exact number of clusters, it may also be specified and reliably returned.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modular Jets for Supervised Pipelines: Diagnosing Mirage vs Identifiability</title>
<link>https://arxiv.org/abs/2512.05638</link>
<guid>https://arxiv.org/abs/2512.05638</guid>
<content:encoded><![CDATA[
<div> modular jets, regression pipelines, identifiability, mirage regimes, machine learning evaluation<br /><br />Summary:<br /><br />This paper addresses the limitations of classical supervised learning evaluations, which focus primarily on predictive risk over hold-out data without considering whether a model's internal modular decomposition is uniquely identifiable. The authors introduce the concept of Modular Jets, which are local linear response maps that characterize how individual modules within a regression or classification pipeline react to small, structured perturbations in the input. They define "mirage" regimes, where different modular decompositions produce indistinguishable jets and therefore cannot be distinguished based on data, contrasting these with "identifiable" regimes in which the modular decomposition is uniquely determined up to natural symmetries. A key theoretical contribution is a jet-identifiability theorem for two-module linear regression pipelines, demonstrating that under certain rank conditions and with access to module-level jets, the internal factorization is uniquely determined. This stands in contrast to traditional evaluation methods based solely on predictive risk, which admit many observationally equivalent but structurally different decompositions. To operationalize their framework, the authors present MoJet, an algorithm for estimating empirical jets and diagnosing mirage regimes, and validate their approach on both linear and deep regression as well as classification pipelines. <div>
arXiv:2512.05638v1 Announce Type: new 
Abstract: Classical supervised learning evaluates models primarily via predictive risk on hold-out data. Such evaluations quantify how well a function behaves on a distribution, but they do not address whether the internal decomposition of a model is uniquely determined by the data and evaluation design. In this paper, we introduce \emph{Modular Jets} for regression and classification pipelines. Given a task manifold (input space), a modular decomposition, and access to module-level representations, we estimate empirical jets, which are local linear response maps that describe how each module reacts to small structured perturbations of the input. We propose an empirical notion of \emph{mirage} regimes, where multiple distinct modular decompositions induce indistinguishable jets and thus remain observationally equivalent, and contrast this with an \emph{identifiable} regime, where the observed jets single out a decomposition up to natural symmetries. In the setting of two-module linear regression pipelines we prove a jet-identifiability theorem. Under mild rank assumptions and access to module-level jets, the internal factorisation is uniquely determined, whereas risk-only evaluation admits a large family of mirage decompositions that implement the same input-to-output map. We then present an algorithm (MoJet) for empirical jet estimation and mirage diagnostics, and illustrate the framework using linear and deep regression as well as pipeline classification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Data Filtering: Knowledge Localization for Capability Removal in LLMs</title>
<link>https://arxiv.org/abs/2512.05648</link>
<guid>https://arxiv.org/abs/2512.05648</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Gradient Routing, Selective Gradient Masking, label noise, unlearning<br /><br />Summary:<br /><br />1. Large Language Models (LLMs) carry dual-use risks, particularly when harmful data is mislabeled during training, making data filtering challenging and potentially ineffective at scale.<br />2. Prior work introduced Gradient Routing to isolate harmful or sensitive knowledge into specific parameters, enabling later removal of that knowledge.<br />3. This paper proposes an improved method called Selective Gradient Masking (SGTM), which zero-masks selected gradients so that target domain examples update only dedicated parameters, enhancing robustness to noisy labels.<br />4. SGTM’s effectiveness is demonstrated via two case studies: removing knowledge of one language from a bilingual synthetic dataset and removing biology knowledge from an English Wikipedia-trained model.<br />5. Compared to data filtering and previous Gradient Routing methods, SGTM achieves a better balance between retaining useful knowledge and forgetting target information despite labeling errors.<br />6. SGTM also shows strong resistance to adversarial fine-tuning, requiring significantly more fine-tuning steps to restore forgotten knowledge compared to finetuning-based unlearning methods.<br />7. The findings suggest SGTM is a promising pretraining-time safety mitigation, particularly valuable where label noise is unavoidable. <div>
arXiv:2512.05648v1 Announce Type: new 
Abstract: Large Language Models increasingly possess capabilities that carry dual-use risks. While data filtering has emerged as a pretraining-time mitigation, it faces significant challenges: labeling whether data is harmful is expensive at scale, and given improving sample efficiency with larger models, even small amounts of mislabeled content could give rise to dangerous capabilities. To address risks associated with mislabeled harmful content, prior work proposed Gradient Routing (Cloud et al., 2024) -- a technique that localizes target knowledge into a dedicated subset of model parameters so they can later be removed. We explore an improved variant of Gradient Routing, which we call Selective GradienT Masking (SGTM), with particular focus on evaluating its robustness to label noise. SGTM zero-masks selected gradients such that target domain examples only update their dedicated parameters. We test SGTM's effectiveness in two applications: removing knowledge of one language from a model trained on a bilingual synthetic dataset, and removing biology knowledge from a model trained on English Wikipedia. In both cases SGTM provides better retain/forget trade-off in the presence of labeling errors compared to both data filtering and a previously proposed instantiation of Gradient Routing. Unlike shallow unlearning approaches that can be quickly undone through fine-tuning, SGTM exhibits strong robustness to adversarial fine-tuning, requiring seven times more fine-tuning steps to reach baseline performance on the forget set compared to a finetuning-based unlearning method (RMU). Our results suggest SGTM provides a promising pretraining-time complement to existing safety mitigations, particularly in settings where label noise is unavoidable.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feasibility of AI-Assisted Programming for End-User Development</title>
<link>https://arxiv.org/abs/2512.05666</link>
<guid>https://arxiv.org/abs/2512.05666</guid>
<content:encoded><![CDATA[
<div> End-user development, low-code/no-code, AI-assisted coding, generative AI, web app development  

<br /><br />Summary:  
The paper explores the concept of AI-assisted end-user coding, where non-programmers create or adapt digital tools through natural language interaction with AI assistants, as opposed to traditional low-code/no-code (LCNC) visual programming platforms. It highlights how generative AI, especially large language models acting as programming copilots, can enhance end-user development by offering greater flexibility, faster creation, and improved reusability while reducing vendor lock-in. The study involved non-programmer participants tasked with building a basic web app using AI assistance. Most participants successfully completed the task within a reasonable timeframe and reported positive experiences with AI-assisted development. The findings suggest that AI-assisted end-user coding is a feasible and promising alternative or complement to LCNC approaches. The paper discusses the implications of this paradigm shift for organizational digital transformation, recommends directions for future research, and considers the impact on academic teaching related to software development and end-user programming. Overall, the research indicates that integrating generative AI into end-user development workflows may broaden accessibility and efficiency in creating customized digital solutions. <div>
arXiv:2512.05666v1 Announce Type: new 
Abstract: End-user development,where non-programmers create or adapt their own digital tools, can play a key role in driving digital transformation within organizations. Currently, low-code/no-code platforms are widely used to enable end-user development through visual programming, minimizing the need for manual coding. Recent advancements in generative AI, particularly large language model-based assistants and "copilots", open new possibilities, as they may enable end users to generate and refine programming code and build apps directly from natural language prompts. This approach, here referred to as AI-assisted end-user coding, promises greater flexibility, broader applicability, faster development, improved reusability, and reduced vendor lock-in compared to the established visual LCNC platforms. This paper investigates whether AI-assisted end-user coding is a feasible paradigm for end-user development, which may complement or even replace the LCNC model in the future. To explore this, we conducted a case study in which non-programmers were asked to develop a basic web app through interaction with AI assistants.The majority of study participants successfully completed the task in reasonable time and also expressed support for AI-assisted end-user coding as a viable approach for end-user development. The paper presents the study design, analyzes the outcomes, and discusses potential implications for practice, future research, and academic teaching.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Learning Multi-armed Bandits for Beam Tracking in 5G and 6G Networks</title>
<link>https://arxiv.org/abs/2512.05680</link>
<guid>https://arxiv.org/abs/2512.05680</guid>
<content:encoded><![CDATA[
<div> Beamforming, 5G, 6G, POMDP, online search<br /><br />Summary:<br /><br />1. The paper addresses beamforming in antenna arrays used in 5G and 6G networks, which require selecting optimal beams from large codebooks to maximize data rates for moving user equipments (UEs).<br /><br />2. Traditional analog beamforming relies on pre-configured codebooks and supervised learning classifiers to predict the next best beam based on historical beam selections; however, this method struggles with large codebooks and environmental effects like reflections and blockages.<br /><br />3. The authors propose reformulating the beam selection problem as a partially observable Markov decision process (POMDP), treating the beam codebook as the environment and the unobservable optimal beam as the hidden state.<br /><br />4. Their method selects candidate beams conditioned on a belief state derived from previously probed beams, effectively performing an online search to locate the optimal beam dynamically.<br /><br />5. Unlike previous supervised approaches, this POMDP-based method adapts to new or unforeseen UE trajectories and environmental changes, achieving significantly better performance and robustness in beam selection tasks. <div>
arXiv:2512.05680v1 Announce Type: new 
Abstract: Beamforming-capable antenna arrays with many elements enable higher data rates in next generation 5G and 6G networks. In current practice, analog beamforming uses a codebook of pre-configured beams with each of them radiating towards a specific direction, and a beam management function continuously selects \textit{optimal} beams for moving user equipments (UEs). However, large codebooks and effects caused by reflections or blockages of beams make an optimal beam selection challenging. In contrast to previous work and standardization efforts that opt for supervised learning to train classifiers to predict the next best beam based on previously selected beams we formulate the problem as a partially observable Markov decision process (POMDP) and model the environment as the codebook itself. At each time step, we select a candidate beam conditioned on the belief state of the unobservable optimal beam and previously probed beams. This frames the beam selection problem as an online search procedure that locates the moving optimal beam. In contrast to previous work, our method handles new or unforeseen trajectories and changes in the physical environment, and outperforms previous work by orders of magnitude.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BERTO: an Adaptive BERT-based Network Time Series Predictor with Operator Preferences in Natural Language</title>
<link>https://arxiv.org/abs/2512.05721</link>
<guid>https://arxiv.org/abs/2512.05721</guid>
<content:encoded><![CDATA[
<div> Keywords: BERTO, traffic prediction, energy optimization, transformer, natural language prompts

<br /><br />Summary:  
This paper introduces BERTO, a BERT-based framework designed for traffic prediction and energy optimization in cellular networks. 1) BERTO leverages transformer architectures to achieve high prediction accuracy in network traffic forecasting. 2) It incorporates a novel Balancing Loss Function, allowing network operators to fine-tune the trade-off between power savings and service performance. 3) The framework utilizes prompt-based customization with natural language inputs, enabling operators to guide the model in handling underprediction and overprediction according to their specific operational goals. 4) Experiments conducted on real-world datasets demonstrate that BERTO reduces mean squared error (MSE) by 4.13% compared to existing models, highlighting its improved predictive capability. 5) Additionally, BERTO supports flexible control over power consumption within a range of 1.4 kW and can manage service quality variations by up to 9 times, making it highly adaptable for intelligent Radio Access Network (RAN) deployments. Overall, BERTO offers a practical and customizable solution for optimizing energy use and maintaining performance in cellular networks through advanced AI techniques coupled with intuitive natural language guidance. <div>
arXiv:2512.05721v1 Announce Type: new 
Abstract: We introduce BERTO, a BERT-based framework for traffic prediction and energy optimization in cellular networks. Built on transformer architectures, BERTO delivers high prediction accuracy, while its Balancing Loss Function and prompt-based customization allow operators to adjust the trade-off between power savings and performance. Natural language prompts guide the model to manage underprediction and overprediction in accordance with the operator's intent. Experiments on real-world datasets show that BERTO improves upon existing models with a $4.13$\% reduction in MSE while introducing the feature of balancing competing objectives of power saving and performance through simple natural language inputs, operating over a flexible range of $1.4$ kW in power and up to $9\times$ variation in service quality, making it well suited for intelligent RAN deployments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching Language Models Mechanistic Explainability Through Arrow-Pushing</title>
<link>https://arxiv.org/abs/2512.05722</link>
<guid>https://arxiv.org/abs/2512.05722</guid>
<content:encoded><![CDATA[
<div> Keywords: chemical reaction mechanisms, language models, arrow pushing formalism, MechSMILES, reaction mechanism prediction<br /><br />Summary:<br /><br />This paper introduces a novel computational framework that teaches language models to predict chemical reaction mechanisms based on the arrow pushing formalism, which represents electron flow while respecting conservation laws. The authors developed MechSMILES, a compact textual format that encodes both molecular structures and the associated electron movements, facilitating mechanistic learning by language models. They trained models on four progressively challenging mechanism prediction tasks using datasets such as mech-USPTO-31k and FlowER. The models demonstrate strong performance, achieving over 95% top-3 accuracy on elementary step predictions and surpassing 73% and 93% complete mechanism retrieval accuracy on the mech-USPTO-31k and FlowER datasets, respectively. The mechanistic insight gained enables three significant applications: (1) serving as post-hoc validators for CASP systems to filter out chemically implausible reaction transformations; (2) enabling comprehensive atom-to-atom mapping that tracks all atoms—including hydrogens—throughout reactions; and (3) extracting catalyst-aware reaction templates that differentiate between recycled catalysts and non-participating spectator species. By grounding predictions in physically meaningful electron movements that ensure mass and charge conservation, this work paves the way toward more explainable, chemically valid computational synthesis planning and provides an architecture-agnostic framework for benchmarking reaction mechanism prediction. <div>
arXiv:2512.05722v1 Announce Type: new 
Abstract: Chemical reaction mechanisms provide crucial insight into synthesizability, yet current Computer-Assisted Synthesis Planning (CASP) systems lack mechanistic grounding. We introduce a computational framework for teaching language models to predict chemical reaction mechanisms through arrow pushing formalism, a century-old notation that tracks electron flow while respecting conservation laws. We developed MechSMILES, a compact textual format encoding molecular structure and electron flow, and trained language models on four mechanism prediction tasks of increasing complexity using mechanistic reaction datasets, such as mech-USPTO-31k and FlowER. Our models achieve more than 95\% top-3 accuracy on elementary step prediction and scores that surpass 73\% on mech-USPTO-31k, and 93\% on FlowER dataset for the retrieval of complete reaction mechanisms on our hardest task. This mechanistic understanding enables three key applications. First, our models serve as post-hoc validators for CASP systems, filtering chemically implausible transformations. Second, they enable holistic atom-to-atom mapping that tracks all atoms, including hydrogens. Third, they extract catalyst-aware reaction templates that distinguish recycled catalysts from spectator species. By grounding predictions in physically meaningful electron moves that ensure conservation of mass and charge, this work provides a pathway toward more explainable and chemically valid computational synthesis planning, while providing an architecture-agnostic framework for the benchmarking of mechanism prediction.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards agent-based-model informed neural networks</title>
<link>https://arxiv.org/abs/2512.05764</link>
<guid>https://arxiv.org/abs/2512.05764</guid>
<content:encoded><![CDATA[
arXiv:2512.05764v1 Announce Type: new 
Abstract: In this article, we present a framework for designing neural networks that remain consistent with the underlying principles of agent-based models. We begin by highlighting the limitations of standard neural differential equations in modeling complex systems, where physical invariants (like energy) are often absent but other constraints (like mass conservation, network locality, bounded rationality) must be enforced. To address this, we introduce Agent-Based-Model informed Neural Networks(ABM-NNs), which leverage restricted graph neural networks and hierarchical decomposition to learn interpretable, structure-preserving dynamics. We validate the framework across three case studies of increasing complexity: (i) a generalized Generalized Lotka--Volterra system, where we recover ground-truth parameters from short trajectories in presence of interventions; (ii) a graph-based SIR contagion model, where our method outperforms state-of-the-art graph learning baselines (GCN, GraphSAGE, Graph Transformer) in out-of-sample forecasting and noise robustness; and (iii) a real-world macroeconomic model of the ten largest economies, where we learn coupled GDP dynamics from empirical data and demonstrate gradient-based counterfactual analysis for policy interventions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learnability Window in Gated Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2512.05790</link>
<guid>https://arxiv.org/abs/2512.05790</guid>
<content:encoded><![CDATA[
arXiv:2512.05790v1 Announce Type: new 
Abstract: We develop a theoretical framework that explains how gating mechanisms determine the learnability window $\mathcal{H}_N$ of recurrent neural networks, defined as the largest temporal horizon over which gradient information remains statistically recoverable. While classical analyses emphasize numerical stability of Jacobian products, we show that stability alone is insufficient: learnability is governed instead by the \emph{effective learning rates} $\mu_{t,\ell}$, per-lag and per-neuron quantities obtained from first-order expansions of gate-induced Jacobian products in Backpropagation Through Time. These effective learning rates act as multiplicative filters that control both the magnitude and anisotropy of gradient transport. Under heavy-tailed ($\alpha$-stable) gradient noise, we prove that the minimal sample size required to detect a dependency at lag~$\ell$ satisfies $N(\ell)\propto f(\ell)^{-\alpha}$, where $f(\ell)=\|\mu_{t,\ell}\|_1$ is the effective learning rate envelope. This leads to an explicit formula for $\mathcal{H}_N$ and closed-form scaling laws for logarithmic, polynomial, and exponential decay of $f(\ell)$. The theory predicts that broader or more heterogeneous gate spectra produce slower decay of $f(\ell)$ and hence larger learnability windows, whereas heavier-tailed noise compresses $\mathcal{H}_N$ by slowing statistical concentration. By linking gate-induced time-scale structure, gradient noise, and sample complexity, the framework identifies the effective learning rates as the fundamental quantities that govern when -- and for how long -- gated recurrent networks can learn long-range temporal dependencies.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mechanistic Interpretability of Antibody Language Models Using SAEs</title>
<link>https://arxiv.org/abs/2512.05794</link>
<guid>https://arxiv.org/abs/2512.05794</guid>
<content:encoded><![CDATA[
arXiv:2512.05794v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) are a mechanistic interpretability technique that have been used to provide insight into learned concepts within large protein language models. Here, we employ TopK and Ordered SAEs to investigate an autoregressive antibody language model, p-IgGen, and steer its generation. We show that TopK SAEs can reveal biologically meaningful latent features, but high feature concept correlation does not guarantee causal control over generation. In contrast, Ordered SAEs impose an hierarchical structure that reliably identifies steerable features, but at the expense of more complex and less interpretable activation patterns. These findings advance the mechanistic interpretability of domain-specific protein language models and suggest that, while TopK SAEs are sufficient for mapping latent features to concepts, Ordered SAEs are preferable when precise generative steering is required.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Utility Boundary of Dataset Distillation: Scaling and Configuration-Coverage Laws</title>
<link>https://arxiv.org/abs/2512.05817</link>
<guid>https://arxiv.org/abs/2512.05817</guid>
<content:encoded><![CDATA[
arXiv:2512.05817v1 Announce Type: new 
Abstract: Dataset distillation (DD) aims to construct compact synthetic datasets that allow models to achieve comparable performance to full-data training while substantially reducing storage and computation. Despite rapid empirical progress, its theoretical foundations remain limited: existing methods (gradient, distribution, trajectory matching) are built on heterogeneous surrogate objectives and optimization assumptions, which makes it difficult to analyze their common principles or provide general guarantees. Moreover, it is still unclear under what conditions distilled data can retain the effectiveness of full datasets when the training configuration, such as optimizer, architecture, or augmentation, changes. To answer these questions, we propose a unified theoretical framework, termed configuration--dynamics--error analysis, which reformulates major DD approaches under a common generalization-error perspective and provides two main results: (i) a scaling law that provides a single-configuration upper bound, characterizing how the error decreases as the distilled sample size increases and explaining the commonly observed performance saturation effect; and (ii) a coverage law showing that the required distilled sample size scales linearly with configuration diversity, with provably matching upper and lower bounds. In addition, our unified analysis reveals that various matching methods are interchangeable surrogates, reducing the same generalization error, clarifying why they can all achieve dataset distillation and providing guidance on how surrogate choices affect sample efficiency and robustness. Experiments across diverse methods and configurations empirically confirm the derived laws, advancing a theoretical foundation for DD and enabling theory-driven design of compact, configuration-robust dataset distillation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Approximation of Box Decomposition Algorithm for Fast Hypervolume-Based Multi-Objective Optimization</title>
<link>https://arxiv.org/abs/2512.05825</link>
<guid>https://arxiv.org/abs/2512.05825</guid>
<content:encoded><![CDATA[
arXiv:2512.05825v1 Announce Type: new 
Abstract: Hypervolume (HV)-based Bayesian optimization (BO) is one of the standard approaches for multi-objective decision-making. However, the computational cost of optimizing the acquisition function remains a significant bottleneck, primarily due to the expense of HV improvement calculations. While HV box-decomposition offers an efficient way to cope with the frequent exact improvement calculations, it suffers from super-polynomial memory complexity $O(MN^{\lfloor \frac{M + 1}{2} \rfloor})$ in the worst case as proposed by Lacour et al. (2017). To tackle this problem, Couckuyt et al. (2012) employed an approximation algorithm. However, a rigorous algorithmic description is currently absent from the literature. This paper bridges this gap by providing comprehensive mathematical and algorithmic details of this approximation algorithm.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NEAT: Neighborhood-Guided, Efficient, Autoregressive Set Transformer for 3D Molecular Generation</title>
<link>https://arxiv.org/abs/2512.05844</link>
<guid>https://arxiv.org/abs/2512.05844</guid>
<content:encoded><![CDATA[
arXiv:2512.05844v1 Announce Type: new 
Abstract: Autoregressive models are a promising alternative to diffusion-based models for 3D molecular structure generation. However, a key limitation is the assumption of a token order: while text has a natural sequential order, the next token prediction given a molecular graph prefix should be invariant to atom permutations. Previous works sidestepped this mismatch by using canonical orders or focus atoms. We argue that this is unnecessary. We introduce NEAT, a Neighborhood-guided, Efficient, Autoregressive, Set Transformer that treats molecular graphs as sets of atoms and learns the order-agnostic distribution over admissible tokens at the graph boundary with an autoregressive flow model. NEAT approaches state-of-the-art performance in 3D molecular generation with high computational efficiency and atom-level permutation invariance, establishing a practical foundation for scalable molecular design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Attention Post-Training for Mechanistic Interpretability</title>
<link>https://arxiv.org/abs/2512.05865</link>
<guid>https://arxiv.org/abs/2512.05865</guid>
<content:encoded><![CDATA[
arXiv:2512.05865v1 Announce Type: new 
Abstract: We introduce a simple post-training method that makes transformer attention sparse without sacrificing performance. Applying a flexible sparsity regularisation under a constrained-loss objective, we show on models up to 1B parameters that it is possible to retain the original pretraining loss while reducing attention connectivity to $\approx 0.3 \%$ of its edges. Unlike sparse-attention methods designed for computational efficiency, our approach leverages sparsity as a structural prior: it preserves capability while exposing a more organized and interpretable connectivity pattern. We find that this local sparsity cascades into global circuit simplification: task-specific circuits involve far fewer components (attention heads and MLPs) with up to 100x fewer edges connecting them. These results demonstrate that transformer attention can be made orders of magnitude sparser, suggesting that much of its computation is redundant and that sparsity may serve as a guiding principle for more structured and interpretable models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Price Movements in High-Frequency Financial Data with Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2512.05868</link>
<guid>https://arxiv.org/abs/2512.05868</guid>
<content:encoded><![CDATA[
arXiv:2512.05868v1 Announce Type: new 
Abstract: Modern high-frequency trading (HFT) environments are characterized by sudden price spikes that present both risk and opportunity, but conventional financial models often fail to capture the required fine temporal structure. Spiking Neural Networks (SNNs) offer a biologically inspired framework well-suited to these challenges due to their natural ability to process discrete events and preserve millisecond-scale timing. This work investigates the application of SNNs to high-frequency price-spike forecasting, enhancing performance via robust hyperparameter tuning with Bayesian Optimization (BO). This work converts high-frequency stock data into spike trains and evaluates three architectures: an established unsupervised STDP-trained SNN, a novel SNN with explicit inhibitory competition, and a supervised backpropagation network. BO was driven by a novel objective, Penalized Spike Accuracy (PSA), designed to ensure a network's predicted price spike rate aligns with the empirical rate of price events. Simulated trading demonstrated that models optimized with PSA consistently outperformed their Spike Accuracy (SA)-tuned counterparts and baselines. Specifically, the extended SNN model with PSA achieved the highest cumulative return (76.8%) in simple backtesting, significantly surpassing the supervised alternative (42.54% return). These results validate the potential of spiking networks, when robustly tuned with task-specific objectives, for effective price spike forecasting in HFT.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Design of Low-Volatility Lubricants for Space Using Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2512.05870</link>
<guid>https://arxiv.org/abs/2512.05870</guid>
<content:encoded><![CDATA[
arXiv:2512.05870v1 Announce Type: new 
Abstract: The function and lifetime of moving mechanical assemblies (MMAs) in space depend on the properties of lubricants. MMAs that experience high speeds or high cycles require liquid based lubricants due to their ability to reflow to the point of contact. However, only a few liquid-based lubricants have vapor pressures low enough for the vacuum conditions of space, each of which has limitations that add constraints to MMA designs. This work introduces a data-driven machine learning (ML) approach to predicting vapor pressure, enabling virtual screening and discovery of new space-suitable liquid lubricants. The ML models are trained with data from both high-throughput molecular dynamics simulations and experimental databases. The models are designed to prioritize interpretability, enabling the relationships between chemical structure and vapor pressure to be identified. Based on these insights, several candidate molecules are proposed that may have promise for future space lubricant applications in MMAs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neural Coherence : Find higher performance to out-of-distribution tasks from few samples</title>
<link>https://arxiv.org/abs/2512.05880</link>
<guid>https://arxiv.org/abs/2512.05880</guid>
<content:encoded><![CDATA[
arXiv:2512.05880v1 Announce Type: new 
Abstract: To create state-of-the-art models for many downstream tasks, it has become common practice to fine-tune a pre-trained large vision model. However, it remains an open question of how to best determine which of the many possible model checkpoints resulting from a large training run to use as the starting point. This becomes especially important when data for the target task of interest is scarce, unlabeled and out-of-distribution. In such scenarios, common methods relying on in-distribution validation data become unreliable or inapplicable. This work proposes a novel approach for model selection that operates reliably on just a few unlabeled examples from the target task. Our approach is based on a novel concept: Neural Coherence, which entails characterizing a model's activation statistics for source and target domains, allowing one to define model selection methods with high data-efficiency. We provide experiments where models are pre-trained on ImageNet1K and examine target domains consisting of Food-101, PlantNet-300K and iNaturalist. We also evaluate it in many meta-learning settings. Our approach significantly improves generalization across these different target domains compared to established baselines. We further demonstrate the versatility of Neural Coherence as a powerful principle by showing its effectiveness in training data selection.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAE-HardNet: A Physics Constrained Neural Network Enforcing Differential-Algebraic Hard Constraints</title>
<link>https://arxiv.org/abs/2512.05881</link>
<guid>https://arxiv.org/abs/2512.05881</guid>
<content:encoded><![CDATA[
arXiv:2512.05881v1 Announce Type: new 
Abstract: Traditional physics-informed neural networks (PINNs) do not always satisfy physics based constraints, especially when the constraints include differential operators. Rather, they minimize the constraint violations in a soft way. Strict satisfaction of differential-algebraic equations (DAEs) to embed domain knowledge and first-principles in data-driven models is generally challenging. This is because data-driven models consider the original functions to be black-box whose derivatives can only be obtained after evaluating the functions. We introduce DAE-HardNet, a physics-constrained (rather than simply physics-informed) neural network that learns both the functions and their derivatives simultaneously, while enforcing algebraic as well as differential constraints. This is done by projecting model predictions onto the constraint manifold using a differentiable projection layer. We apply DAE-HardNet to several systems and test problems governed by DAEs, including the dynamic Lotka-Volterra predator-prey system and transient heat conduction. We also show the ability of DAE-HardNet to estimate unknown parameters through a parameter estimation problem. Compared to multilayer perceptrons (MLPs) and PINNs, DAE-HardNet achieves orders of magnitude reduction in the physics loss while maintaining the prediction accuracy. It has the added benefits of learning the derivatives which improves the constrained learning of the backbone neural network prior to the projection layer. For specific problems, this suggests that the projection layer can be bypassed for faster inference. The current implementation and codes are available at https://github.com/SOULS-TAMU/DAE-HardNet.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroMemFPP: A recurrent neural approach for memory-aware parameter estimation in fractional Poisson process</title>
<link>https://arxiv.org/abs/2512.05893</link>
<guid>https://arxiv.org/abs/2512.05893</guid>
<content:encoded><![CDATA[
arXiv:2512.05893v1 Announce Type: new 
Abstract: In this paper, we propose a recurrent neural network (RNN)-based framework for estimating the parameters of the fractional Poisson process (FPP), which models event arrivals with memory and long-range dependence. The Long Short-Term Memory (LSTM) network estimates the key parameters $\mu >0$ and $\beta \in(0,1)$ from sequences of inter-arrival times, effectively modeling their temporal dependencies. Our experiments on synthetic data show that the proposed approach reduces the mean squared error (MSE) by about 55.3\% compared to the traditional method of moments (MOM) and performs reliably across different training conditions. We tested the method on two real-world high-frequency datasets: emergency call records from Montgomery County, PA, and AAPL stock trading data. The results show that the LSTM can effectively track daily patterns and parameter changes, indicating its effectiveness on real-world data with complex time dependencies.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LDLT $\mathcal{L}$-Lipschitz Network: Generalized Deep End-To-End Lipschitz Network Construction</title>
<link>https://arxiv.org/abs/2512.05915</link>
<guid>https://arxiv.org/abs/2512.05915</guid>
<content:encoded><![CDATA[
arXiv:2512.05915v1 Announce Type: new 
Abstract: Deep residual networks (ResNets) have demonstrated outstanding success in computer vision tasks, attributed to their ability to maintain gradient flow through deep architectures. Simultaneously, controlling the Lipschitz constant in neural networks has emerged as an essential area of research to enhance adversarial robustness and network certifiability. This paper presents a rigorous approach to the general design of $\mathcal{L}$-Lipschitz deep residual networks using a Linear Matrix Inequality (LMI) framework. Initially, the ResNet architecture was reformulated as a cyclic tridiagonal LMI, and closed-form constraints on network parameters were derived to ensure $\mathcal{L}$-Lipschitz continuity; however, using a new $LDL^\top$ decomposition approach for certifying LMI feasibility, we extend the construction of $\mathcal{L}$-Lipchitz networks to any other nonlinear architecture. Our contributions include a provable parameterization methodology for constructing Lipschitz-constrained residual networks and other hierarchical architectures. Cholesky decomposition is also used for efficient parameterization. These findings enable robust network designs applicable to adversarial robustness, certified training, and control systems. The $LDL^\top$ formulation is shown to be a tight relaxation of the SDP-based network, maintaining full expressiveness and achieving 3\%-13\% accuracy gains over SLL Layers on 121 UCI data sets.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KQ-SVD: Compressing the KV Cache with Provable Guarantees on Attention Fidelity</title>
<link>https://arxiv.org/abs/2512.05916</link>
<guid>https://arxiv.org/abs/2512.05916</guid>
<content:encoded><![CDATA[
arXiv:2512.05916v1 Announce Type: new 
Abstract: The Key-Value (KV) cache is central to the efficiency of transformer-based large language models (LLMs), storing previously computed vectors to accelerate inference. Yet, as sequence length and batch size grow, the cache becomes a major memory bottleneck. Prior compression methods typically apply low-rank decomposition to keys alone or attempt to jointly embed queries and keys, but both approaches neglect that attention fundamentally depends on their inner products. In this work, we prove that such strategies are suboptimal for approximating the attention matrix. We introduce KQ-SVD, a simple and computationally efficient method that directly performs an optimal low-rank decomposition of the attention matrix via a closed-form solution. By targeting the true source of redundancy, KQ-SVD preserves attention outputs with higher fidelity under compression. Extensive evaluations on LLaMA and Mistral models demonstrate that our approach consistently delivers superior projection quality.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Bayes Inconsistency of Disagreement Discrepancy Surrogates</title>
<link>https://arxiv.org/abs/2512.05931</link>
<guid>https://arxiv.org/abs/2512.05931</guid>
<content:encoded><![CDATA[
arXiv:2512.05931v1 Announce Type: new 
Abstract: Deep neural networks often fail when deployed in real-world contexts due to distribution shift, a critical barrier to building safe and reliable systems. An emerging approach to address this problem relies on \emph{disagreement discrepancy} -- a measure of how the disagreement between two models changes under a shifting distribution. The process of maximizing this measure has seen applications in bounding error under shifts, testing for harmful shifts, and training more robust models. However, this optimization involves the non-differentiable zero-one loss, necessitating the use of practical surrogate losses. We prove that existing surrogates for disagreement discrepancy are not Bayes consistent, revealing a fundamental flaw: maximizing these surrogates can fail to maximize the true disagreement discrepancy. To address this, we introduce new theoretical results providing both upper and lower bounds on the optimality gap for such surrogates. Guided by this theory, we propose a novel disagreement loss that, when paired with cross-entropy, yields a provably consistent surrogate for disagreement discrepancy. Empirical evaluations across diverse benchmarks demonstrate that our method provides more accurate and robust estimates of disagreement discrepancy than existing approaches, particularly under challenging adversarial conditions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
<link>https://arxiv.org/abs/2512.05948</link>
<guid>https://arxiv.org/abs/2512.05948</guid>
<content:encoded><![CDATA[
arXiv:2512.05948v1 Announce Type: new 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Impugan: Learning Conditional Generative Models for Robust Data Imputation</title>
<link>https://arxiv.org/abs/2512.05950</link>
<guid>https://arxiv.org/abs/2512.05950</guid>
<content:encoded><![CDATA[
arXiv:2512.05950v1 Announce Type: new 
Abstract: Incomplete data are common in real-world applications. Sensors fail, records are inconsistent, and datasets collected from different sources often differ in scale, sampling rate, and quality. These differences create missing values that make it difficult to combine data and build reliable models. Standard imputation methods such as regression models, expectation-maximization, and multiple imputation rely on strong assumptions about linearity and independence. These assumptions rarely hold for complex or heterogeneous data, which can lead to biased or over-smoothed estimates. We propose Impugan, a conditional Generative Adversarial Network (cGAN) for imputing missing values and integrating heterogeneous datasets. The model is trained on complete samples to learn how missing variables depend on observed ones. During inference, the generator reconstructs missing entries from available features, and the discriminator enforces realism by distinguishing true from imputed data. This adversarial process allows Impugan to capture nonlinear and multimodal relationships that conventional methods cannot represent. In experiments on benchmark datasets and a multi-source integration task, Impugan achieves up to 82\% lower Earth Mover's Distance (EMD) and 70\% lower mutual-information deviation (MI) compared to leading baselines. These results show that adversarially trained generative models provide a scalable and principled approach for imputing and merging incomplete, heterogeneous data. Our model is available at: github.com/zalishmahmud/impuganBigData2025
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaxShapley: Towards Incentive-compatible Generative Search with Fair Context Attribution</title>
<link>https://arxiv.org/abs/2512.05958</link>
<guid>https://arxiv.org/abs/2512.05958</guid>
<content:encoded><![CDATA[
arXiv:2512.05958v1 Announce Type: new 
Abstract: Generative search engines based on large language models (LLMs) are replacing traditional search, fundamentally changing how information providers are compensated. To sustain this ecosystem, we need fair mechanisms to attribute and compensate content providers based on their contributions to generated answers. We introduce MaxShapley, an efficient algorithm for fair attribution in generative search pipelines that use retrieval-augmented generation (RAG). MaxShapley is a special case of the celebrated Shapley value; it leverages a decomposable max-sum utility function to compute attributions with linear computation in the number of documents, as opposed to the exponential cost of Shapley values. We evaluate MaxShapley on three multi-hop QA datasets (HotPotQA, MuSiQUE, MS MARCO); MaxShapley achieves comparable attribution quality to exact Shapley computation, while consuming a fraction of its tokens--for instance, it gives up to an 8x reduction in resource consumption over prior state-of-the-art methods at the same attribution accuracy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Whatever Remains Must Be True: Filtering Drives Reasoning in LLMs, Shaping Diversity</title>
<link>https://arxiv.org/abs/2512.05962</link>
<guid>https://arxiv.org/abs/2512.05962</guid>
<content:encoded><![CDATA[
arXiv:2512.05962v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has become the de facto standard for tuning LLMs to solve tasks involving reasoning. However, growing evidence shows that models trained in such way often suffer from a significant loss in diversity. We argue that this arises because RL implicitly optimizes the "mode-seeking" or "zero-forcing" Reverse KL to a target distribution causing the model to concentrate mass on certain high-probability regions of the target while neglecting others. In this work, we instead begin from an explicit target distribution, obtained by filtering out incorrect answers while preserving the relative probabilities of correct ones. Starting from a pre-trained LLM, we approximate this target distribution using the $\alpha$-divergence family, which unifies prior approaches and enables direct control of the precision-diversity trade-off by interpolating between mode-seeking and mass-covering divergences. On a Lean theorem-proving benchmark, our method achieves state-of-the-art performance along the coverage-precision Pareto frontier, outperforming all prior methods on the coverage axis.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Optimization and Convolutional Neural Networks for Zernike-Based Wavefront Correction in High Harmonic Generation</title>
<link>https://arxiv.org/abs/2512.05127</link>
<guid>https://arxiv.org/abs/2512.05127</guid>
<content:encoded><![CDATA[
arXiv:2512.05127v1 Announce Type: cross 
Abstract: High harmonic generation (HHG) is a nonlinear process that enables table-top generation of tunable, high-energy, coherent, ultrashort radiation pulses in the extreme ultraviolet (EUV) to soft X-ray range. These pulses find applications in photoemission spectroscopy in condensed matter physics, pump-probe spectroscopy for high-energy-density plasmas, and attosecond science. However, optical aberrations in the high-power laser systems required for HHG degrade beam quality and reduce efficiency. We present a machine learning approach to optimize aberration correction using a spatial light modulator. We implemented and compared Bayesian optimization and convolutional neural network (CNN) methods to predict optimal Zernike polynomial coefficients for wavefront correction. Our CNN achieved promising results with 80.39% accuracy on test data, demonstrating the potential for automated aberration correction in HHG systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InvarDiff: Cross-Scale Invariance Caching for Accelerated Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05134</link>
<guid>https://arxiv.org/abs/2512.05134</guid>
<content:encoded><![CDATA[
arXiv:2512.05134v1 Announce Type: cross 
Abstract: Diffusion models deliver high-fidelity synthesis but remain slow due to iterative sampling. We empirically observe there exists feature invariance in deterministic sampling, and present InvarDiff, a training-free acceleration method that exploits the relative temporal invariance across timestep-scale and layer-scale. From a few deterministic runs, we compute a per-timestep, per-layer, per-module binary cache plan matrix and use a re-sampling correction to avoid drift when consecutive caches occur. Using quantile-based change metrics, this matrix specifies which module at which step is reused rather than recomputed. The same invariance criterion is applied at the step scale to enable cross-timestep caching, deciding whether an entire step can reuse cached results. During inference, InvarDiff performs step-first and layer-wise caching guided by this matrix. When applied to DiT and FLUX, our approach reduces redundant compute while preserving fidelity. Experiments show that InvarDiff achieves $2$-$3\times$ end-to-end speed-ups with minimal impact on standard quality metrics. Qualitatively, we observe almost no degradation in visual quality compared with full computations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Satellite Image Downscaling with Transfer Encoders and Autoregressive Generative Models</title>
<link>https://arxiv.org/abs/2512.05139</link>
<guid>https://arxiv.org/abs/2512.05139</guid>
<content:encoded><![CDATA[
arXiv:2512.05139v1 Announce Type: cross 
Abstract: We present a transfer-learning generative downscaling framework to reconstruct fine resolution satellite images from coarse scale inputs. Our approach combines a lightweight U-Net transfer encoder with a diffusion-based generative model. The simpler U-Net is first pretrained on a long time series of coarse resolution data to learn spatiotemporal representations; its encoder is then frozen and transferred to a larger downscaling model as physically meaningful latent features. Our application uses NASA's MERRA-2 reanalysis as the low resolution source domain (50 km) and the GEOS-5 Nature Run (G5NR) as the high resolution target (7 km). Our study area included a large area in Asia, which was made computationally tractable by splitting into two subregions and four seasons. We conducted domain similarity analysis using Wasserstein distances confirmed minimal distributional shift between MERRA-2 and G5NR, validating the safety of parameter frozen transfer. Across seasonal regional splits, our model achieved excellent performance (R2 = 0.65 to 0.94), outperforming comparison models including deterministic U-Nets, variational autoencoders, and prior transfer learning baselines. Out of data evaluations using semivariograms, ACF/PACF, and lag-based RMSE/R2 demonstrated that the predicted downscaled images preserved physically consistent spatial variability and temporal autocorrelation, enabling stable autoregressive reconstruction beyond the G5NR record. These results show that transfer enhanced diffusion models provide a robust and physically coherent solution for downscaling a long time series of coarse resolution images with limited training periods. This advancement has significant implications for improving environmental exposure assessment and long term environmental monitoring.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Faithfulness and Entropy Production Measures to Tame Your LLM Demons and Manage Hallucinations</title>
<link>https://arxiv.org/abs/2512.05156</link>
<guid>https://arxiv.org/abs/2512.05156</guid>
<content:encoded><![CDATA[
arXiv:2512.05156v1 Announce Type: cross 
Abstract: Evaluating faithfulness of Large Language Models (LLMs) to a given task is a complex challenge. We propose two new unsupervised metrics for faithfulness evaluation using insights from information theory and thermodynamics. Our approach treats an LLM as a bipartite information engine where hidden layers act as a Maxwell demon controlling transformations of context $C $ into answer $A$ via prompt $Q$. We model Question-Context-Answer (QCA) triplets as probability distributions over shared topics. Topic transformations from $C$ to $Q$ and $A$ are modeled as transition matrices ${\bf Q}$ and ${\bf A}$ encoding the query goal and actual result, respectively. Our semantic faithfulness (SF) metric quantifies faithfulness for any given QCA triplet by the Kullback-Leibler (KL) divergence between these matrices. Both matrices are inferred simultaneously via convex optimization of this KL divergence, and the final SF metric is obtained by mapping the minimal divergence onto the unit interval [0,1], where higher scores indicate greater faithfulness. Furthermore, we propose a thermodynamics-based semantic entropy production (SEP) metric in answer generation, and show that high faithfulness generally implies low entropy production. The SF and SEP metrics can be used jointly or separately for LLM evaluation and hallucination control. We demonstrate our framework on LLM summarization of corporate SEC 10-K filings.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-Time Homeostatic Dynamics for Reentrant Inference Models</title>
<link>https://arxiv.org/abs/2512.05158</link>
<guid>https://arxiv.org/abs/2512.05158</guid>
<content:encoded><![CDATA[
arXiv:2512.05158v1 Announce Type: cross 
Abstract: We formulate the Fast-Weights Homeostatic Reentry Network (FHRN) as a continuous-time neural-ODE system, revealing its role as a norm-regulated reentrant dynamical process. Starting from the discrete reentry rule $x_t = x_t^{(\mathrm{ex})} + \gamma\, W_r\, g(\|y_{t-1}\|)\, y_{t-1}$, we derive the coupled system $\dot{y}=-y+f(W_ry;\,x,\,A)+g_{\mathrm{h}}(y)$ showing that the network couples fast associative memory with global radial homeostasis. The dynamics admit bounded attractors governed by an energy functional, yielding a ring-like manifold. A Jacobian spectral analysis identifies a \emph{reflective regime} in which reentry induces stable oscillatory trajectories rather than divergence or collapse. Unlike continuous-time recurrent neural networks or liquid neural networks, FHRN achieves stability through population-level gain modulation rather than fixed recurrence or neuron-local time adaptation. These results establish the reentry network as a distinct class of self-referential neural dynamics supporting recursive yet bounded computation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Tame Your LLM: Semantic Collapse in Continuous Systems</title>
<link>https://arxiv.org/abs/2512.05162</link>
<guid>https://arxiv.org/abs/2512.05162</guid>
<content:encoded><![CDATA[
arXiv:2512.05162v1 Announce Type: cross 
Abstract: We develop a general theory of semantic dynamics for large language models by formalizing them as Continuous State Machines (CSMs): smooth dynamical systems whose latent manifolds evolve under probabilistic transition operators. The associated transfer operator $P: L^2(M,\mu) \to L^2(M,\mu)$ encodes the propagation of semantic mass. Under mild regularity assumptions (compactness, ergodicity, bounded Jacobian), $P$ is compact with discrete spectrum. Within this setting, we prove the Semantic Characterization Theorem (SCT): the leading eigenfunctions of $P$ induce finitely many spectral basins of invariant meaning, each definable in an o-minimal structure over $\mathbb{R}$. Thus spectral lumpability and logical tameness coincide. This explains how discrete symbolic semantics can emerge from continuous computation: the continuous activation manifold collapses into a finite, logically interpretable ontology. We further extend the SCT to stochastic and adiabatic (time-inhomogeneous) settings, showing that slowly drifting kernels preserve compactness, spectral coherence, and basin structure.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Your Latent Mask is Wrong: Pixel-Equivalent Latent Compositing for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.05198</link>
<guid>https://arxiv.org/abs/2512.05198</guid>
<content:encoded><![CDATA[
arXiv:2512.05198v1 Announce Type: cross 
Abstract: Latent inpainting in diffusion models still relies almost universally on linearly interpolating VAE latents under a downsampled mask. We propose a key principle for compositing image latents: Pixel-Equivalent Latent Compositing (PELC). An equivalent latent compositor should be the same as compositing in pixel space. This principle enables full-resolution mask control and true soft-edge alpha compositing, even though VAEs compress images 8x spatially. Modern VAEs capture global context beyond patch-aligned local structure, so linear latent blending cannot be pixel-equivalent: it produces large artifacts at mask seams and global degradation and color shifts. We introduce DecFormer, a 7.7M-parameter transformer that predicts per-channel blend weights and an off-manifold residual correction to realize mask-consistent latent fusion. DecFormer is trained so that decoding after fusion matches pixel-space alpha compositing, is plug-compatible with existing diffusion pipelines, requires no backbone finetuning and adds only 0.07% of FLUX.1-Dev's parameters and 3.5% FLOP overhead. On the FLUX.1 family, DecFormer restores global color consistency, soft-mask support, sharp boundaries, and high-fidelity masking, reducing error metrics around edges by up to 53% over standard mask interpolation. Used as an inpainting prior, a lightweight LoRA on FLUX.1-Dev with DecFormer achieves fidelity comparable to FLUX.1-Fill, a fully finetuned inpainting model. While we focus on inpainting, PELC is a general recipe for pixel-equivalent latent editing, as we demonstrate on a complex color-correction task.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning for the Dynamic VNE with Alternatives Problem</title>
<link>https://arxiv.org/abs/2512.05207</link>
<guid>https://arxiv.org/abs/2512.05207</guid>
<content:encoded><![CDATA[
arXiv:2512.05207v1 Announce Type: cross 
Abstract: Virtual Network Embedding (VNE) is a key enabler of network slicing, yet most formulations assume that each Virtual Network Request (VNR) has a fixed topology. Recently, VNE with Alternative topologies (VNEAP) was introduced to capture malleable VNRs, where each request can be instantiated using one of several functionally equivalent topologies that trade resources differently. While this flexibility enlarges the feasible space, it also introduces an additional decision layer, making dynamic embedding more challenging. This paper proposes HRL-VNEAP, a hierarchical reinforcement learning approach for VNEAP under dynamic arrivals. A high-level policy selects the most suitable alternative topology (or rejects the request), and a low-level policy embeds the chosen topology onto the substrate network. Experiments on realistic substrate topologies under multiple traffic loads show that naive exploitation strategies provide only modest gains, whereas HRL-VNEAP consistently achieves the best performance across all metrics. Compared to the strongest tested baselines, HRL-VNEAP improves acceptance ratio by up to \textbf{20.7\%}, total revenue by up to \textbf{36.2\%}, and revenue-over-cost by up to \textbf{22.1\%}. Finally, we benchmark against an MILP formulation on tractable instances to quantify the remaining gap to optimality and motivate future work on learning- and optimization-based VNEAP solutions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STAR-GO: Improving Protein Function Prediction by Learning to Hierarchically Integrate Ontology-Informed Semantic Embeddings</title>
<link>https://arxiv.org/abs/2512.05245</link>
<guid>https://arxiv.org/abs/2512.05245</guid>
<content:encoded><![CDATA[
arXiv:2512.05245v1 Announce Type: cross 
Abstract: Accurate prediction of protein function is essential for elucidating molecular mechanisms and advancing biological and therapeutic discovery. Yet experimental annotation lags far behind the rapid growth of protein sequence data. Computational approaches address this gap by associating proteins with Gene Ontology (GO) terms, which encode functional knowledge through hierarchical relations and textual definitions. However, existing models often emphasize one modality over the other, limiting their ability to generalize, particularly to unseen or newly introduced GO terms that frequently arise as the ontology evolves, and making the previously trained models outdated. We present STAR-GO, a Transformer-based framework that jointly models the semantic and structural characteristics of GO terms to enhance zero-shot protein function prediction. STAR-GO integrates textual definitions with ontology graph structure to learn unified GO representations, which are processed in hierarchical order to propagate information from general to specific terms. These representations are then aligned with protein sequence embeddings to capture sequence-function relationships. STAR-GO achieves state-of-the-art performance and superior zero-shot generalization, demonstrating the utility of integrating semantics and structure for robust and adaptable protein function prediction. Code is available at https://github.com/boun-tabi-lifelu/stargo.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-Step Diffusion Samplers via Self-Distillation and Deterministic Flow</title>
<link>https://arxiv.org/abs/2512.05251</link>
<guid>https://arxiv.org/abs/2512.05251</guid>
<content:encoded><![CDATA[
arXiv:2512.05251v1 Announce Type: cross 
Abstract: Sampling from unnormalized target distributions is a fundamental yet challenging task in machine learning and statistics. Existing sampling algorithms typically require many iterative steps to produce high-quality samples, leading to high computational costs. We introduce one-step diffusion samplers which learn a step-conditioned ODE so that one large step reproduces the trajectory of many small ones via a state-space consistency loss. We further show that standard ELBO estimates in diffusion samplers degrade in the few-step regime because common discrete integrators yield mismatched forward/backward transition kernels. Motivated by this analysis, we derive a deterministic-flow (DF) importance weight for ELBO estimation without a backward kernel. To calibrate DF, we introduce a volume-consistency regularization that aligns the accumulated volume change along the flow across step resolutions. Our proposed sampler therefore achieves both sampling and stable evidence estimate in only one or few steps. Across challenging synthetic and Bayesian benchmarks, it achieves competitive sample quality with orders-of-magnitude fewer network evaluations while maintaining robust ELBO estimates.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty-Aware Data-Efficient AI: An Information-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2512.05267</link>
<guid>https://arxiv.org/abs/2512.05267</guid>
<content:encoded><![CDATA[
arXiv:2512.05267v1 Announce Type: cross 
Abstract: In context-specific applications such as robotics, telecommunications, and healthcare, artificial intelligence systems often face the challenge of limited training data. This scarcity introduces epistemic uncertainty, i.e., reducible uncertainty stemming from incomplete knowledge of the underlying data distribution, which fundamentally limits predictive performance. This review paper examines formal methodologies that address data-limited regimes through two complementary approaches: quantifying epistemic uncertainty and mitigating data scarcity via synthetic data augmentation. We begin by reviewing generalized Bayesian learning frameworks that characterize epistemic uncertainty through generalized posteriors in the model parameter space, as well as ``post-Bayes'' learning frameworks. We continue by presenting information-theoretic generalization bounds that formalize the relationship between training data quantity and predictive uncertainty, providing a theoretical justification for generalized Bayesian learning. Moving beyond methods with asymptotic statistical validity, we survey uncertainty quantification methods that provide finite-sample statistical guarantees, including conformal prediction and conformal risk control. Finally, we examine recent advances in data efficiency by combining limited labeled data with abundant model predictions or synthetic data. Throughout, we take an information-theoretic perspective, highlighting the role of information measures in quantifying the impact of data scarcity.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust forecast aggregation via additional queries</title>
<link>https://arxiv.org/abs/2512.05271</link>
<guid>https://arxiv.org/abs/2512.05271</guid>
<content:encoded><![CDATA[
arXiv:2512.05271v1 Announce Type: cross 
Abstract: We study the problem of robust forecast aggregation: combining expert forecasts with provable accuracy guarantees compared to the best possible aggregation of the underlying information. Prior work shows strong impossibility results, e.g. that even under natural assumptions, no aggregation of the experts' individual forecasts can outperform simply following a random expert (Neyman and Roughgarden, 2022).
  In this paper, we introduce a more general framework that allows the principal to elicit richer information from experts through structured queries. Our framework ensures that experts will truthfully report their underlying beliefs, and also enables us to define notions of complexity over the difficulty of asking these queries. Under a general model of independent but overlapping expert signals, we show that optimal aggregation is achievable in the worst case with each complexity measure bounded above by the number of agents $n$. We further establish tight tradeoffs between accuracy and query complexity: aggregation error decreases linearly with the number of queries, and vanishes when the "order of reasoning" and number of agents relevant to a query is $\omega(\sqrt{n})$. These results demonstrate that modest extensions to the space of expert queries dramatically strengthen the power of robust forecast aggregation. We therefore expect that our new query framework will open up a fruitful line of research in this area.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Detection: A Comprehensive Benchmark and Study on Representation Learning for Fine-Grained Webshell Family Classification</title>
<link>https://arxiv.org/abs/2512.05288</link>
<guid>https://arxiv.org/abs/2512.05288</guid>
<content:encoded><![CDATA[
arXiv:2512.05288v1 Announce Type: cross 
Abstract: Malicious WebShells pose a significant and evolving threat by compromising critical digital infrastructures and endangering public services in sectors such as healthcare and finance. While the research community has made significant progress in WebShell detection (i.e., distinguishing malicious samples from benign ones), we argue that it is time to transition from passive detection to in-depth analysis and proactive defense. One promising direction is the automation of WebShell family classification, which involves identifying the specific malware lineage in order to understand an adversary's tactics and enable a precise, rapid response. This crucial task, however, remains a largely unexplored area that currently relies on slow, manual expert analysis. To address this gap, we present the first systematic study to automate WebShell family classification. Our method begins with extracting dynamic function call traces to capture inherent behaviors that are resistant to common encryption and obfuscation. To enhance the scale and diversity of our dataset for a more stable evaluation, we augment these real-world traces with new variants synthesized by Large Language Models. These augmented traces are then abstracted into sequences, graphs, and trees, providing a foundation to benchmark a comprehensive suite of representation methods. Our evaluation spans classic sequence-based embeddings (CBOW, GloVe), transformers (BERT, SimCSE), and a range of structure-aware algorithms, including Graph Kernels, Graph Edit Distance, Graph2Vec, and various Graph Neural Networks. Through extensive experiments on four real-world, family-annotated datasets under both supervised and unsupervised settings, we establish a robust baseline and provide practical insights into the most effective combinations of data abstractions, representation models, and learning paradigms for this challenge.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>To Think or Not to Think: The Hidden Cost of Meta-Training with Excessive CoT Examples</title>
<link>https://arxiv.org/abs/2512.05318</link>
<guid>https://arxiv.org/abs/2512.05318</guid>
<content:encoded><![CDATA[
arXiv:2512.05318v1 Announce Type: cross 
Abstract: Chain-of-thought (CoT) prompting combined with few-shot in-context learning (ICL) has unlocked significant reasoning capabilities in large language models (LLMs). However, ICL with CoT examples is ineffective on novel tasks when the pre-training knowledge is insufficient. We study this problem in a controlled setting using the CoT-ICL Lab framework, and propose meta-training techniques to learn novel abstract reasoning tasks in-context. Although CoT examples facilitate reasoning, we noticed that their excessive inclusion during meta-training degrades performance when CoT supervision is limited. To mitigate such behavior, we propose CoT-Recipe, a formal approach to modulate the mix of CoT and non-CoT examples in meta-training sequences. We demonstrate that careful modulation via CoT-Recipe can increase the accuracy of transformers on novel tasks by up to 300% even when there are no CoT examples available in-context. We confirm the broader effectiveness of these techniques by applying them to pretrained LLMs (Qwen2.5 series) for symbolic reasoning tasks and observing gains of up to 130% in accuracy.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LYNX: Learning Dynamic Exits for Confidence-Controlled Reasoning</title>
<link>https://arxiv.org/abs/2512.05325</link>
<guid>https://arxiv.org/abs/2512.05325</guid>
<content:encoded><![CDATA[
arXiv:2512.05325v1 Announce Type: cross 
Abstract: Large reasoning models achieve strong performance on complex tasks by generating extended chains of thought, but they often "overthink": continuing to reason long after they have enough information to answer correctly. This wastes inference-time compute and can hurt accuracy. Existing attempts to stop early either manipulate decoding with extra sampling and heuristics, rely on auxiliary verifier models, or operate only as post-hoc analysis pipelines without formal guarantees. We introduce LYNX, an online early-exit mechanism that turns a model's own hidden-state awareness into confidence-controlled stopping decisions. LYNX attaches exit decisions to naturally occurring reasoning cues (e.g., "hmm", "wait") during generation, trains a lightweight probe on hidden states at those cue tokens using supervision from forced exits, and wraps the resulting scores in split conformal prediction to obtain distribution-free control over premature exits. Crucially, we train and calibrate this probe once on a generic mathematical corpus and reuse it unchanged across benchmarks, decoding temperatures, and even non-mathematical tasks. Across three model families spanning 1.5B to 32B parameters, a single mathematically trained probe per base model yields strong accuracy--efficiency tradeoffs. On GSM8K, LYNX matches or improves baseline accuracy while reducing tokens by 40--65\%; on MATH-500 it improves accuracy by up to 12 points with roughly 35--60\% fewer tokens; on AIME 2024 it recovers baseline accuracy with more than 50\% token savings; and on CommonsenseQA, a non-math benchmark, it transfers zero-shot with modest accuracy gains and up to 70\% fewer tokens. Compared to state-of-the-art early-exit methods, LYNX offers competitive or superior Pareto frontiers while remaining fully online, requiring no proxy models at inference, and providing explicit, user-tunable confidence guarantees.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exposing Pink Slime Journalism: Linguistic Signatures and Robust Detection Against LLM-Generated Threats</title>
<link>https://arxiv.org/abs/2512.05331</link>
<guid>https://arxiv.org/abs/2512.05331</guid>
<content:encoded><![CDATA[
arXiv:2512.05331v1 Announce Type: cross 
Abstract: The local news landscape, a vital source of reliable information for 28 million Americans, faces a growing threat from Pink Slime Journalism, a low-quality, auto-generated articles that mimic legitimate local reporting. Detecting these deceptive articles requires a fine-grained analysis of their linguistic, stylistic, and lexical characteristics. In this work, we conduct a comprehensive study to uncover the distinguishing patterns of Pink Slime content and propose detection strategies based on these insights. Beyond traditional generation methods, we highlight a new adversarial vector: modifications through large language models (LLMs). Our findings reveal that even consumer-accessible LLMs can significantly undermine existing detection systems, reducing their performance by up to 40% in F1-score. To counter this threat, we introduce a robust learning framework specifically designed to resist LLM-based adversarial attacks and adapt to the evolving landscape of automated pink slime journalism, and showed and improvement by up to 27%.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Symmetric Linear Dynamical Systems are Learnable from Few Observations</title>
<link>https://arxiv.org/abs/2512.05337</link>
<guid>https://arxiv.org/abs/2512.05337</guid>
<content:encoded><![CDATA[
arXiv:2512.05337v1 Announce Type: cross 
Abstract: We consider the problem of learning the parameters of a $N$-dimensional stochastic linear dynamics under both full and partial observations from a single trajectory of time $T$. We introduce and analyze a new estimator that achieves a small maximum element-wise error on the recovery of symmetric dynamic matrices using only $T=\mathcal{O}(\log N)$ observations, irrespective of whether the matrix is sparse or dense. This estimator is based on the method of moments and does not rely on problem-specific regularization. This is especially important for applications such as structure discovery.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FieldSeer I: Physics-Guided World Models for Long-Horizon Electromagnetic Dynamics under Partial Observability</title>
<link>https://arxiv.org/abs/2512.05361</link>
<guid>https://arxiv.org/abs/2512.05361</guid>
<content:encoded><![CDATA[
arXiv:2512.05361v1 Announce Type: cross 
Abstract: We introduce FieldSeer I, a geometry-aware world model that forecasts electromagnetic field dynamics from partial observations in 2-D TE waveguides. The model assimilates a short prefix of observed fields, conditions on a scalar source action and structure/material map, and generates closed-loop rollouts in the physical domain. Training in a symmetric-log domain ensures numerical stability. Evaluated on a reproducible FDTD benchmark (200 unique simulations, structure-wise split), FieldSeer I achieves higher suffix fidelity than GRU and deterministic baselines across three practical settings: (i) software-in-the-loop filtering (64x64, P=80->Q=80), (ii) offline single-file rollouts (80x140, P=240->Q=40), and (iii) offline multi-structure rollouts (80x140, P=180->Q=100). Crucially, it enables edit-after-prefix geometry modifications without re-assimilation. Results demonstrate that geometry-conditioned world models provide a practical path toward interactive digital twins for photonic design.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PoolNet: Deep Learning for 2D to 3D Video Process Validation</title>
<link>https://arxiv.org/abs/2512.05362</link>
<guid>https://arxiv.org/abs/2512.05362</guid>
<content:encoded><![CDATA[
arXiv:2512.05362v1 Announce Type: cross 
Abstract: Lifting Structure-from-Motion (SfM) information from sequential and non-sequential image data is a time-consuming and computationally expensive task. In addition to this, the majority of publicly available data is unfit for processing due to inadequate camera pose variation, obscuring scene elements, and noisy data. To solve this problem, we introduce PoolNet, a versatile deep learning framework for frame-level and scene-level validation of in-the-wild data. We demonstrate that our model successfully differentiates SfM ready scenes from those unfit for processing while significantly undercutting the amount of time state of the art algorithms take to obtain structure-from-motion data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Moving object detection from multi-depth images with an attention-enhanced CNN</title>
<link>https://arxiv.org/abs/2512.05415</link>
<guid>https://arxiv.org/abs/2512.05415</guid>
<content:encoded><![CDATA[
arXiv:2512.05415v1 Announce Type: cross 
Abstract: One of the greatest challenges for detecting moving objects in the solar system from wide-field survey data is determining whether a signal indicates a true object or is due to some other source, like noise. Object verification has relied heavily on human eyes, which usually results in significant labor costs. In order to address this limitation and reduce the reliance on manual intervention, we propose a multi-input convolutional neural network integrated with a convolutional block attention module. This method is specifically tailored to enhance the moving object detection system that we have developed and used previously. The current method introduces two innovations. This first one is a multi-input architecture that processes multiple stacked images simultaneously. The second is the incorporation of the convolutional block attention module which enables the model to focus on essential features in both spatial and channel dimensions. These advancements facilitate efficient learning from multiple inputs, leading to more robust detection of moving objects. The performance of the model is evaluated on a dataset consisting of approximately 2,000 observational images. We achieved an accuracy of nearly 99% with AUC (an Area Under the Curve) of >0.99. These metrics indicate that the proposed model achieves excellent classification performance. By adjusting the threshold for object detection, the new model reduces the human workload by more than 99% compared to manual verification.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArtistMus: A Globally Diverse, Artist-Centric Benchmark for Retrieval-Augmented Music Question Answering</title>
<link>https://arxiv.org/abs/2512.05430</link>
<guid>https://arxiv.org/abs/2512.05430</guid>
<content:encoded><![CDATA[
arXiv:2512.05430v1 Announce Type: cross 
Abstract: Recent advances in large language models (LLMs) have transformed open-domain question answering, yet their effectiveness in music-related reasoning remains limited due to sparse music knowledge in pretraining data. While music information retrieval and computational musicology have explored structured and multimodal understanding, few resources support factual and contextual music question answering (MQA) grounded in artist metadata or historical context. We introduce MusWikiDB, a vector database of 3.2M passages from 144K music-related Wikipedia pages, and ArtistMus, a benchmark of 1,000 questions on 500 diverse artists with metadata such as genre, debut year, and topic. These resources enable systematic evaluation of retrieval-augmented generation (RAG) for MQA. Experiments show that RAG markedly improves factual accuracy; open-source models gain up to +56.8 percentage points (for example, Qwen3 8B improves from 35.0 to 91.8), approaching proprietary model performance. RAG-style fine-tuning further boosts both factual recall and contextual reasoning, improving results on both in-domain and out-of-domain benchmarks. MusWikiDB also yields approximately 6 percentage points higher accuracy and 40% faster retrieval than a general-purpose Wikipedia corpus. We release MusWikiDB and ArtistMus to advance research in music information retrieval and domain-specific question answering, establishing a foundation for retrieval-augmented reasoning in culturally rich domains such as music.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EXR: An Interactive Immersive EHR Visualization in Extended Reality</title>
<link>https://arxiv.org/abs/2512.05438</link>
<guid>https://arxiv.org/abs/2512.05438</guid>
<content:encoded><![CDATA[
arXiv:2512.05438v1 Announce Type: cross 
Abstract: This paper presents the design and implementation of an Extended Reality (XR) platform for immersive, interactive visualization of Electronic Health Records (EHRs). The system extends beyond conventional 2D interfaces by visualizing both structured and unstructured patient data into a shared 3D environment, enabling intuitive exploration and real-time collaboration. The modular infrastructure integrates FHIR-based EHR data with volumetric medical imaging and AI-generated segmentation, ensuring interoperability with modern healthcare systems. The platform's capabilities are demonstrated using synthetic EHR datasets and computed tomography (CT)-derived spine models processed through an AI-powered segmentation pipeline. This work suggests that such integrated XR solutions could form the foundation for next-generation clinical decision-support tools, where advanced data infrastructures are directly accessible in an interactive and spatially rich environment.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Do We Really Even Need Data? A Modern Look at Drawing Inference with Predicted Data</title>
<link>https://arxiv.org/abs/2512.05456</link>
<guid>https://arxiv.org/abs/2512.05456</guid>
<content:encoded><![CDATA[
arXiv:2512.05456v1 Announce Type: cross 
Abstract: As artificial intelligence and machine learning tools become more accessible, and scientists face new obstacles to data collection (e.g., rising costs, declining survey response rates), researchers increasingly use predictions from pre-trained algorithms as substitutes for missing or unobserved data. Though appealing for financial and logistical reasons, using standard tools for inference can misrepresent the association between independent variables and the outcome of interest when the true, unobserved outcome is replaced by a predicted value. In this paper, we characterize the statistical challenges inherent to drawing inference with predicted data (IPD) and show that high predictive accuracy does not guarantee valid downstream inference. We show that all such failures reduce to statistical notions of (i) bias, when predictions systematically shift the estimand or distort relationships among variables, and (ii) variance, when uncertainty from the prediction model and the intrinsic variability of the true data are ignored. We then review recent methods for conducting IPD and discuss how this framework is deeply rooted in classical statistical theory. We then comment on some open questions and interesting avenues for future work in this area, and end with some comments on how to use predicted data in scientific studies that is both transparent and statistically principled.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model Gateway: Model Management Platform for Model-Driven Drug Discovery</title>
<link>https://arxiv.org/abs/2512.05462</link>
<guid>https://arxiv.org/abs/2512.05462</guid>
<content:encoded><![CDATA[
arXiv:2512.05462v1 Announce Type: cross 
Abstract: This paper presents the Model Gateway, a management platform for managing machine learning (ML) and scientific computational models in the drug discovery pipeline. The platform supports Large Language Model (LLM) Agents and Generative AI-based tools to perform ML model management tasks in our Machine Learning operations (MLOps) pipelines, such as the dynamic consensus model, a model that aggregates several scientific computational models, registration and management, retrieving model information, asynchronous submission/execution of models, and receiving results once the model complete executions. The platform includes a Model Owner Control Panel, Platform Admin Tools, and Model Gateway API service for interacting with the platform and tracking model execution. The platform achieves a 0% failure rate when testing scaling beyond 10k simultaneous application clients consume models. The Model Gateway is a fundamental part of our model-driven drug discovery pipeline. It has the potential to significantly accelerate the development of new drugs with the maturity of our MLOps infrastructure and the integration of LLM Agents and Generative AI tools.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SSDLabeler: Realistic semi-synthetic data generation for multi-label artifact classification in EEG</title>
<link>https://arxiv.org/abs/2512.05500</link>
<guid>https://arxiv.org/abs/2512.05500</guid>
<content:encoded><![CDATA[
arXiv:2512.05500v1 Announce Type: cross 
Abstract: EEG recordings are inherently contaminated by artifacts such as ocular, muscular, and environmental noise, which obscure neural activity and complicate preprocessing. Artifact classification offers advantages in stability and transparency, providing a viable alternative to ICA-based methods that enable flexible use alongside human inspections and across various applications. However, artifact classification is limited by its training data as it requires extensive manual labeling, which cannot fully cover the diversity of real-world EEG. Semi-synthetic data (SSD) methods have been proposed to address this limitation, but prior approaches typically injected single artifact types using ICA components or required separately recorded artifact signals, reducing both the realism of the generated data and the applicability of the method. To overcome these issues, we introduce SSDLabeler, a framework that generates realistic, annotated SSDs by decomposing real EEG with ICA, epoch-level artifact verification using RMS and PSD criteria, and reinjecting multiple artifact types into clean data. When applied to train a multi-label artifact classifier, it improved accuracy on raw EEG across diverse conditions compared to prior SSD and raw EEG training, establishing a scalable foundation for artifact handling that captures the co-occurrence and complexity of real EEG.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lyrics Matter: Exploiting the Power of Learnt Representations for Music Popularity Prediction</title>
<link>https://arxiv.org/abs/2512.05508</link>
<guid>https://arxiv.org/abs/2512.05508</guid>
<content:encoded><![CDATA[
arXiv:2512.05508v1 Announce Type: cross 
Abstract: Accurately predicting music popularity is a critical challenge in the music industry, offering benefits to artists, producers, and streaming platforms. Prior research has largely focused on audio features, social metadata, or model architectures. This work addresses the under-explored role of lyrics in predicting popularity. We present an automated pipeline that uses LLM to extract high-dimensional lyric embeddings, capturing semantic, syntactic, and sequential information. These features are integrated into HitMusicLyricNet, a multimodal architecture that combines audio, lyrics, and social metadata for popularity score prediction in the range 0-100. Our method outperforms existing baselines on the SpotGenTrack dataset, which contains over 100,000 tracks, achieving 9% and 20% improvements in MAE and MSE, respectively. Ablation confirms that gains arise from our LLM-driven lyrics feature pipeline (LyricsAENet), underscoring the value of dense lyric representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DashFusion: Dual-stream Alignment with Hierarchical Bottleneck Fusion for Multimodal Sentiment Analysis</title>
<link>https://arxiv.org/abs/2512.05515</link>
<guid>https://arxiv.org/abs/2512.05515</guid>
<content:encoded><![CDATA[
arXiv:2512.05515v1 Announce Type: cross 
Abstract: Multimodal sentiment analysis (MSA) integrates various modalities, such as text, image, and audio, to provide a more comprehensive understanding of sentiment. However, effective MSA is challenged by alignment and fusion issues. Alignment requires synchronizing both temporal and semantic information across modalities, while fusion involves integrating these aligned features into a unified representation. Existing methods often address alignment or fusion in isolation, leading to limitations in performance and efficiency. To tackle these issues, we propose a novel framework called Dual-stream Alignment with Hierarchical Bottleneck Fusion (DashFusion). Firstly, dual-stream alignment module synchronizes multimodal features through temporal and semantic alignment. Temporal alignment employs cross-modal attention to establish frame-level correspondences among multimodal sequences. Semantic alignment ensures consistency across the feature space through contrastive learning. Secondly, supervised contrastive learning leverages label information to refine the modality features. Finally, hierarchical bottleneck fusion progressively integrates multimodal information through compressed bottleneck tokens, which achieves a balance between performance and computational efficiency. We evaluate DashFusion on three datasets: CMU-MOSI, CMU-MOSEI, and CH-SIMS. Experimental results demonstrate that DashFusion achieves state-of-the-art performance across various metrics, and ablation studies confirm the effectiveness of our alignment and fusion techniques. The codes for our experiments are available at https://github.com/ultramarineX/DashFusion.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Poodle: Seamlessly Scaling Down Large Language Models with Just-in-Time Model Replacement</title>
<link>https://arxiv.org/abs/2512.05525</link>
<guid>https://arxiv.org/abs/2512.05525</guid>
<content:encoded><![CDATA[
arXiv:2512.05525v1 Announce Type: cross 
Abstract: Businesses increasingly rely on large language models (LLMs) to automate simple repetitive tasks instead of developing custom machine learning models. LLMs require few, if any, training examples and can be utilized by users without expertise in model development. However, this comes at the cost of substantially higher resource and energy consumption compared to smaller models, which often achieve similar predictive performance for simple tasks. In this paper, we present our vision for just-in-time model replacement (JITR), where, upon identifying a recurring task in calls to an LLM, the model is replaced transparently with a cheaper alternative that performs well for this specific task. JITR retains the ease of use and low development effort of LLMs, while saving significant cost and energy. We discuss the main challenges in realizing our vision regarding the identification of recurring tasks and the creation of a custom model. Specifically, we argue that model search and transfer learning will play a crucial role in JITR to efficiently identify and fine-tune models for a recurring task. Using our JITR prototype Poodle, we achieve significant savings for exemplary tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Selective Auditory Attention to Musical Elements in Ecologically Valid Music Listening</title>
<link>https://arxiv.org/abs/2512.05528</link>
<guid>https://arxiv.org/abs/2512.05528</guid>
<content:encoded><![CDATA[
arXiv:2512.05528v1 Announce Type: cross 
Abstract: Art has long played a profound role in shaping human emotion, cognition, and behavior. While visual arts such as painting and architecture have been studied through eye tracking, revealing distinct gaze patterns between experts and novices, analogous methods for auditory art forms remain underdeveloped. Music, despite being a pervasive component of modern life and culture, still lacks objective tools to quantify listeners' attention and perceptual focus during natural listening experiences. To our knowledge, this is the first attempt to decode selective attention to musical elements using naturalistic, studio-produced songs and a lightweight consumer-grade EEG device with only four electrodes. By analyzing neural responses during real world like music listening, we test whether decoding is feasible under conditions that minimize participant burden and preserve the authenticity of the musical experience. Our contributions are fourfold: (i) decoding music attention in real studio-produced songs, (ii) demonstrating feasibility with a four-channel consumer EEG, (iii) providing insights for music attention decoding, and (iv) demonstrating improved model ability over prior work. Our findings suggest that musical attention can be decoded not only for novel songs but also across new subjects, showing performance improvements compared to existing approaches under our tested conditions. These findings show that consumer-grade devices can reliably capture signals, and that neural decoding in music could be feasible in real-world settings. This paves the way for applications in education, personalized music technologies, and therapeutic interventions.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Design-marginal calibration of Gaussian process predictive distributions: Bayesian and conformal approaches</title>
<link>https://arxiv.org/abs/2512.05611</link>
<guid>https://arxiv.org/abs/2512.05611</guid>
<content:encoded><![CDATA[
arXiv:2512.05611v1 Announce Type: cross 
Abstract: We study the calibration of Gaussian process (GP) predictive distributions in the interpolation setting from a design-marginal perspective. Conditioning on the data and averaging over a design measure \mu, we formalize \mu-coverage for central intervals and \mu-probabilistic calibration through randomized probability integral transforms. We introduce two methods. cps-gp adapts conformal predictive systems to GP interpolation using standardized leave-one-out residuals, yielding stepwise predictive distributions with finite-sample marginal calibration. bcr-gp retains the GP posterior mean and replaces the Gaussian residual by a generalized normal model fitted to cross-validated standardized residuals. A Bayesian selection rule-based either on a posterior upper quantile of the variance for conservative prediction or on a cross-posterior Kolmogorov-Smirnov criterion for probabilistic calibration-controls dispersion and tail behavior while producing smooth predictive distributions suitable for sequential design. Numerical experiments on benchmark functions compare cps-gp, bcr-gp, Jackknife+ for GPs, and the full conformal Gaussian process, using calibration metrics (coverage, Kolmogorov-Smirnov, integral absolute error) and accuracy or sharpness through the scaled continuous ranked probability score.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-the-Air Semantic Alignment with Stacked Intelligent Metasurfaces</title>
<link>https://arxiv.org/abs/2512.05657</link>
<guid>https://arxiv.org/abs/2512.05657</guid>
<content:encoded><![CDATA[
arXiv:2512.05657v1 Announce Type: cross 
Abstract: Semantic communication systems aim to transmit task-relevant information between devices capable of artificial intelligence, but their performance can degrade when heterogeneous transmitter-receiver models produce misaligned latent representations. Existing semantic alignment methods typically rely on additional digital processing at the transmitter or receiver, increasing overall device complexity. In this work, we introduce the first over-the-air semantic alignment framework based on stacked intelligent metasurfaces (SIM), which enables latent-space alignment directly in the wave domain, reducing substantially the computational burden at the device level. We model SIMs as trainable linear operators capable of emulating both supervised linear aligners and zero-shot Parseval-frame-based equalizers. To realize these operators physically, we develop a gradient-based optimization procedure that tailors the metasurface transfer function to a desired semantic mapping. Experiments with heterogeneous vision transformer (ViT) encoders show that SIMs can accurately reproduce both supervised and zero-shot semantic equalizers, achieving up to 90% task accuracy in regimes with high signal-to-noise ratio (SNR), while maintaining strong robustness even at low SNR values.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InverseCrafter: Efficient Video ReCapture as a Latent Domain Inverse Problem</title>
<link>https://arxiv.org/abs/2512.05672</link>
<guid>https://arxiv.org/abs/2512.05672</guid>
<content:encoded><![CDATA[
arXiv:2512.05672v1 Announce Type: cross 
Abstract: Recent approaches to controllable 4D video generation often rely on fine-tuning pre-trained Video Diffusion Models (VDMs). This dominant paradigm is computationally expensive, requiring large-scale datasets and architectural modifications, and frequently suffers from catastrophic forgetting of the model's original generative priors. Here, we propose InverseCrafter, an efficient inpainting inverse solver that reformulates the 4D generation task as an inpainting problem solved in the latent space. The core of our method is a principled mechanism to encode the pixel space degradation operator into a continuous, multi-channel latent mask, thereby bypassing the costly bottleneck of repeated VAE operations and backpropagation. InverseCrafter not only achieves comparable novel view generation and superior measurement consistency in camera control tasks with near-zero computational overhead, but also excels at general-purpose video inpainting with editing. Code is available at https://github.com/yeobinhong/InverseCrafter.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing the latent features of universal machine-learning interatomic potentials</title>
<link>https://arxiv.org/abs/2512.05717</link>
<guid>https://arxiv.org/abs/2512.05717</guid>
<content:encoded><![CDATA[
arXiv:2512.05717v1 Announce Type: cross 
Abstract: The past few years have seen the development of ``universal'' machine-learning interatomic potentials (uMLIPs) capable of approximating the ground-state potential energy surface across a wide range of chemical structures and compositions with reasonable accuracy. While these models differ in the architecture and the dataset used, they share the ability to compress a staggering amount of chemical information into descriptive latent features. Herein, we systematically analyze what the different uMLIPs have learned by quantitatively assessing the relative information content of their latent features with feature reconstruction errors as metrics, and observing how the trends are affected by the choice of training set and training protocol. We find that the uMLIPs encode chemical space in significantly distinct ways, with substantial cross-model feature reconstruction errors. When variants of the same model architecture are considered, trends become dependent on the dataset, target, and training protocol of choice. We also observe that fine-tuning of a uMLIP retains a strong pre-training bias in the latent features. Finally, we discuss how atom-level features, which are directly output by MLIPs, can be compressed into global structure-level features via concatenation of progressive cumulants, each adding significantly new information about the variability across the atomic environments within a given system.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KANFormer for Predicting Fill Probabilities via Survival Analysis in Limit Order Books</title>
<link>https://arxiv.org/abs/2512.05734</link>
<guid>https://arxiv.org/abs/2512.05734</guid>
<content:encoded><![CDATA[
arXiv:2512.05734v1 Announce Type: cross 
Abstract: This paper introduces KANFormer, a novel deep-learning-based model for predicting the time-to-fill of limit orders by leveraging both market- and agent-level information. KANFormer combines a Dilated Causal Convolutional network with a Transformer encoder, enhanced by Kolmogorov-Arnold Networks (KANs), which improve nonlinear approximation. Unlike existing models that rely solely on a series of snapshots of the limit order book, KANFormer integrates the actions of agents related to LOB dynamics and the position of the order in the queue to more effectively capture patterns related to execution likelihood. We evaluate the model using CAC 40 index futures data with labeled orders. The results show that KANFormer outperforms existing works in both calibration (Right-Censored Log-Likelihood, Integrated Brier Score) and discrimination (C-index, time-dependent AUC). We further analyze feature importance over time using SHAP (SHapley Additive exPlanations). Our results highlight the benefits of combining rich market signals with expressive neural architectures to achieve accurate and interpretabl predictions of fill probabilities.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast Anti-Jamming Cognitive Radar Deployment Algorithm Based on Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05753</link>
<guid>https://arxiv.org/abs/2512.05753</guid>
<content:encoded><![CDATA[
arXiv:2512.05753v1 Announce Type: cross 
Abstract: The fast deployment of cognitive radar to counter jamming remains a critical challenge in modern warfare, where more efficient deployment leads to quicker detection of targets. Existing methods are primarily based on evolutionary algorithms, which are time-consuming and prone to falling into local optima. We tackle these drawbacks via the efficient inference of neural networks and propose a brand new framework: Fast Anti-Jamming Radar Deployment Algorithm (FARDA). We first model the radar deployment problem as an end-to-end task and design deep reinforcement learning algorithms to solve it, where we develop integrated neural modules to perceive heatmap information and a brand new reward format. Empirical results demonstrate that our method achieves coverage comparable to evolutionary algorithms while deploying radars approximately 7,000 times faster. Further ablation experiments confirm the necessity of each component of FARDA.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Missing Layer of AGI: From Pattern Alchemy to Coordination Physics</title>
<link>https://arxiv.org/abs/2512.05765</link>
<guid>https://arxiv.org/abs/2512.05765</guid>
<content:encoded><![CDATA[
arXiv:2512.05765v1 Announce Type: cross 
Abstract: Influential critiques argue that Large Language Models (LLMs) are a dead end for AGI: "mere pattern matchers" structurally incapable of reasoning or planning. We argue this conclusion misidentifies the bottleneck: it confuses the ocean with the net. Pattern repositories are the necessary System-1 substrate; the missing component is a System-2 coordination layer that selects, constrains, and binds these patterns. We formalize this layer via UCCT, a theory of semantic anchoring that models reasoning as a phase transition governed by effective support (rho_d), representational mismatch (d_r), and an adaptive anchoring budget (gamma log k). Under this lens, ungrounded generation is simply an unbaited retrieval of the substrate's maximum likelihood prior, while "reasoning" emerges when anchors shift the posterior toward goal-directed constraints. We translate UCCT into architecture with MACI, a coordination stack that implements baiting (behavior-modulated debate), filtering (Socratic judging), and persistence (transactional memory). By reframing common objections as testable coordination failures, we argue that the path to AGI runs through LLMs, not around them.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curvature-Regularized Variational Autoencoder for 3D Scene Reconstruction from Sparse Depth</title>
<link>https://arxiv.org/abs/2512.05783</link>
<guid>https://arxiv.org/abs/2512.05783</guid>
<content:encoded><![CDATA[
arXiv:2512.05783v1 Announce Type: cross 
Abstract: When depth sensors provide only 5% of needed measurements, reconstructing complete 3D scenes becomes difficult. Autonomous vehicles and robots cannot tolerate the geometric errors that sparse reconstruction introduces. We propose curvature regularization through a discrete Laplacian operator, achieving 18.1% better reconstruction accuracy than standard variational autoencoders. Our contribution challenges an implicit assumption in geometric deep learning: that combining multiple geometric constraints improves performance. A single well-designed regularization term not only matches but exceeds the effectiveness of complex multi-term formulations. The discrete Laplacian offers stable gradients and noise suppression with just 15% training overhead and zero inference cost. Code and models are available at https://github.com/Maryousefi/GeoVAE-3D.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine-learning-enabled interpretation of tribological deformation patterns in large-scale MD data</title>
<link>https://arxiv.org/abs/2512.05818</link>
<guid>https://arxiv.org/abs/2512.05818</guid>
<content:encoded><![CDATA[
arXiv:2512.05818v1 Announce Type: cross 
Abstract: Molecular dynamics (MD) simulations have become indispensable for exploring tribological deformation patterns at the atomic scale. However, transforming the resulting high-dimensional data into interpretable deformation pattern maps remains a resource-intensive and largely manual process. In this work, we introduce a data-driven workflow that automates this interpretation step using unsupervised and supervised learning. Grain-orientation-colored computational tomograph pictures obtained from CuNi alloy simulations were first compressed through an autoencoder to a 32-dimensional global feature vector. Despite this strong compression, the reconstructed images retained the essential microstructural motifs: grain boundaries, stacking faults, twins, and partial lattice rotations, while omitting only the finest defects. The learned representations were then combined with simulation metadata (composition, load, time, temperature, and spatial position) to train a CNN-MLP model to predict the dominant deformation pattern. The resulting model achieves a prediction accuracy of approximately 96% on validation data. A refined evaluation strategy, in which an entire spatial region containing distinct grains was excluded from training, provides a more robust measure of generalization. The approach demonstrates that essential tribological deformation signatures can be automatically identified and classified from structural images using Machine Learning. This proof of concept constitutes a first step towards fully automated, data-driven construction of tribological mechanism maps and, ultimately, toward predictive modeling frameworks that may reduce the need for large-scale MD simulation campaigns.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrapping Fuzzers for Compilers of Low-Resource Language Dialects Using Language Models</title>
<link>https://arxiv.org/abs/2512.05887</link>
<guid>https://arxiv.org/abs/2512.05887</guid>
<content:encoded><![CDATA[
arXiv:2512.05887v1 Announce Type: cross 
Abstract: Modern extensible compiler frameworks-such as MLIR-enable rapid creation of domain-specific language dialects. This flexibility, however, makes correctness harder to ensure as the same extensibility that accelerates development also complicates maintaining the testing infrastructure. Extensible languages require automated test generation that is both dialect-agnostic (works across dialects without manual adaptation) and dialect-effective (targets dialect-specific features to find bugs). Existing approaches typically sacrifice one of these goals by either requiring manually constructed seed corpora for each dialect, or by failing to be effective. We present a dialect-agnostic and dialect-effective grammar-based and coverage-guided fuzzing approach for extensible compilers that combines two key insights from existing work: (i) the grammars of dialects, which already encode the structural and type constraints, can often be extracted automatically from the dialect specification; and (ii) these grammars can be used in combination with pre-trained large language models to automatically generate representative and diverse seed inputs from the full dialect space without requiring any manual input or training data. These seeds can then be used to bootstrap coverage-guided fuzzers. We built this approach into a tool, Germinator. When evaluated on six MLIR projects spanning 91 dialects, Germinator generated seeds improve line coverage by 10-120% over grammar-based baselines. We compare against grammar-based baselines because they are the only class of existing automatic seed generators that can be applied uniformly across MLIR's heterogeneous dialect ecosystem. Germinator discovers 88 previously unknown bugs (40 confirmed), including 23 in dialects with no prior automated test generators, demonstrating effective and controllable testing of low-resource dialects at scale.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NICE: Neural Implicit Craniofacial Model for Orthognathic Surgery Prediction</title>
<link>https://arxiv.org/abs/2512.05920</link>
<guid>https://arxiv.org/abs/2512.05920</guid>
<content:encoded><![CDATA[
arXiv:2512.05920v1 Announce Type: cross 
Abstract: Orthognathic surgery is a crucial intervention for correcting dentofacial skeletal deformities to enhance occlusal functionality and facial aesthetics. Accurate postoperative facial appearance prediction remains challenging due to the complex nonlinear interactions between skeletal movements and facial soft tissue. Existing biomechanical, parametric models and deep-learning approaches either lack computational efficiency or fail to fully capture these intricate interactions. To address these limitations, we propose Neural Implicit Craniofacial Model (NICE) which employs implicit neural representations for accurate anatomical reconstruction and surgical outcome prediction. NICE comprises a shape module, which employs region-specific implicit Signed Distance Function (SDF) decoders to reconstruct the facial surface, maxilla, and mandible, and a surgery module, which employs region-specific deformation decoders. These deformation decoders are driven by a shared surgical latent code to effectively model the complex, nonlinear biomechanical response of the facial surface to skeletal movements, incorporating anatomical prior knowledge. The deformation decoders output point-wise displacement fields, enabling precise modeling of surgical outcomes. Extensive experiments demonstrate that NICE outperforms current state-of-the-art methods, notably improving prediction accuracy in critical facial regions such as lips and chin, while robustly preserving anatomical integrity. This work provides a clinically viable tool for enhanced surgical planning and patient consultation in orthognathic procedures.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BalLOT: Balanced $k$-means clustering with optimal transport</title>
<link>https://arxiv.org/abs/2512.05926</link>
<guid>https://arxiv.org/abs/2512.05926</guid>
<content:encoded><![CDATA[
arXiv:2512.05926v1 Announce Type: cross 
Abstract: We consider the fundamental problem of balanced $k$-means clustering. In particular, we introduce an optimal transport approach to alternating minimization called BalLOT, and we show that it delivers a fast and effective solution to this problem. We establish this with a variety of numerical experiments before proving several theoretical guarantees. First, we prove that for generic data, BalLOT produces integral couplings at each step. Next, we perform a landscape analysis to provide theoretical guarantees for both exact and partial recoveries of planted clusters under the stochastic ball model. Finally, we propose initialization schemes that achieve one-step recovery of planted clusters.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Designing an Optimal Sensor Network via Minimizing Information Loss</title>
<link>https://arxiv.org/abs/2512.05940</link>
<guid>https://arxiv.org/abs/2512.05940</guid>
<content:encoded><![CDATA[
arXiv:2512.05940v1 Announce Type: cross 
Abstract: Optimal experimental design is a classic topic in statistics, with many well-studied problems, applications, and solutions. The design problem we study is the placement of sensors to monitor spatiotemporal processes, explicitly accounting for the temporal dimension in our modeling and optimization. We observe that recent advancements in computational sciences often yield large datasets based on physics-based simulations, which are rarely leveraged in experimental design. We introduce a novel model-based sensor placement criterion, along with a highly-efficient optimization algorithm, which integrates physics-based simulations and Bayesian experimental design principles to identify sensor networks that "minimize information loss" from simulated data. Our technique relies on sparse variational inference and (separable) Gauss-Markov priors, and thus may adapt many techniques from Bayesian experimental design. We validate our method through a case study monitoring air temperature in Phoenix, Arizona, using state-of-the-art physics-based simulations. Our results show our framework to be superior to random or quasi-random sampling, particularly with a limited number of sensors. We conclude by discussing practical considerations and implications of our framework, including more complex modeling tools and real-world deployments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consequences of Kernel Regularity for Bandit Optimization</title>
<link>https://arxiv.org/abs/2512.05957</link>
<guid>https://arxiv.org/abs/2512.05957</guid>
<content:encoded><![CDATA[
arXiv:2512.05957v1 Announce Type: cross 
Abstract: In this work we investigate the relationship between kernel regularity and algorithmic performance in the bandit optimization of RKHS functions. While reproducing kernel Hilbert space (RKHS) methods traditionally rely on global kernel regressors, it is also common to use a smoothness-based approach that exploits local approximations. We show that these perspectives are deeply connected through the spectral properties of isotropic kernels. In particular, we characterize the Fourier spectra of the Mat\'ern, square-exponential, rational-quadratic, $\gamma$-exponential, piecewise-polynomial, and Dirichlet kernels, and show that the decay rate determines asymptotic regret from both viewpoints. For kernelized bandit algorithms, spectral decay yields upper bounds on the maximum information gain, governing worst-case regret, while for smoothness-based methods, the same decay rates establish H\"older space embeddings and Besov space norm-equivalences, enabling local continuity analysis. These connections show that kernel-based and locally adaptive algorithms can be analyzed within a unified framework. This allows us to derive explicit regret bounds for each kernel family, obtaining novel results in several cases and providing improved analysis for others. Furthermore, we analyze LP-GP-UCB, an algorithm that combines both approaches, augmenting global Gaussian process surrogates with local polynomial estimators. While the hybrid approach does not uniformly dominate specialized methods, it achieves order-optimality across multiple kernel families.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Retrieval-Augmented Generation with Entity Linking for Educational Platforms</title>
<link>https://arxiv.org/abs/2512.05967</link>
<guid>https://arxiv.org/abs/2512.05967</guid>
<content:encoded><![CDATA[
arXiv:2512.05967v1 Announce Type: cross 
Abstract: In the era of Large Language Models (LLMs), Retrieval-Augmented Generation (RAG) architectures are gaining significant attention for their ability to ground language generation in reliable knowledge sources. Despite their impressive effectiveness in many areas, RAG systems based solely on semantic similarity often fail to ensure factual accuracy in specialized domains, where terminological ambiguity can affect retrieval relevance. This study proposes an enhanced RAG architecture that integrates a factual signal derived from Entity Linking to improve the accuracy of educational question-answering systems in Italian. The system includes a Wikidata-based Entity Linking module and implements three re-ranking strategies to combine semantic and entity-based information: a hybrid score weighting model, reciprocal rank fusion, and a cross-encoder re-ranker. Experiments were conducted on two benchmarks: a custom academic dataset and the standard SQuAD-it dataset. Results show that, in domain-specific contexts, the hybrid schema based on reciprocal rank fusion significantly outperforms both the baseline and the cross-encoder approach, while the cross-encoder achieves the best results on the general-domain dataset. These findings confirm the presence of an effect of domain mismatch and highlight the importance of domain adaptation and hybrid ranking strategies to enhance factual precision and reliability in retrieval-augmented generation. They also demonstrate the potential of entity-aware RAG systems in educational environments, fostering adaptive and reliable AI-based tutoring tools.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical Guarantees for Approximate Stationary Points of Shallow Neural Networks</title>
<link>https://arxiv.org/abs/2205.04491</link>
<guid>https://arxiv.org/abs/2205.04491</guid>
<content:encoded><![CDATA[
arXiv:2205.04491v2 Announce Type: replace 
Abstract: Since statistical guarantees for neural networks are usually restricted to global optima of intricate objective functions, it is unclear whether these theories explain the performances of actual outputs of neural network pipelines. The goal of this paper is, therefore, to bring statistical theory closer to practice. We develop statistical guarantees for shallow linear neural networks that coincide up to logarithmic factors with the global optima but apply to stationary points and the points nearby. These results support the common notion that neural networks do not necessarily need to be optimized globally from a mathematical perspective. We then extend our statistical guarantees to shallow ReLU neural networks, assuming the first layer weight matrices are nearly identical for the stationary network and the target. More generally, despite being limited to shallow neural networks for now, our theories make an important step forward in describing the practical properties of neural networks in mathematical terms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Learning of Gaussian Process Latent Variable Models through Stochastic Gradient Annealed Importance Sampling</title>
<link>https://arxiv.org/abs/2408.06710</link>
<guid>https://arxiv.org/abs/2408.06710</guid>
<content:encoded><![CDATA[
arXiv:2408.06710v2 Announce Type: replace 
Abstract: Gaussian Process Latent Variable Models (GPLVMs) have become increasingly popular for unsupervised tasks such as dimensionality reduction and missing data recovery due to their flexibility and non-linear nature. An importance-weighted version of the Bayesian GPLVMs has been proposed to obtain a tighter variational bound. However, this version of the approach is primarily limited to analyzing simple data structures, as the generation of an effective proposal distribution can become quite challenging in high-dimensional spaces or with complex data sets. In this work, we propose an Annealed Importance Sampling (AIS) approach to address these issues. By transforming the posterior into a sequence of intermediate distributions using annealing, we combine the strengths of Sequential Monte Carlo samplers and VI to explore a wider range of posterior distributions and gradually approach the target distribution. We further propose an efficient algorithm by reparameterizing all variables in the evidence lower bound (ELBO). Experimental results on both toy and image datasets demonstrate that our method outperforms state-of-the-art methods in terms of tighter variational bounds, higher log-likelihoods, and more robust convergence.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting the Future: All-at-Once Event Sequence Forecasting with Horizon Matching</title>
<link>https://arxiv.org/abs/2408.13131</link>
<guid>https://arxiv.org/abs/2408.13131</guid>
<content:encoded><![CDATA[
arXiv:2408.13131v3 Announce Type: replace 
Abstract: Long-horizon events forecasting is a crucial task across various domains, including retail, finance, healthcare, and social networks. Traditional models for event sequences often extend to forecasting on a horizon using an autoregressive (recursive) multi-step strategy, which has limited effectiveness due to typical convergence to constant or repetitive outputs. To address this limitation, we introduce DEF, a novel approach for simultaneous forecasting of multiple future events on a horizon with high accuracy and diversity. Our method optimally aligns predictions with ground truth events during training by using a novel matching-based loss function. We establish a new state-of-the-art in long-horizon event prediction, achieving up to a 50% relative improvement over existing temporal point processes and event prediction models. Furthermore, we achieve state-of-the-art performance in next-event prediction tasks while demonstrating high computational efficiency during inference.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARTAN: A Sparse Transformer World Model Attending to What Matters</title>
<link>https://arxiv.org/abs/2411.06890</link>
<guid>https://arxiv.org/abs/2411.06890</guid>
<content:encoded><![CDATA[
arXiv:2411.06890v3 Announce Type: replace 
Abstract: Capturing the interactions between entities in a structured way plays a central role in world models that flexibly adapt to changes in the environment. Recent works motivate the benefits of models that explicitly represent the structure of interactions and formulate the problem as discovering local causal structures. In this work, we demonstrate that reliably capturing these relationships in complex settings remains challenging. To remedy this shortcoming, we postulate that sparsity is a critical ingredient for the discovery of such local structures. To this end, we present the SPARse TrANsformer World model (SPARTAN), a Transformer-based world model that learns context-dependent interaction structures between entities in a scene. By applying sparsity regularisation on the attention patterns between object-factored tokens, SPARTAN learns sparse, context-dependent interaction graphs that accurately predict future object states. We further extend our model to adapt to sparse interventions with unknown targets in the dynamics of the environment. This results in a highly interpretable world model that can efficiently adapt to changes. Empirically, we evaluate SPARTAN against the current state-of-the-art in object-centric world models in observation-based environments and demonstrate that our model can learn local causal graphs that accurately reflect the underlying interactions between objects, achieving significantly improved few-shot adaptation to dynamics changes, as well as robustness against distractors.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Control Perspective on Training PINNs</title>
<link>https://arxiv.org/abs/2501.18582</link>
<guid>https://arxiv.org/abs/2501.18582</guid>
<content:encoded><![CDATA[
arXiv:2501.18582v3 Announce Type: replace 
Abstract: We investigate the training of Physics-Informed Neural Networks (PINNs) from a control-theoretic perspective. Using gradient descent with resampling, we interpret the training dynamics as asymptotically equivalent to a stochastic control-affine system, where sampling effects act as process disturbances and measurement noise. Within this framework, we introduce two controllers for dynamically adapting the physics weight: an integral controller and a leaky integral controller. We theoretically analyze their asymptotic properties under the accuracy-robustness trade-off, and we evaluate them on a toy example. Numerical evidence suggests that the integral controller achieves accurate and robust convergence when the physical model is correct, whereas the leaky integrator provides improved performance in the presence of model mismatch. This work represents a first step toward convergence guarantees and principled training algorithms tailored to the distinct characteristics of PINN tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Bias of Spectral Descent and Muon on Multiclass Separable Data</title>
<link>https://arxiv.org/abs/2502.04664</link>
<guid>https://arxiv.org/abs/2502.04664</guid>
<content:encoded><![CDATA[
arXiv:2502.04664v4 Announce Type: replace 
Abstract: Different gradient-based methods for optimizing overparameterized models can all achieve zero training error yet converge to distinctly different solutions inducing different generalization properties. We provide the first complete characterization of implicit optimization bias for p-norm normalized steepest descent (NSD) and momentum steepest descent (NMD) algorithms in multi-class linear classification with cross-entropy loss. Our key theoretical contribution is proving that these algorithms converge to solutions maximizing the margin with respect to the classifier matrix's p-norm, with established convergence rates. These results encompass important special cases including Spectral Descent and Muon, which we show converge to max-margin solutions with respect to the spectral norm. A key insight of our contribution is that the analysis of general entry-wise and Schatten p-norms can be reduced to the analysis of NSD/NMD with max-norm by exploiting a natural ordering property between all p-norms relative to the max-norm and its dual sum-norm. For the specific case of descent with respect to the max-norm, we further extend our analysis to include preconditioning, showing that Adam converges to the matrix's max-norm solution. Our results demonstrate that the multi-class linear setting, which is inherently richer than the binary counterpart, provides the most transparent framework for studying implicit biases of matrix-parameter optimization algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypergraph Foundation Model</title>
<link>https://arxiv.org/abs/2503.01203</link>
<guid>https://arxiv.org/abs/2503.01203</guid>
<content:encoded><![CDATA[
arXiv:2503.01203v2 Announce Type: replace 
Abstract: Hypergraph neural networks (HGNNs) effectively model complex high-order relationships in domains like protein interactions and social networks by connecting multiple vertices through hyperedges, enhancing modeling capabilities, and reducing information loss. Developing foundation models for hypergraphs is challenging due to their distinct data, which includes both vertex features and intricate structural information. We present Hyper-FM, a Hypergraph Foundation Model for multi-domain knowledge extraction, featuring Hierarchical High-Order Neighbor Guided Vertex Knowledge Embedding for vertex feature representation and Hierarchical Multi-Hypergraph Guided Structural Knowledge Extraction for structural information. Additionally, we curate 11 text-attributed hypergraph datasets to advance research between HGNNs and LLMs. Experiments on these datasets show that Hyper-FM outperforms baseline methods by approximately 13.4%, validating our approach. Furthermore, we propose the first scaling law for hypergraph foundation models, demonstrating that increasing domain diversity significantly enhances performance, unlike merely augmenting vertex and hyperedge counts. This underscores the critical role of domain diversity in scaling hypergraph models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interval Regression: A Comparative Study with Proposed Models</title>
<link>https://arxiv.org/abs/2503.02011</link>
<guid>https://arxiv.org/abs/2503.02011</guid>
<content:encoded><![CDATA[
arXiv:2503.02011v2 Announce Type: replace 
Abstract: Regression models are essential for a wide range of real-world applications. However, in practice, target values are not always precisely known; instead, they may be represented as intervals of acceptable values. This challenge has led to the development of Interval Regression models. In this study, we provide a comprehensive review of existing Interval Regression models and introduce alternative models for comparative analysis. Experiments are conducted on both real-world and synthetic datasets to offer a broad perspective on model performance. The results demonstrate that no single model is universally optimal, highlighting the importance of selecting the most suitable model for each specific scenario.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fair Text Classification via Transferable Representations</title>
<link>https://arxiv.org/abs/2503.07691</link>
<guid>https://arxiv.org/abs/2503.07691</guid>
<content:encoded><![CDATA[
arXiv:2503.07691v2 Announce Type: replace 
Abstract: Group fairness is a central research topic in text classification, where reaching fair treatment between sensitive groups (e.g., women and men) remains an open challenge. We propose an approach that extends the use of the Wasserstein Dependency Measure for learning unbiased neural text classifiers. Given the challenge of distinguishing fair from unfair information in a text encoder, we draw inspiration from adversarial training by inducing independence between representations learned for the target label and those for a sensitive attribute. We further show that Domain Adaptation can be efficiently leveraged to remove the need for access to the sensitive attributes in the dataset we cure. We provide both theoretical and empirical evidence that our approach is well-founded.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Weight Imprinting: Insights from Neural Collapse and Proxy-Based Aggregation</title>
<link>https://arxiv.org/abs/2503.14572</link>
<guid>https://arxiv.org/abs/2503.14572</guid>
<content:encoded><![CDATA[
arXiv:2503.14572v3 Announce Type: replace 
Abstract: The capacity of foundation models allows for their application to new, unseen tasks. The adaptation to such tasks is called transfer learning. An efficient transfer learning method that circumvents parameter optimization is imprinting. The conceptual differences between studies on imprinting form the basis of our systematic investigation. In this work, we propose the general \texttt{IMPRINT} framework, identifying three main components: generation, normalization, and aggregation. Through the lens of this framework, we conduct an in-depth analysis and a comparison of the existing methods. Our findings reveal the benefits of representing novel data with multiple proxies in the generation step and show the importance of proper normalization. Beyond an extensive analytical grounding, our framework enables us to propose a novel variant of imprinting which outperforms previous work on transfer learning tasks by 4\%. This variant determines proxies through clustering motivated by the neural collapse phenomenon -- a connection that we draw for the first time. We publicly release our code at https://github.com/DATEXIS/IMPRINT.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Trustworthy By Design Classification Model for Building Energy Retrofit Decision Support</title>
<link>https://arxiv.org/abs/2504.06055</link>
<guid>https://arxiv.org/abs/2504.06055</guid>
<content:encoded><![CDATA[
arXiv:2504.06055v2 Announce Type: replace 
Abstract: Improving energy efficiency in residential buildings is critical to combating climate change and reducing greenhouse gas emissions. Retrofitting existing buildings, which contribute a significant share of energy use, is therefore a key priority, especially in regions with outdated building stock. Artificial Intelligence (AI) and Machine Learning (ML) can automate retrofit decision-making and find retrofit strategies. However, their use faces challenges of data availability, model transparency, and compliance with national and EU AI regulations including the AI act, ethics guidelines and the ALTAI. This paper presents a trustworthy-by-design ML-based decision support framework that recommends energy efficiency strategies for residential buildings using minimal user-accessible inputs. The framework merges Conditional Tabular Generative Adversarial Networks (CTGAN) to augment limited and imbalanced data with a neural network-based multi-label classifier that predicts potential combinations of retrofit actions. To support explanation and trustworthiness, an Explainable AI (XAI) layer using SHapley Additive exPlanations (SHAP) clarifies the rationale behind recommendations and guides feature engineering. Two case studies validate performance and generalization: the first leveraging a well-established, large EPC dataset for England and Wales; the second using a small, imbalanced post-retrofit dataset from Latvia (RETROFIT-LAT). Results show that the framework can handle diverse data conditions and improve performance up to 53% compared to the baseline. Overall, the proposed framework provides a feasible, interpretable, and trustworthy AI system for building retrofit decision support through assured performance, usability, and transparency to aid stakeholders in prioritizing effective energy investments and support regulation-compliant, data-driven innovation in sustainable energy transition.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Conformal Prediction Under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2505.15721</link>
<guid>https://arxiv.org/abs/2505.15721</guid>
<content:encoded><![CDATA[
arXiv:2505.15721v2 Announce Type: replace 
Abstract: Conformal prediction (CP) provides sets of candidate classes with a guaranteed probability of containing the true class. However, it typically relies on a calibration set with clean labels. We address privacy-sensitive scenarios where the aggregator is untrusted and can only access a perturbed version of the true labels. We propose two complementary approaches under local differential privacy (LDP). In the first approach, users do not access the model but instead provide their input features and a perturbed label using a k-ary randomized response. In the second approach, which enforces stricter privacy constraints, users add noise to their conformity score by binary search response. This method requires access to the classification model but preserves both data and label privacy. Both approaches compute the conformal threshold directly from noisy data without accessing the true labels. We prove finite-sample coverage guarantees and demonstrate robust coverage even under severe randomization. This approach unifies strong local privacy with predictive uncertainty control, making it well-suited for sensitive applications such as medical imaging or large language model queries, regardless of whether users can (or are willing to) compute their own scores.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IF-GUIDE: Influence Function-Guided Detoxification of LLMs</title>
<link>https://arxiv.org/abs/2506.01790</link>
<guid>https://arxiv.org/abs/2506.01790</guid>
<content:encoded><![CDATA[
arXiv:2506.01790v3 Announce Type: replace 
Abstract: We study how training data contributes to the emergence of toxic behaviors in large language models. Most prior work on reducing model toxicity adopts reactive approaches, such as fine-tuning pre-trained (and potentially toxic) models to align them with human values. In contrast, we propose a proactive approach, IF-GUIDE, that leverages influence functions to identify and suppress harmful tokens in the training data. To this end, we first show that standard influence functions are ineffective at discovering harmful training records. We then present a novel adaptation that measures token-level attributions from training data to model toxicity, along with techniques for selecting toxic training documents and a learning objective that can be integrated into both pre-training and fine-tuning. Moreover, IF-GUIDE does not rely on human-preference data, which is typically required by existing alignment methods. In our evaluation, we demonstrate that IF-GUIDE substantially reduces both explicit and implicit toxicity-by up to 10$\times$ compared to uncensored models, and up to 3$\times$ compared to baseline alignment methods such as DPO and RAD-across both pre-training and fine-tuning scenarios. IF-GUIDE is computationally efficient: a billion-parameter model is not necessary for computing influence scores; a million-parameter model-with 7.5$\times$ fewer parameters-can effectively serve as a proxy for identifying harmful data. Our code is publicly available at: https://github.com/ztcoalson/IF-Guide
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2506.07413</link>
<guid>https://arxiv.org/abs/2506.07413</guid>
<content:encoded><![CDATA[
arXiv:2506.07413v3 Announce Type: replace 
Abstract: Contrastive learning has proven to be highly efficient and adaptable in shaping representation spaces across diverse modalities by pulling similar samples together and pushing dissimilar ones apart. However, two key limitations persist: (1) Without explicit regulation of the embedding distribution, semantically related instances can inadvertently be pushed apart unless complementary signals guide pair selection, and (2) excessive reliance on large in-batch negatives and tailored augmentations hinders generalization. To address these limitations, we propose Variational Supervised Contrastive Learning (VarCon), which reformulates supervised contrastive learning as variational inference over latent class variables and maximizes a posterior-weighted evidence lower bound (ELBO) that replaces exhaustive pair-wise comparisons for efficient class-aware matching and grants fine-grained control over intra-class dispersion in the embedding space. Trained exclusively on image data, our experiments on CIFAR-10, CIFAR-100, ImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art performance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy on ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while converging in just 200 epochs; (2) yields substantially clearer decision boundaries and semantic organization in the embedding space, as evidenced by KNN classification, hierarchical clustering results, and transfer-learning assessments; and (3) demonstrates superior performance in few-shot learning than supervised baseline and superior robustness across various augmentation strategies. Our code is available at https://github.com/ziwenwang28/VarContrast.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LittleBit: Ultra Low-Bit Quantization via Latent Factorization</title>
<link>https://arxiv.org/abs/2506.13771</link>
<guid>https://arxiv.org/abs/2506.13771</guid>
<content:encoded><![CDATA[
arXiv:2506.13771v3 Announce Type: replace 
Abstract: Deploying large language models (LLMs) often faces challenges from substantial memory and computational costs. Quantization offers a solution, yet performance degradation in the sub-1-bit regime remains particularly difficult. This paper introduces LittleBit, a novel method for extreme LLM compression. It targets levels like 0.1 bits per weight (BPW), achieving nearly 31$\times$ memory reduction, e.g., Llama2-13B to under 0.9 GB. LittleBit represents weights in a low-rank form using latent matrix factorization, subsequently binarizing these factors. To counteract information loss from this extreme precision, it integrates a multi-scale compensation mechanism. This includes row, column, and an additional latent dimension that learns per-rank importance. Two key contributions enable effective training: Dual Sign-Value-Independent Decomposition (Dual-SVID) for quantization-aware training (QAT) initialization, and integrated Residual Compensation to mitigate errors. Extensive experiments confirm LittleBit's superiority in sub-1-bit quantization: e.g., its 0.1 BPW performance on Llama2-7B surpasses the leading method's 0.7 BPW. LittleBit establishes a new, viable size-performance trade-off--unlocking a potential 11.6$\times$ speedup over FP16 at the kernel level--and makes powerful LLMs practical for resource-constrained environments. Our code can be found at https://github.com/SamsungLabs/LittleBit.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional Generative Modeling for Enhanced Credit Risk Management in Supply Chain Finance</title>
<link>https://arxiv.org/abs/2506.15305</link>
<guid>https://arxiv.org/abs/2506.15305</guid>
<content:encoded><![CDATA[
arXiv:2506.15305v3 Announce Type: replace 
Abstract: The rapid expansion of cross-border e-commerce (CBEC) has created significant opportunities for small- and medium-sized sellers, yet financing remains a critical challenge due to their limited credit histories. Third-party logistics (3PL)-led supply chain finance (SCF) has emerged as a promising solution, leveraging in-transit inventory as collateral. We propose an advanced credit risk management framework tailored for 3PL-led SCF, addressing the dual challenges of credit risk assessment and loan size determination. Specifically, we leverage conditional generative modeling of sales distributions through Quantile-Regression-based Generative Metamodeling (QRGMM) as the foundation for risk measures estimation. We propose a unified framework that enables flexible estimation of multiple risk measures while introducing a functional risk measure formulation that systematically captures the relationship between these risk measures and varying loan levels, supported by theoretical guarantees. To capture complex covariate interactions in e-commerce sales data, we integrate QRGMM with Deep Factorization Machines (DeepFM). Extensive experiments on synthetic and real-world data validate the efficacy of our model for credit risk assessment and loan size determination. This study explores the use of generative models in CBEC SCF risk management, illustrating their potential to strengthen credit assessment and support financing for small- and medium-sized sellers.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Establishing Validity for Distance Functions and Internal Clustering Validity Indices in Correlation Space</title>
<link>https://arxiv.org/abs/2507.16497</link>
<guid>https://arxiv.org/abs/2507.16497</guid>
<content:encoded><![CDATA[
arXiv:2507.16497v2 Announce Type: replace 
Abstract: Internal clustering validity indices (ICVIs) assess clustering quality without ground truth labels. Comparative studies consistently find that no single ICVI outperforms others across datasets, leaving practitioners without principled ICVI selection. We argue that inconsistent ICVI performance arises because studies evaluate them based on matching human labels rather than measuring the quality of the discovered structure in the data, using datasets without formally quantifying the structure type and quality. Structure type refers to the mathematical organisation in data that clustering aims to discover. Validity theory requires a theoretical definition of clustering quality, which depends on structure type. We demonstrate this through the first validity assessment of clustering quality measures for correlation patterns, a structure type that arises from clustering time series by correlation relationships. We formalise 23 canonical correlation patterns as the theoretical optimal clustering and use synthetic data modelling this structure with controlled perturbations to evaluate validity across content, criterion, construct, and external validity. Our findings show that Silhouette Width Criterion (SWC) and Davies-Bouldin Index (DBI) are valid for correlation patterns, whilst Calinski-Harabasz (VRC) and Pakhira-Bandyopadhyay-Maulik (PBM) indices fail. Simple Lp norm distances achieve validity, whilst correlation-specific functions fail structural, criterion, and external validity. These results differ from previous studies where VRC and PBM performed well, demonstrating that validity depends on structure type. Our structure-type-specific validation method provides both practical guidance (quality thresholds SWC>0.9, DBI<0.15) and a methodological template for establishing validity for other structure types.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REINA: Regularized Entropy Information-Based Loss for Efficient Simultaneous Speech Translation</title>
<link>https://arxiv.org/abs/2508.04946</link>
<guid>https://arxiv.org/abs/2508.04946</guid>
<content:encoded><![CDATA[
arXiv:2508.04946v3 Announce Type: replace 
Abstract: Simultaneous Speech Translation (SimulST) systems stream in audio while simultaneously emitting translated text or speech. Such systems face the significant challenge of balancing translation quality and latency. We introduce a strategy to optimize this tradeoff: wait for more input only if you gain information by doing so. Based on this strategy, we present Regularized Entropy INformation Adaptation (REINA), a novel loss to train an adaptive policy using an existing non-streaming translation model. We derive REINA from information theory principles and show that REINA helps push the reported Pareto frontier of the latency/quality tradeoff over prior works. Utilizing REINA, we train a SimulST model on French, Spanish and German, both from and into English. Training on only open source or synthetically generated data, we achieve state-of-the-art (SOTA) streaming results for models of comparable size. We also introduce a metric for streaming efficiency, quantitatively showing REINA improves the latency/quality trade-off by as much as 21% compared to prior approaches, normalized against non-streaming baseline BLEU scores.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retro-Expert: Collaborative Reasoning for Interpretable Retrosynthesis</title>
<link>https://arxiv.org/abs/2508.10967</link>
<guid>https://arxiv.org/abs/2508.10967</guid>
<content:encoded><![CDATA[
arXiv:2508.10967v2 Announce Type: replace 
Abstract: Retrosynthesis prediction aims to infer the reactant molecule based on a given product molecule, which is a fundamental task in chemical synthesis. However, existing models rely on static pattern-matching paradigm, which limits their ability to perform effective logic decision-making, leading to black-box decision-making. Building on this, we propose Retro-Expert, an interpretable retrosynthesis framework that performs collaborative reasoning by combining the complementary reasoning strengths of Large Language Models and specialized models via reinforcement learning. It outputs natural language explanations grounded in chemical logic through three components: (1) specialized models analyze the product to construct high-quality chemical decision space, (2) LLM-driven critical reasoning to generate predictions and corresponding interpretable reasoning path, and (3) reinforcement learning optimizing interpretable decision policy. Experiments show that Retro-Expert not only surpasses both LLM-based and specialized models across different metrics but also provides expert-aligned explanations that bridge the gap between AI predictions and actionable chemical insights.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse but Wrong: Incorrect L0 Leads to Incorrect Features in Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2508.16560</link>
<guid>https://arxiv.org/abs/2508.16560</guid>
<content:encoded><![CDATA[
arXiv:2508.16560v3 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) extract features from LLM internal activations, meant to correspond to interpretable concepts. A core SAE training hyperparameter is L0: how many SAE features should fire per token on average. Existing work compares SAE algorithms using sparsity-reconstruction tradeoff plots, implying L0 is a free parameter with no single correct value aside from its effect on reconstruction. In this work we study the effect of L0 on SAEs, and show that if L0 is not set correctly, the SAE fails to disentangle the underlying features of the LLM. If L0 is too low, the SAE will mix correlated features to improve reconstruction. If L0 is too high, the SAE finds degenerate solutions that also mix features. Further, we present a proxy metric that can help guide the search for the correct L0 for an SAE on a given training distribution. We show that our method finds the correct L0 in toy models and coincides with peak sparse probing performance in LLM SAEs. We find that most commonly used SAEs have an L0 that is too low. Our work shows that L0 must be set correctly to train SAEs with correct features.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IPA: An Information-Reconstructive Input Projection Framework for Efficient Foundation Model Adaptation</title>
<link>https://arxiv.org/abs/2509.04398</link>
<guid>https://arxiv.org/abs/2509.04398</guid>
<content:encoded><![CDATA[
arXiv:2509.04398v2 Announce Type: replace 
Abstract: Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce adaptation cost by injecting low-rank updates into pretrained weights. However, LoRA's down-projection is randomly initialized and data-agnostic, discarding potentially useful information. Prior analyses show that this projection changes little during training, while the up-projection carries most of the adaptation, making the random input compression a performance bottleneck. We propose IPA, a feature-aware projection framework that explicitly aims to reconstruct the original input within a reduced hidden space. In the linear case, we instantiate IPA with algorithms approximating top principal components, enabling efficient projector pretraining with negligible inference overhead. Across language and vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on VTAB-1k, while matching full LoRA performance with roughly half the trainable parameters when the projection is frozen. Code available at https://github.com/valeoai/peft-ipa .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Sparse Autoencoders: Select-and-Project for Fairness and Control from Encoder Features Alone</title>
<link>https://arxiv.org/abs/2509.10809</link>
<guid>https://arxiv.org/abs/2509.10809</guid>
<content:encoded><![CDATA[
arXiv:2509.10809v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) are widely employed for mechanistic interpretability and model steering. Within this context, steering is by design performed by means of decoding altered SAE intermediate representations. This procedure essentially rewrites the original activations as a weighted sum of decoder features. In contrast to existing literature, we forward an encoder-centric alternative to model steering which demonstrates a stronger cross-modal performance. We introduce S&amp;P Top-K, a retraining-free and computationally lightweight Selection and Projection framework that identifies Top-K encoder features aligned with a sensitive attribute or behavior, optionally aggregates them into a single control axis, and computes an orthogonal projection to be subsequently applied directly in the model's native embedding space. In vision-language models, it improves fairness metrics on CelebA and FairFace by up to 3.2 times over conventional SAE usage, and in large language models, it substantially reduces aggressiveness and sycophancy in Llama-3 8B Instruct, achieving up to 3.6 times gains over masked reconstruction. These findings suggest that encoder-centric interventions provide a general, efficient, and more effective mechanism for shaping model behavior at inference time than the traditional decoder-centric use of SAEs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Learning of Graph Representations for Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2509.16625</link>
<guid>https://arxiv.org/abs/2509.16625</guid>
<content:encoded><![CDATA[
arXiv:2509.16625v4 Announce Type: replace 
Abstract: Detecting intrusions in network traffic is a challenging task, particularly under limited supervision and constantly evolving attack patterns. While recent works have leveraged graph neural networks for network intrusion detection, they often decouple representation learning from anomaly detection, limiting the utility of the embeddings for identifying attacks. We propose GraphIDS, a self-supervised intrusion detection model that unifies these two stages by learning local graph representations of normal communication patterns through a masked autoencoder. An inductive graph neural network embeds each flow with its local topological context to capture typical network behavior, while a Transformer-based encoder-decoder reconstructs these embeddings, implicitly learning global co-occurrence patterns via self-attention without requiring explicit positional information. During inference, flows with unusually high reconstruction errors are flagged as potential intrusions. This end-to-end framework ensures that embeddings are directly optimized for the downstream task, facilitating the recognition of malicious traffic. On diverse NetFlow benchmarks, GraphIDS achieves up to 99.98% PR-AUC and 99.61% macro F1-score, outperforming baselines by 5-25 percentage points.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization</title>
<link>https://arxiv.org/abs/2509.22161</link>
<guid>https://arxiv.org/abs/2509.22161</guid>
<content:encoded><![CDATA[
arXiv:2509.22161v2 Announce Type: replace 
Abstract: Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a codebook), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation. Smoothed vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires two properties: (1) smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and (2) all codebook entries should be utilized, preventing code collapse. Existing methods typically address these desiderata separately. By contrast, the present study introduces a simple and intuitive regularization that promotes both simultaneously by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks, including discrete image autoencoding and contrastive speech representation learning, demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-marginal temporal Schr\"odinger Bridge Matching from unpaired data</title>
<link>https://arxiv.org/abs/2510.01894</link>
<guid>https://arxiv.org/abs/2510.01894</guid>
<content:encoded><![CDATA[
arXiv:2510.01894v2 Announce Type: replace 
Abstract: Many natural dynamic processes -- such as in vivo cellular differentiation or disease progression -- can only be observed through the lens of static sample snapshots. While challenging, reconstructing their temporal evolution to decipher underlying dynamic properties is of major interest to scientific research. Existing approaches enable data transport along a temporal axis but are poorly scalable in high dimension and require restrictive assumptions to be met. To address these issues, we propose Multi-Marginal temporal Schr\"odinger Bridge Matching (MMtSBM) from unpaired data, extending the theoretical guarantees and empirical efficiency of Diffusion Schr\"odinger Bridge Matching (arXiv:2303.16852) by deriving the Iterative Markovian Fitting algorithm to multiple marginals in a novel factorized fashion. Experiments show that MMtSBM retains theoretical properties on toy examples, achieves state-of-the-art performance on real-world datasets such as transcriptomic trajectory inference in 100 dimensions, and, for the first time, recovers couplings and dynamics in very high-dimensional image settings. Our work establishes multi-marginal Schr\"odinger bridges as a practical and principled approach for recovering hidden dynamics from static data.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforce-Ada: An Adaptive Sampling Framework under Non-linear RL Objectives</title>
<link>https://arxiv.org/abs/2510.04996</link>
<guid>https://arxiv.org/abs/2510.04996</guid>
<content:encoded><![CDATA[
arXiv:2510.04996v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) for large language model reasoning is frequently hindered by signal loss, a phenomenon where standard uniform sampling with small group sizes fails to uncover informative learning signals for difficult prompts. We demonstrate that this collapse is a statistical artifact of undersampling rather than an inherent model limitation. To address this systematically, we introduce a theoretical framework based on optimizing a non-linear RL objective (e.g., log-likelihood). We show that this objective naturally induces a weighted gradient estimator that prioritizes difficult prompts, which can be robustly realized through adaptive sampling. Guided by this framework, we propose Reinforce-Ada, a family of algorithms that dynamically allocates inference budgets based on prompt difficulty, effectively scaling up RL compute to where it is needed most. Unlike passive filtering methods that discard low-signal prompts, Reinforce-Ada actively invests compute to recover them. We introduce two efficient realizations: an estimation-based approach and a model-free sequential sampling approach. Extensive experiments across multiple benchmarks show that Reinforce-Ada significantly outperforms uniform baselines like GRPO, recovering lost signals and accelerating convergence by up to $2\times$ while maintaining the same total inference budget. Code is available at https://github.com/RLHFlow/Reinforce-Ada.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference</title>
<link>https://arxiv.org/abs/2510.09665</link>
<guid>https://arxiv.org/abs/2510.09665</guid>
<content:encoded><![CDATA[
arXiv:2510.09665v2 Announce Type: replace 
Abstract: KV cache has traditionally been stored in GPU memory to accelerate the decoding phase of large language model (LLM) inference. However, it is increasingly necessary to move KV caches outside GPU devices, to enable cache reuse across different queries and inference engines. Our real-world usage statistics confirm this trend: over time, the total KV cache stored by users has grown rapidly, far exceeding the capacity of GPU memory. Despite this need, there lacks an efficient solution for offloading and transferring KV caches. We present LMCACHE, the first and so far the most efficient open-source KV caching solution, which extracts and stores KV caches generated by modern LLM engines (vLLM and SGLang) out of the GPU memory and shares them across engines and queries. LMCACHE supports both cache offloading (prefix reuse across queries) and prefill-decode (PD) disaggregation (cross-engine/GPU cache transfer). LMCACHE's high performance and wide adoption stem from the following contributions: (1) highly optimized KV cache data movement powered by batched data movement operations, compute and I/O pipelining; (2) a modular KV cache connector component, decoupling LMCACHE from the rapid evolution of inference engines; (3) a first-class control API for flexible cache orchestration across GPU, CPU, storage, and network layers. Our evaluation shows that combining LMCACHE with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answering and document analysis. Large-scale adoption of LMCACHE in enterprise settings provides us valuable insights, for example, fetching KV cache from remote storage has unsurprisingly benefits to prefill delay, and that context truncation, which is a widely applied technique in industry, can greatly reduce prefix cache hit ratio by half. The source code of LMCACHE is at: https://github.com/LMCache/LMCache.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stronger-MAS: Multi-Agent Reinforcement Learning for Collaborative LLMs</title>
<link>https://arxiv.org/abs/2510.11062</link>
<guid>https://arxiv.org/abs/2510.11062</guid>
<content:encoded><![CDATA[
arXiv:2510.11062v3 Announce Type: replace 
Abstract: Multi-agent systems (MAS) and reinforcement learning (RL) are widely used to enhance the agentic capabilities of large language models (LLMs). MAS improves task performance through role-based orchestration, while RL uses environmental rewards to learn stronger policies, such as GRPO-style optimization. However, applying on-policy RL to MAS remains underexplored and presents unique challenges. Algorithmically, standard GRPO grouping assumptions break down because prompts vary by role and by turn. System-wise, the training stack must support MAS-workflow rollouts and on-policy updates for both single-policy and multi-policy models.
  We propose AT-GRPO, which includes (i) an agent- and turn-wise grouped RL algorithm tailored to MAS and (ii) a training system that supports both single- and multi-policy regimes. Across game, planning, coding, and math tasks, AT-GRPO delivers substantial gains. On long-horizon planning, it increases accuracy from a 14.0 to 47.0 percent single-agent RL baseline to 96.0 to 99.5 percent. It also improves reasoning performance, with average gains of 3.87 to 7.62 percent on coding tasks and 9.0 to 17.93 percent on math. Code and environments are available at: https://github.com/pettingllms-ai/PettingLLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Augmented Deep Learning for Downhole Depth Sensing and Field Validation</title>
<link>https://arxiv.org/abs/2511.00129</link>
<guid>https://arxiv.org/abs/2511.00129</guid>
<content:encoded><![CDATA[
arXiv:2511.00129v2 Announce Type: replace 
Abstract: Accurate downhole depth measurement is essential for oil and gas well operations, directly influencing reservoir contact, production efficiency, and operational safety. Collar correlation using a casing collar locator (CCL) is fundamental for precise depth calibration. While neural network-based CCL signal recognition has achieved significant progress in collar identification, preprocessing methods for such applications remain underdeveloped. Moreover, the limited availability of real well data poses substantial challenges for training neural network models that require extensive datasets. This paper presents a system integrated into downhole tools for CCL signal acquisition to facilitate dataset construction. We propose comprehensive preprocessing methods for data augmentation and evaluate their effectiveness using our neural network models. Through systematic experimentation across various configuration combinations, we analyze the contribution of each augmentation method. Results demonstrate that standardization, label distribution smoothing (LDS), and random cropping are fundamental requirements for model training, while label smoothing regularization (LSR), time scaling, and multiple sampling significantly enhance model generalization capability. The F1 scores of our two benchmark models trained with the proposed augmentation methods maximumly improve from 0.937 and 0.952 to 1.0 and 1.0, respectively. Performance validation on real CCL waveforms confirms the effectiveness and practical applicability of our approach. This work addresses the gaps in data augmentation methodologies for training casing collar recognition models in CCL data-limited environments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling</title>
<link>https://arxiv.org/abs/2511.05811</link>
<guid>https://arxiv.org/abs/2511.05811</guid>
<content:encoded><![CDATA[
arXiv:2511.05811v2 Announce Type: replace 
Abstract: Training large language models with FP8 formats offers significant efficiency gains. However, the reduced numerical precision of FP8 poses challenges for stable and accurate training. Current frameworks preserve training performance using mixed-granularity quantization, i.e., applying per-group quantization for activations and per-tensor/block quantization for weights. While effective, per-group quantization requires scaling along the inner dimension of matrix multiplication, introducing additional dequantization overhead. Moreover, these frameworks often rely on just-in-time scaling to dynamically adjust scaling factors based on the current data distribution. However, this online quantization is inefficient for FP8 training, as it involves multiple memory reads and writes that negate the performance benefits of FP8. To overcome these limitations, we propose MOSS, a novel FP8 training framework that ensures both efficiency and numerical stability. MOSS introduces two key innovations: (1) a two-level microscaling strategy for quantizing sensitive activations, which balances precision and dequantization cost by combining a high-precision global scale with compact, power-of-two local scales; and (2) automatic scaling for weights in linear layers, which eliminates the need for costly max-reduction operations by predicting and adjusting scaling factors during training. Leveraging these techniques, MOSS enables efficient FP8 training of a 7B parameter model, achieving performance comparable to the BF16 baseline while achieving up to 34% higher training throughput.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAST-CAD: A Fairness-Aware Framework for Non-Contact Stroke Diagnosis</title>
<link>https://arxiv.org/abs/2511.08887</link>
<guid>https://arxiv.org/abs/2511.08887</guid>
<content:encoded><![CDATA[
arXiv:2511.08887v3 Announce Type: replace 
Abstract: Stroke is an acute cerebrovascular disease, and timely diagnosis significantly improves patient survival. However, existing automated diagnosis methods suffer from fairness issues across demographic groups, potentially exacerbating healthcare disparities. In this work we propose FAST-CAD, a theoretically grounded framework that combines domain-adversarial training (DAT) with group distributionally robust optimization (Group-DRO) for fair and accurate non-contact stroke diagnosis. Our approach is built on domain adaptation and minimax fairness theory and provides convergence guarantees and fairness bounds. We curate a multimodal dataset covering 12 demographic subgroups defined by age, gender, and posture. FAST-CAD employs self-supervised encoders with adversarial domain discrimination to learn demographic-invariant representations, while Group-DRO optimizes worst-group risk to ensure robust performance across all subgroups. Extensive experiments show that our method achieves superior diagnostic performance while maintaining fairness across demographic groups, and our theoretical analysis supports the effectiveness of the unified DAT + Group-DRO framework. This work provides both practical advances and theoretical insights for fair medical AI systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a Generalisable Cyber Defence Agent for Real-World Computer Networks</title>
<link>https://arxiv.org/abs/2511.09114</link>
<guid>https://arxiv.org/abs/2511.09114</guid>
<content:encoded><![CDATA[
arXiv:2511.09114v2 Announce Type: replace 
Abstract: Recent advances in deep reinforcement learning for autonomous cyber defence have resulted in agents that can successfully defend simulated computer networks against cyber-attacks. However, many of these agents would need retraining to defend networks with differing topology or size, making them poorly suited to real-world networks where topology and size can vary over time. In this research we introduce a novel set of Topological Extensions for Reinforcement Learning Agents (TERLA) that provide generalisability for the defence of networks with differing topology and size, without the need for retraining. Our approach involves the use of heterogeneous graph neural network layers to produce a fixed-size latent embedding representing the observed network state. This representation learning stage is coupled with a reduced, fixed-size, semantically meaningful and interpretable action space. We apply TERLA to a standard deep reinforcement learning Proximal Policy Optimisation (PPO) agent model, and to reduce the sim-to-real gap, conduct our research using Cyber Autonomy Gym for Experimentation (CAGE) Challenge 4. This Cyber Operations Research Gym environment has many of the features of a real-world network, such as realistic Intrusion Detection System (IDS) events and multiple agents defending network segments of differing topology and size. TERLA agents retain the defensive performance of vanilla PPO agents whilst showing improved action efficiency. Generalisability has been demonstrated by showing that all TERLA agents have the same network-agnostic neural network architecture, and by deploying a single TERLA agent multiple times to defend network segments with differing topology and size, showing improved defensive performance and efficiency.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Laplacian: Interpolated Spectral Augmentation for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2511.11928</link>
<guid>https://arxiv.org/abs/2511.11928</guid>
<content:encoded><![CDATA[
arXiv:2511.11928v2 Announce Type: replace 
Abstract: Graph neural networks (GNNs) are fundamental tools in graph machine learning. The performance of GNNs relies crucially on the availability of informative node features, which can be limited or absent in real-life datasets and applications. A natural remedy is to augment the node features with embeddings computed from eigenvectors of the graph Laplacian matrix. While it is natural to default to Laplacian spectral embeddings, which capture meaningful graph connectivity information, we ask whether spectral embeddings from alternative graph matrices can also provide useful representations for learning. We introduce Interpolated Laplacian Embeddings (ILEs), which are derived from a simple yet expressive family of graph matrices. Using tools from spectral graph theory, we offer a straightforward interpretation of the structural information that ILEs capture. We demonstrate through simulations and experiments on real-world datasets that feature augmentation via ILEs can improve performance across commonly used GNN architectures. Our work offers a straightforward and practical approach that broadens the practitioner's spectral augmentation toolkit when node features are limited.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>xLSTM-PINN: Memory-Gated Spectral Remodeling for Physics-Informed Learning</title>
<link>https://arxiv.org/abs/2511.12512</link>
<guid>https://arxiv.org/abs/2511.12512</guid>
<content:encoded><![CDATA[
arXiv:2511.12512v2 Announce Type: replace 
Abstract: Physics-informed neural networks (PINN) face significant challenges from spectral bias, which impedes their ability to model high-frequency phenomena and limits extrapolation performance. To address this, we introduce xLSTM-PINN, a novel architecture that performs representation-level spectral remodeling through memory gating and residual micro-steps. Our method consistently achieves markedly lower spectral error and root mean square error (RMSE) across four diverse partial differential equation (PDE) benchmarks, along withhhh a broader stable learning-rate window. Frequency-domain analysis confirms that xLSTM-PINN elevates high-frequency kernel weights, shifts the resolvable bandwidth rightward, and shortens the convergence time for high-wavenumber components. Without modifying automatic differentiation or physics loss constraints, this work provides a robust pathway to suppress spectral bias, thereby improving accuracy, reproducibility, and transferability in physics-informed learning.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observational Auditing of Label Privacy</title>
<link>https://arxiv.org/abs/2511.14084</link>
<guid>https://arxiv.org/abs/2511.14084</guid>
<content:encoded><![CDATA[
arXiv:2511.14084v2 Announce Type: replace 
Abstract: Differential privacy (DP) auditing is essential for evaluating privacy guarantees in machine learning systems. Existing auditing methods, however, pose a significant challenge for large-scale systems since they require modifying the training dataset -- for instance, by injecting out-of-distribution canaries or removing samples from training. Such interventions on the training data pipeline are resource-intensive and involve considerable engineering overhead. We introduce a novel observational auditing framework that leverages the inherent randomness of data distributions, enabling privacy evaluation without altering the original dataset. Our approach extends privacy auditing beyond traditional membership inference to protected attributes, with labels as a special case, addressing a key gap in existing techniques. We provide theoretical foundations for our method and perform experiments on Criteo and CIFAR-10 datasets that demonstrate its effectiveness in auditing label privacy guarantees. This work opens new avenues for practical privacy auditing in large-scale production environments.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Data-efficient Customer Intent Recognition with Prompt-based Learning Paradigm</title>
<link>https://arxiv.org/abs/2309.14779</link>
<guid>https://arxiv.org/abs/2309.14779</guid>
<content:encoded><![CDATA[
arXiv:2309.14779v2 Announce Type: replace-cross 
Abstract: Recognizing customer intent accurately with language models based on customer-agent conversational data is essential in today's digital customer service marketplace, but it is often hindered by the lack of sufficient labeled data. In this paper, we introduce the prompt-based learning paradigm that significantly reduces the dependency on extensive datasets. Utilizing prompted training combined with answer mapping techniques, this approach allows small language models to achieve competitive intent recognition performance with only a minimal amount of training data. Furthermore, We enhance the performance by integrating active sampling and ensemble learning strategies in the prompted training pipeline. Additionally, preliminary tests in a zero-shot setting demonstrate that, with well-crafted and detailed prompts, small language models show considerable instruction-following potential even without any further training. These results highlight the viability of semantic modeling of conversational data in a more data-efficient manner with minimal data use, paving the way for advancements in AI-driven customer service.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Data-Efficient 3D Scene Understanding for Autonomous Driving</title>
<link>https://arxiv.org/abs/2405.05258</link>
<guid>https://arxiv.org/abs/2405.05258</guid>
<content:encoded><![CDATA[
arXiv:2405.05258v3 Announce Type: replace-cross 
Abstract: Efficient data utilization is crucial for advancing 3D scene understanding in autonomous driving, where reliance on heavily human-annotated LiDAR point clouds challenges fully supervised methods. Addressing this, our study extends into semi-supervised learning for LiDAR semantic segmentation, leveraging the intrinsic spatial priors of driving scenes and multi-sensor complements to augment the efficacy of unlabeled datasets. We introduce LaserMix++, an evolved framework that integrates laser beam manipulations from disparate LiDAR scans and incorporates LiDAR-camera correspondences to further assist data-efficient learning. Our framework is tailored to enhance 3D scene consistency regularization by incorporating multi-modality, including 1) multi-modal LaserMix operation for fine-grained cross-sensor interactions; 2) camera-to-LiDAR feature distillation that enhances LiDAR feature learning; and 3) language-driven knowledge guidance generating auxiliary supervisions using open-vocabulary models. The versatility of LaserMix++ enables applications across LiDAR representations, establishing it as a universally applicable solution. Our framework is rigorously validated through theoretical analysis and extensive experiments on popular driving perception datasets. Results demonstrate that LaserMix++ markedly outperforms fully supervised alternatives, achieving comparable accuracy with five times fewer annotations and significantly improving the supervised-only baselines. This substantial advancement underscores the potential of semi-supervised approaches in reducing the reliance on extensive labeled data in LiDAR-based 3D scene understanding systems.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test</title>
<link>https://arxiv.org/abs/2406.18397</link>
<guid>https://arxiv.org/abs/2406.18397</guid>
<content:encoded><![CDATA[
arXiv:2406.18397v3 Announce Type: replace-cross 
Abstract: In this article, we introduce the novel concept of the second maximum of a Gaussian random field on a Riemannian submanifold. This second maximum serves as a powerful tool for characterizing the distribution of the maximum. By utilizing an ad-hoc Kac Rice formula, we derive the explicit form of the maximum's distribution, conditioned on the second maximum and some regressed component of the Riemannian Hessian. This approach results in an exact test, based on the evaluation of spacing between these maxima, which we refer to as the spacing test.
  We investigate the applicability of this test in detecting sparse alternatives within Gaussian symmetric tensors, continuous sparse deconvolution, and two-layered neural networks with smooth rectifiers. Our theoretical results are supported by numerical experiments, which illustrate the calibration and power of the proposed tests. More generally, this test can be applied to any Gaussian random field on a Riemannian manifold, and we provide a general framework for the application of the spacing test in continuous sparse kernel regression.
  Furthermore, when the variance-covariance function of the Gaussian random field is known up to a scaling factor, we derive an exact Studentized version of our test, coined the $t$-spacing test. This test is perfectly calibrated under the null hypothesis and has high power for detecting sparse alternatives.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Correlation to Causation: Max-Pooling-Based Multi-Instance Learning Leads to More Robust Whole Slide Image Classification</title>
<link>https://arxiv.org/abs/2408.09449</link>
<guid>https://arxiv.org/abs/2408.09449</guid>
<content:encoded><![CDATA[
arXiv:2408.09449v3 Announce Type: replace-cross 
Abstract: In whole slide images (WSIs) analysis, attention-based multi-instance learning (MIL) models are susceptible to spurious correlations and degrade under domain shift. These methods may assign high attention weights to non-tumor regions, such as staining biases or artifacts, leading to unreliable tumor region localization. In this paper, we revisit max-pooling-based MIL methods from a causal perspective. Under mild assumptions, our theoretical results demonstrate that max-pooling encourages the model to focus on causal factors while ignoring bias-related factors. Furthermore, we discover that existing max-pooling-based methods may overfit the training set through rote memorization of instance features and fail to learn meaningful patterns. To address these issues, we propose FocusMIL, which couples max-pooling with an instance-level variational information bottleneck (VIB) to learn compact, predictive latent representations, and employs a multi-bag mini-batch scheme to stabilize optimization. We conduct comprehensive experiments on three real-world datasets and one semi-synthetic dataset. The results show that, by capturing causal factors, FocusMIL exhibits significant advantages in out-of-distribution scenarios and instance-level tumor region localization tasks.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnhancedRL: An Enhanced-State Reinforcement Learning Algorithm for Multi-Task Fusion in Recommender Systems</title>
<link>https://arxiv.org/abs/2409.11678</link>
<guid>https://arxiv.org/abs/2409.11678</guid>
<content:encoded><![CDATA[
arXiv:2409.11678v4 Announce Type: replace-cross 
Abstract: As a key stage of Recommender Systems (RSs), Multi-Task Fusion (MTF) is responsible for merging multiple scores output by Multi-Task Learning (MTL) into a single score, finally determining the recommendation results. Recently, Reinforcement Learning (RL) has been applied to MTF to maximize long-term user satisfaction within a recommendation session. However, due to limitations in modeling paradigm, all existing RL algorithms for MTF can only utilize user features and statistical features as the state to generate actions at the user level, but unable to leverage item features and other valuable features, which leads to suboptimal performance. Overcoming this problem requires a breakthrough in the existing modeling paradigm, yet, to date, no prior work has addressed it. To tackle this challenge, we propose EnhancedRL, an innovative RL algorithm. Unlike existing RL-MTF methods, EnhancedRL takes the enhanced state as input, incorporating not only user features but also item features and other valuable information. Furthermore, it introduces a tailored actor-critic framework - including redesigned actor and critics and a novel learning procedure - to optimize long-term rewards at the user-item pair level within a recommendation session. Extensive offline and online experiments are conducted in an industrial RS and the results demonstrate that EnhancedRL outperforms other methods remarkably, achieving a +3.84% increase in user valid consumption and a +0.58% increase in user duration time. To the best of our knowledge, EnhancedRL is the first work to address this challenge, and it has been fully deployed in a large-scale RS since September 14, 2023, yielding significant improvements.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Communication and Control Co-Design for Multi-Objective Distinct Dynamics</title>
<link>https://arxiv.org/abs/2410.02303</link>
<guid>https://arxiv.org/abs/2410.02303</guid>
<content:encoded><![CDATA[
arXiv:2410.02303v2 Announce Type: replace-cross 
Abstract: This letter introduces a machine-learning approach to learning the semantic dynamics of correlated systems with different control rules and dynamics. By leveraging the Koopman operator in an autoencoder (AE) framework, the system's state evolution is linearized in the latent space using a dynamic semantic Koopman (DSK) model, capturing the baseline semantic dynamics. Signal temporal logic (STL) is incorporated through a logical semantic Koopman (LSK) model to encode system-specific control rules. These models form the proposed logical Koopman AE framework that reduces communication costs while improving state prediction accuracy and control performance, showing a 91.65% reduction in communication samples and significant performance gains in simulation.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SS4Rec: Continuous-Time Sequential Recommendation with State Space Models</title>
<link>https://arxiv.org/abs/2502.08132</link>
<guid>https://arxiv.org/abs/2502.08132</guid>
<content:encoded><![CDATA[
arXiv:2502.08132v3 Announce Type: replace-cross 
Abstract: Sequential recommendation is a key area in the field of recommendation systems aiming to model user interest based on historical interaction sequences with irregular intervals. While previous recurrent neural network-based and attention-based approaches have achieved significant results, they have limitations in capturing system continuity due to the discrete characteristics. In the context of continuous-time modeling, state space model (SSM) offers a potential solution, as it can effectively capture the dynamic evolution of user interest over time. However, existing SSM-based approaches ignore the impact of irregular time intervals within historical user interactions, making it difficult to model complexed user-item transitions in sequences. To address this issue, we propose a hybrid SSM-based model called SS4Rec for continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM to handle irregular time intervals and a relation-aware SSM to model contextual dependencies, enabling it to infer user interest from both temporal and sequential perspectives. In the training process, the time-aware SSM and the relation-aware SSM are discretized by variable stepsizes according to user interaction time intervals and input data, respectively. This helps capture the continuous dependency from irregular time intervals and provides time-specific personalized recommendations. Experimental studies on five benchmark datasets demonstrate the superiority and effectiveness of SS4Rec.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ICNN-enhanced 2SP: Leveraging input convex neural networks for solving two-stage stochastic programming</title>
<link>https://arxiv.org/abs/2505.05261</link>
<guid>https://arxiv.org/abs/2505.05261</guid>
<content:encoded><![CDATA[
arXiv:2505.05261v2 Announce Type: replace-cross 
Abstract: Two-stage stochastic programming (2SP) offers a basic framework for modelling decision-making under uncertainty, yet scalability remains a challenge due to the computational complexity of recourse function evaluation. Existing learning-based methods like Neural Two-Stage Stochastic Programming (Neur2SP) employ neural networks (NNs) as recourse function surrogates but rely on computationally intensive mixed-integer programming (MIP) formulations. We propose ICNN-enhanced 2SP, a method that leverages Input Convex Neural Networks (ICNNs) to exploit linear programming (LP) representability in convex 2SP problems. By architecturally enforcing convexity and enabling exact inference through LP, our approach eliminates the need for integer variables inherent to the conventional MIP-based formulation while retaining an exact embedding of the ICNN surrogate within the 2SP framework. This results in a more computationally efficient alternative, and we show that good solution quality can be maintained. Comprehensive experiments reveal that ICNNs incur only marginally longer training times while achieving validation accuracy on par with their standard NN counterparts. Across benchmark problems, ICNN-enhanced 2SP often exhibits considerably faster solution times than the MIP-based formulations while preserving solution quality, with these advantages becoming significantly more pronounced as problem scale increases. For the most challenging instances, the method achieves speedups of up to 100$\times$ and solution quality superior to MIP-based formulations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes</title>
<link>https://arxiv.org/abs/2505.07315</link>
<guid>https://arxiv.org/abs/2505.07315</guid>
<content:encoded><![CDATA[
arXiv:2505.07315v2 Announce Type: replace-cross 
Abstract: Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focus on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features that from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A quantitative analysis of semantic information in deep representations of text and images</title>
<link>https://arxiv.org/abs/2505.17101</link>
<guid>https://arxiv.org/abs/2505.17101</guid>
<content:encoded><![CDATA[
arXiv:2505.17101v4 Announce Type: replace-cross 
Abstract: Deep neural networks are known to develop similar representations for semantically related data, even when they belong to different domains, such as an image and its description, or the same text in different languages. We present a method for quantitatively investigating this phenomenon by measuring the relative information content of the representations of semantically related data and probing how it is encoded into multiple tokens of large language models (LLMs) and vision transformers. Looking first at how LLMs process pairs of translated sentences, we identify inner ``semantic'' layers containing the most language-transferable information. We find moreover that, on these layers, a larger LLM (DeepSeek-V3) extracts significantly more general information than a smaller one (Llama3.1-8B). Semantic information of English text is spread across many tokens and it is characterized by long-distance correlations between tokens and by a causal left-to-right (i.e., past-future) asymmetry. We also identify layers encoding semantic information within visual transformers. We show that caption representations in the semantic layers of LLMs predict visual representations of the corresponding images. We observe significant and model-dependent information asymmetries between image and text representations.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Mamba Meets Hyperbolic Geometry: A New Paradigm for Structured Language Embeddings</title>
<link>https://arxiv.org/abs/2505.18973</link>
<guid>https://arxiv.org/abs/2505.18973</guid>
<content:encoded><![CDATA[
arXiv:2505.18973v3 Announce Type: replace-cross 
Abstract: Selective state-space models excel at long-sequence modeling, but their capacity for language representation -- in complex hierarchical reasoning -- remains underexplored. Most large language models rely on \textit{flat} Euclidean embeddings, limiting their ability to capture latent hierarchies. To address this, we propose {\it Hierarchical Mamba (HiM)}, integrating efficient Mamba2 with hyperbolic geometry to learn hierarchy-aware language embeddings for deeper linguistic understanding. Mamba2-processed sequences are projected to the Poincar\'e ball or Lorentzian manifold with ``learnable'' curvature, optimized with a hyperbolic loss. Our HiM model facilitates the capture of relational distances across varying hierarchical levels, enabling effective long-range reasoning for tasks like mixed-hop prediction and multi-hop inference in hierarchical classification. Experimental results show both HiM variants effectively capture hierarchical relationships across four linguistic and medical datasets, surpassing Euclidean baselines, with HiM-Poincar\'e providing fine-grained distinctions with higher h-norms, while HiM-Lorentz offers more stable, compact, and hierarchy-preserving embeddings-favoring robustness. The source code is publicly available at https://github.com/BerryByte/HiM.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Balancing Performance and Costs in Best Arm Identification</title>
<link>https://arxiv.org/abs/2505.20583</link>
<guid>https://arxiv.org/abs/2505.20583</guid>
<content:encoded><![CDATA[
arXiv:2505.20583v2 Announce Type: replace-cross 
Abstract: We consider the problem of identifying the best arm in a multi-armed bandit model. Despite a wealth of literature in the traditional fixed budget and fixed confidence regimes of the best arm identification problem, it still remains a mystery to most practitioners as to how to choose an approach and corresponding budget or confidence parameter. We propose a new formalism to avoid this dilemma altogether by minimizing a risk functional which explicitly balances the performance of the recommended arm and the cost incurred by learning this arm. In this framework, a cost is incurred for each observation during the sampling phase, and upon recommending an arm, a performance penalty is incurred for identifying a suboptimal arm. The learner's goal is to minimize the sum of the penalty and cost. This new regime mirrors the priorities of many practitioners, e.g. maximizing profit in an A/B testing framework, better than classical fixed budget or confidence settings. We derive theoretical lower bounds for the risk of each of two choices for the performance penalty, the probability of misidentification and the simple regret, and propose an algorithm called DBCARE to match these lower bounds up to polylog factors on nearly all problem instances. We then demonstrate the performance of DBCARE on a number of simulated models, comparing to fixed budget and confidence algorithms to show the shortfalls of existing BAI paradigms on this problem.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Execution of Action Chunking Flow Policies</title>
<link>https://arxiv.org/abs/2506.07339</link>
<guid>https://arxiv.org/abs/2506.07339</guid>
<content:encoded><![CDATA[
arXiv:2506.07339v2 Announce Type: replace-cross 
Abstract: Modern AI systems, especially those interacting with the physical world, increasingly require real-time performance. However, the high latency of state-of-the-art generalist models, including recent vision-language action models (VLAs), poses a significant challenge. While action chunking has enabled temporal consistency in high-frequency control tasks, it does not fully address the latency problem, leading to pauses or out-of-distribution jerky movements at chunk boundaries. This paper presents a novel inference-time algorithm that enables smooth asynchronous execution of action chunking policies. Our method, real-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out of the box with no re-training. It generates the next action chunk while executing the current one, "freezing" actions guaranteed to execute and "inpainting" the rest. To test RTC, we introduce a new benchmark of 12 highly dynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging real-world bimanual manipulation tasks. Results demonstrate that RTC is fast, performant, and uniquely robust to inference delay, significantly improving task throughput and enabling high success rates in precise tasks $\unicode{x2013}$ such as lighting a match $\unicode{x2013}$ even in the presence of significant latency. See https://pi.website/research/real_time_chunking for videos.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IS-Bench: Evaluating Interactive Safety of VLM-Driven Embodied Agents in Daily Household Tasks</title>
<link>https://arxiv.org/abs/2506.16402</link>
<guid>https://arxiv.org/abs/2506.16402</guid>
<content:encoded><![CDATA[
arXiv:2506.16402v3 Announce Type: replace-cross 
Abstract: Flawed planning from VLM-driven embodied agents poses significant safety hazards, hindering their deployment in real-world household tasks. However, existing static, non-interactive evaluation paradigms fail to adequately assess risks within these interactive environments, since they cannot simulate dynamic risks that emerge from an agent's actions and rely on unreliable post-hoc evaluations that ignore unsafe intermediate steps. To bridge this critical gap, we propose evaluating an agent's interactive safety: its ability to perceive emergent risks and execute mitigation steps in the correct procedural order. We thus present IS-Bench, the first multi-modal benchmark designed for interactive safety, featuring 161 challenging scenarios with 388 unique safety risks instantiated in a high-fidelity simulator. Crucially, it facilitates a novel process-oriented evaluation that verifies whether risk mitigation actions are performed before/after specific risk-prone steps. Extensive experiments on leading VLMs, including the GPT-4o and Gemini-2.5 series, reveal that current agents lack interactive safety awareness, and that while safety-aware Chain-of-Thought can improve performance, it often compromises task completion. By highlighting these critical limitations, IS-Bench provides a foundation for developing safer and more reliable embodied AI systems. Code and data are released under https://github.com/AI45Lab/IS-Bench.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Language Integration in Fine-Tuning Multimodal Large Language Models for Image-Based Regression</title>
<link>https://arxiv.org/abs/2507.14997</link>
<guid>https://arxiv.org/abs/2507.14997</guid>
<content:encoded><![CDATA[
arXiv:2507.14997v2 Announce Type: replace-cross 
Abstract: Multimodal Large Language Models (MLLMs) show promise for image-based regression tasks, but current approaches face key limitations. Recent methods fine-tune MLLMs using preset output vocabularies and generic task-level prompts (e.g., "How would you rate this image?"), assuming this mimics human rating behavior. \textbf{Our analysis reveals that these approaches provide no benefit over image-only training}. Models using preset vocabularies and generic prompts perform equivalently to image-only models, failing to leverage semantic understanding from textual input. We propose \textbf{Regression via Transformer-Based Classification} (RvTC), which replaces vocabulary-constrained classification with a flexible bin-based approach. Unlike approaches that address discretization errors through complex distributional modeling, RvTC eliminates manual vocabulary crafting through straightforward bin increase, achieving state-of-the-art performance on four image assessment datasets using only images. \textbf{More importantly, we demonstrate that data-specific prompts dramatically improve performance}. Unlike generic task descriptions, prompts containing semantic information about specific images enable MLLMs to leverage cross-modal understanding. On the AVA dataset, adding challenge titles to prompts substantially improves our already state-of-the-art image-only baseline. We demonstrate through empirical evidence from the AVA and AGIQA-3k datasets that MLLMs benefit from semantic prompt information, surpassing mere statistical biases. We validate RvTC across two different MLLM architectures, demonstrating consistent improvements and method generalizability.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReSem3D: Refinable 3D Spatial Constraints via Fine-Grained Semantic Grounding for Generalizable Robotic Manipulation</title>
<link>https://arxiv.org/abs/2507.18262</link>
<guid>https://arxiv.org/abs/2507.18262</guid>
<content:encoded><![CDATA[
arXiv:2507.18262v3 Announce Type: replace-cross 
Abstract: Semantics-driven 3D spatial constraints align highlevel semantic representations with low-level action spaces, facilitating the unification of task understanding and execution in robotic manipulation. The synergistic reasoning of Multimodal Large Language Models (MLLMs) and Vision Foundation Models (VFMs) enables cross-modal 3D spatial constraint construction. Nevertheless, existing methods have three key limitations: (1) coarse semantic granularity in constraint modeling, (2) lack of real-time closed-loop planning, (3) compromised robustness in semantically diverse environments. To address these challenges, we propose ReSem3D, a unified manipulation framework for semantically diverse environments, leveraging the synergy between VFMs and MLLMs to achieve fine-grained visual grounding and dynamically constructs hierarchical 3D spatial constraints for real-time manipulation. Specifically, the framework is driven by hierarchical recursive reasoning in MLLMs, which interact with VFMs to automatically construct 3D spatial constraints from natural language instructions and RGB-D observations in two stages: part-level extraction and region-level refinement. Subsequently, these constraints are encoded as real-time optimization objectives in joint space, enabling reactive behavior to dynamic disturbances. Extensive simulation and real-world experiments are conducted in semantically rich household and sparse chemical lab environments. The results demonstrate that ReSem3D performs diverse manipulation tasks under zero-shot conditions, exhibiting strong adaptability and generalization. Code and videos are available at https://github.com/scy-v/ReSem3D and https://resem3d.github.io.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A note on the Artstein-Avidan-Milman's generalized Legendre transforms</title>
<link>https://arxiv.org/abs/2507.20577</link>
<guid>https://arxiv.org/abs/2507.20577</guid>
<content:encoded><![CDATA[
arXiv:2507.20577v2 Announce Type: replace-cross 
Abstract: Artstein-Avidan and Milman [Annals of mathematics (2009), (169):661-674] characterized invertible reverse-ordering transforms on the space of lower semi-continuous extended real-valued convex functions as affine deformations of the ordinary Legendre transform. In this work, we first prove that all those generalized Legendre transforms on functions correspond to the ordinary Legendre transform on dually corresponding affine-deformed functions: In short, generalized convex conjugates are ordinary convex conjugates of dually affine-deformed functions. Second, we explain how these generalized Legendre transforms can be derived from the dual Hessian structures of information geometry.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Survey on Diffusion Language Models</title>
<link>https://arxiv.org/abs/2508.10875</link>
<guid>https://arxiv.org/abs/2508.10875</guid>
<content:encoded><![CDATA[
arXiv:2508.10875v2 Announce Type: replace-cross 
Abstract: Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator learning meets inverse problems: A probabilistic perspective</title>
<link>https://arxiv.org/abs/2508.20207</link>
<guid>https://arxiv.org/abs/2508.20207</guid>
<content:encoded><![CDATA[
arXiv:2508.20207v2 Announce Type: replace-cross 
Abstract: Operator learning offers a robust framework for approximating mappings between infinite-dimensional function spaces. It has also become a powerful tool for solving inverse problems in the computational sciences. This chapter surveys methodological and theoretical developments at the intersection of operator learning and inverse problems. It begins by summarizing the probabilistic and deterministic approaches to inverse problems, and pays special attention to emerging measure-centric formulations that treat observed data or unknown parameters as probability distributions. The discussion then turns to operator learning by covering essential components such as data generation, loss functions, and widely used architectures for representing function-to-function maps. The core of the chapter centers on the end-to-end inverse operator learning paradigm, which aims to directly map observed data to the solution of the inverse problem without requiring explicit knowledge of the forward map. It highlights the unique challenge that noise plays in this data-driven inversion setting, presents structure-aware architectures for both point predictions and posterior estimates, and surveys relevant theory for linear and nonlinear inverse problems. The chapter also discusses the estimation of priors and regularizers, where operator learning is used more selectively within classical inversion algorithms.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Uncertainty Decomposition for In-Context Learning</title>
<link>https://arxiv.org/abs/2509.02327</link>
<guid>https://arxiv.org/abs/2509.02327</guid>
<content:encoded><![CDATA[
arXiv:2509.02327v3 Announce Type: replace-cross 
Abstract: As large language models (LLMs) gain popularity in conducting prediction tasks in-context, understanding the sources of uncertainty in in-context learning becomes essential to ensuring reliability. The recent hypothesis of in-context learning performing predictive Bayesian inference opens the avenue for Bayesian uncertainty estimation, particularly for decomposing uncertainty into epistemic uncertainty due to lack of in-context data and aleatoric uncertainty inherent in the in-context prediction task. However, the decomposition idea remains under-explored due to the intractability of the latent parameter posterior from the underlying Bayesian model. In this work, we introduce a variational uncertainty decomposition framework for in-context learning without explicitly sampling from the latent parameter posterior, by optimising auxiliary queries as probes to obtain an upper bound to the aleatoric uncertainty of an LLM's in-context learning procedure, which also induces a lower bound to the epistemic uncertainty. Through experiments on synthetic and real-world tasks, we show quantitatively and qualitatively that the decomposed uncertainties obtained from our method exhibit desirable properties of epistemic and aleatoric uncertainty.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KNARsack: Teaching Neural Algorithmic Reasoners to Solve Pseudo-Polynomial Problems</title>
<link>https://arxiv.org/abs/2509.15239</link>
<guid>https://arxiv.org/abs/2509.15239</guid>
<content:encoded><![CDATA[
arXiv:2509.15239v2 Announce Type: replace-cross 
Abstract: Neural algorithmic reasoning (NAR) is a growing field that aims to embed algorithmic logic into neural networks by imitating classical algorithms. In this extended abstract, we detail our attempt to build a neural algorithmic reasoner that can solve Knapsack, a pseudo-polynomial problem bridging classical algorithms and combinatorial optimisation, but omitted in standard NAR benchmarks. Our neural algorithmic reasoner is designed to closely follow the two-phase pipeline for the Knapsack problem, which involves first constructing the dynamic programming table and then reconstructing the solution from it. The approach, which models intermediate states through dynamic programming supervision, achieves better generalization to larger problem instances than a direct-prediction baseline that attempts to select the optimal subset only from the problem inputs.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable bayesian shadow tomography for quantum property estimation with set transformers</title>
<link>https://arxiv.org/abs/2509.18674</link>
<guid>https://arxiv.org/abs/2509.18674</guid>
<content:encoded><![CDATA[
arXiv:2509.18674v2 Announce Type: replace-cross 
Abstract: A scalable Bayesian machine learning framework is introduced for estimating scalar properties of an unknown quantum state from measurement data, which bypasses full density matrix reconstruction. This work is the first to integrate the classical shadows protocol with a permutation-invariant set transformer architecture, enabling the approach to predict and correct bias in existing estimators to approximate the true Bayesian posterior mean. Measurement outcomes are encoded as fixed-dimensional feature vectors, and the network outputs a residual correction to a baseline estimator. Scalability to large quantum systems is ensured by the polynomial dependence of input size on system size and number of measurements. On Greenberger-Horne-Zeilinger state fidelity and second-order R\'enyi entropy estimation tasks -- using random Pauli and random Clifford measurements -- this Bayesian estimator always achieves lower mean squared error than classical shadows alone, with more than a 99\% reduction in the few copy regime.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks</title>
<link>https://arxiv.org/abs/2509.22467</link>
<guid>https://arxiv.org/abs/2509.22467</guid>
<content:encoded><![CDATA[
arXiv:2509.22467v2 Announce Type: replace-cross 
Abstract: Deep neural networks achieve state-of-the-art performance in estimating heterogeneous treatment effects, but their opacity limits trust and adoption in sensitive domains such as medicine, economics, and public policy. Building on well-established and high-performing causal neural architectures, we propose causalKANs, a framework that transforms neural estimators of conditional average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By incorporating pruning and symbolic simplification, causalKANs yields interpretable closed-form formulas while preserving predictive accuracy. Experiments on benchmark datasets demonstrate that causalKANs perform on par with neural baselines in CATE error metrics, and that even simple KAN variants achieve competitive performance, offering a favorable accuracy--interpretability trade-off. By combining reliability with analytic accessibility, causalKANs provide auditable estimators supported by closed-form expressions and interpretable plots, enabling trustworthy individualized decision-making in high-stakes settings. We release the code for reproducibility at https://github.com/aalmodovares/causalkans .
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why is topology hard to learn?</title>
<link>https://arxiv.org/abs/2509.26261</link>
<guid>https://arxiv.org/abs/2509.26261</guid>
<content:encoded><![CDATA[
arXiv:2509.26261v2 Announce Type: replace-cross 
Abstract: Much attention has been devoted to the use of machine learning to approximate physical concepts. Yet, due to challenges in interpretability of machine learning techniques, the question of what physics machine learning models are able to learn remains open. Here we bridge the concept a physical quantity and its machine learning approximation in the context of the original application of neural networks in physics: topological phase classification. We construct a hybrid tensor-neural network object that exactly expresses real space topological invariant and rigorously assess its trainability and generalization. Specifically, we benchmark the accuracy and trainability of a tensor-neural network to multiple types of neural networks, thus exemplifying the differences in trainability and representational power. Our work highlights the challenges in learning topological invariants and constitutes a stepping stone towards more accurate and better generalizable machine learning representations in condensed matter physics.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Parallel Scaling with Interdependent Generations</title>
<link>https://arxiv.org/abs/2510.01143</link>
<guid>https://arxiv.org/abs/2510.01143</guid>
<content:encoded><![CDATA[
arXiv:2510.01143v2 Announce Type: replace-cross 
Abstract: Parallel LLM inference scaling involves sampling a set of $N>1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 39% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TempoControl: Temporal Attention Guidance for Text-to-Video Models</title>
<link>https://arxiv.org/abs/2510.02226</link>
<guid>https://arxiv.org/abs/2510.02226</guid>
<content:encoded><![CDATA[
arXiv:2510.02226v2 Announce Type: replace-cross 
Abstract: Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal pattern with a control signal (correlation), adjusting its strength where visibility is required (magnitude), and preserving semantic consistency (entropy). TempoControl provides precise temporal control while maintaining high video quality and diversity. We demonstrate its effectiveness across various applications, including temporal reordering of single and multiple objects, action timing, and audio-aligned video generation. Please see our project page for more details: https://shira-schiber.github.io/TempoControl/.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Noise to Data: Generative Flows from 1D Processes</title>
<link>https://arxiv.org/abs/2510.12636</link>
<guid>https://arxiv.org/abs/2510.12636</guid>
<content:encoded><![CDATA[
arXiv:2510.12636v3 Announce Type: replace-cross 
Abstract: We introduce a general framework for constructing generative models using one-dimensional noising processes. Beyond diffusion processes, we outline examples that demonstrate the flexibility of our approach. Motivated by this, we propose a novel framework in which the 1D processes themselves are learnable, achieved by parameterizing the noise distribution through quantile functions that adapt to the data. Our construction integrates seamlessly with standard objectives, including Flow Matching and consistency models. Learning quantile-based noise naturally captures heavy tails and compact supports when present. Numerical experiments highlight both the flexibility and the effectiveness of our method.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</title>
<link>https://arxiv.org/abs/2510.18526</link>
<guid>https://arxiv.org/abs/2510.18526</guid>
<content:encoded><![CDATA[
arXiv:2510.18526v2 Announce Type: replace-cross 
Abstract: As large language models (LLMs) become increasingly integrated into applications serving users across diverse cultures, communities and demographics, it is critical to align LLMs with pluralistic human values beyond average principles (e.g., HHH). In psychological and social value theories such as Schwartz's Value Theory, pluralistic values are represented by multiple value dimensions paired with various priorities. However, existing methods encounter two challenges when aligning with such fine-grained value objectives: 1) they often treat multiple values as independent and equally important, ignoring their interdependence and relative priorities (value complexity); 2) they struggle to precisely control nuanced value priorities, especially those underrepresented ones (value steerability). To handle these challenges, we propose COUPLE, a COUnterfactual reasoning framework for PLuralistic valuE alignment. It introduces a structural causal model (SCM) to feature complex interdependency and prioritization among features, as well as the causal relationship between high-level value dimensions and behaviors. Moreover, it applies counterfactual reasoning to generate outputs aligned with any desired value objectives. Benefitting from explicit causal modeling, COUPLE also provides better interpretability. We evaluate COUPLE on two datasets with different value systems and demonstrate that COUPLE advances other baselines across diverse types of value objectives.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein Distributionally Robust Nash Equilibrium Seeking with Heterogeneous Data: A Lagrangian Approach</title>
<link>https://arxiv.org/abs/2511.14048</link>
<guid>https://arxiv.org/abs/2511.14048</guid>
<content:encoded><![CDATA[
arXiv:2511.14048v2 Announce Type: replace-cross 
Abstract: We study a class of distributionally robust games where agents are allowed to heterogeneously choose their risk aversion with respect to distributional shifts of the uncertainty. In our formulation, heterogeneous Wasserstein ball constraints on each distribution are enforced through a penalty function leveraging a Lagrangian formulation. We then formulate the distributionally robust Nash equilibrium problem and show that under certain assumptions it is equivalent to a finite-dimensional variational inequality problem with a strongly monotone mapping. We then design an approximate Nash equilibrium seeking algorithm and prove convergence of the average regret to a quantity that diminishes with the number of iterations, thus learning the desired equilibrium up to an a priori specified accuracy. Numerical simulations corroborate our theoretical findings.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Visually Interpretable Oscillator Networks for Soft Continuum Robots from Video</title>
<link>https://arxiv.org/abs/2511.18322</link>
<guid>https://arxiv.org/abs/2511.18322</guid>
<content:encoded><![CDATA[
arXiv:2511.18322v2 Announce Type: replace-cross 
Abstract: Data-driven learning of soft continuum robot (SCR) dynamics from high-dimensional observations offers flexibility but often lacks physical interpretability, while model-based approaches require prior knowledge and can be computationally expensive. We bridge this gap by introducing (1) the Attention Broadcast Decoder (ABCD), a plug-and-play module for autoencoder-based latent dynamics learning that generates pixel-accurate attention maps localizing each latent dimension's contribution while filtering static backgrounds. (2) By coupling these attention maps to 2D oscillator networks, we enable direct on-image visualization of learned dynamics (masses, stiffness, and forces) without prior knowledge. We validate our approach on single- and double-segment SCRs, demonstrating that ABCD-based models significantly improve multi-step prediction accuracy: 5.7x error reduction for Koopman operators and 3.5x for oscillator networks on the two-segment robot. The learned oscillator network autonomously discovers a chain structure of oscillators. Unlike standard methods, ABCD models enable smooth latent space extrapolation beyond training data. This fully data-driven approach yields compact, physically interpretable models suitable for control applications.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structure is Supervision: Multiview Masked Autoencoders for Radiology</title>
<link>https://arxiv.org/abs/2511.22294</link>
<guid>https://arxiv.org/abs/2511.22294</guid>
<content:encoded><![CDATA[
arXiv:2511.22294v3 Announce Type: replace-cross 
Abstract: Building robust medical machine learning systems requires pretraining strategies that exploit the intrinsic structure present in clinical data. We introduce Multiview Masked Autoencoder (MVMAE), a self-supervised framework that leverages the natural multi-view organization of radiology studies to learn view-invariant and disease-relevant representations. MVMAE combines masked image reconstruction with cross-view alignment, transforming clinical redundancy across projections into a powerful self-supervisory signal. We further extend this approach with MVMAE-V2T, which incorporates radiology reports as an auxiliary text-based learning signal to enhance semantic grounding while preserving fully vision-based inference. Evaluated on a downstream disease classification task on three large-scale public datasets, MIMIC-CXR, CheXpert, and PadChest, MVMAE consistently outperforms supervised and vision-language baselines. Furthermore, MVMAE-V2T provides additional gains, particularly in low-label regimes where structured textual supervision is most beneficial. Together, these results establish the importance of structural and textual supervision as complementary paths toward scalable, clinically grounded medical foundation models.
]]></content:encoded>
<pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASCIIBench: Evaluating Language-Model-Based Understanding of Visually-Oriented Text</title>
<link>https://arxiv.org/abs/2512.04125</link>
<guid>https://arxiv.org/abs/2512.04125</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, ASCII art, ASCIIBench, multimodal representation, CLIP embeddings<br /><br />Summary:<br /><br />1. Large language models (LLMs) show emergent behaviors such as reasoning and fluent long-form text generation but continue to face difficulties with tasks demanding precise spatial and positional reasoning.  
2. ASCII art, which uses characters to encode structure and form symbolically, serves as a challenging domain to probe these limitations in LLMs.  
3. The authors introduce ASCIIBench, a new benchmark comprising a filtered dataset of 5,315 class-labeled ASCII images, designed for evaluating both the generation and classification of ASCII-text images. This is the first publicly available benchmark dedicated to this purpose.  
4. Along with the dataset, the paper releases weights for a fine-tuned CLIP model specifically adapted to understand ASCII structure, enabling an evaluation framework for LLM-generated ASCII art.  
5. Analysis reveals that cosine similarity on CLIP embeddings generally fails to differentiate most ASCII categories, performing at chance level even for classes with low intra-class variance. However, categories exhibiting high internal similarity demonstrate clear discriminability, indicating that the main obstacle lies in the quality of the representation rather than generational variance.  
6. These outcomes highlight ASCII art as a stringent test for multimodal representation methods and motivate the creation of novel embeddings or evaluation metrics that better handle symbolic visual modalities.  
7. All resources including dataset and model weights are publicly available at https://github.com/ASCIIBench/ASCIIBench. <div>
arXiv:2512.04125v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated several emergent behaviors with scale, including reasoning and fluency in long-form text generation. However, they continue to struggle with tasks requiring precise spatial and positional reasoning. ASCII art, a symbolic medium where characters encode structure and form, provides a unique probe of this limitation. We introduce ASCIIBench, a novel benchmark for evaluating both the generation and classification of ASCII-text images. ASCIIBench consists of a filtered dataset of 5,315 class-labeled ASCII images and is, to our knowledge, the first publicly available benchmark of its kind. Alongside the dataset, we release weights for a fine-tuned CLIP model adapted to capture ASCII structure, enabling the evaluation of LLM-generated ASCII art. Our analysis shows that cosine similarity over CLIP embeddings fails to separate most ASCII categories, yielding chance-level performance even for low-variance classes. In contrast, classes with high internal mean similarity exhibit clear discriminability, revealing that the bottleneck lies in representation rather than generational variance. These findings position ASCII art as a stress test for multimodal representations and motivate the development of new embedding methods or evaluation metrics tailored to symbolic visual modalities. All resources are available at https://github.com/ASCIIBench/ASCIIBench.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decoding Large Language Diffusion Models with Foreseeing Movement</title>
<link>https://arxiv.org/abs/2512.04135</link>
<guid>https://arxiv.org/abs/2512.04135</guid>
<content:encoded><![CDATA[
<div> Large Language Diffusion Models, Decoding Order, Foreseeing Decoding Method, Search-Based Optimization, Efficiency-Performance Trade-off<br /><br />Summary:<br /><br />Large Language Diffusion Models (LLDMs) offer benefits such as flexible decoding, enabling parallelized inference and controllable outputs over traditional autoregressive models. However, their inference performance is highly sensitive to the token decoding order, which poses a significant challenge. Existing heuristic approaches primarily address local impacts, neglecting the long-term effects on performance. To overcome this, the authors introduce the Foreseeing Decoding Method (FDM), which incorporates both local and global factors through a search-based strategy for effective optimization in discrete token spaces. In addition, they propose FDM with Acceleration (FDM-A), a variant that restricts intensive exploration to critical decoding steps determined by analyzing token consistency, focusing on exploration and balance circumstances. Extensive experiments across various benchmarks and architectures demonstrate the scalability of FDM and highlight the superior efficiency-performance trade-off provided by FDM-A. This research represents a principled advancement toward more powerful and effective decoding methods tailored for LLDMs, potentially improving the quality and speed of generated outputs in large language diffusion tasks. <div>
arXiv:2512.04135v1 Announce Type: new 
Abstract: Large Language Diffusion Models (LLDMs) benefit from a flexible decoding mechanism that enables parallelized inference and controllable generations over autoregressive models. Yet such flexibility introduces a critical challenge: inference performance becomes highly sensitive to the decoding order of tokens. Existing heuristic methods, however, focus mainly on local effects while overlooking long-term impacts. To address this limitation, we propose the Foreseeing Decoding Method (FDM), a novel approach that integrates both local and global considerations to unlock the full potential, employing a search-based strategy to enable effective optimization in discrete spaces. Furthermore, by analyzing the consistency of chosen tokens in the full decoding process, we develop a variant, FDM with Acceleration (FDM-A), which restricts deep exploration to critical steps identified as the exploration and balance circumantences. Extensive experiments across diverse benchmarks and model architectures validate the scalability of FDM and demonstrate the superior efficiency-performance trade-off achieved by FDM-A. Our work might potentially provide a principled step toward more powerful decoding methods for LLDMs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MechDetect: Detecting Data-Dependent Errors</title>
<link>https://arxiv.org/abs/2512.04138</link>
<guid>https://arxiv.org/abs/2512.04138</guid>
<content:encoded><![CDATA[
<div> Data quality, error generation, missing values, MechDetect, machine learning<br /><br />Summary: This paper addresses the challenge of data quality monitoring by focusing on understanding error generation mechanisms rather than just detecting data errors or shifts. The authors introduce MechDetect, a novel yet simple algorithm designed to investigate whether errors in tabular datasets depend on the underlying data, using machine learning models. Building on statistical methods traditionally used for analyzing missing data mechanisms, MechDetect extends these principles to a broader range of error types, requiring only an error mask that marks erroneous entries. The approach not only detects errors but also helps trace their origins, facilitating more effective error fixing and data quality improvement. Experiments on established benchmark datasets demonstrate the practical effectiveness of MechDetect, validating its ability to accurately identify dependencies between errors and data. This work represents an important step in enhancing data quality monitoring by shifting the focus from error detection to understanding error causation, potentially impacting diverse applications that rely heavily on clean and reliable datasets. <div>
arXiv:2512.04138v1 Announce Type: new 
Abstract: Data quality monitoring is a core challenge in modern information processing systems. While many approaches to detect data errors or shifts have been proposed, few studies investigate the mechanisms governing error generation. We argue that knowing how errors were generated can be key to tracing and fixing them. In this study, we build on existing work in the statistics literature on missing values and propose MechDetect, a simple algorithm to investigate error generation mechanisms. Given a tabular data set and a corresponding error mask, the algorithm estimates whether or not the errors depend on the data using machine learning models. Our work extends established approaches to detect mechanisms underlying missing values and can be readily applied to other error types, provided that an error mask is available. We demonstrate the effectiveness of MechDetect in experiments on established benchmark datasets.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating the Curse of Detail: Scaling Arguments for Feature Learning and Sample Complexity</title>
<link>https://arxiv.org/abs/2512.04165</link>
<guid>https://arxiv.org/abs/2512.04165</guid>
<content:encoded><![CDATA[
<div> feature learning, implicit bias, deep networks, scaling analysis, attention heads<br /><br />Summary:<br /><br />This article addresses two central challenges in deep learning theory: understanding feature learning mechanisms and characterizing the implicit bias of networks in the rich learning regime. Most existing theories focus on simplified models such as networks with one or two trainable layers or deep linear networks, often resulting in complex nonlinear equations that are computationally demanding to solve. To overcome this complexity, the authors propose a heuristic scale analysis approach that predicts the data and network width scales at which specific feature learning patterns arise. This method simplifies the analytical landscape while successfully reproducing the scaling exponents found in prior exact theoretical results. Moreover, the approach extends beyond previously studied scenarios by offering new predictions for more complex network architectures, including three-layer nonlinear networks and attention mechanisms. Thus, their work broadens the foundation of first-principle theories in deep learning by providing a more accessible yet effective tool for investigating feature learning dynamics in richer and more realistic network configurations. <div>
arXiv:2512.04165v1 Announce Type: new 
Abstract: Two pressing topics in the theory of deep learning are the interpretation of feature learning mechanisms and the determination of implicit bias of networks in the rich regime. Current theories of rich feature learning effects revolve around networks with one or two trainable layers or deep linear networks. Furthermore, even under such limiting settings, predictions often appear in the form of high-dimensional non-linear equations, which require computationally intensive numerical solutions. Given the many details that go into defining a deep learning problem, this analytical complexity is a significant and often unavoidable challenge. Here, we propose a powerful heuristic route for predicting the data and width scales at which various patterns of feature learning emerge. This form of scale analysis is considerably simpler than such exact theories and reproduces the scaling exponents of various known results. In addition, we make novel predictions on complex toy architectures, such as three-layer non-linear networks and attention heads, thus extending the scope of first-principle theories of deep learning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BEP: A Binary Error Propagation Algorithm for Binary Neural Networks Training</title>
<link>https://arxiv.org/abs/2512.04189</link>
<guid>https://arxiv.org/abs/2512.04189</guid>
<content:encoded><![CDATA[
<div> Binary Neural Networks, Binary Error Propagation, Discrete Backpropagation, Bitwise Operations, Recurrent Neural Networks<br /><br />Summary:<br /><br />This paper addresses the challenges of training Binary Neural Networks (BNNs), which use binary weights and activations to reduce complexity, memory, and energy use, making them ideal for resource-limited devices. Traditional quantization-aware training requires maintaining full-precision parameters and backward passes with floating-point arithmetic, compromising efficiency during training. Existing alternatives using local learning rules fail at global credit assignment and backpropagation through multiple layers. The authors propose Binary Error Propagation (BEP), a novel learning algorithm that creates a discrete analog of the backpropagation chain rule. BEP propagates error signals as binary vectors through multiple layers using exclusively bitwise operations both in forward and backward passes. This breakthrough enables fully binary end-to-end training, including for recurrent neural networks, which was previously unattainable. Experiments on multilayer perceptrons and recurrent neural networks demonstrate significant improvements in test accuracy, with gains up to +6.89% and +10.57%, respectively. Additionally, the algorithm is made publicly available as open-source software, promoting further research and practical deployment of fully binary-trained models. <div>
arXiv:2512.04189v1 Announce Type: new 
Abstract: Binary Neural Networks (BNNs), which constrain both weights and activations to binary values, offer substantial reductions in computational complexity, memory footprint, and energy consumption. These advantages make them particularly well suited for deployment on resource-constrained devices. However, training BNNs via gradient-based optimization remains challenging due to the discrete nature of their variables. The dominant approach, quantization-aware training, circumvents this issue by employing surrogate gradients. Yet, this method requires maintaining latent full-precision parameters and performing the backward pass with floating-point arithmetic, thereby forfeiting the efficiency of binary operations during training. While alternative approaches based on local learning rules exist, they are unsuitable for global credit assignment and for back-propagating errors in multi-layer architectures. This paper introduces Binary Error Propagation (BEP), the first learning algorithm to establish a principled, discrete analog of the backpropagation chain rule. This mechanism enables error signals, represented as binary vectors, to be propagated backward through multiple layers of a neural network. BEP operates entirely on binary variables, with all forward and backward computations performed using only bitwise operations. Crucially, this makes BEP the first solution to enable end-to-end binary training for recurrent neural network architectures. We validate the effectiveness of BEP on both multi-layer perceptrons and recurrent neural networks, demonstrating gains of up to +6.89% and +10.57% in test accuracy, respectively. The proposed algorithm is released as an open-source repository.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Network of Theseus (like the ship)</title>
<link>https://arxiv.org/abs/2512.04198</link>
<guid>https://arxiv.org/abs/2512.04198</guid>
<content:encoded><![CDATA[
<div> Keywords: inductive bias, architecture conversion, representational similarity, optimization decoupling, efficiency tradeoffs<br /><br />Summary:  
1. The paper challenges the common assumption in deep learning that the architecture used during training must be the same as the one used for inference, highlighting this as a limitation in exploring efficient or novel architectures.  
2. The authors introduce Network of Theseus (NoT), a method that progressively transforms a guide network architecture into a different target architecture part-by-part without losing the original network's performance.  
3. This transformation approach involves incrementally replacing components of the guide network with modules from the target architecture, aligning them at each step using representational similarity metrics to preserve functional integrity.  
4. NoT can handle substantial architectural changes, such as converting convolutional networks to multilayer perceptrons or GPT-2 models to recurrent neural networks, demonstrating flexibility across diverse network types.  
5. By decoupling optimization (training) from deployment (inference), NoT broadens the range of architectures that can be used during inference, promoting better accuracy-efficiency tradeoffs and enabling more strategic architectural design exploration. <div>
arXiv:2512.04198v1 Announce Type: new 
Abstract: A standard assumption in deep learning is that the inductive bias introduced by a neural network architecture must persist from training through inference. The architecture you train with is the architecture you deploy. This assumption constrains the community from selecting architectures that may have desirable efficiency or design properties due to difficulties with optimization. We challenge this assumption with Network of Theseus (NoT), a method for progressively converting a trained, or even untrained, guide network architecture part-by-part into an entirely different target network architecture while preserving the performance of the guide network. At each stage, components in the guide network architecture are incrementally replaced with target architecture modules and aligned via representational similarity metrics. This procedure largely preserves the functionality of the guide network even under substantial architectural changes-for example, converting a convolutional network into a multilayer perceptron, or GPT-2 into a recurrent neural network. By decoupling optimization from deployment, NoT expands the space of viable inference-time architectures, opening opportunities for better accuracy-efficiency tradeoffs and enabling more directed exploration of the architectural design space.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ActVAE: Modelling human activity schedules with a deep conditional generative approach</title>
<link>https://arxiv.org/abs/2512.04223</link>
<guid>https://arxiv.org/abs/2512.04223</guid>
<content:encoded><![CDATA[
<div> Keywords: human activity scheduling, conditional VAE, deep generative models, latent generative approach, demand modelling<br /><br />Summary:<br /><br />This paper addresses the challenge of modelling the complexity and diversity inherent in human activity scheduling behavior. The authors propose a novel deep conditional-generative machine learning method based on a Conditional Variational Autoencoder (VAE) architecture that generates realistic activity schedules conditioned on input labels such as age and employment status. The approach combines a structured latent generative framework with conditional inputs, enabling rapid creation of precise and varied schedules tailored to different individual profiles. Extensive evaluation is performed through a joint density estimation framework and multiple case studies to validate model performance. The study also highlights the practicality of the method in terms of data and computational requirements, making it suitable for integration into both new and existing demand forecasting frameworks. A key contribution is the comparison of their conditional-generative model against (i) purely generative models lacking conditionality and (ii) purely conditional models that produce a single most likely schedule. This comparison underscores the advantage of explicitly modelling the inherent randomness and diversity of human behavior, which enhances the realism and usefulness of generated activity schedules in various applications. <div>
arXiv:2512.04223v1 Announce Type: new 
Abstract: Modelling the complexity and diversity of human activity scheduling behaviour is inherently challenging. We demonstrate a deep conditional-generative machine learning approach for the modelling of realistic activity schedules depending on input labels such as an individual's age, employment status, or other information relevant to their scheduling. We combine (i) a structured latent generative approach, with (ii) a conditional approach, through a novel Conditional VAE architecture. This allows for the rapid generation of precise and realistic schedules for different input labels. We extensively evaluate model capabilities using a joint density estimation framework and several case studies. We additionally show that our approach has practical data and computational requirements, and can be deployed within new and existing demand modelling frameworks. We evaluate the importance of generative capability more generally, by comparing our combined approach to (i) a purely generative model without conditionality, and (ii) a purely conditional model which outputs the most likely schedule given the input labels. This comparison highlights the usefulness of explicitly modelling the randomness of complex and diverse human behaviours using deep generative approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fine-Tuning ChemBERTa for Predicting Inhibitory Activity Against TDP1 Using Deep Learning</title>
<link>https://arxiv.org/abs/2512.04252</link>
<guid>https://arxiv.org/abs/2512.04252</guid>
<content:encoded><![CDATA[
<div> Keywords: TDP1 inhibitors, deep learning, ChemBERTa, pIC50 prediction, virtual screening<br /><br />Summary:<br /><br />1. The study addresses the challenge of predicting the inhibitory potency (pIC50) of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1), a key target implicated in overcoming cancer chemoresistance.<br />2. A deep learning framework based on ChemBERTa, a pre-trained chemical language model that ingests molecular SMILES strings, is fine-tuned to regress pIC50 values quantitatively.<br />3. The authors utilize a large, consensus dataset comprising 177,092 compounds, confronting severe activity imbalance with only 2.1% active compounds by deploying stratified data splits and sample weighting.<br />4. Two pre-training strategies—Masked Language Modeling (MLM) and Masked Token Regression (MTR)—are systematically evaluated to optimize model performance under these constraints.<br />5. The proposed model outperforms a random predictor baseline in regression accuracy and virtual screening utility, and achieves competitive results relative to Random Forest models, featuring enrichment factor EF@1% of 17.4 and precision Precision@1% of 37.4 among top predictions.<br />6. Rigorous ablation and hyperparameter studies validate the robustness and readiness of the model for practical application.<br />7. By enabling direct, 3D-structure-free prediction of pIC50 from SMILES strings, this work highlights the transformative potential of chemical transformer models in accelerating target-specific drug discovery efforts. <div>
arXiv:2512.04252v1 Announce Type: new 
Abstract: Predicting the inhibitory potency of small molecules against Tyrosyl-DNA Phosphodiesterase 1 (TDP1)-a key target in overcoming cancer chemoresistance-remains a critical challenge in early drug discovery. We present a deep learning framework for the quantitative regression of pIC50 values from molecular Simplified Molecular Input Line Entry System (SMILES) strings using fine-tuned variants of ChemBERTa, a pre-trained chemical language model. Leveraging a large-scale consensus dataset of 177,092 compounds, we systematically evaluate two pre-training strategies-Masked Language Modeling (MLM) and Masked Token Regression (MTR)-under stratified data splits and sample weighting to address severe activity imbalance which only 2.1% are active. Our approach outperforms classical baselines Random Predictor in both regression accuracy and virtual screening utility, and has competitive performance compared to Random Forest, achieving high enrichment factor EF@1% 17.4 and precision Precision@1% 37.4 among top-ranked predictions. The resulting model, validated through rigorous ablation and hyperparameter studies, provides a robust, ready-to-deploy tool for prioritizing TDP1 inhibitors for experimental testing. By enabling accurate, 3D-structure-free pIC50 prediction directly from SMILES, this work demonstrates the transformative potential of chemical transformers in accelerating target-specific drug discovery.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Studying Various Activation Functions and Non-IID Data for Machine Learning Model Robustness</title>
<link>https://arxiv.org/abs/2512.04264</link>
<guid>https://arxiv.org/abs/2512.04264</guid>
<content:encoded><![CDATA[
<div> Activation functions, Adversarial training, Model robustness, Federated learning, Data sharing<br /><br />Summary:<br /><br />This paper investigates the robustness of machine learning (ML) models using adversarial training with ten different activation functions beyond the commonly used ReLU, focusing on both centralized and federated learning environments. In the centralized setting, the authors present an advanced adversarial training method that combines architectural changes, soft labeling, simplified data augmentation, and varied learning rates to boost model robustness. Experiments on CIFAR-10 demonstrate that their approach achieves a natural accuracy of 77.08% and a robust accuracy of 67.96% against fast gradient sign attacks, with ReLU generally outperforming other activation functions. Extending to federated learning scenarios, both IID and non-IID data distributions are analyzed, revealing a substantial drop in robust accuracy, particularly under non-IID conditions. To address this decline, the paper proposes implementing data sharing among clients; when 40% of data is shared, the natural and robust accuracies improve to 70.09% and 54.79%, respectively, surpassing the performance of the CalFAT algorithm. The study concludes that proper data sharing can significantly enhance ML model robustness in federated learning, offering practical benefits for real-world applications. <div>
arXiv:2512.04264v1 Announce Type: new 
Abstract: Adversarial training is an effective method to improve the machine learning (ML) model robustness. Most existing studies typically consider the Rectified linear unit (ReLU) activation function and centralized training environments. In this paper, we study the ML model robustness using ten different activation functions through adversarial training in centralized environments and explore the ML model robustness in federal learning environments. In the centralized environment, we first propose an advanced adversarial training approach to improving the ML model robustness by incorporating model architecture change, soft labeling, simplified data augmentation, and varying learning rates. Then, we conduct extensive experiments on ten well-known activation functions in addition to ReLU to better understand how they impact the ML model robustness. Furthermore, we extend the proposed adversarial training approach to the federal learning environment, where both independent and identically distributed (IID) and non-IID data settings are considered. Our proposed centralized adversarial training approach achieves a natural and robust accuracy of 77.08% and 67.96%, respectively on CIFAR-10 against the fast gradient sign attacks. Experiments on ten activation functions reveal ReLU usually performs best. In the federated learning environment, however, the robust accuracy decreases significantly, especially on non-IID data. To address the significant performance drop in the non-IID data case, we introduce data sharing and achieve the natural and robust accuracy of 70.09% and 54.79%, respectively, surpassing the CalFAT algorithm, when 40% data sharing is used. That is, a proper percentage of data sharing can significantly improve the ML model robustness, which is useful to some real-world applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Initialization Determines Whether In-Context Learning Is Gradient Descent</title>
<link>https://arxiv.org/abs/2512.04268</link>
<guid>https://arxiv.org/abs/2512.04268</guid>
<content:encoded><![CDATA[
<div> In-context learning, linear self-attention, gradient descent, multi-head attention, initial guess  

<br /><br />Summary:  
This paper investigates the relationship between multi-head linear self-attention (LSA) in large language models and gradient descent (GD) within the framework of in-context learning (ICL). It challenges prior assumptions that linked LSA to GD under restrictive conditions such as zero-mean Gaussian priors and zero initialization. The authors extend the study to incorporate non-zero Gaussian prior means in linear regression setups typical in ICL. They introduce the concept of an initial guess embedded in the multi-head LSA embedding matrix, proving an upper bound on the number of attention heads needed for effective ICL-based linear regression. Experimental results confirm this theoretical bound but reveal a performance gap between traditional one-step GD and multi-head LSA. To mitigate this gap, the paper proposes yq-LSA, a generalized single-head LSA with a trainable initial guess parameter (yq). Theoretical analysis and experiments demonstrate the enhanced expressivity and performance of yq-LSA on linear regression tasks. Finally, extending the concept beyond linear regression, they show that equipping large language models with initial guess capabilities improves performance on semantic similarity tasks, suggesting broader implications for optimization-like inference in LLMs. <div>
arXiv:2512.04268v1 Announce Type: new 
Abstract: In-context learning (ICL) in large language models (LLMs) is a striking phenomenon, yet its underlying mechanisms remain only partially understood. Previous work connects linear self-attention (LSA) to gradient descent (GD), this connection has primarily been established under simplified conditions with zero-mean Gaussian priors and zero initialization for GD. However, subsequent studies have challenged this simplified view by highlighting its overly restrictive assumptions, demonstrating instead that under conditions such as multi-layer or nonlinear attention, self-attention performs optimization-like inference, akin to but distinct from GD. We investigate how multi-head LSA approximates GD under more realistic conditions specifically when incorporating non-zero Gaussian prior means in linear regression formulations of ICL. We first extend multi-head LSA embedding matrix by introducing an initial estimation of the query, referred to as the initial guess. We prove an upper bound on the number of heads needed for ICL linear regression setup. Our experiments confirm this result and further observe that a performance gap between one-step GD and multi-head LSA persists. To address this gap, we introduce yq-LSA, a simple generalization of single-head LSA with a trainable initial guess yq. We theoretically establish the capabilities of yq-LSA and provide experimental validation on linear regression tasks, thereby extending the theory that bridges ICL and GD. Finally, inspired by our findings in the case of linear regression, we consider widespread LLMs augmented with initial guess capabilities, and show that their performance is improved on a semantic similarity task.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bootstrapped Mixed Rewards for RL Post-Training: Injecting Canonical Action Order</title>
<link>https://arxiv.org/abs/2512.04277</link>
<guid>https://arxiv.org/abs/2512.04277</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, Transformer, Sudoku, policy optimization, solver ordering<br /><br />Summary:  
1. The paper investigates post-training of models with reinforcement learning (RL) focusing on incorporating structural hints rather than optimizing a single scalar objective.  
2. Specifically, it explores whether providing a scalar hint aligned with a canonical solver ordering during RL post-training can improve model performance, even when fine-tuned on randomized solution sequences.  
3. The study uses a Transformer model trained on Sudoku-solving tasks, initially fine-tuned with randomized cell-solving orders. Post-training is then conducted with Group Relative Policy Optimization (GRPO) that optimizes two reward components: cell accuracy and an ordering reward that favors emission sequences matching a known solver order.  
4. To fairly compare the combined reward signals, the authors use fixed mixtures and a bootstrapped scaling method to balance the magnitude of rewards at initialization.  
5. Results demonstrate that mixed rewards (cell accuracy plus ordering) outperform using cell accuracy alone, with the best reward mixture substantially increasing test accuracy, closely approaching the performance of models fine-tuned purely on solver-order sequences.  
The findings suggest that introducing coarse ordering signals during RL post-training can guide model behavior toward more canonical solution trajectories without changing the supervised training data or model architecture. <div>
arXiv:2512.04277v1 Announce Type: new 
Abstract: Post-training with reinforcement learning (RL) typically optimizes a single scalar objective and ignores structure in how solutions are produced. We ask whether a scalar hint toward a canonical solver ordering, used only during RL post-training, improves performance even when fine-tuned on randomized solution sequences. On Sudoku, we train a Transformer with standard fine-tuning on randomized solving orders, then post-train it with Group Relative Policy Optimization (GRPO) with two rewards: cell accuracy and an ordering reward that increases when the model's emission order aligns with the solver order. To compare signals cleanly, we combine them via fixed mixtures and use a simple bootstrapped scaling to equalize component magnitudes at initialization. Mixed rewards generally outperform cell-only optimization--the best mixture yields substantially higher test accuracy than the fine-tuned-only model trained on random-order and approaches the fine-tuned-only model trained on solver-order sequences in accuracy. These results suggest that coarse ordering signals can steer RL post-training toward solver-order trajectories without modifying supervised data or architecture.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRASP: GRouped Activation Shared Parameterization for Parameter-Efficient Fine-Tuning and Robust Inference of Transformers</title>
<link>https://arxiv.org/abs/2512.04296</link>
<guid>https://arxiv.org/abs/2512.04296</guid>
<content:encoded><![CDATA[
<div> Keywords: Parameter-efficient fine-tuning, GRASP, StochGRASP, robustness, noise-aware loss function<br /><br />Summary:<br />1. The paper introduces GRASP (GRouped Activation Shared Parameterization), a parameter-efficient fine-tuning (PEFT) method that partitions token representations into groups and applies shared scaling and shifting vectors, significantly reducing trainable parameters while maintaining task-specific learning capacity.<br />2. Building on GRASP, the authors propose StochGRASP, which models Gaussian perturbations to pre-trained weights, offering a probabilistic parameterization instead of deterministic weight updates.<br />3. StochGRASP incorporates a noise-aware loss function designed to improve robustness against hardware-level variability and non-ideal inference conditions common in edge AI hardware.<br />4. Experimental results on benchmarks like GLUE (with RoBERTa-base and RoBERTa-large) and E2E NLG (using GPT-2 Medium) show that GRASP matches or outperforms existing PEFT techniques while reducing the number of trainable parameters by an order of magnitude compared to methods such as LoRA and BitFit.<br />5. Under different noise levels, StochGRASP consistently surpasses deterministic approaches, making it particularly well-suited for deployment on energy-efficient, noise-prone AI hardware platforms. <div>
arXiv:2512.04296v1 Announce Type: new 
Abstract: Parameter-efficient fine-tuning (PEFT) provides a scalable alternative to full-model adaptation by updating only a small subset of parameters in large pre-trained models. We introduce GRASP - GRouped Activation Shared Parameterization - a lightweight PEFT framework that partitions the D-dimensional token representations of selected layers into K << D groups and learns a shared scaling and shifting vector for each group. This grouped modulation reduces the number of trainable parameters significantly while preserving the ability of the model to learn task-specific features. Building on this formulation, we further propose StochGRASP, which learns Gaussian distributions as perturbations to the pre-trained weights rather than deterministic values. This probabilistic parameterization along with a noise-aware loss function formulation enables modelling hardware-level variability in programmed weights and significantly improves robustness under non-ideal inference conditions-an important requirement for deployment on edge-based emerging AI hardware. Across GLUE (RoBERTa-base & RoBERTa-large) and E2E NLG (GPT-2 Medium), GRASP matches or exceeds the performance of established PEFT methods while achieving an order of magnitude reduction in trainable parameters compared to LoRA and BitFit. Under varying levels of noise, StochGRASP consistently outperforms deterministic variants, demonstrating its suitability for energy-efficient and noise-prone hardware platforms.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When do spectral gradient updates help in deep learning?</title>
<link>https://arxiv.org/abs/2512.04299</link>
<guid>https://arxiv.org/abs/2512.04299</guid>
<content:encoded><![CDATA[
<div> Keywords: spectral gradient methods, Muon optimizer, stable rank, nuclear-to-Frobenius ratio, deep neural networks<br /><br />Summary:<br /><br />This paper investigates spectral gradient methods, focusing on the Muon optimizer, as alternatives to traditional Euclidean gradient descent in training deep neural networks and transformers. The authors propose a novel layerwise analytical condition to predict when spectral updates outperform Euclidean gradient steps. This condition evaluates the relationship between the squared nuclear-to-Frobenius ratio of the gradient and the stable rank of the incoming activations for each parameter block. The study first establishes that post-activation matrices possess low stable rank at Gaussian initialization within random feature regression models, feedforward networks, and transformer blocks. Further, in spiked random feature models, they demonstrate that after an initial burn-in period, the Euclidean gradient's nuclear-to-Frobenius ratio increases with data dimension, while the stable rank of activations remains bounded; this suggests an increasing advantage for spectral updates as dimension grows. Empirical validation is performed through synthetic regression experiments and training of language models at the NanoGPT scale, confirming that intermediate activations retain low stable rank and gradients sustain high nuclear-to-Frobenius ratios during training. Overall, the findings elucidate precise conditions where spectral gradient methods like Muon can effectively enhance deep learning optimization. <div>
arXiv:2512.04299v1 Announce Type: new 
Abstract: Spectral gradient methods, such as the recently popularized Muon optimizer, are a promising alternative to standard Euclidean gradient descent for training deep neural networks and transformers, but it is still unclear in which regimes they are expected to perform better. We propose a simple layerwise condition that predicts when a spectral update yields a larger decrease in the loss than a Euclidean gradient step. This condition compares, for each parameter block, the squared nuclear-to-Frobenius ratio of the gradient to the stable rank of the incoming activations. To understand when this condition may be satisfied, we first prove that post-activation matrices have low stable rank at Gaussian initialization in random feature regression, feedforward networks, and transformer blocks. In spiked random feature models we then show that, after a short burn-in, the Euclidean gradient's nuclear-to-Frobenius ratio grows with the data dimension while the stable rank of the activations remains bounded, so the predicted advantage of spectral updates scales with dimension. We validate these predictions in synthetic regression experiments and in NanoGPT-scale language model training, where we find that intermediate activations have low-stable-rank throughout training and the corresponding gradients maintain large nuclear-to-Frobenius ratios. Together, these results identify conditions for spectral gradient methods, such as Muon, to be effective in training deep networks and transformers.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evaluating Long-Context Reasoning in LLM-Based WebAgents</title>
<link>https://arxiv.org/abs/2512.04307</link>
<guid>https://arxiv.org/abs/2512.04307</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, long context reasoning, WebAgents, multi-session interactions, retrieval-augmented generation (RAG)  

<br /><br />Summary:  
This paper addresses the challenge of enabling WebAgents powered by large language models (LLMs) to effectively reason over long interaction histories in digital environments. It introduces a benchmark focused on sequentially dependent subtasks that require retrieving and applying information from extended user interaction histories spanning 25,000 to 150,000 tokens. The evaluation framework simulates realistic multi-session user interactions by inserting irrelevant task trajectories between relevant subtasks, thereby increasing complexity and context length. Four popular models—Claude-3.7, GPT-4.1, Llama 4, and o4-mini—were tested, showing a significant drop in task success rates from 40-50% in short contexts to below 10% when handling long contexts. Error analysis revealed primary failure modes such as agents getting stuck in repetitive loops and losing track of the original task goals. To mitigate these issues, the authors proposed an implicit Retrieval-Augmented Generation (RAG) technique that generates task-relevant summaries, enabling some improvement. Despite this, fundamental limitations remain, indicating the need for more advanced architectures to support coherent, long-term task execution in WebAgents. The findings highlight critical challenges for practical deployment of LLM-based agents in extended, realistic user interactions and offer insight toward future research directions. <div>
arXiv:2512.04307v1 Announce Type: new 
Abstract: As large language model (LLM)-based agents become increasingly integrated into daily digital interactions, their ability to reason across long interaction histories becomes crucial for providing personalized and contextually aware assistance. However, the performance of these agents in long context scenarios, particularly for action-taking WebAgents operating in realistic web environments, remains largely unexplored. This paper introduces a benchmark for evaluating long context reasoning capabilities of WebAgents through sequentially dependent subtasks that require retrieval and application of information from extended interaction histories. We develop a novel evaluation framework that simulates multi-session user interactions by injecting irrelevant task trajectories between dependent subtasks, creating contexts ranging from 25,000 to 150,000 tokens. Through extensive evaluation of four popular models, Claude-3.7, GPT-4.1, Llama 4, and o4-mini, we observe a dramatic performance degradation as context length increases, with success rates dropping from 40-50\% in baseline conditions to less than 10\% in long context scenarios. Our detailed error analysis reveals that agents primarily fail due to getting stuck in loops and losing track of original task objectives. We further propose an implicit RAG approach that provides modest improvements by generating task-relevant summaries, though fundamental limitations in long context reasoning persist. These findings highlight critical challenges for deploying WebAgents in realistic, long-term user interaction scenarios and provide insights for developing more robust agent architectures capable of maintaining coherent task execution across extended contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RNNs perform task computations by dynamically warping neural representations</title>
<link>https://arxiv.org/abs/2512.04310</link>
<guid>https://arxiv.org/abs/2512.04310</guid>
<content:encoded><![CDATA[
<div> Keywords: recurrent neural networks, representational geometry, dynamical systems, Riemannian geometry, dynamic warping<br /><br />Summary:<br /><br />1. The study focuses on understanding how neural networks, particularly recurrent neural networks (RNNs), represent and process data features through their activations, which is crucial for interpreting their task performance.<br />2. While prior research has extensively analyzed the geometric properties of neural representations, there remains a gap in linking these static geometric characterizations to the dynamic computations performed by neural networks on time-varying inputs.<br />3. The authors propose the hypothesis that RNNs execute computations by dynamically warping their internal representations of task-relevant variables over time.<br />4. To investigate this, they develop a novel Riemannian geometric framework that connects the topology and geometry of the neural system’s manifold to that of its input manifold, allowing characterization of the time-dependent geometric transformations within RNNs.<br />5. Empirical results demonstrate that dynamic warping of neural representations is an inherent and fundamental aspect of how RNNs perform computations, thereby providing a deeper theoretical understanding that bridges representational geometry and dynamical system-based computation in machine learning models. <div>
arXiv:2512.04310v1 Announce Type: new 
Abstract: Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such "neural representations." In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-regularized Reinforcement Learning for Diffusion Models at Scale</title>
<link>https://arxiv.org/abs/2512.04332</link>
<guid>https://arxiv.org/abs/2512.04332</guid>
<content:encoded><![CDATA[
<div> Diffusion models, reinforcement learning, reward hacking, KL divergence, video generation  

<br /><br />Summary:  
This paper addresses the challenge of aligning generative diffusion models with human preferences using reinforcement learning (RL), a process often undermined by reward hacking issues such as quality degradation, over-stylization, and reduced diversity. The authors identify that these problems stem from the limitations of existing regularization methods, which provide unreliable penalties during training. To overcome this, they propose Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that employs forward KL divergence to anchor the RL policy to an off-policy data distribution, ensuring robust and unbiased integration of RL with standard diffusion training. DDRL effectively combines reward maximization with diffusion loss minimization, leading to a simple and efficient algorithm. The approach was extensively validated with over a million GPU hours and thousands of double-blind human evaluations, focusing on high-resolution video generation tasks. Results show that DDRL significantly enhances reward performance while mitigating reward hacking behaviors present in baseline models. Ultimately, DDRL achieves the highest human preference scores, establishing a robust and scalable paradigm for diffusion model post-training aligned with human preferences. <div>
arXiv:2512.04332v1 Announce Type: new 
Abstract: Aligning generative diffusion models with human preferences via reinforcement learning (RL) is critical yet challenging. Most existing algorithms are often vulnerable to reward hacking, such as quality degradation, over-stylization, or reduced diversity. Our analysis demonstrates that this can be attributed to the inherent limitations of their regularization, which provides unreliable penalties. We introduce Data-regularized Diffusion Reinforcement Learning (DDRL), a novel framework that uses the forward KL divergence to anchor the policy to an off-policy data distribution. Theoretically, DDRL enables robust, unbiased integration of RL with standard diffusion training. Empirically, this translates into a simple yet effective algorithm that combines reward maximization with diffusion loss minimization. With over a million GPU hours of experiments and ten thousand double-blind human evaluations, we demonstrate on high-resolution video generation tasks that DDRL significantly improves rewards while alleviating the reward hacking seen in baselines, achieving the highest human preference and establishing a robust and scalable paradigm for diffusion post-training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RGE-GCN: Recursive Gene Elimination with Graph Convolutional Networks for RNA-seq based Early Cancer Detection</title>
<link>https://arxiv.org/abs/2512.04333</link>
<guid>https://arxiv.org/abs/2512.04333</guid>
<content:encoded><![CDATA[
<div> Keywords: cancer detection, RNA-seq, Graph Convolutional Networks, feature selection, biomarkers  

<br /><br />Summary:  
1. Early cancer detection is crucial for improving survival, but identifying reliable RNA-seq biomarkers remains challenging due to high-dimensional data and complex gene interactions.  
2. The authors propose RGE-GCN, a novel framework that integrates feature selection and classification via constructing a gene expression graph and applying Graph Convolutional Networks (GCN).  
3. RGE-GCN employs Integrated Gradients to quantify gene importance and recursively eliminates less informative genes, resulting in a compact, interpretable biomarker set.  
4. The method was tested on both synthetic datasets and real RNA-seq cohorts from lung, kidney, and cervical cancers, outperforming established tools like DESeq2, edgeR, and limma-voom in accuracy and F1-score.  
5. The selected biomarkers correspond well with known cancer-related pathways such as PI3K-AKT, MAPK, SUMOylation, and immune regulation, supporting the biological relevance of the approach.  
6. Overall, RGE-GCN demonstrates potential as a generalizable and effective tool for early cancer detection and biomarker discovery from RNA-seq data. <div>
arXiv:2512.04333v1 Announce Type: new 
Abstract: Early detection of cancer plays a key role in improving survival rates, but identifying reliable biomarkers from RNA-seq data is still a major challenge. The data are high-dimensional, and conventional statistical methods often fail to capture the complex relationships between genes. In this study, we introduce RGE-GCN (Recursive Gene Elimination with Graph Convolutional Networks), a framework that combines feature selection and classification in a single pipeline. Our approach builds a graph from gene expression profiles, uses a Graph Convolutional Network to classify cancer versus normal samples, and applies Integrated Gradients to highlight the most informative genes. By recursively removing less relevant genes, the model converges to a compact set of biomarkers that are both interpretable and predictive. We evaluated RGE-GCN on synthetic data as well as real-world RNA-seq cohorts of lung, kidney, and cervical cancers. Across all datasets, the method consistently achieved higher accuracy and F1-scores than standard tools such as DESeq2, edgeR, and limma-voom. Importantly, the selected genes aligned with well-known cancer pathways including PI3K-AKT, MAPK, SUMOylation, and immune regulation. These results suggest that RGE-GCN shows promise as a generalizable approach for RNA-seq based early cancer detection and biomarker discovery (https://rce-gcn.streamlit.app/ ).
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Long-Horizon Model-Based Offline Reinforcement Learning Without Conservatism</title>
<link>https://arxiv.org/abs/2512.04341</link>
<guid>https://arxiv.org/abs/2512.04341</guid>
<content:encoded><![CDATA[
<div> Bayesian RL, offline reinforcement learning, conservatism, model-based RL, planning horizons<br /><br />Summary:<br /><br />This work challenges the prevailing principle of conservatism in offline reinforcement learning (RL), which typically penalizes out-of-dataset actions or limits planning horizons to reduce errors. Instead, the authors propose a Bayesian approach that models a posterior distribution over plausible world models to explicitly capture epistemic uncertainty inherent in offline data. By training a history-dependent agent to maximize expected rewards under this framework, the method enables improved generalization at test time. The approach, named Neubay, is first demonstrated in a bandit setting, where it outperforms conservative methods on low-quality datasets that traditionally cause failure. Scaling up to complex, realistic tasks, the study identifies important design choices—such as layer normalization in the world model and adaptive long-horizon planning—that reduce compounding errors and value overestimation. Neubay matches or exceeds the performance of state-of-the-art conservative algorithms on D4RL and NeoRL benchmarks, achieving top results on seven datasets. A significant finding is Neubay’s ability to succeed with planning horizons extending to several hundred steps, contradicting the common belief that offline RL requires short horizons. Finally, the paper delineates scenarios where Neubay’s Bayesian approach is preferable, providing a foundation for advancing offline and model-based RL beyond conservative methods. <div>
arXiv:2512.04341v1 Announce Type: new 
Abstract: Popular offline reinforcement learning (RL) methods rely on conservatism, either by penalizing out-of-dataset actions or by restricting planning horizons. In this work, we question the universality of this principle and instead revisit a complementary one: a Bayesian perspective. Rather than enforcing conservatism, the Bayesian approach tackles epistemic uncertainty in offline data by modeling a posterior distribution over plausible world models and training a history-dependent agent to maximize expected rewards, enabling test-time generalization. We first illustrate, in a bandit setting, that Bayesianism excels on low-quality datasets where conservatism fails. We then scale the principle to realistic tasks, identifying key design choices, such as layer normalization in the world model and adaptive long-horizon planning, that mitigate compounding error and value overestimation. These yield our practical algorithm, Neubay, grounded in the neutral Bayesian principle. On D4RL and NeoRL benchmarks, Neubay generally matches or surpasses leading conservative algorithms, achieving new state-of-the-art on 7 datasets. Notably, it succeeds with planning horizons of several hundred steps, challenging common belief. Finally, we characterize when Neubay is preferable to conservatism, laying the foundation for a new direction in offline and model-based RL.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distance Is All You Need: Radial Dispersion for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2512.04351</link>
<guid>https://arxiv.org/abs/2512.04351</guid>
<content:encoded><![CDATA[
<div> Keywords: Radial Dispersion Score, uncertainty detection, large language models, hallucination detection, embedding space<br /><br />Summary:<br /><br />1. The paper introduces the Radial Dispersion Score (RDS), a novel uncertainty metric designed for large language models (LLMs) to detect uncertainty in their outputs reliably. <br />2. Unlike existing methods that depend on complex semantic clustering or internal model states, RDS is simple, parameter-free, and fully model-agnostic, relying on the radial dispersion of sampled generations within an embedding space. <br />3. The authors propose a probability-weighted variant of RDS that integrates the LLM’s token probability distributions, which further enhances performance compared to nine strong baseline methods. <br />4. RDS can be applied at the per-sample level, enabling practical applications like best-of-N selection and confidence-based filtering of model outputs. <br />5. Experimental results across four challenging free-form question-answering datasets and multiple LLMs demonstrate that RDS achieves state-of-the-art performance in hallucination detection and answer selection tasks, while maintaining robustness and scalability with respect to sample size and choices of embeddings. <div>
arXiv:2512.04351v1 Announce Type: new 
Abstract: Detecting when large language models (LLMs) are uncertain is critical for building reliable systems, yet existing methods are overly complicated, relying on brittle semantic clustering or internal states. We introduce \textbf{Radial Dispersion Score (RDS)}, a simple, parameter-free, fully model-agnostic uncertainty metric that measures the radial dispersion of sampled generations in embedding space. A lightweight probability-weighted variant further incorporates the model's own token probabilities when available, outperforming different nine strong baselines. Moroever, RDS naturally extends to per-sample scoring, enabling applications such as best-of-$N$ selection and confidence-based filtering. Across four challenging free-form QA datasets and multiple LLMs, our metrics achieve state-of-the-art hallucination detection and answer selection performance, while remaining robust and scalable with respect to sample size and embedding choice.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SmartAlert: Implementing Machine Learning-Driven Clinical Decision Support for Inpatient Lab Utilization Reduction</title>
<link>https://arxiv.org/abs/2512.04354</link>
<guid>https://arxiv.org/abs/2512.04354</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, clinical decision support, laboratory testing, repetitive testing reduction, healthcare implementation<br /><br />Summary: This study presents SmartAlert, a machine learning-driven clinical decision support (CDS) system embedded in electronic health records to predict stable laboratory results and reduce unnecessary repeat complete blood count (CBC) testing. Implemented in a randomized controlled pilot across 9270 admissions in eight acute care units at two hospitals from August 2024 to March 2025, SmartAlert demonstrated a significant 15% relative reduction in repetitive CBC testing, with the average number of CBC results within 52 hours decreasing from 1.82 to 1.54 (p <0.01). The intervention achieved this reduction without adverse impacts on secondary safety outcomes, highlighting its safety and efficacy. Key challenges during deployment included interpreting probabilistic model outputs in clinical contexts, aligning stakeholder expectations to define acceptable model behavior, and establishing governance processes for a complex model in a clinical environment. Additionally, the study identified critical user interface design considerations and the importance of aligning the system with clinical operational priorities. Gathering qualitative feedback from end users proved valuable in refining the system. The findings conclude that an ML-driven CDS system, supported by thoughtful implementation and governance, can provide precise, context-aware guidance to safely reduce unnecessary repetitive inpatient laboratory testing, potentially curbing patient burden and healthcare costs. <div>
arXiv:2512.04354v1 Announce Type: new 
Abstract: Repetitive laboratory testing unlikely to yield clinically useful information is a common practice that burdens patients and increases healthcare costs. Education and feedback interventions have limited success, while general test ordering restrictions and electronic alerts impede appropriate clinical care. We introduce and evaluate SmartAlert, a machine learning (ML)-driven clinical decision support (CDS) system integrated into the electronic health record that predicts stable laboratory results to reduce unnecessary repeat testing. This case study describes the implementation process, challenges, and lessons learned from deploying SmartAlert targeting complete blood count (CBC) utilization in a randomized controlled pilot across 9270 admissions in eight acute care units across two hospitals between August 15, 2024, and March 15, 2025. Results show significant decrease in number of CBC results within 52 hours of SmartAlert display (1.54 vs 1.82, p <0.01) without adverse effect on secondary safety outcomes, representing a 15% relative reduction in repetitive testing. Implementation lessons learned include interpretation of probabilistic model predictions in clinical contexts, stakeholder engagement to define acceptable model behavior, governance processes for deploying a complex model in a clinical environment, user interface design considerations, alignment with clinical operational priorities, and the value of qualitative feedback from end users. In conclusion, a machine learning-driven CDS system backed by a deliberate implementation and governance process can provide precision guidance on inpatient laboratory testing to safely reduce unnecessary repetitive testing.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STeP-Diff: Spatio-Temporal Physics-Informed Diffusion Models for Mobile Fine-Grained Pollution Forecasting</title>
<link>https://arxiv.org/abs/2512.04385</link>
<guid>https://arxiv.org/abs/2512.04385</guid>
<content:encoded><![CDATA[
<div> Keywords: Air Pollution Forecasting, Spatio-Temporal Modeling, Diffusion Models, PDE-Constrained Regularization, Portable Sensors  

<br /><br />Summary:  
1. Fine-grained air pollution forecasting is essential for effective urban management and promoting healthy indoor environments.  
2. The study introduces STeP-Diff, a novel Spatio-Temporal Physics-Informed Diffusion Model designed to handle incomplete and temporally inconsistent data gathered from portable sensors mounted on mobile platforms such as cars and buses.  
3. STeP-Diff integrates DeepONet to model spatial measurement sequences and incorporates a PDE-informed diffusion process, ensuring predictions follow real-world convection-diffusion physics that govern pollution dispersion.  
4. The model uses a PDE-constrained regularization framework where the diffusion denoising process converges asymptotically to the convection-diffusion dynamics, grounding forecasts in fundamental physical laws.  
5. Experimental validation involved deploying 59 custom portable sensors across two cities over 14 days, collecting extensive air pollution data under real-world conditions.  
6. Performance evaluation showed that STeP-Diff substantially outperforms competing algorithms, improving Mean Absolute Error (MAE) by up to 89.12%, Root Mean Square Error (RMSE) by 82.30%, and Mean Absolute Percentage Error (MAPE) by 25.00%.  
7. These results demonstrate STeP-Diff’s strong ability to capture complex spatio-temporal dependencies in pollution fields, making it a promising approach for accurate, physics-informed air quality forecasting. <div>
arXiv:2512.04385v1 Announce Type: new 
Abstract: Fine-grained air pollution forecasting is crucial for urban management and the development of healthy buildings. Deploying portable sensors on mobile platforms such as cars and buses offers a low-cost, easy-to-maintain, and wide-coverage data collection solution. However, due to the random and uncontrollable movement patterns of these non-dedicated mobile platforms, the resulting sensor data are often incomplete and temporally inconsistent. By exploring potential training patterns in the reverse process of diffusion models, we propose Spatio-Temporal Physics-Informed Diffusion Models (STeP-Diff). STeP-Diff leverages DeepONet to model the spatial sequence of measurements along with a PDE-informed diffusion model to forecast the spatio-temporal field from incomplete and time-varying data. Through a PDE-constrained regularization framework, the denoising process asymptotically converges to the convection-diffusion dynamics, ensuring that predictions are both grounded in real-world measurements and aligned with the fundamental physics governing pollution dispersion. To assess the performance of the system, we deployed 59 self-designed portable sensing devices in two cities, operating for 14 days to collect air pollution data. Compared to the second-best performing algorithm, our model achieved improvements of up to 89.12% in MAE, 82.30% in RMSE, and 25.00% in MAPE, with extensive evaluations demonstrating that STeP-Diff effectively captures the spatio-temporal dependencies in air pollution fields.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Orchestrate Agents in Natural Language with the Conductor</title>
<link>https://arxiv.org/abs/2512.04388</link>
<guid>https://arxiv.org/abs/2512.04388</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Coordination Strategies, Conductor Model, Reasoning Benchmarks<br /><br />Summary:<br /><br />This paper introduces a novel Conductor model trained using reinforcement learning to optimize coordination among multiple large language models (LLMs) from different providers. The Conductor is capable of designing targeted communication topologies that enable effective collaboration between individual LLM agents. Additionally, it learns to engineer precise prompts tailored to each LLM's strengths to maximize their combined potential. Experimental results demonstrate that a 7-billion-parameter Conductor surpasses the performance of any single worker LLM, achieving state-of-the-art outcomes on complex reasoning benchmarks such as LiveCodeBench and GPQA. The training with randomized pools of agent LLMs allows the Conductor to adapt dynamically to a wide variety of open- and closed-source LLM sets, thus meeting diverse user requirements. A novel feature includes the Conductor selecting itself as a worker, enabling recursive coordination topologies that lead to iterative, dynamic performance improvements during test time. This approach showcases one of the earliest instances where effective language model coordination emerges naturally through pure end-to-end reward maximization via reinforcement learning, highlighting a promising direction for multi-agent LLM systems. <div>
arXiv:2512.04388v1 Announce Type: new 
Abstract: Powerful large language models (LLMs) from different providers have been expensively trained and finetuned to specialize across varying domains. In this work, we introduce a new kind of Conductor model trained with reinforcement learning to automatically discover powerful coordination strategies among LLMs. Our Conductor learns not only to design targeted communication topologies for effective agent-to-agent collaboration, but also to prompt engineer focused instructions to the LLMs to maximally leverage their individual capabilities. We show that, by learning optimal coordination strategies over pools of powerful worker LLMs, a 7B Conductor achieves significant performance gains beyond any individual worker, attaining state-of-the-art results in challenging reasoning benchmarks, such as LiveCodeBench and GPQA. By training with randomized agent pools, our conductor effectively adapts to arbitrary sets of open- and closed-source agents, meeting any user requirements. Furthermore, allowing the Conductor to select itself as a worker gives rise to recursive topologies, elevating performance with a new form of dynamic test-time scaling through online iterative adaptation. More broadly, ours is among the early work demonstrating language model coordination can be unlocked through RL, where powerful coordination strategies emerge naturally in LLMs through pure end-to-end reward maximization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature Engineering vs. Deep Learning for Automated Coin Grading: A Comparative Study on Saint-Gaudens Double Eagles</title>
<link>https://arxiv.org/abs/2512.04464</link>
<guid>https://arxiv.org/abs/2512.04464</guid>
<content:encoded><![CDATA[
<div> deep learning, coin grading, feature-based ANN, EfficientNetV2, data scarcity<br /><br />Summary:<br /><br />1. This study questions the prevailing assumption that deep learning methods always outperform traditional techniques, demonstrated through the automatic grading of Saint-Gaudens Double Eagle gold coins.<br />2. The authors develop a feature-based Artificial Neural Network (ANN) using 192 custom features derived from Sobel edge detection and HSV color analysis, and compare its performance against a hybrid Convolutional Neural Network (CNN) incorporating EfficientNetV2 and a Support Vector Machine (SVM) as a control.<br />3. Evaluating 1,785 expert-graded coins, the feature-based ANN achieved an 86% exact match rate and 98% accuracy within a 3-grade tolerance.<br />4. Conversely, the CNN and SVM methods largely guessed the most common grade, attaining only 31% and 30% exact matches, respectively. Although the CNN showed better results on broader tolerance metrics, this improvement was due to averaging in regression, which masked poor precise grade predictions.<br />5. The findings underscore that with limited and imbalanced datasets, incorporating domain expertise through custom feature design surpasses complex deep learning architectures, a principle that applies to other specialized quality assessments where data is limited and expert knowledge is crucial. <div>
arXiv:2512.04464v1 Announce Type: new 
Abstract: We challenge the common belief that deep learning always trumps older techniques, using the example of grading Saint-Gaudens Double Eagle gold coins automatically. In our work, we put a feature-based Artificial Neural Network built around 192 custom features pulled from Sobel edge detection and HSV color analysis up against a hybrid Convolutional Neural Network that blends in EfficientNetV2, plus a straightforward Support Vector Machine as the control. Testing 1,785 coins graded by experts, the ANN nailed 86% exact matches and hit 98% when allowing a 3-grade leeway. On the flip side, CNN and SVM mostly just guessed the most common grade, scraping by with 31% and 30% exact hits. Sure, the CNN looked good on broader tolerance metrics, but that is because of some averaging trick in regression that hides how it totally flops at picking out specific grades. All told, when you are stuck with under 2,000 examples and lopsided classes, baking in real coin-expert knowledge through feature design beats out those inscrutable, all-in-one deep learning setups. This rings true for other niche quality checks where data's thin and know-how matters more than raw compute.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphBench: Next-generation graph learning benchmarking</title>
<link>https://arxiv.org/abs/2512.04475</link>
<guid>https://arxiv.org/abs/2512.04475</guid>
<content:encoded><![CDATA[
<div> graph neural networks, benchmarking, graph datasets, evaluation protocols, hyperparameter tuning<br /><br />Summary:<br /><br />1. The paper addresses the current challenges in machine learning on graphs, where benchmarking efforts are fragmented and often limited to narrow, task-specific datasets with inconsistent evaluation methods.<br />2. To solve this, the authors introduce GraphBench, a comprehensive benchmarking suite designed to cover a wide range of domains and prediction tasks including node-level, edge-level, graph-level, and generative problems.<br />3. GraphBench standardizes evaluation protocols by providing consistent dataset splits and performance metrics that consider out-of-distribution generalization, improving reproducibility and comparability across different models.<br />4. It also incorporates a unified hyperparameter tuning framework, ensuring fair and comparable model optimization.<br />5. The paper benchmarks GraphBench using popular graph neural network architectures such as message-passing neural networks and graph transformers, offering principled baseline results and establishing reference performance levels for future research.<br /><br />Further details and resources are accessible at www.graphbench.io. <div>
arXiv:2512.04475v1 Announce Type: new 
Abstract: Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Context-Aware Mixture-of-Experts Inference on CXL-Enabled GPU-NDP Systems</title>
<link>https://arxiv.org/abs/2512.04476</link>
<guid>https://arxiv.org/abs/2512.04476</guid>
<content:encoded><![CDATA[
<div> Mixture-of-Experts, Near-Data Processing, CXL, Mixed-Precision Quantization, GPU Memory<br /><br />Summary:<br /><br />This paper addresses the challenge of memory bottlenecks during inference of large Mixture-of-Experts (MoE) language models when expert weights exceed GPU memory capacity. Traditional methods require offloading weights to external memory, causing expensive and repetitive data transfers. The authors propose the use of Compute Express Link (CXL)-attached near-data processing (NDP) as an offloading tier to execute cold experts directly in place, converting costly parameter movement into cheaper activation movement. Unlike previous GPU-NDP systems which are reactive and context-agnostic, the newly developed system is context-aware, leveraging prefill-stage activation statistics to inform expert placement during decoding. This enables dynamic pinning of frequently used ("hot") experts in GPU high-bandwidth memory (HBM), while less active experts are mapped to CXL-NDP. To address the limited compute throughput of NDP, the study introduces context-aware mixed-precision quantization that assigns bitwidths from 1 to 4 bits per expert based on prefill data. The resulting inference system effectively overlaps GPU and NDP execution and minimizes cross-device data movement. Evaluation demonstrates an up to 8.7× improvement in decoding throughput compared to the state-of-the-art, with only a 0.13% average accuracy loss. <div>
arXiv:2512.04476v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) models scale large language models through conditional computation, but inference becomes memory-bound once expert weights exceed the capacity of GPU memory. In this case, weights must be offloaded to external memory, and fetching them incurs costly and repeated transfers. We address this by adopting CXL-attached near-data processing (CXL-NDP) as the offloading tier to execute cold experts in place, converting expensive parameter movement into cheaper activation movement. Unlike prior GPU-NDP systems that are largely context-agnostic and reactive, we develop a context-aware MoE system that uses prefill-stage activation statistics to guide decoding-stage expert placement, dynamically pins hot experts in GPU-side HBM, and maps the remainder to CXL-NDP. To meet NDP's limited compute throughput, we introduce context-aware mixed-precision quantization that allocates per-expert bitwidths (1-4 bit) based on prefill stage. The resulting MoE inference system overlaps GPU and NDP execution while minimizing cross-device movement. The evaluation on the GPU-NDP system shows that our approach achieves up to an 8.7-fold decoding throughput improvement over the state-of-the-art method, while incurring only a 0.13% average accuracy drop.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prototype-Based Semantic Consistency Alignment for Domain Adaptive Retrieval</title>
<link>https://arxiv.org/abs/2512.04524</link>
<guid>https://arxiv.org/abs/2512.04524</guid>
<content:encoded><![CDATA[
<div> Keywords: domain adaptive retrieval, semantic consistency, prototype learning, pseudo-label confidence, hash coding<br /><br />Summary: Domain adaptive retrieval is designed to transfer knowledge from a labeled source domain to an unlabeled target domain to facilitate effective retrieval while addressing domain discrepancies. Existing approaches, however, suffer from critical drawbacks including neglecting class-level semantic alignment in favor of pair-wise sample alignment, inadequate consideration of pseudo-label reliability or geometric guidance for label correctness assessment, and directly quantizing original features influenced by domain shifts which degrade hash code quality. To overcome these limitations, the authors propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework. In the first stage, orthogonal prototypes create class-level semantic connections by maximizing inter-class separability and aggregating intra-class samples. Geometric proximity acts as a reliability indicator to weight pseudo-label confidences adaptively during semantic consistency alignment. The membership matrix and learned prototypes support feature reconstruction, enabling quantization on these reconstructed features rather than on original, domain-shifted ones to enhance hash code quality. In the second stage, domain-specific quantization functions process reconstructed features under mutual approximation constraints, producing unified binary hash codes consistent across domains. Extensive experiments on multiple datasets demonstrate PSCA's superior retrieval performance, validating its effectiveness in bridging domain gaps while preserving semantic structure for adaptive retrieval tasks. <div>
arXiv:2512.04524v1 Announce Type: new 
Abstract: Domain adaptive retrieval aims to transfer knowledge from a labeled source domain to an unlabeled target domain, enabling effective retrieval while mitigating domain discrepancies. However, existing methods encounter several fundamental limitations: 1) neglecting class-level semantic alignment and excessively pursuing pair-wise sample alignment; 2) lacking either pseudo-label reliability consideration or geometric guidance for assessing label correctness; 3) directly quantizing original features affected by domain shift, undermining the quality of learned hash codes. In view of these limitations, we propose Prototype-Based Semantic Consistency Alignment (PSCA), a two-stage framework for effective domain adaptive retrieval. In the first stage, a set of orthogonal prototypes directly establishes class-level semantic connections, maximizing inter-class separability while gathering intra-class samples. During the prototype learning, geometric proximity provides a reliability indicator for semantic consistency alignment through adaptive weighting of pseudo-label confidences. The resulting membership matrix and prototypes facilitate feature reconstruction, ensuring quantization on reconstructed rather than original features, thereby improving subsequent hash coding quality and seamlessly connecting both stages. In the second stage, domain-specific quantization functions process the reconstructed features under mutual approximation constraints, generating unified binary hash codes across domains. Extensive experiments validate PSCA's superior performance across multiple datasets.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Explainable Graph Representation Learning via Graph Pattern Analysis</title>
<link>https://arxiv.org/abs/2512.04530</link>
<guid>https://arxiv.org/abs/2512.04530</guid>
<content:encoded><![CDATA[
<div> Keywords: explainable artificial intelligence, graph representation learning, graph patterns, substructure sampling, PXGL-GNN<br /><br />Summary:  
This paper addresses the challenge of explainability in graph representation learning, a less explored aspect compared to model-level and instance-level explainability in graph learning. The authors raise a fundamental question about the specific graph information captured in learned graph representations. Inspired by graph kernels that measure graph similarity via substructure counts, the study proposes a novel framework called PXGL-GNN to create interpretable graph representations by analyzing graph patterns. The framework begins by sampling various graph substructures, learning representations for these patterns, and combining them with weighted sums where weights reflect each pattern’s importance. PXGL-GNN overcomes limitations of traditional pattern counting vectors—such as ignoring node features and high dimensionality—by integrating learned representations. The authors provide theoretical analyses of their approach, covering robustness and generalization capabilities. Empirical evaluations on real-world datasets demonstrate how the framework can learn and explain graph representations effectively through pattern analysis. Moreover, they benchmark PXGL-GNN against several baseline methods in both supervised and unsupervised learning settings, showing superior performance and interpretability. This work advances explainable graph representation learning by linking explainability to fundamental graph substructures and providing a practical, theoretically grounded method for analysis and explanation. <div>
arXiv:2512.04530v1 Announce Type: new 
Abstract: Explainable artificial intelligence (XAI) is an important area in the AI community, and interpretability is crucial for building robust and trustworthy AI models. While previous work has explored model-level and instance-level explainable graph learning, there has been limited investigation into explainable graph representation learning. In this paper, we focus on representation-level explainable graph learning and ask a fundamental question: What specific information about a graph is captured in graph representations? Our approach is inspired by graph kernels, which evaluate graph similarities by counting substructures within specific graph patterns. Although the pattern counting vector can serve as an explainable representation, it has limitations such as ignoring node features and being high-dimensional. To address these limitations, we introduce a framework (PXGL-GNN) for learning and explaining graph representations through graph pattern analysis. We start by sampling graph substructures of various patterns. Then, we learn the representations of these patterns and combine them using a weighted sum, where the weights indicate the importance of each graph pattern's contribution. We also provide theoretical analyses of our methods, including robustness and generalization. In our experiments, we show how to learn and explain graph representations for real-world data using pattern analysis. Additionally, we compare our method against multiple baselines in both supervised and unsupervised learning tasks to demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Limits of Test-Time Compute: Sequential Reward Filtering for Better Inference</title>
<link>https://arxiv.org/abs/2512.04558</link>
<guid>https://arxiv.org/abs/2512.04558</guid>
<content:encoded><![CDATA[
<div> Test-time compute, best-of-n sampling, reward-filtered sequential inference, large language models, sequential revision<br /><br />Summary:<br /><br />This paper investigates the limitations of test-time compute (TTC) strategies used to enhance large language models (LLMs), with a particular focus on best-of-n (BoN) sampling and sequential revision methods. The authors establish that the commonly adopted BoN approach is fundamentally suboptimal by analyzing a mixture-of-reference policy model. To improve upon this, they propose and study reward-filtered sequential inference, a straightforward yet effective procedure that selectively includes only high-reward output generations into the ongoing context for subsequent inference steps. This selective mechanism effectively concentrates computational resources on superior policy candidates while suppressing less optimal ones, thereby improving model output quality. Theoretically, the paper demonstrates that reward-filtered sequential inference provides strictly stronger guarantees compared to traditional TTC paradigms. Empirically, the authors evaluate this approach across various benchmarks and confirm consistent performance gains over widely-used inference methods. Overall, the study contributes both theoretical insights and practical algorithms that advance the state of the art in adaptive inference and optimization of LLMs under compute constraints. <div>
arXiv:2512.04558v1 Announce Type: new 
Abstract: Test-time compute (TTC) has become an increasingly prominent paradigm for enhancing large language models (LLMs). Despite the empirical success of methods such as best-of-$n$ (BoN) sampling and sequential revision, their fundamental limits remain unclear. We address this gap by analyzing a mixture-of-reference policy model and proving that standard BoN is inherently suboptimal. To move closer to the optimal frontier, we study reward-filtered sequential inference, a simple procedure that selectively incorporates only high-reward generations into the context. This mechanism concentrates computation on superior policy candidates and suppresses inferior ones. On the theoretical side, we show that reward-filtered sequential inference yields strictly stronger guarantees than standard TTC paradigms. On the empirical side, we evaluate such an inference strategy across diverse benchmarks and observe consistent improvements over widely used approaches, demonstrating the practical effectiveness of our framework.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diffusion Fine-Tuning via Reparameterized Policy Gradient of the Soft Q-Function</title>
<link>https://arxiv.org/abs/2512.04559</link>
<guid>https://arxiv.org/abs/2512.04559</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Fine-tuning, Soft Q-function, Reward over-optimization, Text-to-image alignment<br /><br />Summary:<br /><br />This paper addresses the challenge of aligning diffusion models with downstream objectives without sacrificing sample naturalness and diversity. Current fine-tuning approaches often lead to reward over-optimization, generating unnatural samples and reducing diversity. To overcome this, the authors propose Soft Q-based Diffusion Finetuning (SQDF), a novel reinforcement learning method that is KL-regularized and uses a reparameterized policy gradient based on a training-free, differentiable estimation of the soft Q-function. SQDF introduces three key innovations: incorporating a discount factor to ensure proper credit assignment during the denoising steps of diffusion, integrating consistency models to improve the accuracy of Q-function estimates, and employing an off-policy replay buffer to better manage the trade-off between reward maximization and mode coverage. Experimental results demonstrate that SQDF achieves higher target rewards while maintaining sample diversity in text-to-image generation tasks. Additionally, SQDF proves efficient in online black-box optimization scenarios, yielding natural and diverse outputs with fewer samples. Overall, SQDF presents a significant advancement in diffusion model alignment by balancing the competing goals of reward optimization and sample quality. <div>
arXiv:2512.04559v1 Announce Type: new 
Abstract: Diffusion models excel at generating high-likelihood samples but often require alignment with downstream objectives. Existing fine-tuning methods for diffusion models significantly suffer from reward over-optimization, resulting in high-reward but unnatural samples and degraded diversity. To mitigate over-optimization, we propose \textbf{Soft Q-based Diffusion Finetuning (SQDF)}, a novel KL-regularized RL method for diffusion alignment that applies a reparameterized policy gradient of a training-free, differentiable estimation of the soft Q-function. SQDF is further enhanced with three innovations: a discount factor for proper credit assignment in the denoising process, the integration of consistency models to refine Q-function estimates, and the use of an off-policy replay buffer to improve mode coverage and manage the reward-diversity trade-off. Our experiments demonstrate that SQDF achieves superior target rewards while preserving diversity in text-to-image alignment. Furthermore, in online black-box optimization, SQDF attains high sample efficiency while maintaining naturalness and diversity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LeMat-GenBench: A Unified Evaluation Framework for Crystal Generative Models</title>
<link>https://arxiv.org/abs/2512.04562</link>
<guid>https://arxiv.org/abs/2512.04562</guid>
<content:encoded><![CDATA[
<div> Keywords: generative machine learning, crystalline materials, benchmark, evaluation metrics, materials discovery<br /><br />Summary:<br /><br />This work introduces LeMat-GenBench, a unified benchmark designed specifically for generative machine learning models focused on inorganic crystalline materials. The benchmark addresses the critical need for standardized evaluation frameworks that enable meaningful assessment, comparison, and development of generative models in materials science. LeMat-GenBench provides a comprehensive set of evaluation metrics aimed at capturing key performance aspects such as stability, novelty, and diversity of generated crystal structures, which are essential for guiding model improvement and practical applications. The authors have made their benchmark openly accessible by releasing an open-source evaluation suite alongside a public leaderboard hosted on Hugging Face, promoting transparency and broad adoption. In an extensive study, they benchmarked 12 recent generative models, revealing a general trade-off where improvements in structural stability tend to reduce novelty and diversity. Notably, no single model outperformed others across all evaluated dimensions, highlighting the complexity of the task and the need for balanced model design. Overall, LeMat-GenBench establishes a reproducible and extensible foundation for fair and consistent evaluation, aiming to accelerate and guide the development of more reliable, discovery-focused generative models that can advance the inverse design of crystalline materials. <div>
arXiv:2512.04562v1 Announce Type: new 
Abstract: Generative machine learning (ML) models hold great promise for accelerating materials discovery through the inverse design of inorganic crystals, enabling an unprecedented exploration of chemical space. Yet, the lack of standardized evaluation frameworks makes it challenging to evaluate, compare, and further develop these ML models meaningfully. In this work, we introduce LeMat-GenBench, a unified benchmark for generative models of crystalline materials, supported by a set of evaluation metrics designed to better inform model development and downstream applications. We release both an open-source evaluation suite and a public leaderboard on Hugging Face, and benchmark 12 recent generative models. Results reveal that an increase in stability leads to a decrease in novelty and diversity on average, with no model excelling across all dimensions. Altogether, LeMat-GenBench establishes a reproducible and extensible foundation for fair model comparison and aims to guide the development of more reliable, discovery-oriented generative models for crystalline materials.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reliable Statistical Guarantees for Conformal Predictors with Small Datasets</title>
<link>https://arxiv.org/abs/2512.04566</link>
<guid>https://arxiv.org/abs/2512.04566</guid>
<content:encoded><![CDATA[
<div> Surrogate models, uncertainty quantification, conformal prediction, coverage guarantees, small calibration sets<br /><br />Summary:<br /><br />1. The paper addresses the challenge of reliable uncertainty quantification (UQ) for surrogate models like deep neural networks used in science and engineering, especially in safety-critical contexts.<br /><br />2. It focuses on conformal prediction (CP), a data-agnostic method offering statistical guarantees for UQ that do not assume specific error distributions, but notes CP's classical marginal coverage guarantee is less reliable for small calibration datasets.<br /><br />3. Small calibration sets frequently occur in practical surrogate modeling scenarios where local error quantification is necessary, leading to large variability in coverage and often suboptimal reliability.<br /><br />4. The authors propose a novel statistical guarantee framework that provides probabilistic information about the coverage of a single conformal predictor, improving reliability especially for small calibration set sizes.<br /><br />5. This new framework converges to the classical CP guarantees as calibration size grows, ensuring applicability across data scenarios.<br /><br />6. Validation is demonstrated with several examples, and the authors provide open-access software compatible with existing CP libraries, facilitating adoption of their improved uncertainty models.<br /><br />7. The work bridges the gap between theoretical CP guarantees and practical surrogate model uncertainty estimation, enhancing trustworthiness in safety-critical applications. <div>
arXiv:2512.04566v1 Announce Type: new 
Abstract: Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the reliability of the uncertainty model, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers reliable information about the coverage of a conformal predictor for small data sizes. We illustrate and validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temp-SCONE: A Novel Out-of-Distribution Detection and Domain Generalization Framework for Wild Data with Temporal Shift</title>
<link>https://arxiv.org/abs/2512.04571</link>
<guid>https://arxiv.org/abs/2512.04571</guid>
<content:encoded><![CDATA[
<div> Open-world learning, out-of-distribution detection, temporal shifts, confidence regularization, dynamic environments  

<br /><br />Summary:  
Open-world learning (OWL) demands models that adapt to changing environments while accurately identifying out-of-distribution (OOD) inputs. Existing methods like SCONE provide robustness against covariate and semantic shifts but assume a static environment, reducing effectiveness in dynamic settings. This paper introduces Temp-SCONE, an enhancement of SCONE that incorporates temporal consistency to manage temporal shifts in evolving environments. Temp-SCONE employs a confidence-driven regularization loss based on Average Thresholded Confidence (ATC) to reduce prediction instability over time while maintaining SCONE’s energy-margin separation. Experimental results on dynamic datasets demonstrate that Temp-SCONE significantly outperforms SCONE, achieving better accuracy on corrupted data and more reliable OOD detection under temporal drift. When tested on datasets lacking temporal continuity, Temp-SCONE sustains comparable performance, emphasizing both the benefits and limitations of temporal regularization. Furthermore, the authors provide theoretical insights into temporal stability and generalization error, supporting Temp-SCONE's effectiveness in improving robustness for OWL models in dynamic, time-evolving environments. This work marks a significant step toward reliable open-world learning in scenarios with continuous temporal shifts. <div>
arXiv:2512.04571v1 Announce Type: new 
Abstract: Open-world learning (OWL) requires models that can adapt to evolving environments while reliably detecting out-of-distribution (OOD) inputs. Existing approaches, such as SCONE, achieve robustness to covariate and semantic shifts but assume static environments, leading to degraded performance in dynamic domains. In this paper, we propose Temp-SCONE, a temporally consistent extension of SCONE designed to handle temporal shifts in dynamic environments. Temp-SCONE introduces a confidence-driven regularization loss based on Average Thresholded Confidence (ATC), penalizing instability in predictions across time steps while preserving SCONE's energy-margin separation. Experiments on dynamic datasets demonstrate that Temp-SCONE significantly improves robustness under temporal drift, yielding higher corrupted-data accuracy and more reliable OOD detection compared to SCONE. On distinct datasets without temporal continuity, Temp-SCONE maintains comparable performance, highlighting the importance and limitations of temporal regularization. Our theoretical insights on temporal stability and generalization error further establish Temp-SCONE as a step toward reliable OWL in evolving dynamic environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting \texttt{ftrace}'s \texttt{function\_graph} Tracer Features for Machine Learning: A Case Study on Encryption Detection</title>
<link>https://arxiv.org/abs/2512.04590</link>
<guid>https://arxiv.org/abs/2512.04590</guid>
<content:encoded><![CDATA[
<div> Keywords: Linux kernel, ftrace, machine learning, function graph tracer, encryption detection

<br /><br />Summary:  
This paper introduces the use of the Linux kernel’s ftrace framework, with a focus on the function graph tracer, to produce detailed system-level data suitable for machine learning applications. It addresses the challenge of detecting encryption activities within a large file dataset by utilizing function call traces alongside graph-based features. The experimental results demonstrate remarkable performance, achieving an accuracy of 99.28% using several learning algorithms, which highlights the strength of features derived from the function graph tracer. To validate the approach further, the authors conducted an additional multilabel classification experiment that involved identifying running programs solely from trace data, confirming the robustness of the proposed methodology. The work provides thorough preprocessing techniques for raw trace data and outlines methods for extracting graph-based features. The contributions significantly advance the application of machine learning in system behavior analysis, program identification, and anomaly detection. By integrating detailed system tracing with machine learning, this research opens new avenues for enhanced performance monitoring and security analytics, offering practical tools for innovative solutions in these areas. <div>
arXiv:2512.04590v1 Announce Type: new 
Abstract: This paper proposes using the Linux kernel ftrace framework, particularly the function graph tracer, to generate informative system level data for machine learning (ML) applications. Experiments on a real world encryption detection task demonstrate the efficacy of the proposed features across several learning algorithms. The learner faces the problem of detecting encryption activities across a large dataset of files, using function call traces and graph based features. Empirical results highlight an outstanding accuracy of 99.28 on the task at hand, underscoring the efficacy of features derived from the function graph tracer. The results were further validated in an additional experiment targeting a multilabel classification problem, in which running programs were identified from trace data. This work provides comprehensive methodologies for preprocessing raw trace data and extracting graph based features, offering significant advancements in applying ML to system behavior analysis, program identification, and anomaly detection. By bridging the gap between system tracing and ML, this paper paves the way for innovative solutions in performance monitoring and security analytics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QoSDiff: An Implicit Topological Embedding Learning Framework Leveraging Denoising Diffusion and Adversarial Attention for Robust QoS Prediction</title>
<link>https://arxiv.org/abs/2512.04596</link>
<guid>https://arxiv.org/abs/2512.04596</guid>
<content:encoded><![CDATA[
<div> Keywords: QoS prediction, denoising diffusion probabilistic model, embedding learning, adversarial interaction module, graph neural networks<br /><br />Summary:<br /><br />Accurate Quality of Service (QoS) prediction is crucial in service computing for guiding service selection and enhancing user experience. Existing methods, particularly those based on Graph Neural Networks (GNNs), depend on explicit user-service interaction graphs, which cause scalability issues and performance degradation when data is sparse or noisy. To overcome these limitations, the authors propose QoSDiff, an innovative embedding learning framework that eliminates the need for explicit graph construction by employing a denoising diffusion probabilistic model to reconstruct intrinsic latent structures from noisy inputs. Additionally, QoSDiff introduces an adversarial interaction module that uses a bidirectional hybrid attention mechanism to capture high-order user-service interactions effectively. This adversarial approach helps differentiate meaningful patterns from noise, enabling more nuanced modeling of complex associations. Experimental results on two large-scale real-world datasets show that QoSDiff significantly outperforms current state-of-the-art baselines. Furthermore, the framework demonstrates strong cross-dataset generalization and exceptional resilience to challenges posed by data sparsity and observational noise, suggesting its practicality and robustness in diverse service computing environments. <div>
arXiv:2512.04596v1 Announce Type: new 
Abstract: Accurate Quality of Service (QoS) prediction is fundamental to service computing, providing essential data-driven guidance for service selection and ensuring superior user experiences. However, prevalent approaches, particularly Graph Neural Networks (GNNs), heavily rely on constructing explicit user--service interaction graphs. This dependency introduces severe scalability bottlenecks and limits performance when explicit connections are sparse or corrupted by noise. To address these challenges, this paper introduces \emph{QoSDiff}, a novel embedding learning framework that bypasses the prerequisite of explicit graph construction. Specifically, it leverages a denoising diffusion probabilistic model to recover intrinsic latent structures from noisy initializations. To further capture high-order interactions, we propose an adversarial interaction module that integrates a bidirectional hybrid attention mechanism. This adversarial paradigm dynamically distinguishes informative patterns from noise, enabling a dual-perspective modeling of intricate user--service associations. Extensive experiments on two large-scale real-world datasets demonstrate that QoSDiff significantly outperforms state-of-the-art baselines. Notably, the results highlight the framework's superior cross-dataset generalization capability and exceptional robustness against data sparsity and observational noise.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space</title>
<link>https://arxiv.org/abs/2512.04601</link>
<guid>https://arxiv.org/abs/2512.04601</guid>
<content:encoded><![CDATA[
<div> Large Language Models, Actor-Critic, Natural Language, Off-policy Training, Sparse Rewards<br /><br />Summary:<br /><br />This paper addresses the challenges of training large language model (LLM) agents that interact dynamically with environments over long time horizons, especially in complex tasks requiring tool-use, web browsing, or dialogue. Traditional policy gradient methods struggle with sparse rewards and noisy trajectory-level feedback, resulting in unstable and sample-inefficient learning. To overcome these issues, the authors propose Natural Language Actor-Critic (NLAC), a novel algorithm that employs a generative LLM as a critic, which outputs natural language explanations rather than scalar reward values. These explanations provide richer, more actionable feedback, helping LLM policies understand why specific actions are suboptimal and how to improve without relying on random exploration in large, open-ended natural language action spaces. Furthermore, NLAC supports off-policy training, avoiding policy gradients and enabling more stable and data-efficient learning. The approach is evaluated on a diverse set of reasoning, web browsing, tool-use, and dialogue tasks, where it demonstrates improved performance and scalability compared to existing training methods. This work offers a promising new direction for efficiently training LLM agents in complex, long-horizon environments with sparse rewards. <div>
arXiv:2512.04601v1 Announce Type: new 
Abstract: Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score Matching for Estimating Finite Point Processes</title>
<link>https://arxiv.org/abs/2512.04617</link>
<guid>https://arxiv.org/abs/2512.04617</guid>
<content:encoded><![CDATA[
<div> Score matching, point processes, Janossy measures, autoregressive estimator, survival-classification<br /><br />Summary:<br /><br />This paper addresses the limitations of existing score matching estimators for point processes, particularly their application to finite point processes defined on bounded spaces where standard assumptions fail. To overcome this, the authors develop a formal framework for score matching based on Janossy measures, allowing rigorous treatment of finite point processes. Within this framework, they propose an autoregressive weighted score-matching estimator and analyze its statistical properties in classical parametric models. The work further reveals that in general nonparametric settings, such as deep point process models, score matching alone cannot uniquely recover the true distribution due to normalization issues. To remedy this, the authors introduce a simple survival-classification augmentation, providing a complete, integration-free training objective suitable for any intensity-based spatio-temporal point process model. Empirical validation on synthetic and real-world temporal and spatio-temporal datasets demonstrates that the proposed method effectively recovers intensities and achieves performance comparable to maximum likelihood estimation while offering improved computational efficiency. This study advances score matching methodology for point processes by providing theoretical grounding, algorithmic innovations, and practical tools for inference in both parametric and complex nonparametric models. <div>
arXiv:2512.04617v1 Announce Type: new 
Abstract: Score matching estimators have garnered significant attention in recent years because they eliminate the need to compute normalizing constants, thereby mitigating the computational challenges associated with maximum likelihood estimation (MLE).While several studies have proposed score matching estimators for point processes, this work highlights the limitations of these existing methods, which stem primarily from the lack of a mathematically rigorous analysis of how score matching behaves on finite point processes -- special random configurations on bounded spaces where many of the usual assumptions and properties of score matching no longer hold. To this end, we develop a formal framework for score matching on finite point processes via Janossy measures and, within this framework, introduce an (autoregressive) weighted score-matching estimator, whose statistical properties we analyze in classical parametric settings. For general nonparametric (e.g., deep) point process models, we show that score matching alone does not uniquely identify the ground-truth distribution due to subtle normalization issues, and we propose a simple survival-classification augmentation that yields a complete, integration-free training objective for any intensity-based point process model for spatio-temporal case. Experiments on synthetic and real-world temporal and spatio-temporal datasets, demonstrate that our method accurately recovers intensities and achieves performance comparable to MLE with better efficiency.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Decoupled Knowledge Distillation: A Predictive Distribution Perspective</title>
<link>https://arxiv.org/abs/2512.04625</link>
<guid>https://arxiv.org/abs/2512.04625</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Distillation, Decoupled Knowledge Distillation, Predictive Distribution, Logit Partitioning, Multimodality

<br /><br />Summary:  
This paper revisits Decoupled Knowledge Distillation (DKD) by analyzing its mechanisms from the perspective of the teacher model’s predictive distribution. It proposes an improved version called Generalized Decoupled Knowledge Distillation (GDKD) loss, offering a more flexible approach to decoupling logits. The study highlights two novel insights that influence GDKD’s effectiveness: firstly, partitioning based on the top logit enhances the relationship among non-top logits; secondly, increasing focus on distillation loss from non-top logits improves knowledge transfer within them. Leveraging these findings, the authors introduce a more efficient partition strategy to better manage the multimodal nature of teacher predictive distributions. Comprehensive experiments across diverse datasets—including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes—demonstrate that GDKD consistently outperforms the original DKD method and other state-of-the-art knowledge distillation techniques. The study’s findings deepen the understanding of logit-based knowledge distillation and provide practical algorithms for enhanced teacher-student training. The authors have also made their code publicly accessible to facilitate further research and application. <div>
arXiv:2512.04625v1 Announce Type: new 
Abstract: In the history of knowledge distillation, the focus has once shifted over time from logit-based to feature-based approaches. However, this transition has been revisited with the advent of Decoupled Knowledge Distillation (DKD), which re-emphasizes the importance of logit knowledge through advanced decoupling and weighting strategies. While DKD marks a significant advancement, its underlying mechanisms merit deeper exploration. As a response, we rethink DKD from a predictive distribution perspective. First, we introduce an enhanced version, the Generalized Decoupled Knowledge Distillation (GDKD) loss, which offers a more versatile method for decoupling logits. Then we pay particular attention to the teacher model's predictive distribution and its impact on the gradients of GDKD loss, uncovering two critical insights often overlooked: (1) the partitioning by the top logit considerably improves the interrelationship of non-top logits, and (2) amplifying the focus on the distillation loss of non-top logits enhances the knowledge extraction among them. Utilizing these insights, we further propose a streamlined GDKD algorithm with an efficient partition strategy to handle the multimodality of teacher models' predictive distribution. Our comprehensive experiments conducted on a variety of benchmarks, including CIFAR-100, ImageNet, Tiny-ImageNet, CUB-200-2011, and Cityscapes, demonstrate GDKD's superior performance over both the original DKD and other leading knowledge distillation methods. The code is available at https://github.com/ZaberKo/GDKD.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning for Anomaly Detection in Maritime Movement Data</title>
<link>https://arxiv.org/abs/2512.04635</link>
<guid>https://arxiv.org/abs/2512.04635</guid>
<content:encoded><![CDATA[
<div> Keywords: federated learning, movement anomaly detection, M3fed, maritime AIS data, communication costs<br /><br />Summary:<br /><br />This paper introduces M3fed, a novel federated learning (FL) framework specifically designed for movement anomaly detection models. The proposed method aims to enhance data privacy by enabling learning across distributed data sources without centralized data collection. M3fed also targets a reduction in communication costs, a critical factor in federated learning implementations. The authors detail innovative FL strategies integrated into M3fed to optimize training effectiveness under these constraints. An experimental evaluation is conducted using maritime Automatic Identification System (AIS) data to demonstrate the practical applicability of the approach. Performance comparisons are made between the traditional centralized M3 model and the newly proposed federated M3fed model. The results highlight that M3fed achieves competitive model quality while significantly alleviating communication overheads associated with centralized training setups. This work contributes to the development of privacy-preserving, cost-efficient machine learning solutions in the domain of movement anomaly detection, offering promising directions for further research and real-world deployment. <div>
arXiv:2512.04635v1 Announce Type: new 
Abstract: This paper introduces M3fed, a novel solution for federated learning of movement anomaly detection models. This innovation has the potential to improve data privacy and reduce communication costs in machine learning for movement anomaly detection. We present the novel federated learning (FL) strategies employed to train M3fed, perform an example experiment with maritime AIS data, and evaluate the results with respect to communication costs and FL model quality by comparing classic centralized M3 and the new federated M3fed.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contract-Governed Training for Earth Observation: Observed Service Agreement Graphs and Coverage-Accuracy Trade-offs</title>
<link>https://arxiv.org/abs/2512.04644</link>
<guid>https://arxiv.org/abs/2512.04644</guid>
<content:encoded><![CDATA[
<div> Keywords: Earth observation, service contracts, coverage governance, sampling policy, model accuracy<br /><br />Summary:<br /><br />This paper addresses the challenge of training Earth observation (EO) models under implicit sampling policies that prioritize overall accuracy but lack explicit guarantees on performance across different regions, classes, or mission-critical groups. The authors propose a contract-governed training paradigm where training samples are organized into semantically meaningful service contracts defined by attributes such as dataset, geographic region, or rare-crop indicators. Each contract is assigned a target service share to ensure balanced representation during training. They implement this approach as the Observed Service Agreement Graph (OSAG), a governance framework that monitors contract-level coverage during optimization, adjusts sampling weights to meet target shares, and allows tuning of the trade-off between global accuracy and contract-level coverage through two parameters: the sampling mixture coefficient alpha and the contract-regularization weight lambda_C. The paper provides theoretical analysis in a simplified setting, demonstrating how OSAG sampling effectively aligns empirical coverage with contract targets, bounds the deviation in service risk, and shows that the granularity of contract design impacts governance cost. Empirical validation on AVIRIS hyperspectral datasets (Indian Pines and Salinas) and Sentinel-2 EuroSAT multispectral data reveals that OSAG can significantly reduce coverage errors for prioritized contracts, maintain overall model accuracy, and improve accuracy on high-priority segments. Additionally, an ablation study on EuroSAT contracts shows that finer semantic contracts reduce the accuracy cost associated with governance improvements. <div>
arXiv:2512.04644v1 Announce Type: new 
Abstract: Earth observation (EO) models are frequently trained under implicit sampling policies that optimize global accuracy but provide no explicit guarantees on who (which regions, classes, or mission-critical strata) is being served throughout training. This paper introduces a contract-governed training paradigm for EO in which training samples are grouped into service contracts -- semantically meaningful units such as (dataset, region, rare-crop indicator) -- and each contract is assigned a target service share. We instantiate this paradigm as an Observed Service Agreement Graph (OSAG), a lightweight governance layer that (i) monitors contract-level exposure (coverage) during optimization, (ii) drives empirical coverage toward target shares via contract-normalized sampling weights, and (iii) exposes explicit accuracy-governance trade-offs through two knobs: a sampling mixture coefficient alpha and a contract-regularization weight lambda_C. We provide a compact theory in a toy setting: OSAG sampling concentrates empirical coverage to targets; coverage deviations upper-bound service-risk deviations; and contract design (coarse vs. fine) modulates governance cost. Experiments on AVIRIS hyperspectral scenes (Indian Pines plus Salinas) and multispectral Sentinel-2 EuroSAT demonstrate that OSAG can substantially reduce priority coverage error while maintaining global accuracy and improving high-priority accuracy. A EuroSAT coarse-vs-fine contract ablation further evidences how semantically refined contracts can reduce the accuracy cost per unit of governance improvement.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TimesNet-Gen: Deep Learning-based Site Specific Strong Motion Generation</title>
<link>https://arxiv.org/abs/2512.04694</link>
<guid>https://arxiv.org/abs/2512.04694</guid>
<content:encoded><![CDATA[
<div> Keywords: earthquake risk reduction, site-specific evaluation, ground motion generation, TimesNet-Gen, strong motion synthesis<br /><br />Summary:<br /><br />1. The article addresses the critical need for accurate, site-specific evaluations to effectively reduce earthquake risk by modeling local site conditions' influence on ground motion characteristics.<br />2. It introduces TimesNet-Gen, a novel time-domain conditional generative model designed to synthesize strong ground motion accelerometer records with station-specific latent bottlenecks.<br />3. The method focuses on generating synthetic ground motion data that closely match real observations at individual stations, emphasizing site-specific features.<br />4. Evaluation techniques include comparing Horizontal-to-Vertical Spectral Ratio (HVSR) curves and fundamental site frequency ($f_0$) distributions between real and generated records on a per-station basis.<br />5. A station specificity score is proposed, derived from confusion matrices of the $f_0$ distributions, to quantify alignment between generated and actual data.<br />6. TimesNet-Gen shows strong station-wise alignment and outperforms a spectrogram-based conditional variational autoencoder (VAE) baseline in site-specific strong motion synthesis.<br />7. The authors have made their code publicly available on GitHub for further research and application. <div>
arXiv:2512.04694v1 Announce Type: new 
Abstract: Effective earthquake risk reduction relies on accurate site-specific evaluations. This requires models that can represent the influence of local site conditions on ground motion characteristics. In this context, data driven approaches that learn site controlled signatures from recorded ground motions offer a promising direction. We address strong ground motion generation from time-domain accelerometer records and introduce the TimesNet-Gen, a time-domain conditional generator. The approach uses a station specific latent bottleneck. We evaluate generation by comparing HVSR curves and fundamental site-frequency $f_0$ distributions between real and generated records per station, and summarize station specificity with a score based on the $f_0$ distribution confusion matrices. TimesNet-Gen achieves strong station-wise alignment and compares favorably with a spectrogram-based conditional VAE baseline for site-specific strong motion synthesis. Our codes are available via https://github.com/brsylmz23/TimesNet-Gen.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRINITY: An Evolved LLM Coordinator</title>
<link>https://arxiv.org/abs/2512.04695</link>
<guid>https://arxiv.org/abs/2512.04695</guid>
<content:encoded><![CDATA[
<div> Foundation models, weight-merging, coordinator, evolutionary strategy, LiveCodeBench<br /><br />Summary:<br /><br />1. The paper introduces Trinity, a framework designed to combine diverse foundation models, especially large language models (LLMs), overcoming limitations of traditional weight-merging approaches which struggle due to mismatched architectures and closed APIs.  
2. Trinity employs a lightweight coordinator consisting of a compact language model (~0.6B parameters) and a lightweight head (~10K parameters), using an evolutionary strategy to efficiently delegate tasks among LLMs.  
3. The coordinator assigns one of three roles—Thinker, Worker, or Verifier—to selected LLMs in a multi-turn query process, thereby offloading complex skill acquisition from the coordinator and enabling effective collaboration.  
4. Experimental results demonstrate that Trinity outperforms individual models and existing combination methods across tasks including coding, math, reasoning, and domain knowledge, while also generalizing well to out-of-distribution scenarios.  
5. On benchmarks such as LiveCodeBench, Trinity achieves state-of-the-art results, e.g., an 86.2% score. The paper’s theoretical and empirical analyses underline two success factors: the coordinator’s rich hidden-state contextualization and the effectiveness of the separable Covariance Matrix Adaptation Evolution Strategy under high dimensionality and tight resource constraints. <div>
arXiv:2512.04695v1 Announce Type: new 
Abstract: Combining diverse foundation models is promising, but weight-merging is limited by mismatched architectures and closed APIs. Trinity addresses this with a lightweight coordinator that orchestrates collaboration among large language models (LLMs). The coordinator, comprising a compact language model (approximately $0.6$B parameters) and a lightweight head (approximately $10$K parameters), is optimized with an evolutionary strategy for efficient and adaptive delegation. Trinity processes queries over multiple turns, where at each turn the coordinator assigns one of three roles (Thinker, Worker, or Verifier) to a selected LLM, effectively offloading complex skill acquisition from the coordinator itself. Experiments show that Trinity consistently outperforms individual models and existing methods across coding, math, reasoning, and domain knowledge tasks, and generalizes robustly to out-of-distribution tasks. On standard benchmarks, Trinity achieves state-of-the-art results, including a score of 86.2% on LiveCodeBench. Theoretical and empirical analyses identify two main factors behind this performance: (1) the coordinator's hidden-state representations provide rich contextualization of inputs, and (2) under high dimensionality and strict budget constraints, the separable Covariance Matrix Adaptation Evolution Strategy offers advantages over reinforcement learning, imitation learning, and random search by exploiting potential block-epsilon-separability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Continuous-Time Approximations for Stochastic Gradient Descent without Replacement</title>
<link>https://arxiv.org/abs/2512.04703</link>
<guid>https://arxiv.org/abs/2512.04703</guid>
<content:encoded><![CDATA[
<div> SGDo, stochastic gradient descent, epoched Brownian motion, continuous-time approximation, strongly convex objectives<br /><br />Summary:<br /><br />This article addresses the limited mathematical understanding of stochastic gradient descent without replacement (SGDo), commonly utilized in machine learning training with epochs. The authors introduce a novel stochastic continuous-time approximation framework for SGDo, modeled as a Young differential equation driven by an "epoched Brownian motion," a newly defined stochastic process. This approach captures the dynamics of SGDo with additive noise, distinguishing it from traditional "with replacement" or "one-pass" SGD methods. The study rigorously proves the almost sure convergence of this continuous-time approximation under strongly convex objective functions and learning rate schedules of the form \(u_t = \frac{1}{(1+t)^\beta}\), where \(\beta\) lies in the interval (0,1). Furthermore, the paper derives an explicit upper bound on the asymptotic rate of this convergence. Notably, the convergence rate established is comparable to or better than existing theoretical results for SGDo. This advancement not only deepens the theoretical insights into SGDo but also potentially guides the design of more effective and reliable epoch-based optimization algorithms in practice. <div>
arXiv:2512.04703v1 Announce Type: new 
Abstract: Gradient optimization algorithms using epochs, that is those based on stochastic gradient descent without replacement (SGDo), are predominantly used to train machine learning models in practice. However, the mathematical theory of SGDo and related algorithms remain underexplored compared to their "with replacement" and "one-pass" counterparts. In this article, we propose a stochastic, continuous-time approximation to SGDo with additive noise based on a Young differential equation driven by a stochastic process we call an "epoched Brownian motion". We show its usefulness by proving the almost sure convergence of the continuous-time approximation for strongly convex objectives and learning rate schedules of the form $u_t = \frac{1}{(1+t)^\beta}, \beta \in (0,1)$. Moreover, we compute an upper bound on the asymptotic rate of almost sure convergence, which is as good or better than previous results for SGDo.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Tutorial on Regression Analysis: From Linear Models to Deep Learning -- Lecture Notes on Artificial Intelligence</title>
<link>https://arxiv.org/abs/2512.04747</link>
<guid>https://arxiv.org/abs/2512.04747</guid>
<content:encoded><![CDATA[
<div> Keywords: regression analysis, linear regression, logistic regression, regularization, machine learning<br /><br />Summary:<br /><br />This article provides comprehensive lecture notes on regression analysis tailored for students with basic university-level mathematics background, including calculus, linear algebra, and probability theory. It covers a wide range of regression models such as linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. The notes emphasize fundamental concepts and theoretical foundations, ensuring a self-contained learning experience without the need for external references. Key methodological topics include the design of loss functions, parameter estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques like Ridge and LASSO regression. Detailed mathematical derivations and illustrative examples support the materials alongside intuitive visual explanations to help students understand model construction, optimization processes, and the interpretation of relationships between features and response variables. By integrating classical statistical modeling approaches with modern machine learning practices, the lecture notes aim to build a solid conceptual and technical foundation. This foundation prepares students for advanced study and research in artificial intelligence and related fields. <div>
arXiv:2512.04747v1 Announce Type: new 
Abstract: This article serves as the regression analysis lecture notes in the Intelligent Computing course cluster (including the courses of Artificial Intelligence, Data Mining, Machine Learning, and Pattern Recognition). It aims to provide students -- who are assumed to possess only basic university-level mathematics (i.e., with prerequisite courses in calculus, linear algebra, and probability theory) -- with a comprehensive and self-contained understanding of regression analysis without requiring any additional references. The lecture notes systematically introduce the fundamental concepts, modeling components, and theoretical foundations of regression analysis, covering linear regression, logistic regression, multinomial logistic regression, polynomial regression, basis-function models, kernel-based methods, and neural-network-based nonlinear regression. Core methodological topics include loss-function design, parameter-estimation principles, ordinary least squares, gradient-based optimization algorithms and their variants, as well as regularization techniques such as Ridge and LASSO regression. Through detailed mathematical derivations, illustrative examples, and intuitive visual explanations, the materials help students understand not only how regression models are constructed and optimized, but also how they reveal the underlying relationships between features and response variables. By bridging classical statistical modeling and modern machine-learning practice, these lecture notes aim to equip students with a solid conceptual and technical foundation for further study in advanced artificial intelligence models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>RLHFSpec: Breaking the Efficiency Bottleneck in RLHF Training via Adaptive Drafting</title>
<link>https://arxiv.org/abs/2512.04752</link>
<guid>https://arxiv.org/abs/2512.04752</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning from Human Feedback, speculative decoding, generation bottleneck, workload-aware drafting, sample reallocation<br /><br />Summary:<br /><br />Reinforcement Learning from Human Feedback (RLHF) is a key fine-tuning method for large language models and involves three main stages: generation, inference, and training. The generation stage, responsible for producing samples used in training, has been identified as the primary bottleneck in the RLHF pipeline. To address this, the paper proposes RLHFSpec, the first system to integrate speculative decoding into RLHF’s generation stage, thereby accelerating sample generation. RLHFSpec introduces an adaptive mechanism that selects workload-aware drafting strategies by balancing verification costs with the number of accepted tokens, optimizing decoding performance dynamically. Additionally, the system incorporates a sample reallocation mechanism designed to maximize GPU resource utilization, enhanced by an efficient sample migration process. Experimental results demonstrate that RLHFSpec significantly increases throughput in the generation stage compared to existing state-of-the-art methods. Furthermore, by alleviating the generation bottleneck effectively, RLHFSpec contributes to an overall speedup in the entire RLHF execution process, making it a promising advancement for efficient large language model fine-tuning. <div>
arXiv:2512.04752v1 Announce Type: new 
Abstract: Reinforcement Learning from Human Feedback (RLHF) is an important fine-tuning technique for large language models (LLMs) and comprises three stages: generation, inference, and training. The generation stage generates samples that are then used to infer learnable experiences for training. We observe that the generation stage is the bottleneck of the entire execution process and consider it a key point for optimization. Specifically, we realize the first attempt to integrate speculative decoding into the RLHF generation stage and propose RLHFSpec, an RLHF system that accelerates generation execution with adaptive speculative decoding and sample reallocation. To fully exploit the performance potential provided by speculative decoding, especially dealing with the dynamic workload of the generation stage, RLHFSpec proposes a workload-aware drafting strategy selection mechanism, which selects the near-optimal strategy by jointly considering the verification cost and the number of accepted tokens. Moreover, RLHFSpec also proposes sample reallocation to fully utilize the GPU resources, and optimizes it with an efficient sample migration mechanism. The experimental results show that the RLHFSpec can achieve higher throughput in the generation stage compared to state-of-the-art works. Moreover, due to the effective alleviation of the generation bottleneck, RLHFSpec also shows significant performance speedup in the entire RLHF execution.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MemLoRA: Distilling Expert Adapters for On-Device Memory Systems</title>
<link>https://arxiv.org/abs/2512.04763</link>
<guid>https://arxiv.org/abs/2512.04763</guid>
<content:encoded><![CDATA[
<div> Memory-augmented LLMs, Small Language Models, on-device deployment, vision-language models, multimodal understanding  

<br /><br />Summary:  
1. The paper addresses the challenge of deploying memory-augmented language models on local devices where large language models (LLMs) are too resource-intensive, and small language models (SLMs) often underperform.  
2. It introduces MemLoRA, a novel memory system that uses specialized adapters to empower SLMs with memory capabilities for knowledge extraction, memory updating, and memory-augmented generation, enabling effective on-device inference without cloud dependency.  
3. To extend capabilities to visual understanding, the authors propose MemLoRA-V, which integrates small vision-language models (SVLMs) into the memory system, allowing native multimodal reasoning including visual question answering.  
4. MemLoRA achieves text-only performance surpassing models 10 to 60 times larger on the LoCoMo benchmark, demonstrating the effectiveness of specialized memory adapters on SLMs.  
5. In multimodal tasks, MemLoRA-V significantly outperforms caption-based approaches on visual question answering with a notable accuracy improvement (81.3% vs. 23.7%), while maintaining strong text task performance, proving its strength in combined text-visual contexts. <div>
arXiv:2512.04763v1 Announce Type: new 
Abstract: Memory-augmented Large Language Models (LLMs) have demonstrated remarkable consistency during prolonged dialogues by storing relevant memories and incorporating them as context. Such memory-based personalization is also key in on-device settings that allow users to keep their conversations and data private. However, memory-augmented systems typically rely on LLMs that are too costly for local on-device deployment. Even though Small Language Models (SLMs) are more suitable for on-device inference than LLMs, they cannot achieve sufficient performance. Additionally, these LLM-based systems lack native visual capabilities, limiting their applicability in multimodal contexts. In this paper, we introduce (i) MemLoRA, a novel memory system that enables local deployment by equipping SLMs with specialized memory adapters, and (ii) its vision extension MemLoRA-V, which integrates small Vision-Language Models (SVLMs) to memory systems, enabling native visual understanding. Following knowledge distillation principles, each adapter is trained separately for specific memory operations$\unicode{x2013}$knowledge extraction, memory update, and memory-augmented generation. Equipped with memory adapters, small models enable accurate on-device memory operations without cloud dependency. On text-only operations, MemLoRA outperforms 10$\times$ larger baseline models (e.g., Gemma2-27B) and achieves performance comparable to 60$\times$ larger models (e.g., GPT-OSS-120B) on the LoCoMo benchmark. To evaluate visual understanding operations instead, we extend LoCoMo with challenging Visual Question Answering tasks that require direct visual reasoning. On this, our VLM-integrated MemLoRA-V shows massive improvements over caption-based approaches (81.3 vs. 23.7 accuracy) while keeping strong performance in text-based tasks, demonstrating the efficacy of our method in multimodal contexts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A result relating convex n-widths to covering numbers with some applications to neural networks</title>
<link>https://arxiv.org/abs/2512.04912</link>
<guid>https://arxiv.org/abs/2512.04912</guid>
<content:encoded><![CDATA[
<div> Keywords: high-dimensional approximation, basis functions, curse of dimensionality, covering numbers, neural networks

<br /><br />Summary:  
This paper addresses the challenge of approximating function classes defined over high-dimensional input spaces by using linear combinations of a fixed set of basis functions or features. It begins by highlighting the traditional difficulty: the worst-case approximation error typically degrades at a rate of \(\Theta(n^{-1/d})\), where \(n\) is the number of basis functions and \(d\) the input dimension, illustrating the "curse of dimensionality." Despite this, some practical problems in high-dimensional pattern recognition, such as face recognition, can be effectively solved with linear combinations of small feature sets, indicating that certain function classes avoid this curse. Motivated by this, the authors seek a characterization for these high-dimensional function classes that remain well-approximated by few features. Their main result relates the approximation error to the covering number of the function class's "convex core," a geometric quantity expressing complexity. For one-hidden-layer neural networks, they show that the covering numbers for functions produced by a single hidden node bound those of the convex core. Utilizing known theoretical results on covering numbers, they derive upper bounds on the function approximation rates achievable by such neural network classes. This links geometric complexity measures directly to neural network approximation performance in high dimensions. <div>
arXiv:2512.04912v1 Announce Type: new 
Abstract: In general, approximating classes of functions defined over high-dimensional input spaces by linear combinations of a fixed set of basis functions or ``features'' is known to be hard. Typically, the worst-case error of the best basis set decays only as fast as $\Theta\(n^{-1/d}\)$, where $n$ is the number of basis functions and $d$ is the input dimension. However, there are many examples of high-dimensional pattern recognition problems (such as face recognition) where linear combinations of small sets of features do solve the problem well. Hence these function classes do not suffer from the ``curse of dimensionality'' associated with more general classes. It is natural then, to look for characterizations of high-dimensional function classes that nevertheless are approximated well by linear combinations of small sets of features. In this paper we give a general result relating the error of approximation of a function class to the covering number of its ``convex core''. For one-hidden-layer neural networks, covering numbers of the class of functions computed by a single hidden node upper bound the covering numbers of the convex core. Hence, using standard results we obtain upper bounds on the approximation rate of neural network classes.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Agent Reinforcement Learning for Intraday Operating Rooms Scheduling under Uncertainty</title>
<link>https://arxiv.org/abs/2512.04918</link>
<guid>https://arxiv.org/abs/2512.04918</guid>
<content:encoded><![CDATA[
<div> Keywords: surgical scheduling, multi-agent reinforcement learning, Proximal Policy Optimization, operating rooms, delay penalties<br /><br />Summary:  
1. The article addresses intraday surgical scheduling, a multi-objective decision problem under uncertainty involving elective throughput, urgent and emergency demands, delays, sequence-dependent setups, and overtime.  
2. The authors model the problem as a cooperative Markov game and introduce a multi-agent reinforcement learning (MARL) framework, where each operating room (OR) acts as an agent trained centrally but executes decisions in a decentralized manner.  
3. Agents share a policy trained using Proximal Policy Optimization (PPO) that maps complex system states to scheduling actions, with a sequential assignment protocol ensuring conflict-free joint schedules across multiple ORs.  
4. A mixed-integer pre-schedule sets reference starting times for elective surgeries; penalties for delays and overtime are incorporated into a combined reward function reflecting throughput, timeliness, and workload.  
5. Simulations with six ORs and eight surgery types, including random urgent arrivals, show the learned policy outperforms six rule-based heuristics across seven performance metrics and three evaluation subsets.  
6. Comparison with an ex post mixed-integer programming (MIP) oracle quantifies the optimality gap. Policy analytics reveal interpretable behaviors such as prioritizing emergencies, batching similar cases to reduce setups, and deferring less valuable electives.  
7. The paper also develops a suboptimality bound for the sequential decomposition under simplifying assumptions, discusses limitations like OR homogeneity and lack of explicit staffing constraints, and suggests future extensions.  
8. Overall, the approach provides a practical, interpretable, and tunable data-driven method that complements traditional optimization for real-time surgical scheduling in hospitals. <div>
arXiv:2512.04918v1 Announce Type: new 
Abstract: Intraday surgical scheduling is a multi-objective decision problem under uncertainty-balancing elective throughput, urgent and emergency demand, delays, sequence-dependent setups, and overtime. We formulate the problem as a cooperative Markov game and propose a multi-agent reinforcement learning (MARL) framework in which each operating room (OR) is an agent trained with centralized training and decentralized execution. All agents share a policy trained via Proximal Policy Optimization (PPO), which maps rich system states to actions, while a within-epoch sequential assignment protocol constructs conflict-free joint schedules across ORs. A mixed-integer pre-schedule provides reference starting times for electives; we impose type-specific quadratic delay penalties relative to these references and a terminal overtime penalty, yielding a single reward that captures throughput, timeliness, and staff workload. In simulations reflecting a realistic hospital mix (six ORs, eight surgery types, random urgent and emergency arrivals), the learned policy outperforms six rule-based heuristics across seven metrics and three evaluation subsets, and, relative to an ex post MIP oracle, quantifies optimality gaps. Policy analytics reveal interpretable behavior-prioritizing emergencies, batching similar cases to reduce setups, and deferring lower-value electives. We also derive a suboptimality bound for the sequential decomposition under simplifying assumptions. We discuss limitations-including OR homogeneity and the omission of explicit staffing constraints-and outline extensions. Overall, the approach offers a practical, interpretable, and tunable data-driven complement to optimization for real-time OR scheduling.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CARL: Critical Action Focused Reinforcement Learning for Multi-Step Agent</title>
<link>https://arxiv.org/abs/2512.04949</link>
<guid>https://arxiv.org/abs/2512.04949</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-step reinforcement learning, critical actions, policy optimization, training efficiency, CARL algorithm<br /><br />Summary:<br /><br />This article addresses the limitations of conventional group-level policy optimization algorithms in multi-step reinforcement learning tasks, highlighting that these traditional methods treat all actions as equally important, which is not reflective of their actual impact. The authors reveal through analysis that only a small subset of actions significantly influences the final outcome of the task. Based on this insight, they introduce CARL, a novel reinforcement learning algorithm designed to focus specifically on these critical actions. CARL enhances the training process by providing targeted optimization signals exclusively to high-criticality actions while disregarding low-criticality ones during model updates. This selective approach improves both the focus and efficacy of learning. Extensive experiments conducted across various evaluation settings demonstrate that CARL not only outperforms existing methods in terms of final task performance but also offers higher efficiency during both training and inference phases. Thus, the proposed method contributes a more effective and computationally efficient approach for optimizing multi-step agent behaviors by prioritizing actions that truly matter. <div>
arXiv:2512.04949v1 Announce Type: new 
Abstract: Agents capable of accomplishing complex tasks through multiple interactions with the environment have emerged as a popular research direction. However, in such multi-step settings, the conventional group-level policy optimization algorithm becomes suboptimal because of its underlying assumption that each action holds equal contribution, which deviates significantly from reality. Our analysis reveals that only a small fraction of actions are critical in determining the final outcome. Building on this insight, we propose CARL, a critical-action-focused reinforcement learning algorithm tailored for multi-step agents. CARL achieves focused training through providing action-level optimization signals for high-criticality actions while excluding low-criticality actions from model update. Extensive experiments demonstrate that CARL achieves both stronger performance and higher efficiency during training and inference across diverse evaluation settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Inference of Multi-Modal Posteriors using Likelihood-Weighted Normalizing Flows</title>
<link>https://arxiv.org/abs/2512.04954</link>
<guid>https://arxiv.org/abs/2512.04954</guid>
<content:encoded><![CDATA[
<div> Keywords: amortized posterior estimation, Normalizing Flows, likelihood-weighted importance sampling, Gaussian Mixture Model, multi-modal inference

<br /><br />Summary:  
The paper introduces a novel method for amortized posterior estimation leveraging Normalizing Flows trained via likelihood-weighted importance sampling, enabling efficient inference of theoretical parameters in complex high-dimensional inverse problems without requiring posterior training samples. The method is tested on multi-modal benchmark problems in 2D and 3D to evaluate its performance and effectiveness. A key insight from the study is the significant influence that the topology of the base distributions has on the modeled posterior distributions. When using standard unimodal base distributions, the method struggles to accurately represent disconnected support regions, leading to artificial probability connections between distinct modes. To overcome this limitation, the authors propose initializing the Normalizing Flow with a Gaussian Mixture Model whose number of components matches that of the target modes. This approach enhances the reconstruction quality substantially, which is quantitatively confirmed through various distance and divergence metrics. The results indicate that the choice of base distribution topology is critical for accurately modeling complex posterior shapes in amortized inference frameworks. <div>
arXiv:2512.04954v1 Announce Type: new 
Abstract: We present a novel technique for amortized posterior estimation using Normalizing Flows trained with likelihood-weighted importance sampling. This approach allows for the efficient inference of theoretical parameters in high-dimensional inverse problems without the need for posterior training samples. We implement the method on multi-modal benchmark tasks in 2D and 3D to check for the efficacy. A critical observation of our study is the impact of the topology of the base distributions on the modelled posteriors. We find that standard unimodal base distributions fail to capture disconnected support, resulting in spurious probability bridges between modes. We demonstrate that initializing the flow with a Gaussian Mixture Model that matches the cardinality of the target modes significantly improves reconstruction fidelity, as measured by some distance and divergence metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Realizable Abstractions: Near-Optimal Hierarchical Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.04958</link>
<guid>https://arxiv.org/abs/2512.04958</guid>
<content:encoded><![CDATA[
<div> Hierarchical Reinforcement Learning, Markov Decision Processes, Realizable Abstractions, near-optimality guarantees, compositional policies  

<br /><br />Summary:  
This paper tackles foundational challenges in Hierarchical Reinforcement Learning (HRL) by introducing Realizable Abstractions, a novel concept that formally connects low-level Markov Decision Processes (MDPs) with their high-level decision process abstractions. Unlike previous HRL abstraction approaches, Realizable Abstractions avoid issues related to non-Markovian dynamics and provide strong theoretical near-optimality guarantees. Specifically, the authors prove that any policy derived at the abstract level can be effectively translated into a near-optimal policy for the underlying detailed MDP using a suitable combination of options, which are solutions to constrained MDPs. Building on this theoretical framework, the paper presents RARL, a new HRL algorithm designed to leverage the Realizable Abstraction given as input. RARL constructs compositional and near-optimal low-level policies with rigorous Probably Approximately Correct (PAC) guarantees, demonstrating convergence within a polynomial number of samples. Furthermore, RARL is robust to imperfections or inaccuracies in the provided abstractions, making it a practical solution for scalable, modular RL problem solving. Overall, this work advances HRL by combining formal abstraction theory with practical algorithm design, facilitating efficient learning in large-scale MDPs through modular, hierarchical decomposition. <div>
arXiv:2512.04958v1 Announce Type: new 
Abstract: The main focus of Hierarchical Reinforcement Learning (HRL) is studying how large Markov Decision Processes (MDPs) can be more efficiently solved when addressed in a modular way, by combining partial solutions computed for smaller subtasks. Despite their very intuitive role for learning, most notions of MDP abstractions proposed in the HRL literature have limited expressive power or do not possess formal efficiency guarantees. This work addresses these fundamental issues by defining Realizable Abstractions, a new relation between generic low-level MDPs and their associated high-level decision processes. The notion we propose avoids non-Markovianity issues and has desirable near-optimality guarantees. Indeed, we show that any abstract policy for Realizable Abstractions can be translated into near-optimal policies for the low-level MDP, through a suitable composition of options. As demonstrated in the paper, these options can be expressed as solutions of specific constrained MDPs. Based on these findings, we propose RARL, a new HRL algorithm that returns compositional and near-optimal low-level policies, taking advantage of the Realizable Abstraction given in the input. We show that RARL is Probably Approximately Correct, it converges in a polynomial number of samples, and it is robust to inaccuracies in the abstraction.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Generative Transformer Operators For Million-Point PDEs</title>
<link>https://arxiv.org/abs/2512.04974</link>
<guid>https://arxiv.org/abs/2512.04974</guid>
<content:encoded><![CDATA[
arXiv:2512.04974v1 Announce Type: new 
Abstract: We introduce ECHO, a transformer-operator framework for generating million-point PDE trajectories. While existing neural operators (NOs) have shown promise for solving partial differential equations, they remain limited in practice due to poor scalability on dense grids, error accumulation during dynamic unrolling, and task-specific design. ECHO addresses these challenges through three key innovations. (i) It employs a hierarchical convolutional encode-decode architecture that achieves a 100 $\times$ spatio-temporal compression while preserving fidelity on mesh points. (ii) It incorporates a training and adaptation strategy that enables high-resolution PDE solution generation from sparse input grids. (iii) It adopts a generative modeling paradigm that learns complete trajectory segments, mitigating long-horizon error drift. The training strategy decouples representation learning from downstream task supervision, allowing the model to tackle multiple tasks such as trajectory generation, forward and inverse problems, and interpolation. The generative model further supports both conditional and unconditional generation. We demonstrate state-of-the-art performance on million-point simulations across diverse PDE systems featuring complex geometries, high-frequency dynamics, and long-term horizons.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Path Region-Guided Attention Network for Ground Reaction Force and Moment Regression</title>
<link>https://arxiv.org/abs/2512.05030</link>
<guid>https://arxiv.org/abs/2512.05030</guid>
<content:encoded><![CDATA[
arXiv:2512.05030v1 Announce Type: new 
Abstract: Accurate estimation of three-dimensional ground reaction forces and moments (GRFs/GRMs) is crucial for both biomechanics research and clinical rehabilitation evaluation. In this study, we focus on insole-based GRF/GRM estimation and further validate our approach on a public walking dataset. We propose a Dual-Path Region-Guided Attention Network that integrates anatomy-inspired spatial priors and temporal priors into a region-level attention mechanism, while a complementary path captures context from the full sensor field. The two paths are trained jointly and their outputs are combined to produce the final GRF/GRM predictions. Conclusions: Our model outperforms strong baseline models, including CNN and CNN-LSTM architectures on two datasets, achieving the lowest six-component average NRMSE of 5.78% on the insole dataset and 1.42% for the vertical ground reaction force on the public dataset. This demonstrates robust performance for ground reaction force and moment estimation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SuperActivators: Only the Tail of the Distribution Contains Reliable Concept Signals</title>
<link>https://arxiv.org/abs/2512.05038</link>
<guid>https://arxiv.org/abs/2512.05038</guid>
<content:encoded><![CDATA[
arXiv:2512.05038v1 Announce Type: new 
Abstract: Concept vectors aim to enhance model interpretability by linking internal representations with human-understandable semantics, but their utility is often limited by noisy and inconsistent activations. In this work, we uncover a clear pattern within the noise, which we term the SuperActivator Mechanism: while in-concept and out-of-concept activations overlap considerably, the token activations in the extreme high tail of the in-concept distribution provide a reliable signal of concept presence. We demonstrate the generality of this mechanism by showing that SuperActivator tokens consistently outperform standard vector-based and prompting concept detection approaches, achieving up to a 14% higher F1 score across image and text modalities, model architectures, model layers, and concept extraction techniques. Finally, we leverage SuperActivator tokens to improve feature attributions for concepts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-LLM Collaboration for Medication Recommendation</title>
<link>https://arxiv.org/abs/2512.05066</link>
<guid>https://arxiv.org/abs/2512.05066</guid>
<content:encoded><![CDATA[
arXiv:2512.05066v1 Announce Type: new 
Abstract: As healthcare increasingly turns to AI for scalable and trustworthy clinical decision support, ensuring reliability in model reasoning remains a critical challenge. Individual large language models (LLMs) are susceptible to hallucinations and inconsistency, whereas naive ensembles of models often fail to deliver stable and credible recommendations. Building on our previous work on LLM Chemistry, which quantifies the collaborative compatibility among LLMs, we apply this framework to improve the reliability in medication recommendation from brief clinical vignettes. Our approach leverages multi-LLM collaboration guided by Chemistry-inspired interaction modeling, enabling ensembles that are effective (exploiting complementary strengths), stable (producing consistent quality), and calibrated (minimizing interference and error amplification). We evaluate our Chemistry-based Multi-LLM collaboration strategy on real-world clinical scenarios to investigate whether such interaction-aware ensembles can generate credible, patient-specific medication recommendations. Preliminary results are encouraging, suggesting that LLM Chemistry-guided collaboration may offer a promising path toward reliable and trustworthy AI assistants in clinical practice.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Quantum-Classical Autoencoders for Unsupervised Network Intrusion Detection</title>
<link>https://arxiv.org/abs/2512.05069</link>
<guid>https://arxiv.org/abs/2512.05069</guid>
<content:encoded><![CDATA[
arXiv:2512.05069v1 Announce Type: new 
Abstract: Unsupervised anomaly-based intrusion detection requires models that can generalize to attack patterns not observed during training. This work presents the first large-scale evaluation of hybrid quantum-classical (HQC) autoencoders for this task. We construct a unified experimental framework that iterates over key quantum design choices, including quantum-layer placement, measurement approach, variational and non-variational formulations, and latent-space regularization. Experiments across three benchmark NIDS datasets show that HQC autoencoders can match or exceed classical performance in their best configurations, although they exhibit higher sensitivity to architectural decisions. Under zero-day evaluation, well-configured HQC models provide stronger and more stable generalization than classical and supervised baselines. Simulated gate-noise experiments reveal early performance degradation, indicating the need for noise-aware HQC designs. These results provide the first data-driven characterization of HQC autoencoder behavior for network intrusion detection and outline key factors that govern their practical viability. All experiment code and configurations are available at https://github.com/arasyi/hqcae-network-intrusion-detection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>David vs. Goliath: Can Small Models Win Big with Agentic AI in Hardware Design?</title>
<link>https://arxiv.org/abs/2512.05073</link>
<guid>https://arxiv.org/abs/2512.05073</guid>
<content:encoded><![CDATA[
arXiv:2512.05073v1 Announce Type: new 
Abstract: Large Language Model(LLM) inference demands massive compute and energy, making domain-specific tasks expensive and unsustainable. As foundation models keep scaling, we ask: Is bigger always better for hardware design? Our work tests this by evaluating Small Language Models coupled with a curated agentic AI framework on NVIDIA's Comprehensive Verilog Design Problems(CVDP) benchmark. Results show that agentic workflows: through task decomposition, iterative feedback, and correction - not only unlock near-LLM performance at a fraction of the cost but also create learning opportunities for agents, paving the way for efficient, adaptive solutions in complex design tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OMTRA: A Multi-Task Generative Model for Structure-Based Drug Design</title>
<link>https://arxiv.org/abs/2512.05080</link>
<guid>https://arxiv.org/abs/2512.05080</guid>
<content:encoded><![CDATA[
arXiv:2512.05080v1 Announce Type: new 
Abstract: Structure-based drug design (SBDD) focuses on designing small-molecule ligands that bind to specific protein pockets. Computational methods are integral in modern SBDD workflows and often make use of virtual screening methods via docking or pharmacophore search. Modern generative modeling approaches have focused on improving novel ligand discovery by enabling de novo design. In this work, we recognize that these tasks share a common structure and can therefore be represented as different instantiations of a consistent generative modeling framework. We propose a unified approach in OMTRA, a multi-modal flow matching model that flexibly performs many tasks relevant to SBDD, including some with no analogue in conventional workflows. Additionally, we curate a dataset of 500M 3D molecular conformers, complementing protein-ligand data and expanding the chemical diversity available for training. OMTRA obtains state of the art performance on pocket-conditioned de novo design and docking; however, the effects of large-scale pretraining and multi-task training are modest. All code, trained models, and dataset for reproducing this work are available at https://github.com/gnina/OMTRA
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Descent with Provably Tuned Learning-rate Schedules</title>
<link>https://arxiv.org/abs/2512.05084</link>
<guid>https://arxiv.org/abs/2512.05084</guid>
<content:encoded><![CDATA[
arXiv:2512.05084v1 Announce Type: new 
Abstract: Gradient-based iterative optimization methods are the workhorse of modern machine learning. They crucially rely on careful tuning of parameters like learning rate and momentum. However, one typically sets them using heuristic approaches without formal near-optimality guarantees. Recent work by Gupta and Roughgarden studies how to learn a good step-size in gradient descent. However, like most of the literature with theoretical guarantees for gradient-based optimization, their results rely on strong assumptions on the function class including convexity and smoothness which do not hold in typical applications. In this work, we develop novel analytical tools for provably tuning hyperparameters in gradient-based algorithms that apply to non-convex and non-smooth functions. We obtain matching sample complexity bounds for learning the step-size in gradient descent shown for smooth, convex functions in prior work (up to logarithmic factors) but for a much broader class of functions. Our analysis applies to gradient descent on neural networks with commonly used activation functions (including ReLU, sigmoid and tanh). We extend our framework to tuning multiple hyperparameters, including tuning the learning rate schedule, simultaneously tuning momentum and step-size, and pre-training the initialization vector. Our approach can be used to bound the sample complexity for minimizing both the validation loss as well as the number of gradient descent iterations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Intelligence: Deterministic Functional Topology as a Foundation for Real-World Perception</title>
<link>https://arxiv.org/abs/2512.05089</link>
<guid>https://arxiv.org/abs/2512.05089</guid>
<content:encoded><![CDATA[
arXiv:2512.05089v1 Announce Type: new 
Abstract: Real-world physical processes do not generate arbitrary variability: their signals concentrate on compact and low-variability subsets of functional space. This geometric structure enables rapid generalization from a few examples in both biological and artificial systems.
  This work develops a deterministic functional-topological framework in which the set of valid realizations of a physical phenomenon forms a compact perceptual manifold with stable invariants and a finite Hausdorff radius. We show that the boundaries of this manifold can be discovered in a fully self-supervised manner through Monte Carlo sampling, even when the governing equations of the system are unknown.
  We provide theoretical guarantees, practical estimators of knowledge boundaries, and empirical validations across three domains: electromechanical railway point machines, electrochemical battery discharge curves, and physiological ECG signals.
  Our results demonstrate that deterministic functional topology offers a unified mathematical foundation for perception, representation, and world-model construction, explaining why biological learners and self-supervised AI models can generalize from limited observations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TV2TV: A Unified Framework for Interleaved Language and Video Generation</title>
<link>https://arxiv.org/abs/2512.05103</link>
<guid>https://arxiv.org/abs/2512.05103</guid>
<content:encoded><![CDATA[
arXiv:2512.05103v1 Announce Type: new 
Abstract: Video generation models are rapidly advancing, but can still struggle with complex video outputs that require significant semantic branching or repeated high-level reasoning about what should happen next. In this paper, we introduce a new class of omni video-text models that integrate ideas from recent LM reasoning advances to address this challenge. More specifically, we present TV2TV, a unified generative modeling framework which decomposes video generation into an interleaved text and video generation process. TV2TV jointly learns language modeling (next-token prediction) and video flow matching (next-frame prediction) using a Mixture-of-Transformers (MoT) architecture. At inference time, TV2TV decides when to alternate between generating text and video frames, allowing the model to "think in words" about subsequent content before ``acting in pixels'' to produce frames. This design offloads much of the responsibility for deciding what should happen next to the language modeling tower, enabling improved visual quality and prompt alignment of generated videos. It also enables fine-grained controllability, allowing users to modify the video generation trajectory through text interventions at any point in the process. In controlled experiments on video game data, TV2TV demonstrates substantial improvements in both visual quality and controllability. TV2TV also scales to natural videos, as we show by augmenting sports videos with interleaved natural language action descriptions using vision-language models (VLMs). Training TV2TV on this corpus yields strong visual quality and prompt alignment, showcasing the model's ability to reason about and generate complex real-world action sequences. Together, these results highlight TV2TV as a promising step toward video generation with open-ended textual reasoning and control.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep infant brain segmentation from multi-contrast MRI</title>
<link>https://arxiv.org/abs/2512.05114</link>
<guid>https://arxiv.org/abs/2512.05114</guid>
<content:encoded><![CDATA[
arXiv:2512.05114v1 Announce Type: new 
Abstract: Segmentation of magnetic resonance images (MRI) facilitates analysis of human brain development by delineating anatomical structures. However, in infants and young children, accurate segmentation is challenging due to development and imaging constraints. Pediatric brain MRI is notoriously difficult to acquire, with inconsistent availability of imaging modalities, substantial non-head anatomy in the field of view, and frequent motion artifacts. This has led to specialized segmentation models that are often limited to specific image types or narrow age groups, or that are fragile for more variable images such as those acquired clinically. We address this method fragmentation with BabySeg, a deep learning brain segmentation framework for infants and young children that supports diverse MRI protocols, including repeat scans and image types unavailable during training. Our approach builds on recent domain randomization techniques, which synthesize training images far beyond realistic bounds to promote dataset shift invariance. We also describe a mechanism that enables models to flexibly pool and interact features from any number of input scans. We demonstrate state-of-the-art performance that matches or exceeds the accuracy of several existing methods for various age cohorts and input configurations using a single model, in a fraction of the runtime required by many existing tools.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Value Gradient Guidance for Flow Matching Alignment</title>
<link>https://arxiv.org/abs/2512.05116</link>
<guid>https://arxiv.org/abs/2512.05116</guid>
<content:encoded><![CDATA[
arXiv:2512.05116v1 Announce Type: new 
Abstract: While methods exist for aligning flow matching models--a popular and effective class of generative models--with human preferences, existing approaches fail to achieve both adaptation efficiency and probabilistically sound prior preservation. In this work, we leverage the theory of optimal control and propose VGG-Flow, a gradient-matching-based method for finetuning pretrained flow matching models. The key idea behind this algorithm is that the optimal difference between the finetuned velocity field and the pretrained one should be matched with the gradient field of a value function. This method not only incorporates first-order information from the reward model but also benefits from heuristic initialization of the value function to enable fast adaptation. Empirically, we show on a popular text-to-image flow matching model, Stable Diffusion 3, that our method can finetune flow matching models under limited computational budgets while achieving effective and prior-preserving alignment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Universal Weight Subspace Hypothesis</title>
<link>https://arxiv.org/abs/2512.05117</link>
<guid>https://arxiv.org/abs/2512.05117</guid>
<content:encoded><![CDATA[
arXiv:2512.05117v1 Announce Type: new 
Abstract: We show that deep neural networks trained across diverse tasks exhibit remarkably similar low-dimensional parametric subspaces. We provide the first large-scale empirical evidence that demonstrates that neural networks systematically converge to shared spectral subspaces regardless of initialization, task, or domain. Through mode-wise spectral analysis of over 1100 models - including 500 Mistral-7B LoRAs, 500 Vision Transformers, and 50 LLaMA-8B models - we identify universal subspaces capturing majority variance in just a few principal directions. By applying spectral decomposition techniques to the weight matrices of various architectures trained on a wide range of tasks and datasets, we identify sparse, joint subspaces that are consistently exploited, within shared architectures across diverse tasks and datasets. Our findings offer new insights into the intrinsic organization of information within deep networks and raise important questions about the possibility of discovering these universal subspaces without the need for extensive data and computational resources. Furthermore, this inherent structure has significant implications for model reusability, multi-task learning, model merging, and the development of training and inference-efficient algorithms, potentially reducing the carbon footprint of large-scale neural models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking AI Evaluation in Education: The TEACH-AI Framework and Benchmark for Generative AI Assistants</title>
<link>https://arxiv.org/abs/2512.04107</link>
<guid>https://arxiv.org/abs/2512.04107</guid>
<content:encoded><![CDATA[
arXiv:2512.04107v1 Announce Type: cross 
Abstract: As generative artificial intelligence (AI) continues to transform education, most existing AI evaluations rely primarily on technical performance metrics such as accuracy or task efficiency while overlooking human identity, learner agency, contextual learning processes, and ethical considerations. In this paper, we present TEACH-AI (Trustworthy and Effective AI Classroom Heuristics), a domain-independent, pedagogically grounded, and stakeholder-aligned framework with measurable indicators and a practical toolkit for guiding the design, development, and evaluation of generative AI systems in educational contexts. Built on an extensive literature review and synthesis, the ten-component assessment framework and toolkit checklist provide a foundation for scalable, value-aligned AI evaluation in education. TEACH-AI rethinks "evaluation" through sociotechnical, educational, theoretical, and applied lenses, engaging designers, developers, researchers, and policymakers across AI and education. Our work invites the community to reconsider what constructs "effective" AI in education and to design model evaluation approaches that promote co-creation, inclusivity, and long-term human, social, and educational impact.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI-Enabled grading with near-domain data for scaling feedback with human-level accuracy</title>
<link>https://arxiv.org/abs/2512.04113</link>
<guid>https://arxiv.org/abs/2512.04113</guid>
<content:encoded><![CDATA[
arXiv:2512.04113v1 Announce Type: cross 
Abstract: Constructed-response questions are crucial to encourage generative processing and test a learner's understanding of core concepts. However, the limited availability of instructor time, large class sizes, and other resource constraints pose significant challenges in providing timely and detailed evaluation, which is crucial for a holistic educational experience. In addition, providing timely and frequent assessments is challenging since manual grading is labor intensive, and automated grading is complex to generalize to every possible response scenario. This paper proposes a novel and practical approach to grade short-answer constructed-response questions. We discuss why this problem is challenging, define the nature of questions on which our method works, and finally propose a framework that instructors can use to evaluate their students' open-responses, utilizing near-domain data like data from similar questions administered in previous years. The proposed method outperforms the state of the art machine learning models as well as non-fine-tuned large language models like GPT 3.5, GPT 4, and GPT 4o by a considerable margin of over 10-20% in some cases, even after providing the LLMs with reference/model answers. Our framework does not require pre-written grading rubrics and is designed explicitly with practical classroom settings in mind. Our results also reveal exciting insights about learning from near-domain data, including what we term as accuracy and data advantages using human-labeled data, and we believe this is the first work to formalize the problem of automated short answer grading based on the near-domain data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Patient Safety Risks from AI Scribes: Signals from End-User Feedback</title>
<link>https://arxiv.org/abs/2512.04118</link>
<guid>https://arxiv.org/abs/2512.04118</guid>
<content:encoded><![CDATA[
arXiv:2512.04118v1 Announce Type: cross 
Abstract: AI scribes are transforming clinical documentation at scale. However, their real-world performance remains understudied, especially regarding their impacts on patient safety. To this end, we initiate a mixed-methods study of patient safety issues raised in feedback submitted by AI scribe users (healthcare providers) in a large U.S. hospital system. Both quantitative and qualitative analysis suggest that AI scribes may induce various patient safety risks due to errors in transcription, most significantly regarding medication and treatment; however, further study is needed to contextualize the absolute degree of risk.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring Agents in Production</title>
<link>https://arxiv.org/abs/2512.04123</link>
<guid>https://arxiv.org/abs/2512.04123</guid>
<content:encoded><![CDATA[
arXiv:2512.04123v1 Announce Type: cross 
Abstract: AI agents are actively running in production across diverse industries, yet little is publicly known about which technical approaches enable successful real-world deployments. We present the first large-scale systematic study of AI agents in production, surveying 306 practitioners and conducting 20 in-depth case studies via interviews across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and what the top development challenges are. We find that production agents are typically built using simple, controllable approaches: 68% execute at most 10 steps before requiring human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability remains the top development challenge, driven by difficulties in ensuring and evaluating agent correctness. Despite these challenges, simple yet effective methods already enable agents to deliver impact across diverse industries. Our study documents the current state of practice and bridges the gap between research and deployment by providing researchers visibility into production challenges while offering practitioners proven patterns from successful deployments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing next token prediction based pre-training for jet foundation models</title>
<link>https://arxiv.org/abs/2512.04149</link>
<guid>https://arxiv.org/abs/2512.04149</guid>
<content:encoded><![CDATA[
arXiv:2512.04149v1 Announce Type: cross 
Abstract: Next token prediction is an attractive pre-training task for jet foundation models, in that it is simulation free and enables excellent generative capabilities that can transfer across datasets. Here we study multiple improvements to next token prediction, building on the initial work of OmniJet-$\alpha$. Instead of tokenizing particles and subsequently only using the token-ID as the model input for both the generative and the classification task, we adopt a hybrid setup, which allows us to use continuous feature vectors as model input while only using token-IDs in the next token prediction target. Secondly, we explore a combined pre-training strategy that combines masked particle modeling and generative learning objectives. Taken together, these changes greatly improve the performance in downstream classification tasks without any loss in generative performance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Educational Cone Model in Embedding Vector Spaces</title>
<link>https://arxiv.org/abs/2512.04227</link>
<guid>https://arxiv.org/abs/2512.04227</guid>
<content:encoded><![CDATA[
arXiv:2512.04227v1 Announce Type: cross 
Abstract: Human-annotated datasets with explicit difficulty ratings are essential in intelligent educational systems. Although embedding vector spaces are widely used to represent semantic closeness and are promising for analyzing text difficulty, the abundance of embedding methods creates a challenge in selecting the most suitable method. This study proposes the Educational Cone Model, which is a geometric framework based on the assumption that easier texts are less diverse (focusing on fundamental concepts), whereas harder texts are more diverse. This assumption leads to a cone-shaped distribution in the embedding space regardless of the embedding method used. The model frames the evaluation of embeddings as an optimization problem with the aim of detecting structured difficulty-based patterns. By designing specific loss functions, efficient closed-form solutions are derived that avoid costly computation. Empirical tests on real-world datasets validated the model's effectiveness and speed in identifying the embedding spaces that are best aligned with difficulty-annotated educational texts.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Computational Linguistics Meets Libyan Dialect: A Study on Dialect Identification</title>
<link>https://arxiv.org/abs/2512.04257</link>
<guid>https://arxiv.org/abs/2512.04257</guid>
<content:encoded><![CDATA[
arXiv:2512.04257v1 Announce Type: cross 
Abstract: This study investigates logistic regression, linear support vector machine, multinomial Naive Bayes, and Bernoulli Naive Bayes for classifying Libyan dialect utterances gathered from Twitter. The dataset used is the QADI corpus, which consists of 540,000 sentences across 18 Arabic dialects. Preprocessing challenges include handling inconsistent orthographic variations and non-standard spellings typical of the Libyan dialect. The chi-square analysis revealed that certain features, such as email mentions and emotion indicators, were not significantly associated with dialect classification and were thus excluded from further analysis. Two main experiments were conducted: (1) evaluating the significance of meta-features extracted from the corpus using the chi-square test and (2) assessing classifier performance using different word and character n-gram representations. The classification experiments showed that Multinomial Naive Bayes (MNB) achieved the highest accuracy of 85.89% and an F1-score of 0.85741 when using a (1,2) word n-gram and (1,5) character n-gram representation. In contrast, Logistic Regression and Linear SVM exhibited slightly lower performance, with maximum accuracies of 84.41% and 84.73%, respectively. Additional evaluation metrics, including log loss, Cohen kappa, and Matthew correlation coefficient, further supported the effectiveness of MNB in this task. The results indicate that carefully selected n-gram representations and classification models play a crucial role in improving the accuracy of Libyan dialect identification. This study provides empirical benchmarks and insights for future research in Arabic dialect NLP applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polynomiogram: An Integrated Framework for Root Visualization and Generative Art</title>
<link>https://arxiv.org/abs/2512.04263</link>
<guid>https://arxiv.org/abs/2512.04263</guid>
<content:encoded><![CDATA[
arXiv:2512.04263v1 Announce Type: cross 
Abstract: This work presents the Polynomiogram framework, an integrated computational platform for exploring, visualizing, and generating art from polynomial root systems. The main innovation is a flexible sampling scheme in which two independent parameters are drawn from user defined domains and mapped to the polynomial coefficients through a generating function. This design allows the same mathematical foundation to support both scientific investigation and generative algorithmic art. The framework integrates two complementary numerical engines: NumPy companion matrix solver for fast, large scale computation and MPSolve for high precision, scientifically rigorous validation. This dual architecture enables efficient visualization for creative use and accurate computation for research and education. Numerical accuracy was verified using classical ensembles, including the Kac and Lucas polynomials. The method was applied to the cubic polynomial system to analyze its bifurcation structure, demonstrating its value as both a scientific tool for exploring root phenomena and an educational aid for visualizing fundamental concepts in algebra and dynamical systems. Beyond analysis, the Polynomiogram also demonstrated its potential as a tool for personalized generative art. Examples include the use of the platform to generate a natural form resembling a hibiscus flower and to create personalized artwork expressing gratitude toward advances in artificial intelligence and large language models through a tribute composition.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Geometry of Benchmarks: A New Path Toward AGI</title>
<link>https://arxiv.org/abs/2512.04276</link>
<guid>https://arxiv.org/abs/2512.04276</guid>
<content:encoded><![CDATA[
arXiv:2512.04276v1 Announce Type: cross 
Abstract: Benchmarks are the primary tool for assessing progress in artificial intelligence (AI), yet current practice evaluates models on isolated test suites and provides little guidance for reasoning about generality or autonomous self-improvement. Here we introduce a geometric framework in which all psychometric batteries for AI agents are treated as points in a structured moduli space, and agent performance is described by capability functionals over this space. First, we define an Autonomous AI (AAI) Scale, a Kardashev-style hierarchy of autonomy grounded in measurable performance on batteries spanning families of tasks (for example reasoning, planning, tool use and long-horizon control). Second, we construct a moduli space of batteries, identifying equivalence classes of benchmarks that are indistinguishable at the level of agent orderings and capability inferences. This geometry yields determinacy results: dense families of batteries suffice to certify performance on entire regions of task space. Third, we introduce a general Generator-Verifier-Updater (GVU) operator that subsumes reinforcement learning, self-play, debate and verifier-based fine-tuning as special cases, and we define a self-improvement coefficient $\kappa$ as the Lie derivative of a capability functional along the induced flow. A variance inequality on the combined noise of generation and verification provides sufficient conditions for $\kappa > 0$. Our results suggest that progress toward artificial general intelligence (AGI) is best understood as a flow on moduli of benchmarks, driven by GVU dynamics rather than by scores on individual leaderboards.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Inference-time Stochastic Refinement of GRU-Normalizing Flow for Real-time Video Motion Transfer</title>
<link>https://arxiv.org/abs/2512.04282</link>
<guid>https://arxiv.org/abs/2512.04282</guid>
<content:encoded><![CDATA[
arXiv:2512.04282v1 Announce Type: cross 
Abstract: Real-time video motion transfer applications such as immersive gaming and vision-based anomaly detection require accurate yet diverse future predictions to support realistic synthesis and robust downstream decision making under uncertainty. To improve the diversity of such sequential forecasts we propose a novel inference-time refinement technique that combines Gated Recurrent Unit-Normalizing Flows (GRU-NF) with stochastic sampling methods. While GRU-NF can capture multimodal distributions through its integration of normalizing flows within a temporal forecasting framework, its deterministic transformation structure can limit expressivity. To address this, inspired by Stochastic Normalizing Flows (SNF), we introduce Markov Chain Monte Carlo (MCMC) steps during GRU-NF inference, enabling the model to explore a richer output space and better approximate the true data distribution without retraining. We validate our approach in a keypoint-based video motion transfer pipeline, where capturing temporally coherent and perceptually diverse future trajectories is essential for realistic samples and low bandwidth communication. Experiments show that our inference framework, Gated Recurrent Unit- Stochastic Normalizing Flows (GRU-SNF) outperforms GRU-NF in generating diverse outputs without sacrificing accuracy, even under longer prediction horizons. By injecting stochasticity during inference, our approach captures multimodal behavior more effectively. These results highlight the potential of integrating stochastic dynamics with flow-based sequence models for generative time series forecasting.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plug-and-Play Image Restoration with Flow Matching: A Continuous Viewpoint</title>
<link>https://arxiv.org/abs/2512.04283</link>
<guid>https://arxiv.org/abs/2512.04283</guid>
<content:encoded><![CDATA[
arXiv:2512.04283v1 Announce Type: cross 
Abstract: Flow matching-based generative models have been integrated into the plug-and-play image restoration framework, and the resulting plug-and-play flow matching (PnP-Flow) model has achieved some remarkable empirical success for image restoration. However, the theoretical understanding of PnP-Flow lags its empirical success. In this paper, we derive a continuous limit for PnP-Flow, resulting in a stochastic differential equation (SDE) surrogate model of PnP-Flow. The SDE model provides two particular insights to improve PnP-Flow for image restoration: (1) It enables us to quantify the error for image restoration, informing us to improve step scheduling and regularize the Lipschitz constant of the neural network-parameterized vector field for error reduction. (2) It informs us to accelerate off-the-shelf PnP-Flow models via extrapolation, resulting in a rescaled version of the proposed SDE model. We validate the efficacy of the SDE-informed improved PnP-Flow using several benchmark tasks, including image denoising, deblurring, super-resolution, and inpainting. Numerical results show that our method significantly outperforms the baseline PnP-Flow and other state-of-the-art approaches, achieving superior performance across evaluation metrics.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayes-DIC Net: Estimating Digital Image Correlation Uncertainty with Bayesian Neural Networks</title>
<link>https://arxiv.org/abs/2512.04323</link>
<guid>https://arxiv.org/abs/2512.04323</guid>
<content:encoded><![CDATA[
arXiv:2512.04323v1 Announce Type: cross 
Abstract: This paper introduces a novel method for generating high-quality Digital Image Correlation (DIC) dataset based on non-uniform B-spline surfaces. By randomly generating control point coordinates, we construct displacement fields that encompass a variety of realistic displacement scenarios, which are subsequently used to generate speckle pattern datasets. This approach enables the generation of a large-scale dataset that capture real-world displacement field situations, thereby enhancing the training and generalization capabilities of deep learning-based DIC algorithms. Additionally, we propose a novel network architecture, termed Bayes-DIC Net, which extracts information at multiple levels during the down-sampling phase and facilitates the aggregation of information across various levels through a single skip connection during the up-sampling phase. Bayes-DIC Net incorporates a series of lightweight convolutional blocks designed to expand the receptive field and capture rich contextual information while minimizing computational costs. Furthermore, by integrating appropriate dropout modules into Bayes-DIC Net and activating them during the network inference stage, Bayes-DIC Net is transformed into a Bayesian neural network. This transformation allows the network to provide not only predictive results but also confidence levels in these predictions when processing real unlabeled datasets. This feature significantly enhances the practicality and reliability of our network in real-world displacement field prediction tasks. Through these innovations, this paper offers new perspectives and methods for dataset generation and algorithm performance enhancement in the field of DIC.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One Detector Fits All: Robust and Adaptive Detection of Malicious Packages from PyPI to Enterprises</title>
<link>https://arxiv.org/abs/2512.04338</link>
<guid>https://arxiv.org/abs/2512.04338</guid>
<content:encoded><![CDATA[
arXiv:2512.04338v1 Announce Type: cross 
Abstract: The rise of supply chain attacks via malicious Python packages demands robust detection solutions. Current approaches, however, overlook two critical challenges: robustness against adversarial source code transformations and adaptability to the varying false positive rate (FPR) requirements of different actors, from repository maintainers (requiring low FPR) to enterprise security teams (higher FPR tolerance).
  We introduce a robust detector capable of seamless integration into both public repositories like PyPI and enterprise ecosystems. To ensure robustness, we propose a novel methodology for generating adversarial packages using fine-grained code obfuscation. Combining these with adversarial training (AT) enhances detector robustness by 2.5x. We comprehensively evaluate AT effectiveness by testing our detector against 122,398 packages collected daily from PyPI over 80 days, showing that AT needs careful application: it makes the detector more robust to obfuscations and allows finding 10% more obfuscated packages, but slightly decreases performance on non-obfuscated packages.
  We demonstrate production adaptability of our detector via two case studies: (i) one for PyPI maintainers (tuned at 0.1% FPR) and (ii) one for enterprise teams (tuned at 10% FPR). In the former, we analyze 91,949 packages collected from PyPI over 37 days, achieving a daily detection rate of 2.48 malicious packages with only 2.18 false positives. In the latter, we analyze 1,596 packages adopted by a multinational software company, obtaining only 1.24 false positives daily. These results show that our detector can be seamlessly integrated into both public repositories like PyPI and enterprise ecosystems, ensuring a very low time budget of a few minutes to review the false positives.
  Overall, we uncovered 346 malicious packages, now reported to the community.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Object and Action Hallucinations in Multimodal LLMs via Self-Augmented Contrastive Alignment</title>
<link>https://arxiv.org/abs/2512.04356</link>
<guid>https://arxiv.org/abs/2512.04356</guid>
<content:encoded><![CDATA[
arXiv:2512.04356v1 Announce Type: cross 
Abstract: Recent advancement in multimodal LLMs (MLLMs) has demonstrated their remarkable capability to generate descriptive captions for input videos. However, these models suffer from factual inaccuracies in the generated descriptions, causing severe hallucination issues. While prior works have explored alleviating hallucinations for static images, jointly mitigating visual object and temporal action hallucinations for dynamic videos remains a challenging and unsolved task. To tackle this challenge, we propose a Self-Augmented Contrastive Alignment (SANTA) framework for enabling object and action faithfulness by exempting the spurious correlations and enforcing the emphasis on visual facts. SANTA employs a hallucinative self-augmentation scheme to identify the potential hallucinations that lie in the MLLM and transform the original captions to the contrasted negatives. Furthermore, we develop a tracklet-phrase contrastive alignment to match the regional objects and relation-guided actions with their corresponding visual and temporal phrases. Extensive experiments demonstrate that SANTA outperforms existing methods in alleviating object and action hallucinations, yielding superior performance on the hallucination examination benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoGuard: A Self-Healing Proactive Security Layer for DevSecOps Pipelines Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.04368</link>
<guid>https://arxiv.org/abs/2512.04368</guid>
<content:encoded><![CDATA[
arXiv:2512.04368v1 Announce Type: cross 
Abstract: Contemporary DevSecOps pipelines have to deal with the evolution of security in an ever-continuously integrated and deployed environment. Existing methods,such as rule-based intrusion detection and static vulnerability scanning, are inadequate and unreceptive to changes in the system, causing longer response times and organization needs exposure to emerging attack vectors. In light of the previous constraints, we introduce AutoGuard to the DevSecOps ecosystem, a reinforcement learning (RL)-powered self-healing security framework built to pre-emptively protect DevSecOps environments. AutoGuard is a self-securing security environment that continuously observes pipeline activities for potential anomalies while preemptively remediating the environment. The model observes and reacts based on a policy that is continually learned dynamically over time. The RL agent improves each action over time through reward-based learning aimed at improving the agent's ability to prevent, detect and respond to a security incident in real-time. Testing using simulated ContinuousIntegration / Continuous Deployment (CI/CD) environments showed AutoGuard to successfully improve threat detection accuracy by 22%, reduce mean time torecovery (MTTR) for incidents by 38% and increase overall resilience to incidents as compared to traditional methods.
  Keywords- DevSecOps, Reinforcement Learning, Self- Healing Security, Continuous Integration, Automated Threat Mitigation
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructive Approximation under Carleman's Condition, with Applications to Smoothed Analysis</title>
<link>https://arxiv.org/abs/2512.04371</link>
<guid>https://arxiv.org/abs/2512.04371</guid>
<content:encoded><![CDATA[
arXiv:2512.04371v1 Announce Type: cross 
Abstract: A classical result of Carleman, based on the theory of quasianalytic functions, shows that polynomials are dense in $L^2(\mu)$ for any $\mu$ such that the moments $\int x^k d\mu$ do not grow too rapidly as $k \to \infty$. In this work, we develop a fairly tight quantitative analogue of the underlying Denjoy-Carleman theorem via complex analysis, and show that this allows for nonasymptotic control of the rate of approximation by polynomials for any smooth function with polynomial growth at infinity. In many cases, this allows us to establish $L^2$ approximation-theoretic results for functions over general classes of distributions (e.g., multivariate sub-Gaussian or sub-exponential distributions) which were previously known only in special cases. As one application, we show that the Paley--Wiener class of functions bandlimited to $[-\Omega,\Omega]$ admits superexponential rates of approximation over all strictly sub-exponential distributions, which leads to a new characterization of the class. As another application, we solve an open problem recently posed by Chandrasekaran, Klivans, Kontonis, Meka and Stavropoulos on the smoothed analysis of learning, and also obtain quantitative improvements to their main results and applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Informative missingness and its implications in semi-supervised learning</title>
<link>https://arxiv.org/abs/2512.04392</link>
<guid>https://arxiv.org/abs/2512.04392</guid>
<content:encoded><![CDATA[
arXiv:2512.04392v1 Announce Type: cross 
Abstract: Semi-supervised learning (SSL) constructs classifiers using both labelled and unlabelled data. It leverages information from labelled samples, whose acquisition is often costly or labour-intensive, together with unlabelled data to enhance prediction performance. This defines an incomplete-data problem, which statistically can be formulated within the likelihood framework for finite mixture models that can be fitted using the expectation-maximisation (EM) algorithm. Ideally, one would prefer a completely labelled sample, as one would anticipate that a labelled observation provides more information than an unlabelled one. However, when the mechanism governing label absence depends on the observed features or the class labels or both, the missingness indicators themselves contain useful information. In certain situations, the information gained from modelling the missing-label mechanism can even outweigh the loss due to missing labels, yielding a classifier with a smaller expected error than one based on a completely labelled sample analysed. This improvement arises particularly when class overlap is moderate, labelled data are sparse, and the missingness is informative. Modelling such informative missingness thus offers a coherent statistical framework that unifies likelihood-based inference with the behaviour of empirical SSL methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sarcasm Detection on Reddit Using Classical Machine Learning and Feature Engineering</title>
<link>https://arxiv.org/abs/2512.04396</link>
<guid>https://arxiv.org/abs/2512.04396</guid>
<content:encoded><![CDATA[
arXiv:2512.04396v1 Announce Type: cross 
Abstract: Sarcasm is common in online discussions, yet difficult for machines to identify because the intended meaning often contradicts the literal wording. In this work, I study sarcasm detection using only classical machine learning methods and explicit feature engineering, without relying on neural networks or context from parent comments. Using a 100,000-comment subsample of the Self-Annotated Reddit Corpus (SARC 2.0), I combine word-level and character-level TF-IDF features with simple stylistic indicators. Four models are evaluated: logistic regression, a linear SVM, multinomial Naive Bayes, and a random forest. Naive Bayes and logistic regression perform the strongest, achieving F1-scores around 0.57 for sarcastic comments. Although the lack of conversational context limits performance, the results offer a clear and reproducible baseline for sarcasm detection using lightweight and interpretable methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Time-Dependent Flow Over Complex Geometries Using Operator Networks</title>
<link>https://arxiv.org/abs/2512.04434</link>
<guid>https://arxiv.org/abs/2512.04434</guid>
<content:encoded><![CDATA[
arXiv:2512.04434v1 Announce Type: cross 
Abstract: Fast, geometry-generalizing surrogates for unsteady flow remain challenging. We present a time-dependent, geometry-aware Deep Operator Network that predicts velocity fields for moderate-Re flows around parametric and non-parametric shapes. The model encodes geometry via a signed distance field (SDF) trunk and flow history via a CNN branch, trained on 841 high-fidelity simulations. On held-out shapes, it attains $\sim 5\%$ relative L2 single-step error and up to 1000X speedups over CFD. We provide physics-centric rollout diagnostics, including phase error at probes and divergence norms, to quantify long-horizon fidelity. These reveal accurate near-term transients but error accumulation in fine-scale wakes, most pronounced for sharp-cornered geometries. We analyze failure modes and outline practical mitigations. Code, splits, and scripts are openly released at: https://github.com/baskargroup/TimeDependent-DeepONet to support reproducibility and benchmarking.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NORi: An ML-Augmented Ocean Boundary Layer Parameterization</title>
<link>https://arxiv.org/abs/2512.04452</link>
<guid>https://arxiv.org/abs/2512.04452</guid>
<content:encoded><![CDATA[
arXiv:2512.04452v1 Announce Type: cross 
Abstract: NORi is a machine-learned (ML) parameterization of ocean boundary layer turbulence that is physics-based and augmented with neural networks. NORi stands for neural ordinary differential equations (NODEs) Richardson number (Ri) closure. The physical parameterization is controlled by a Richardson number-dependent diffusivity and viscosity. The NODEs are trained to capture the entrainment through the base of the boundary layer, which cannot be represented with a local diffusive closure. The parameterization is trained using large-eddy simulations in an "a posteriori" fashion, where parameters are calibrated with a loss function that explicitly depends on the actual time-integrated variables of interest rather than the instantaneous subgrid fluxes, which are inherently noisy. NORi is designed for the realistic nonlinear equation of state of seawater and demonstrates excellent prediction and generalization capabilities in capturing entrainment dynamics under different convective strengths, oceanic background stratifications, rotation strengths, and surface wind forcings. NORi is numerically stable for at least 100 years of integration time in large-scale simulations, despite only being trained on 2-day horizons, and can be run with time steps as long as one hour. The highly expressive neural networks, combined with a physically-rigorous base closure, prove to be a robust paradigm for designing parameterizations for climate models where data requirements are drastically reduced, inference performance can be directly targeted and optimized, and numerical stability is implicitly encouraged during training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mathematical Framing for Different Agent Strategies</title>
<link>https://arxiv.org/abs/2512.04469</link>
<guid>https://arxiv.org/abs/2512.04469</guid>
<content:encoded><![CDATA[
arXiv:2512.04469v1 Announce Type: cross 
Abstract: We introduce a unified mathematical and probabilistic framework for understanding and comparing diverse AI agent strategies. We bridge the gap between high-level agent design concepts, such as ReAct, multi-agent systems, and control flows, and a rigorous mathematical formulation. Our approach frames agentic processes as a chain of probabilities, enabling a detailed analysis of how different strategies manipulate these probabilities to achieve desired outcomes. Our framework provides a common language for discussing the trade-offs inherent in various agent architectures. One of our many key contributions is the introduction of the "Degrees of Freedom" concept, which intuitively differentiates the optimizable levers available for each approach, thereby guiding the selection of appropriate strategies for specific tasks. This work aims to enhance the clarity and precision in designing and evaluating AI agents, offering insights into maximizing the probability of successful actions within complex agentic systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Robots Should Say "I Don't Know": Benchmarking Abstention in Embodied Question Answering</title>
<link>https://arxiv.org/abs/2512.04597</link>
<guid>https://arxiv.org/abs/2512.04597</guid>
<content:encoded><![CDATA[
arXiv:2512.04597v1 Announce Type: cross 
Abstract: Embodied Question Answering (EQA) requires an agent to interpret language, perceive its environment, and navigate within 3D scenes to produce responses. Existing EQA benchmarks assume that every question must be answered, but embodied agents should know when they do not have sufficient information to answer. In this work, we focus on a minimal requirement for EQA agents, abstention: knowing when to withhold an answer. From an initial study of 500 human queries, we find that 32.4% contain missing or underspecified context. Drawing on this initial study and cognitive theories of human communication errors, we derive five representative categories requiring abstention: actionability limitation, referential underspecification, preference dependence, information unavailability, and false presupposition. We augment OpenEQA by having annotators transform well-posed questions into ambiguous variants outlined by these categories. The resulting dataset, AbstainEQA, comprises 1,636 annotated abstention cases paired with 1,636 original OpenEQA instances for balanced evaluation. Evaluating on AbstainEQA, we find that even the best frontier model only attains 42.79% abstention recall, while humans achieve 91.17%. We also find that scaling, prompting, and reasoning only yield marginal gains, and that fine-tuned models overfit to textual cues. Together, these results position abstention as a fundamental prerequisite for reliable interaction in embodied settings and as a necessary basis for effective clarification.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SEASON: Mitigating Temporal Hallucination in Video Large Language Models via Self-Diagnostic Contrastive Decoding</title>
<link>https://arxiv.org/abs/2512.04643</link>
<guid>https://arxiv.org/abs/2512.04643</guid>
<content:encoded><![CDATA[
arXiv:2512.04643v1 Announce Type: cross 
Abstract: Video Large Language Models (VideoLLMs) have shown remarkable progress in video understanding. However, these models still struggle to effectively perceive and exploit rich temporal information in videos when responding to user queries. Therefore, they often generate descriptions of events that are temporal inconsistent or causally implausible, causing severe hallucination issues. While most prior studies have focused on spatial hallucinations (e.g. object mismatches), temporal reasoning in video understanding remains relatively underexplored. To address this issue, we propose Self-Diagnostic Contrastive Decoding (SEASON), a training-free method that adaptively enhances temporal and spatial faithfulness for each output token. It achieves this by dynamically diagnosing each token's hallucination tendency and applying adaptive contrastive decoding against its corresponding temporal and spatial negatives. Extensive experiments demonstrate that SEASON outperforms all existing training-free hallucination mitigation approaches on three hallucination examination benchmarks, while further improves VideoLLMs across four general video understanding benchmarks. The code will be released upon acceptance.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semi Centralized Training Decentralized Execution Architecture for Multi Agent Deep Reinforcement Learning in Traffic Signal Control</title>
<link>https://arxiv.org/abs/2512.04653</link>
<guid>https://arxiv.org/abs/2512.04653</guid>
<content:encoded><![CDATA[
arXiv:2512.04653v1 Announce Type: cross 
Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising paradigm for adaptive traffic signal control (ATSC) of multiple intersections. Existing approaches typically follow either a fully centralized or a fully decentralized design. Fully centralized approaches suffer from the curse of dimensionality, and reliance on a single learning server, whereas purely decentralized approaches operate under severe partial observability and lack explicit coordination resulting in suboptimal performance. These limitations motivate region-based MARL, where the network is partitioned into smaller, tightly coupled intersections that form regions, and training is organized around these regions. This paper introduces a Semi-Centralized Training, Decentralized Execution (SEMI-CTDE) architecture for multi intersection ATSC. Within each region, SEMI-CTDE performs centralized training with regional parameter sharing and employs composite state and reward formulations that jointly encode local and regional information. The architecture is highly transferable across different policy backbones and state-reward instantiations. Building on this architecture, we implement two models with distinct design objectives. A multi-perspective experimental analysis of the two implemented SEMI-CTDE-based models covering ablations of the architecture's core elements including rule based and fully decentralized baselines shows that they achieve consistently superior performance and remain effective across a wide range of traffic densities and distributions.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fermionic neural Gibbs states</title>
<link>https://arxiv.org/abs/2512.04663</link>
<guid>https://arxiv.org/abs/2512.04663</guid>
<content:encoded><![CDATA[
arXiv:2512.04663v1 Announce Type: cross 
Abstract: We introduce fermionic neural Gibbs states (fNGS), a variational framework for modeling finite-temperature properties of strongly interacting fermions. fNGS starts from a reference mean-field thermofield-double state and uses neural-network transformations together with imaginary-time evolution to systematically build strong correlations. Applied to the doped Fermi-Hubbard model, a minimal lattice model capturing essential features of strong electronic correlations, fNGS accurately reproduces thermal energies over a broad range of temperatures, interaction strengths, even at large dopings, for system sizes beyond the reach of exact methods. These results demonstrate a scalable route to studying finite-temperature properties of strongly correlated fermionic systems beyond one dimension with neural-network representations of quantum states.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Recurrent Neural Networks with Linear Structures for Electricity Price Forecasting</title>
<link>https://arxiv.org/abs/2512.04690</link>
<guid>https://arxiv.org/abs/2512.04690</guid>
<content:encoded><![CDATA[
arXiv:2512.04690v1 Announce Type: cross 
Abstract: We present a novel recurrent neural network architecture designed explicitly for day-ahead electricity price forecasting, aimed at improving short-term decision-making and operational management in energy systems. Our combined forecasting model embeds linear structures, such as expert models and Kalman filters, into recurrent networks, enabling efficient computation and enhanced interpretability. The design leverages the strengths of both linear and non-linear model structures, allowing it to capture all relevant stylised price characteristics in power markets, including calendar and autoregressive effects, as well as influences from load, renewable energy, and related fuel and carbon markets. For empirical testing, we use hourly data from the largest European electricity market spanning 2018 to 2025 in a comprehensive forecasting study, comparing our model against state-of-the-art approaches, particularly high-dimensional linear and neural network models. The proposed model achieves approximately 12% higher accuracy than leading benchmarks. We evaluate the contributions of the interpretable model components and conclude on the impact of combining linear and non-linear structures.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable FDR Control for Deep Feature Selection: Deep MLPs and Beyond</title>
<link>https://arxiv.org/abs/2512.04696</link>
<guid>https://arxiv.org/abs/2512.04696</guid>
<content:encoded><![CDATA[
arXiv:2512.04696v1 Announce Type: cross 
Abstract: We develop a flexible feature selection framework based on deep neural networks that approximately controls the false discovery rate (FDR), a measure of Type-I error. The method applies to architectures whose first layer is fully connected. From the second layer onward, it accommodates multilayer perceptrons (MLPs) of arbitrary width and depth, convolutional and recurrent networks, attention mechanisms, residual connections, and dropout. The procedure also accommodates stochastic gradient descent with data-independent initializations and learning rates. To the best of our knowledge, this is the first work to provide a theoretical guarantee of FDR control for feature selection within such a general deep learning setting.
  Our analysis is built upon a multi-index data-generating model and an asymptotic regime in which the feature dimension $n$ diverges faster than the latent dimension $q^{*}$, while the sample size, the number of training iterations, the network depth, and hidden layer widths are left unrestricted. Under this setting, we show that each coordinate of the gradient-based feature-importance vector admits a marginal normal approximation, thereby supporting the validity of asymptotic FDR control. As a theoretical limitation, we assume $\mathbf{B}$-right orthogonal invariance of the design matrix, and we discuss broader generalizations. We also present numerical experiments that underscore the theoretical findings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Continuous-time reinforcement learning for optimal switching over multiple regimes</title>
<link>https://arxiv.org/abs/2512.04697</link>
<guid>https://arxiv.org/abs/2512.04697</guid>
<content:encoded><![CDATA[
arXiv:2512.04697v1 Announce Type: cross 
Abstract: This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequential Enumeration in Large Language Models</title>
<link>https://arxiv.org/abs/2512.04727</link>
<guid>https://arxiv.org/abs/2512.04727</guid>
<content:encoded><![CDATA[
arXiv:2512.04727v1 Announce Type: cross 
Abstract: Reliably counting and generating sequences of items remain a significant challenge for neural networks, including Large Language Models (LLMs). Indeed, although this capability is readily handled by rule-based symbolic systems based on serial computation, learning to systematically deploy counting procedures is difficult for neural models, which should acquire these skills through learning. Previous research has demonstrated that recurrent architectures can only approximately track and enumerate sequences of events, and it remains unclear whether modern deep learning systems, including LLMs, can deploy systematic counting procedures over sequences of discrete symbols. This paper aims to fill this gap by investigating the sequential enumeration abilities of five state-of-the-art LLMs, including proprietary, open-source, and reasoning models. We probe LLMs in sequential naming and production tasks involving lists of letters and words, adopting a variety of prompting instructions to explore the role of chain-of-thought in the spontaneous emerging of counting strategies. We also evaluate open-source models with the same architecture but increasing size to see whether the mastering of counting principles follows scaling laws, and we analyze the embedding dynamics during sequential enumeration to investigate the emergent encoding of numerosity. We find that some LLMs are indeed capable of deploying counting procedures when explicitly prompted to do so, but none of them spontaneously engage in counting when simply asked to enumerate the number of items in a sequence. Our results suggest that, despite their impressive emergent abilities, LLMs cannot yet robustly and systematically deploy counting procedures, highlighting a persistent gap between neural and symbolic approaches to compositional generalization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Complementary Characterization of Agent-Based Models via Computational Mechanics and Diffusion Models</title>
<link>https://arxiv.org/abs/2512.04771</link>
<guid>https://arxiv.org/abs/2512.04771</guid>
<content:encoded><![CDATA[
arXiv:2512.04771v1 Announce Type: cross 
Abstract: This article extends the preprint "Characterizing Agent-Based Model Dynamics via $\epsilon$-Machines and Kolmogorov-Style Complexity" by introducing diffusion models as orthogonal and complementary tools for characterizing the output of agent-based models (ABMs). Where $\epsilon$-machines capture the predictive temporal structure and intrinsic computation of ABM-generated time series, diffusion models characterize high-dimensional cross-sectional distributions, learn underlying data manifolds, and enable synthetic generation of plausible population-level outcomes. We provide a formal analysis demonstrating that the two approaches operate on distinct mathematical domains -processes vs.\ distributions- and show that their combination yields a two-axis representation of ABM behavior based on temporal organization and distributional geometry. To our knowledge, this is the first framework to integrate computational mechanics with score-based generative modeling for the structural analysis of ABM outputs, thereby situating ABM characterization within the broader landscape of modern machine-learning methods for density estimation and intrinsic computation. The framework is validated using the same elder-caregiver ABM dataset introduced in the companion paper, and we provide precise definitions and propositions formalizing the mathematical complementarity between $\epsilon$-machines and diffusion models. This establishes a principled methodology for jointly analyzing temporal predictability and high-dimensional distributional structure in complex simulation models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pick-to-Learn for Systems and Control: Data-driven Synthesis with State-of-the-art Safety Guarantees</title>
<link>https://arxiv.org/abs/2512.04781</link>
<guid>https://arxiv.org/abs/2512.04781</guid>
<content:encoded><![CDATA[
arXiv:2512.04781v1 Announce Type: cross 
Abstract: Data-driven methods have become paramount in modern systems and control problems characterized by growing levels of complexity. In safety-critical environments, deploying these methods requires rigorous guarantees, a need that has motivated much recent work at the interface of statistical learning and control. However, many existing approaches achieve this goal at the cost of sacrificing valuable data for testing and calibration, or by constraining the choice of learning algorithm, thus leading to suboptimal performances. In this paper, we describe Pick-to-Learn (P2L) for Systems and Control, a framework that allows any data-driven control method to be equipped with state-of-the-art safety and performance guarantees. P2L enables the use of all available data to jointly synthesize and certify the design, eliminating the need to set aside data for calibration or validation purposes. In presenting a comprehensive version of P2L for systems and control, this paper demonstrates its effectiveness across a range of core problems, including optimal control, reachability analysis, safe synthesis, and robust control. In many of these applications, P2L delivers designs and certificates that outperform commonly employed methods, and shows strong potential for broad applicability in diverse practical settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contract-Driven QoE Auditing for Speech and Singing Services: From MOS Regression to Service Graphs</title>
<link>https://arxiv.org/abs/2512.04827</link>
<guid>https://arxiv.org/abs/2512.04827</guid>
<content:encoded><![CDATA[
arXiv:2512.04827v1 Announce Type: cross 
Abstract: Subjective mean opinion scores (MOS) remain the de-facto target for non-intrusive speech and singing quality assessment. However, MOS is a scalar that collapses heterogeneous user expectations, ignores service-level objectives, and is difficult to compare across deployment graphs. We propose a contract-driven QoE auditing framework: each service graph G is evaluated under a set of human-interpretable experience contracts C, yielding a contract-level satisfaction vector Q(G, C). We show that (i) classical MOS regression is a special case with a degenerate contract set, (ii) contract-driven quality is more stable than MOS under graph view transformations (e.g., pooling by system vs. by system type), and (iii) the effective sample complexity of learning contracts is governed by contract semantics rather than merely the dimensionality of C. We instantiate the framework on URGENT2024 MOS (6.9k speech utterances with raw rating vectors) and SingMOS v1 (7,981 singing clips; 80 systems). On URGENT, we train a contract-aware neural auditor on self-supervised WavLM embeddings; on SingMOS, we perform contract-driven graph auditing using released rating vectors and metadata without decoding audio. Empirically, our auditor matches strong MOS predictors in MOS accuracy while providing calibrated contract probabilities; on SingMOS, Q(G, C) exhibits substantially smaller cross-view drift than raw MOS and graph-only baselines; on URGENT, difficulty curves reveal that mis-specified "simple" contracts can be harder to learn than richer but better aligned contract sets.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tokenizing Buildings: A Transformer for Layout Synthesis</title>
<link>https://arxiv.org/abs/2512.04832</link>
<guid>https://arxiv.org/abs/2512.04832</guid>
<content:encoded><![CDATA[
arXiv:2512.04832v1 Announce Type: cross 
Abstract: We introduce Small Building Model (SBM), a Transformer-based architecture for layout synthesis in Building Information Modeling (BIM) scenes. We address the question of how to tokenize buildings by unifying heterogeneous feature sets of architectural elements into sequences while preserving compositional structure. Such feature sets are represented as a sparse attribute-feature matrix that captures room properties. We then design a unified embedding module that learns joint representations of categorical and possibly correlated continuous feature groups. Lastly, we train a single Transformer backbone in two modes: an encoder-only pathway that yields high-fidelity room embeddings, and an encoder-decoder pipeline for autoregressive prediction of room entities, referred to as Data-Driven Entity Prediction (DDEP). Experiments across retrieval and generative layout synthesis show that SBM learns compact room embeddings that reliably cluster by type and topology, enabling strong semantic retrieval. In DDEP mode, SBM produces functionally sound layouts, with fewer collisions and boundary violations and improved navigability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Series of quasi-uniform scatterings with fast search, root systems and neural network classifications</title>
<link>https://arxiv.org/abs/2512.04865</link>
<guid>https://arxiv.org/abs/2512.04865</guid>
<content:encoded><![CDATA[
arXiv:2512.04865v1 Announce Type: cross 
Abstract: In this paper we describe an approach to construct large extendable collections of vectors in predefined spaces of given dimensions. These collections are useful for neural network latent space configuration and training. For classification problem with large or unknown number of classes this allows to construct classifiers without classification layer and extend the number of classes without retraining of network from the very beginning. The construction allows to create large well-spaced vector collections in spaces of minimal possible dimension. If the number of classes is known or approximately predictable, one can choose sufficient enough vector collection size. If one needs to significantly extend the number of classes, one can extend the collection in the same latent space, or to incorporate the collection into collection of higher dimensions with same spacing between vectors. Also, regular symmetric structure of constructed vector collections can significantly simplify problems of search for nearest cluster centers or embeddings in the latent space. Construction of vector collections is based on combinatorics and geometry of semi-simple Lie groups irreducible representations with highest weight.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STELLA: Guiding Large Language Models for Time Series Forecasting with Semantic Abstractions</title>
<link>https://arxiv.org/abs/2512.04871</link>
<guid>https://arxiv.org/abs/2512.04871</guid>
<content:encoded><![CDATA[
arXiv:2512.04871v1 Announce Type: cross 
Abstract: Recent adaptations of Large Language Models (LLMs) for time series forecasting often fail to effectively enhance information for raw series, leaving LLM reasoning capabilities underutilized. Existing prompting strategies rely on static correlations rather than generative interpretations of dynamic behavior, lacking critical global and instance-specific context. To address this, we propose STELLA (Semantic-Temporal Alignment with Language Abstractions), a framework that systematically mines and injects structured supplementary and complementary information. STELLA employs a dynamic semantic abstraction mechanism that decouples input series into trend, seasonality, and residual components. It then translates intrinsic behavioral features of these components into Hierarchical Semantic Anchors: a Corpus-level Semantic Prior (CSP) for global context and a Fine-grained Behavioral Prompt (FBP) for instance-level patterns. Using these anchors as prefix-prompts, STELLA guides the LLM to model intrinsic dynamics. Experiments on eight benchmark datasets demonstrate that STELLA outperforms state-of-the-art methods in long- and short-term forecasting, showing superior generalization in zero-shot and few-shot settings. Ablation studies further validate the effectiveness of our dynamically generated semantic anchors.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Shorting Dynamics and Structured Kernel Regularization</title>
<link>https://arxiv.org/abs/2512.04874</link>
<guid>https://arxiv.org/abs/2512.04874</guid>
<content:encoded><![CDATA[
arXiv:2512.04874v1 Announce Type: cross 
Abstract: This paper develops a nonlinear operator dynamic that progressively removes the influence of a prescribed feature subspace while retaining maximal structure elsewhere. The induced sequence of positive operators is monotone, admits an exact residual decomposition, and converges to the classical shorted operator. Transporting this dynamic to reproducing kernel Hilbert spaces yields a corresponding family of kernels that converges to the largest kernel dominated by the original one and annihilating the given subspace. In the finite-sample setting, the associated Gram operators inherit a structured residual decomposition that leads to a canonical form of kernel ridge regression and a principled way to enforce nuisance invariance. This gives a unified operator-analytic approach to invariant kernel construction and structured regularization in data analysis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Environment-Aware Channel Inference via Cross-Modal Flow: From Multimodal Sensing to Wireless Channels</title>
<link>https://arxiv.org/abs/2512.04966</link>
<guid>https://arxiv.org/abs/2512.04966</guid>
<content:encoded><![CDATA[
arXiv:2512.04966v1 Announce Type: cross 
Abstract: Accurate channel state information (CSI) underpins reliable and efficient wireless communication. However, acquiring CSI via pilot estimation incurs substantial overhead, especially in massive multiple-input multiple-output (MIMO) systems operating in high-Doppler environments. By leveraging the growing availability of environmental sensing data, this treatise investigates pilot-free channel inference that estimates complete CSI directly from multimodal observations, including camera images, LiDAR point clouds, and GPS coordinates. In contrast to prior studies that rely on predefined channel models, we develop a data-driven framework that formulates the sensing-to-channel mapping as a cross-modal flow matching problem. The framework fuses multimodal features into a latent distribution within the channel domain, and learns a velocity field that continuously transforms the latent distribution toward the channel distribution. To make this formulation tractable and efficient, we reformulate the problem as an equivalent conditional flow matching objective and incorporate a modality alignment loss, while adopting low-latency inference mechanisms to enable real-time CSI estimation. In experiments, we build a procedural data generator based on Sionna and Blender to support realistic modeling of sensing scenes and wireless propagation. System-level evaluations demonstrate significant improvements over pilot- and sensing-based benchmarks in both channel estimation accuracy and spectral efficiency for the downstream beamforming task.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking the Use of Vision Transformers for AI-Generated Image Detection</title>
<link>https://arxiv.org/abs/2512.04969</link>
<guid>https://arxiv.org/abs/2512.04969</guid>
<content:encoded><![CDATA[
arXiv:2512.04969v1 Announce Type: cross 
Abstract: Rich feature representations derived from CLIP-ViT have been widely utilized in AI-generated image detection. While most existing methods primarily leverage features from the final layer, we systematically analyze the contributions of layer-wise features to this task. Our study reveals that earlier layers provide more localized and generalizable features, often surpassing the performance of final-layer features in detection tasks. Moreover, we find that different layers capture distinct aspects of the data, each contributing uniquely to AI-generated image detection. Motivated by these findings, we introduce a novel adaptive method, termed MoLD, which dynamically integrates features from multiple ViT layers using a gating-based mechanism. Extensive experiments on both GAN- and diffusion-generated images demonstrate that MoLD significantly improves detection performance, enhances generalization across diverse generative models, and exhibits robustness in real-world scenarios. Finally, we illustrate the scalability and versatility of our approach by successfully applying it to other pre-trained ViTs, such as DINOv2.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Causality for Longitudinal Data</title>
<link>https://arxiv.org/abs/2512.04980</link>
<guid>https://arxiv.org/abs/2512.04980</guid>
<content:encoded><![CDATA[
arXiv:2512.04980v1 Announce Type: cross 
Abstract: This thesis develops methods for causal inference and causal representation learning (CRL) in high-dimensional, time-varying data.
  The first contribution introduces the Causal Dynamic Variational Autoencoder (CDVAE), a model for estimating Individual Treatment Effects (ITEs) by capturing unobserved heterogeneity in treatment response driven by latent risk factors that affect only outcomes. CDVAE comes with theoretical guarantees on valid latent adjustment and generalization bounds for ITE error. Experiments on synthetic and real datasets show that CDVAE outperforms baselines, and that state-of-the-art models greatly improve when augmented with its latent substitutes, approaching oracle performance without access to true adjustment variables.
  The second contribution proposes an efficient framework for long-term counterfactual regression based on RNNs enhanced with Contrastive Predictive Coding (CPC) and InfoMax. It captures long-range dependencies under time-varying confounding while avoiding the computational cost of transformers, achieving state-of-the-art results and introducing CPC into causal inference.
  The third contribution advances CRL by addressing how latent causes manifest in observed variables. We introduce a model-agnostic interpretability layer based on the geometry of the decoder Jacobian. A sparse self-expression prior induces modular, possibly overlapping groups of observed features aligned with shared latent influences. We provide recovery guarantees in both disjoint and overlapping settings and show that meaningful latent-to-observed structure can be recovered without anchor features or single-parent assumptions. Scalable Jacobian-based regularization techniques are also developed.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligned but Stereotypical? The Hidden Influence of System Prompts on Social Bias in LVLM-Based Text-to-Image Models</title>
<link>https://arxiv.org/abs/2512.04981</link>
<guid>https://arxiv.org/abs/2512.04981</guid>
<content:encoded><![CDATA[
arXiv:2512.04981v1 Announce Type: cross 
Abstract: Large vision-language model (LVLM) based text-to-image (T2I) systems have become the dominant paradigm in image generation, yet whether they amplify social biases remains insufficiently understood. In this paper, we show that LVLM-based models produce markedly more socially biased images than non-LVLM-based models. We introduce a 1,024 prompt benchmark spanning four levels of linguistic complexity and evaluate demographic bias across multiple attributes in a systematic manner. Our analysis identifies system prompts, the predefined instructions guiding LVLMs, as a primary driver of biased behavior. Through decoded intermediate representations, token-probability diagnostics, and embedding-association analyses, we reveal how system prompts encode demographic priors that propagate into image synthesis. To this end, we propose FairPro, a training-free meta-prompting framework that enables LVLMs to self-audit and construct fairness-aware system prompts at test time. Experiments on two LVLM-based T2I models, SANA and Qwen-Image, show that FairPro substantially reduces demographic bias while preserving text-image alignment. We believe our findings provide deeper insight into the central role of system prompts in bias propagation and offer a practical, deployable approach for building more socially responsible T2I systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards a unified framework for guided diffusion models</title>
<link>https://arxiv.org/abs/2512.04985</link>
<guid>https://arxiv.org/abs/2512.04985</guid>
<content:encoded><![CDATA[
arXiv:2512.04985v1 Announce Type: cross 
Abstract: Guided or controlled data generation with diffusion models\blfootnote{Partial preliminary results of this work appeared in International Conference on Machine Learning 2025 \citep{li2025provable}.} has become a cornerstone of modern generative modeling. Despite substantial advances in diffusion model theory, the theoretical understanding of guided diffusion samplers remains severely limited. We make progress by developing a unified algorithmic and theoretical framework that accommodates both diffusion guidance and reward-guided diffusion. Aimed at fine-tuning diffusion models to improve certain rewards, we propose injecting a reward guidance term -- constructed from the difference between the original and reward-reweighted scores -- into the backward diffusion process, and rigorously quantify the resulting reward improvement over the unguided counterpart. As a key application, our framework shows that classifier-free guidance (CFG) decreases the expected reciprocal of the classifier probability, providing the first theoretical characterization of the specific performance metric that CFG improves for general target distributions. When applied to reward-guided diffusion, our framework yields a new sampler that is easy-to-train and requires no full diffusion trajectories during training. Numerical experiments further corroborate our theoretical findings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Evolutionary Architecture Search through Grammar-Based Sequence Alignment</title>
<link>https://arxiv.org/abs/2512.04992</link>
<guid>https://arxiv.org/abs/2512.04992</guid>
<content:encoded><![CDATA[
arXiv:2512.04992v1 Announce Type: cross 
Abstract: Neural architecture search (NAS) in expressive search spaces is a computationally hard problem, but it also holds the potential to automatically discover completely novel and performant architectures. To achieve this we need effective search algorithms that can identify powerful components and reuse them in new candidate architectures. In this paper, we introduce two adapted variants of the Smith-Waterman algorithm for local sequence alignment and use them to compute the edit distance in a grammar-based evolutionary architecture search. These algorithms enable us to efficiently calculate a distance metric for neural architectures and to generate a set of hybrid offspring from two parent models. This facilitates the deployment of crossover-based search heuristics, allows us to perform a thorough analysis on the architectural loss landscape, and track population diversity during search. We highlight how our method vastly improves computational complexity over previous work and enables us to efficiently compute shortest paths between architectures. When instantiating the crossover in evolutionary searches, we achieve competitive results, outperforming competing methods. Future work can build upon this new tool, discovering novel components that can be used more broadly across neural architecture design, and broadening its applications beyond NAS.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTR-ConvText: Leveraging Convolution and Textual Information for Handwritten Text Recognition</title>
<link>https://arxiv.org/abs/2512.05021</link>
<guid>https://arxiv.org/abs/2512.05021</guid>
<content:encoded><![CDATA[
arXiv:2512.05021v1 Announce Type: cross 
Abstract: Handwritten Text Recognition remains challenging due to the limited data, high writing style variance, and scripts with complex diacritics. Existing approaches, though partially address these issues, often struggle to generalize without massive synthetic data. To address these challenges, we propose HTR-ConvText, a model designed to capture fine-grained, stroke-level local features while preserving global contextual dependencies. In the feature extraction stage, we integrate a residual Convolutional Neural Network backbone with a MobileViT with Positional Encoding block. This enables the model to both capture structural patterns and learn subtle writing details. We then introduce the ConvText encoder, a hybrid architecture combining global context and local features within a hierarchical structure that reduces sequence length for improved efficiency. Additionally, an auxiliary module injects textual context to mitigate the weakness of Connectionist Temporal Classification. Evaluations on IAM, READ2016, LAM and HANDS-VNOnDB demonstrate that our approach achieves improved performance and better generalization compared to existing methods, especially in scenarios with limited training samples and high handwriting diversity.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Free Assessment of Simulator Fidelity via Quantile Curves</title>
<link>https://arxiv.org/abs/2512.05024</link>
<guid>https://arxiv.org/abs/2512.05024</guid>
<content:encoded><![CDATA[
arXiv:2512.05024v1 Announce Type: cross 
Abstract: Simulation of complex systems originated in manufacturing and queuing applications. It is now widely used for large-scale, ML-based systems in research, education, and consumer surveys. However, characterizing the discrepancy between simulators and ground truth remains challenging for increasingly complex, machine-learning-based systems. We propose a computationally tractable method to estimate the quantile function of the discrepancy between the simulated and ground-truth outcome distributions. Our approach focuses on output uncertainty and treats the simulator as a black box, imposing no modeling assumptions on its internals, and hence applies broadly across many parameter families, from Bernoulli and multinomial models to continuous, vector-valued settings. The resulting quantile curve supports confidence interval construction for unseen scenarios, risk-aware summaries of sim-to-real discrepancy (e.g., VaR/CVaR), and comparison of simulators' performance. We demonstrate our methodology in an application assessing LLM simulation fidelity on the WorldValueBench dataset spanning four LLMs.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Arbitrage: Efficient Reasoning via Advantage-Aware Speculation</title>
<link>https://arxiv.org/abs/2512.05033</link>
<guid>https://arxiv.org/abs/2512.05033</guid>
<content:encoded><![CDATA[
arXiv:2512.05033v1 Announce Type: cross 
Abstract: Modern Large Language Models achieve impressive reasoning capabilities with long Chain of Thoughts, but they incur substantial computational cost during inference, and this motivates techniques to improve the performance-cost ratio. Among these techniques, Speculative Decoding accelerates inference by employing a fast but inaccurate draft model to autoregressively propose tokens, which are then verified in parallel by a more capable target model. However, due to unnecessary rejections caused by token mismatches in semantically equivalent steps, traditional token-level Speculative Decoding struggles in reasoning tasks. Although recent works have shifted to step-level semantic verification, which improve efficiency by accepting or rejecting entire reasoning steps, existing step-level methods still regenerate many rejected steps with little improvement, wasting valuable target compute. To address this challenge, we propose Arbitrage, a novel step-level speculative generation framework that routes generation dynamically based on the relative advantage between draft and target models. Instead of applying a fixed acceptance threshold, Arbitrage uses a lightweight router trained to predict when the target model is likely to produce a meaningfully better step. This routing approximates an ideal Arbitrage Oracle that always chooses the higher-quality step, achieving near-optimal efficiency-accuracy trade-offs. Across multiple mathematical reasoning benchmarks, Arbitrage consistently surpasses prior step-level Speculative Decoding baselines, reducing inference latency by up to $\sim2\times$ at matched accuracy.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QKAN-LSTM: Quantum-inspired Kolmogorov-Arnold Long Short-term Memory</title>
<link>https://arxiv.org/abs/2512.05049</link>
<guid>https://arxiv.org/abs/2512.05049</guid>
<content:encoded><![CDATA[
arXiv:2512.05049v1 Announce Type: cross 
Abstract: Long short-term memory (LSTM) models are a particular type of recurrent neural networks (RNNs) that are central to sequential modeling tasks in domains such as urban telecommunication forecasting, where temporal correlations and nonlinear dependencies dominate. However, conventional LSTMs suffer from high parameter redundancy and limited nonlinear expressivity. In this work, we propose the Quantum-inspired Kolmogorov-Arnold Long Short-Term Memory (QKAN-LSTM), which integrates Data Re-Uploading Activation (DARUAN) modules into the gating structure of LSTMs. Each DARUAN acts as a quantum variational activation function (QVAF), enhancing frequency adaptability and enabling an exponentially enriched spectral representation without multi-qubit entanglement. The resulting architecture preserves quantum-level expressivity while remaining fully executable on classical hardware. Empirical evaluations on three datasets, Damped Simple Harmonic Motion, Bessel Function, and Urban Telecommunication, demonstrate that QKAN-LSTM achieves superior predictive accuracy and generalization with a 79% reduction in trainable parameters compared to classical LSTMs. We extend the framework to the Jiang-Huang-Chen-Goan Network (JHCG Net), which generalizes KAN to encoder-decoder structures, and then further use QKAN to realize the latent KAN, thereby creating a Hybrid QKAN (HQKAN) for hierarchical representation learning. The proposed HQKAN-LSTM thus provides a scalable and interpretable pathway toward quantum-inspired sequential modeling in real-world data environments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Meta-Learning for Quantum Optimization via Quantum Sequence Model</title>
<link>https://arxiv.org/abs/2512.05058</link>
<guid>https://arxiv.org/abs/2512.05058</guid>
<content:encoded><![CDATA[
arXiv:2512.05058v1 Announce Type: cross 
Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a leading approach for solving combinatorial optimization problems on near-term quantum processors. However, finding good variational parameters remains a significant challenge due to the non-convex energy landscape, often resulting in slow convergence and poor solution quality. In this work, we propose a quantum meta-learning framework that trains advanced quantum sequence models to generate effective parameter initialization policies. We investigate four classical or quantum sequence models, including the Quantum Kernel-based Long Short-Term Memory (QK-LSTM), as learned optimizers in a "learning to learn" paradigm. Our numerical experiments on the Max-Cut problem demonstrate that the QK-LSTM optimizer achieves superior performance, obtaining the highest approximation ratios and exhibiting the fastest convergence rate across all tested problem sizes (n=10 to 13). Crucially, the QK-LSTM model achieves perfect parameter transferability by synthesizing a single, fixed set of near-optimal parameters, leading to a remarkable sustained acceleration of convergence even when generalizing to larger problems. This capability, enabled by the compact and expressive power of the quantum kernel architecture, underscores its effectiveness. The QK-LSTM, with only 43 trainable parameters, substantially outperforms the classical LSTM (56 parameters) and other quantum sequence models, establishing a robust pathway toward highly efficient parameter initialization for variational quantum algorithms in the NISQ era.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Control Consistency Losses for Diffusion Bridges</title>
<link>https://arxiv.org/abs/2512.05070</link>
<guid>https://arxiv.org/abs/2512.05070</guid>
<content:encoded><![CDATA[
arXiv:2512.05070v1 Announce Type: cross 
Abstract: Simulating the conditioned dynamics of diffusion processes, given their initial and terminal states, is an important but challenging problem in the sciences. The difficulty is particularly pronounced for rare events, for which the unconditioned dynamics rarely reach the terminal state. In this work, we leverage a self-consistency property of the conditioned dynamics to learn the diffusion bridge in an iterative online manner, and demonstrate promising empirical results in a range of settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Foundations of Diffusion Models in General State Spaces: A Self-Contained Introduction</title>
<link>https://arxiv.org/abs/2512.05092</link>
<guid>https://arxiv.org/abs/2512.05092</guid>
<content:encoded><![CDATA[
arXiv:2512.05092v1 Announce Type: cross 
Abstract: Although diffusion models now occupy a central place in generative modeling, introductory treatments commonly assume Euclidean data and seldom clarify their connection to discrete-state analogues. This article is a self-contained primer on diffusion over general state spaces, unifying continuous domains and discrete/categorical structures under one lens. We develop the discrete-time view (forward noising via Markov kernels and learned reverse dynamics) alongside its continuous-time limits -- stochastic differential equations (SDEs) in $\mathbb{R}^d$ and continuous-time Markov chains (CTMCs) on finite alphabets -- and derive the associated Fokker--Planck and master equations. A common variational treatment yields the ELBO that underpins standard training losses. We make explicit how forward corruption choices -- Gaussian processes in continuous spaces and structured categorical transition kernels (uniform, masking/absorbing and more) in discrete spaces -- shape reverse dynamics and the ELBO. The presentation is layered for three audiences: newcomers seeking a self-contained intuitive introduction; diffusion practitioners wanting a global theoretical synthesis; and continuous-diffusion experts looking for an analogy-first path into discrete diffusion. The result is a unified roadmap to modern diffusion methodology across continuous domains and discrete sequences, highlighting a compact set of reusable proofs, identities, and core theoretical principles.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structured Document Translation via Format Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05100</link>
<guid>https://arxiv.org/abs/2512.05100</guid>
<content:encoded><![CDATA[
arXiv:2512.05100v1 Announce Type: cross 
Abstract: Recent works on structured text translation remain limited to the sentence level, as they struggle to effectively handle the complex document-level XML or HTML structures. To address this, we propose \textbf{Format Reinforcement Learning (FormatRL)}, which employs Group Relative Policy Optimization on top of a supervised fine-tuning model to directly optimize novel structure-aware rewards: 1) TreeSim, which measures structural similarity between predicted and reference XML trees and 2) Node-chrF, which measures translation quality at the level of XML nodes. Additionally, we apply StrucAUC, a fine-grained metric distinguishing between minor errors and major structural failures. Experiments on the SAP software-documentation benchmark demonstrate improvements across six metrics and an analysis further shows how different reward functions contribute to improvements in both structural and translation quality.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Semantic Soft Bootstrapping: Long Context Reasoning in LLMs without Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.05105</link>
<guid>https://arxiv.org/abs/2512.05105</guid>
<content:encoded><![CDATA[
arXiv:2512.05105v1 Announce Type: cross 
Abstract: Long context reasoning in large language models (LLMs) has demonstrated enhancement of their cognitive capabilities via chain-of-thought (CoT) inference. Training such models is usually done via reinforcement learning with verifiable rewards (RLVR) in reasoning based problems, like math and programming. However, RLVR is limited by several bottlenecks, such as, lack of dense reward, and inadequate sample efficiency. As a result, it requires significant compute resources in post-training phase. To overcome these limitations, in this work, we propose \textbf{Semantic Soft Bootstrapping (SSB)}, a self-distillation technique, in which the same base language model plays the role of both teacher and student, but receives different semantic contexts about the correctness of its outcome at training time. The model is first prompted with a math problem and several rollouts are generated. From them, the correct and most common incorrect response are filtered, and then provided to the model in context to produce a more robust, step-by-step explanation with a verified final answer. This pipeline automatically curates a paired teacher-student training set from raw problem-answer data, without any human intervention. This generation process also produces a sequence of logits, which is what the student model tries to match in the training phase just from the bare question alone. In our experiment, Qwen2.5-3B-Instruct on GSM8K dataset via parameter-efficient fine-tuning. We then tested its accuracy on MATH500, and AIME2024 benchmarks. Our experiments show a jump of 10.6%, and 10% improvements in accuracy, respectively, over group relative policy optimization (GRPO), which is a commonly used RLVR algorithm. Our code is available at https://github.com/purbeshmitra/semantic-soft-bootstrapping, and the model, curated dataset is available at https://huggingface.co/purbeshmitra/semantic-soft-bootstrapping.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuralRemaster: Phase-Preserving Diffusion for Structure-Aligned Generation</title>
<link>https://arxiv.org/abs/2512.05106</link>
<guid>https://arxiv.org/abs/2512.05106</guid>
<content:encoded><![CDATA[
arXiv:2512.05106v1 Announce Type: cross 
Abstract: Standard diffusion corrupts data using Gaussian noise whose Fourier coefficients have random magnitudes and random phases. While effective for unconditional or text-to-image generation, corrupting phase components destroys spatial structure, making it ill-suited for tasks requiring geometric consistency, such as re-rendering, simulation enhancement, and image-to-image translation. We introduce Phase-Preserving Diffusion {\phi}-PD, a model-agnostic reformulation of the diffusion process that preserves input phase while randomizing magnitude, enabling structure-aligned generation without architectural changes or additional parameters. We further propose Frequency-Selective Structured (FSS) noise, which provides continuous control over structural rigidity via a single frequency-cutoff parameter. {\phi}-PD adds no inference-time cost and is compatible with any diffusion model for images or videos. Across photorealistic and stylized re-rendering, as well as sim-to-real enhancement for driving planners, {\phi}-PD produces controllable, spatially aligned results. When applied to the CARLA simulator, {\phi}-PD improves CARLA-to-Waymo planner performance by 50\%. The method is complementary to existing conditioning approaches and broadly applicable to image-to-image and video-to-video generation. Videos, additional examples, and code are available on our \href{https://yuzeng-at-tri.github.io/ppd-page/}{project page}.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DraCo: Draft as CoT for Text-to-Image Preview and Rare Concept Generation</title>
<link>https://arxiv.org/abs/2512.05112</link>
<guid>https://arxiv.org/abs/2512.05112</guid>
<content:encoded><![CDATA[
arXiv:2512.05112v1 Announce Type: cross 
Abstract: Recent unified multimodal large language models (MLLMs) have shown impressive capabilities, incorporating chain-of-thought (CoT) reasoning for enhanced text-to-image generation. However, existing approaches remain limited, either treating the model merely as a standalone generator or relying on abstract textual planning. To this end, we propose Draft-as-CoT (DraCo), a novel interleaved reasoning paradigm that fully leverages both textual and visual contents in CoT for better planning and verification. Our method first generates a low-resolution draft image as preview, providing more concrete and structural visual planning and guidance. Then, we employ the model's inherent understanding capability to verify potential semantic misalignments between the draft and input prompt, and performs refinement through selective corrections with super-resolution. In this way, our approach addresses two fundamental challenges: the coarse-grained nature of textual planning and the difficulty in generating rare attribute combinations. To support training, we curate DraCo-240K, aiming to enhance three atomic capabilities spanning general correction, instance manipulation, and layout reorganization. Supported by DraCo-CFG, a specialized classifier-free guidance (CFG) strategy for interleaved reasoning, DraCo achieves a tremendous increase on GenEval (+8%), Imagine-Bench (+0.91), and GenEval++ (+3%), significantly outperforming direct generation and other generation methods empowered by CoT.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe Online Bid Optimization with Return on Investment and Budget Constraints</title>
<link>https://arxiv.org/abs/2201.07139</link>
<guid>https://arxiv.org/abs/2201.07139</guid>
<content:encoded><![CDATA[
arXiv:2201.07139v2 Announce Type: replace 
Abstract: In online marketing, the advertisers aim to balance achieving high volumes and high profitability. The companies' business units address this tradeoff by maximizing the volumes while guaranteeing a minimum Return On Investment (ROI) level. Such a task can be naturally modeled as a combinatorial optimization problem subject to ROI and budget constraints that can be solved online. In this picture, the learner's uncertainty over the constraints' parameters plays a crucial role since the algorithms' exploration choices might lead to their violation during the entire learning process. Such violations represent a major obstacle to adopting online techniques in real-world applications. Thus, controlling the algorithms' exploration during learning is paramount to making humans trust online learning tools. This paper studies the nature of both optimization and learning problems. In particular, we show that the learning problem is inapproximable within any factor (unless P = NP) and provide a pseudo-polynomial-time algorithm to solve its discretized version. Subsequently, we prove that no online learning algorithm can violate the (ROI or budget) constraints a sublinear number of times during the learning process while guaranteeing a sublinear regret. We provide the $GCB$ algorithm that guarantees sublinear regret at the cost of a linear number of constraint violations and $GCB_{safe}$ that guarantees w.h.p. a constant upper bound on the number of constraint violations at the cost of a linear regret. Moreover, we designed $GCB_{safe}(\psi,\phi)$, which guarantees both sublinear regret and safety w.h.p. at the cost of accepting tolerances $\psi$ and $\phi$ in the satisfaction of the ROI and budget constraints, respectively. Finally, we provide experimental results to compare the regret and constraint violations of $GCB$, $GCB_{safe}$, and $GCB_{safe}(\psi,\phi)$.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ImageNot: A contrast with ImageNet preserves model rankings</title>
<link>https://arxiv.org/abs/2404.02112</link>
<guid>https://arxiv.org/abs/2404.02112</guid>
<content:encoded><![CDATA[
arXiv:2404.02112v2 Announce Type: replace 
Abstract: We introduce ImageNot, a dataset constructed explicitly to be drastically different than ImageNet while matching its scale. ImageNot is designed to test the external validity of deep learning progress on ImageNet. We show that key model architectures developed for ImageNet over the years rank identically to how they rank on ImageNet when trained from scratch and evaluated on ImageNot. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models when trained and evaluated on an entirely different dataset. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FusionBench: A Unified Library and Comprehensive Benchmark for Deep Model Fusion</title>
<link>https://arxiv.org/abs/2406.03280</link>
<guid>https://arxiv.org/abs/2406.03280</guid>
<content:encoded><![CDATA[
arXiv:2406.03280v4 Announce Type: replace 
Abstract: Deep model fusion is an emerging technique that unifies the predictions or parameters of several deep neural networks into a single better-performing model in a cost-effective and data-efficient manner. Although a variety of deep model fusion techniques have been introduced, their evaluations tend to be inconsistent and often inadequate to validate their effectiveness and robustness. We present FusionBench, the first benchmark and a unified library designed specifically for deep model fusion. Our benchmark consists of multiple tasks, each with different settings of models and datasets. This variety allows us to compare fusion methods across different scenarios and model scales. Additionally, FusionBench serves as a unified library for easy implementation and testing of new fusion techniques. FusionBench is open source and actively maintained, with community contributions encouraged. Homepage https://github.com/tanganke/fusion_bench
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalizability of experimental studies</title>
<link>https://arxiv.org/abs/2406.17374</link>
<guid>https://arxiv.org/abs/2406.17374</guid>
<content:encoded><![CDATA[
arXiv:2406.17374v3 Announce Type: replace 
Abstract: Experimental studies are a cornerstone of Machine Learning (ML) research. A common and often implicit assumption is that the study's results will generalize beyond the study itself, e.g., to new data. That is, repeating the same study under different conditions will likely yield similar results. Existing frameworks to measure generalizability, borrowed from the casual inference literature, cannot capture the complexity of the results and the goals of an ML study. The problem of measuring generalizability in the more general ML setting is thus still open, also due to the lack of a mathematical formalization of experimental studies. In this paper, we propose such a formalization, use it to develop a framework to quantify generalizability, and propose an instantiation based on rankings and the Maximum Mean Discrepancy. We show how our framework offers insights into the number of experiments necessary for a generalizable study, and how experimenters can benefit from it. Finally, we release the genexpy Python package, which allows for an effortless evaluation of the generalizability of other experimental studies.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NITRO-D: Native Integer-only Training of Deep Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2407.11698</link>
<guid>https://arxiv.org/abs/2407.11698</guid>
<content:encoded><![CDATA[
arXiv:2407.11698v3 Announce Type: replace 
Abstract: Quantization is a pivotal technique for managing the growing computational and memory demands of Deep Neural Networks (DNNs). By reducing the number of bits used to represent weights and activations (typically from 32-bit Floating-Point (FP) to 16-bit or 8-bit integers), quantization reduces memory footprint, energy consumption, and execution time of DNNs. However, most existing methods typically target DNN inference, while training still relies on FP operations, limiting applicability in environments where FP arithmetic is unavailable. To date, only one prior work has addressed integer-only training, and only for Multi-Layer Perceptron (MLP) architectures. This paper introduces NITRO-D, a novel framework for training deep integer-only Convolutional Neural Networks (CNNs) that operate entirely in the integer domain for both training and inference. NITRO-D enables training of integer CNNs without requiring a separate quantization scheme. Specifically, it introduces a novel architecture that integrates multiple local-loss blocks, which include the proposed NITRO-Scaling layer and NITRO-ReLU activation function. The proposed framework also features a novel learning algorithm that employs local error signals and leverages IntegerSGD, an optimizer specifically designed for integer computations. NITRO-D is implemented as an open-source Python library. Extensive evaluations on state-of-the-art image recognition datasets demonstrate its effectiveness. For integer-only MLPs, NITRO-D improves test accuracy by up to +5.96% over the state-of-the-art. It also successfully trains integer-only CNNs, reducing memory requirements and energy consumption by up to 76.14% and 32.42%, respectively, compared to the traditional FP backpropagation algorithm.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence Analysis for Deep Sparse Coding via Convolutional Neural Networks</title>
<link>https://arxiv.org/abs/2408.05540</link>
<guid>https://arxiv.org/abs/2408.05540</guid>
<content:encoded><![CDATA[
arXiv:2408.05540v3 Announce Type: replace 
Abstract: In this work, we explore the intersection of sparse coding theory and deep learning to enhance our understanding of feature extraction capabilities in advanced neural network architectures. We begin by introducing a novel class of Deep Sparse Coding (DSC) models and establish a thorough theoretical analysis of their uniqueness and stability properties. By applying iterative algorithms to these DSC models, we derive convergence rates for convolutional neural networks (CNNs) in their ability to extract sparse features. This provides a strong theoretical foundation for the use of CNNs in sparse feature-learning tasks. We additionally extend this convergence analysis to more general neural network architectures, including those with diverse activation functions, as well as self-attention and transformer-based models. This broadens the applicability of our findings to a wide range of deep learning methods for the extraction of deep-sparse features. Inspired by the strong connection between sparse coding and CNNs, we also explore training strategies to encourage neural networks to learn sparser features. Through numerical experiments, we demonstrate the effectiveness of these approaches, providing valuable insight for the design of efficient and interpretable deep learning models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing common misinterpretations of KART and UAT in neural network literature</title>
<link>https://arxiv.org/abs/2408.16389</link>
<guid>https://arxiv.org/abs/2408.16389</guid>
<content:encoded><![CDATA[
arXiv:2408.16389v5 Announce Type: replace 
Abstract: This note addresses the Kolmogorov-Arnold Representation Theorem (KART) and the Universal Approximation Theorem (UAT), focusing on their frequent misinterpretations found in the neural network literature. Our remarks aim to support a more accurate understanding of KART and UAT among neural network specialists. In addition, we explore the minimal number of neurons required for universal approximation, showing that the same number of neurons needed for exact representation of functions in KART-based networks also suffices for standard multilayer perceptrons in the context of approximation.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Concept Bottleneck Models with LLM Priors</title>
<link>https://arxiv.org/abs/2410.15555</link>
<guid>https://arxiv.org/abs/2410.15555</guid>
<content:encoded><![CDATA[
arXiv:2410.15555v3 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) have been proposed as a compromise between white-box and black-box models, aiming to achieve interpretability without sacrificing accuracy. The standard training procedure for CBMs is to predefine a candidate set of human-interpretable concepts, extract their values from the training data, and identify a sparse subset as inputs to a transparent prediction model. However, such approaches are often hampered by the tradeoff between exploring a sufficiently large set of concepts versus controlling the cost of obtaining concept extractions, resulting in a large interpretability-accuracy tradeoff. This work investigates a novel approach that sidesteps these challenges: BC-LLM iteratively searches over a potentially infinite set of concepts within a Bayesian framework, in which Large Language Models (LLMs) serve as both a concept extraction mechanism and prior. Even though LLMs can be miscalibrated and hallucinate, we prove that BC-LLM can provide rigorous statistical inference and uncertainty quantification. Across image, text, and tabular datasets, BC-LLM outperforms interpretable baselines and even black-box models in certain settings, converges more rapidly towards relevant concepts, and is more robust to out-of-distribution samples.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unsupervised Time Series Anomaly Prediction with Importance-based Generative Contrastive Learning</title>
<link>https://arxiv.org/abs/2410.16888</link>
<guid>https://arxiv.org/abs/2410.16888</guid>
<content:encoded><![CDATA[
arXiv:2410.16888v3 Announce Type: replace 
Abstract: Time series anomaly prediction plays an essential role in many real-world scenarios, such as environmental prevention and prompt maintenance of cyber-physical systems. However, existing time series anomaly prediction methods mainly require supervised training with plenty of manually labeled data, which are difficult to obtain in practice. Besides, unseen anomalies can occur during inference, which could differ from the labeled training data and make these models fail to predict such new anomalies. In this paper, we study a novel problem of unsupervised time series anomaly prediction. We provide a theoretical analysis and propose Importance-based Generative Contrastive Learning (IGCL) to address the aforementioned problems. IGCL distinguishes between normal and anomaly precursors, which are generated by our anomaly precursor pattern generation module. To address the efficiency issues caused by the potential complex anomaly precursor combinations, we propose a memory bank with importance-based scores to adaptively store representative anomaly precursors and generate more complicated anomaly precursors. Extensive experiments on seven benchmark datasets show our method outperforms state-of-the-art baselines on unsupervised time series anomaly prediction problems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ArterialNet: Reconstructing Arterial Blood Pressure Waveform with Wearable Pulsatile Signals, a Cohort-Aware Approach</title>
<link>https://arxiv.org/abs/2410.18895</link>
<guid>https://arxiv.org/abs/2410.18895</guid>
<content:encoded><![CDATA[
arXiv:2410.18895v3 Announce Type: replace 
Abstract: Goal: Continuous arterial blood pressure (ABP) waveform is invasive but essential for hemodynamic monitoring. Current non-invasive techniques reconstruct ABP waveforms with pulsatile signals but derived inaccurate systolic and diastolic blood pressure (SBP/DBP) and were sensitive to individual variability. Methods: ArterialNet integrates generalized pulsatile-to-ABP signal translation and personalized feature extraction using hybrid loss functions and regularizations. Results: ArterialNet achieved a root mean square error (RMSE) of 5.41 -+ 1.35 mmHg on MIMIC-III, achieving 58% lower standard deviation than existing signal translation techniques. ArterialNet also reconstructed ABP with RMSE of 7.99 -+ 1.91 mmHg in remote health scenario. Conclusion: ArterialNet achieved superior performance in ABP reconstruction and SBP/DBP estimations with significantly reduced subject variance, demonstrating its potential in remote health settings. We also ablated ArterialNet's architecture to investigate contributions of each component and evaluated ArterialNet's translational impact and robustness by conducting a series of ablations on data quality and availability.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoK: Decentralized AI (DeAI)</title>
<link>https://arxiv.org/abs/2411.17461</link>
<guid>https://arxiv.org/abs/2411.17461</guid>
<content:encoded><![CDATA[
arXiv:2411.17461v4 Announce Type: replace 
Abstract: Centralization enhances the efficiency of Artificial Intelligence (AI) but also introduces critical challenges, including single points of failure, inherent biases, data privacy risks, and scalability limitations. To address these issues, blockchain-based Decentralized Artificial Intelligence (DeAI) has emerged as a promising paradigm that leverages decentralization and transparency to improve the trustworthiness of AI systems. Despite rapid adoption in industry, the academic community lacks a systematic analysis of DeAI's technical foundations, opportunities, and challenges. This work presents the first Systematization of Knowledge (SoK) on DeAI, offering a formal definition, a taxonomy of existing solutions based on the AI lifecycle, and an in-depth investigation of the roles of blockchain in enabling secure and incentive-compatible collaboration. We further review security risks across the DeAI lifecycle and empirically evaluate representative mitigation techniques. Finally, we highlight open research challenges and future directions for advancing blockchain-based DeAI.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Extending Graph Condensation to Multi-Label Datasets: A Benchmark Study</title>
<link>https://arxiv.org/abs/2412.17961</link>
<guid>https://arxiv.org/abs/2412.17961</guid>
<content:encoded><![CDATA[
arXiv:2412.17961v2 Announce Type: replace 
Abstract: As graph data grows increasingly complicate, training graph neural networks (GNNs) on large-scale datasets presents significant challenges, including computational resource constraints, data redundancy, and transmission inefficiencies. While existing graph condensation techniques have shown promise in addressing these issues, they are predominantly designed for single-label datasets, where each node is associated with a single class label. However, many real-world applications, such as social network analysis and bioinformatics, involve multi-label graph datasets, where one node can have various related labels. To deal with this problem, we extends traditional graph condensation approaches to accommodate multi-label datasets by introducing modifications to synthetic dataset initialization and condensing optimization. Through experiments on eight real-world multi-label graph datasets, we prove the effectiveness of our method. In experiment, the GCond framework, combined with K-Center initialization and binary cross-entropy loss (BCELoss), achieves best performance in general. This benchmark for multi-label graph condensation not only enhances the scalability and efficiency of GNNs for multi-label graph data, but also offering substantial benefits for diverse real-world applications.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detection of AI Deepfake and Fraud in Online Payments Using GAN-Based Models</title>
<link>https://arxiv.org/abs/2501.07033</link>
<guid>https://arxiv.org/abs/2501.07033</guid>
<content:encoded><![CDATA[
arXiv:2501.07033v2 Announce Type: replace 
Abstract: This study explores the use of Generative Adversarial Networks (GANs) to detect AI deepfakes and fraudulent activities in online payment systems. With the growing prevalence of deepfake technology, which can manipulate facial features in images and videos, the potential for fraud in online transactions has escalated. Traditional security systems struggle to identify these sophisticated forms of fraud. This research proposes a novel GAN-based model that enhances online payment security by identifying subtle manipulations in payment images. The model is trained on a dataset consisting of real-world online payment images and deepfake images generated using advanced GAN architectures, such as StyleGAN and DeepFake. The results demonstrate that the proposed model can accurately distinguish between legitimate transactions and deepfakes, achieving a high detection rate above 95%. This approach significantly improves the robustness of payment systems against AI-driven fraud. The paper contributes to the growing field of digital security, offering insights into the application of GANs for fraud detection in financial services. Keywords- Payment Security, Image Recognition, Generative Adversarial Networks, AI Deepfake, Fraudulent Activities
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Renewable Energy Prediction: A Comparative Study of Deep Learning Models for Complex Dataset Analysis</title>
<link>https://arxiv.org/abs/2501.15731</link>
<guid>https://arxiv.org/abs/2501.15731</guid>
<content:encoded><![CDATA[
arXiv:2501.15731v2 Announce Type: replace 
Abstract: The increasing focus on predicting renewable energy production aligns with advancements in deep learning (DL). The inherent variability of renewable sources and the complexity of prediction methods require robust approaches, such as DL models, in the renewable energy sector. DL models are preferred over traditional machine learning (ML) because they capture complex, nonlinear relationships in renewable energy datasets. This study examines key factors influencing DL technique accuracy, including sampling and hyperparameter optimization, by comparing various methods and training and test ratios within a DL framework. Seven machine learning methods, LSTM, Stacked LSTM, CNN, CNN-LSTM, DNN, Time-Distributed MLP (TD-MLP), and Autoencoder (AE), are evaluated using a dataset combining weather and photovoltaic power output data from 12 locations. Regularization techniques such as early stopping, neuron dropout, L1 and L2 regularization are applied to address overfitting. The results demonstrate that the combination of early stopping, dropout, and L1 regularization provides the best performance to reduce overfitting in the CNN and TD-MLP models with larger training set, while the combination of early stopping, dropout, and L2 regularization is the most effective to reduce the overfitting in CNN-LSTM and AE models with smaller training set.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Product Provenance Verification using Data Valuation Methods</title>
<link>https://arxiv.org/abs/2502.15177</link>
<guid>https://arxiv.org/abs/2502.15177</guid>
<content:encoded><![CDATA[
arXiv:2502.15177v3 Announce Type: replace 
Abstract: Determining and verifying product provenance remains a critical challenge in global supply chains, particularly as geopolitical conflicts and shifting borders create new incentives for misrepresentation of commodities, such as hiding the origin of illegally harvested timber or stolen agricultural products. Stable Isotope Ratio Analysis (SIRA), combined with Gaussian process regression-based isoscapes, has emerged as a powerful tool for geographic origin verification. While these models are now actively deployed in operational settings supporting regulators, certification bodies, and companies, they remain constrained by data scarcity and suboptimal dataset selection. In this work, we introduce a novel deployed data valuation framework designed to enhance the selection and utilization of training data for machine learning models applied in SIRA. By quantifying the marginal utility of individual samples using Shapley values, our method guides strategic, cost-effective, and robust sampling campaigns within active monitoring programs. By prioritizing high-informative samples, our approach improves model robustness and predictive accuracy across diverse datasets and geographies. Our framework has been implemented and validated in a live provenance verification system currently used by enforcement agencies, demonstrating tangible, real-world impact. Through extensive experiments and deployment in a live provenance verification system, we show that this system significantly enhances provenance verification, mitigates fraudulent trade practices, and strengthens regulatory enforcement of global supply chains.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay</title>
<link>https://arxiv.org/abs/2502.15522</link>
<guid>https://arxiv.org/abs/2502.15522</guid>
<content:encoded><![CDATA[
arXiv:2502.15522v3 Announce Type: replace 
Abstract: Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few indirect measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution operators that map measurements to estimates of the target signal. A standard remedy (e.g., in compressed sensing) for establishing the uniqueness of the solution mapping is to assume the existence of a latent low-dimensional structure in the source signal. We ask the following question: do deep linear neural networks adapt to unknown low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution mapping that accurately solves the inverse problem while implicitly encoding latent subspace structure. We show rigorously that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Experience Replay with Random Reshuffling</title>
<link>https://arxiv.org/abs/2503.02269</link>
<guid>https://arxiv.org/abs/2503.02269</guid>
<content:encoded><![CDATA[
arXiv:2503.02269v2 Announce Type: replace 
Abstract: Experience replay is a key component in reinforcement learning for stabilizing learning and improving sample efficiency. Its typical implementation samples transitions with replacement from a replay buffer. In contrast, in supervised learning with a fixed dataset, it is a common practice to shuffle the dataset every epoch and consume data sequentially, which is called random reshuffling (RR). RR enjoys theoretically better convergence properties and has been shown to outperform with-replacement sampling empirically. To leverage the benefits of RR in reinforcement learning, we propose sampling methods that extend RR to experience replay, both in uniform and prioritized settings, and analyze their properties via theoretical analysis and simulations. We evaluate our sampling methods on Atari benchmarks, demonstrating their effectiveness in deep reinforcement learning. Code is available at https://github.com/pfnet-research/errr.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bant: Byzantine Antidote via Trial Function and Trust Scores</title>
<link>https://arxiv.org/abs/2505.07614</link>
<guid>https://arxiv.org/abs/2505.07614</guid>
<content:encoded><![CDATA[
arXiv:2505.07614v5 Announce Type: replace 
Abstract: Recent advancements in machine learning have improved performance while also increasing computational demands. While federated and distributed setups address these issues, their structures remain vulnerable to malicious influences. In this paper, we address a specific threat: Byzantine attacks, wherein compromised clients inject adversarial updates to derail global convergence. We combine the concept of trust scores with trial function methodology to dynamically filter outliers. Our methods address the critical limitations of previous approaches, allowing operation even when Byzantine nodes are in the majority. Moreover, our algorithms adapt to widely used scaled methods such as Adam and RMSProp, as well as practical scenarios, including local training and partial participation. We validate the robustness of our methods by conducting extensive experiments on both public datasets and private ECG data collected from medical institutions. Furthermore, we provide a broad theoretical analysis of our algorithms and their extensions to the aforementioned practical setups. The convergence guaranties of our methods are comparable to those of classical algorithms developed without Byzantine interference.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fast Kernel-based Conditional Independence test with Application to Causal Discovery</title>
<link>https://arxiv.org/abs/2505.11085</link>
<guid>https://arxiv.org/abs/2505.11085</guid>
<content:encoded><![CDATA[
arXiv:2505.11085v2 Announce Type: replace 
Abstract: Kernel-based conditional independence (KCI) testing is a powerful nonparametric method commonly employed in causal discovery tasks. Despite its flexibility and statistical reliability, cubic computational complexity limits its application to large datasets. To address this computational bottleneck, we propose \textit{FastKCI}, a scalable and parallelizable kernel-based conditional independence test that utilizes a mixture-of-experts approach inspired by embarrassingly parallel inference techniques for Gaussian processes. By partitioning the dataset based on a Gaussian mixture model over the conditioning variables, FastKCI conducts local KCI tests in parallel, aggregating the results using an importance-weighted sampling scheme. Experiments on synthetic datasets and benchmarks on real-world production data validate that FastKCI maintains the statistical power of the original KCI test while achieving substantial computational speedups. FastKCI thus represents a practical and efficient solution for conditional independence testing in causal inference on large-scale data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sequential Monte Carlo for Policy Optimization in Continuous POMDPs</title>
<link>https://arxiv.org/abs/2505.16732</link>
<guid>https://arxiv.org/abs/2505.16732</guid>
<content:encoded><![CDATA[
arXiv:2505.16732v3 Announce Type: replace 
Abstract: Optimal decision-making under partial observability requires agents to balance reducing uncertainty (exploration) against pursuing immediate objectives (exploitation). In this paper, we introduce a novel policy optimization framework for continuous partially observable Markov decision processes (POMDPs) that explicitly addresses this challenge. Our method casts policy learning as probabilistic inference in a non-Markovian Feynman--Kac model that inherently captures the value of information gathering by anticipating future observations, without requiring suboptimal approximations or handcrafted heuristics. To optimize policies under this model, we develop a nested sequential Monte Carlo (SMC) algorithm that efficiently estimates a history-dependent policy gradient under samples from the optimal trajectory distribution induced by the POMDP. We demonstrate the effectiveness of our algorithm across standard continuous POMDP benchmarks, where existing methods struggle to act under uncertainty.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Mixing Can Induce Phase Transitions in Knowledge Acquisition</title>
<link>https://arxiv.org/abs/2505.18091</link>
<guid>https://arxiv.org/abs/2505.18091</guid>
<content:encoded><![CDATA[
arXiv:2505.18091v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are typically trained on data mixtures: most data come from web scrapes, while a small portion is curated from high-quality sources with dense domain-specific knowledge. In this paper, we show that when training LLMs on such data mixtures, knowledge acquisition from knowledge-dense datasets, unlike training exclusively on knowledge-dense data (arXiv:2404.05405), does not always follow a smooth scaling law but can exhibit phase transitions with respect to the mixing ratio and model size. Through controlled experiments on a synthetic biography dataset mixed with web-scraped data, we demonstrate that: (1) as we increase the model size to a critical value, the model suddenly transitions from memorizing very few to most of the biographies; (2) below a critical mixing ratio, the model memorizes almost nothing even with extensive training, but beyond this threshold, it rapidly memorizes more biographies. We attribute these phase transitions to a capacity allocation phenomenon: a model with bounded capacity must act like a knapsack problem solver to minimize the overall test loss, and the optimal allocation across datasets can change discontinuously as the model size or mixing ratio varies. We formalize this intuition in an information-theoretic framework and reveal that these phase transitions are predictable, with the critical mixing ratio following a power-law relationship with the model size. Our findings highlight a concrete case where a good mixing recipe for large models may not be optimal for small models, and vice versa.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference</title>
<link>https://arxiv.org/abs/2505.22758</link>
<guid>https://arxiv.org/abs/2505.22758</guid>
<content:encoded><![CDATA[
arXiv:2505.22758v2 Announce Type: replace 
Abstract: The size and compute characteristics of modern large language models have led to an increased interest in developing specialized kernels tailored for particular training and inference workloads. Existing kernels primarily optimize for compute utilization, targeting the large-batch training and inference settings. However, low-batch inference, where memory bandwidth and kernel launch overheads are significant factors, remains important for many applications of interest such as in edge deployment and latency-sensitive applications. This paper describes FlashFormer, which fuses the entire transformer forward pass into a single kernel for accelerating low-batch inference of large language models. Across various model sizes and quantizations settings, FlashFormer achieves nontrivial speedups compared to existing inference kernels.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SoftStep: Learning Sparse Similarity Powers Deep Neighbor-Based Regression</title>
<link>https://arxiv.org/abs/2506.08139</link>
<guid>https://arxiv.org/abs/2506.08139</guid>
<content:encoded><![CDATA[
arXiv:2506.08139v2 Announce Type: replace 
Abstract: Neighbor-based methods are a natural alternative to linear prediction for tabular data when relationships between inputs and targets exhibit complexity such as nonlinearity, periodicity, or heteroscedasticity. Yet in deep learning on unstructured data, nonparametric neighbor-based approaches are rarely implemented in lieu of simple linear heads. This is primarily due to the ability of systems equipped with linear regression heads to co-learn internal representations along with the linear head's parameters. To unlock the full potential of neighbor-based methods in neural networks we introduce SoftStep, a parametric module that learns sparse instance-wise similarity measures directly from data. When integrated with existing neighbor-based methods, SoftStep enables regression models that consistently outperform linear heads across diverse architectures, domains, and training scenarios. We focus on regression tasks, where we show theoretically that neighbor-based prediction with a mean squared error objective constitutes a metric learning algorithm that induces well-structured embedding spaces. We then demonstrate analytically and empirically that this representational structure translates into superior performance when combined with the sparse, instance-wise similarity measures introduced by SoftStep. Beyond regression, SoftStep is a general method for learning instance-wise similarity in deep neural networks, with broad applicability to attention mechanisms, metric learning, representational alignment, and related paradigms.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design</title>
<link>https://arxiv.org/abs/2506.09508</link>
<guid>https://arxiv.org/abs/2506.09508</guid>
<content:encoded><![CDATA[
arXiv:2506.09508v2 Announce Type: replace 
Abstract: We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models</title>
<link>https://arxiv.org/abs/2506.09532</link>
<guid>https://arxiv.org/abs/2506.09532</guid>
<content:encoded><![CDATA[
arXiv:2506.09532v4 Announce Type: replace 
Abstract: We present Athena-PRM, a multimodal process reward model (PRM) designed to evaluate the reward score for each step in solving complex reasoning problems. Developing high-performance PRMs typically demands significant time and financial investment, primarily due to the necessity for step-level annotations of reasoning steps. Conventional automated labeling methods, such as Monte Carlo estimation, often produce noisy labels and incur substantial computational costs. To efficiently generate high-quality process-labeled data, we propose leveraging prediction consistency between weak and strong completers as a criterion for identifying reliable process labels. Remarkably, Athena-PRM demonstrates outstanding effectiveness across various scenarios and benchmarks with just 5,000 samples. Furthermore, we also develop two effective strategies to improve the performance of PRMs: ORM initialization and up-sampling for negative data. We validate our approach in three specific scenarios: verification for test time scaling, direct evaluation of reasoning step correctness, and reward ranked fine-tuning. Our Athena-PRM consistently achieves superior performance across multiple benchmarks and scenarios. Notably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances performance by 10.2 points on WeMath and 7.1 points on MathVista for test time scaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in VisualProcessBench and outperforms the previous SoTA by 3.9 F1-score, showcasing its robust capability to accurately assess the correctness of the reasoning step. Additionally, utilizing Athena-PRM as the reward model, we develop Athena-7B with reward ranked fine-tuning and outperforms baseline with a significant margin on five benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Path Channels and Plan Extension Kernels: a Mechanistic Description of Planning in a Sokoban RNN</title>
<link>https://arxiv.org/abs/2506.10138</link>
<guid>https://arxiv.org/abs/2506.10138</guid>
<content:encoded><![CDATA[
arXiv:2506.10138v2 Announce Type: replace 
Abstract: We partially reverse-engineer a convolutional recurrent neural network (RNN) trained with model-free reinforcement learning to play the box-pushing game Sokoban. We find that the RNN stores future moves (plans) as activations in particular channels of the hidden state, which we call path channels. A high activation in a particular location means that, when a box is in that location, it will get pushed in the channel's assigned direction. We examine the convolutional kernels between path channels and find that they encode the change in position resulting from each possible action, thus representing part of a learned transition model. The RNN constructs plans by starting at the boxes and goals. These kernels extend activations in path channels forwards from boxes and backwards from the goal. Negative values are placed in channels at obstacles. This causes the extension kernels to propagate the negative value in reverse, thus pruning the last few steps and letting an alternative plan emerge; a form of backtracking. Our work shows that, a precise understanding of the plan representation allows us to directly understand the bidirectional planning-like algorithm learned by model-free training in more familiar terms.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Multi-color Message Passing Graph Neural Networks for Blood-brain Barrier Permeability Prediction</title>
<link>https://arxiv.org/abs/2507.18926</link>
<guid>https://arxiv.org/abs/2507.18926</guid>
<content:encoded><![CDATA[
arXiv:2507.18926v4 Announce Type: replace 
Abstract: Accurate prediction of blood-brain barrier permeability (BBBP) is essential for central nervous system (CNS) drug development. While graph neural networks (GNNs) have advanced molecular property prediction, they often rely on molecular topology and neglect the three-dimensional geometric information crucial for modeling transport mechanisms. This paper introduces the geometric multi-color message-passing graph neural network (GMC-MPNN), a novel framework that enhances standard message-passing architectures by explicitly incorporating atomic-level geometric features and long-range interactions. Our model constructs weighted colored subgraphs based on atom types to capture the spatial relationships and chemical context that govern BBB permeability. We evaluated GMC-MPNN on three benchmark datasets for both classification and regression tasks, using rigorous scaffold-based splitting to ensure a robust assessment of generalization. The results demonstrate that GMC-MPNN consistently outperforms existing state-of-the-art models, achieving superior performance in both classifying compounds as permeable/non-permeable (AUC-ROC of 0.9704 and 0.9685) and in regressing continuous permeability values (RMSE of 0.4609, Pearson correlation of 0.7759). An ablation study further quantified the impact of specific atom-pair interactions, revealing that the model's predictive power derives from its ability to learn from both common and rare, but chemically significant, functional motifs. By integrating spatial geometry into the graph representation, GMC-MPNN sets a new performance benchmark and offers a more accurate and generalizable tool for drug discovery pipelines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bi-cephalic self-attended model to classify Parkinson's disease patients with freezing of gait</title>
<link>https://arxiv.org/abs/2507.20862</link>
<guid>https://arxiv.org/abs/2507.20862</guid>
<content:encoded><![CDATA[
arXiv:2507.20862v2 Announce Type: replace 
Abstract: Parkinson Disease (PD) often results in motor and cognitive impairments, including gait dysfunction, particularly in patients with freezing of gait (FOG). Current detection methods are either subjective or reliant on specialized gait analysis tools. This study aims to develop an objective, data-driven, and multi-modal classification model to detect gait dysfunction in PD patients using resting-state EEG signals combined with demographic and clinical variables. We utilized a dataset of 124 participants: 42 PD patients with FOG (PDFOG+), 41 without FOG (PDFOG-), and 41 age-matched healthy controls. Features extracted from resting-state EEG and descriptive variables (age, education, disease duration) were used to train a novel Bi-cephalic Self-Attention Model (BiSAM). We tested three modalities: signal-only, descriptive-only, and multi-modal, across different EEG channel subsets (BiSAM-63, -16, -8, and -4). Signal-only and descriptive-only models showed limited performance, achieving a maximum accuracy of 55% and 68%, respectively. In contrast, the multi-modal models significantly outperformed both, with BiSAM-8 and BiSAM-4 achieving the highest classification accuracy of 88%. These results demonstrate the value of integrating EEG with objective descriptive features for robust PDFOG+ detection. This study introduces a multi-modal, attention-based architecture that objectively classifies PDFOG+ using minimal EEG channels and descriptive variables. This approach offers a scalable and efficient alternative to traditional assessments, with potential applications in routine clinical monitoring and early diagnosis of PD-related gait dysfunction.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-shot Class-incremental Fault Diagnosis by Preserving Class-Agnostic Knowledge with Dual-Granularity Representations</title>
<link>https://arxiv.org/abs/2508.16634</link>
<guid>https://arxiv.org/abs/2508.16634</guid>
<content:encoded><![CDATA[
arXiv:2508.16634v4 Announce Type: replace 
Abstract: Few-Shot Class-Incremental Fault Diagnosis (FSC-FD), which aims to continuously learn from new fault classes with only a few samples without forgetting old ones, is critical for real-world industrial systems. However, this challenging task severely amplifies the issues of catastrophic forgetting of old knowledge and overfitting on scarce new data. To address these challenges, this paper proposes a novel framework built upon Dual-Granularity Representations, termed the Dual-Granularity Guidance Network (DGGN). Our DGGN explicitly decouples feature learning into two parallel streams: 1) a fine-grained representation stream, which utilizes a novel Multi-Order Interaction Aggregation module to capture discriminative, class-specific features from the limited new samples. 2) a coarse-grained representation stream, designed to model and preserve general, class-agnostic knowledge shared across all fault types. These two representations are dynamically fused by a multi-semantic cross-attention mechanism, where the stable coarse-grained knowledge guides the learning of fine-grained features, preventing overfitting and alleviating feature conflicts. To further mitigate catastrophic forgetting, we design a Boundary-Aware Exemplar Prioritization strategy. Moreover, a decoupled Balanced Random Forest classifier is employed to counter the decision boundary bias caused by data imbalance. Extensive experiments on the TEP benchmark and a real-world MFF dataset demonstrate that our proposed DGGN achieves superior diagnostic performance and stability compared to state-of-the-art FSC-FD approaches. Our code is publicly available at https://github.com/MentaY/DGGN
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolutional Monge Mapping between EEG Datasets to Support Independent Component Labeling</title>
<link>https://arxiv.org/abs/2509.01721</link>
<guid>https://arxiv.org/abs/2509.01721</guid>
<content:encoded><![CDATA[
arXiv:2509.01721v2 Announce Type: replace 
Abstract: EEG recordings contain rich information about neural activity but are subject to artifacts, noise, and superficial differences due to sensors, amplifiers, and filtering. Independent component analysis and automatic labeling of independent components (ICs) enable artifact removal in EEG pipelines. Convolutional Monge Mapping Normalization (CMMN) is a recent tool used to achieve spectral conformity of EEG signals, which was shown to improve deep neural network approaches for sleep staging. Here we propose a novel extension of the CMMN method with two alternative approaches to computing the source reference spectrum the target signals are mapped to: (1) channel-averaged and $l_1$-normalized barycenter, and (2) a subject-to-subject mapping that finds the source subject with the closest spectrum to the target subject. Notably, our extension yields space-time separable filters that can be used to map between datasets with different numbers of EEG channels. We apply these filters in an IC classification task, and show significant improvement in recognizing brain versus non-brain ICs.
  Clinical relevance - EEG recordings are used in the diagnosis and monitoring of multiple neuropathologies, including epilepsy and psychosis. While EEG analysis can benefit from automating artifact removal through independent component analysis and labeling, differences in recording equipment and context (the presence of noise from electrical wiring and other devices) may impact the performance of machine learning models, but these differences can be minimized by appropriate spectral normalization through filtering.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Output Faithfulness: Learning Attributions that Preserve Computational Pathways</title>
<link>https://arxiv.org/abs/2509.04588</link>
<guid>https://arxiv.org/abs/2509.04588</guid>
<content:encoded><![CDATA[
arXiv:2509.04588v2 Announce Type: replace 
Abstract: Faithfulness metrics such as insertion and deletion evaluate how feature removal affects model outputs but overlook whether explanations preserve the computational pathway the network actually uses. We show that external metrics can be maximized through alternative pathways -- perturbations that reroute computation via different feature detectors while preserving output behavior. To address this, we propose activation preservation as a tractable proxy for preserving computational pathways
  We introduce Faithfulness-guided Ensemble Interpretation (FEI), which jointly optimizes external faithfulness (via ensemble quantile optimization of insertion/deletion curves) and internal faithfulness (via selective gradient clipping). Across VGG and ResNet on ImageNet and CUB-200-2011, FEI achieves state-of-the-art insertion/deletion scores while maintaining significantly lower activation deviation, showing that both external and internal faithfulness are essential for reliable explanations.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
<link>https://arxiv.org/abs/2509.04734</link>
<guid>https://arxiv.org/abs/2509.04734</guid>
<content:encoded><![CDATA[
arXiv:2509.04734v2 Announce Type: replace 
Abstract: The Information Contrastive (I-Con) framework revealed that over 23 representation learning methods implicitly minimize KL divergence between data and learned distributions that encode similarities between data points. However, a KL-based loss may be misaligned with the true objective, and properties of KL divergence such as asymmetry and unboundedness may create optimization challenges. We present Beyond I-Con, a framework that enables systematic discovery of novel loss functions by exploring alternative statistical divergences. Key findings: (1) on unsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-art results by modifying the PMI algorithm to use total variation (TV) distance; (2) supervised contrastive learning with Euclidean distance as the feature space metric is improved by replacing the standard loss function with Jenson-Shannon divergence (JSD); (3) on dimensionality reduction, we achieve superior qualitative results and better performance on downstream tasks than SNE by replacing KL with a bounded $f$-divergence. Our results highlight the importance of considering divergence choices in representation learning optimization.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Similarity-Distance-Magnitude Activations</title>
<link>https://arxiv.org/abs/2509.12760</link>
<guid>https://arxiv.org/abs/2509.12760</guid>
<content:encoded><![CDATA[
arXiv:2509.12760v3 Announce Type: replace 
Abstract: We introduce the Similarity-Distance-Magnitude (SDM) activation function, a more robust and interpretable formulation of the standard softmax activation function, adding Similarity (i.e., correctly predicted depth-matches into training) awareness and Distance-to-training-distribution awareness to the existing output Magnitude (i.e., decision-boundary) awareness, and enabling interpretability-by-exemplar via dense matching. We further introduce the SDM estimator, based on a data-driven partitioning of the class-wise empirical CDFs via the SDM activation, to control the class- and prediction-conditional accuracy among selective classifications. When used as the final-layer activation over pre-trained language models for selective classification, the SDM estimator is more robust to co-variate shifts and out-of-distribution inputs than existing calibration methods using softmax activations, while remaining informative over in-distribution data.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Rate of Convergence of Kolmogorov-Arnold Network Regression Estimators</title>
<link>https://arxiv.org/abs/2509.19830</link>
<guid>https://arxiv.org/abs/2509.19830</guid>
<content:encoded><![CDATA[
arXiv:2509.19830v2 Announce Type: replace 
Abstract: Kolmogorov-Arnold Networks (KANs) offer a structured and interpretable framework for multivariate function approximation by composing univariate transformations through additive or multiplicative aggregation. This paper establishes theoretical convergence guarantees for KANs when the univariate components are represented by B-splines. We prove that both additive and hybrid additive-multiplicative KANs attain the minimax-optimal convergence rate $O(n^{-2r/(2r+1)})$ for functions in Sobolev spaces of smoothness $r$. We further derive guidelines for selecting the optimal number of knots in the B-splines. The theory is supported by simulation studies that confirm the predicted convergence rates. These results provide a theoretical foundation for using KANs in nonparametric regression and highlight their potential as a structured alternative to existing methods.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR</title>
<link>https://arxiv.org/abs/2509.23808</link>
<guid>https://arxiv.org/abs/2509.23808</guid>
<content:encoded><![CDATA[
arXiv:2509.23808v3 Announce Type: replace 
Abstract: A prevailing view in Reinforcement Learning with Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named ER Velocity and ER Acceleration, to capture exploitation dynamics. Our analysis reveals that in the semantic space, exploration and exploitation could be decoupled (Sec.~4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random Feature Spiking Neural Networks</title>
<link>https://arxiv.org/abs/2510.01012</link>
<guid>https://arxiv.org/abs/2510.01012</guid>
<content:encoded><![CDATA[
arXiv:2510.01012v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) as Machine Learning (ML) models have recently received a lot of attention as a potentially more energy-efficient alternative to conventional Artificial Neural Networks. The non-differentiability and sparsity of the spiking mechanism can make these models very difficult to train with algorithms based on propagating gradients through the spiking non-linearity. We address this problem by adapting the paradigm of Random Feature Methods (RFMs) from Artificial Neural Networks (ANNs) to Spike Response Model (SRM) SNNs. This approach allows training of SNNs without approximation of the spike function gradient. Concretely, we propose a novel data-driven, fast, high-performance, and interpretable algorithm for end-to-end training of SNNs inspired by the SWIM algorithm for RFM-ANNs, which we coin S-SWIM. We provide a thorough theoretical discussion and supplementary numerical experiments showing that S-SWIM can reach high accuracies on time series forecasting as a standalone strategy and serve as an effective initialisation strategy before gradient-based training. Additional ablation studies show that our proposed method performs better than random sampling of network weights.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Fine-Tuning through Advanced Initialization Strategies for Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2510.03731</link>
<guid>https://arxiv.org/abs/2510.03731</guid>
<content:encoded><![CDATA[
arXiv:2510.03731v2 Announce Type: replace 
Abstract: The rapid development of parameter-efficient fine-tuning methods has noticeably improved the efficiency of adapting large language models. Among these, LoRA has gained widespread popularity due to its strong balance of effectiveness and parameter efficiency. However, LoRA relies on initializing two low-rank matrices whose product is zero, which limits its ability to effectively activate and leverage the original model weights-creating a potential bottleneck for optimal performance. To address this limitation, we propose \textbf{IniLoRA}, a novel initialization strategy that initializes the low-rank matrices to closely approximate the original model weights. Experimental results indicate that IniLoRA achieves better performance than LoRA across a range of models and tasks. Additionally, we introduce two variants, IniLoRA-$\alpha$ and IniLoRA-$\beta$, both leveraging distinct initialization methods to enhance performance further.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Discriminative-Generative Modeling via Dual Adversarial Training</title>
<link>https://arxiv.org/abs/2510.13872</link>
<guid>https://arxiv.org/abs/2510.13872</guid>
<content:encoded><![CDATA[
arXiv:2510.13872v2 Announce Type: replace 
Abstract: Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in Stochastic Gradient Langevin Dynamics (SGLD)-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and Projected Gradient Descent (PGD)-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training strategy that addresses normalization-related instabilities and enables leveraging pretrained robust classifiers, generalizing effectively across diverse architectures. Experiments on CIFAR-10/100 and ImageNet demonstrate that our approach: (1) is the first EBM-based hybrid to scale to high-resolution datasets with high training stability, simultaneously achieving state-of-the-art discriminative and generative performance on ImageNet 256$\times$256; (2) uniquely combines generative quality with adversarial robustness, enabling critical applications like robust counterfactual explanations; and (3) functions as a competitive standalone generative model, matching the generative quality of autoregressive methods (VAR-d16) and surpassing diffusion models while offering unique versatility.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of Stochastic Gradient Langevin Dynamics in the Lazy Training Regime</title>
<link>https://arxiv.org/abs/2510.21245</link>
<guid>https://arxiv.org/abs/2510.21245</guid>
<content:encoded><![CDATA[
arXiv:2510.21245v2 Announce Type: replace 
Abstract: Continuous-time models provide important insights into the training dynamics of optimization algorithms in deep learning. In this work, we establish a non-asymptotic convergence analysis of stochastic gradient Langevin dynamics (SGLD), which is an It\^o stochastic differential equation (SDE) approximation of stochastic gradient descent in continuous time, in the lazy training regime. We show that, under regularity conditions on the Hessian of the loss function, SGLD with multiplicative and state-dependent noise (i) yields a non-degenerate kernel throughout the training process with high probability, and (ii) achieves exponential convergence to the empirical risk minimizer in expectation, and we establish finite-time and finite-width bounds on the optimality gap. We corroborate our theoretical findings with numerical examples in the regression setting.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bilevel Models for Adversarial Learning and A Case Study</title>
<link>https://arxiv.org/abs/2510.25121</link>
<guid>https://arxiv.org/abs/2510.25121</guid>
<content:encoded><![CDATA[
arXiv:2510.25121v2 Announce Type: replace 
Abstract: Adversarial learning has been attracting more and more attention thanks to the fast development of machine learning and artificial intelligence. However, due to the complicated structure of most machine learning models, the mechanism of adversarial attacks is not well interpreted. How to measure the effect of attacks is still not quite clear.
  In this paper, we investigate the adversarial learning from the perturbation analysis point of view.
  We characterize the robustness of learning models through the calmness of the solution mapping.
  In the case of convex clustering models, we identify the conditions under which the clustering results remain the same under perturbations.
  When the noise level is large, it leads to an attack.
  Therefore, we propose two bilevel models for adversarial learning where the effect of adversarial learning is measured
  by some deviation function.
  Specifically, we systematically study the so-called $\delta$-measure and show that under certain conditions, it can be used as a deviation function in adversarial learning for convex clustering models.
  Finally, we conduct numerical tests to verify the above theoretical results as well as the efficiency of the two proposed bilevel models.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SynQuE: Estimating Synthetic Dataset Quality Without Annotations</title>
<link>https://arxiv.org/abs/2511.03928</link>
<guid>https://arxiv.org/abs/2511.03928</guid>
<content:encoded><![CDATA[
arXiv:2511.03928v2 Announce Type: replace 
Abstract: We introduce and formalize the Synthetic Dataset Quality Estimation (SynQuE) problem: ranking synthetic datasets by their expected real-world task performance using only limited unannotated real data. This addresses a critical and open challenge where data is scarce due to collection costs or privacy constraints. We establish the first comprehensive benchmarks for this problem by introducing and evaluating proxy metrics that choose synthetic data for training to maximize task performance on real data. We introduce the first proxy metrics for SynQuE by adapting distribution and diversity-based distance measures to our context via embedding models. To address the shortcomings of these metrics on complex planning tasks, we propose LENS, a novel proxy that leverages large language model reasoning. Our results show that SynQuE proxies correlate with real task performance across diverse tasks, including sentiment analysis, Text2SQL, web navigation, and image classification, with LENS consistently outperforming others on complex tasks by capturing nuanced characteristics. For instance, on text-to-SQL parsing, training on the top-3 synthetic datasets selected via SynQuE proxies can raise accuracy from 30.4% to 38.4 (+8.1)% on average compared to selecting data indiscriminately. This work establishes SynQuE as a practical framework for synthetic data selection under real-data scarcity and motivates future research on foundation model-based data characterization and fine-grained data selection.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ground-Truth Subgraphs for Better Training and Evaluation of Knowledge Graph Augmented LLMs</title>
<link>https://arxiv.org/abs/2511.04473</link>
<guid>https://arxiv.org/abs/2511.04473</guid>
<content:encoded><![CDATA[
arXiv:2511.04473v2 Announce Type: replace 
Abstract: Retrieval of information from graph-structured knowledge bases represents a promising direction for improving the factuality of LLMs. While various solutions have been proposed, a comparison of methods is difficult due to the lack of challenging QA datasets with ground-truth targets for graph retrieval. We present SynthKGQA, an LLM-powered framework for generating high-quality Knowledge Graph Question Answering datasets from any Knowledge Graph, providing the full set of ground-truth facts in the KG to reason over questions. We show how, in addition to enabling more informative benchmarking of KG retrievers, the data produced with SynthKGQA also allows us to train better models.We apply SynthKGQA to Wikidata to generate GTSQA, a new dataset designed to test zero-shot generalization abilities of KG retrievers with respect to unseen graph structures and relation types, and benchmark popular solutions for KG-augmented LLMs on it.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Resilience Inference for Supply Chains with Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2511.06208</link>
<guid>https://arxiv.org/abs/2511.06208</guid>
<content:encoded><![CDATA[
arXiv:2511.06208v2 Announce Type: replace 
Abstract: Supply chains are integral to global economic stability, yet disruptions can swiftly propagate through interconnected networks, resulting in substantial economic impacts. Accurate and timely inference of supply chain resilience the capability to maintain core functions during disruptions is crucial for proactive risk mitigation and robust network design. However, existing approaches lack effective mechanisms to infer supply chain resilience without explicit system dynamics and struggle to represent the higher-order, multi-entity dependencies inherent in supply chain networks. These limitations motivate the definition of a novel problem and the development of targeted modeling solutions. To address these challenges, we formalize a novel problem: Supply Chain Resilience Inference (SCRI), defined as predicting supply chain resilience using hypergraph topology and observed inventory trajectories without explicit dynamic equations. To solve this problem, we propose the Supply Chain Resilience Inference Hypergraph Network (SC-RIHN), a novel hypergraph-based model leveraging set-based encoding and hypergraph message passing to capture multi-party firm-product interactions. Comprehensive experiments demonstrate that SC-RIHN significantly outperforms traditional MLP, representative graph neural network variants, and ResInf baselines across synthetic benchmarks, underscoring its potential for practical, early-warning risk assessment in complex supply chain systems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLMscape</title>
<link>https://arxiv.org/abs/2511.07161</link>
<guid>https://arxiv.org/abs/2511.07161</guid>
<content:encoded><![CDATA[
arXiv:2511.07161v2 Announce Type: replace 
Abstract: LLMscape is an interactive installation that investigates how humans and AI construct meaning under shared conditions of uncertainty. Within a mutable, projection-mapped landscape, human participants reshape the world and engage with multiple AI agents, each developing incomplete and provisional accounts of their environment. Exhibited in Shanghai and continually evolving, the work positions AI not as deterministic tools but as embodied co-witnesses to an unstable world, examining the parallels between human and artificial meaning-making and inviting reflection on our shared epistemic limits.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Global and Local Bounds in Gaussian Process Regression via Chaining</title>
<link>https://arxiv.org/abs/2511.09144</link>
<guid>https://arxiv.org/abs/2511.09144</guid>
<content:encoded><![CDATA[
arXiv:2511.09144v4 Announce Type: replace 
Abstract: Gaussian process regression (GPR) is a popular nonparametric Bayesian method that provides predictive uncertainty estimates and is widely used in safety-critical applications. While prior research has introduced various uncertainty bounds, most existing approaches require access to specific input features, and rely on posterior mean and variance estimates or the tuning of hyperparameters. These limitations hinder robustness and fail to capture the model's global behavior in expectation. To address these limitations, we propose a chaining-based framework for estimating upper and lower bounds on the expected extreme values over unseen data, without requiring access to specific input features. We provide kernel-specific refinements for commonly used kernels such as RBF and Mat\'ern, in which our bounds are tighter than generic constructions. We further improve numerical tightness by avoiding analytical relaxations. In addition to global estimation, we also develop a novel method for local uncertainty quantification at specified inputs. This approach leverages chaining geometry through partition diameters, adapting to local structures without relying on posterior variance scaling. Our experimental results validate the theoretical findings and demonstrate that our method outperforms existing approaches on both synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Incoherent Beliefs &amp; Inconsistent Actions in Large Language Models</title>
<link>https://arxiv.org/abs/2511.13240</link>
<guid>https://arxiv.org/abs/2511.13240</guid>
<content:encoded><![CDATA[
arXiv:2511.13240v2 Announce Type: replace 
Abstract: Real-world tasks and environments exhibit differences from the static datasets that large language models (LLMs) are typically evaluated on. Such tasks can involve sequential interaction, requiring coherent updating of beliefs in light of new evidence, and making appropriate decisions based on those beliefs. Predicting how LLMs will perform in such dynamic environments is important, but can be tricky to determine from measurements in static settings. In this work, we examine two critical components of LLM performance: the ability of LLMs to coherently update their beliefs, and the extent to which the actions they take are consistent with those beliefs. First, we find that LLMs are largely inconsistent in how they update their beliefs; models can exhibit up to a 30% average difference between the directly elicited posterior, and the correct update of their prior. Second, we find that LLMs also often take actions which are inconsistent with the beliefs they hold. On a betting market, for example, LLMs often do not even bet in the same direction as their internally held beliefs over the underlying outcomes. We also find they have moderate self-inconsistency in how they respond to challenges by users to given answers. Finally, we show that the above properties hold even for strong models that obtain high accuracy or that are well-calibrated on the tasks at hand. Our results highlight the difficulties of predicting LLM behavior in complex real-world settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Intervention Efficiency and Perturbation Validation Framework: Capacity-Aware and Robust Clinical Model Selection under the Rashomon Effect</title>
<link>https://arxiv.org/abs/2511.14317</link>
<guid>https://arxiv.org/abs/2511.14317</guid>
<content:encoded><![CDATA[
arXiv:2511.14317v3 Announce Type: replace 
Abstract: In clinical machine learning, the coexistence of multiple models with comparable performance -- a manifestation of the Rashomon Effect -- poses fundamental challenges for trustworthy deployment and evaluation. Small, imbalanced, and noisy datasets, coupled with high-dimensional and weakly identified clinical features, amplify this multiplicity and make conventional validation schemes unreliable. As a result, selecting among equally performing models becomes uncertain, particularly when resource constraints and operational priorities are not considered by conventional metrics like F1 score. To address these issues, we propose two complementary tools for robust model assessment and selection: Intervention Efficiency (IE) and the Perturbation Validation Framework (PVF). IE is a capacity-aware metric that quantifies how efficiently a model identifies actionable true positives when only limited interventions are feasible, thereby linking predictive performance with clinical utility. PVF introduces a structured approach to assess the stability of models under data perturbations, identifying models whose performance remains most invariant across noisy or shifted validation sets. Empirical results on synthetic and real-world healthcare datasets show that using these tools facilitates the selection of models that generalize more robustly and align with capacity constraints, offering a new direction for tackling the Rashomon Effect in clinical settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CID: Measuring Feature Importance Through Counterfactual Distributions</title>
<link>https://arxiv.org/abs/2511.15371</link>
<guid>https://arxiv.org/abs/2511.15371</guid>
<content:encoded><![CDATA[
arXiv:2511.15371v2 Announce Type: replace 
Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DS-Span: Single-Phase Discriminative Subgraph Mining for Efficient Graph Embeddings</title>
<link>https://arxiv.org/abs/2511.17419</link>
<guid>https://arxiv.org/abs/2511.17419</guid>
<content:encoded><![CDATA[
arXiv:2511.17419v2 Announce Type: replace 
Abstract: Graph representation learning seeks to transform complex, high-dimensional graph structures into compact vector spaces that preserve both topology and semantics. Among the various strategies, subgraph-based methods provide an interpretable bridge between symbolic pattern discovery and continuous embedding learning. Yet, existing frequent or discriminative subgraph mining approaches often suffer from redundant multi-phase pipelines, high computational cost, and weak coupling between mined structures and their discriminative relevance. We propose DS-Span, a single-phase discriminative subgraph mining framework that unifies pattern growth, pruning, and supervision-driven scoring within one traversal of the search space. DS-Span introduces a coverage-capped eligibility mechanism that dynamically limits exploration once a graph is sufficiently represented, and an information-gain-guided selection that promotes subgraphs with strong class-separating ability while minimizing redundancy. The resulting subgraph set serves as an efficient, interpretable basis for downstream graph embedding and classification. Extensive experiments across benchmarks demonstrate that DS-Span generates more compact and discriminative subgraph features than prior multi-stage methods, achieving higher or comparable accuracy with significantly reduced runtime. These results highlight the potential of unified, single-phase discriminative mining as a foundation for scalable and interpretable graph representation learning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Adaptive Resonance Theory-based Topological Clustering Algorithm with a Self-Adjusting Vigilance Parameter</title>
<link>https://arxiv.org/abs/2511.17983</link>
<guid>https://arxiv.org/abs/2511.17983</guid>
<content:encoded><![CDATA[
arXiv:2511.17983v4 Announce Type: replace 
Abstract: Clustering in stationary and nonstationary settings, where data distributions remain static or evolve over time, requires models that can adapt to distributional shifts while preserving previously learned cluster structures. This paper proposes an Adaptive Resonance Theory (ART)-based topological clustering algorithm that autonomously adjusts its recalculation interval and vigilance threshold through a diversity-driven adaptation mechanism. This mechanism enables hyperparameter-free learning that maintains cluster stability and continuity in dynamic environments. Experiments on 24 real-world datasets demonstrate that the proposed algorithm outperforms state-of-the-art methods in both clustering performance and continual learning capability. These results highlight the effectiveness of the proposed parameter adaptation in mitigating catastrophic forgetting and maintaining consistent clustering in evolving data streams. Source code is available at https://github.com/Masuyama-lab/IDAT
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Active Learning Framework in Materials Science with Large Language Models</title>
<link>https://arxiv.org/abs/2511.19730</link>
<guid>https://arxiv.org/abs/2511.19730</guid>
<content:encoded><![CDATA[
arXiv:2511.19730v2 Announce Type: replace 
Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI</title>
<link>https://arxiv.org/abs/2511.20395</link>
<guid>https://arxiv.org/abs/2511.20395</guid>
<content:encoded><![CDATA[
arXiv:2511.20395v2 Announce Type: replace 
Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Machine Learning for Early Trust Prediction in Human-AI Interaction Using Face Image and GSR Bio Signals</title>
<link>https://arxiv.org/abs/2511.21908</link>
<guid>https://arxiv.org/abs/2511.21908</guid>
<content:encoded><![CDATA[
arXiv:2511.21908v2 Announce Type: replace 
Abstract: Predicting human trust in AI systems is crucial for safe integration of AI-based decision support tools, especially in healthcare. This study proposes a multi-modal machine learning framework that combines image and galvanic skin response (GSR) data to predict early user trust in AI- or human-generated recommendations in a simulated ADHD mHealth context. Facial video data were processed using OpenCV for frame extraction and transferred learning with a pre-trained transformer model to derive emotional features. Concurrently, GSR signals were decomposed into tonic and phasic components to capture physiological arousal patterns. Two temporal windows were defined for trust prediction: the Early Detection Window (6 to 3 seconds before decision-making) and the Proximal Detection Window (3 to 0 seconds before decision-making). For each window, trust prediction was conducted separately using image-based, GSR-based, and multimodal (image + GSR) features. Each modality was analyzed using machine learning algorithms, and the top-performing unimodal models were integrated through a multimodal stacking ensemble for final prediction. Experimental results showed that combining facial and physiological cues significantly improved prediction performance. The multimodal stacking framework achieved an accuracy of 0.83, F1-score of 0.88, and ROC-AUC of 0.87 in the Early Detection Window, and an accuracy of 0.75, F1-score of 0.82, and ROC-AUC of 0.66 in the Proximal Detection Window. These results demonstrate the potential of bio signals as real-time, objective markers of user trust, enabling adaptive AI systems that dynamically adjust their responses to maintain calibrated trust which is a critical capability in mental health applications where mis-calibrated trust can affect diagnostic and treatment outcomes.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards an end-to-end artificial intelligence driven global weather forecasting system</title>
<link>https://arxiv.org/abs/2312.12462</link>
<guid>https://arxiv.org/abs/2312.12462</guid>
<content:encoded><![CDATA[
arXiv:2312.12462v4 Announce Type: replace-cross 
Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. The initial states are typically generated by traditional data assimilation components, which are computationally expensive and time-consuming. Here, by cyclic training to model the steady-state background error covariance and introducing the confidence matrix to characterize the quality of observations, we present an AI-based data assimilation model, i.e., Adas, for global weather variables. Further, we combine Adas with the advanced AI-based forecasting model (i.e., FengWu) to construct an end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate global conventional observations to produce high-quality analysis, enabling the system to operate stably for long term. Moreover, the system can generate accurate end-to-end weather forecasts with comparable skill to those of the IFS, demonstrating the promising potential of data-driven approaches.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Autonomy-Alignment Problem in Open-Ended Learning Robots: Formalising the Purpose Framework</title>
<link>https://arxiv.org/abs/2403.02514</link>
<guid>https://arxiv.org/abs/2403.02514</guid>
<content:encoded><![CDATA[
arXiv:2403.02514v3 Announce Type: replace-cross 
Abstract: The rapid advancement of artificial intelligence is enabling the development of increasingly autonomous robots capable of operating beyond engineered factory settings and into the unstructured environments of human life. This shift raises a critical autonomy-alignment problem: how to ensure that a robot's autonomous learning focuses on acquiring knowledge and behaviours that serve human practical objectives while remaining aligned with broader human values (e.g., safety and ethics). This problem remains largely underexplored and lacks a unifying conceptual and formal framework. Here, we address one of its most challenging instances of the problem: open-ended learning (OEL) robots, which autonomously acquire new knowledge and skills through interaction with the environment, guided by intrinsic motivations and self-generated goals. We propose a computational framework, introduced qualitatively and then formalised, to guide the design of OEL architectures that balance autonomy with human control. At its core is the novel concept of purpose, which specifies what humans (designers or users) want the robot to learn, do, or avoid, independently of specific task domains. The framework decomposes the autonomy-alignment problem into four tractable sub-problems: the alignment of robot purposes (hardwired or learnt) with human purposes; the arbitration between multiple purposes; the grounding of abstract purposes into domain-specific goals; and the acquisition of competence to achieve those goals. The framework supports formal definitions of alignment across multiple cases and proofs of necessary and sufficient conditions under which alignment holds. Illustrative hypothetical scenarios showcase the applicability of the framework for guiding the development of purpose-aligned autonomous robots.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimum Weighted Feedback Arc Sets for Ranking from Pairwise Comparisons</title>
<link>https://arxiv.org/abs/2412.16181</link>
<guid>https://arxiv.org/abs/2412.16181</guid>
<content:encoded><![CDATA[
arXiv:2412.16181v3 Announce Type: replace-cross 
Abstract: The Minimum Weighted Feedback Arc Set (MWFAS) problem is closely related to the task of deriving a global ranking from pairwise comparisons. Recent work by He et al. (ICML 2022) advanced the state of the art on ranking benchmarks using learning based methods, but did not examine the underlying connection to MWFAS. In this paper, we investigate this relationship and introduce efficient combinatorial algorithms for solving MWFAS as a means of addressing the ranking problem. Our experimental results show that these simple, learning free methods achieve substantially faster runtimes than recent learning based approaches, while also delivering competitive, and in many cases superior, ranking accuracy. These findings suggest that lightweight combinatorial techniques offer a scalable and effective alternative to deep learning for large scale ranking tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ENTIRE: Learning-based Volume Rendering Time Prediction</title>
<link>https://arxiv.org/abs/2501.12119</link>
<guid>https://arxiv.org/abs/2501.12119</guid>
<content:encoded><![CDATA[
arXiv:2501.12119v2 Announce Type: replace-cross 
Abstract: We introduce ENTIRE, a novel deep learning-based approach for fast and accurate volume rendering time prediction. Predicting rendering time is inherently challenging due to its dependence on multiple factors, including volume data characteristics, image resolution, camera configuration, and transfer function settings. Our method addresses this by first extracting a feature vector that encodes structural volume properties relevant to rendering performance. This feature vector is then integrated with additional rendering parameters, such as image resolution, camera setup, and transfer function settings, to produce the final prediction. We evaluate ENTIRE across multiple rendering frameworks (CPU- and GPU-based) and configurations (with and without single-scattering) on diverse datasets. The results demonstrate that our model achieves high prediction accuracy with fast inference speed. Furthermore, we showcase ENTIRE's effectiveness in two case studies, where it enables dynamic parameter adaptation for stable frame rates and load balancing.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent spectral clustering in sparse tensor block models</title>
<link>https://arxiv.org/abs/2501.13820</link>
<guid>https://arxiv.org/abs/2501.13820</guid>
<content:encoded><![CDATA[
arXiv:2501.13820v2 Announce Type: replace-cross 
Abstract: High-order clustering aims to classify objects in multiway datasets that are prevalent in various fields such as bioinformatics, recommendation systems, and social network analysis. Such data are often sparse and high-dimensional, posing significant statistical and computational challenges. This paper introduces a tensor block model specifically designed for sparse integer-valued data tensors. We propose a simple spectral clustering algorithm augmented with a trimming step to mitigate noise fluctuations, and identify a density threshold that ensures the algorithm's consistency. Our approach models sparsity using a sub-Poisson noise concentration framework, accommodating heavier than sub-Gaussian tails. Remarkably, this natural class of tensor block models is closed under aggregation across arbitrary modes. Consequently, we obtain a comprehensive framework for evaluating the tradeoff between signal loss and noise reduction incurred by aggregating data. The analysis is based on a novel concentration bound for sparse random Gram matrices. The theoretical findings are illustrated through numerical experiments.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DAVE: Diagnostic benchmark for Audio Visual Evaluation</title>
<link>https://arxiv.org/abs/2503.09321</link>
<guid>https://arxiv.org/abs/2503.09321</guid>
<content:encoded><![CDATA[
arXiv:2503.09321v2 Announce Type: replace-cross 
Abstract: Audio-visual understanding is a rapidly evolving field that seeks to integrate and interpret information from both auditory and visual modalities. Despite recent advances in multi-modal learning, existing benchmarks often suffer from strong visual bias -- when answers can be inferred from visual data alone -- and provide only aggregate scores that conflate multiple sources of error. This makes it difficult to determine whether models struggle with visual understanding, audio interpretation, or audio-visual alignment. In this work, we introduce DAVE: Diagnostic Audio Visual Evaluation, a novel benchmark dataset designed to systematically evaluate audio-visual models across controlled settings. DAVE alleviates existing limitations by (i) ensuring both modalities are necessary to answer correctly and (ii) decoupling evaluation into atomic subcategories. Our detailed analysis of state-of-the-art models reveals specific failure modes and provides targeted insights for improvement. By offering this standardized diagnostic framework, we aim to facilitate more robust development of audio-visual models.
  Dataset: https://huggingface.co/datasets/gorjanradevski/dave
  Code: https://github.com/gorjanradevski/dave
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformalized Decision Risk Assessment</title>
<link>https://arxiv.org/abs/2505.13243</link>
<guid>https://arxiv.org/abs/2505.13243</guid>
<content:encoded><![CDATA[
arXiv:2505.13243v2 Announce Type: replace-cross 
Abstract: In many operational settings, decision-makers must commit to actions before uncertainty resolves, but existing optimization tools rarely quantify how consistently a chosen decision remains optimal across plausible futures. This paper introduces CREDO -- Conformalized Risk Estimation for Decision Optimization, a distribution-free framework that quantifies the probability that a prescribed decision remains (near-)optimal across realizations of uncertainty. CREDO reformulates decision risk through the inverse feasible region -- the set of outcomes under which a decision is optimal -- and estimates its probability using inner approximations constructed from conformal prediction balls generated by a conditional generative model. By calibrating each ball to lie entirely within the inverse feasible region, CREDO obtains finite-sample valid lower bounds on decision optimality without parametric assumptions. The method avoids the conservatism of worst-case robust optimization, is compatible with modern generative models, and applies broadly to convex optimization problems. We establish theoretical validity guarantees, develop efficient computational procedures, and demonstrate through extensive numerical experiments that CREDO provides accurate, interpretable, and reliable assessments of decision reliability in both synthetic and application-motivated settings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistency-based Abductive Reasoning over Perceptual Errors of Multiple Pre-trained Models in Novel Environments</title>
<link>https://arxiv.org/abs/2505.19361</link>
<guid>https://arxiv.org/abs/2505.19361</guid>
<content:encoded><![CDATA[
arXiv:2505.19361v4 Announce Type: replace-cross 
Abstract: The deployment of pre-trained perception models in novel environments often leads to performance degradation due to distributional shifts. Although recent artificial intelligence approaches for metacognition use logical rules to characterize and filter model errors, improving precision often comes at the cost of reduced recall. This paper addresses the hypothesis that leveraging multiple pre-trained models can mitigate this recall reduction. We formulate the challenge of identifying and managing conflicting predictions from various models as a consistency-based abduction problem, building on the idea of abductive learning (ABL) but applying it to test-time instead of training. The input predictions and the learned error detection rules derived from each model are encoded in a logic program. We then seek an abductive explanation--a subset of model predictions--that maximizes prediction coverage while ensuring the rate of logical inconsistencies (derived from domain constraints) remains below a specified threshold. We propose two algorithms for this knowledge representation task: an exact method based on Integer Programming (IP) and an efficient Heuristic Search (HS). Through extensive experiments on a simulated aerial imagery dataset featuring controlled, complex distributional shifts, we demonstrate that our abduction-based framework outperforms individual models and standard ensemble baselines, achieving, for instance, average relative improvements of approximately 13.6\% in F1-score and 16.6\% in accuracy across 15 diverse test datasets when compared to the best individual model. Our results validate the use of consistency-based abduction as an effective mechanism to robustly integrate knowledge from multiple imperfect models in challenging, novel scenarios.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Large language models can learn and generalize steganographic chain-of-thought under process supervision</title>
<link>https://arxiv.org/abs/2506.01926</link>
<guid>https://arxiv.org/abs/2506.01926</guid>
<content:encoded><![CDATA[
arXiv:2506.01926v2 Announce Type: replace-cross 
Abstract: Chain-of-thought (CoT) reasoning not only enhances large language model performance but also provides critical insights into decision-making processes, marking it as a useful tool for monitoring model intent and planning. However, recent works have shown that banning the mention of a specific example of reward hacking causes obfuscation of the undesired reasoning traces but the persistence of the undesired behavior, threatening the reliability of CoT monitoring. We provide an extension to these results with regard to the ability of models to learn a specific type of obfuscated reasoning: steganography. First, we show that penalizing the use of specific strings within load-bearing reasoning traces causes models to substitute alternative strings. Crucially, this does not alter the underlying method by which the model performs the task, demonstrating that the model can learn to steganographically encode its reasoning.We further demonstrate that models can generalize an encoding scheme. When the penalized strings belong to an overarching class, the model learns not only to substitute strings seen in training, but also develops a general encoding scheme for all members of the class which it can apply to held-out testing strings.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers</title>
<link>https://arxiv.org/abs/2506.09495</link>
<guid>https://arxiv.org/abs/2506.09495</guid>
<content:encoded><![CDATA[
arXiv:2506.09495v2 Announce Type: replace-cross 
Abstract: Suicide remains a leading cause of death in Western countries. As social media becomes central to daily life, digital footprints offer valuable insight into suicidal behavior. Focusing on individuals who attempted suicide while uploading videos to their channels, we investigate: How do linguistic patterns on YouTube reflect suicidal behavior, and how do these patterns align with or differ from expert knowledge? We examined linguistic changes around suicide attempts and compared individuals who attempted suicide while actively uploading to their channel with three control groups: those with prior attempts, those experiencing major life events, and matched individuals from the broader cohort. Applying complementary bottom-up, hybrid, and expert-driven approaches, we analyzed a novel longitudinal dataset of 181 suicide-attempt channels and 134 controls. In the bottom-up analysis, LLM-based topic-modeling identified 166 topics; five were linked to suicide attempts, two also showed attempt-related temporal changes (Mental Health Struggles, $OR = 1.74$; YouTube Engagement, $OR = 1.67$; $p < .01$). In the hybrid approach, clinical experts reviewed LLM-derived topics and flagged 19 as suicide-related. However, none showed significant effects beyond those identified bottom-up. YouTube Engagement, a platform-specific indicator, was not flagged, underscoring the value of bottom-up discovery. A top-down psychological assessment of suicide narratives revealed differing motivations: individuals describing prior attempts aimed to help others ($\beta=-1.69$, $p<.01$), whereas those attempted during the uploading period emphasized personal recovery ($\beta=1.08$, $p<.01$). By integrating these approaches, we offer a nuanced understanding of suicidality, bridging digital behavior and clinical insights.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating the Joint Probability of Scenario Parameters with Gaussian Mixture Copula Models</title>
<link>https://arxiv.org/abs/2506.10098</link>
<guid>https://arxiv.org/abs/2506.10098</guid>
<content:encoded><![CDATA[
arXiv:2506.10098v3 Announce Type: replace-cross 
Abstract: This paper presents the first application of Gaussian Mixture Copula Models to the statistical modeling of driving scenarios for the safety validation of automated driving systems. Knowledge of the joint probability distribution of scenario parameters is essential for scenario-based safety assessment, where risk quantification depends on the likelihood of concrete parameter combinations. Gaussian Mixture Copula Models bring together the multimodal expressivity of Gaussian Mixture Models and the flexibility of copulas, enabling separate modeling of marginal distributions and dependencies. We benchmark Gaussian Mixture Copula Models against previously proposed approaches - Gaussian Mixture Models and Gaussian Copula Models - using real-world driving data drawn from two scenarios defined in United Nations Regulation No. 157. Our evaluation on approximately 18 million instances of these two scenarios demonstrates that Gaussian Mixture Copula Models consistently surpass Gaussian Copula Models and perform better than, or at least comparably to, Gaussian Mixture Models, as measured by both log-likelihood and Sinkhorn distance. These results are promising for the adoption of Gaussian Mixture Copula Models as a statistical foundation for future scenario-based validation frameworks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NeuroPhysNet: A FitzHugh-Nagumo-Based Physics-Informed Neural Network Framework for Electroencephalograph (EEG) Analysis and Motor Imagery Classification</title>
<link>https://arxiv.org/abs/2506.13222</link>
<guid>https://arxiv.org/abs/2506.13222</guid>
<content:encoded><![CDATA[
arXiv:2506.13222v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) is extensively employed in medical diagnostics and brain-computer interface (BCI) applications due to its non-invasive nature and high temporal resolution. However, EEG analysis faces significant challenges, including noise, nonstationarity, and inter-subject variability, which hinder its clinical utility. Traditional neural networks often lack integration with biophysical knowledge, limiting their interpretability, robustness, and potential for medical translation. To address these limitations, this study introduces NeuroPhysNet, a novel Physics-Informed Neural Network (PINN) framework tailored for EEG signal analysis and motor imagery classification in medical contexts. NeuroPhysNet incorporates the FitzHugh-Nagumo model, embedding neurodynamical principles to constrain predictions and enhance model robustness. Evaluated on the BCIC-IV-2a dataset, the framework achieved superior accuracy and generalization compared to conventional methods, especially in data-limited and cross-subject scenarios, which are common in clinical settings. By effectively integrating biophysical insights with data-driven techniques, NeuroPhysNet not only advances BCI applications but also holds significant promise for enhancing the precision and reliability of clinical diagnostics, such as motor disorder assessments and neurorehabilitation planning.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Q-STAC: Q-Guided Stein Variational Model Predictive Actor-Critic</title>
<link>https://arxiv.org/abs/2507.06625</link>
<guid>https://arxiv.org/abs/2507.06625</guid>
<content:encoded><![CDATA[
arXiv:2507.06625v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning (DRL) often struggles with complex robotic manipulation tasks due to low sample efficiency and biased value estimation. Model-based reinforcement learning (MBRL) improves efficiency by leveraging environment dynamics, with prior work integrating Model Predictive Control (MPC) to enhance policy robustness through online trajectory optimization. However, existing MBRL approaches still suffer from high model bias, task-specific cost function design, and significant computational overhead. To address these challenges, we propose Q-guided Stein Variational Model Predictive Actor-Critic (Q-STAC)--a unified framework that bridges Bayesian MPC and Soft Actor-Critic (SAC). Q-STAC employs Stein Variational Gradient Descent (SVGD) to iteratively optimize action sequences sampled from a learned prior distribution guided by Q-values, thereby eliminating manual cost-function engineering. By performing short-horizon model-predictive rollouts, Q-STAC reduces cumulative prediction errors, improves training stability and reduces computational complexity. Experiments on simulated particle navigation, diverse robotic manipulation tasks, and a real-world fruit-picking scenario demonstrate that Q-STAC consistently achieves superior sample efficiency, stability, and overall performance compared to both model-free and model-based baselines.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Investigation of Robustness of LLMs in Mathematical Reasoning: Benchmarking with Mathematically-Equivalent Transformation of Advanced Mathematical Problems</title>
<link>https://arxiv.org/abs/2508.08833</link>
<guid>https://arxiv.org/abs/2508.08833</guid>
<content:encoded><![CDATA[
arXiv:2508.08833v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce a systematic framework beyond conventional method to assess LLMs' mathematical-reasoning robustness by stress-testing them on advanced math problems that are mathematically equivalent but with linguistic and parametric variation. These transformations allow us to measure the sensitivity of LLMs to non-mathematical perturbations, thereby enabling a more accurate evaluation of their mathematical reasoning capabilities. Using this new evaluation methodology, we created PutnamGAP, a new benchmark dataset with multiple mathematically-equivalent variations of competition-level math problems. With the new dataset, we evaluate multiple families of representative LLMs and examine their robustness. Across 18 commercial and open-source models we observe sharp performance degradation on the variants. OpenAI's flagship reasoning model, O3, scores 51.5% on the originals but drops by 4.7 percentage points on surface-renaming variants, and by 12.9 percentage points on parametric variants, while smaller models fare far worse. Overall, the results show that the proposed new evaluation methodology is effective for deepening our understanding of the robustness of LLMs and generating new insights for further improving their mathematical reasoning capabilities.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OPTIC-ER: A Reinforcement Learning Framework for Real-Time Emergency Response and Equitable Resource Allocation in Underserved African Communities</title>
<link>https://arxiv.org/abs/2508.12943</link>
<guid>https://arxiv.org/abs/2508.12943</guid>
<content:encoded><![CDATA[
arXiv:2508.12943v2 Announce Type: replace-cross 
Abstract: Public service systems in many African regions suffer from delayed emergency response and spatial inequity, causing avoidable suffering. This paper introduces OPTIC-ER, a reinforcement learning (RL) framework for real-time, adaptive, and equitable emergency response. OPTIC-ER uses an attention-guided actor-critic architecture to manage the complexity of dispatch environments. Its key innovations are a Context-Rich State Vector, encoding action sub-optimality, and a Precision Reward Function, which penalizes inefficiency. Training occurs in a high-fidelity simulation using real data from Rivers State, Nigeria, accelerated by a precomputed Travel Time Atlas. The system is built on the TALS framework (Thin computing, Adaptability, Low-cost, Scalability) for deployment in low-resource settings. In evaluations on 500 unseen incidents, OPTIC-ER achieved a 100.00% optimal action selection rate, confirming its robustness and generalization. Beyond dispatch, the system generates Infrastructure Deficiency Maps and Equity Monitoring Dashboards to guide proactive governance and data-informed development. This work presents a validated blueprint for AI-augmented public services, showing how context-aware RL can bridge the gap between algorithmic decision-making and measurable human impact.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>One-shot acceleration of transient PDE solvers via online-learned preconditioners</title>
<link>https://arxiv.org/abs/2509.08765</link>
<guid>https://arxiv.org/abs/2509.08765</guid>
<content:encoded><![CDATA[
arXiv:2509.08765v3 Announce Type: replace-cross 
Abstract: Data-driven acceleration of scientific computing workflows has been a high-profile aim of machine learning (ML) for science, with numerical simulation of transient partial differential equations (PDEs) being one of the main applications. The focus thus far has been on methods that require classical simulations to train, which when combined with the data-hungriness and optimization challenges of neural networks has caused difficulties in demonstrating a convincing advantage against strong classical baselines. We consider an alternative paradigm in which the learner uses a classical solver's own data to accelerate it, enabling a one-shot speedup of the simulation. Concretely, since transient PDEs often require solving a sequence of related linear systems, the feedback from repeated calls to a linear solver such as preconditioned conjugate gradient (PCG) can be used by a bandit algorithm to online-learn an adaptive sequence of solver configurations (e.g. preconditioners). The method we develop, PCGBandit, is implemented directly on top of the popular open source software OpenFOAM, which we use to show its effectiveness on a set of fluid and magnetohydrodynamics (MHD) problems.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Downscaling climate projections to 1 km with single-image super resolution</title>
<link>https://arxiv.org/abs/2509.21399</link>
<guid>https://arxiv.org/abs/2509.21399</guid>
<content:encoded><![CDATA[
arXiv:2509.21399v2 Announce Type: replace-cross 
Abstract: High-resolution climate projections are essential for local decision-making. However, available climate projections have low spatial resolution (e.g. 12.5 km), which limits their usability. We address this limitation by leveraging single-image super-resolution models to statistically downscale climate projections to 1-km resolution. Since high-resolution climate projections are unavailable, we train models on a high-resolution observational gridded data set and apply them to low-resolution climate projections. We cannot evaluate downscaled climate projections with common metrics (e.g. pixel-wise root-mean-square error) because we lack ground-truth high-resolution climate projections. Therefore, we evaluate climate indicators computed at weather station locations. Experiments on daily mean temperature demonstrate that single-image super-resolution models can downscale climate projections without increasing the error of climate indicators compared to low-resolution climate projections.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Near-Optimal Experiment Design in Linear non-Gaussian Cyclic Models</title>
<link>https://arxiv.org/abs/2509.21423</link>
<guid>https://arxiv.org/abs/2509.21423</guid>
<content:encoded><![CDATA[
arXiv:2509.21423v2 Announce Type: replace-cross 
Abstract: We study the problem of causal structure learning from a combination of observational and interventional data generated by a linear non-Gaussian structural equation model that might contain cycles. Recent results show that using mere observational data identifies the causal graph only up to a permutation-equivalence class. We obtain a combinatorial characterization of this class by showing that each graph in an equivalence class corresponds to a perfect matching in a bipartite graph. This bipartite representation allows us to analyze how interventions modify or constrain the matchings. Specifically, we show that each atomic intervention reveals one edge of the true matching and eliminates all incompatible causal graphs. Consequently, we formalize the optimal experiment design task as an adaptive stochastic optimization problem over the set of equivalence classes with a natural reward function that quantifies how many graphs are eliminated from the equivalence class by an intervention. We show that this reward function is adaptive submodular and provide a greedy policy with a provable near-optimal performance guarantee. A key technical challenge is to efficiently estimate the reward function without having to explicitly enumerate all the graphs in the equivalence class. We propose a sampling-based estimator using random matchings and analyze its bias and concentration behavior. Our simulation results show that performing a small number of interventions guided by our stochastic optimization framework recovers the true underlying causal structure.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>IndiSeek learns information-guided disentangled representations</title>
<link>https://arxiv.org/abs/2509.21584</link>
<guid>https://arxiv.org/abs/2509.21584</guid>
<content:encoded><![CDATA[
arXiv:2509.21584v4 Announce Type: replace-cross 
Abstract: Learning disentangled representations is a fundamental task in multi-modal learning. In modern applications such as single-cell multi-omics, both shared and modality-specific features are critical for characterizing cell states and supporting downstream analyses. Ideally, modality-specific features should be independent of shared ones while also capturing all complementary information within each modality. This tradeoff is naturally expressed through information-theoretic criteria, but mutual-information-based objectives are difficult to estimate reliably, and their variational surrogates often underperform in practice. In this paper, we introduce IndiSeek, a novel disentangled representation learning approach that addresses this challenge by combining an independence-enforcing objective with a computationally efficient reconstruction loss that bounds conditional mutual information. This formulation explicitly balances independence and completeness, enabling principled extraction of modality-specific features. We demonstrate the effectiveness of IndiSeek on synthetic simulations, a CITE-seq dataset and multiple real-world multi-modal benchmarks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MORPH: PDE Foundation Models with Arbitrary Data Modality</title>
<link>https://arxiv.org/abs/2509.21670</link>
<guid>https://arxiv.org/abs/2509.21670</guid>
<content:encoded><![CDATA[
arXiv:2509.21670v3 Announce Type: replace-cross 
Abstract: We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Stochastic Optimization of LogSumExp</title>
<link>https://arxiv.org/abs/2509.24894</link>
<guid>https://arxiv.org/abs/2509.24894</guid>
<content:encoded><![CDATA[
arXiv:2509.24894v2 Announce Type: replace-cross 
Abstract: The LogSumExp function, also known as the free energy, plays a central role in many important optimization problems, including entropy-regularized optimal transport and distributionally robust optimization (DRO). It is also the dual to the Kullback-Leibler (KL) divergence, which is widely used in machine learning. In practice, when the number of exponential terms inside the logarithm is large or infinite, optimization becomes challenging since computing the gradient requires differentiating every term. Previous approaches that replace the full sum with a small batch introduce significant bias. We propose a novel approximation to LogSumExp that can be efficiently optimized using stochastic gradient methods. This approximation is rooted in a sound modification of the KL divergence in the dual, resulting in a new $f$-divergence called the safe KL divergence. The accuracy of the approximation is controlled by a tunable parameter and can be made arbitrarily small. Like the LogSumExp, our approximation preserves convexity. Moreover, when applied to an $L$-smooth function bounded from below, the smoothness constant of the resulting objective scales linearly with $L$. Experiments in DRO and continuous optimal transport demonstrate the advantages of our approach over state-of-the-art baselines and the effective treatment of numerical issues associated with the standard LogSumExp and KL.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Kernel Selection for Stein Variational Gradient Descent</title>
<link>https://arxiv.org/abs/2510.02067</link>
<guid>https://arxiv.org/abs/2510.02067</guid>
<content:encoded><![CDATA[
arXiv:2510.02067v2 Announce Type: replace-cross 
Abstract: A central challenge in Bayesian inference is efficiently approximating posterior distributions. Stein Variational Gradient Descent (SVGD) is a popular variational inference method which transports a set of particles to approximate a target distribution. The SVGD dynamics are governed by a reproducing kernel Hilbert space (RKHS) and are highly sensitive to the choice of the kernel function, which directly influences both convergence and approximation quality. The commonly used median heuristic offers a simple approach for setting kernel bandwidths but lacks flexibility and often performs poorly, particularly in high-dimensional settings. In this work, we propose an alternative strategy for adaptively choosing kernel parameters over an abstract family of kernels. Recent convergence analyses based on the kernelized Stein discrepancy (KSD) suggest that optimizing the kernel parameters by maximizing the KSD can improve performance. Building on this insight, we introduce Adaptive SVGD (Ad-SVGD), a method that alternates between updating the particles via SVGD and adaptively tuning kernel bandwidths through gradient ascent on the KSD. We provide a simplified theoretical analysis that extends existing results on minimizing the KSD for fixed kernels to our adaptive setting, showing convergence properties for the maximal KSD over our kernel class. Our empirical results further support this intuition: Ad-SVGD consistently outperforms standard heuristics in a variety of tasks.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting</title>
<link>https://arxiv.org/abs/2510.05497</link>
<guid>https://arxiv.org/abs/2510.05497</guid>
<content:encoded><![CDATA[
arXiv:2510.05497v2 Announce Type: replace-cross 
Abstract: Large-scale Mixture of Experts (MoE) Large Language Models (LLMs) have recently become the frontier open weight models, achieving remarkable model capability similar to proprietary ones. But their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit LLM serving systems.
  To understand the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across four state-of-the-art large-scale MoE models released in 2025 (200B-1000B) using over 24,000 requests spanning diverse workloads. We perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. With our insights, we then demonstrate how to improve wafer-scale GPUs as a case study, and show that minor architectural modifications leveraging the insights achieve substantial performance gains, delivering 5.3x and 3.1x average speedups on DeepSeek V3 and Qwen3, respectively. Our work presents the first comprehensive data-centric analysis of large-scale MoE models and a concrete design study using the learned lessons, with profiling traces and simulation framework already open-sourced with $>$1k downloads. Our traces and results are publicly available at https://huggingface.co/datasets/core12345/MoE_expert_selection_trace
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Locality-Sensitive Hashing-Based Efficient Point Transformer for Charged Particle Reconstruction</title>
<link>https://arxiv.org/abs/2510.07594</link>
<guid>https://arxiv.org/abs/2510.07594</guid>
<content:encoded><![CDATA[
arXiv:2510.07594v2 Announce Type: replace-cross 
Abstract: Charged particle track reconstruction is a foundational task in collider experiments and the main computational bottleneck in particle reconstruction. Graph neural networks (GNNs) have shown strong performance for this problem, but costly graph construction, irregular computations, and random memory access patterns substantially limit their throughput. The recently proposed Hashing-based Efficient Point Transformer (HEPT) offers a theoretically guaranteed near-linear complexity for large point cloud processing via locality-sensitive hashing (LSH) in attention computations; however, its evaluations have largely focused on embedding quality, and the object condensation pipeline on which HEPT relies requires a post-hoc clustering step (e.g., DBScan) that can dominate runtime. In this work, we make two contributions. First, we present a unified, fair evaluation of physics tracking performance for HEPT and a representative GNN-based pipeline under the same dataset and metrics. Second, we introduce HEPTv2 by extending HEPT with a lightweight decoder that eliminates the clustering stage and directly predicts track assignments. This modification preserves HEPT's regular, hardware-friendly computations while enabling ultra-fast end-to-end inference. On the TrackML dataset, optimized HEPTv2 achieves approximately 28 ms per event on an A100 while maintaining competitive tracking efficiency. These results position HEPTv2 as a practical, scalable alternative to GNN-based pipelines for fast tracking.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Triangle Multiplication Is All You Need For Biomolecular Structure Representations</title>
<link>https://arxiv.org/abs/2510.18870</link>
<guid>https://arxiv.org/abs/2510.18870</guid>
<content:encoded><![CDATA[
arXiv:2510.18870v2 Announce Type: replace-cross 
Abstract: AlphaFold has transformed protein structure prediction, but emerging applications such as virtual ligand screening, proteome-wide folding, and de novo binder design demand predictions at a massive scale, where runtime and memory costs become prohibitive. A major bottleneck lies in the Pairformer backbone of AlphaFold3-style models, which relies on computationally expensive triangular primitives-especially triangle attention-for pairwise reasoning. We introduce Pairmixer, a streamlined alternative that eliminates triangle attention while preserving higher-order geometric reasoning capabilities that are critical for structure prediction. Pairmixer substantially improves computational efficiency, matching state-of-the-art structure predictors across folding and docking benchmarks, delivering up to 4x faster inference on long sequences while reducing training cost by 34%. Its efficiency alleviates the computational burden of downstream applications such as modeling large protein complexes, high-throughput ligand and binder screening, and hallucination-based design. Within BoltzDesign, for example, Pairmixer delivers over 2x faster sampling and scales to sequences ~30% longer than the memory limits of Pairformer. Code is available at https://github.com/genesistherapeutics/pairmixer.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TraceTrans: Translation and Spatial Tracing for Surgical Prediction</title>
<link>https://arxiv.org/abs/2510.22379</link>
<guid>https://arxiv.org/abs/2510.22379</guid>
<content:encoded><![CDATA[
arXiv:2510.22379v4 Announce Type: replace-cross 
Abstract: Image-to-image translation models have achieved notable success in converting images across visual domains and are increasingly used for medical tasks such as predicting post-operative outcomes and modeling disease progression. However, most existing methods primarily aim to match the target distribution and often neglect spatial correspondences between the source and translated images. This limitation can lead to structural inconsistencies and hallucinations, undermining the reliability and interpretability of the predictions. These challenges are accentuated in clinical applications by the stringent requirement for anatomical accuracy. In this work, we present TraceTrans, a novel deformable image translation model designed for post-operative prediction that generates images aligned with the target distribution while explicitly revealing spatial correspondences with the pre-operative input. The framework employs an encoder for feature extraction and dual decoders for predicting spatial deformations and synthesizing the translated image. The predicted deformation field imposes spatial constraints on the generated output, ensuring anatomical consistency with the source. Extensive experiments on medical cosmetology and brain MRI datasets demonstrate that TraceTrans delivers accurate and interpretable post-operative predictions, highlighting its potential for reliable clinical deployment.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enabling Fast and Accurate Neutral Atom Readout through Image Denoising</title>
<link>https://arxiv.org/abs/2510.25982</link>
<guid>https://arxiv.org/abs/2510.25982</guid>
<content:encoded><![CDATA[
arXiv:2510.25982v2 Announce Type: replace-cross 
Abstract: Neutral atom quantum computers hold promise for scaling up to hundreds of thousands or more qubits, but their progress is constrained by slow qubit readout. Parallel measurement of qubit arrays currently takes milliseconds, much longer than the underlying quantum gate operations-making readout the primary bottleneck in deploying quantum error correction. Because each round of QEC depends on measurement, long readout times increase cycle duration and slow down program execution. Reducing the readout duration speeds up cycles and reduces decoherence errors that accumulate while qubits idle, but it also lowers the number of collected photons, making measurements noisier and more error-prone. This tradeoff leaves neutral atom systems stuck between slow but accurate readout and fast but unreliable readout.
  We show that image denoising can resolve this tension. Our framework, GANDALF, uses explicit denoising using image translation to reconstruct clear signals from short, low-photon measurements, enabling reliable classification at up to 1.6x shorter readout times. Combined with lightweight classifiers and a pipelined readout design, our approach both reduces logical error rate by up to 35x and overall QEC cycle time up to 1.77x compared to state-of-the-art convolutional neural network (CNN)-based readout for Cesium (Cs) Neutral Atom arrays.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Epidemiology of Large Language Models: A Benchmark for Observational Distribution Knowledge</title>
<link>https://arxiv.org/abs/2511.03070</link>
<guid>https://arxiv.org/abs/2511.03070</guid>
<content:encoded><![CDATA[
arXiv:2511.03070v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) systems hold great promise for advancing various scientific disciplines, and are increasingly used in real-world applications. Despite their remarkable progress, further capabilities are expected in order to achieve more general types of intelligence. A critical distinction in this context is between factual knowledge, which can be evaluated against true or false answers (e.g., "what is the capital of England?"), and probabilistic knowledge, reflecting probabilistic properties of the real world (e.g., "what is the sex of a computer science graduate in the US?"). In this paper, our goal is to build a benchmark for understanding the capabilities of LLMs in terms of knowledge of probability distributions describing the real world. Given that LLMs are trained on vast amounts of text, it may be plausible that they internalize aspects of these distributions. Indeed, LLMs are touted as powerful universal approximators of real-world distributions. At the same time, classical results in statistics, known as curse of dimensionality, highlight fundamental challenges in learning distributions in high dimensions, challenging the notion of universal distributional learning. In this work, we develop the first benchmark to directly test this hypothesis, evaluating whether LLMs have access to empirical distributions describing real-world populations across domains such as economics, health, education, and social behavior. Our results demonstrate that LLMs perform poorly overall, and do not seem to internalize real-world statistics naturally. When interpreted in the context of Pearl's Causal Hierarchy (PCH), our benchmark demonstrates that language models do not contain knowledge on observational distributions (Layer 1 of PCH), and thus the Causal Hierarchy Theorem implies that interventional (Layer 2) and counterfactual (Layer 3) knowledge of these models is also limited.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Peril of Preference: Why GRPO fails on Ordinal Rewards</title>
<link>https://arxiv.org/abs/2511.04439</link>
<guid>https://arxiv.org/abs/2511.04439</guid>
<content:encoded><![CDATA[
arXiv:2511.04439v2 Announce Type: replace-cross 
Abstract: Group-relative Policy Optimization's (GRPO) simplicity makes it highly desirable for adapting LLMs to become experts at specific tasks. But this simplicity also makes it ill-specified as we seek to enhance RL training with richer, non-binary feedback. When using ordinal rewards to give partial credit, GRPO's simplicity starts to hurt, as its group-average baseline often assigns a positive advantage to failed trajectories and reinforces incorrect behavior.
  We introduce Correctness Relative Policy Optimization (CoRPO), a new formulation that solves this flaw. CoRPO uses an adaptive baseline that enforces a minimum quality threshold, ensuring failed solutions are never positively reinforced. Once the policy consistently meets this threshold, the baseline automatically transitions to a relative preference mode, pushing the model to find optimal solutions rather than just "acceptable" ones. We empirically validate CoRPO on a code verification task, where it demonstrates more stable convergence and better out-of-domain generalization.
  This work represents a critical step in our broader research program to enable LLMs to learn genuinely new capabilities through reinforcement learning. We achieve this by enabling LLMs to learn from rich, multi-dimensional feedback - progressing from binary to ordinal rewards in this work, and onward to denser, per-step supervision.
]]></content:encoded>
<pubDate>Fri, 05 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sharpness of Minima in Deep Matrix Factorization: Exact Expressions</title>
<link>https://arxiv.org/abs/2509.25783</link>
<guid>https://arxiv.org/abs/2509.25783</guid>
<content:encoded><![CDATA[
<div> Keywords: loss landscape, Hessian maximum eigenvalue, deep matrix factorization, sharpness, gradient-based optimization<br /><br />Summary:  
1. The paper addresses the challenge of understanding the geometry of the loss landscape near minima in non-convex optimization problems such as deep neural network training and deep matrix factorization.  
2. A key measure to characterize this geometry is the maximum eigenvalue of the Hessian matrix of the loss function, which quantifies the sharpness or flatness of minima.  
3. Prior to this work, no exact formulas for this sharpness measure existed for general settings, limiting the understanding of its precise role.  
4. The authors provide the first exact closed-form expression for the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in overparameterized deep matrix factorization problems (equivalent to deep linear neural networks), resolving an open question posed by Mulayoff & Michaeli (2020).  
5. Their results reveal a fundamental property for depth-2 matrix factorization: a minimum is flat if and only if it is spectral-norm balanced, distinguishing it from Frobenius-norm balanced minima, indicating that flat minima are not necessarily balanced in Frobenius norm.  
6. To validate their theoretical findings, they empirically investigate a phenomenon in gradient-based training where the model escapes near-minima regions, a behavior that critically depends on their exact sharpness expression. <div>
arXiv:2509.25783v4 Announce Type: replace-cross 
Abstract: Understanding the geometry of the loss landscape near a minimum is key to explaining the implicit bias of gradient-based methods in non-convex optimization problems such as deep neural network training and deep matrix factorization. A central quantity to characterize this geometry is the maximum eigenvalue of the Hessian of the loss, which measures the sharpness of the landscape. Currently, its precise role has been obfuscated because no exact expressions for this sharpness measure were known in general settings. In this paper, we present the first exact expression for the maximum eigenvalue of the Hessian of the squared-error loss at any minimizer in general overparameterized deep matrix factorization (i.e., deep linear neural network training) problems, resolving an open question posed by Mulayoff & Michaeli (2020). This expression uncovers a fundamental property of the loss landscape of depth-2 matrix factorization problems: a minimum is flat if and only if it is spectral-norm balanced, which implies that flat minima are not necessarily Frobenius-norm balanced. Furthermore, to complement our theory, we empirically investigate an escape phenomenon observed during gradient-based training near a minimum that crucially relies on our exact expression of the sharpness.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Informed Machine Learning for Steel Development: A Computational Framework and CCT Diagram Modelling</title>
<link>https://arxiv.org/abs/2512.03050</link>
<guid>https://arxiv.org/abs/2512.03050</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, continuous cooling transformation, steels, phase classification, materials design<br /><br />Summary:<br /><br />1. The paper addresses the challenge of applying general-purpose machine learning (ML) frameworks to complex industrial materials like steel, where capturing the relationship among chemical composition, processing parameters, microstructure, and properties is critical. <br />2. The authors present a physics-informed computational framework that combines physical insights with ML to develop a continuous cooling transformation (CCT) model specifically for steels. <br />3. The model is trained on a large dataset comprising 4,100 CCT diagrams and validated against both literature and experimental data to ensure accuracy. <br />4. The framework achieves high computational efficiency, able to generate complete CCT diagrams featuring 100 cooling curves within under 5 seconds. <br />5. The model demonstrates strong generalizability across various alloy steels, scoring F1 classification above 88% for all phases and maintaining mean absolute errors below 20 °C for phase transition temperature regression, except for bainite, which exhibits a slightly higher error of 27 °C. <br />6. The proposed framework is extensible with additional ML models and intended to become part of a universal digital twin platform for heat treatment processes. <br />7. Integration with complementary simulation techniques and targeted experiments is suggested to further accelerate materials design and manufacturing workflows. <div>
arXiv:2512.03050v1 Announce Type: new 
Abstract: Machine learning (ML) has emerged as a powerful tool for accelerating the computational design and production of materials. In materials science, ML has primarily supported large-scale discovery of novel compounds using first-principles data and digital twin applications for optimizing manufacturing processes. However, applying general-purpose ML frameworks to complex industrial materials such as steel remains a challenge. A key obstacle is accurately capturing the intricate relationship between chemical composition, processing parameters, and the resulting microstructure and properties. To address this, we introduce a computational framework that combines physical insights with ML to develop a physics-informed continuous cooling transformation (CCT) model for steels. Our model, trained on a dataset of 4,100 diagrams, is validated against literature and experimental data. It demonstrates high computational efficiency, generating complete CCT diagrams with 100 cooling curves in under 5 seconds. It also shows strong generalizability across alloy steels, achieving phase classification F1 scores above 88% for all phases. For phase transition temperature regression, it attains mean absolute errors (MAE) below 20 {\deg}C across all phases except bainite, which shows a slightly higher MAE of 27 {\deg}C. This framework can be extended with additional generic and customized ML models to establish a universal digital twin platform for heat treatment. Integration with complementary simulation tools and targeted experiments will further support accelerated materials design workflows.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating hallucinations and omissions in LLMs for invertible problems: An application to hardware logic design automation</title>
<link>https://arxiv.org/abs/2512.03053</link>
<guid>https://arxiv.org/abs/2512.03053</guid>
<content:encoded><![CDATA[
<div> Keywords: invertible problems, large language models, lossless encoding, hardware description language, design verification<br /><br />Summary: This paper addresses invertible problems where data is transformed between a source domain, such as Logic Condition Tables (LCTs), and a destination domain, such as Hardware Description Language (HDL) code. The authors propose using Large Language Models (LLMs) as lossless encoders and decoders, effectively treating the LLMs like compression tools that can translate from source to destination and back without data loss. This method helps to reduce common issues with LLMs, including hallucinations and omissions during code generation. The approach is demonstrated by generating complete HDL code for a two-dimensional network-on-chip router from LCT inputs using seven different LLMs. They then reconstructed the original LCTs from this auto-generated HDL and compared them to the originals. Results show that this technique not only confirms correctly generated logic and detects errors in the LLM output but also enhances productivity by assisting developers in identifying specification mistakes. The study highlights the potential of LLMs as reliable, reversible translators between complex technical domains, improving accuracy and confidence in automated code generation workflows. <div>
arXiv:2512.03053v1 Announce Type: new 
Abstract: We show for invertible problems that transform data from a source domain (for example, Logic Condition Tables (LCTs)) to a destination domain (for example, Hardware Description Language (HDL) code), an approach of using Large Language Models (LLMs) as a lossless encoder from source to destination followed by as a lossless decoder back to the source, comparable to lossless compression in information theory, can mitigate most of the LLM drawbacks of hallucinations and omissions. Specifically, using LCTs as inputs, we generate the full HDL for a two-dimensional network-on-chip router (13 units, 1500-2000 lines of code) using seven different LLMs, reconstruct the LCTs from the auto-generated HDL, and compare the original and reconstructed LCTs. This approach yields significant productivity improvements, not only confirming correctly generated LLM logic and detecting incorrectly generated LLM logic but also assisting developers in finding design specification errors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Efficient Federated Learning via Adaptive Encoder Freezing for MRI-to-CT Conversion: A Green AI-Guided Research</title>
<link>https://arxiv.org/abs/2512.03054</link>
<guid>https://arxiv.org/abs/2512.03054</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Green AI, adaptive layer-freezing, MRI-to-CT conversion, energy efficiency<br /><br />Summary:<br /><br />1. Federated Learning (FL) enables multiple healthcare institutions to collaboratively train deep learning models without sharing data, helping advance equality in health.  
2. FL's high computational and energy demands tend to exclude centers with limited resources, potentially worsening healthcare disparities.  
3. The authors propose a Green AI-oriented adaptive layer-freezing strategy that reduces energy consumption and computational load during federated training while preserving model quality.  
4. Their approach selectively freezes encoder weights based on the relative weight changes between training rounds, using a patience mechanism to ensure freezing only occurs when updates remain consistently small.  
5. This method was applied to various federated architectures aimed at converting MRI images to CT images.  
6. Using the CodeCarbon library, they monitored energy use and CO2 equivalent emissions, finding reductions of up to 23% in training time, energy consumption, and CO2 emissions compared to non-frozen training.  
7. The MRI-to-CT conversion performance, measured by Mean Absolute Error, was maintained with minimal variation; three of five architectures showed no statistically significant differences, while two showed significant improvements.  
8. This work supports the development of sustainable, equitable, and privacy-preserving AI models in healthcare, aligning clinical performance with environmental and social responsibility.  
9. It establishes a foundation for new evaluation frameworks in federated learning, promoting justice and equity in AI-driven medical applications. <div>
arXiv:2512.03054v1 Announce Type: new 
Abstract: Federated Learning (FL) holds the potential to advance equality in health by enabling diverse institutions to collaboratively train deep learning (DL) models, even with limited data. However, the significant resource requirements of FL often exclude centres with limited computational infrastructure, further widening existing healthcare disparities. To address this issue, we propose a Green AI-oriented adaptive layer-freezing strategy designed to reduce energy consumption and computational load while maintaining model performance. We tested our approach using different federated architectures for Magnetic Resonance Imaging (MRI)-to-Computed Tomography (CT) conversion. The proposed adaptive strategy optimises the federated training by selectively freezing the encoder weights based on the monitored relative difference of the encoder weights from round to round. A patience-based mechanism ensures that freezing only occurs when updates remain consistently minimal. The energy consumption and CO2eq emissions of the federation were tracked using the CodeCarbon library. Compared to equivalent non-frozen counterparts, our approach reduced training time, total energy consumption and CO2eq emissions by up to 23%. At the same time, the MRI-to-CT conversion performance was maintained, with only small variations in the Mean Absolute Error (MAE). Notably, for three out of the five evaluated architectures, no statistically significant differences were observed, while two architectures exhibited statistically significant improvements. Our work aligns with a research paradigm that promotes DL-based frameworks meeting clinical requirements while ensuring climatic, social, and economic sustainability. It lays the groundwork for novel FL evaluation frameworks, advancing privacy, equity and, more broadly, justice in AI-driven healthcare.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-informed self-supervised learning for predictive modeling of coronary artery digital twins</title>
<link>https://arxiv.org/abs/2512.03055</link>
<guid>https://arxiv.org/abs/2512.03055</guid>
<content:encoded><![CDATA[
<div> Keywords: cardiovascular disease, coronary artery disease, physics-informed learning, graph neural networks, fractional flow reserve  

<br /><br />Summary:  
Cardiovascular disease is the leading cause of death worldwide, with coronary artery disease (CAD) as the most common form, emphasizing the need for early risk prediction. Traditional assessment methods using 3D coronary artery digital twins rely heavily on computational fluid dynamics (CFD), which is computationally expensive and limits widespread application. Data-driven approaches face challenges due to limited labeled clinical data and the lack of integrated physiological knowledge. To overcome these barriers, the study introduces PINS-CAD, a physics-informed self-supervised learning framework that pre-trains graph neural networks on 200,000 synthetic coronary digital twins. The model incorporates physical laws such as the 1D Navier-Stokes equations and pressure-drop relationships, thereby eliminating the dependency on CFD simulations and labeled datasets. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD achieves an AUC of 0.73 in predicting future cardiovascular events, exceeding the performance of standard clinical risk scores and purely data-driven models. The framework also produces spatially resolved pressure and fractional flow reserve curves, providing interpretable, physiologically meaningful biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD enables a scalable, simulation-free, and physiology-aware analysis of routine angiography, advancing preventive cardiology practices. <div>
arXiv:2512.03055v1 Announce Type: new 
Abstract: Cardiovascular disease is the leading global cause of mortality, with coronary artery disease (CAD) as its most prevalent form, necessitating early risk prediction. While 3D coronary artery digital twins reconstructed from imaging offer detailed anatomy for personalized assessment, their analysis relies on computationally intensive computational fluid dynamics (CFD), limiting scalability. Data-driven approaches are hindered by scarce labeled data and lack of physiological priors. To address this, we present PINS-CAD, a physics-informed self-supervised learning framework. It pre-trains graph neural networks on 200,000 synthetic coronary digital twins to predict pressure and flow, guided by 1D Navier-Stokes equations and pressure-drop laws, eliminating the need for CFD or labeled data. When fine-tuned on clinical data from 635 patients in the multicenter FAME2 study, PINS-CAD predicts future cardiovascular events with an AUC of 0.73, outperforming clinical risk scores and data-driven baselines. This demonstrates that physics-informed pretraining boosts sample efficiency and yields physiologically meaningful representations. Furthermore, PINS-CAD generates spatially resolved pressure and fractional flow reserve curves, providing interpretable biomarkers. By embedding physical priors into geometric deep learning, PINS-CAD transforms routine angiography into a simulation-free, physiology-aware framework for scalable, preventive cardiology.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Delta Sampling: Data-Free Knowledge Transfer Across Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03056</link>
<guid>https://arxiv.org/abs/2512.03056</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Models, Stable Diffusion, Knowledge Transfer, Delta Sampling, Image Synthesis<br /><br />Summary:  
1. This paper addresses the challenge of adapting parameter-efficient components (like LoRA, LyCORIS, ControlNet) to upgraded diffusion models, especially when the base models differ significantly in architecture or parameters, such as between Stable Diffusion (SD) versions 1.x and 2.x.  
2. The authors propose Delta Sampling (DS), a novel inference-time method that facilitates knowledge transfer across different base models without needing access to the original training data.  
3. DS works by capturing the "delta," or the difference in model predictions before and after adaptation of a source base model, and then using this delta to guide the denoising steps during inference with a new base model.  
4. This approach allows the reuse of adaptation components like fine-tuned checkpoints and adapters on different underlying diffusion models, overcoming the typical incompatibility caused by architectural changes.  
5. Experimental results across multiple Stable Diffusion versions show that DS consistently improves the generation of the intended visual effects (including style, semantics, and structure) under various sampling techniques, making DS an effective, plug-and-play tool for cross-version knowledge transfer in diffusion-based image synthesis.  
6. The authors provide code to implement Delta Sampling, encouraging further adoption within the open-source community. <div>
arXiv:2512.03056v1 Announce Type: new 
Abstract: Diffusion models like Stable Diffusion (SD) drive a vibrant open-source ecosystem including fully fine-tuned checkpoints and parameter-efficient adapters such as LoRA, LyCORIS, and ControlNet. However, these adaptation components are tightly coupled to a specific base model, making them difficult to reuse when the base model is upgraded (e.g., from SD 1.x to 2.x) due to substantial changes in model parameters and architecture. In this work, we propose Delta Sampling (DS), a novel method that enables knowledge transfer across base models with different architectures, without requiring access to the original training data. DS operates entirely at inference time by leveraging the delta: the difference in model predictions before and after the adaptation of a base model. This delta is then used to guide the denoising process of a new base model. We evaluate DS across various SD versions, demonstrating that DS achieves consistent improvements in creating desired effects (e.g., visual styles, semantic concepts, and structures) under different sampling strategies. These results highlight DS as an effective, plug-and-play mechanism for knowledge transfer in diffusion-based image synthesis. Code:~ https://github.com/Zhidong-Gao/DeltaSampling
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamical Properties of Tokens in Self-Attention and Effects of Positional Encoding</title>
<link>https://arxiv.org/abs/2512.03058</link>
<guid>https://arxiv.org/abs/2512.03058</guid>
<content:encoded><![CDATA[
<div> Tokens dynamics, pre-trained Transformers, positional encoding, convergence behavior, Transformer refinement  

<br /><br />Summary:  
This paper studies the dynamical behavior of tokens within pre-trained Transformer models by analyzing the continuous-time limit of these models as dynamical systems. It characterizes how tokens move relative to each other over time and identifies the asymptotic outcomes, such as tokens converging to zero or diverging to infinity, based on model parameters. The authors provide a set of sufficient conditions for these behaviors that are more general and applicable to practical, real-world Transformer models compared to previous work. Additionally, the paper investigates the influence of different positional encoding schemes—specifically absolute positional encoding and rotary positional encoding—on these dynamical regimes and finds that convergence scenarios negatively impact model performance. Motivated by these insights, the authors propose straightforward architectural refinements to Transformers that help mitigate the undesirable convergence behavior in models employing either absolute or rotary positional encoding. The results collectively provide both theoretical insights and practical design principles aimed at improving the stability and performance of Transformer models by controlling token dynamics. <div>
arXiv:2512.03058v1 Announce Type: new 
Abstract: This paper investigates the dynamical properties of tokens in pre-trained Transformer models and explores their application to improving Transformers. To this end, we analyze the dynamical system governing the continuous-time limit of the pre-trained model and characterize the asymptotic behavior of its solutions. Specifically, we characterize when tokens move closer to or farther from one another over time, depending on the model parameters. We provide sufficient conditions, based on these parameters, to identify scenarios where tokens either converge to zero or diverge to infinity. Unlike prior works, our conditions are broader in scope and more applicable to real-world models. Furthermore, we investigate how different forms of positional encoding -- specifically absolute and rotary -- affect these dynamical regimes. Empirical evidence reveals that the convergence scenario adversely impacts model performance. Motivated by these insights, we propose simple refinements to Transformer architectures that mitigate convergence behavior in models with absolute or rotary positional encoding. These findings support theoretical foundations and design principles for improving Transformer models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safe and Sustainable Electric Bus Charging Scheduling with Constrained Hierarchical DRL</title>
<link>https://arxiv.org/abs/2512.03059</link>
<guid>https://arxiv.org/abs/2512.03059</guid>
<content:encoded><![CDATA[
<div> Electric Buses, Renewable Energy, Hierarchical Deep Reinforcement Learning, Charging Scheduling, Constrained Markov Decision Process  

<br /><br />Summary:  
This paper addresses the challenge of optimizing Electric Bus (EB) charging schedules integrated with photovoltaic (PV) renewable energy sources to minimize operational costs while ensuring battery safety. The authors recognize real-world uncertainties such as variable PV generation, dynamic electricity prices, fluctuating travel times, and limited charging infrastructure. To tackle these challenges, they model the problem as a Constrained Markov Decision Process (CMDP) with options that allow temporally abstract decision-making. They propose a novel Hierarchical Deep Reinforcement Learning (HDRL) framework, introducing the Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian) algorithm. This method integrates Lagrangian relaxation within a Double Actor-Critic framework to handle constraints effectively. At the high level, a centralized PPO-Lagrangian learns safe charger allocation policies, while at the low level, MAPPO-Lagrangian handles decentralized decisions for charging power using Centralized Training and Decentralized Execution (CTDE). Extensive experiments leveraging real-world data demonstrate that the proposed approach surpasses existing baselines in minimizing costs and maintaining safety compliance. Additionally, the method achieves fast convergence, proving it effective and practical for real-world EB charging scheduling under uncertain operational conditions. <div>
arXiv:2512.03059v1 Announce Type: new 
Abstract: The integration of Electric Buses (EBs) with renewable energy sources such as photovoltaic (PV) panels is a promising approach to promote sustainable and low-carbon public transportation. However, optimizing EB charging schedules to minimize operational costs while ensuring safe operation without battery depletion remains challenging - especially under real-world conditions, where uncertainties in PV generation, dynamic electricity prices, variable travel times, and limited charging infrastructure must be accounted for. In this paper, we propose a safe Hierarchical Deep Reinforcement Learning (HDRL) framework for solving the EB Charging Scheduling Problem (EBCSP) under multi-source uncertainties. We formulate the problem as a Constrained Markov Decision Process (CMDP) with options to enable temporally abstract decision-making. We develop a novel HDRL algorithm, namely Double Actor-Critic Multi-Agent Proximal Policy Optimization Lagrangian (DAC-MAPPO-Lagrangian), which integrates Lagrangian relaxation into the Double Actor-Critic (DAC) framework. At the high level, we adopt a centralized PPO-Lagrangian algorithm to learn safe charger allocation policies. At the low level, we incorporate MAPPO-Lagrangian to learn decentralized charging power decisions under the Centralized Training and Decentralized Execution (CTDE) paradigm. Extensive experiments with real-world data demonstrate that the proposed approach outperforms existing baselines in both cost minimization and safety compliance, while maintaining fast convergence speed.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Large Scale Heterogeneous Treatment Effect Estimation Framework and Its Applications of Users' Journey at Snap</title>
<link>https://arxiv.org/abs/2512.03060</link>
<guid>https://arxiv.org/abs/2512.03060</guid>
<content:encoded><![CDATA[
<div> Keywords: Heterogeneous Treatment Effect, Conditional Average Treatment Effect, Snapchat, experimental data, ad influenceability  

<br /><br />Summary:  
1. The paper presents a large-scale industrial framework for estimating Heterogeneous Treatment Effects (HTE) and Conditional Average Treatment Effects (CATE) using experimental data from hundreds of millions of Snapchat users.  
2. The framework leverages data across many experiments to uncover latent user characteristics that were previously unmeasurable, enabling a more nuanced understanding of treatment effects beyond the one-size-fits-all assumption.  
3. Key components of the framework include careful experiment selection, a well-designed base learner, and an incremental training method that allows for stable and scalable treatment effect estimation.  
4. Two primary applications highlighted are measuring user influenceability to ads and sensitivity to ads, which help in understanding how different users respond differently to advertising interventions.  
5. An online A/B test using influenceability scores for targeting demonstrated substantial business improvements, with gains over six times larger than the typical significance threshold, proving the practical effectiveness of the proposed approach in real-world advertising contexts. <div>
arXiv:2512.03060v1 Announce Type: new 
Abstract: Heterogeneous Treatment Effect (HTE) and Conditional Average Treatment Effect (CATE) models relax the assumption that treatment effects are the same for every user. We present a large scale industrial framework for estimating HTE using experimental data from hundreds of millions of Snapchat users. By combining results across many experiments, the framework uncovers latent user characteristics that were previously unmeasurable and produces stable treatment effect estimates at scale.
  We describe the core components that enabled this system, including experiment selection, base learner design, and incremental training. We also highlight two applications: user influenceability to ads and user sensitivity to ads. An online A/B test using influenceability scores for targeting showed an improvement on key business metrics that is more than six times larger than what is typically considered significant.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Globally optimized SVD compression of LLMs via Fermi-function-based rank selection and gauge fixing</title>
<link>https://arxiv.org/abs/2512.03062</link>
<guid>https://arxiv.org/abs/2512.03062</guid>
<content:encoded><![CDATA[
<div> Low-rank decomposition, Singular Value Decomposition, Large Language Models, model compression, gradient descent<br /><br />Summary:<br /><br />This paper addresses the challenge of compressing Large Language Models (LLMs), which are computationally intensive, by focusing on low-rank decomposition techniques such as Singular Value Decomposition (SVD). The authors identify practical difficulties in this approach, including the selection of appropriate ranks for each layer and eliminating parameter redundancy. To overcome these challenges, they propose two physics-inspired improvements to SVD for LLM compression. First, they introduce FermiGrad, a novel gradient-descent algorithm that optimizes layer-wise ranks globally by converting the typically discrete singular-value truncation process into a continuous optimization problem using the Fermi function. Second, they present PivGa, a lossless compression technique that leverages the intrinsic gauge freedom in the parametrization of low-rank factors, thereby reducing redundancy without any loss of information. Together, these methods enhance the efficiency and effectiveness of compressing LLMs, potentially reducing resource demands while preserving model performance. <div>
arXiv:2512.03062v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are very demanding in terms of their computational resources. Low-rank decompositions of LLM weights, e.g. via Singular Value Decomposition (SVD), is a promising approach for LLM compression, but presents several practical hurdles, e.g. selecting appropriate layer-wise ranks and getting rid of its parameter redundancy. In this work, we present two physics-inspired improvements to SVD LLM compression: (1) \textbf{FermiGrad}, a gradient-descent algorithm that determines globally optimal layer-wise ranks by relaxing the discrete singular-value truncation into a continuous optimization using the Fermi function; (2) \textbf{PivGa}, an additional \textit{lossless} compression of the low-rank factors that exploits the intrinsic gauge freedom in their parametrization.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimizing Life Sciences Agents in Real-Time using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03065</link>
<guid>https://arxiv.org/abs/2512.03065</guid>
<content:encoded><![CDATA[
<div> Keywords: Generative AI agents, Thompson Sampling, contextual bandits, life sciences, user feedback<br /><br />Summary:  
The article addresses a key challenge faced by generative AI agents in life sciences: selecting the best approach to handle a diverse range of queries, from simple factual questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, which lack adaptability to evolving conditions or user needs. To overcome this, the authors introduce a novel framework that integrates AWS Strands Agents with Thompson Sampling contextual bandits, enabling AI agents to learn optimal decision-making strategies purely from user feedback. Their system optimizes three main dimensions: the choice of generation strategy (direct versus chain-of-thought), selection of tools (such as literature search or drug databases), and domain routing among specialized fields like pharmacology, molecular biology, and clinical expertise. Empirical evaluation on real life science queries demonstrated an improvement in user satisfaction by 15-30% compared to random baseline systems. The learning effect became evident after about 20-30 queries, highlighting the system’s adaptability. Notably, this approach requires no ground truth labels, continuously adapts to user preferences, and effectively balances the exploration-exploitation trade-off essential for agentic AI systems operating in complex and dynamic environments. <div>
arXiv:2512.03065v1 Announce Type: new 
Abstract: Generative AI agents in life sciences face a critical challenge: determining the optimal approach for diverse queries ranging from simple factoid questions to complex mechanistic reasoning. Traditional methods rely on fixed rules or expensive labeled training data, neither of which adapts to changing conditions or user preferences. We present a novel framework that combines AWS Strands Agents with Thompson Sampling contextual bandits to enable AI agents to learn optimal decision-making strategies from user feedback alone. Our system optimizes three key dimensions: generation strategy selection (direct vs. chain-of-thought), tool selection (literature search, drug databases, etc.), and domain routing (pharmacology, molecular biology, clinical specialists). Through empirical evaluation on life science queries, we demonstrate 15-30\% improvement in user satisfaction compared to random baselines, with clear learning patterns emerging after 20-30 queries. Our approach requires no ground truth labels, adapts continuously to user preferences, and provides a principled solution to the exploration-exploitation dilemma in agentic AI systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical clustering of complex energy systems using pretopology</title>
<link>https://arxiv.org/abs/2512.03069</link>
<guid>https://arxiv.org/abs/2512.03069</guid>
<content:encoded><![CDATA[
<div> Keywords: energy consumption, pretopology, hierarchical classification, time series clustering, automated recommendations<br /><br />Summary: This article addresses the challenge of modeling and classifying energy consumption profiles across a large distributed territory to optimize building energy management. It highlights the impracticality of conducting detailed audits on thousands of buildings due to significant resource demands, proposing an automated approach instead. The authors utilize pretopology to model the consumption profiles of sites and develop a multi-criterion hierarchical classification algorithm that leverages properties of pretopological spaces. This algorithm has been implemented in a Python library for practical use. To validate the method, three datasets are employed: a set of generated points of varying sizes in two-dimensional space, a generated set of time series, and a real dataset comprising 400 consumption time series from a French energy company. The algorithm successfully identifies clusters in the point dataset by considering both spatial position and size parameters. Applied to the generated time series, it achieves perfect clustering (an Adjusted Rand Index of 1) based on Pearson's correlation. These results demonstrate the method’s effectiveness in classifying and modeling energy consumption profiles, paving the way for scalable, data-driven recommendations to optimize energy use in buildings. <div>
arXiv:2512.03069v1 Announce Type: new 
Abstract: This article attempts answering the following problematic: How to model and classify energy consumption profiles over a large distributed territory to optimize the management of buildings' consumption?
  Doing case-by-case in depth auditing of thousands of buildings would require a massive amount of time and money as well as a significant number of qualified people. Thus, an automated method must be developed to establish a relevant and effective recommendations system.
  To answer this problematic, pretopology is used to model the sites' consumption profiles and a multi-criterion hierarchical classification algorithm, using the properties of pretopological space, has been developed in a Python library.
  To evaluate the results, three data sets are used: A generated set of dots of various sizes in a 2D space, a generated set of time series and a set of consumption time series of 400 real consumption sites from a French Energy company.
  On the point data set, the algorithm is able to identify the clusters of points using their position in space and their size as parameter. On the generated time series, the algorithm is able to identify the time series clusters using Pearson's correlation with an Adjusted Rand Index (ARI) of 1.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed Data Clustering Survey and Challenges</title>
<link>https://arxiv.org/abs/2512.03070</link>
<guid>https://arxiv.org/abs/2512.03070</guid>
<content:encoded><![CDATA[
<div> Keywords: big data, mixed-data clustering, pretopological spaces, hierarchical algorithms, explainability  

<br /><br />Summary:  
The article addresses the challenges posed by the big data paradigm, characterized by vast volumes, high velocity, and diverse data variety, which complicate effective data analysis and management. It emphasizes the specific difficulties in clustering heterogeneous datasets that include both numerical and categorical variables, noting that traditional clustering methods designed for homogeneous data are often inadequate. The paper advocates for hierarchical and explainable clustering algorithms, highlighting their importance in producing structured and interpretable results that aid decision-making processes. Central to the study is the introduction of a novel clustering method based on pretopological spaces, which offers a fresh theoretical framework for tackling mixed-data clustering problems. The proposed method is rigorously benchmarked against classical numerical clustering algorithms as well as existing pretopological approaches to evaluate performance and effectiveness. Findings from these comparisons provide valuable insights, demonstrating the potential advantages of the new approach within the context of big data analytics. Overall, the research contributes to advancing clustering methodologies tailored to mixed data, combining theoretical innovation with practical evaluation to address a critical need in the era of big data. <div>
arXiv:2512.03070v1 Announce Type: new 
Abstract: The advent of the big data paradigm has transformed how industries manage and analyze information, ushering in an era of unprecedented data volume, velocity, and variety. Within this landscape, mixed-data clustering has become a critical challenge, requiring innovative methods that can effectively exploit heterogeneous data types, including numerical and categorical variables. Traditional clustering techniques, typically designed for homogeneous datasets, often struggle to capture the additional complexity introduced by mixed data, underscoring the need for approaches specifically tailored to this setting. Hierarchical and explainable algorithms are particularly valuable in this context, as they provide structured, interpretable clustering results that support informed decision-making. This paper introduces a clustering method grounded in pretopological spaces. In addition, benchmarking against classical numerical clustering algorithms and existing pretopological approaches yields insights into the performance and effectiveness of the proposed method within the big data paradigm.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PretopoMD: Pretopology-based Mixed Data Hierarchical Clustering</title>
<link>https://arxiv.org/abs/2512.03071</link>
<guid>https://arxiv.org/abs/2512.03071</guid>
<content:encoded><![CDATA[
<div> Keywords: pretopology, mixed data clustering, Disjunctive Normal Form, hierarchical clusters, explainability  
  
<br /><br />Summary:  
This article introduces a novel clustering algorithm based on pretopology specifically designed to handle mixed data types without relying on dimensionality reduction techniques. The method utilizes Disjunctive Normal Form (DNF) to define customizable logical rules, enabling users to create hierarchical clusters tailored to the specific needs of heterogeneous datasets. Adjustable hyperparameters provide flexibility in cluster construction, allowing for more precise and interpretable grouping. The approach constructs clusters directly from raw data, preserving the integrity and original structure of the datasets. By applying hierarchical dendrogram analysis alongside comparative clustering metrics, the algorithm demonstrates superior performance compared to traditional methods. Empirical results confirm the robustness of the method in generating meaningful and explainable clusters, highlighting its potential to address challenges related to cluster interpretability in mixed data scenarios. The key innovation of this work lies in bypassing dimensionality reduction and instead using logical rule-based criteria to enhance both cluster formation accuracy and transparency. Overall, this research contributes a significant advancement to clustering methodologies for mixed data by improving both performance and explainability while maintaining the fidelity of the original data. <div>
arXiv:2512.03071v1 Announce Type: new 
Abstract: This article presents a novel pretopology-based algorithm designed to address the challenges of clustering mixed data without the need for dimensionality reduction. Leveraging Disjunctive Normal Form, our approach formulates customizable logical rules and adjustable hyperparameters that allow for user-defined hierarchical cluster construction and facilitate tailored solutions for heterogeneous datasets. Through hierarchical dendrogram analysis and comparative clustering metrics, our method demonstrates superior performance by accurately and interpretably delineating clusters directly from raw data, thus preserving data integrity. Empirical findings highlight the algorithm's robustness in constructing meaningful clusters and reveal its potential in overcoming issues related to clustered data explainability. The novelty of this work lies in its departure from traditional dimensionality reduction techniques and its innovative use of logical rules that enhance both cluster formation and clarity, thereby contributing a significant advancement to the discourse on clustering mixed data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Model-Agnostic Fairness Regularization for GNNs with Incomplete Sensitive Information</title>
<link>https://arxiv.org/abs/2512.03074</link>
<guid>https://arxiv.org/abs/2512.03074</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, fairness, sensitive attributes, regularization, bias mitigation<br /><br />Summary:<br /><br />1. Graph Neural Networks (GNNs) are effective for relational learning tasks such as node classification and link prediction but raise fairness concerns because they can perpetuate societal biases. <br /><br />2. These biases stem from node features, graph topology, and the message-passing process, disproportionately affecting protected groups identified by sensitive attributes like race or gender.<br /><br />3. Existing fairness-aware GNN approaches assume full availability of sensitive attributes during training, limiting their applicability due to privacy and data collection challenges.<br /><br />4. The paper introduces a novel, model-agnostic fairness regularization framework that works under partial availability of sensitive attributes, integrating equal opportunity and statistical parity into a differentiable objective function.<br /><br />5. Experimental results on five real-world datasets demonstrate significant bias mitigation with minimal accuracy loss, outperforming baseline methods and achieving a superior fairness-accuracy trade-off; the authors will publicly release the datasets and code. <div>
arXiv:2512.03074v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have demonstrated exceptional efficacy in relational learning tasks, including node classification and link prediction. However, their application raises significant fairness concerns, as GNNs can perpetuate and even amplify societal biases against protected groups defined by sensitive attributes such as race or gender. These biases are often inherent in the node features, structural topology, and message-passing mechanisms of the graph itself. A critical limitation of existing fairness-aware GNN methods is their reliance on the strong assumption that sensitive attributes are fully available for all nodes during training--a condition that poses a practical impediment due to privacy concerns and data collection constraints. To address this gap, we propose a novel, model-agnostic fairness regularization framework designed for the realistic scenario where sensitive attributes are only partially available. Our approach formalizes a fairness-aware objective function that integrates both equal opportunity and statistical parity as differentiable regularization terms. Through a comprehensive empirical evaluation across five real-world benchmark datasets, we demonstrate that the proposed method significantly mitigates bias across key fairness metrics while maintaining competitive node classification performance. Results show that our framework consistently outperforms baseline models in achieving a favorable fairness-accuracy trade-off, with minimal degradation in predictive accuracy. The datasets and source code will be publicly released at https://github.com/mtavassoli/GNN-FC.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Entropic Flow Matching</title>
<link>https://arxiv.org/abs/2512.03078</link>
<guid>https://arxiv.org/abs/2512.03078</guid>
<content:encoded><![CDATA[
<div> Tilted risk, Flow Matching, entropic objective, neural velocity field, risk-sensitive loss  

<br /><br />Summary:  
This work investigates the application of tilted (entropic) risk—a log-exponential transform of a base loss—to Flow Matching (FM), a technique that learns a velocity field mapping samples from a source distribution to data via ODE integration. Traditional rectified FM uses a mean squared error loss to train on linearly interpolated sample pairs, which results in predicting only the conditional mean of velocity fields at each space-time point. This approach neglects higher-order conditional information such as variance, skewness, and multi-modality, which are critical for capturing the data manifold's geometric nuances and minority branches. The authors propose applying the risk-sensitive tilted loss to the FM setting, deriving a conditional entropic FM objective that naturally upper bounds the original loss at every space-time location. They further perform a first-order gradient expansion revealing two key corrections: covariance preconditioning of the FM residual and an asymmetric skew tail term that favors rare or minority branches. Experiments on synthetic ambiguous and heavy-tailed data demonstrate that this tilted loss improves statistical performance and more accurately recovers geometric structures compared to standard FM. This approach thus enhances FM’s ability to capture complex conditional distributions and refine learned velocity fields in flow-based generative modeling. <div>
arXiv:2512.03078v1 Announce Type: new 
Abstract: Tilted (entropic) risk, obtained by applying a log-exponential transform to a base loss, is a well established tool in statistics and machine learning for emphasizing rare or high loss events while retaining a tractable optimization problem. In this work, our aim is to interpret its structure for Flow Matching (FM). FM learns a velocity field that transports samples from a simple source distribution to data by integrating an ODE. In rectified FM, training pairs are obtained by linearly interpolating between a source sample and a data sample, and a neural velocity field is trained to predict the straight line displacement using a mean squared error loss. This squared loss collapses all velocity targets that reach the same space-time point into a single conditional mean, thereby ignoring higher order conditional information (variance, skewness, multi-modality) that encodes fine geometric structure about the data manifold and minority branches. We apply the standard risk-sensitive (log-exponential) transform to the conditional FM loss and show that the resulting tilted risk loss is a natural upper-bound on a meaningful conditional entropic FM objective defined at each space-time point. Furthermore, we show that a small order expansion of the gradient of this conditional entropic objective yields two interpretable first order corrections: covariance preconditioning of the FM residual, and a skew tail term that favors asymmetric or rare branches. On synthetic data designed to probe ambiguity and tails, the resulting risk-sensitive loss improves statistical metrics and recovers geometric structure more faithfully than standard rectified FM.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALARM: Automated MLLM-Based Anomaly Detection in Complex-EnviRonment Monitoring with Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2512.03101</link>
<guid>https://arxiv.org/abs/2512.03101</guid>
<content:encoded><![CDATA[
<div> Large Language Models, multi-modal, visual anomaly detection, uncertainty quantification, ALARM  

<br /><br />Summary:  
1. This paper addresses the challenge of developing multi-modal large language model (MLLM) based visual anomaly detection (VAD) systems suited for complex environments where anomalies are contextual and ambiguous.  
2. The authors emphasize the importance of uncertainty quantification (UQ) in enhancing the reliability and robustness of MLLM-based VAD systems.  
3. They propose a novel framework named ALARM that integrates UQ with quality-assurance mechanisms including reasoning chains, self-reflection, and MLLM ensembles to improve detection accuracy and robustness.  
4. ALARM is designed on a solid foundation of probabilistic inference and computational processes, ensuring rigorous and systematic reasoning in anomaly detection tasks.  
5. The effectiveness and generalizability of ALARM are validated through extensive empirical evaluations on real-world smart-home benchmark datasets and wound image classification tasks, demonstrating superior performance and versatility across different domains for reliable decision-making. <div>
arXiv:2512.03101v1 Announce Type: new 
Abstract: The advance of Large Language Models (LLMs) has greatly stimulated research interest in developing multi-modal LLM (MLLM)-based visual anomaly detection (VAD) algorithms that can be deployed in complex environments. The challenge is that in these complex environments, the anomalies are sometimes highly contextual and also ambiguous, and thereby, uncertainty quantification (UQ) is a crucial capacity for an MLLM-based VAD system to succeed. In this paper, we introduce our UQ-supported MLLM-based VAD framework called ALARM. ALARM integrates UQ with quality-assurance techniques like reasoning chain, self-reflection, and MLLM ensemble for robust and accurate performance and is designed based on a rigorous probabilistic inference pipeline and computational process. Extensive empirical evaluations are conducted using the real-world smart-home benchmark data and wound image classification data, which shows ALARM's superior performance and its generic applicability across different domains for reliable decision-making.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Correction of Erroneous State Estimates via Diffusion Bayesian Exploration</title>
<link>https://arxiv.org/abs/2512.03102</link>
<guid>https://arxiv.org/abs/2512.03102</guid>
<content:encoded><![CDATA[
<div> Keywords: bootstrap particle filters, state estimation, diffusion-driven Bayesian exploration, Stationarity-Induced Posterior Support Invariance, hazardous-gas localization<br /><br />Summary:  
1. The paper addresses critical challenges in early-stage state estimation within emergency response scenarios, where errors can lead to catastrophic outcomes such as delays and resource misallocation.  
2. A key issue identified is Stationarity-Induced Posterior Support Invariance (S-PSI), observed in bootstrap particle filters under the stationary bootstrap baseline, where parts of the state space excluded by the initial prior remain forever unvisited, even if new contradictory evidence emerges.  
3. Existing classical perturbation methods that can break this lock-in are inefficient as they operate continuously and indiscriminately.  
4. To resolve this, the authors propose a novel diffusion-driven Bayesian exploration framework that enables dynamic, real-time correction of early estimation errors by expanding the posterior support through entropy-regularized sampling combined with covariance-scaled diffusion.  
5. Their approach incorporates a Metropolis-Hastings acceptance step to ensure adaptivity and statistical soundness.  
6. Empirical validation on complex hazardous-gas localization tasks demonstrates that their method matches reinforcement learning and planning baselines when priors are accurate, while significantly outperforming classical sequential Monte Carlo perturbations and RL-based methods when priors are misaligned.  
7. Theoretical guarantees are provided to confirm that the proposed Diffusion-Driven Exploration Particle Filter (DEPF) effectively resolves S-PSI while maintaining statistical rigor. <div>
arXiv:2512.03102v1 Announce Type: new 
Abstract: In emergency response and other high-stakes societal applications, early-stage state estimates critically shape downstream outcomes. Yet, these initial state estimates-often based on limited or biased information-can be severely misaligned with reality, constraining subsequent actions and potentially causing catastrophic delays, resource misallocation, and human harm. Under the stationary bootstrap baseline (zero transition and no rejuvenation), bootstrap particle filters exhibit Stationarity-Induced Posterior Support Invariance (S-PSI), wherein regions excluded by the initial prior remain permanently unexplorable, making corrections impossible even when new evidence contradicts current beliefs. While classical perturbations can in principle break this lock-in, they operate in an always-on fashion and may be inefficient. To overcome this, we propose a diffusion-driven Bayesian exploration framework that enables principled, real-time correction of early state estimation errors. Our method expands posterior support via entropy-regularized sampling and covariance-scaled diffusion. A Metropolis-Hastings check validates proposals and keeps inference adaptive to unexpected evidence. Empirical evaluations on realistic hazardous-gas localization tasks show that our approach matches reinforcement learning and planning baselines when priors are correct. It substantially outperforms classical SMC perturbations and RL-based methods under misalignment, and we provide theoretical guarantees that DEPF resolves S-PSI while maintaining statistical rigor.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting AI Hallucinations in Finance: An Information-Theoretic Method Cuts Hallucination Rate by 92%</title>
<link>https://arxiv.org/abs/2512.03107</link>
<guid>https://arxiv.org/abs/2512.03107</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, hallucination detection, semantic entropy, perplexity decomposition, evidence utilization  

<br /><br />Summary:  
1. This paper addresses hallucinations in large language models (LLMs), defined as fluent but unsupported answers, which constrain safe use in critical areas.  
2. The authors introduce ECLIPSE, a framework that models hallucination as a mismatch between a model’s semantic entropy and the capacity of accessible evidence supporting its output.  
3. ECLIPSE uniquely combines entropy estimation via multi-sample clustering with a novel perplexity decomposition that assesses how well a model leverages retrieved evidence during response generation.  
4. They prove that the entropy-capacity objective formulated by ECLIPSE is strictly convex under mild conditions, guaranteeing a unique stable optimum for detecting hallucination.  
5. Evaluation on a carefully controlled financial question-answering dataset with GPT-3.5-turbo (200 balanced samples including synthetic hallucinations) shows ECLIPSE achieving strong performance (ROC AUC 0.89, average precision 0.90), significantly outperforming an entropy-only baseline with AUC 0.50.  
6. An ablation study using Claude-3-Haiku, lacking token-level log probabilities, saw performance drop to an AUC of 0.59 and a 95% decrease in coefficient magnitudes, confirming ECLIPSE’s dependence on calibrated token-level uncertainties.  
7. The perplexity decomposition features carry the largest learned coefficients, indicating that how the model uses evidence is critical for detecting hallucinations.  
8. The work is positioned as a mechanism study under controlled settings, with wider validation on diverse domains and real-world hallucinations left for future investigation. <div>
arXiv:2512.03107v1 Announce Type: new 
Abstract: Large language models (LLMs) produce fluent but unsupported answers - hallucinations - limiting safe deployment in high-stakes domains. We propose ECLIPSE, a framework that treats hallucination as a mismatch between a model's semantic entropy and the capacity of available evidence. We combine entropy estimation via multi-sample clustering with a novel perplexity decomposition that measures how models use retrieved evidence. We prove that under mild conditions, the resulting entropy-capacity objective is strictly convex with a unique stable optimum. We evaluate on a controlled financial question answering dataset with GPT-3.5-turbo (n=200 balanced samples with synthetic hallucinations), where ECLIPSE achieves ROC AUC of 0.89 and average precision of 0.90, substantially outperforming a semantic entropy-only baseline (AUC 0.50). A controlled ablation with Claude-3-Haiku, which lacks token-level log probabilities, shows AUC dropping to 0.59 with coefficient magnitudes decreasing by 95% - demonstrating that ECLIPSE is a logprob-native mechanism whose effectiveness depends on calibrated token-level uncertainties. The perplexity decomposition features exhibit the largest learned coefficients, confirming that evidence utilization is central to hallucination detection. We position this work as a controlled mechanism study; broader validation across domains and naturally occurring hallucinations remains future work.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing</title>
<link>https://arxiv.org/abs/2512.03109</link>
<guid>https://arxiv.org/abs/2512.03109</guid>
<content:encoded><![CDATA[
<div> Agentic AI, verifiers, sequential hypothesis testing, e-processes, false alarm control<br /><br />Summary:<br /><br />1. Agentic AI systems perform sequences of actions in response to user prompts, such as reasoning steps or tool calls, whose success needs to be evaluated reliably.<br />2. Existing verifiers like large language model (LLM) judges and process-reward models provide heuristic scores to assess trajectory quality but lack guarantees for correctness in predicting successful outcomes.<br />3. The paper introduces e-valuator, a novel method that transforms any black-box verifier score into a decision rule with provable control over false alarm rates.<br />4. E-valuator reframes success evaluation as a sequential hypothesis testing problem and leverages e-processes to maintain statistical validity at every action step, enabling real-time monitoring over arbitrarily long sequences.<br />5. Empirical results across six datasets and three agent types show that e-valuator outperforms alternative methods in statistical power and false alarm rate control, and it also facilitates early termination of problematic trajectories to save computation resources.<br />6. As a lightweight, model-agnostic framework, e-valuator offers statistically guaranteed decision-making that can improve the reliability and efficiency of deploying agentic AI systems. <div>
arXiv:2512.03109v1 Announce Type: new 
Abstract: Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Additivity: Sparse Isotonic Shapley Regression toward Nonlinear Explainability</title>
<link>https://arxiv.org/abs/2512.03112</link>
<guid>https://arxiv.org/abs/2512.03112</guid>
<content:encoded><![CDATA[
<div> Keywords: Shapley values, Explainable AI, nonlinear transformation, sparsity, isotonic regression<br /><br />Summary:<br /><br />This paper addresses two key challenges in using Shapley values for feature attribution in Explainable AI: the common assumption of additive payoff functions which often fails in real-world scenarios, and the difficulty of achieving sparse, consistent explanations in high-dimensional settings. To overcome these issues, the authors propose Sparse Isotonic Shapley Regression (SISR), a novel framework that simultaneously learns a monotonic transformation to restore additivity without requiring explicit functional forms, while enforcing an L0 sparsity constraint on Shapley values to improve computational efficiency. The optimization algorithm utilizes Pool-Adjacent-Violators for isotonic regression and normalized hard-thresholding for feature selection, providing ease of implementation and guaranteed global convergence. Theoretical analysis demonstrates that SISR can accurately recover the true transformation under various conditions and effectively identify relevant features even in noisy environments. The work highlights that irrelevant features and feature dependencies can cause nonlinearity in the true payoff function, which standard Shapley methods fail to capture. Empirical evaluations on regression, logistic regression, and tree-based models show that SISR produces stable, consistent attributions, correctly filters out irrelevant features, and avoids the rank and sign distortions seen in conventional Shapley attributions. Overall, SISR advances nonlinear explainability by integrating nonlinear transformation learning with sparsity pursuit in a principled attribution framework. <div>
arXiv:2512.03112v1 Announce Type: new 
Abstract: Shapley values, a gold standard for feature attribution in Explainable AI, face two primary challenges. First, the canonical Shapley framework assumes that the worth function is additive, yet real-world payoff constructions--driven by non-Gaussian distributions, heavy tails, feature dependence, or domain-specific loss scales--often violate this assumption, leading to distorted attributions. Secondly, achieving sparse explanations in high dimensions by computing dense Shapley values and then applying ad hoc thresholding is prohibitively costly and risks inconsistency. We introduce Sparse Isotonic Shapley Regression (SISR), a unified nonlinear explanation framework. SISR simultaneously learns a monotonic transformation to restore additivity--obviating the need for a closed-form specification--and enforces an L0 sparsity constraint on the Shapley vector, enhancing computational efficiency in large feature spaces. Its optimization algorithm leverages Pool-Adjacent-Violators for efficient isotonic regression and normalized hard-thresholding for support selection, yielding implementation ease and global convergence guarantees. Analysis shows that SISR recovers the true transformation in a wide range of scenarios and achieves strong support recovery even in high noise. Moreover, we are the first to demonstrate that irrelevant features and inter-feature dependencies can induce a true payoff transformation that deviates substantially from linearity. Experiments in regression, logistic regression, and tree ensembles demonstrate that SISR stabilizes attributions across payoff schemes, correctly filters irrelevant features while standard Shapley values suffer severe rank and sign distortions. By unifying nonlinear transformation estimation with sparsity pursuit, SISR advances the frontier of nonlinear explainability, providing a theoretically grounded and practical attribution framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Temporal Graph Neural Networks for Early Anomaly Detection and Performance Prediction via PV System Monitoring Data</title>
<link>https://arxiv.org/abs/2512.03114</link>
<guid>https://arxiv.org/abs/2512.03114</guid>
<content:encoded><![CDATA[
<div> solar photovoltaic, Temporal Graph Neural Network, anomaly detection, power prediction, environmental parameters<br /><br />Summary:<br /><br />1. The study addresses the increasing need for effective performance monitoring and anomaly detection in solar photovoltaic (PV) systems due to their rapid growth.  
2. It introduces a novel Temporal Graph Neural Network (Temporal GNN) approach designed to predict solar PV output power accurately and identify anomalies based on environmental and operational data.  
3. The model captures temporal and graph-structured relationships among critical PV system parameters, such as solar irradiance and both module and ambient temperature, to enhance prediction accuracy.  
4. The research utilizes real-world data collected from an outdoor rooftop facility in Lyon, France, which includes power output measurements from a PV module alongside meteorological information.  
5. The proposed method demonstrates the potential of leveraging temporal graph-based deep learning techniques for improved PV system monitoring, enabling proactive maintenance and optimized operational efficiency. <div>
arXiv:2512.03114v1 Announce Type: new 
Abstract: The rapid growth of solar photovoltaic (PV) systems necessitates advanced methods for performance monitoring and anomaly detection to ensure optimal operation. In this study, we propose a novel approach leveraging Temporal Graph Neural Network (Temporal GNN) to predict solar PV output power and detect anomalies using environmental and operational parameters. The proposed model utilizes graph-based temporal relationships among key PV system parameters, including irradiance, module and ambient temperature to predict electrical power output. This study is based on data collected from an outdoor facility located on a rooftop in Lyon (France) including power measurements from a PV module and meteorological parameters.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Real-Time Structural Health Monitoring with Bayesian Neural Networks: Distinguishing Aleatoric and Epistemic Uncertainty for Digital Twin Frameworks</title>
<link>https://arxiv.org/abs/2512.03115</link>
<guid>https://arxiv.org/abs/2512.03115</guid>
<content:encoded><![CDATA[
<div> Keywords: structural health monitoring, Bayesian neural network, uncertainty quantification, principal component analysis, carbon fiber reinforced polymer<br /><br />Summary:  
This article presents an integrated framework for structural health monitoring (SHM) that combines principal component analysis (PCA), Bayesian neural networks (BNN), and Hamiltonian Monte Carlo (HMC) inference to achieve reliable real-time analysis of sensor data. The framework maps sparse strain gauge measurements onto the leading PCA modes to reconstruct full-field strain distributions, enabling spatially resolved quantification of both aleatoric and epistemic uncertainties. The method was experimentally validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with different crack lengths. The results showed accurate strain field reconstructions with R squared values above 0.9, demonstrating the framework's robustness even in the presence of noise and crack-induced strain singularities. The BNN provides explicit uncertainty representations that help distinguish between data-inherent noise and model-related limitations at a local scale. This capability allows for more trustworthy decision-making in SHM by highlighting regions of low confidence and their underlying causes. Overall, the proposed approach advances SHM toward the realization of trustworthy digital twins and risk-aware structural diagnostics, promising improved monitoring and maintenance of high-value assets with quantified confidence levels in predictions. <div>
arXiv:2512.03115v1 Announce Type: new 
Abstract: Reliable real-time analysis of sensor data is essential for structural health monitoring (SHM) of high-value assets, yet a major challenge is to obtain spatially resolved full-field aleatoric and epistemic uncertainties for trustworthy decision-making. We present an integrated SHM framework that combines principal component analysis (PCA), a Bayesian neural network (BNN), and Hamiltonian Monte Carlo (HMC) inference, mapping sparse strain gauge measurements onto leading PCA modes to reconstruct full-field strain distributions with uncertainty quantification. The framework was validated through cyclic four-point bending tests on carbon fiber reinforced polymer (CFRP) specimens with varying crack lengths, achieving accurate strain field reconstruction (R squared value > 0.9) while simultaneously producing real-time uncertainty fields. A key contribution is that the BNN yields robust full-field strain reconstructions from noisy experimental data with crack-induced strain singularities, while also providing explicit representations of two complementary uncertainty fields. Considered jointly in full-field form, the aleatoric and epistemic uncertainty fields make it possible to diagnose at a local level, whether low-confidence regions are driven by data-inherent issues or by model-related limitations, thereby supporting reliable decision-making. Collectively, the results demonstrate that the proposed framework advances SHM toward trustworthy digital twin deployment and risk-aware structural diagnostics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mitigating Intra- and Inter-modal Forgetting in Continual Learning of Unified Multimodal Models</title>
<link>https://arxiv.org/abs/2512.03125</link>
<guid>https://arxiv.org/abs/2512.03125</guid>
<content:encoded><![CDATA[
<div> Unified Multimodal Generative Models, catastrophic forgetting, inter-modal forgetting, modality-decoupled experts, continual learning<br /><br />Summary:<br /><br />1. The paper addresses the challenge of catastrophic forgetting in Unified Multimodal Generative Models (UMGMs), which integrate visual understanding and image generation into a single autoregressive architecture. <br />2. It highlights that forgetting occurs both within a single modality (intra-modal) and between different modalities (inter-modal), with the latter being less explored in prior research. <br />3. The authors identify and empirically validate inter-modal forgetting and attribute it to gradient conflicts during model updates across modalities, providing a theoretical explanation for this phenomenon. <br />4. To mitigate both intra- and inter-modal forgetting, the paper introduces Modality-Decoupled Experts (MoDE), a lightweight, scalable design that isolates modality-specific updates, thereby reducing interference caused by conflicting gradients. <br />5. MoDE incorporates knowledge distillation techniques to preserve pre-trained knowledge and prevent catastrophic forgetting, which improves performance on diverse multimodal generation benchmarks compared to previous continual learning approaches. <div>
arXiv:2512.03125v1 Announce Type: new 
Abstract: Unified Multimodal Generative Models (UMGMs) unify visual understanding and image generation within a single autoregressive framework. However, their ability to continually learn new tasks is severely hindered by catastrophic forgetting, both within a modality (intra-modal) and across modalities (inter-modal). While intra-modal forgetting has been studied in prior continual learning (CL) work, inter-modal forgetting remains largely unexplored. In this paper, we identify and empirically validate this phenomenon in UMGMs and provide a theoretical explanation rooted in gradient conflict between modalities. To address both intra- and inter-modal forgetting, we propose Modality-Decoupled Experts (MoDE), a lightweight and scalable architecture that isolates modality-specific updates to mitigate the gradient conflict and leverages knowledge distillation to prevent catastrophic forgetting and preserve pre-trained capabilities. Unlike previous CL methods that remain modality-coupled and suffer from modality gradient conflict, MoDE explicitly decouples modalities to prevent interference. Experiments across diverse benchmarks demonstrate that MoDE significantly mitigates both inter- and intra-modal forgetting, outperforming prior CL baselines in unified multimodal generation settings. Codes will be publicly available: https://github.com/Christina200/MoDE-official.git
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Atomic Diffusion Models for Small Molecule Structure Elucidation from NMR Spectra</title>
<link>https://arxiv.org/abs/2512.03127</link>
<guid>https://arxiv.org/abs/2512.03127</guid>
<content:encoded><![CDATA[
<div> Keywords: NMR spectroscopy, structure elucidation, deep learning, natural products, atomic diffusion model<br /><br />Summary:<br /><br />1. Nuclear Magnetic Resonance (NMR) spectroscopy is essential for determining small molecule structures, playing a key role in natural product discovery and clinical therapeutics.<br /><br />2. Traditional interpretation of NMR spectra is a manual, time-consuming process that requires significant domain expertise.<br /><br />3. The authors present ChefNMR, an end-to-end framework that predicts molecular structures directly from 1D NMR spectra combined with the chemical formula.<br /><br />4. ChefNMR approaches structure elucidation as a conditional generation task using an atomic diffusion model based on a non-equivariant transformer architecture.<br /><br />5. To address the complexity of natural product chemistry, a comprehensive dataset of simulated 1D NMR spectra for over 111,000 natural products was created.<br /><br />6. ChefNMR achieves over 65% accuracy in predicting challenging natural product structures, surpassing existing methods.<br /><br />7. This work represents a significant advancement toward automating small-molecule structure elucidation and demonstrates the potential of deep learning to accelerate molecular discovery.<br /><br />8. The source code for ChefNMR is publicly available at https://github.com/ml-struct-bio/chefnmr. <div>
arXiv:2512.03127v1 Announce Type: new 
Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy is a cornerstone technique for determining the structures of small molecules and is especially critical in the discovery of novel natural products and clinical therapeutics. Yet, interpreting NMR spectra remains a time-consuming, manual process requiring extensive domain expertise. We introduce ChefNMR (CHemical Elucidation From NMR), an end-to-end framework that directly predicts an unknown molecule's structure solely from its 1D NMR spectra and chemical formula. We frame structure elucidation as conditional generation from an atomic diffusion model built on a non-equivariant transformer architecture. To model the complex chemical groups found in natural products, we generated a dataset of simulated 1D NMR spectra for over 111,000 natural products. ChefNMR predicts the structures of challenging natural product compounds with an unsurpassed accuracy of over 65%. This work takes a significant step toward solving the grand challenge of automating small-molecule structure elucidation and highlights the potential of deep learning in accelerating molecular discovery. Code is available at https://github.com/ml-struct-bio/chefnmr.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contrastive Deep Learning for Variant Detection in Wastewater Genomic Sequencing</title>
<link>https://arxiv.org/abs/2512.03158</link>
<guid>https://arxiv.org/abs/2512.03158</guid>
<content:encoded><![CDATA[
<div> Keywords: wastewater genomic surveillance, viral variant detection, VQ-VAE, contrastive learning, reference-free framework<br /><br />Summary:<br /><br />1. The study addresses challenges in wastewater-based genomic surveillance, which is crucial for monitoring viral variants at the population level but is hindered by high sequencing noise, low coverage, fragmented reads, and absence of labeled variant data.<br />2. Traditional variant calling methods struggle with novel mutations and demand heavy computational resources, motivating the need for alternative approaches.<br />3. The authors propose an unsupervised framework using Vector-Quantized Variational Autoencoders (VQ-VAE), which learns discrete genomic pattern representations directly from k-mer tokenized sequences without requiring reference genomes or variant labels.<br />4. The model incorporates masked reconstruction pretraining to improve robustness against missing data and employs contrastive learning to generate highly discriminative embeddings.<br />5. Tested on approximately 100,000 SARS-CoV-2 wastewater reads, the VQ-VAE achieved high token-level accuracy (99.52%), moderate exact sequence match rate (56.33%), and efficient codebook utilization (19.73%).<br />6. Contrastive fine-tuning enhanced clustering performance significantly, with 64-dimensional embeddings improving the Silhouette score by 35% and 128-dimensional embeddings by 42%, showing the influence of embedding dimension on variant discrimination.<br />7. This scalable, interpretable, reference-free approach has direct implications for public health by enabling effective genomic surveillance without reliance on prior variant annotations or references. <div>
arXiv:2512.03158v1 Announce Type: new 
Abstract: Wastewater-based genomic surveillance has emerged as a powerful tool for population-level viral monitoring, offering comprehensive insights into circulating viral variants across entire communities. However, this approach faces significant computational challenges stemming from high sequencing noise, low viral coverage, fragmented reads, and the complete absence of labeled variant annotations. Traditional reference-based variant calling pipelines struggle with novel mutations and require extensive computational resources. We present a comprehensive framework for unsupervised viral variant detection using Vector-Quantized Variational Autoencoders (VQ-VAE) that learns discrete codebooks of genomic patterns from k-mer tokenized sequences without requiring reference genomes or variant labels. Our approach extends the base VQ-VAE architecture with masked reconstruction pretraining for robustness to missing data and contrastive learning for highly discriminative embeddings. Evaluated on SARS-CoV-2 wastewater sequencing data comprising approximately 100,000 reads, our VQ-VAE achieves 99.52% mean token-level accuracy and 56.33% exact sequence match rate while maintaining 19.73% codebook utilization (101 of 512 codes active), demonstrating efficient discrete representation learning. Contrastive fine-tuning with different projection dimensions yields substantial clustering improvements: 64-dimensional embeddings achieve +35% Silhouette score improvement (0.31 to 0.42), while 128-dimensional embeddings achieve +42% improvement (0.31 to 0.44), clearly demonstrating the impact of embedding dimensionality on variant discrimination capability. Our reference-free framework provides a scalable, interpretable approach to genomic surveillance with direct applications to public health monitoring.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Plantain: Plan-Answer Interleaved Reasoning</title>
<link>https://arxiv.org/abs/2512.03176</link>
<guid>https://arxiv.org/abs/2512.03176</guid>
<content:encoded><![CDATA[
<div> Keywords: interleaved reasoning, Plantain, incremental grounding, pass@1 improvement, reduced latency  

<br /><br />Summary:  
1. Traditional reasoning models operate by first fully thinking through a problem before generating any visible response, which can waste user time if the model’s reasoning is based on a false premise.  
2. Unlike these models, human communication involves lightweight, incremental grounding acts that confirm mutual understanding and allow early corrections, improving conversation flow.  
3. Inspired by this, the authors propose interleaved reasoning (IR), which alternates between thinking steps and surfacing intermediate outputs, giving users earlier insights into the model’s reasoning progress.  
4. IR aims to reduce perceived latency—how long a user waits for an initial response—without lowering the quality of the final answer.  
5. A specific form of IR called Plantain (Plan-Thought-Answer Interleaving) begins by outputting a step-by-step plan as the first intermediate response, enabling user intervention and feedback during the reasoning process.  
6. Experiments show that Plantain achieves about a 6% improvement in pass@1 accuracy over challenging math and coding benchmarks compared to traditional think-then-answer models.  
7. Additionally, Plantain reduces the time-to-first-response by over 60%, significantly enhancing responsiveness while maintaining or improving output quality. <div>
arXiv:2512.03176v1 Announce Type: new 
Abstract: Reasoning models often spend a significant amount of time thinking before they generate a visible response. In the meantime, they do not give the user any hints as to whether their reasoning is on the right track, and do not give the user any recourse to stop and correct them if their reasoning is flawed. This creates a frustrating, but unfortunately common, experience: the user's time is wasted while the model reasons from a false premise that could have easily been corrected. In contrast, human speakers typically perform lightweight, incremental grounding acts to ensure that participants in the conversation are on the same page; here we ask if language models can learn to leverage a similar type of behavior? With this motivation, we propose interleaved reasoning (IR), in which the model alternates between thinking and surfacing intermediate responses, as an alternative to the standard "think-then-answer" approach. By providing useful information to the user earlier, IR reduces perceived latency, the time a user waits for an initial output, without compromising the quality of the final response. We further introduce a specialization of interleaved reasoning, Plantain (Plan-Thought-Answer Interleaving), where the first intermediate response is an explicit, step-by-step plan for executing the task. This plan-first strategy allows for user intervention and early feedback for subsequent reasoning steps. We demonstrate that Plantain yields an ~6% improvement in pass@1 across several challenging math reasoning and coding benchmarks, while reducing time-to-first-response by over 60% relative to think-then-answer baselines.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neighborhood density estimation using space-partitioning based hashing schemes</title>
<link>https://arxiv.org/abs/2512.03187</link>
<guid>https://arxiv.org/abs/2512.03187</guid>
<content:encoded><![CDATA[
<div> Rare cell detection, single-cell RNA sequencing, sketching algorithm, anomaly detection, concept drift  

<br /><br />Summary:  
This article presents FiRE/FiRE.1, an innovative sketching-based algorithm designed for anomaly detection specifically aimed at rapidly identifying rare cell sub-populations within large-scale single-cell RNA sequencing datasets. The method outperforms existing state-of-the-art approaches, offering both speed and accuracy in detecting these elusive cellular populations. In addition, the work introduces Enhash, a novel ensemble learning framework that leverages projection hashing to efficiently detect concept drift in streaming data environments. Enhash excels in balancing computational resource use and detection performance, proving highly competitive in terms of processing time and accuracy when tested against various types of concept drift. Collectively, these contributions advance the fields of computational biology and data stream analysis by providing scalable, fast, and resource-efficient techniques applicable to real-world complex data scenarios. <div>
arXiv:2512.03187v1 Announce Type: new 
Abstract: This work introduces FiRE/FiRE.1, a novel sketching-based algorithm for anomaly detection to quickly identify rare cell sub-populations in large-scale single-cell RNA sequencing data. This method demonstrated superior performance against state-of-the-art techniques. Furthermore, the thesis proposes Enhash, a fast and resource-efficient ensemble learner that uses projection hashing to detect concept drift in streaming data, proving highly competitive in time and accuracy across various drift types.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Internal-State Policy-Gradient Methods for POMDPs</title>
<link>https://arxiv.org/abs/2512.03204</link>
<guid>https://arxiv.org/abs/2512.03204</guid>
<content:encoded><![CDATA[
<div> Policy-gradient, memory, POMDP, infinite-horizon, robot navigation<br /><br />Summary:<br /><br />This paper focuses on improving policy-gradient methods for reinforcement learning in partially observable environments, especially when memory in the policy is necessary. The authors develop several novel algorithms designed to learn policies with memory within an infinite-horizon framework, addressing both scenarios where the environment model is known and where learning must be done through simulation. These advancements aim to overcome the limitations of traditional policy-gradient approaches that perform well for memoryless policies but struggle when memory is required. The study includes comprehensive comparisons across challenging large-scale Partially Observable Markov Decision Processes (POMDPs), such as noisy robot navigation tasks and multi-agent settings. The results demonstrate the effectiveness of the proposed algorithms, showing improved learning performance and policy quality in environments requiring internal memory to manage partial observability. Overall, this research contributes to advancing the applicability of policy-gradient methods to complex domains where agents benefit from maintaining memory to make better decisions over an infinite time horizon. <div>
arXiv:2512.03204v1 Announce Type: new 
Abstract: Policy-gradient methods have received increased attention recently as a mechanism for learning to act in partially observable environments. They have shown promise for problems admitting memoryless policies but have been less successful when memory is required. In this paper we develop several improved algorithms for learning policies with memory in an infinite-horizon setting -- directly when a known model of the environment is available, and via simulation otherwise. We compare these algorithms on some large POMDPs, including noisy robot navigation and multi-agent problems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Multi-Agent, Policy-Gradient approach to Network Routing</title>
<link>https://arxiv.org/abs/2512.03211</link>
<guid>https://arxiv.org/abs/2512.03211</guid>
<content:encoded><![CDATA[
<div> Network routing, policy-gradient, reinforcement learning, multi-agent cooperation, reward shaping<br /><br />Summary:<br /><br />This paper addresses network routing as a distributed decision problem evaluated by numerical performance measures like average packet travel time. The authors apply OLPOMDP, a policy-gradient reinforcement learning algorithm, to simulated network routing scenarios across various network models. Multiple distributed agents, representing routers, learned to cooperate effectively without explicit communication channels. Notably, the system avoided selfish routing behaviors which, although beneficial on an individual level, were detrimental to overall network performance. An important contribution is the introduction of reward shaping, where the reward signal is modified to penalize sub-optimal routing behaviors explicitly. This modification significantly improved the convergence speed of the learning algorithm. The results demonstrate that reinforcement learning can successfully foster cooperative network routing strategies in a distributed manner, optimizing collective outcomes through shaped incentives. <div>
arXiv:2512.03211v1 Announce Type: new 
Abstract: Network routing is a distributed decision problem which naturally admits numerical performance measures, such as the average time for a packet to travel from source to destination. OLPOMDP, a policy-gradient reinforcement learning algorithm, was successfully applied to simulated network routing under a number of network models. Multiple distributed agents (routers) learned co-operative behavior without explicit inter-agent communication, and they avoided behavior which was individually desirable, but detrimental to the group's overall performance. Furthermore, shaping the reward signal by explicitly penalizing certain patterns of sub-optimal behavior was found to dramatically improve the convergence rate.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Perch 2.0 transfers 'whale' to underwater tasks</title>
<link>https://arxiv.org/abs/2512.03219</link>
<guid>https://arxiv.org/abs/2512.03219</guid>
<content:encoded><![CDATA[
<div> bioacoustics, Perch 2.0, marine mammals, few-shot learning, embeddings<br /><br />Summary:<br /><br />1. Perch 2.0 is a supervised bioacoustics foundation model pretrained on audio data from 14,597 species spanning birds, mammals, amphibians, and insects.<br />2. The model demonstrates state-of-the-art performance on multiple bioacoustic benchmarks despite minimal marine mammal or underwater audio data in its training set.<br />3. The study evaluates the effectiveness of Perch 2.0 for marine mammal and underwater audio classification tasks via few-shot transfer learning.<br />4. Linear probing is conducted on embeddings generated by Perch 2.0 and compared against other pretrained bioacoustic models like Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3.<br />5. Results indicate that Perch 2.0 embeddings consistently achieve superior performance in few-shot transfer learning scenarios, making it the recommended choice for developing linear classifiers for marine mammal classification with limited labeled data. <div>
arXiv:2512.03219v1 Announce Type: new 
Abstract: Perch 2.0 is a supervised bioacoustics foundation model pretrained on 14,597 species, including birds, mammals, amphibians, and insects, and has state-of-the-art performance on multiple benchmarks. Given that Perch 2.0 includes almost no marine mammal audio or classes in the training data, we evaluate Perch 2.0 performance on marine mammal and underwater audio tasks through few-shot transfer learning. We perform linear probing with the embeddings generated from this foundation model and compare performance to other pretrained bioacoustics models. In particular, we compare Perch 2.0 with previous multispecies whale, Perch 1.0, SurfPerch, AVES-bio, BirdAVES, and Birdnet V2.3 models, which have open-source tools for transfer-learning and agile modeling. We show that the embeddings from the Perch 2.0 model have consistently high performance for few-shot transfer learning, generally outperforming alternative embedding models on the majority of tasks, and thus is recommended when developing new linear classifiers for marine mammal classification with few labeled examples.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPARK: Stepwise Process-Aware Rewards for Reference-Free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03244</link>
<guid>https://arxiv.org/abs/2512.03244</guid>
<content:encoded><![CDATA[
<div> Keywords: Process reward models, self-consistency, meta-critique, reinforcement learning, mathematical reasoning<br /><br />Summary:  
This paper introduces SPARK, a novel three-stage framework designed to improve reinforcement learning (RL) by generating dense, step-level reward feedback without relying on costly annotations or ground truth references. First, SPARK uses a generator model to create diverse solution candidates and applies a verifier model that evaluates them through two verification methods: parallel scaling (self-consistency) and sequential scaling (meta-critique). Second, the verification results are synthesized into training data to fine-tune generative process reward models (PRMs), which act as reward functions for RL training. Results show that training PRMs on aggregated step-level independent verifications surpasses conventional ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench for detecting errors in mathematical reasoning steps, compared to 66.4 for reference-based and 61.9 for GPT-4o methods. In the final stage, the authors integrate the generative PRM with chain-of-thought verification (PRM-CoT) into RL to solve mathematical reasoning problems, introducing format constraints to mitigate reward hacking. Using the Qwen2.5-Math-7B model, they attain 47.4% accuracy averaged over six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR methods (43.9%). This approach enables effective reference-free RL training, extending applications to domains without verifiable answers or accessible ground truth. <div>
arXiv:2512.03244v1 Announce Type: new 
Abstract: Process reward models (PRMs) that provide dense, step-level feedback have shown promise for reinforcement learning, yet their adoption remains limited by the need for expensive step-level annotations or ground truth references. We propose SPARK: a three-stage framework where in the first stage a generator model produces diverse solutions and a verifier model evaluates them using parallel scaling (self-consistency) and sequential scaling (meta-critique). In the second stage, we use these verification outputs as synthetic training data to fine-tune generative process reward models, which subsequently serve as reward signals during training. We show that aggregating multiple independent verifications at the step level produces training data for process reward models that surpass ground-truth outcome supervision, achieving 67.5 F1 on ProcessBench (a benchmark for identifying erroneous steps in mathematical reasoning) compared to 66.4 for reference-guided training and 61.9 for GPT-4o. In the final stage, we apply our generative PRM with chain-of-thought verification (PRM-CoT) as the reward model in RL experiments on mathematical reasoning, and introduce format constraints to prevent reward hacking. Using Qwen2.5-Math-7B, we achieve 47.4% average accuracy across six mathematical reasoning benchmarks, outperforming ground-truth-based RLVR (43.9%). Our work enables reference-free RL training that exceeds ground-truth methods, opening new possibilities for domains lacking verifiable answers or accessible ground truth.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Too Late to Recall: Explaining the Two-Hop Problem in Multimodal Knowledge Retrieval</title>
<link>https://arxiv.org/abs/2512.03276</link>
<guid>https://arxiv.org/abs/2512.03276</guid>
<content:encoded><![CDATA[
<div> Keywords: vision language models, factual recall, entity representations, mechanistic analysis, multimodal alignment<br /><br />Summary: Training vision language models (VLMs) aims to align visual input representations from a vision encoder with textual embeddings of pretrained large language models (LLMs). This study reveals that many VLMs suffer from degraded factual recall compared to their standalone LLM backbones, raising concerns about the effectiveness of multimodal fine-tuning for extending LLM knowledge to visual inputs. The authors conceptualize factual recall in VLMs as a two-hop problem: first, forming accurate entity representations from visual data, and second, using those representations to retrieve associated factual knowledge. By benchmarking 14 VLMs of different architectures (LLaVA, Native, Cross-Attention), parameter sizes (7B–124B), and training setups on factual recall tasks against their LLM backbones, they find that 11 out of 14 models display factual recall degradation. An in-depth mechanistic analysis focusing on five selected models (three high-performing and two low-performing in recall) demonstrates that poor-performing VLMs resolve entity representations late in the computation, hindering the reuse of existing factual recall circuits in LLMs. In contrast, better-performing VLMs resolve entities early, enabling effective use of pretrained factual knowledge. Finally, the authors propose two interventions to recover or enhance performance: patching entity representations from LLM backbones into VLMs and employing chain-of-thought prompting. Overall, the work highlights that timely early entity resolution is critical and demonstrates how mechanistic interpretability can elucidate and address multimodal alignment failures. <div>
arXiv:2512.03276v1 Announce Type: new 
Abstract: Training vision language models (VLMs) aims to align visual representations from a vision encoder with the textual representations of a pretrained large language model (LLM). However, many VLMs exhibit reduced factual recall performance compared to their LLM backbones, raising the question of how effective multimodal fine-tuning is at extending existing mechanisms within the LLM to visual inputs. We argue that factual recall based on visual inputs requires VLMs to solve a two-hop problem: (1) forming entity representations from visual inputs, and (2) recalling associated factual knowledge based on these entity representations. By benchmarking 14 VLMs with various architectures (LLaVA, Native, Cross-Attention), sizes (7B-124B parameters), and training setups on factual recall tasks against their original LLM backbone models, we find that 11 of 14 models exhibit factual recall degradation. We select three models with high and two models with low performance degradation, and use attribution patching, activation patching, and probing to show that degraded VLMs struggle to use the existing factual recall circuit of their LLM backbone, because they resolve the first hop too late in the computation. In contrast, high-performing VLMs resolve entity representations early enough to reuse the existing factual recall mechanism. Finally, we demonstrate two methods to recover performance: patching entity representations from the LLM backbone into the VLM, and prompting with chain-of-thought reasoning. Our results highlight that the speed of early entity resolution critically determines how effective VLMs are in using preexisting LLM mechanisms. More broadly, our work illustrates how mechanistic analysis can explain and unveil systematic failures in multimodal alignment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BlendedNet++: A Large-Scale Blended Wing Body Aerodynamics Dataset and Benchmark</title>
<link>https://arxiv.org/abs/2512.03280</link>
<guid>https://arxiv.org/abs/2512.03280</guid>
<content:encoded><![CDATA[
<div> Keywords: aerodynamic surrogates, blended wing body, CFD dataset, inverse design, diffusion model<br /><br />Summary:  
The paper introduces BlendedNet++, a large-scale aerodynamic dataset specifically designed for blended wing body (BWB) aircraft, addressing the lack of extensive field-resolved datasets for machine learning in aerodynamics. It contains over 12,000 unique geometry cases with steady RANS CFD simulations carried out under a single flight condition, providing 12,490 aerodynamic results. Each case includes integrated force/moment coefficients (CL, CD, CM) and dense surface fields of pressure and skin friction coefficients (Cp, Cfx, Cfy, Cfz), facilitating pointwise aerodynamic analysis. The authors establish a standardized benchmark for forward surrogate modeling and evaluate six different model architectures: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), FiLMNet, and a Graph Neural Operator Transformer (GNOT). For inverse design, they formulate a task to achieve a target lift-to-drag ratio under fixed flight conditions using a conditional diffusion model. Performance of this approach is benchmarked against gradient-based optimization and a hybrid method combining diffusion sampling and further optimization. BlendedNet++ offers a unified forward and inverse modeling protocol with multi-model baselines, promoting fair, reproducible comparisons across various architectures and optimization strategies. The dataset and associated resources will be publicly released upon acceptance to support further research in aerodynamic field prediction and aircraft design optimization. <div>
arXiv:2512.03280v1 Announce Type: new 
Abstract: Despite progress in machine learning-based aerodynamic surrogates, the scarcity of large, field-resolved datasets limits progress on accurate pointwise prediction and reproducible inverse design for aircraft. We introduce BlendedNet++, a large-scale aerodynamic dataset and benchmark focused on blended wing body (BWB) aircraft. The dataset contains over 12,000 unique geometries, each simulated at a single flight condition, yielding 12,490 aerodynamic results for steady RANS CFD. For every case, we provide (i) integrated force/moment coefficients CL, CD, CM and (ii) dense surface fields of pressure and skin friction coefficients Cp and (Cfx, Cfy, Cfz). Using this dataset, we standardize a forward-surrogate benchmark to predict pointwise fields across six model families: GraphSAGE, GraphUNet, PointNet, a coordinate Transformer (Transolver-style), a FiLMNet (coordinate MLP with feature-wise modulation), and a Graph Neural Operator Transformer (GNOT). Finally, we present an inverse design task of achieving a specified lift-to-drag ratio under fixed flight conditions, implemented via a conditional diffusion model. To assess performance, we benchmark this approach against gradient-based optimization on the same surrogate and a diffusion-optimization hybrid that first samples with the conditional diffusion model and then further optimizes the designs. BlendedNet++ provides a unified forward and inverse protocol with multi-model baselines, enabling fair, reproducible comparison across architectures and optimization paradigms. We expect BlendedNet++ to catalyze reproducible research in field-level aerodynamics and inverse design; resources (dataset, splits, baselines, and scripts) will be released upon acceptance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Frequency Federated Learning for Human Activity Recognition Using Head-Worn Sensors</title>
<link>https://arxiv.org/abs/2512.03287</link>
<guid>https://arxiv.org/abs/2512.03287</guid>
<content:encoded><![CDATA[
<div> Human Activity Recognition, Federated Learning, Multi-frequency, Privacy-aware ML, Head-worn devices<br /><br />Summary:<br /><br />This article addresses the challenge of Human Activity Recognition (HAR) using head-worn devices such as earbuds and smart glasses, a less explored area compared to smartwatch or smartphone-based HAR. Traditional HAR methods rely on centralized data collection, which raises privacy concerns due to the need to upload sensitive user information to central servers. To overcome this, the authors propose a multi-frequency Federated Learning (FL) approach that allows devices with varying data sampling frequencies to collaboratively learn a joint machine learning model without sharing raw data, thereby enhancing privacy. Multi-frequency FL supports the integration of heterogeneous devices that operate at different sampling rates, improving the flexibility and applicability of HAR systems. The research demonstrates improved performance on two benchmark datasets when compared to frequency-specific models, showing the efficacy of the multi-frequency FL strategy. This method not only advances privacy-aware HAR but also expands its reach to head-worn devices. Additionally, the authors provide a publicly available implementation of their proposed network to encourage further research and development in this domain. Overall, this work presents promising directions for privacy-preserving, multi-device, multi-frequency HAR utilizing Federated Learning. <div>
arXiv:2512.03287v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) benefits various application domains, including health and elderly care. Traditional HAR involves constructing pipelines reliant on centralized user data, which can pose privacy concerns as they necessitate the uploading of user data to a centralized server. This work proposes multi-frequency Federated Learning (FL) to enable: (1) privacy-aware ML; (2) joint ML model learning across devices with varying sampling frequency. We focus on head-worn devices (e.g., earbuds and smart glasses), a relatively unexplored domain compared to traditional smartwatch- or smartphone-based HAR. Results have shown improvements on two datasets against frequency-specific approaches, indicating a promising future in the multi-frequency FL-HAR task. The proposed network's implementation is publicly available for further research and development.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ASPEN: An Adaptive Spectral Physics-Enabled Network for Ginzburg-Landau Dynamics</title>
<link>https://arxiv.org/abs/2512.03290</link>
<guid>https://arxiv.org/abs/2512.03290</guid>
<content:encoded><![CDATA[
<div> Keywords: Physics-Informed Neural Networks, Adaptive Spectral Layer, Fourier Features, Complex Ginzburg-Landau Equation, Nonlinear Dynamical Systems<br /><br />Summary:<br /><br />1. Physics-Informed Neural Networks (PINNs) are a popular mesh-free method for solving partial differential equations but face challenges with stiff, multi-scale, and nonlinear problems due to spectral bias limiting high-frequency representation.<br />2. The paper introduces the Adaptive Spectral Physics-Enabled Network (ASPEN), which incorporates an adaptive spectral layer with learnable Fourier features at the input stage, allowing dynamic tuning of the network’s spectral basis during training.<br />3. ASPEN is applied to the challenging Complex Ginzburg-Landau Equation (CGLE), a benchmark for nonlinear and stiff spatio-temporal dynamics, where standard PINNs fail by producing non-physical oscillations.<br />4. Results show ASPEN successfully solves the CGLE with high accuracy, achieving a low median physics residual (5.10 x 10^-3) and producing solutions visually indistinguishable from high-resolution ground truth.<br />5. ASPEN's solutions are physically consistent, capturing critical emergent properties such as rapid free energy relaxation and long-term domain wall front stability, demonstrating robustness in complex dynamical system modeling where standard PINNs fail. <div>
arXiv:2512.03290v1 Announce Type: new 
Abstract: Physics-Informed Neural Networks (PINNs) have emerged as a powerful, mesh-free paradigm for solving partial differential equations (PDEs). However, they notoriously struggle with stiff, multi-scale, and nonlinear systems due to the inherent spectral bias of standard multilayer perceptron (MLP) architectures, which prevents them from adequately representing high-frequency components. In this work, we introduce the Adaptive Spectral Physics-Enabled Network (ASPEN), a novel architecture designed to overcome this critical limitation. ASPEN integrates an adaptive spectral layer with learnable Fourier features directly into the network's input stage. This mechanism allows the model to dynamically tune its own spectral basis during training, enabling it to efficiently learn and represent the precise frequency content required by the solution. We demonstrate the efficacy of ASPEN by applying it to the complex Ginzburg-Landau equation (CGLE), a canonical and challenging benchmark for nonlinear, stiff spatio-temporal dynamics. Our results show that a standard PINN architecture catastrophically fails on this problem, diverging into non-physical oscillations. In contrast, ASPEN successfully solves the CGLE with exceptional accuracy. The predicted solution is visually indistinguishable from the high-resolution ground truth, achieving a low median physics residual of 5.10 x 10^-3. Furthermore, we validate that ASPEN's solution is not only pointwise accurate but also physically consistent, correctly capturing emergent physical properties, including the rapid free energy relaxation and the long-term stability of the domain wall front. This work demonstrates that by incorporating an adaptive spectral basis, our framework provides a robust and physically-consistent solver for complex dynamical systems where standard PINNs fail, opening new options for machine learning in challenging physical domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Regime-Switching Forecasts with Distribution-Free Uncertainty: Deep Switching State-Space Models Meet Conformal Prediction</title>
<link>https://arxiv.org/abs/2512.03298</link>
<guid>https://arxiv.org/abs/2512.03298</guid>
<content:encoded><![CDATA[
<div> Keywords: regime transitions, uncertainty quantification, Deep Switching State Space Models, Adaptive Conformal Inference, nonstationarity  

<br /><br />Summary:  
This article addresses the challenge of uncertainty quantification in time series forecasting under regime transitions, which typically violate stationarity assumptions. The authors propose a methodology that combines Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI) to provide distribution-free uncertainty estimates. They introduce a unified conformal wrapper that integrates with strong sequence forecasting baselines such as S4, MC-Dropout GRU, sparse Gaussian processes, and change-point local models. This wrapper enables the generation of online predictive confidence bands that offer finite-sample marginal guarantees even in the presence of nonstationarity and model misspecification. The approach is comprehensively tested on both synthetic and real-world datasets. Results demonstrate that the conformalized forecasters maintain near-nominal coverage probabilities, indicating reliable uncertainty quantification, while maintaining competitive point prediction accuracy. Additionally, the method generally produces more efficient predictive bands, meaning tighter and more informative uncertainty intervals. Overall, the work contributes a robust framework for uncertainty-aware forecasting in complex, regime-switching environments prevalent in many real-world time series applications. <div>
arXiv:2512.03298v1 Announce Type: new 
Abstract: Regime transitions routinely break stationarity in time series, making calibrated uncertainty as important as point accuracy. We study distribution-free uncertainty for regime-switching forecasting by coupling Deep Switching State Space Models with Adaptive Conformal Inference (ACI) and its aggregated variant (AgACI). We also introduce a unified conformal wrapper that sits atop strong sequence baselines including S4, MC-Dropout GRU, sparse Gaussian processes, and a change-point local model to produce online predictive bands with finite-sample marginal guarantees under nonstationarity and model misspecification. Across synthetic and real datasets, conformalized forecasters achieve near-nominal coverage with competitive accuracy and generally improved band efficiency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HydroDCM: Hydrological Domain-Conditioned Modulation for Cross-Reservoir Inflow Prediction</title>
<link>https://arxiv.org/abs/2512.03300</link>
<guid>https://arxiv.org/abs/2512.03300</guid>
<content:encoded><![CDATA[
<div> reservoir inflow prediction, domain generalization, domain shift, spatial metadata, adversarial learning  

<br /><br />Summary:  
Deep learning models for reservoir inflow prediction often experience performance drops when applied to new reservoirs due to domain shifts caused by differing data distributions. Traditional domain generalization (DG) approaches attempt to find domain-invariant representations to address this but face limitations in hydrological systems where each reservoir has distinct inflow dynamics and spatial metadata influences patterns indirectly. To tackle these challenges, the paper introduces HydroDCM, a novel scalable DG framework designed for cross-reservoir inflow forecasting. HydroDCM leverages spatial metadata to generate pseudo-domain labels, which then guide adversarial learning aimed at extracting invariant temporal features relevant across domains. Additionally, the framework includes lightweight conditioning layers during inference that adapt these learned features based on the target reservoir's metadata, allowing a balance between DG invariance and location-specific adjustment. Experimental validation on 30 real reservoirs from the Upper Colorado River Basin shows HydroDCM significantly outperforms existing state-of-the-art DG methods under scenarios involving many domains. Besides improved accuracy, the approach remains computationally efficient, making it a practical solution for real-world hydrological forecasting applications involving diverse reservoir systems. <div>
arXiv:2512.03300v1 Announce Type: new 
Abstract: Deep learning models have shown promise in reservoir inflow prediction, yet their performance often deteriorates when applied to different reservoirs due to distributional differences, referred to as the domain shift problem. Domain generalization (DG) solutions aim to address this issue by extracting domain-invariant representations that mitigate errors in unseen domains. However, in hydrological settings, each reservoir exhibits unique inflow patterns, while some metadata beyond observations like spatial information exerts indirect but significant influence. This mismatch limits the applicability of conventional DG techniques to many-domain hydrological systems. To overcome these challenges, we propose HydroDCM, a scalable DG framework for cross-reservoir inflow forecasting. Spatial metadata of reservoirs is used to construct pseudo-domain labels that guide adversarial learning of invariant temporal features. During inference, HydroDCM adapts these features through light-weight conditioning layers informed by the target reservoir's metadata, reconciling DG's invariance with location-specific adaptation. Experiment results on 30 real-world reservoirs in the Upper Colorado River Basin demonstrate that our method substantially outperforms state-of-the-art DG baselines under many-domain conditions and remains computationally efficient.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2512.03307</link>
<guid>https://arxiv.org/abs/2512.03307</guid>
<content:encoded><![CDATA[
<div> Keywords: Tabular Foundation Models, synthetic datasets, adversarial training, optimality gap, TabPFN V2<br /><br />Summary: The paper presents advancements in tabular foundation models (TFMs), emphasizing their growing ability to outperform traditional machine learning techniques for structured data. A significant insight is that TFMs can be pretrained exclusively on synthetic data, which allows for the deliberate design of data generators to instill desirable model characteristics. Unlike previous approaches that focused on improving generator priors, this work introduces a parameterized generator distribution to adopt an adversarial robustness perspective. Specifically, the generator is adapted during training to highlight datasets that are most challenging for the model. The authors formalize this concept through an optimality gap metric, which measures the difference between the TFM’s performance and the best achievable outcomes estimated by strong baseline models such as XGBoost, CatBoost, and Random Forests. Building upon this framework, they develop Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training strategy. When applied to the TabPFN V2 classifier, RTFM yields up to a 6% improvement in mean normalized AUC compared to the original TabPFN and other baseline algorithms, with fewer than 100,000 additional synthetic datasets needed. These findings suggest a new promising direction for targeted adversarial training and fine-tuning of TFMs solely using synthetic data. <div>
arXiv:2512.03307v1 Announce Type: new 
Abstract: The development of tabular foundation models (TFMs) has accelerated in recent years, showing strong potential to outperform traditional ML methods for structured data. A key finding is that TFMs can be pretrained entirely on synthetic datasets, opening opportunities to design data generators that encourage desirable model properties. Prior work has mainly focused on crafting high-quality priors over generators to improve overall pretraining performance. Our insight is that parameterizing the generator distribution enables an adversarial robustness perspective: during training, we can adapt the generator to emphasize datasets that are particularly challenging for the model. We formalize this by introducing an optimality gap measure, given by the difference between TFM performance and the best achievable performance as estimated by strong baselines such as XGBoost, CatBoost, and Random Forests. Building on this idea, we propose Robust Tabular Foundation Models (RTFM), a model-agnostic adversarial training framework. Applied to the TabPFN V2 classifier, RTFM improves benchmark performance, with up to a 6% increase in mean normalized AUC over the original TabPFN and other baseline algorithms, while requiring less than 100k additional synthetic datasets. These results highlight a promising new direction for targeted adversarial training and fine-tuning of TFMs using synthetic data alone.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrofitting Earth System Models with Cadence-Limited Neural Operator Updates</title>
<link>https://arxiv.org/abs/2512.03309</link>
<guid>https://arxiv.org/abs/2512.03309</guid>
<content:encoded><![CDATA[
<div> coarse resolution, bias correction, operator learning, Earth-system model, U-Net<br /><br />Summary:<br /><br />This article addresses the challenges of coarse resolution, imperfect parameterizations, and uncertain initial states that limit the accuracy of Earth-system model (ESM) predictions. Traditional bias correction methods improve constrained simulations but are less effective when models run freely. To overcome this, the authors introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies, applying these corrections online during model integration. They design two operator architectures, the Inception U-Net (IUNet) and a multi-scale network (M&amp;M), which incorporate diverse upsampling techniques and varying receptive fields to capture multiscale nonlinear features while adhering to Energy Exascale Earth System Model (E3SM) runtime constraints. Training on two years of E3SM simulations nudged toward ERA5 reanalysis enables these operators to generalize well across vertical height levels and seasonal variations. Offline tests show that both architectures outperform standard U-Net baselines, suggesting that the functional diversity of the networks, rather than parameter count, is key to performance. In online hybrid E3SM experiments, the M&amp;M architecture achieves the most consistent bias reductions across several variables and vertical layers. Importantly, the ML-augmented model configurations maintain long-term stability and computational feasibility in multi-year runs, paving the way for scalable hybrid Earth-system modeling that emphasizes stability, portability, and cadence-limited updates. <div>
arXiv:2512.03309v1 Announce Type: new 
Abstract: Coarse resolution, imperfect parameterizations, and uncertain initial states and forcings limit Earth-system model (ESM) predictions. Traditional bias correction via data assimilation improves constrained simulations but offers limited benefit once models run freely. We introduce an operator-learning framework that maps instantaneous model states to bias-correction tendencies and applies them online during integration. Building on a U-Net backbone, we develop two operator architectures Inception U-Net (IUNet) and a multi-scale network (M\&amp;M) that combine diverse upsampling and receptive fields to capture multiscale nonlinear features under Energy Exascale Earth System Model (E3SM) runtime constraints. Trained on two years E3SM simulations nudged toward ERA5 reanalysis, the operators generalize across height levels and seasons. Both architectures outperform standard U-Net baselines in offline tests, indicating that functional richness rather than parameter count drives performance. In online hybrid E3SM runs, M\&amp;M delivers the most consistent bias reductions across variables and vertical levels. The ML-augmented configurations remain stable and computationally feasible in multi-year simulations, providing a practical pathway for scalable hybrid modeling. Our framework emphasizes long-term stability, portability, and cadence-limited updates, demonstrating the utility of expressive ML operators for learning structured, cross-scale relationships and retrofitting legacy ESMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cache What Lasts: Token Retention for Memory-Bounded KV Cache in LLMs</title>
<link>https://arxiv.org/abs/2512.03324</link>
<guid>https://arxiv.org/abs/2512.03324</guid>
<content:encoded><![CDATA[
<div> Memory, KV cache, retention gate, token eviction, LLM interpretability<br /><br />Summary:<br /><br />1. The paper addresses the memory and computational bottlenecks in long-horizon large language model (LLM) inference caused by the quadratic complexity of self-attention and growing key-value (KV) caches.  
2. It introduces TRIM-KV, a novel method that assigns each token an intrinsic importance score at creation through a lightweight retention gate, which predicts a scalar retention score that decays over time, specific to layer and attention head.  
3. Tokens with low retention scores are evicted when memory capacity is exceeded, ensuring the KV cache prioritizes the most critical tokens while maintaining efficiency.  
4. TRIM-KV is trained efficiently by distilling knowledge from a frozen LLM and incorporating a capacity loss, requiring only fine-tuning of the retention gates and adding minimal inference overhead.  
5. Experimental results across diverse tasks—mathematical reasoning, procedural generation, conversational long-memory, and long-context understanding—show TRIM-KV consistently outperforms existing eviction and retrieval baselines, especially in memory-constrained settings, sometimes even surpassing full-cache models by reducing noise from uninformative tokens.  
6. Qualitative analysis reveals that retention scores align well with human intuition and naturally recover known heuristics like sink tokens, sliding windows, and gist compression without explicit programming.  
7. Beyond efficiency gains, the learned retention scores offer interpretability into layer- and head-specific functions, pointing toward new directions in understanding LLM behavior. <div>
arXiv:2512.03324v1 Announce Type: new 
Abstract: Memory and computation remain core bottlenecks in long-horizon LLM inference due to the quadratic cost of self-attention and the ever-growing key-value (KV) cache. Existing strategies for memory-bounded inference, such as quantization, offloading, or heuristic KV eviction, either incur high orchestration costs or rely on unreliable attention-based proxies of importance. We propose TRIM-KV, a novel approach that learns each token's intrinsic importance at creation time via a lightweight retention gate. Each gate predicts a scalar retention score that decays over time, reflecting the long-term utility of the token for a specific layer and head. Tokens with low scores are evicted when the memory budget is exceeded, ensuring that the cache always contains the most critical tokens. TRIM-KV is trained efficiently through distillation from a frozen LLM combined with a capacity loss, requiring only gate fine-tuning and adding negligible inference overhead. Across mathematical reasoning (GSM8K, MATH-500, AIME24), procedural generation (LongProc), conversational long-memory benchmarks (LongMemEval), and long-context understanding (LongBench and SCBench), TRIM-KV consistently outperforms strong eviction and learnable retrieval baselines, especially in low-memory regimes. Remarkably, it even surpasses full-cache models in some settings, showing that selective retention can serve as a form of regularization, suppressing noise from uninformative tokens. Qualitative analyses further reveal that learned retention scores align with human intuition, naturally recovering heuristics such as sink tokens, sliding windows, and gist compression without explicit design. Beyond efficiency, retention scores provide insights into layer- and head-specific roles, suggesting a new path toward LLM interpretability.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Single-Round Scalable Analytic Federated Learning</title>
<link>https://arxiv.org/abs/2512.03336</link>
<guid>https://arxiv.org/abs/2512.03336</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Learning, Analytic FL, non-linear models, communication efficiency, scalable embeddings<br /><br />Summary:<br /><br />1. Federated Learning (FL) faces two main challenges: high communication overhead and significant performance degradation when handling heterogeneous (non-IID) data.<br /><br />2. Analytic FL (AFL) addresses these issues by offering a single-round, data distribution invariant aggregation method, but it is restricted to linear models, limiting its expressiveness.<br /><br />3. Recent non-linear approaches such as DeepAFL improve model accuracy but lose the single-round communication efficiency by requiring multiple rounds.<br /><br />4. The proposed SAFLe framework overcomes this trade-off by introducing a structured head utilizing bucketed features and sparse, grouped embeddings, enabling scalable non-linear modeling.<br /><br />5. SAFLe’s non-linear architecture is mathematically equivalent to a high-dimensional linear regression, enabling it to leverage AFL’s single-shot invariant aggregation law.<br /><br />6. Empirical results show SAFLe achieves state-of-the-art performance in analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL across various benchmarks.<br /><br />7. This demonstrates SAFLe as an efficient, scalable, and effective federated learning solution for vision applications, combining the strengths of analytic FL and non-linear expressivity without increased communication rounds. <div>
arXiv:2512.03336v1 Announce Type: new 
Abstract: Federated Learning (FL) is plagued by two key challenges: high communication overhead and performance collapse on heterogeneous (non-IID) data. Analytic FL (AFL) provides a single-round, data distribution invariant solution, but is limited to linear models. Subsequent non-linear approaches, like DeepAFL, regain accuracy but sacrifice the single-round benefit. In this work, we break this trade-off. We propose SAFLe, a framework that achieves scalable non-linear expressivity by introducing a structured head of bucketed features and sparse, grouped embeddings. We prove this non-linear architecture is mathematically equivalent to a high-dimensional linear regression. This key equivalence allows SAFLe to be solved with AFL's single-shot, invariant aggregation law. Empirically, SAFLe establishes a new state-of-the-art for analytic FL, significantly outperforming both linear AFL and multi-round DeepAFL in accuracy across all benchmarks, demonstrating a highly efficient and scalable solution for federated vision.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions</title>
<link>https://arxiv.org/abs/2512.03354</link>
<guid>https://arxiv.org/abs/2512.03354</guid>
<content:encoded><![CDATA[
<div> Keywords: Off-Policy Evaluation, deterministic auctions, bid landscape model, Self-Normalized Inverse Propensity Scoring, online advertising  

<br /><br />Summary:  
This paper addresses the challenge of applying Off-Policy Evaluation (OPE) in the context of online advertising auctions, which are deterministic and winner-takes-all, unlike stochastic policy settings common in recommender systems. Traditional OPE methods fail here due to zero exposure probability for non-winning ads. The authors propose the first principled OPE framework tailored for deterministic auctions by utilizing the bid landscape model to approximate propensity scores. This approximation enables the use of stable estimators such as Self-Normalized Inverse Propensity Scoring (SNIPS) for reliable counterfactual evaluation. Their approach is validated on both the AuctionNet simulation benchmark and through a two-week online A/B test on a large-scale industrial platform. Results demonstrate strong alignment with real online outcomes, achieving 92% Mean Directional Accuracy (MDA) in Click-Through Rate (CTR) prediction, a key metric for deployment decisions as it indicates whether a new model will improve or degrade performance. Overall, this work delivers the first practical, validated, and efficient framework for reliable OPE in deterministic auction environments, providing a safer and resource-efficient alternative to costly and risky live online experiments. <div>
arXiv:2512.03354v1 Announce Type: new 
Abstract: Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A2G-QFL: Adaptive Aggregation with Two Gains in Quantum Federated learning</title>
<link>https://arxiv.org/abs/2512.03363</link>
<guid>https://arxiv.org/abs/2512.03363</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Quantum Networks, Adaptive Aggregation, Teleportation Fidelity, Heterogeneous Systems<br /><br />Summary:<br /><br />This paper addresses the challenges faced by federated learning (FL) deployed across quantum-enabled and heterogeneous classical networks, where uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatches between local and global models severely degrade performance. It highlights the limitations of classical aggregation rules which assume Euclidean topology and uniform communication reliability, making them inadequate for emerging quantum federated systems. The authors propose A2G (Adaptive Aggregation with Two Gains), a novel framework that incorporates dual gains: a geometry gain for regulating geometric blending between models and a QoS gain that reflects client importance based on teleportation fidelity, latency, and device instability. The paper formulates the A2G update rule and provides theoretical convergence guarantees under assumptions of smoothness and bounded variance. Moreover, A2G generalizes existing methods by recovering FedAvg, QoS-aware averaging, and manifold-based aggregation as special cases. Experimental evaluation on a quantum-classical hybrid testbed demonstrates that A2G achieves improved training stability and higher accuracy in the presence of heterogeneous and noisy network conditions, thus validating its effectiveness for federated learning in quantum-enabled environments. <div>
arXiv:2512.03363v1 Announce Type: new 
Abstract: Federated learning (FL) deployed over quantum enabled and heterogeneous classical networks faces significant performance degradation due to uneven client quality, stochastic teleportation fidelity, device instability, and geometric mismatch between local and global models. Classical aggregation rules assume euclidean topology and uniform communication reliability, limiting their suitability for emerging quantum federated systems. This paper introduces A2G (Adaptive Aggregation with Two Gains), a dual gain framework that jointly regulates geometric blending through a geometry gain and modulates client importance using a QoS gain derived from teleportation fidelity, latency, and instability. We develop the A2G update rule, establish convergence guarantees under smoothness and bounded variance assumptions, and show that A2G recovers FedAvg, QoS aware averaging, and manifold based aggregation as special cases. Experiments on a quantum classical hybrid testbed demonstrate improved stability and higher accuracy under heterogeneous and noisy conditions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAGE-ID: A Multimodal Generative Framework for Intrusion Detection Systems</title>
<link>https://arxiv.org/abs/2512.03375</link>
<guid>https://arxiv.org/abs/2512.03375</guid>
<content:encoded><![CDATA[
<div> Intrusion Detection, Multimodal Data, Diffusion Model, Data Augmentation, Cybersecurity<br /><br />Summary:<br /><br />This paper addresses critical challenges faced by modern Intrusion Detection Systems (IDS), specifically dealing with heterogeneous network traffic, evolving cyber threats, and significant imbalances between benign and attack flow data. Existing data augmentation methods using generative models are typically limited to single data modalities and do not effectively capture dependencies across different data domains. To overcome these limitations, the authors propose MAGE-ID, a novel multimodal generative framework based on diffusion processes that integrates tabular flow features with their corresponding transformed images using a shared latent prior. MAGE-ID employs a combined architecture featuring Transformer-based and CNN-based variational encoders alongside an EDM style denoiser to enable coherent and balanced synthesis of multimodal data. Evaluations conducted on popular benchmark datasets CIC-IDS-2017 and NSL-KDD reveal that MAGE-ID significantly outperforms existing baselines such as TabSyn and TabDDPM in terms of data fidelity and diversity. Importantly, this improved data generation translates into better downstream intrusion detection performance. The results demonstrate that MAGE-ID is an effective augmentation approach capable of enhancing the robustness and accuracy of IDS by leveraging multimodal data correlations and advanced generative modeling techniques. <div>
arXiv:2512.03375v1 Announce Type: new 
Abstract: Modern Intrusion Detection Systems (IDS) face severe challenges due to heterogeneous network traffic, evolving cyber threats, and pronounced data imbalance between benign and attack flows. While generative models have shown promise in data augmentation, existing approaches are limited to single modalities and fail to capture cross-domain dependencies. This paper introduces MAGE-ID (Multimodal Attack Generator for Intrusion Detection), a diffusion-based generative framework that couples tabular flow features with their transformed images through a unified latent prior. By jointly training Transformer and CNN-based variational encoders with an EDM style denoiser, MAGE-ID achieves balanced and coherent multimodal synthesis. Evaluations on CIC-IDS-2017 and NSL-KDD demonstrate significant improvements in fidelity, diversity, and downstream detection performance over TabSyn and TabDDPM, highlighting the effectiveness of MAGE-ID for multimodal IDS augmentation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs</title>
<link>https://arxiv.org/abs/2512.03383</link>
<guid>https://arxiv.org/abs/2512.03383</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, post-training quantization, low-rank compression, pruning, edge devices<br /><br />Summary: Deploying large language models (LLMs) on mobile and edge devices faces challenges due to limited memory and shared computational resources, which are further complicated by varying device workloads. The paper introduces UniQL, a unified framework combining post-training quantization and low-rank compression with configurable on-device pruning to optimize LLM deployment on resource-constrained platforms. UniQL supports various model architectures, including Transformers, State Space Models (SSMs), and hybrid models, making it versatile for diverse edge applications. Key innovations include a structured weight-sorting method that accelerates computation by 20 times, quantization-aware singular value decomposition (SVD) to reduce quantization noise, state-aware weight sorting specifically designed for SSMs, and a fused rotary positional embedding (RoPE) kernel optimized for pruned models. The framework integrates weight-sorting, fine-tuning, and quantization into a single cloud-based pass, while allowing devices to configure pruning rates up to 35% at runtime. Experimental results demonstrate memory footprint reductions between 4x to 5.7x and throughput improvements of 2.7x to 3.4x, all while maintaining accuracy within 5% of original uncompressed models at 15% pruning levels across tested models including Llama3, Qwen2.5, Mamba2, Nemotron-H, and Bamba-v2. The code and pre-quantized models are made publicly available on GitHub for further research and development. <div>
arXiv:2512.03383v1 Announce Type: new 
Abstract: Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tuning-Free Structured Sparse Recovery of Multiple Measurement Vectors using Implicit Regularization</title>
<link>https://arxiv.org/abs/2512.03393</link>
<guid>https://arxiv.org/abs/2512.03393</guid>
<content:encoded><![CDATA[
<div> Keywords: joint sparsity, multiple measurement vectors, implicit regularization, overparameterization, gradient descent<br /><br />Summary:<br /><br />This paper addresses the problem of recovering jointly sparse signals in the multiple measurement vectors (MMV) setting, a common challenge in machine learning. Traditional algorithms such as M-OMP and M-FOCUSS require careful tuning of parameters or prior knowledge about signal sparsity and noise variance, which can be impractical. The authors propose a novel tuning-free approach that leverages Implicit Regularization (IR) emerging from overparameterization to overcome these limitations. Their method reparameterizes the estimation matrix into factors, which effectively separate the shared row support of the signals from the entries corresponding to individual measurement vectors. By applying gradient descent to a standard least-squares objective defined on these factors, the optimization dynamics inherently encourage the emergence of row-sparsity without explicit regularization. Theoretical analysis demonstrates that with a sufficiently small and balanced initialization, the optimization behaves like a "momentum-like" process, causing rows corresponding to the true support to grow significantly faster in norm than others, thus guaranteeing convergence towards an ideal row-sparse solution. Experimental results confirm that the proposed framework achieves performance on par with traditional, tuned methods but without the need for any prior parameter knowledge or tuning, offering a practical and theoretically grounded alternative for MMV sparse recovery tasks. <div>
arXiv:2512.03393v1 Announce Type: new 
Abstract: Recovering jointly sparse signals in the multiple measurement vectors (MMV) setting is a fundamental problem in machine learning, but traditional methods like multiple measurement vectors orthogonal matching pursuit (M-OMP) and multiple measurement vectors FOCal Underdetermined System Solver (M-FOCUSS) often require careful parameter tuning or prior knowledge of the sparsity of the signal and/or noise variance. We introduce a novel tuning-free framework that leverages Implicit Regularization (IR) from overparameterization to overcome this limitation. Our approach reparameterizes the estimation matrix into factors that decouple the shared row-support from individual vector entries. We show that the optimization dynamics inherently promote the desired row-sparse structure by applying gradient descent to a standard least-squares objective on these factors. We prove that with a sufficiently small and balanced initialization, the optimization dynamics exhibit a "momentum-like" effect, causing the norms of rows in the true support to grow significantly faster than others. This formally guarantees that the solution trajectory converges towards an idealized row-sparse solution. Additionally, empirical results demonstrate that our approach achieves performance comparable to established methods without requiring any prior information or tuning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VS-Graph: Scalable and Efficient Graph Classification Using Hyperdimensional Computing</title>
<link>https://arxiv.org/abs/2512.03394</link>
<guid>https://arxiv.org/abs/2512.03394</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph classification, Hyperdimensional Computing, Vector Symbolic Architectures, Spike Diffusion, Associative Message Passing  

<br /><br />Summary:  
This paper addresses the fundamental task of graph classification, relevant in areas such as molecular property prediction and materials design. It recognizes that while Graph Neural Networks (GNNs) provide strong predictive performance through message passing, their high computational cost limits scalability and deployment on resource-constrained platforms. As an alternative, the work leverages Hyperdimensional Computing (HDC), also called Vector Symbolic Architectures (VSA), which is lightweight and brain-inspired but traditionally less accurate than GNNs in graph tasks. The authors propose a novel framework named VS-Graph that bridges the gap between HDC efficiency and GNN expressive power. VS-Graph incorporates a Spike Diffusion mechanism to identify nodes based on graph topology and an Associative Message Passing scheme to aggregate multi-hop neighborhood information entirely within high-dimensional vector spaces. Uniquely, this method does not rely on gradient-based optimization or backpropagation yet achieves competitive accuracy with modern GNNs, surpassing previous HDC baselines by 4-5% on benchmarks like MUTAG and DD. Additionally, VS-Graph matches or exceeds GNN performance on other datasets while speeding up training by up to 450 times. The framework also sustains high accuracy with vector dimensionality compressed down to 128, enabling ultra-efficient operation on edge devices and neuromorphic hardware. <div>
arXiv:2512.03394v1 Announce Type: new 
Abstract: Graph classification is a fundamental task in domains ranging from molecular property prediction to materials design. While graph neural networks (GNNs) achieve strong performance by learning expressive representations via message passing, they incur high computational costs, limiting their scalability and deployment on resource-constrained devices. Hyperdimensional Computing (HDC), also known as Vector Symbolic Architectures (VSA), offers a lightweight, brain-inspired alternative, yet existing HDC-based graph methods typically struggle to match the predictive performance of GNNs. In this work, we propose VS-Graph, a vector-symbolic graph learning framework that narrows the gap between the efficiency of HDC and the expressive power of message passing. VS-Graph introduces a Spike Diffusion mechanism for topology-driven node identification and an Associative Message Passing scheme for multi-hop neighborhood aggregation entirely within the high-dimensional vector space. Without gradient-based optimization or backpropagation, our method achieves competitive accuracy with modern GNNs, outperforming the prior HDC baseline by 4-5% on standard benchmarks such as MUTAG and DD. It also matches or exceeds the performance of the GNN baselines on several datasets while accelerating the training by a factor of up to 450x. Furthermore, VS-Graph maintains high accuracy even with the hypervector dimensionality reduced to D=128, demonstrating robustness under aggressive dimension compression and paving the way for ultra-efficient execution on edge and neuromorphic hardware.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Full-Stack Alignment: Co-Aligning AI and Institutions with Thick Models of Value</title>
<link>https://arxiv.org/abs/2512.03399</link>
<guid>https://arxiv.org/abs/2512.03399</guid>
<content:encoded><![CDATA[
<div> Keywords: full-stack alignment, values, normative reasoning, AI systems, collective goods<br /><br />Summary:<br /><br />This article argues that aligning individual AI systems solely with their operators' intentions is insufficient to ensure beneficial societal outcomes, as organizational goals may conflict with broader social interests. It introduces the concept of full-stack alignment, which calls for the simultaneous alignment of AI systems and the institutions governing them with the values held by people, without prescribing a fixed vision of well-being. The paper critiques current methods of representing values, such as utility functions and preference orderings, highlighting their deficiencies in distinguishing values from transient preferences, enabling normative reasoning, and modeling collective goods. To address these issues, the authors propose "thick models of value," structured frameworks that capture enduring values, acknowledge the social context of individual choices, and facilitate normative reasoning to apply values across diverse situations. They illustrate the utility of thick value models through five key applications: guiding AI value stewardship, creating normatively competent AI agents, designing negotiation systems that yield mutually beneficial outcomes, developing economic mechanisms that preserve meaning, and establishing democratic regulatory institutions. This approach aims to foster AI systems and social structures that better reflect complex human values and promote positive societal impacts. <div>
arXiv:2512.03399v1 Announce Type: new 
Abstract: Beneficial societal outcomes cannot be guaranteed by aligning individual AI systems with the intentions of their operators or users. Even an AI system that is perfectly aligned to the intentions of its operating organization can lead to bad outcomes if the goals of that organization are misaligned with those of other institutions and individuals. For this reason, we need full-stack alignment, the concurrent alignment of AI systems and the institutions that shape them with what people value. This can be done without imposing a particular vision of individual or collective flourishing. We argue that current approaches for representing values, such as utility functions, preference orderings, or unstructured text, struggle to address these and other issues effectively. They struggle to distinguish values from other signals, to support principled normative reasoning, and to model collective goods. We propose thick models of value will be needed. These structure the way values and norms are represented, enabling systems to distinguish enduring values from fleeting preferences, to model the social embedding of individual choices, and to reason normatively, applying values in new domains. We demonstrate this approach in five areas: AI value stewardship, normatively competent agents, win-win negotiation systems, meaning-preserving economic mechanisms, and democratic regulatory institutions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Better World Models Can Lead to Better Post-Training Performance</title>
<link>https://arxiv.org/abs/2512.03400</link>
<guid>https://arxiv.org/abs/2512.03400</guid>
<content:encoded><![CDATA[
<div> Keywords: world-modeling, Transformers, Rubik's Cube, state-prediction, reinforcement learning  

<br /><br />Summary:  
This paper investigates the impact of explicit world-modeling objectives on the internal representations and downstream performance of Transformer models during different phases of training. Using a controlled 2x2x2 Rubik’s Cube environment, the authors explore two primary questions: how explicit pretraining to model the world influences latent state representations, and how the quality of these learned world models affects performance after reinforcement learning (RL) post-training. They compare standard next-token prediction training with two explicit world-modeling approaches: (i) state-prediction pretraining alone and (ii) a joint objective combining state prediction and next-token prediction. After pretraining, the models undergo Group Relative Policy Optimization (GRPO) as a post-training method for solving the cube tasks. The study evaluates the learned representations through linear probing and causal intervention techniques to measure decodability and controllability of internal states. Results show that explicit world-modeling improves both the linear decodability and causal steerability of latent state representations. Crucially, higher-quality state representations enable better gains from subsequent GRPO training, particularly on more complex cube states. The findings suggest that enhancing state representation quality through explicit world-modeling can significantly improve post-training effectiveness for sequence-planning problems. <div>
arXiv:2512.03400v1 Announce Type: new 
Abstract: In this work we study how explicit world-modeling objectives affect the internal representations and downstream capability of Transformers across different training stages. We use a controlled 2x2x2 Rubik's Cube and ask: (1) how does explicitly pretraining a world model affect the model's latent representations, and (2) how does world-model quality affect the model's performance after reinforcement learning post-training? We compare standard next-token prediction to two explicit world-modeling strategies -- (i) state-prediction pretraining and (ii) a joint state-prediction + next-token objective -- and assess task performance after Group Relative Policy Optimization (GRPO) is applied as post-training. We evaluate the representation quality with linear probes and causal interventions. We find that explicit world-modeling yields more linearly decodable and causally steerable state representations. More importantly, we find that improved state representations lead to higher gains for GRPO, especially on harder cube states. Our results indicate that sharpening state representations can improve the effectiveness of post-training for sequence-planning tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GaussDetect-LiNGAM:Causal Direction Identification without Gaussianity test</title>
<link>https://arxiv.org/abs/2512.03428</link>
<guid>https://arxiv.org/abs/2512.03428</guid>
<content:encoded><![CDATA[
<div> GaussDetect-LiNGAM, bivariate causal discovery, Gaussianity, kernel independence tests, causal inference<br /><br />Summary:<br /><br />This paper introduces GaussDetect-LiNGAM, a new method for bivariate causal discovery that removes the need for direct Gaussianity tests by utilizing an equivalence between noise Gaussianity in the forward regression and residual independence in the reverse regression. Operating under standard LiNGAM assumptions—linearity, acyclicity, and exogeneity—the authors prove that Gaussian noise in the forward causal model corresponds exactly to independence between the regressor and residual in the reverse model. This key theoretical finding replaces conventional Gaussianity tests, which are often fragile and sensitive to sample size, with more robust kernel-based independence tests. Experimental evaluations confirm both the theoretical equivalence and practical advantages of GaussDetect-LiNGAM. The method shows consistent performance across various noise distributions and sample sizes while requiring fewer tests per decision (TPD), enhancing efficiency. Overall, GaussDetect-LiNGAM improves the practicality and reliability of causal inference within the LiNGAM framework, making it more accessible for real-world data analysis by reducing test complexity and increasing robustness against noisy or limited data scenarios. <div>
arXiv:2512.03428v1 Announce Type: new 
Abstract: We propose GaussDetect-LiNGAM, a novel approach for bivariate causal discovery that eliminates the need for explicit Gaussianity tests by leveraging a fundamental equivalence between noise Gaussianity and residual independence in the reverse regression. Under the standard LiNGAM assumptions of linearity, acyclicity, and exogeneity, we prove that the Gaussianity of the forward-model noise is equivalent to the independence between the regressor and residual in the reverse model. This theoretical insight allows us to replace fragile and sample-sensitive Gaussianity tests with robust kernel-based independence tests. Experimental results validate the equivalence and demonstrate that GaussDetect-LiNGAM maintains high consistency across diverse noise types and sample sizes, while reducing the number of tests per decision (TPD). Our method enhances both the efficiency and practical applicability of causal inference, making LiNGAM more accessible and reliable in real-world scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grokked Models are Better Unlearners</title>
<link>https://arxiv.org/abs/2512.03437</link>
<guid>https://arxiv.org/abs/2512.03437</guid>
<content:encoded><![CDATA[
arXiv:2512.03437v1 Announce Type: new 
Abstract: Grokking-delayed generalization that emerges well after a model has fit the training data-has been linked to robustness and representation quality. We ask whether this training regime also helps with machine unlearning, i.e., removing the influence of specified data without full retraining. We compare applying standard unlearning methods before versus after the grokking transition across vision (CNNs/ResNets on CIFAR, SVHN, and ImageNet) and language (a transformer on a TOFU-style setup). Starting from grokked checkpoints consistently yields (i) more efficient forgetting (fewer updates to reach a target forget level), (ii) less collateral damage (smaller drops on retained and test performance), and (iii) more stable updates across seeds, relative to early-stopped counterparts under identical unlearning algorithms. Analyses of features and curvature further suggest that post-grokking models learn more modular representations with reduced gradient alignment between forget and retain subsets, which facilitates selective forgetting. Our results highlight when a model is trained (pre- vs. post-grokking) as an orthogonal lever to how unlearning is performed, providing a practical recipe to improve existing unlearning methods without altering their algorithms.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal Opinion Integration for Financial Sentiment Analysis using Cross-Modal Attention</title>
<link>https://arxiv.org/abs/2512.03464</link>
<guid>https://arxiv.org/abs/2512.03464</guid>
<content:encoded><![CDATA[
arXiv:2512.03464v1 Announce Type: new 
Abstract: In recent years, financial sentiment analysis of public opinion has become increasingly important for market forecasting and risk assessment. However, existing methods often struggle to effectively integrate diverse opinion modalities and capture fine-grained interactions across them. This paper proposes an end-to-end deep learning framework that integrates two distinct modalities of financial opinions: recency modality (timely opinions) and popularity modality (trending opinions), through a novel cross-modal attention mechanism specifically designed for financial sentiment analysis. While both modalities consist of textual data, they represent fundamentally different information channels: recency-driven market updates versus popularity-driven collective sentiment. Our model first uses BERT (Chinese-wwm-ext) for feature embedding and then employs our proposed Financial Multi-Head Cross-Attention (FMHCA) structure to facilitate information exchange between these distinct opinion modalities. The processed features are optimized through a transformer layer and fused using multimodal factored bilinear pooling for classification into negative, neutral, and positive sentiment. Extensive experiments on a comprehensive dataset covering 837 companies demonstrate that our approach achieves an accuracy of 83.5%, significantly outperforming baselines including BERT+Transformer by 21 percent. These results highlight the potential of our framework to support more accurate financial decision-making and risk management.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Event-Based Model for Disease Subtype and Stage Inference</title>
<link>https://arxiv.org/abs/2512.03467</link>
<guid>https://arxiv.org/abs/2512.03467</guid>
<content:encoded><![CDATA[
arXiv:2512.03467v1 Announce Type: new 
Abstract: Chronic diseases often progress differently across patients. Rather than randomly varying, there are typically a small number of subtypes for how a disease progresses across patients. To capture this structured heterogeneity, the Subtype and Stage Inference Event-Based Model (SuStaIn) estimates the number of subtypes, the order of disease progression for each subtype, and assigns each patient to a subtype from primarily cross-sectional data. It has been widely applied to uncover the subtypes of many diseases and inform our understanding of them. But how robust is its performance? In this paper, we develop a principled Bayesian subtype variant of the event-based model (BEBMS) and compare its performance to SuStaIn in a variety of synthetic data experiments with varied levels of model misspecification. BEBMS substantially outperforms SuStaIn across ordering, staging, and subtype assignment tasks. Further, we apply BEBMS and SuStaIn to a real-world Alzheimer's data set. We find BEBMS has results that are more consistent with the scientific consensus of Alzheimer's disease progression than SuStaIn.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SweetDeep: A Wearable AI Solution for Real-Time Non-Invasive Diabetes Screening</title>
<link>https://arxiv.org/abs/2512.03471</link>
<guid>https://arxiv.org/abs/2512.03471</guid>
<content:encoded><![CDATA[
arXiv:2512.03471v1 Announce Type: new 
Abstract: The global rise in type 2 diabetes underscores the need for scalable and cost-effective screening methods. Current diagnosis requires biochemical assays, which are invasive and costly. Advances in consumer wearables have enabled early explorations of machine learning-based disease detection, but prior studies were limited to controlled settings. We present SweetDeep, a compact neural network trained on physiological and demographic data from 285 (diabetic and non-diabetic) participants in the EU and MENA regions, collected using Samsung Galaxy Watch 7 devices in free-living conditions over six days. Each participant contributed multiple 2-minute sensor recordings per day, totaling approximately 20 recordings per individual. Despite comprising fewer than 3,000 parameters, SweetDeep achieves 82.5% patient-level accuracy (82.1% macro-F1, 79.7% sensitivity, 84.6% specificity) under three-fold cross-validation, with an expected calibration error of 5.5%. Allowing the model to abstain on less than 10% of low-confidence patient predictions yields an accuracy of 84.5% on the remaining patients. These findings demonstrate that combining engineered features with lightweight architectures can support accurate, rapid, and generalizable detection of type 2 diabetes in real-world wearable settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Progression Modeling (JPM): A Probabilistic Framework for Mixed-Pathology Progression</title>
<link>https://arxiv.org/abs/2512.03475</link>
<guid>https://arxiv.org/abs/2512.03475</guid>
<content:encoded><![CDATA[
arXiv:2512.03475v1 Announce Type: new 
Abstract: Event-based models (EBMs) infer disease progression from cross-sectional data, and standard EBMs assume a single underlying disease per individual. In contrast, mixed pathologies are common in neurodegeneration. We introduce the Joint Progression Model (JPM), a probabilistic framework that treats single-disease trajectories as partial rankings and builds a prior over joint progressions. We study several JPM variants (Pairwise, Bradley-Terry, Plackett-Luce, and Mallows) and analyze three properties: (i) calibration -- whether lower model energy predicts smaller distance to the ground truth ordering; (ii) separation -- the degree to which sampled rankings are distinguishable from random permutations; and (iii) sharpness -- the stability of sampled aggregate rankings. All variants are calibrated, and all achieve near-perfect separation; sharpness varies by variant and is well-predicted by simple features of the input partial rankings (number and length of rankings, conflict, and overlap). In synthetic experiments, JPM improves ordering accuracy by roughly 21 percent over a strong EBM baseline (SA-EBM) that treats the joint disease as a single condition. Finally, using NACC, we find that the Mallows variant of JPM and the baseline model (SA-EBM) have results that are more consistent with prior literature on the possible disease progression of the mixed pathology of AD and VaD.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ATHENA: Agentic Team for Hierarchical Evolutionary Numerical Algorithms</title>
<link>https://arxiv.org/abs/2512.03476</link>
<guid>https://arxiv.org/abs/2512.03476</guid>
<content:encoded><![CDATA[
arXiv:2512.03476v1 Announce Type: new 
Abstract: Bridging the gap between theoretical conceptualization and computational implementation is a major bottleneck in Scientific Computing (SciC) and Scientific Machine Learning (SciML). We introduce ATHENA (Agentic Team for Hierarchical Evolutionary Numerical Algorithms), an agentic framework designed as an Autonomous Lab to manage the end-to-end computational research lifecycle. Its core is the HENA loop, a knowledge-driven diagnostic process framed as a Contextual Bandit problem. Acting as an online learner, the system analyzes prior trials to select structural `actions' ($A_n$) from combinatorial spaces guided by expert blueprints (e.g., Universal Approximation, Physics-Informed constraints). These actions are translated into executable code ($S_n$) to generate scientific rewards ($R_n$). ATHENA transcends standard automation: in SciC, it autonomously identifies mathematical symmetries for exact analytical solutions or derives stable numerical solvers where foundation models fail. In SciML, it performs deep diagnosis to tackle ill-posed formulations and combines hybrid symbolic-numeric workflows (e.g., coupling PINNs with FEM) to resolve multiphysics problems. The framework achieves super-human performance, reaching validation errors of $10^{-14}$. Furthermore, collaborative ``human-in-the-loop" intervention allows the system to bridge stability gaps, improving results by an order of magnitude. This paradigm shift focuses from implementation mechanics to methodological innovation, accelerating scientific discovery.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modal Logical Neural Networks</title>
<link>https://arxiv.org/abs/2512.03491</link>
<guid>https://arxiv.org/abs/2512.03491</guid>
<content:encoded><![CDATA[
arXiv:2512.03491v1 Announce Type: new 
Abstract: We propose Modal Logical Neural Networks (MLNNs), a neurosymbolic framework that integrates deep learning with the formal semantics of modal logic, enabling reasoning about necessity and possibility. Drawing on Kripke semantics, we introduce specialized neurons for the modal operators $\Box$ and $\Diamond$ that operate over a set of possible worlds, enabling the framework to act as a differentiable ``logical guardrail.'' The architecture is highly flexible: the accessibility relation between worlds can either be fixed by the user to enforce known rules or, as an inductive feature, be parameterized by a neural network. This allows the model to optionally learn the relational structure of a logical system from data while simultaneously performing deductive reasoning within that structure.
  This versatile construction is designed for flexibility. The entire framework is differentiable from end to end, with learning driven by minimizing a logical contradiction loss. This not only makes the system resilient to inconsistent knowledge but also enables it to learn nonlinear relationships that can help define the logic of a problem space. We illustrate MLNNs on four case studies: grammatical guardrailing, axiomatic detection of the unknown, multi-agent epistemic trust, and detecting constructive deception in natural language negotiation. These experiments demonstrate how enforcing or learning accessibility can increase logical consistency and interpretability without changing the underlying task architecture.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Driven Learning Framework for Tomographic Tactile Sensing</title>
<link>https://arxiv.org/abs/2512.03512</link>
<guid>https://arxiv.org/abs/2512.03512</guid>
<content:encoded><![CDATA[
arXiv:2512.03512v1 Announce Type: new 
Abstract: Electrical impedance tomography (EIT) provides an attractive solution for large-area tactile sensing due to its minimal wiring and shape flexibility, but its nonlinear inverse problem often leads to severe artifacts and inaccurate contact reconstruction. This work presents PhyDNN, a physics-driven deep reconstruction framework that embeds the EIT forward model directly into the learning objective. By jointly minimizing the discrepancy between predicted and ground-truth conductivity maps and enforcing consistency with the forward PDE, PhyDNN reduces the black-box nature of deep networks and improves both physical plausibility and generalization. To enable efficient backpropagation, we design a differentiable forward-operator network that accurately approximates the nonlinear EIT response, allowing fast physics-guided training. Extensive simulations and real tactile experiments on a 16-electrode soft sensor show that PhyDNN consistently outperforms NOSER, TV, and standard DNNs in reconstructing contact shape, location, and pressure distribution. PhyDNN yields fewer artifacts, sharper boundaries, and higher metric scores, demonstrating its effectiveness for high-quality tomographic tactile sensing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive sampling using variational autoencoder and reinforcement learning</title>
<link>https://arxiv.org/abs/2512.03525</link>
<guid>https://arxiv.org/abs/2512.03525</guid>
<content:encoded><![CDATA[
arXiv:2512.03525v1 Announce Type: new 
Abstract: Compressed sensing enables sparse sampling but relies on generic bases and random measurements, limiting efficiency and reconstruction quality. Optimal sensor placement uses historcal data to design tailored sampling patterns, yet its fixed, linear bases cannot adapt to nonlinear or sample-specific variations. Generative model-based compressed sensing improves reconstruction using deep generative priors but still employs suboptimal random sampling. We propose an adaptive sparse sensing framework that couples a variational autoencoder prior with reinforcement learning to select measurements sequentially. Experiments show that this approach outperforms CS, OSP, and Generative model-based reconstruction from sparse measurements.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parameter-Efficient Augment Plugin for Class-Incremental Learning</title>
<link>https://arxiv.org/abs/2512.03537</link>
<guid>https://arxiv.org/abs/2512.03537</guid>
<content:encoded><![CDATA[
arXiv:2512.03537v1 Announce Type: new 
Abstract: Existing class-incremental learning (CIL) approaches based on replay or knowledge distillation are often constrained by forgetting or the stability-plasticity dilemma. Some expansion-based approaches could achieve higher accuracy. However, they always require significant parameter increases. In this paper, we propose a plugin extension paradigm termed the Deployment of extra LoRA Components (DLC) for non-pre-trained CIL scenarios.We treat the feature extractor trained through replay or distillation as a base model with rich knowledge. For each task, we use Low-Rank Adaptation (LoRA) to inject task-specific residuals into the base model's deep layers. During inference, representations with task-specific residuals are aggregated to produce classification predictions. To mitigate interference from non-target LoRA plugins, we introduce a lightweight weighting unit. This unit learns to assign importance scores to different LoRA-tuned representations. Like downloadable contents in software, our method serves as a plug-and-play enhancement that efficiently extends the base methods. Remarkably, on the large-scale ImageNet-100, with merely 4 % of the parameters of a standard ResNet-18, our DLC model achieves a significant 8 % improvement in accuracy, demonstrating exceptional efficiency. Moreover, it could surpass state-of-the-art methods under the fixed memory budget.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Irreversible Machine Unlearning for Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03564</link>
<guid>https://arxiv.org/abs/2512.03564</guid>
<content:encoded><![CDATA[
arXiv:2512.03564v1 Announce Type: new 
Abstract: Diffusion models are renowned for their state-of-the-art performance in generating synthetic images. However, concerns related to safety, privacy, and copyright highlight the need for machine unlearning, which can make diffusion models forget specific training data and prevent the generation of sensitive or unwanted content. Current machine unlearning methods for diffusion models are primarily designed for conditional diffusion models and focus on unlearning specific data classes or features. Among these methods, finetuning-based machine unlearning methods are recognized for their efficiency and effectiveness, which update the parameters of pre-trained diffusion models by minimizing carefully designed loss functions. However, in this paper, we propose a novel attack named Diffusion Model Relearning Attack (DiMRA), which can reverse the finetuning-based machine unlearning methods, posing a significant vulnerability of this kind of technique. Without prior knowledge of the unlearning elements, DiMRA optimizes the unlearned diffusion model on an auxiliary dataset to reverse the unlearning, enabling the model to regenerate previously unlearned elements. To mitigate this vulnerability, we propose a novel machine unlearning method for diffusion models, termed as Diffusion Model Unlearning by Memorization (DiMUM). Unlike traditional methods that focus on forgetting, DiMUM memorizes alternative data or features to replace targeted unlearning data or features in order to prevent generating such elements. In our experiments, we demonstrate the effectiveness of DiMRA in reversing state-of-the-art finetuning-based machine unlearning methods for diffusion models, highlighting the need for more robust solutions. We extensively evaluate DiMUM, demonstrating its superior ability to preserve the generative performance of diffusion models while enhancing robustness against DiMRA.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When, How Long and How Much? Interpretable Neural Networks for Time Series Regression by Learning to Mask and Aggregate</title>
<link>https://arxiv.org/abs/2512.03578</link>
<guid>https://arxiv.org/abs/2512.03578</guid>
<content:encoded><![CDATA[
arXiv:2512.03578v1 Announce Type: new 
Abstract: Time series extrinsic regression (TSER) refers to the task of predicting a continuous target variable from an input time series. It appears in many domains, including healthcare, finance, environmental monitoring, and engineering. In these settings, accurate predictions and trustworthy reasoning are both essential. Although state-of-the-art TSER models achieve strong predictive performance, they typically operate as black boxes, making it difficult to understand which temporal patterns drive their decisions. Post-hoc interpretability techniques, such as feature attribution, aim to to explain how the model arrives at its predictions, but often produce coarse, noisy, or unstable explanations. Recently, inherently interpretable approaches based on concepts, additive decompositions, or symbolic regression, have emerged as promising alternatives. However, these approaches remain limited: they require explicit supervision on the concepts themselves, often cannot capture interactions between time-series features, lack expressiveness for complex temporal patterns, and struggle to scale to high-dimensional multivariate data.
  To address these limitations, we propose MAGNETS (Mask-and-AGgregate NEtwork for Time Series), an inherently interpretable neural architecture for TSER. MAGNETS learns a compact set of human-understandable concepts without requiring any annotations. Each concept corresponds to a learned, mask-based aggregation over selected input features, explicitly revealing both which features drive predictions and when they matter in the sequence. Predictions are formed as combinations of these learned concepts through a transparent, additive structure, enabling clear insight into the model's decision process.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optimal Transportation and Alignment Between Gaussian Measures</title>
<link>https://arxiv.org/abs/2512.03579</link>
<guid>https://arxiv.org/abs/2512.03579</guid>
<content:encoded><![CDATA[
arXiv:2512.03579v1 Announce Type: new 
Abstract: Optimal transport (OT) and Gromov-Wasserstein (GW) alignment provide interpretable geometric frameworks for comparing, transforming, and aggregating heterogeneous datasets -- tasks ubiquitous in data science and machine learning. Because these frameworks are computationally expensive, large-scale applications often rely on closed-form solutions for Gaussian distributions under quadratic cost. This work provides a comprehensive treatment of Gaussian, quadratic cost OT and inner product GW (IGW) alignment, closing several gaps in the literature to broaden applicability. First, we treat the open problem of IGW alignment between uncentered Gaussians on separable Hilbert spaces by giving a closed-form expression up to a quadratic optimization over unitary operators, for which we derive tight analytic upper and lower bounds. If at least one Gaussian measure is centered, the solution reduces to a fully closed-form expression, which we further extend to an analytic solution for the IGW barycenter between centered Gaussians. We also present a reduction of Gaussian multimarginal OT with pairwise quadratic costs to a tractable optimization problem and provide an efficient algorithm to solve it using a rank-deficiency constraint. To demonstrate utility, we apply our results to knowledge distillation and heterogeneous clustering on synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Federated Learning and Trajectory Compression for Enhanced AIS Coverage</title>
<link>https://arxiv.org/abs/2512.03584</link>
<guid>https://arxiv.org/abs/2512.03584</guid>
<content:encoded><![CDATA[
arXiv:2512.03584v1 Announce Type: new 
Abstract: This paper presents the VesselEdge system, which leverages federated learning and bandwidth-constrained trajectory compression to enhance maritime situational awareness by extending AIS coverage. VesselEdge transforms vessels into mobile sensors, enabling real-time anomaly detection and efficient data transmission over low-bandwidth connections. The system integrates the M3fed model for federated learning and the BWC-DR-A algorithm for trajectory compression, prioritizing anomalous data. Preliminary results demonstrate the effectiveness of VesselEdge in improving AIS coverage and situational awareness using historical data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observation-driven correction of numerical weather prediction for marine winds</title>
<link>https://arxiv.org/abs/2512.03606</link>
<guid>https://arxiv.org/abs/2512.03606</guid>
<content:encoded><![CDATA[
arXiv:2512.03606v1 Announce Type: new 
Abstract: Accurate marine wind forecasts are essential for safe navigation, ship routing, and energy operations, yet they remain challenging because observations over the ocean are sparse, heterogeneous, and temporally variable. We reformulate wind forecasting as observation-informed correction of a global numerical weather prediction (NWP) model. Rather than forecasting winds directly, we learn local correction patterns by assimilating the latest in-situ observations to adjust the Global Forecast System (GFS) output. We propose a transformer-based deep learning architecture that (i) handles irregular and time-varying observation sets through masking and set-based attention mechanisms, (ii) conditions predictions on recent observation-forecast pairs via cross-attention, and (iii) employs cyclical time embeddings and coordinate-aware location representations to enable single-pass inference at arbitrary spatial coordinates. We evaluate our model over the Atlantic Ocean using observations from the International Comprehensive Ocean-Atmosphere Data Set (ICOADS) as reference. The model reduces GFS 10-meter wind RMSE at all lead times up to 48 hours, achieving 45% improvement at 1-hour lead time and 13% improvement at 48-hour lead time. Spatial analyses reveal the most persistent improvements along coastlines and shipping routes, where observations are most abundant. The tokenized architecture naturally accommodates heterogeneous observing platforms (ships, buoys, tide gauges, and coastal stations) and produces both site-specific predictions and basin-scale gridded products in a single forward pass. These results demonstrate a practical, low-latency post-processing approach that complements NWP by learning to correct systematic forecast errors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoGraM: Context-sensitive granular optimization method with rollback for robust model fusion</title>
<link>https://arxiv.org/abs/2512.03610</link>
<guid>https://arxiv.org/abs/2512.03610</guid>
<content:encoded><![CDATA[
arXiv:2512.03610v1 Announce Type: new 
Abstract: Merging neural networks without retraining is central to federated and distributed learning. Common methods such as weight averaging or Fisher merging often lose accuracy and are unstable across seeds. CoGraM (Contextual Granular Merging) is a multi-stage, context-sensitive, loss-based, and iterative optimization method across layers, neurons, and weight levels that aligns decisions with loss differences and thresholds and prevents harmful updates through rollback. CoGraM is an optimization method that addresses the weaknesses of methods such as Fisher and can significantly improve the merged network.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The promising potential of vision language models for the generation of textual weather forecasts</title>
<link>https://arxiv.org/abs/2512.03623</link>
<guid>https://arxiv.org/abs/2512.03623</guid>
<content:encoded><![CDATA[
arXiv:2512.03623v1 Announce Type: new 
Abstract: Despite the promising capability of multimodal foundation models, their application to the generation of meteorological products and services remains nascent. To accelerate aspiration and adoption, we explore the novel use of a vision language model for writing the iconic Shipping Forecast text directly from video-encoded gridded weather data. These early results demonstrate promising scalable technological opportunities for enhancing production efficiency and service innovation within the weather enterprise and beyond.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conditional updates of neural network weights for increased out of training performance</title>
<link>https://arxiv.org/abs/2512.03653</link>
<guid>https://arxiv.org/abs/2512.03653</guid>
<content:encoded><![CDATA[
arXiv:2512.03653v1 Announce Type: new 
Abstract: This study proposes a method to enhance neural network performance when training data and application data are not very similar, e.g., out of distribution problems, as well as pattern and regime shifts. The method consists of three main steps: 1) Retrain the neural network towards reasonable subsets of the training data set and note down the resulting weight anomalies. 2) Choose reasonable predictors and derive a regression between the predictors and the weight anomalies. 3) Extrapolate the weights, and thereby the neural network, to the application data. We show and discuss this method in three use cases from the climate sciences, which include successful temporal, spatial and cross-domain extrapolations of neural networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cyclical Temporal Encoding and Hybrid Deep Ensembles for Multistep Energy Forecasting</title>
<link>https://arxiv.org/abs/2512.03656</link>
<guid>https://arxiv.org/abs/2512.03656</guid>
<content:encoded><![CDATA[
arXiv:2512.03656v1 Announce Type: new 
Abstract: Accurate electricity consumption forecasting is essential for demand management and smart grid operations. This paper introduces a unified deep learning framework that integrates cyclical temporal encoding with hybrid LSTM-CNN architectures to enhance multistep energy forecasting. We systematically transform calendar-based attributes using sine cosine encodings to preserve periodic structure and evaluate their predictive relevance through correlation analysis. To exploit both long-term seasonal effects and short-term local patterns, we employ an ensemble model composed of an LSTM, a CNN, and a meta-learner of MLP regressors specialized for each forecast horizon. Using a one year national consumption dataset, we conduct an extensive experimental study including ablation analyses with and without cyclical encodings and calendar features and comparisons with established baselines from the literature. Results demonstrate consistent improvements across all seven forecast horizons, with our hybrid model achieving lower RMSE and MAE than individual architectures and prior methods. These findings confirm the benefit of combining cyclical temporal representations with complementary deep learning structures. To our knowledge, this is the first work to jointly evaluate temporal encodings, calendar-based features, and hybrid ensemble architectures within a unified short-term energy forecasting framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamically Scaled Activation Steering</title>
<link>https://arxiv.org/abs/2512.03661</link>
<guid>https://arxiv.org/abs/2512.03661</guid>
<content:encoded><![CDATA[
arXiv:2512.03661v1 Announce Type: new 
Abstract: Activation steering has emerged as a powerful method for guiding the behavior of generative models towards desired outcomes such as toxicity mitigation. However, most existing methods apply interventions uniformly across all inputs, degrading model performance when steering is unnecessary. We introduce Dynamically Scaled Activation Steering (DSAS), a method-agnostic steering framework that decouples when to steer from how to steer. DSAS adaptively modulates the strength of existing steering transformations across layers and inputs, intervening strongly only when undesired behavior is detected. At generation time, DSAS computes context-dependent scaling factors that selectively adjust the strength of any steering method. We also show how DSAS can be jointly optimized end-to-end together with the steering function. When combined with existing steering methods, DSAS consistently improves the Pareto front with respect to steering alone, achieving a better trade-off between toxicity mitigation and utility preservation. We further demonstrate DSAS's generality by applying it to a text-to-image diffusion model, showing how adaptive steering allows the modulation of specific concepts. Finally, DSAS introduces minimal computational overhead while improving interpretability, pinpointing which tokens require steering and by how much.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Feature-aware Modulation for Learning from Temporal Tabular Data</title>
<link>https://arxiv.org/abs/2512.03678</link>
<guid>https://arxiv.org/abs/2512.03678</guid>
<content:encoded><![CDATA[
arXiv:2512.03678v1 Announce Type: new 
Abstract: While tabular machine learning has achieved remarkable success, temporal distribution shifts pose significant challenges in real-world deployment, as the relationships between features and labels continuously evolve. Static models assume fixed mappings to ensure generalization, whereas adaptive models may overfit to transient patterns, creating a dilemma between robustness and adaptability. In this paper, we analyze key factors essential for constructing an effective dynamic mapping for temporal tabular data. We discover that evolving feature semantics-particularly objective and subjective meanings-introduce concept drift over time. Crucially, we identify that feature transformation strategies are able to mitigate discrepancies in feature representations across temporal stages. Motivated by these insights, we propose a feature-aware temporal modulation mechanism that conditions feature representations on temporal context, modulating statistical properties such as scale and skewness. By aligning feature semantics across time, our approach achieves a lightweight yet powerful adaptation, effectively balancing generalizability and adaptability. Benchmark evaluations validate the effectiveness of our method in handling temporal shifts in tabular data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Topological Graph Neural Networks for Detecting Complex Fraud Patterns</title>
<link>https://arxiv.org/abs/2512.03696</link>
<guid>https://arxiv.org/abs/2512.03696</guid>
<content:encoded><![CDATA[
arXiv:2512.03696v1 Announce Type: new 
Abstract: We propose a novel QTGNN framework for detecting fraudulent transactions in large-scale financial networks. By integrating quantum embedding, variational graph convolutions, and topological data analysis, QTGNN captures complex transaction dynamics and structural anomalies indicative of fraud. The methodology includes quantum data embedding with entanglement enhancement, variational quantum graph convolutions with non-linear dynamics, extraction of higher-order topological invariants, hybrid quantum-classical anomaly learning with adaptive optimization, and interpretable decision-making via topological attribution. Rigorous convergence guarantees ensure stable training on noisy intermediate-scale quantum (NISQ) devices, while stability of topological signatures provides robust fraud detection. Optimized for NISQ hardware with circuit simplifications and graph sampling, the framework scales to large transaction networks. Simulations on financial datasets, such as PaySim and Elliptic, benchmark QTGNN against classical and quantum baselines, using metrics like ROC-AUC, precision, and false positive rate. An ablation study evaluates the contributions of quantum embeddings, topological features, non-linear channels, and hybrid learning. QTGNN offers a theoretically sound, interpretable, and practical solution for financial fraud detection, bridging quantum machine learning, graph theory, and topological analysis.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Invisible Urban Traffic Dynamics under Extreme Weather: A New Physics-Constrained Hamiltonian Learning Algorithm</title>
<link>https://arxiv.org/abs/2512.03744</link>
<guid>https://arxiv.org/abs/2512.03744</guid>
<content:encoded><![CDATA[
arXiv:2512.03744v1 Announce Type: new 
Abstract: Urban transportation systems face increasing resilience challenges from extreme weather events, but current assessment methods rely on surface-level recovery indicators that miss hidden structural damage. Existing approaches cannot distinguish between true recovery and "false recovery," where traffic metrics normalize, but the underlying system dynamics permanently degrade. To address this, a new physics-constrained Hamiltonian learning algorithm combining "structural irreversibility detection" and "energy landscape reconstruction" has been developed. Our approach extracts low-dimensional state representations, identifies quasi-Hamiltonian structures through physics-constrained optimization, and quantifies structural changes via energy landscape comparison. Analysis of London's extreme rainfall in 2021 demonstrates that while surface indicators were fully recovered, our algorithm detected 64.8\% structural damage missed by traditional monitoring. Our framework provides tools for proactive structural risk assessment, enabling infrastructure investments based on true system health rather than misleading surface metrics.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universally Converging Representations of Matter Across Scientific Foundation Models</title>
<link>https://arxiv.org/abs/2512.03750</link>
<guid>https://arxiv.org/abs/2512.03750</guid>
<content:encoded><![CDATA[
arXiv:2512.03750v1 Announce Type: new 
Abstract: Machine learning models of vastly different modalities and architectures are being trained to predict the behavior of molecules, materials, and proteins. However, it remains unclear whether they learn similar internal representations of matter. Understanding their latent structure is essential for building scientific foundation models that generalize reliably beyond their training domains. Although representational convergence has been observed in language and vision, its counterpart in the sciences has not been systematically explored. Here, we show that representations learned by nearly sixty scientific models, spanning string-, graph-, 3D atomistic, and protein-based modalities, are highly aligned across a wide range of chemical systems. Models trained on different datasets have highly similar representations of small molecules, and machine learning interatomic potentials converge in representation space as they improve in performance, suggesting that foundation models learn a common underlying representation of physical reality. We then show two distinct regimes of scientific models: on inputs similar to those seen during training, high-performing models align closely and weak models diverge into local sub-optima in representation space; on vastly different structures from those seen during training, nearly all models collapse onto a low-information representation, indicating that today's models remain limited by training data and inductive bias and do not yet encode truly universal structure. Our findings establish representational alignment as a quantitative benchmark for foundation-level generality in scientific models. More broadly, our work can track the emergence of universal representations of matter as models scale, and for selecting and distilling models whose learned representations transfer best across modalities, domains of matter, and scientific tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Origin-Conditional Trajectory Encoding: Measuring Urban Configurational Asymmetries through Neural Decomposition</title>
<link>https://arxiv.org/abs/2512.03755</link>
<guid>https://arxiv.org/abs/2512.03755</guid>
<content:encoded><![CDATA[
arXiv:2512.03755v1 Announce Type: new 
Abstract: Urban analytics increasingly relies on AI-driven trajectory analysis, yet current approaches suffer from methodological fragmentation: trajectory learning captures movement patterns but ignores spatial context, while spatial embedding methods encode street networks but miss temporal dynamics. Three gaps persist: (1) lack of joint training that integrates spatial and temporal representations, (2) origin-agnostic treatment that ignores directional asymmetries in navigation ($A \to B \ne B \to A$), and (3) over-reliance on auxiliary data (POIs, imagery) rather than fundamental geometric properties of urban space. We introduce a conditional trajectory encoder that jointly learns spatial and movement representations while preserving origin-dependent asymmetries using geometric features. This framework decomposes urban navigation into shared cognitive patterns and origin-specific spatial narratives, enabling quantitative measurement of cognitive asymmetries across starting locations. Our bidirectional LSTM processes visibility ratio and curvature features conditioned on learnable origin embeddings, decomposing representations into shared urban patterns and origin-specific signatures through contrastive learning. Results from six synthetic cities and real-world validation on Beijing's Xicheng District demonstrate that urban morphology creates systematic cognitive inequalities. This provides urban planners quantitative tools for assessing experiential equity, offers architects insights into layout decisions' cognitive impacts, and enables origin-aware analytics for navigation systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Unfolding: Recent Developments, Theory, and Design Guidelines</title>
<link>https://arxiv.org/abs/2512.03768</link>
<guid>https://arxiv.org/abs/2512.03768</guid>
<content:encoded><![CDATA[
arXiv:2512.03768v1 Announce Type: new 
Abstract: Optimization methods play a central role in signal processing, serving as the mathematical foundation for inference, estimation, and control. While classical iterative optimization algorithms provide interpretability and theoretical guarantees, they often rely on surrogate objectives, require careful hyperparameter tuning, and exhibit substantial computational latency. Conversely, machine learning (ML ) offers powerful data-driven modeling capabilities but lacks the structure, transparency, and efficiency needed for optimization-driven inference. Deep unfolding has recently emerged as a compelling framework that bridges these two paradigms by systematically transforming iterative optimization algorithms into structured, trainable ML architectures. This article provides a tutorial-style overview of deep unfolding, presenting a unified perspective of methodologies for converting optimization solvers into ML models and highlighting their conceptual, theoretical, and practical implications. We review the foundations of optimization for inference and for learning, introduce four representative design paradigms for deep unfolding, and discuss the distinctive training schemes that arise from their iterative nature. Furthermore, we survey recent theoretical advances that establish convergence and generalization guarantees for unfolded optimizers, and provide comparative qualitative and empirical studies illustrating their relative trade-offs in complexity, interpretability, and robustness.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forensic Activity Classification Using Digital Traces from iPhones: A Machine Learning-based Approach</title>
<link>https://arxiv.org/abs/2512.03786</link>
<guid>https://arxiv.org/abs/2512.03786</guid>
<content:encoded><![CDATA[
arXiv:2512.03786v1 Announce Type: new 
Abstract: Smartphones and smartwatches are ever-present in daily life, and provide a rich source of information on their users' behaviour. In particular, digital traces derived from the phone's embedded movement sensors present an opportunity for a forensic investigator to gain insight into a person's physical activities. In this work, we present a machine learning-based approach to translate digital traces into likelihood ratios (LRs) for different types of physical activities. Evaluating on a new dataset, NFI\_FARED, which contains digital traces from four different types of iPhones labelled with 19 activities, it was found that our approach could produce useful LR systems to distinguish 167 out of a possible 171 activity pairings. The same approach was extended to analyse likelihoods for multiple activities (or groups of activities) simultaneously and create activity timelines to aid in both the early and latter stages of forensic investigations. The dataset and all code required to replicate the results have also been made public to encourage further research on this topic.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Identification and Modeling of Clinical Pathways with Process Mining</title>
<link>https://arxiv.org/abs/2512.03787</link>
<guid>https://arxiv.org/abs/2512.03787</guid>
<content:encoded><![CDATA[
arXiv:2512.03787v1 Announce Type: new 
Abstract: Clinical pathways are specialized healthcare plans that model patient treatment procedures. They are developed to provide criteria-based progression and standardize patient treatment, thereby improving care, reducing resource use, and accelerating patient recovery. However, manual modeling of these pathways based on clinical guidelines and domain expertise is difficult and may not reflect the actual best practices for different variations or combinations of diseases. We propose a two-phase modeling method using process mining, which extends the knowledge base of clinical pathways by leveraging conformance checking diagnostics. In the first phase, historical data of a given disease is collected to capture treatment in the form of a process model. In the second phase, new data is compared against the reference model to verify conformance. Based on the conformance checking results, the knowledge base can be expanded with more specific models tailored to new variants or disease combinations. We demonstrate our approach using Synthea, a benchmark dataset simulating patient treatments for SARS-CoV-2 infections with varying COVID-19 complications. The results show that our method enables expanding the knowledge base of clinical pathways with sufficient precision, peaking to 95.62% AUC while maintaining an arc-degree simplicity of 67.11%.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EfficientECG: Cross-Attention with Feature Fusion for Efficient Electrocardiogram Classification</title>
<link>https://arxiv.org/abs/2512.03804</link>
<guid>https://arxiv.org/abs/2512.03804</guid>
<content:encoded><![CDATA[
arXiv:2512.03804v1 Announce Type: new 
Abstract: Electrocardiogram is a useful diagnostic signal that can detect cardiac abnormalities by measuring the electrical activity generated by the heart. Due to its rapid, non-invasive, and richly informative characteristics, ECG has many emerging applications. In this paper, we study novel deep learning technologies to effectively manage and analyse ECG data, with the aim of building a diagnostic model, accurately and quickly, that can substantially reduce the burden on medical workers. Unlike the existing ECG models that exhibit a high misdiagnosis rate, our deep learning approaches can automatically extract the features of ECG data through end-to-end training. Specifically, we first devise EfficientECG, an accurate and lightweight classification model for ECG analysis based on the existing EfficientNet model, which can effectively handle high-frequency long-sequence ECG data with various leading types. On top of that, we next propose a cross-attention-based feature fusion model of EfficientECG for analysing multi-lead ECG data with multiple features (e.g., gender and age). Our evaluations on representative ECG datasets validate the superiority of our model against state-of-the-art works in terms of high precision, multi-feature fusion, and lightweights.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning for Dynamic Algorithm Configuration: A Case Study on Optimizing OneMax with the (1+($\lambda$,$\lambda$))-GA</title>
<link>https://arxiv.org/abs/2512.03805</link>
<guid>https://arxiv.org/abs/2512.03805</guid>
<content:encoded><![CDATA[
arXiv:2512.03805v1 Announce Type: new 
Abstract: Dynamic Algorithm Configuration (DAC) studies the efficient identification of control policies for parameterized optimization algorithms. Numerous studies have leveraged the robustness of decision-making in Reinforcement Learning (RL) to address the optimization challenges in algorithm configuration. However, applying RL to DAC is challenging and often requires extensive domain expertise. We conduct a comprehensive study of deep-RL algorithms in DAC through a systematic analysis of controlling the population size parameter of the (1+($\lambda$,$\lambda$))-GA on OneMax instances. Our investigation of DDQN and PPO reveals two fundamental challenges that limit their effectiveness in DAC: scalability degradation and learning instability. We trace these issues to two primary causes: under-exploration and planning horizon coverage, each of which can be effectively addressed through targeted solutions. To address under-exploration, we introduce an adaptive reward shifting mechanism that leverages reward distribution statistics to enhance DDQN agent exploration, eliminating the need for instance-specific hyperparameter tuning and ensuring consistent effectiveness across different problem scales. In dealing with the planning horizon coverage problem, we demonstrate that undiscounted learning effectively resolves it in DDQN, while PPO faces fundamental variance issues that necessitate alternative algorithmic designs. We further analyze the hyperparameter dependencies of PPO, showing that while hyperparameter optimization enhances learning stability, it consistently falls short in identifying effective policies across various configurations. Finally, we demonstrate that DDQN equipped with our adaptive reward shifting strategy achieves performance comparable to theoretically derived policies with vastly improved sample efficiency, outperforming prior DAC approaches by several orders of magnitude.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Log Probability Tracking of LLM APIs</title>
<link>https://arxiv.org/abs/2512.03816</link>
<guid>https://arxiv.org/abs/2512.03816</guid>
<content:encoded><![CDATA[
arXiv:2512.03816v1 Announce Type: new 
Abstract: When using an LLM through an API provider, users expect the served model to remain consistent over time, a property crucial for the reliability of downstream applications and the reproducibility of research. Existing audit methods are too costly to apply at regular time intervals to the wide range of available LLM APIs. This means that model updates are left largely unmonitored in practice. In this work, we show that while LLM log probabilities (logprobs) are usually non-deterministic, they can still be used as the basis for cost-effective continuous monitoring of LLM APIs. We apply a simple statistical test based on the average value of each token logprob, requesting only a single token of output. This is enough to detect changes as small as one step of fine-tuning, making this approach more sensitive than existing methods while being 1,000x cheaper. We introduce the TinyChange benchmark as a way to measure the sensitivity of audit methods in the context of small, realistic model changes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transmit Weights, Not Features: Orthogonal-Basis Aided Wireless Point-Cloud Transmission</title>
<link>https://arxiv.org/abs/2512.03819</link>
<guid>https://arxiv.org/abs/2512.03819</guid>
<content:encoded><![CDATA[
arXiv:2512.03819v1 Announce Type: new 
Abstract: The widespread adoption of depth sensors has substantially lowered the barrier to point-cloud acquisition. This letter proposes a semantic wireless transmission framework for three dimension (3D) point clouds built on Deep Joint Source - Channel Coding (DeepJSCC). Instead of sending raw features, the transmitter predicts combination weights over a receiver-side semantic orthogonal feature pool, enabling compact representations and robust reconstruction. A folding-based decoder deforms a 2D grid into 3D, enforcing manifold continuity while preserving geometric fidelity. Trained with Chamfer Distance (CD) and an orthogonality regularizer, the system is evaluated on ModelNet40 across varying Signal-to-Noise Ratios (SNRs) and bandwidths. Results show performance on par with SEmantic Point cloud Transmission (SEPT) at high bandwidth and clear gains in bandwidth-constrained regimes, with consistent improvements in both Peak Signal-to-Noise Ratio (PSNR) and CD. Ablation experiments confirm the benefits of orthogonalization and the folding prior.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DVPO: Distributional Value Modeling-based Policy Optimization for LLM Post-Training</title>
<link>https://arxiv.org/abs/2512.03847</link>
<guid>https://arxiv.org/abs/2512.03847</guid>
<content:encoded><![CDATA[
arXiv:2512.03847v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has shown strong performance in LLM post-training, but real-world deployment often involves noisy or incomplete supervision. In such settings, complex and unreliable supervision signals can destabilize training and harm generalization. While existing approaches such as worst-case optimization (e.g., RFQI, CQL) and mean-based methods (e.g., PPO, GRPO) can improve stability, they often overlook generalization and may produce overly conservative policies, leading to uneven performance across diverse real scenarios. To this end, we introduce DVPO (Distributional Value Modeling with Risk-aware Policy Optimization), a new RL framework that combines conditional risk theory with distributional value modeling to better balance robustness and generalization. DVPO learns token-level value distributions to provide fine-grained supervision, and applies an asymmetric risk regularization to shape the distribution tails: it contracts the lower tail to dampen noisy negative deviations, while expanding the upper tail to preserve exploratory diversity. Across extensive experiments and analysis in multi-turn dialogue, math reasoning, and scientific QA, DVPO consistently outperforms PPO, GRPO, and robust Bellman-based PPO under noisy supervision, showing its potential for LLM post-training in the real-world.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable Decision Focused Learning via Online Trainable Surrogates</title>
<link>https://arxiv.org/abs/2512.03861</link>
<guid>https://arxiv.org/abs/2512.03861</guid>
<content:encoded><![CDATA[
arXiv:2512.03861v1 Announce Type: new 
Abstract: Decision support systems often rely on solving complex optimization problems that may require to estimate uncertain parameters beforehand. Recent studies have shown how using traditionally trained estimators for this task can lead to suboptimal solutions. Using the actual decision cost as a loss function (called Decision Focused Learning) can address this issue, but with a severe loss of scalability at training time. To address this issue, we propose an acceleration method based on replacing costly loss function evaluations with an efficient surrogate. Unlike previously defined surrogates, our approach relies on unbiased estimators reducing the risk of spurious local optima and can provide information on its local confidence allowing one to switch to a fallback method when needed. Furthermore, the surrogate is designed for a black-box setting, which enables compensating for simplifications in the optimization model and account- ing for recourse actions during cost computation. In our results, the method reduces costly inner solver calls, with a solution quality comparable to other state-of-the-art techniques.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperdimensional Computing for Sustainable Manufacturing: An Initial Assessment</title>
<link>https://arxiv.org/abs/2512.03864</link>
<guid>https://arxiv.org/abs/2512.03864</guid>
<content:encoded><![CDATA[
arXiv:2512.03864v1 Announce Type: new 
Abstract: Smart manufacturing can significantly improve efficiency and reduce energy consumption, yet the energy demands of AI models may offset these gains. This study utilizes in-situ sensing-based prediction of geometric quality in smart machining to compare the energy consumption, accuracy, and speed of common AI models. HyperDimensional Computing (HDC) is introduced as an alternative, achieving accuracy comparable to conventional models while drastically reducing energy consumption, 200$\times$ for training and 175 to 1000$\times$ for inference. Furthermore, HDC reduces training times by 200$\times$ and inference times by 300 to 600$\times$, showcasing its potential for energy-efficient smart manufacturing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Automatic Attack Discovery for Few-Shot Class-Incremental Learning via Large Language Models</title>
<link>https://arxiv.org/abs/2512.03882</link>
<guid>https://arxiv.org/abs/2512.03882</guid>
<content:encoded><![CDATA[
arXiv:2512.03882v1 Announce Type: new 
Abstract: Few-shot class incremental learning (FSCIL) is a more realistic and challenging paradigm in continual learning to incrementally learn unseen classes and overcome catastrophic forgetting on base classes with only a few training examples. Previous efforts have primarily centered around studying more effective FSCIL approaches. By contrast, less attention was devoted to thinking the security issues in contributing to FSCIL. This paper aims to provide a holistic study of the impact of attacks on FSCIL. We first derive insights by systematically exploring how human expert-designed attack methods (i.e., PGD, FGSM) affect FSCIL. We find that those methods either fail to attack base classes, or suffer from huge labor costs due to relying on huge expert knowledge. This highlights the need to craft a specialized attack method for FSCIL. Grounded in these insights, in this paper, we propose a simple yet effective ACraft method to automatically steer and discover optimal attack methods targeted at FSCIL by leveraging Large Language Models (LLMs) without human experts. Moreover, to improve the reasoning between LLMs and FSCIL, we introduce a novel Proximal Policy Optimization (PPO) based reinforcement learning to optimize learning, making LLMs generate better attack methods in the next generation by establishing positive feedback. Experiments on mainstream benchmarks show that our ACraft significantly degrades the performance of state-of-the-art FSCIL methods and dramatically beyond human expert-designed attack methods while maintaining the lowest costs of attack.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Probabilistic Foundations of Fuzzy Simplicial Sets for Nonlinear Dimensionality Reduction</title>
<link>https://arxiv.org/abs/2512.03899</link>
<guid>https://arxiv.org/abs/2512.03899</guid>
<content:encoded><![CDATA[
arXiv:2512.03899v1 Announce Type: new 
Abstract: Fuzzy simplicial sets have become an object of interest in dimensionality reduction and manifold learning, most prominently through their role in UMAP. However, their definition through tools from algebraic topology without a clear probabilistic interpretation detaches them from commonly used theoretical frameworks in those areas. In this work we introduce a framework that explains fuzzy simplicial sets as marginals of probability measures on simplicial sets. In particular, this perspective shows that the fuzzy weights of UMAP arise from a generative model that samples Vietoris-Rips filtrations at random scales, yielding cumulative distribution functions of pairwise distances. More generally, the framework connects fuzzy simplicial sets to probabilistic models on the face poset, clarifies the relation between Kullback-Leibler divergence and fuzzy cross-entropy in this setting, and recovers standard t-norms and t-conorms via Boolean operations on the underlying simplicial sets. We then show how new embedding methods may be derived from this framework and illustrate this on an example where we generalize UMAP using \v{C}ech filtrations with triplet sampling. In summary, this probabilistic viewpoint provides a unified probabilistic theoretical foundation for fuzzy simplicial sets, clarifies the role of UMAP within this framework, and enables the systematic derivation of new dimensionality reduction methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum-Classical Physics-Informed Neural Networks for Solving Reservoir Seepage Equations</title>
<link>https://arxiv.org/abs/2512.03923</link>
<guid>https://arxiv.org/abs/2512.03923</guid>
<content:encoded><![CDATA[
arXiv:2512.03923v1 Announce Type: new 
Abstract: Solving partial differential equations (PDEs) for reservoir seepage is critical for optimizing oil and gas field development and predicting production performance. Traditional numerical methods suffer from mesh-dependent errors and high computational costs, while classical Physics-Informed Neural Networks (PINNs) face bottlenecks in parameter efficiency, high-dimensional expression, and strong nonlinear fitting. To address these limitations, we propose a Discrete Variable (DV)-Circuit Quantum-Classical Physics-Informed Neural Network (QCPINN) and apply it to three typical reservoir seepage models for the first time: the pressure diffusion equation for heterogeneous single-phase flow, the nonlinear Buckley-Leverett (BL) equation for two-phase waterflooding, and the convection-diffusion equation for compositional flow considering adsorption. The QCPINN integrates classical preprocessing/postprocessing networks with a DV quantum core, leveraging quantum superposition and entanglement to enhance high-dimensional feature mapping while embedding physical constraints to ensure solution consistency. We test three quantum circuit topologies (Cascade, Cross-mesh, Alternate) and demonstrate through numerical experiments that QCPINNs achieve high prediction accuracy with fewer parameters than classical PINNs. Specifically, the Alternate topology outperforms others in heterogeneous single-phase flow and two-phase BL equation simulations, while the Cascade topology excels in compositional flow with convection-dispersion-adsorption coupling. Our work verifies the feasibility of QCPINN for reservoir engineering applications, bridging the gap between quantum computing research and industrial practice in oil and gas engineering.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Density-Informed VAE (DiVAE): Reliable Log-Prior Probability via Density Alignment Regularization</title>
<link>https://arxiv.org/abs/2512.03928</link>
<guid>https://arxiv.org/abs/2512.03928</guid>
<content:encoded><![CDATA[
arXiv:2512.03928v1 Announce Type: new 
Abstract: We introduce Density-Informed VAE (DiVAE), a lightweight, data-driven regularizer that aligns the VAE log-prior probability $\log p_Z(z)$ with a log-density estimated from data. Standard VAEs match latents to a simple prior, overlooking density structure in the data-space. DiVAE encourages the encoder to allocate posterior mass in proportion to data-space density and, when the prior is learnable, nudges the prior toward high-density regions. This is realized by adding a robust, precision-weighted penalty to the ELBO, incurring negligible computational overhead. On synthetic datasets, DiVAE (i) improves distributional alignment of latent log-densities to its ground truth counterpart, (ii) improves prior coverage, and (iii) yields better OOD uncertainty calibration. On MNIST, DiVAE improves alignment of the prior with external estimates of the density, providing better interpretability, and improves OOD detection for learnable priors.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Technical Report on Text Dataset Distillation</title>
<link>https://arxiv.org/abs/2512.03967</link>
<guid>https://arxiv.org/abs/2512.03967</guid>
<content:encoded><![CDATA[
arXiv:2512.03967v1 Announce Type: new 
Abstract: In the vision domain, dataset distillation arises as a technique to condense a large dataset into a smaller synthetic one that exhibits a similar result in the training process. While image data presents an extensive literature of distillation methods, text dataset distillation has fewer works in comparison. Text dataset distillation initially grew as an adaptation of efforts from the vision universe, as the particularities of the modality became clear obstacles, it rose into a separate branch of research. Several milestones mark the development of this area, such as the introduction of methods that use transformer models, the generation of discrete synthetic text, and the scaling to decoder-only models with over 1B parameters. Despite major advances in modern approaches, the field remains in a maturing phase, with room for improvement on benchmarking standardization, approaches to overcome the discrete nature of text, handling complex tasks, and providing explicit examples of real-world applications. In this report, we review past and recent advances in dataset distillation for text, highlighting different distillation strategies, key contributions, and general challenges.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Flow Policy: Learning from High-Value Actions in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03973</link>
<guid>https://arxiv.org/abs/2512.03973</guid>
<content:encoded><![CDATA[
arXiv:2512.03973v1 Announce Type: new 
Abstract: Offline reinforcement learning often relies on behavior regularization that enforces policies to remain close to the dataset distribution. However, such approaches fail to distinguish between high-value and low-value actions in their regularization components. We introduce Guided Flow Policy (GFP), which couples a multi-step flow-matching policy with a distilled one-step actor. The actor directs the flow policy through weighted behavior cloning to focus on cloning high-value actions from the dataset rather than indiscriminately imitating all state-action pairs. In turn, the flow policy constrains the actor to remain aligned with the dataset's best transitions while maximizing the critic. This mutual guidance enables GFP to achieve state-of-the-art performance across 144 state and pixel-based tasks from the OGBench, Minari, and D4RL benchmarks, with substantial gains on suboptimal datasets and challenging tasks. Webpage: https://simple-robotics.github.io/publications/guided-flow-policy/
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training-Free Policy Violation Detection via Activation-Space Whitening in LLMs</title>
<link>https://arxiv.org/abs/2512.03994</link>
<guid>https://arxiv.org/abs/2512.03994</guid>
<content:encoded><![CDATA[
arXiv:2512.03994v1 Announce Type: new 
Abstract: Aligning proprietary large language models (LLMs) with internal organizational policies has become an urgent priority as organizations increasingly deploy LLMs in sensitive domains such as legal support, finance, and medical services. Beyond generic safety filters, enterprises require reliable mechanisms to detect policy violations within their regulatory and operational frameworks, where breaches can trigger legal and reputational risks. Existing content moderation frameworks, such as guardrails, remain largely confined to the safety domain and lack the robustness to capture nuanced organizational policies. LLM-as-a-judge and fine-tuning approaches, though flexible, introduce significant latency and lack interpretability. To address these limitations, we propose a training-free and efficient method that treats policy violation detection as an out-of-distribution (OOD) detection problem. Inspired by whitening techniques, we apply a linear transformation to decorrelate the model's hidden activations and standardize them to zero mean and unit variance, yielding near-identity covariance matrix. In this transformed space, we use the Euclidean norm as a compliance score to detect policy violations. The method requires only the policy text and a small number of illustrative samples, which makes it light-weight and easily deployable. On a challenging policy benchmark, our approach achieves state-of-the-art results, surpassing both existing guardrails and fine-tuned reasoning models. This work provides organizations with a practical and statistically grounded framework for policy-aware oversight of LLMs, advancing the broader goal of deployable AI governance. Code is available at: https://tinyurl.com/policy-violation-detection
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Physics-Embedded Gaussian Process for Traffic State Estimation</title>
<link>https://arxiv.org/abs/2512.04004</link>
<guid>https://arxiv.org/abs/2512.04004</guid>
<content:encoded><![CDATA[
arXiv:2512.04004v1 Announce Type: new 
Abstract: Traffic state estimation (TSE) becomes challenging when probe-vehicle penetration is low and observations are spatially sparse. Pure data-driven methods lack physical explanations and have poor generalization when observed data is sparse. In contrast, physical models have difficulty integrating uncertainties and capturing the real complexity of traffic. To bridge this gap, recent studies have explored combining them by embedding physical structure into Gaussian process. These approaches typically introduce the governing equations as soft constraints through pseudo-observations, enabling the integration of model structure within a variational framework. However, these methods rely heavily on penalty tuning and lack principled uncertainty calibration, which makes them sensitive to model mis-specification. In this work, we address these limitations by presenting a novel Physics-Embedded Gaussian Process (PEGP), designed to integrate domain knowledge with data-driven methods in traffic state estimation. Specifically, we design two multi-output kernels informed by classic traffic flow models, constructed via the explicit application of the linearized differential operator. Experiments on HighD, NGSIM show consistent improvements over non-physics baselines. PEGP-ARZ proves more reliable under sparse observation, while PEGP-LWR achieves lower errors with denser observation. Ablation study further reveals that PEGP-ARZ residuals align closely with physics and yield calibrated, interpretable uncertainty, whereas PEGP-LWR residuals are more orthogonal and produce nearly constant variance fields. This PEGP framework combines physical priors, uncertainty quantification, which can provide reliable support for TSE.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Diagonalizing the Softmax: Hadamard Initialization for Tractable Cross-Entropy Dynamics</title>
<link>https://arxiv.org/abs/2512.04006</link>
<guid>https://arxiv.org/abs/2512.04006</guid>
<content:encoded><![CDATA[
arXiv:2512.04006v1 Announce Type: new 
Abstract: Cross-entropy (CE) training loss dominates deep learning practice, yet existing theory often relies on simplifications, either replacing it with squared loss or restricting to convex models, that miss essential behavior. CE and squared loss generate fundamentally different dynamics, and convex linear models cannot capture the complexities of non-convex optimization. We provide an in-depth characterization of multi-class CE optimization dynamics beyond the convex regime by analyzing a canonical two-layer linear neural network with standard-basis vectors as inputs: the simplest non-convex extension for which the implicit bias remained unknown. This model coincides with the unconstrained features model used to study neural collapse, making our work the first to prove that gradient flow on CE converges to the neural collapse geometry. We construct an explicit Lyapunov function that establishes global convergence, despite the presence of spurious critical points in the non-convex landscape. A key insight underlying our analysis is an inconspicuous finding: Hadamard Initialization diagonalizes the softmax operator, freezing the singular vectors of the weight matrices and reducing the dynamics entirely to their singular values. This technique opens a pathway for analyzing CE training dynamics well beyond our specific setting considered here.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Public Verification of Private ML via Regularization</title>
<link>https://arxiv.org/abs/2512.04008</link>
<guid>https://arxiv.org/abs/2512.04008</guid>
<content:encoded><![CDATA[
arXiv:2512.04008v1 Announce Type: new 
Abstract: Training with differential privacy (DP) provides a guarantee to members in a dataset that they cannot be identified by users of the released model. However, those data providers, and, in general, the public, lack methods to efficiently verify that models trained on their data satisfy DP guarantees. The amount of compute needed to verify DP guarantees for current algorithms scales with the amount of compute required to train the model. In this paper we design the first DP algorithm with near optimal privacy-utility trade-offs but whose DP guarantees can be verified cheaper than training. We focus on DP stochastic convex optimization (DP-SCO), where optimal privacy-utility trade-offs are known. Here we show we can obtain tight privacy-utility trade-offs by privately minimizing a series of regularized objectives and only using the standard DP composition bound. Crucially, this method can be verified with much less compute than training. This leads to the first known DP-SCO algorithm with near optimal privacy-utility whose DP verification scales better than training cost, significantly reducing verification costs on large datasets.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Domain Feature Collapse: Implications for Out-of-Distribution Detection and Solutions</title>
<link>https://arxiv.org/abs/2512.04034</link>
<guid>https://arxiv.org/abs/2512.04034</guid>
<content:encoded><![CDATA[
arXiv:2512.04034v1 Announce Type: new 
Abstract: Why do state-of-the-art OOD detection methods exhibit catastrophic failure when models are trained on single-domain datasets? We provide the first theoretical explanation for this phenomenon through the lens of information theory. We prove that supervised learning on single-domain data inevitably produces domain feature collapse -- representations where I(x_d; z) = 0, meaning domain-specific information is completely discarded. This is a fundamental consequence of information bottleneck optimization: models trained on single domains (e.g., medical images) learn to rely solely on class-specific features while discarding domain features, leading to catastrophic failure when detecting out-of-domain samples (e.g., achieving only 53% FPR@95 on MNIST). We extend our analysis using Fano's inequality to quantify partial collapse in practical scenarios. To validate our theory, we introduce Domain Bench, a benchmark of single-domain datasets, and demonstrate that preserving I(x_d; z) > 0 through domain filtering (using pretrained representations) resolves the failure mode. While domain filtering itself is conceptually straightforward, its effectiveness provides strong empirical evidence for our information-theoretic framework. Our work explains a puzzling empirical phenomenon, reveals fundamental limitations of supervised learning in narrow domains, and has broader implications for transfer learning and when to fine-tune versus freeze pretrained models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MarkTune: Improving the Quality-Detectability Trade-off in Open-Weight LLM Watermarking</title>
<link>https://arxiv.org/abs/2512.04044</link>
<guid>https://arxiv.org/abs/2512.04044</guid>
<content:encoded><![CDATA[
arXiv:2512.04044v1 Announce Type: new 
Abstract: Watermarking aims to embed hidden signals in generated text that can be reliably detected when given access to a secret key. Open-weight language models pose acute challenges for such watermarking schemes because the inference-time interventions that dominate contemporary approaches cannot be enforced once model weights are public. Existing watermaking techniques for open-weight models, such as the recently proposed GaussMark, typically rely on small modifications to model weights, which can yield signals detectable to those equipped with a secret key, but achieving detection power comparable to inference-time watermarks generally requires weight perturbations that noticeably reduce generation quality. We introduce MarkTune, a theoretically principled, on-policy fine-tuning framework that treats the GaussMark signal as a reward while simultaneously regularizing against degradation in text quality. We derive MarkTune as an improvement on GaussMark and demonstrate that MarkTune consistently improves the quality-detectability trade-off over GaussMark by steering finer-grained, watermark-aware weight updates within the model's representation space while preserving generation quality. Empirically, we show that MarkTune pushes the quality-detectability frontier of GaussMark close to that of inference-time watermarking, remains robust to paraphrasing and fine-tuning attacks, and exhibits strong generalization: a model fine-tuned on one dataset retains substantial watermark detection power on unseen datasets. Together, these results establish MarkTune as a general strategy for embedding robust, high-quality watermarks into open-weight LMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence for Discrete Parameter Updates</title>
<link>https://arxiv.org/abs/2512.04051</link>
<guid>https://arxiv.org/abs/2512.04051</guid>
<content:encoded><![CDATA[
arXiv:2512.04051v1 Announce Type: new 
Abstract: Modern deep learning models require immense computational resources, motivating research into low-precision training. Quantised training addresses this by representing training components in low-bit integers, but typically relies on discretising real-valued updates. We introduce an alternative approach where the update rule itself is discrete, avoiding the quantisation of continuous updates by design. We establish convergence guarantees for a general class of such discrete schemes, and present a multinomial update rule as a concrete example, supported by empirical evaluation. This perspective opens new avenues for efficient training, particularly for models with inherently discrete structure.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Eval Factsheets: A Structured Framework for Documenting AI Evaluations</title>
<link>https://arxiv.org/abs/2512.04062</link>
<guid>https://arxiv.org/abs/2512.04062</guid>
<content:encoded><![CDATA[
arXiv:2512.04062v1 Announce Type: new 
Abstract: The rapid proliferation of benchmarks has created significant challenges in reproducibility, transparency, and informed decision-making. However, unlike datasets and models -- which benefit from structured documentation frameworks like Datasheets and Model Cards -- evaluation methodologies lack systematic documentation standards. We introduce Eval Factsheets, a structured, descriptive framework for documenting AI system evaluations through a comprehensive taxonomy and questionnaire-based approach. Our framework organizes evaluation characteristics across five fundamental dimensions: Context (Who made the evaluation and when?), Scope (What does it evaluate?), Structure (With what the evaluation is built?), Method (How does it work?) and Alignment (In what ways is it reliable/valid/robust?). We implement this taxonomy as a practical questionnaire spanning five sections with mandatory and recommended documentation elements. Through case studies on multiple benchmarks, we demonstrate that Eval Factsheets effectively captures diverse evaluation paradigms -- from traditional benchmarks to LLM-as-judge methodologies -- while maintaining consistency and comparability. We hope Eval Factsheets are incorporated into both existing and newly released evaluation frameworks and lead to more transparency and reproducibility.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fare Comparison App of Uber, Ola and Rapido</title>
<link>https://arxiv.org/abs/2512.04065</link>
<guid>https://arxiv.org/abs/2512.04065</guid>
<content:encoded><![CDATA[
arXiv:2512.04065v1 Announce Type: new 
Abstract: In todays increasing world, it is very important to have good hailing services like Ola, Uber, and Rapido as it is very essential for our daily transportation. Users often face difficulties in choosing the most appropriate and efficient ride that would lead to both cost-effective and would take us to our destination in less time. This project provides you with the web application that helps you to select the most beneficial ride for you by providing users with the fare comparison between Ola, Uber, Rapido for the destination entered by the user. The backend is use to fetch the data, providing users with the fare comparison for the ride and finally providing with the best option using Python. This research paper also addresses the problem and challenges faced in accessing the data using APIs, Android Studios emulator, Appium and location comparison. Thus, the aim of the project is to provide transparency to the users in ride-hailing services and increase efficiency and provide users with better experience.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Steerable Clarification Policies with Collaborative Self-play</title>
<link>https://arxiv.org/abs/2512.04068</link>
<guid>https://arxiv.org/abs/2512.04068</guid>
<content:encoded><![CDATA[
arXiv:2512.04068v1 Announce Type: new 
Abstract: To handle underspecified or ambiguous queries, AI assistants need a policy for managing their uncertainty to determine (a) when to guess the user intent and answer directly, (b) when to enumerate and answer multiple possible intents, and (c) when to ask a clarifying question. However, such policies are contextually dependent on factors such as user preferences or modality. For example, enumerating multiple possible user intentions is cumbersome on small screens or in a voice setting. In this work, we propose to train steerable policies for managing this uncertainty using self-play. Given two agents, one simulating a user and the other an AI assistant, we generate conversations where the user issues a potentially ambiguous query, and the assistant needs to determine how to respond. Importantly, the model takes as input the numerical cost of each clarification question, and each generated word, and is asked to take the action that will maximize its final reward, which is the cost-penalized accuracy. We use Reinforced Self-Training (ReST) to train our model to achieve high reward and show this leads to a steerable policy that changes its behavior predictably conditioned on the provided costs, leading to higher reward and accuracy. Moreover, our procedure also generalizes to numerical cost values that were unobserved at training time.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting</title>
<link>https://arxiv.org/abs/2506.23888</link>
<guid>https://arxiv.org/abs/2506.23888</guid>
<content:encoded><![CDATA[
arXiv:2506.23888v1 Announce Type: cross 
Abstract: Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Entropy-Based Measurement of Value Drift and Alignment Work in Large Language Models</title>
<link>https://arxiv.org/abs/2512.03047</link>
<guid>https://arxiv.org/abs/2512.03047</guid>
<content:encoded><![CDATA[
arXiv:2512.03047v1 Announce Type: cross 
Abstract: Large language model safety is usually assessed with static benchmarks, but key failures are dynamic: value drift under distribution shift, jailbreak attacks, and slow degradation of alignment in deployment. Building on a recent Second Law of Intelligence that treats ethical entropy as a state variable which tends to increase unless countered by alignment work, we make this framework operational for large language models. We define a five-way behavioral taxonomy, train a classifier to estimate ethical entropy S(t) from model transcripts, and measure entropy dynamics for base and instruction-tuned variants of four frontier models across stress tests. Base models show sustained entropy growth, while tuned variants suppress drift and reduce ethical entropy by roughly eighty percent. From these trajectories we estimate an effective alignment work rate gamma_eff and embed S(t) and gamma_eff in a monitoring pipeline that raises alerts when entropy drift exceeds a stability threshold, enabling run-time oversight of value drift.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploring Syntropic Frameworks in AI Alignment: A Philosophical Investigation</title>
<link>https://arxiv.org/abs/2512.03048</link>
<guid>https://arxiv.org/abs/2512.03048</guid>
<content:encoded><![CDATA[
arXiv:2512.03048v1 Announce Type: cross 
Abstract: I argue that AI alignment should be reconceived as architecting syntropic, reasons-responsive agents through process-based, multi-agent, developmental mechanisms rather than encoding fixed human value content. The paper makes three philosophical contributions. First, I articulate the ``specification trap'' argument demonstrating why content-based value specification appears structurally unstable due to the conjunction of the is-ought gap, value pluralism, and the extended frame problem. Second, I propose syntropy -- the recursive reduction of mutual uncertainty between agents through state alignment -- as an information-theoretic framework for understanding multi-agent alignment dynamics. Third, I establish a functional distinction between genuine and simulated moral capacity grounded in compatibilist theories of guidance control, coupled with an embodied experimental paradigm and verification regime providing operational criteria independent of phenomenological claims. This paper represents the philosophical component of a broader research program whose empirical validation is being developed in a separate project currently in preparation. While the framework generates specific, falsifiable predictions about value emergence and moral agency in artificial systems, empirical validation remains pending.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A note on the impossibility of conditional PAC-efficient reasoning in large language models</title>
<link>https://arxiv.org/abs/2512.03057</link>
<guid>https://arxiv.org/abs/2512.03057</guid>
<content:encoded><![CDATA[
arXiv:2512.03057v1 Announce Type: cross 
Abstract: We prove an impossibility result for conditional Probably Approximately Correct (PAC)-efficient reasoning in large language models. While recent work has established marginal PAC efficiency guarantees for composite models that switch between expensive expert models and cheaper fast models, we show that conditional (pointwise) guarantees are impossible in the distribution-free setting. Specifically, for non-atomic input spaces, any algorithm achieving conditional PAC efficiency must be trivial in the sense that it defers to the expert model with probability at least $1-\alpha$ for almost every input.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Watermarks for Embeddings-as-a-Service Large Language Models</title>
<link>https://arxiv.org/abs/2512.03079</link>
<guid>https://arxiv.org/abs/2512.03079</guid>
<content:encoded><![CDATA[
arXiv:2512.03079v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have demonstrated exceptional capabilities in natural language understanding and generation. Based on these LLMs, businesses have started to provide Embeddings-as-a-Service (EaaS), offering feature extraction capabilities (in the form of text embeddings) that benefit downstream natural language processing tasks. However, prior research has demonstrated that EaaS is vulnerable to imitation attacks, where an attacker clones the service's model in a black-box manner without access to the model's internal workings. In response, watermarks have been added to the text embeddings to protect the intellectual property of EaaS providers by allowing them to check for model ownership. This thesis focuses on defending against imitation attacks by investigating EaaS watermarks. To achieve this goal, we unveil novel attacks and propose and validate new watermarking techniques.
  Firstly, we show that existing EaaS watermarks can be removed through paraphrasing the input text when attackers clone the model during imitation attacks. Our study illustrates that paraphrasing can effectively bypass current state-of-the-art EaaS watermarks across various attack setups (including different paraphrasing techniques and models) and datasets in most instances. This demonstrates a new vulnerability in recent EaaS watermarking techniques.
  Subsequently, as a countermeasure, we propose a novel watermarking technique, WET (Watermarking EaaS with Linear Transformation), which employs linear transformation of the embeddings. Watermark verification is conducted by applying a reverse transformation and comparing the similarity between recovered and original embeddings. We demonstrate its robustness against paraphrasing attacks with near-perfect verifiability. We conduct detailed ablation studies to assess the significance of each component and hyperparameter in WET.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Calibrating Geophysical Predictions under Constrained Probabilistic Distributions</title>
<link>https://arxiv.org/abs/2512.03081</link>
<guid>https://arxiv.org/abs/2512.03081</guid>
<content:encoded><![CDATA[
arXiv:2512.03081v1 Announce Type: cross 
Abstract: Machine learning (ML) has shown significant promise in studying complex geophysical dynamical systems, including turbulence and climate processes. Such systems often display sensitive dependence on initial conditions, reflected in positive Lyapunov exponents, where even small perturbations in short-term forecasts can lead to large deviations in long-term outcomes. Thus, meaningful inference requires not only accurate short-term predictions, but also consistency with the system's long-term attractor that is captured by the marginal distribution of state variables. Existing approaches attempt to address this challenge by incorporating spatial and temporal dependence, but these strategies become impractical when data are extremely sparse. In this work, we show that prior knowledge of marginal distributions offers valuable complementary information to short-term observations, motivating a distribution-informed learning framework. We introduce a calibration algorithm based on normalization and the Kernelized Stein Discrepancy (KSD) to enhance ML predictions. The method here employs KSD within a reproducing kernel Hilbert space to calibrate model outputs, improving their fidelity to known physical distributions. This not only sharpens pointwise predictions but also enforces consistency with non-local statistical structures rooted in physical principles. Through synthetic experiments-spanning offline climatological CO2 fluxes and online quasi-geostrophic flow simulations-we demonstrate the robustness and broad utility of the proposed framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Password-Activated Shutdown Protocols for Misaligned Frontier Agents</title>
<link>https://arxiv.org/abs/2512.03089</link>
<guid>https://arxiv.org/abs/2512.03089</guid>
<content:encoded><![CDATA[
arXiv:2512.03089v1 Announce Type: cross 
Abstract: Frontier AI developers may fail to align or control highly-capable AI agents. In many cases, it could be useful to have emergency shutdown mechanisms which effectively prevent misaligned agents from carrying out harmful actions in the world. We introduce password-activated shutdown protocols (PAS protocols) -- methods for designing frontier agents to implement a safe shutdown protocol when given a password. We motivate PAS protocols by describing intuitive use-cases in which they mitigate risks from misaligned systems that subvert other control efforts, for instance, by disabling automated monitors or self-exfiltrating to external data centres. PAS protocols supplement other safety efforts, such as alignment fine-tuning or monitoring, contributing to defence-in-depth against AI risk. We provide a concrete demonstration in SHADE-Arena, a benchmark for AI monitoring and subversion capabilities, in which PAS protocols supplement monitoring to increase safety with little cost to performance. Next, PAS protocols should be robust to malicious actors who want to bypass shutdown. Therefore, we conduct a red-team blue-team game between the developers (blue-team), who must implement a robust PAS protocol, and a red-team trying to subvert the protocol. We conduct experiments in a code-generation setting, finding that there are effective strategies for the red-team, such as using another model to filter inputs, or fine-tuning the model to prevent shutdown behaviour. We then outline key challenges to implementing PAS protocols in real-life systems, including: security considerations of the password and decisions regarding when, and in which systems, to use them. PAS protocols are an intuitive mechanism for increasing the safety of frontier AI. We encourage developers to consider implementing PAS protocols prior to internal deployment of particularly dangerous systems to reduce loss-of-control risks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Performance Analysis of Quantum Support Vector Classifiers and Quantum Neural Networks</title>
<link>https://arxiv.org/abs/2512.03094</link>
<guid>https://arxiv.org/abs/2512.03094</guid>
<content:encoded><![CDATA[
arXiv:2512.03094v1 Announce Type: cross 
Abstract: This study explores the performance of Quantum Support Vector Classifiers (QSVCs) and Quantum Neural Networks (QNNs) in comparison to classical models for machine learning tasks. By evaluating these models on the Iris and MNIST-PCA datasets, we find that quantum models tend to outperform classical approaches as the problem complexity increases. While QSVCs generally provide more consistent results, QNNs exhibit superior performance in higher-complexity tasks due to their increased quantum load. Additionally, we analyze the impact of hyperparameter tuning, showing that feature maps and ansatz configurations significantly influence model accuracy. We also compare the PennyLane and Qiskit frameworks, concluding that Qiskit provides better optimization and efficiency for our implementation. These findings highlight the potential of Quantum Machine Learning (QML) for complex classification problems and provide insights into model selection and optimization strategies
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Many-to-One Adversarial Consensus: Exposing Multi-Agent Collusion Risks in AI-Based Healthcare</title>
<link>https://arxiv.org/abs/2512.03097</link>
<guid>https://arxiv.org/abs/2512.03097</guid>
<content:encoded><![CDATA[
arXiv:2512.03097v1 Announce Type: cross 
Abstract: The integration of large language models (LLMs) into healthcare IoT systems promises faster decisions and improved medical support. LLMs are also deployed as multi-agent teams to assist AI doctors by debating, voting, or advising on decisions. However, when multiple assistant agents interact, coordinated adversaries can collude to create false consensus, pushing an AI doctor toward harmful prescriptions. We develop an experimental framework with scripted and unscripted doctor agents, adversarial assistants, and a verifier agent that checks decisions against clinical guidelines. Using 50 representative clinical questions, we find that collusion drives the Attack Success Rate (ASR) and Harmful Recommendation Rates (HRR) up to 100% in unprotected systems. In contrast, the verifier agent restores 100% accuracy by blocking adversarial consensus. This work provides the first systematic evidence of collusion risk in AI healthcare and demonstrates a practical, lightweight defence that ensures guideline fidelity.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An AI Implementation Science Study to Improve Trustworthy Data in a Large Healthcare System</title>
<link>https://arxiv.org/abs/2512.03098</link>
<guid>https://arxiv.org/abs/2512.03098</guid>
<content:encoded><![CDATA[
arXiv:2512.03098v1 Announce Type: cross 
Abstract: The rapid growth of Artificial Intelligence (AI) in healthcare has sparked interest in Trustworthy AI and AI Implementation Science, both of which are essential for accelerating clinical adoption. However, strict regulations, gaps between research and clinical settings, and challenges in evaluating AI systems continue to hinder real-world implementation. This study presents an AI implementation case study within Shriners Childrens (SC), a large multisite pediatric system, showcasing the modernization of SCs Research Data Warehouse (RDW) to OMOP CDM v5.4 within a secure Microsoft Fabric environment. We introduce a Python-based data quality assessment tool compatible with SCs infrastructure, extending OHDsi's R/Java-based Data Quality Dashboard (DQD) and integrating Trustworthy AI principles using the METRIC framework. This extension enhances data quality evaluation by addressing informative missingness, redundancy, timeliness, and distributional consistency. We also compare systematic and case-specific AI implementation strategies for Craniofacial Microsomia (CFM) using the FHIR standard. Our contributions include a real-world evaluation of AI implementations, integration of Trustworthy AI principles into data quality assessment, and insights into hybrid implementation strategies that blend systematic infrastructure with use-case-driven approaches to advance AI in healthcare.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QGShap: Quantum Acceleration for Faithful GNN Explanations</title>
<link>https://arxiv.org/abs/2512.03099</link>
<guid>https://arxiv.org/abs/2512.03099</guid>
<content:encoded><![CDATA[
arXiv:2512.03099v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) have become indispensable in critical domains such as drug discovery, social network analysis, and recommendation systems, yet their black-box nature hinders deployment in scenarios requiring transparency and accountability. While Shapley value-based methods offer mathematically principled explanations by quantifying each component's contribution to predictions, computing exact values requires evaluating $2^n$ coalitions (or aggregating over $n!$ permutations), which is intractable for real-world graphs. Existing approximation strategies sacrifice either fidelity or efficiency, limiting their practical utility. We introduce QGShap, a quantum computing approach that leverages amplitude amplification to achieve quadratic speedups in coalition evaluation while maintaining exact Shapley computation. Unlike classical sampling or surrogate methods, our approach provides fully faithful explanations without approximation trade-offs for tractable graph sizes. We conduct empirical evaluations on synthetic graph datasets, demonstrating that QGShap achieves consistently high fidelity and explanation accuracy, matching or exceeding the performance of classical methods across all evaluation metrics. These results collectively demonstrate that QGShap not only preserves exact Shapley faithfulness but also delivers interpretable, stable, and structurally consistent explanations that align with the underlying graph reasoning of GNNs. The implementation of QGShap is available at https://github.com/smlab-niser/qgshap.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Discrete Neural Operator with Adaptive Sampling for Surrogate Modeling of Parametric Transient Darcy Flows in Porous Media</title>
<link>https://arxiv.org/abs/2512.03113</link>
<guid>https://arxiv.org/abs/2512.03113</guid>
<content:encoded><![CDATA[
arXiv:2512.03113v1 Announce Type: cross 
Abstract: This study proposes a new discrete neural operator for surrogate modeling of transient Darcy flow fields in heterogeneous porous media with random parameters. The new method integrates temporal encoding, operator learning and UNet to approximate the mapping between vector spaces of random parameter and spatiotemporal flow fields. The new discrete neural operator can achieve higher prediction accuracy than the SOTA attention-residual-UNet structure. Derived from the finite volume method, the transmissibility matrices rather than permeability is adopted as the inputs of surrogates to enhance the prediction accuracy further. To increase sampling efficiency, a generative latent space adaptive sampling method is developed employing the Gaussian mixture model for density estimation of generalization error. Validation is conducted on test cases of 2D/3D single- and two-phase Darcy flow field prediction. Results reveal consistent enhancement in prediction accuracy given limited training set.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Drainage: A Unifying Framework for Addressing Class Uncertainty</title>
<link>https://arxiv.org/abs/2512.03182</link>
<guid>https://arxiv.org/abs/2512.03182</guid>
<content:encoded><![CDATA[
arXiv:2512.03182v1 Announce Type: cross 
Abstract: Modern deep learning faces significant challenges with noisy labels, class ambiguity, as well as the need to robustly reject out-of-distribution or corrupted samples. In this work, we propose a unified framework based on the concept of a "drainage node'' which we add at the output of the network. The node serves to reallocate probability mass toward uncertainty, while preserving desirable properties such as end-to-end training and differentiability. This mechanism provides a natural escape route for highly ambiguous, anomalous, or noisy samples, particularly relevant for instance-dependent and asymmetric label noise. In systematic experiments involving the addition of varying proportions of instance-dependent noise or asymmetric noise to CIFAR-10/100 labels, our drainage formulation achieves an accuracy increase of up to 9\% over existing approaches in the high-noise regime. Our results on real-world datasets, such as mini-WebVision, mini-ImageNet and Clothing-1M, match or surpass existing state-of-the-art methods. Qualitative analysis reveals a denoising effect, where the drainage neuron consistently absorbs corrupt, mislabeled, or outlier data, leading to more stable decision boundaries. Furthermore, our drainage formulation enables applications well beyond classification, with immediate benefits for web-scale, semi-supervised dataset cleaning, and open-set applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In Situ Quantum Analog Pulse Characterization via Structured Signal Processing</title>
<link>https://arxiv.org/abs/2512.03193</link>
<guid>https://arxiv.org/abs/2512.03193</guid>
<content:encoded><![CDATA[
arXiv:2512.03193v1 Announce Type: cross 
Abstract: Analog quantum simulators can directly emulate time-dependent Hamiltonian dynamics, enabling the exploration of diverse physical phenomena such as phase transitions, quench dynamics, and non-equilibrium processes. Realizing accurate analog simulations requires high-fidelity time-dependent pulse control, yet existing calibration schemes are tailored to digital gate characterization and cannot be readily extended to learn continuous pulse trajectories. We present a characterization algorithm for in situ learning of pulse trajectories by extending the Quantum Signal Processing (QSP) framework to analyze time-dependent pulses. By combining QSP with a logical-level analog-digital mapping paradigm, our method reconstructs a smooth pulse directly from queries of the time-ordered propagator, without requiring mid-circuit measurements or additional evolution. Unlike conventional Trotterization-based methods, our approach avoids unscalable performance degradation arising from accumulated local truncation errors as the logical-level segmentation increases. Through rigorous theoretical analysis and extensive numerical simulations, we demonstrate that our method achieves high accuracy with strong efficiency and robustness against SPAM as well as depolarizing errors, providing a lightweight and optimal validation protocol for analog quantum simulators capable of detecting major hardware faults.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAND: Guidance, Rebalancing, and Assignment for Networked Dispatch in Multi-Agent Path Finding</title>
<link>https://arxiv.org/abs/2512.03194</link>
<guid>https://arxiv.org/abs/2512.03194</guid>
<content:encoded><![CDATA[
arXiv:2512.03194v1 Announce Type: cross 
Abstract: Large robot fleets are now common in warehouses and other logistics settings, where small control gains translate into large operational impacts. In this article, we address task scheduling for lifelong Multi-Agent Pickup-and-Delivery (MAPD) and propose a hybrid method that couples learning-based global guidance with lightweight optimization. A graph neural network policy trained via reinforcement learning outputs a desired distribution of free agents over an aggregated warehouse graph. This signal is converted into region-to-region rebalancing through a minimum-cost flow, and finalized by small, local assignment problems, preserving accuracy while keeping per-step latency within a 1 s compute budget. On congested warehouse benchmarks from the League of Robot Runners (LRR) with up to 500 agents, our approach improves throughput by up to 10% over the 2024 winning scheduler while maintaining real-time execution. The results indicate that coupling graph-structured learned guidance with tractable solvers reduces congestion and yields a practical, scalable blueprint for high-throughput scheduling in large fleets.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enhancing Job Matching: Occupation, Skill and Qualification Linking with the ESCO and EQF taxonomies</title>
<link>https://arxiv.org/abs/2512.03195</link>
<guid>https://arxiv.org/abs/2512.03195</guid>
<content:encoded><![CDATA[
arXiv:2512.03195v1 Announce Type: cross 
Abstract: This study investigates the potential of language models to improve the classification of labor market information by linking job vacancy texts to two major European frameworks: the European Skills, Competences, Qualifications and Occupations (ESCO) taxonomy and the European Qualifications Framework (EQF). We examine and compare two prominent methodologies from the literature: Sentence Linking and Entity Linking. In support of ongoing research, we release an open-source tool, incorporating these two methodologies, designed to facilitate further work on labor classification and employment discourse. To move beyond surface-level skill extraction, we introduce two annotated datasets specifically aimed at evaluating how occupations and qualifications are represented within job vacancy texts. Additionally, we examine different ways to utilize generative large language models for this task. Our findings contribute to advancing the state of the art in job entity extraction and offer computational infrastructure for examining work, skills, and labor market narratives in a digitally mediated economy. Our code is made publicly available: https://github.com/tabiya-tech/tabiya-livelihoods-classifier
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ultra-Strong Gradient Diffusion MRI with Self-Supervised Learning for Prostate Cancer Characterization</title>
<link>https://arxiv.org/abs/2512.03196</link>
<guid>https://arxiv.org/abs/2512.03196</guid>
<content:encoded><![CDATA[
arXiv:2512.03196v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) enables non-invasive assessment of prostate microstructure but conventional metrics such as the Apparent Diffusion Coefficient in multiparametric MRI lack specificity to underlying histology. Integrating dMRI with the compartment-based biophysical VERDICT (Vascular, Extracellular, and Restricted Diffusion for Cytometry in Tumours) framework offers richer microstructural insights, though clinical gradient systems (40-80 mT/m) suffer from poor signal-to-noise ratio (SNR) at stronger diffusion weightings due to prolonged echo times. Ultra-strong gradients (up to 300 mT/m) can mitigate these limitations by improving SNR and contrast-to-noise ratios (CNR) but their adoption has until recently been limited to research environments due to challenges with peripheral nerve stimulation thresholds and gradient non-uniformity. This study investigates whether physics-informed self-supervised VERDICT (ssVERDICT) fitting applied to ultra-strong gradients enhances prostate cancer characterization relative to current clinical acquisitions. We developed enhanced ssVERDICT fitting approaches using dense multilayer perceptron (Dense MLP) and convolutional U-Net architectures, benchmarking them against non-linear least-squares (NLLS) fitting and Diffusion Kurtosis Imaging across clinical- to ultra-strong gradient systems. Dense ssVERDICT at ultra-strong gradient notably outperformed NLLS VERDICT, boosting median CNR by 47%, cutting inter-patient Coefficient of Variation by 52%, and reducing pooled f_ic variation by 50%. Overall, it delivered the highest CNR, the most stable parameter estimates, and the clearest tumour-normal contrast compared with conventional methods and clinical gradient systems. These findings highlight the potential of advanced gradient systems and deep learning-based modelling to improve non-invasive prostate cancer characterization and reduce unnecessary biopsies.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Large Language Model Reward Learning under Heterogeneous Human Feedback</title>
<link>https://arxiv.org/abs/2512.03208</link>
<guid>https://arxiv.org/abs/2512.03208</guid>
<content:encoded><![CDATA[
arXiv:2512.03208v1 Announce Type: cross 
Abstract: We study estimation and statistical inference for reward models used in aligning large language models (LLMs). A key component of LLM alignment is reinforcement learning from human feedback (RLHF), where humans compare pairs of model-generated answers and their preferences are used to train a reward model. However, human feedback is inherently heterogeneous, creating significant challenges for reliable reward learning. To address this, we adopt a heterogeneous preference framework that jointly models the latent reward of answers and human rationality. This leads to a challenging biconvex optimization problem, which we solve via an alternating gradient descent algorithm. We establish theoretical guarantees for the resulting estimator, including its convergence and asymptotic distribution. These results enable the construction of confidence intervals for reward estimates. Leveraging these uncertainty quantification results, we conduct valid statistical comparisons between rewards and incorporate uncertainty into the best-of-$N$ (BoN) policy framework. Extensive simulations demonstrate the effectiveness of our method, and applications to real LLM data highlight the practical value of accounting for uncertainty in reward modeling for LLM alignment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flux4D: Flow-based Unsupervised 4D Reconstruction</title>
<link>https://arxiv.org/abs/2512.03210</link>
<guid>https://arxiv.org/abs/2512.03210</guid>
<content:encoded><![CDATA[
arXiv:2512.03210v1 Announce Type: cross 
Abstract: Reconstructing large-scale dynamic scenes from visual observations is a fundamental challenge in computer vision, with critical implications for robotics and autonomous systems. While recent differentiable rendering methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have achieved impressive photorealistic reconstruction, they suffer from scalability limitations and require annotations to decouple actor motion. Existing self-supervised methods attempt to eliminate explicit annotations by leveraging motion cues and geometric priors, yet they remain constrained by per-scene optimization and sensitivity to hyperparameter tuning. In this paper, we introduce Flux4D, a simple and scalable framework for 4D reconstruction of large-scale dynamic scenes. Flux4D directly predicts 3D Gaussians and their motion dynamics to reconstruct sensor observations in a fully unsupervised manner. By adopting only photometric losses and enforcing an "as static as possible" regularization, Flux4D learns to decompose dynamic elements directly from raw data without requiring pre-trained supervised models or foundational priors simply by training across many scenes. Our approach enables efficient reconstruction of dynamic scenes within seconds, scales effectively to large datasets, and generalizes well to unseen environments, including rare and unknown objects. Experiments on outdoor driving datasets show Flux4D significantly outperforms existing methods in scalability, generalization, and reconstruction quality.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergence of a class of gradient-free optimisation schemes when the objective function is noisy, irregular, or both</title>
<link>https://arxiv.org/abs/2512.03225</link>
<guid>https://arxiv.org/abs/2512.03225</guid>
<content:encoded><![CDATA[
arXiv:2512.03225v1 Announce Type: cross 
Abstract: We investigate the convergence properties of a class of iterative algorithms designed to minimize a potentially non-smooth and noisy objective function, which may be algebraically intractable and whose values may be obtained as the output of a black box. The algorithms considered can be cast under the umbrella of a generalised gradient descent recursion, where the gradient is that of a smooth approximation of the objective function. The framework we develop includes as special cases model-based and mollification methods, two classical approaches to zero-th order optimisation. The convergence results are obtained under very weak assumptions on the regularity of the objective function and involve a trade-off between the degree of smoothing and size of the steps taken in the parameter updates. As expected, additional assumptions are required in the stochastic case. We illustrate the relevance of these algorithms and our convergence results through a challenging classification example from machine learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Iterative Tilting for Diffusion Fine-Tuning</title>
<link>https://arxiv.org/abs/2512.03234</link>
<guid>https://arxiv.org/abs/2512.03234</guid>
<content:encoded><![CDATA[
arXiv:2512.03234v1 Announce Type: cross 
Abstract: We introduce iterative tilting, a gradient-free method for fine-tuning diffusion models toward reward-tilted distributions. The method decomposes a large reward tilt $\exp(\lambda r)$ into $N$ sequential smaller tilts, each admitting a tractable score update via first-order Taylor expansion. This requires only forward evaluations of the reward function and avoids backpropagating through sampling chains. We validate on a two-dimensional Gaussian mixture with linear reward, where the exact tilted distribution is available in closed form.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to DP-fy Your Data: A Practical Guide to Generating Synthetic Data With Differential Privacy</title>
<link>https://arxiv.org/abs/2512.03238</link>
<guid>https://arxiv.org/abs/2512.03238</guid>
<content:encoded><![CDATA[
arXiv:2512.03238v1 Announce Type: cross 
Abstract: High quality data is needed to unlock the full potential of AI for end users. However finding new sources of such data is getting harder: most publicly-available human generated data will soon have been used. Additionally, publicly available data often is not representative of users of a particular system -- for example, a research speech dataset of contractors interacting with an AI assistant will likely be more homogeneous, well articulated and self-censored than real world commands that end users will issue. Therefore unlocking high-quality data grounded in real user interactions is of vital interest. However, the direct use of user data comes with significant privacy risks. Differential Privacy (DP) is a well established framework for reasoning about and limiting information leakage, and is a gold standard for protecting user privacy. The focus of this work, \emph{Differentially Private Synthetic data}, refers to synthetic data that preserves the overall trends of source data,, while providing strong privacy guarantees to individuals that contributed to the source dataset. DP synthetic data can unlock the value of datasets that have previously been inaccessible due to privacy concerns and can replace the use of sensitive datasets that previously have only had rudimentary protections like ad-hoc rule-based anonymization.
  In this paper we explore the full suite of techniques surrounding DP synthetic data, the types of privacy protections they offer and the state-of-the-art for various modalities (image, tabular, text and decentralized). We outline all the components needed in a system that generates DP synthetic data, from sensitive data handling and preparation, to tracking the use and empirical privacy testing. We hope that work will result in increased adoption of DP synthetic data, spur additional research and increase trust in DP synthetic data approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Novelty detection on path space</title>
<link>https://arxiv.org/abs/2512.03243</link>
<guid>https://arxiv.org/abs/2512.03243</guid>
<content:encoded><![CDATA[
arXiv:2512.03243v1 Announce Type: cross 
Abstract: We frame novelty detection on path space as a hypothesis testing problem with signature-based test statistics. Using transportation-cost inequalities of Gasteratos and Jacquier (2023), we obtain tail bounds for false positive rates that extend beyond Gaussian measures to laws of RDE solutions with smooth bounded vector fields, yielding estimates of quantiles and p-values. Exploiting the shuffle product, we derive exact formulae for smooth surrogates of conditional value-at-risk (CVaR) in terms of expected signatures, leading to new one-class SVM algorithms optimising smooth CVaR objectives. We then establish lower bounds on type-$\mathrm{II}$ error for alternatives with finite first moment, giving general power bounds when the reference measure and the alternative are absolutely continuous with respect to each other. Finally, we evaluate numerically the type-$\mathrm{I}$ error and statistical power of signature-based test statistic, using synthetic anomalous diffusion data and real-world molecular biology data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Network Sheaves for AI-native Semantic Communication</title>
<link>https://arxiv.org/abs/2512.03248</link>
<guid>https://arxiv.org/abs/2512.03248</guid>
<content:encoded><![CDATA[
arXiv:2512.03248v1 Announce Type: cross 
Abstract: Recent advances in AI call for a paradigm shift from bit-centric communication to goal- and semantics-oriented architectures, paving the way for AI-native 6G networks. In this context, we address a key open challenge: enabling heterogeneous AI agents to exchange compressed latent-space representations while mitigating semantic noise and preserving task-relevant meaning. We cast this challenge as learning both the communication topology and the alignment maps that govern information exchange among agents, yielding a learned network sheaf equipped with orthogonal maps. This learning process is further supported by a semantic denoising end compression module that constructs a shared global semantic space and derives sparse, structured representations of each agent's latent space. This corresponds to a nonconvex dictionary learning problem solved iteratively with closed-form updates. Experiments with mutiple AI agents pre-trained on real image data show that the semantic denoising and compression facilitates AI agents alignment and the extraction of semantic clusters, while preserving high accuracy in downstream task. The resulting communication network provides new insights about semantic heterogeneity across agents, highlighting the interpretability of our methodology.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PyroFocus: A Deep Learning Approach to Real-Time Wildfire Detection in Multispectral Remote Sensing Imagery</title>
<link>https://arxiv.org/abs/2512.03257</link>
<guid>https://arxiv.org/abs/2512.03257</guid>
<content:encoded><![CDATA[
arXiv:2512.03257v1 Announce Type: cross 
Abstract: Rapid and accurate wildfire detection is crucial for emergency response and environmental management. In airborne and spaceborne missions, real-time algorithms must distinguish between no fire, active fire, and post-fire conditions, and estimate fire intensity. Multispectral and hyperspectral thermal imagers provide rich spectral information, but high data dimensionality and limited onboard resources make real-time processing challenging. As wildfires increase in frequency and severity, the need for low-latency and computationally efficient onboard detection methods is critical.
  We present a systematic evaluation of multiple deep learning architectures, including custom Convolutional Neural Networks (CNNs) and Transformer-based models, for multi-class fire classification. We also introduce PyroFocus, a two-stage pipeline that performs fire classification followed by fire radiative power (FRP) regression or segmentation to reduce inference time and computational cost for onboard deployment. Using data from NASA's MODIS/ASTER Airborne Simulator (MASTER), which is similar to a next-generation fire detection sensor, we compare accuracy, inference latency, and resource efficiency.
  Experimental results show that the proposed two-stage pipeline achieves strong trade-offs between speed and accuracy, demonstrating significant potential for real-time edge deployment in future wildfire monitoring missions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Associating Healthcare Teamwork with Patient Outcomes for Predictive Analysis</title>
<link>https://arxiv.org/abs/2512.03296</link>
<guid>https://arxiv.org/abs/2512.03296</guid>
<content:encoded><![CDATA[
arXiv:2512.03296v1 Announce Type: cross 
Abstract: Cancer treatment outcomes are influenced not only by clinical and demographic factors but also by the collaboration of healthcare teams. However, prior work has largely overlooked the potential role of human collaboration in shaping patient survival. This paper presents an applied AI approach to uncovering the impact of healthcare professionals' (HCPs) collaboration-captured through electronic health record (EHR) systems-on cancer patient outcomes. We model EHR-mediated HCP interactions as networks and apply machine learning techniques to detect predictive signals of patient survival embedded in these collaborations. Our models are cross validated to ensure generalizability, and we explain the predictions by identifying key network traits associated with improved outcomes. Importantly, clinical experts and literature validate the relevance of the identified crucial collaboration traits, reinforcing their potential for real-world applications. This work contributes to a practical workflow for leveraging digital traces of collaboration and AI to assess and improve team-based healthcare. The approach is potentially transferable to other domains involving complex collaboration and offers actionable insights to support data-informed interventions in healthcare delivery.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Randomized Masked Finetuning: An Efficient Way to Mitigate Memorization of PIIs in LLMs</title>
<link>https://arxiv.org/abs/2512.03310</link>
<guid>https://arxiv.org/abs/2512.03310</guid>
<content:encoded><![CDATA[
arXiv:2512.03310v1 Announce Type: cross 
Abstract: The current literature on memorization in Natural Language Models, especially Large Language Models (LLMs), poses severe security and privacy risks, as models tend to memorize personally identifying information (PIIs) from training data. We introduce Randomized Masked Fine-Tuning (RMFT), a novel privacy-preserving fine-tuning technique that reduces PII memorization while minimizing performance impact. Using the Enron Email Dataset, we demonstrate that RMFT achieves an 80.81% reduction in Total Extraction Rate and 80.17% reduction in Seen Extraction Rate compared to baseline fine-tuning, outperforming deduplication methods while maintaining only a 5.73% increase in perplexity. We present MaxTER, a Pareto-optimal evaluation framework for assessing privacy-utility tradeoffs, and show the performance of RMFT vs Deduplication by Area Under The Response Curve (AURC) metric.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking hidden biomolecular conformational landscapes in diffusion models at inference time</title>
<link>https://arxiv.org/abs/2512.03312</link>
<guid>https://arxiv.org/abs/2512.03312</guid>
<content:encoded><![CDATA[
arXiv:2512.03312v1 Announce Type: cross 
Abstract: The function of biomolecules such as proteins depends on their ability to interconvert between a wide range of structures or "conformations." Researchers have endeavored for decades to develop computational methods to predict the distribution of conformations, which is far harder to determine experimentally than a static folded structure. We present ConforMix, an inference-time algorithm that enhances sampling of conformational distributions using a combination of classifier guidance, filtering, and free energy estimation. Our approach upgrades diffusion models -- whether trained for static structure prediction or conformational generation -- to enable more efficient discovery of conformational variability without requiring prior knowledge of major degrees of freedom. ConforMix is orthogonal to improvements in model pretraining and would benefit even a hypothetical model that perfectly reproduced the Boltzmann distribution. Remarkably, when applied to a diffusion model trained for static structure prediction, ConforMix captures structural changes including domain motion, cryptic pocket flexibility, and transporter cycling, while avoiding unphysical states. Case studies of biologically critical proteins demonstrate the scalability, accuracy, and utility of this method.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>NavMapFusion: Diffusion-based Fusion of Navigation Maps for Online Vectorized HD Map Construction</title>
<link>https://arxiv.org/abs/2512.03317</link>
<guid>https://arxiv.org/abs/2512.03317</guid>
<content:encoded><![CDATA[
arXiv:2512.03317v1 Announce Type: cross 
Abstract: Accurate environmental representations are essential for autonomous driving, providing the foundation for safe and efficient navigation. Traditionally, high-definition (HD) maps are providing this representation of the static road infrastructure to the autonomous system a priori. However, because the real world is constantly changing, such maps must be constructed online from on-board sensor data. Navigation-grade standard-definition (SD) maps are widely available, but their resolution is insufficient for direct deployment. Instead, they can be used as coarse prior to guide the online map construction process. We propose NavMapFusion, a diffusion-based framework that performs iterative denoising conditioned on high-fidelity sensor data and on low-fidelity navigation maps. This paper strives to answer: (1) How can coarse, potentially outdated navigation maps guide online map construction? (2) What advantages do diffusion models offer for map fusion? We demonstrate that diffusion-based map construction provides a robust framework for map fusion. Our key insight is that discrepancies between the prior map and online perception naturally correspond to noise within the diffusion process; consistent regions reinforce the map construction, whereas outdated segments are suppressed. On the nuScenes benchmark, NavMapFusion conditioned on coarse road lines from OpenStreetMap data reaches a 21.4% relative improvement on 100 m, and even stronger improvements on larger perception ranges, while maintaining real-time capabilities. By fusing low-fidelity priors with high-fidelity sensor data, the proposed method generates accurate and up-to-date environment representations, guiding towards safer and more reliable autonomous driving. The code is available at https://github.com/tmonnin/navmapfusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When does Gaussian equivalence fail and how to fix it: Non-universal behavior of random features with quadratic scaling</title>
<link>https://arxiv.org/abs/2512.03325</link>
<guid>https://arxiv.org/abs/2512.03325</guid>
<content:encoded><![CDATA[
arXiv:2512.03325v1 Announce Type: cross 
Abstract: A major effort in modern high-dimensional statistics has been devoted to the analysis of linear predictors trained on nonlinear feature embeddings via empirical risk minimization (ERM). Gaussian equivalence theory (GET) has emerged as a powerful universality principle in this context: it states that the behavior of high-dimensional, complex features can be captured by Gaussian surrogates, which are more amenable to analysis. Despite its remarkable successes, numerical experiments show that this equivalence can fail even for simple embeddings -- such as polynomial maps -- under general scaling regimes.
  We investigate this breakdown in the setting of random feature (RF) models in the quadratic scaling regime, where both the number of features and the sample size grow quadratically with the data dimension. We show that when the target function depends on a low-dimensional projection of the data, such as generalized linear models, GET yields incorrect predictions. To capture the correct asymptotics, we introduce a Conditional Gaussian Equivalent (CGE) model, which can be viewed as appending a low-dimensional non-Gaussian component to an otherwise high-dimensional Gaussian model. This hybrid model retains the tractability of the Gaussian framework and accurately describes RF models in the quadratic scaling regime. We derive sharp asymptotics for the training and test errors in this setting, which continue to agree with numerical simulations even when GET fails.
  Our analysis combines general results on CLT for Wiener chaos expansions and a careful two-phase Lindeberg swapping argument. Beyond RF models and quadratic scaling, our work hints at a rich landscape of universality phenomena in high-dimensional ERM.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Step-by-step Layered Design Generation</title>
<link>https://arxiv.org/abs/2512.03335</link>
<guid>https://arxiv.org/abs/2512.03335</guid>
<content:encoded><![CDATA[
arXiv:2512.03335v1 Announce Type: cross 
Abstract: Design generation, in its essence, is a step-by-step process where designers progressively refine and enhance their work through careful modifications. Despite this fundamental characteristic, existing approaches mainly treat design synthesis as a single-step generation problem, significantly underestimating the inherent complexity of the creative process. To bridge this gap, we propose a novel problem setting called Step-by-Step Layered Design Generation, which tasks a machine learning model with generating a design that adheres to a sequence of instructions from a designer. Leveraging recent advancements in multi-modal LLMs, we propose SLEDGE: Step-by-step LayEred Design GEnerator to model each update to a design as an atomic, layered change over its previous state, while being grounded in the instruction. To complement our new problem setting, we introduce a new evaluation suite, including a dataset and a benchmark. Our exhaustive experimental analysis and comparison with state-of-the-art approaches tailored to our new setup demonstrate the efficacy of our approach. We hope our work will attract attention to this pragmatic and under-explored research area.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProtoEFNet: Dynamic Prototype Learning for Inherently Interpretable Ejection Fraction Estimation in Echocardiography</title>
<link>https://arxiv.org/abs/2512.03339</link>
<guid>https://arxiv.org/abs/2512.03339</guid>
<content:encoded><![CDATA[
arXiv:2512.03339v1 Announce Type: cross 
Abstract: Ejection fraction (EF) is a crucial metric for assessing cardiac function and diagnosing conditions such as heart failure. Traditionally, EF estimation requires manual tracing and domain expertise, making the process time-consuming and subject to interobserver variability. Most current deep learning methods for EF prediction are black-box models with limited transparency, which reduces clinical trust. Some post-hoc explainability methods have been proposed to interpret the decision-making process after the prediction is made. However, these explanations do not guide the model's internal reasoning and therefore offer limited reliability in clinical applications. To address this, we introduce ProtoEFNet, a novel video-based prototype learning model for continuous EF regression. The model learns dynamic spatiotemporal prototypes that capture clinically meaningful cardiac motion patterns. Additionally, the proposed Prototype Angular Separation (PAS) loss enforces discriminative representations across the continuous EF spectrum. Our experiments on the EchonetDynamic dataset show that ProtoEFNet can achieve accuracy on par with its non-interpretable counterpart while providing clinically relevant insight. The ablation study shows that the proposed loss boosts performance with a 2% increase in F1 score from 77.67$\pm$2.68 to 79.64$\pm$2.10. Our source code is available at: https://github.com/DeepRCL/ProtoEF
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparative algorithm performance evaluation and prediction for the maximum clique problem using instance space analysis</title>
<link>https://arxiv.org/abs/2512.03419</link>
<guid>https://arxiv.org/abs/2512.03419</guid>
<content:encoded><![CDATA[
arXiv:2512.03419v1 Announce Type: cross 
Abstract: The maximum clique problem, a well-known graph-based combinatorial optimization problem, has been addressed through various algorithmic approaches, though systematic analyses of the problem instances remain sparse. This study employs the instance space analysis (ISA) methodology to systematically analyze the instance space of this problem and assess & predict the performance of state-of-the-art (SOTA) algorithms, including exact, heuristic, and graph neural network (GNN)-based methods. A dataset was compiled using graph instances from TWITTER, COLLAB and IMDB-BINARY benchmarks commonly used in graph machine learning research. A set of 33 generic and 2 problem-specific polynomial-time-computable graph-based features, including several spectral properties, was employed for the ISA. A composite performance mea- sure incorporating both solution quality and algorithm runtime was utilized. The comparative analysis demonstrated that the exact algorithm Mixed Order Maximum Clique (MOMC) exhib- ited superior performance across approximately 74.7% of the instance space constituted by the compiled dataset. Gurobi & CliSAT accounted for superior performance in 13.8% and 11% of the instance space, respectively. The ISA-based algorithm performance prediction model run on 34 challenging test instances compiled from the BHOSLIB and DIMACS datasets yielded top-1 and top-2 best performing algorithm prediction accuracies of 88% and 97%, respectively.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>KeyPointDiffuser: Unsupervised 3D Keypoint Learning via Latent Diffusion Models</title>
<link>https://arxiv.org/abs/2512.03450</link>
<guid>https://arxiv.org/abs/2512.03450</guid>
<content:encoded><![CDATA[
arXiv:2512.03450v1 Announce Type: cross 
Abstract: Understanding and representing the structure of 3D objects in an unsupervised manner remains a core challenge in computer vision and graphics. Most existing unsupervised keypoint methods are not designed for unconditional generative settings, restricting their use in modern 3D generative pipelines; our formulation explicitly bridges this gap. We present an unsupervised framework for learning spatially structured 3D keypoints from point cloud data. These keypoints serve as a compact and interpretable representation that conditions an Elucidated Diffusion Model (EDM) to reconstruct the full shape. The learned keypoints exhibit repeatable spatial structure across object instances and support smooth interpolation in keypoint space, indicating that they capture geometric variation. Our method achieves strong performance across diverse object categories, yielding a 6 percentage-point improvement in keypoint consistency compared to prior approaches.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GalaxyDiT: Efficient Video Generation with Guidance Alignment and Adaptive Proxy in Diffusion Transformers</title>
<link>https://arxiv.org/abs/2512.03451</link>
<guid>https://arxiv.org/abs/2512.03451</guid>
<content:encoded><![CDATA[
arXiv:2512.03451v1 Announce Type: cross 
Abstract: Diffusion models have revolutionized video generation, becoming essential tools in creative content generation and physical simulation. Transformer-based architectures (DiTs) and classifier-free guidance (CFG) are two cornerstones of this success, enabling strong prompt adherence and realistic video quality. Despite their versatility and superior performance, these models require intensive computation. Each video generation requires dozens of iterative steps, and CFG doubles the required compute. This inefficiency hinders broader adoption in downstream applications.
  We introduce GalaxyDiT, a training-free method to accelerate video generation with guidance alignment and systematic proxy selection for reuse metrics. Through rank-order correlation analysis, our technique identifies the optimal proxy for each video model, across model families and parameter scales, thereby ensuring optimal computational reuse. We achieve $1.87\times$ and $2.37\times$ speedup on Wan2.1-1.3B and Wan2.1-14B with only 0.97% and 0.72% drops on the VBench-2.0 benchmark. At high speedup rates, our approach maintains superior fidelity to the base model, exceeding prior state-of-the-art approaches by 5 to 10 dB in peak signal-to-noise ratio (PSNR).
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Convolutional Framework for Mapping Imagined Auditory MEG into Listened Brain Responses</title>
<link>https://arxiv.org/abs/2512.03458</link>
<guid>https://arxiv.org/abs/2512.03458</guid>
<content:encoded><![CDATA[
arXiv:2512.03458v1 Announce Type: cross 
Abstract: Decoding imagined speech engages complex neural processes that are difficult to interpret due to uncertainty in timing and the limited availability of imagined-response datasets. In this study, we present a Magnetoencephalography (MEG) dataset collected from trained musicians as they imagined and listened to musical and poetic stimuli. We show that both imagined and perceived brain responses contain consistent, condition-specific information. Using a sliding-window ridge regression model, we first mapped imagined responses to listened responses at the single-subject level, but found limited generalization across subjects. At the group level, we developed an encoder-decoder convolutional neural network with a subject-specific calibration layer that produced stable and generalizable mappings. The CNN consistently outperformed the null model, yielding significantly higher correlations between predicted and true listened responses for nearly all held-out subjects. Our findings demonstrate that imagined neural activity can be transformed into perception-like responses, providing a foundation for future brain-computer interface applications involving imagined speech and music.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning From Limited Data and Feedback for Cell Culture Process Monitoring: A Comparative Study</title>
<link>https://arxiv.org/abs/2512.03460</link>
<guid>https://arxiv.org/abs/2512.03460</guid>
<content:encoded><![CDATA[
arXiv:2512.03460v1 Announce Type: cross 
Abstract: In cell culture bioprocessing, real-time batch process monitoring (BPM) refers to the continuous tracking and analysis of key process variables such as viable cell density, nutrient levels, metabolite concentrations, and product titer throughout the duration of a batch run. This enables early detection of deviations and supports timely control actions to ensure optimal cell growth and product quality. BPM plays a critical role in ensuring the quality and regulatory compliance of biopharmaceutical manufacturing processes. However, the development of accurate soft sensors for BPM is hindered by key challenges, including limited historical data, infrequent feedback, heterogeneous process conditions, and high-dimensional sensory inputs. This study presents a comprehensive benchmarking analysis of machine learning (ML) methods designed to address these challenges, with a focus on learning from historical data with limited volume and relevance in the context of bioprocess monitoring. We evaluate multiple ML approaches including feature dimensionality reduction, online learning, and just-in-time learning across three datasets, one in silico dataset and two real-world experimental datasets. Our findings highlight the importance of training strategies in handling limited data and feedback, with batch learning proving effective in homogeneous settings, while just-in-time learning and online learning demonstrate superior adaptability in cold-start scenarios. Additionally, we identify key meta-features, such as feed media composition and process control strategies, that significantly impact model transferability. The results also suggest that integrating Raman-based predictions with lagged offline measurements enhances monitoring accuracy, offering a promising direction for future bioprocess soft sensor development.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hybrid Deep Learning and Anomaly Detection Framework for Real-Time Malicious URL Classification</title>
<link>https://arxiv.org/abs/2512.03462</link>
<guid>https://arxiv.org/abs/2512.03462</guid>
<content:encoded><![CDATA[
arXiv:2512.03462v1 Announce Type: cross 
Abstract: Malicious URLs remain a primary vector for phishing, malware, and cyberthreats. This study proposes a hybrid deep learning framework combining \texttt{HashingVectorizer} n-gram analysis, SMOTE balancing, Isolation Forest anomaly filtering, and a lightweight neural network classifier for real-time URL classification. The multi-stage pipeline processes URLs from open-source repositories with statistical features (length, dot count, entropy), achieving $O(NL + EBdh)$ training complexity and a 20\,ms prediction latency. Empirical evaluation yields 96.4\% accuracy, 95.4\% F1-score, and 97.3\% ROC-AUC, outperforming CNN (94.8\%) and SVM baselines with a $50\!\times$--$100\!\times$ speedup (Table~\ref{tab:comp-complexity}). A multilingual Tkinter GUI (Arabic/English/French) enables real-time threat assessment with clipboard integration. The framework demonstrates superior scalability and resilience against obfuscated URL patterns.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness-Aware Fine-Tuning of Vision-Language Models for Medical Glaucoma Diagnosis</title>
<link>https://arxiv.org/abs/2512.03477</link>
<guid>https://arxiv.org/abs/2512.03477</guid>
<content:encoded><![CDATA[
arXiv:2512.03477v1 Announce Type: cross 
Abstract: Vision-language models achieve expert-level performance on medical imaging tasks but exhibit significant diagnostic accuracy disparities across demographic groups. We introduce fairness-aware Low-Rank Adaptation for medical VLMs, combining parameter efficiency with explicit fairness optimization. Our key algorithmic contribution is a differentiable MaxAccGap loss that enables end-to-end optimization of accuracy parity across demographic groups. We propose three methods: FR-LoRA integrates MaxAccGap regularization into the training objective, GR-LoRA applies inverse frequency weighting to balance gradient contributions, and Hybrid-LoRA combines both mechanisms.Evaluated on 10,000 glaucoma fundus images, GR-LoRA reduces diagnostic accuracy disparities by 69% while maintaining 53.15% overall accuracy. Ablation studies reveal that strong regularization strength achieves optimal fairness with minimal accuracy trade-off, and race-specific optimization yields 60% disparity reduction. Our approach requires only 0.24% trainable parameters, enabling practical deployment of fair medical AI in resource-constrained healthcare settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Preliminary Study on the Promises and Challenges of Native Top-$k$ Sparse Attention</title>
<link>https://arxiv.org/abs/2512.03494</link>
<guid>https://arxiv.org/abs/2512.03494</guid>
<content:encoded><![CDATA[
arXiv:2512.03494v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly prevalent in the field of long-context modeling, however, their inference computational costs have become a critical bottleneck hindering the advancement of tasks such as agents and multimodal applications. This report conducts a preliminary investigation into the effectiveness and theoretical mechanisms of the Top-$k$ Attention mechanism during both the decoding and training phases. First, we validate the effectiveness of exact Top-$k$ Decoding through extensive experimentation. Experiments demonstrate that retaining only the pivotal Keys with the highest similarity to the Query as the context window during the decoding stage achieves performance comparable to, or even surpassing, full attention on downstream tasks such as HELMET and LongBench v2. Second, we further explore the native Top-$k$ Attention training strategy. Experiments confirm that ensuring the consistency between training and inference regarding Top-$k$ Attention operations facilitates the further unlocking of Top-$k$ Decoding's potential, thereby significantly enhancing model performance. Furthermore, considering the high computational complexity of exact Top-$k$ Attention, we investigate the impact of approximate Top-$k$ algorithm precision on downstream tasks. Our research confirms a positive correlation between downstream task performance and approximation fidelity, and we provide statistical evaluations of the Lightning Indexer's precision within the DeepSeek-V3.2-Exp model. Finally, this report provides a theoretical interpretation from the perspective of Entropy. Experimental observations indicate that models subjected to Top-$k$ Attention SFT exhibit a distinct phenomenon of entropy reduction in downstream tasks, which validates the hypothesis that low-entropy states are better adapted to Top-$k$ Decoding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Space Synergy: A Unified Framework for Multimodal Emotion Recognition in Conversation</title>
<link>https://arxiv.org/abs/2512.03521</link>
<guid>https://arxiv.org/abs/2512.03521</guid>
<content:encoded><![CDATA[
arXiv:2512.03521v1 Announce Type: cross 
Abstract: Multimodal Emotion Recognition in Conversation (MERC) aims to predict speakers' emotions by integrating textual, acoustic, and visual cues. Existing approaches either struggle to capture complex cross-modal interactions or experience gradient conflicts and unstable training when using deeper architectures. To address these issues, we propose Cross-Space Synergy (CSS), which couples a representation component with an optimization component. Synergistic Polynomial Fusion (SPF) serves the representation role, leveraging low-rank tensor factorization to efficiently capture high-order cross-modal interactions. Pareto Gradient Modulator (PGM) serves the optimization role, steering updates along Pareto-optimal directions across competing objectives to alleviate gradient conflicts and improve stability. Experiments show that CSS outperforms existing representative methods on IEMOCAP and MELD in both accuracy and training stability, demonstrating its effectiveness in complex multimodal scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Learning to Predict Slot Usage in TSCH Wireless Sensor Networks</title>
<link>https://arxiv.org/abs/2512.03570</link>
<guid>https://arxiv.org/abs/2512.03570</guid>
<content:encoded><![CDATA[
arXiv:2512.03570v1 Announce Type: cross 
Abstract: Wireless sensor networks (WSNs) are employed across a wide range of industrial applications where ultra-low power consumption is a critical prerequisite. At the same time, these systems must maintain a certain level of determinism to ensure reliable and predictable operation. In this view, time slotted channel hopping (TSCH) is a communication technology that meets both conditions, making it an attractive option for its usage in industrial WSNs. This work proposes the use of machine learning to learn the traffic pattern generated in networks based on the TSCH protocol, in order to turn nodes into a deep sleep state when no transmission is planned and thus to improve the energy efficiency of the WSN. The ability of machine learning models to make good predictions at different network levels in a typical tree network topology was analyzed in depth, showing how their capabilities degrade while approaching the root of the tree. The application of these models on simulated data based on an accurate modeling of wireless sensor nodes indicates that the investigated algorithms can be suitably used to further and substantially reduce the power consumption of a TSCH network.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EnCompass: Enhancing Agent Programming with Search Over Program Execution Paths</title>
<link>https://arxiv.org/abs/2512.03571</link>
<guid>https://arxiv.org/abs/2512.03571</guid>
<content:encoded><![CDATA[
arXiv:2512.03571v1 Announce Type: cross 
Abstract: We introduce a new approach to agent programming, the development of LLM-based agents. Current approaches to agent programming often entangle two aspects of agent design: the core workflow logic and the inference-time strategy (e.g., tree search). We introduce "probabilistic angelic nondeterminism" ("PAN"), a programming model that disentangles these two concerns, allowing the programmer to describe the agent workflow and independently experiment with different inference-time strategies by simply changing a few inputs. We provide an implementation of PAN in Python as the EnCompass framework, which uses a Python decorator to compile agent workflow programs into a search space. We present three case studies that demonstrate how the framework lets the programmer quickly improve the reliability of an agent and easily switch between different inference-time strategies, all with little additional coding.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SELF: A Robust Singular Value and Eigenvalue Approach for LLM Fingerprinting</title>
<link>https://arxiv.org/abs/2512.03620</link>
<guid>https://arxiv.org/abs/2512.03620</guid>
<content:encoded><![CDATA[
arXiv:2512.03620v1 Announce Type: cross 
Abstract: The protection of Intellectual Property (IP) in Large Language Models (LLMs) represents a critical challenge in contemporary AI research. While fingerprinting techniques have emerged as a fundamental mechanism for detecting unauthorized model usage, existing methods -- whether behavior-based or structural -- suffer from vulnerabilities such as false claim attacks or susceptible to weight manipulations. To overcome these limitations, we propose SELF, a novel intrinsic weight-based fingerprinting scheme that eliminates dependency on input and inherently resists false claims. SELF achieves robust IP protection through two key innovations: 1) unique, scalable and transformation-invariant fingerprint extraction via singular value and eigenvalue decomposition of LLM attention weights, and 2) effective neural network-based fingerprint similarity comparison based on few-shot learning and data augmentation. Experimental results demonstrate SELF maintains high IP infringement detection accuracy while showing strong robustness against various downstream modifications, including quantization, pruning, and fine-tuning attacks. Our code is available at https://github.com/HanxiuZhang/SELF_v2.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AaPE: Aliasing-aware Patch Embedding for Self-Supervised Audio Representation Learning</title>
<link>https://arxiv.org/abs/2512.03637</link>
<guid>https://arxiv.org/abs/2512.03637</guid>
<content:encoded><![CDATA[
arXiv:2512.03637v1 Announce Type: cross 
Abstract: Transformer-based audio SSL (self-supervised learning) models often treat spectrograms as images, applying convolutional patchification with heavy temporal downsampling. This lowers the effective Nyquist frequency and introduces aliasing, while na\"ive low-pass filtering removes task-relevant high-frequency cues. In this study, we present Aliasing-aware Patch Embedding (AaPE), a drop-in patch stem that mitigates aliasing while preserving high-frequency information. AaPE augments standard patch tokens with features produced by a band-limited complex sinusoidal kernel using a two-sided exponential window that dynamically targets alias-prone bands. Frequency and decay parameters of the kernel are estimated from the input, enabling parallel, adaptive subband analysis whose outputs are fused with the standard patch tokens. AaPE integrates seamlessly into the masked teacher-student self-supervised learning. In addition, we combine a multi-mask strategy with a contrastive objective to enforce consistency across diverse mask patterns, stabilizing training. Pre-training on AudioSet followed by fine-tuning evaluation across diverse downstream benchmarks, which spanned categories, such as environmental sounds and other common audio domains. This approach yields state-of-the-art performance on a subset of tasks and competitive results across the remainder. Complementary linear probing evaluation mirrors this pattern, yielding clear gains on several benchmarks and strong performance elsewhere. The collective analysis of these results indicates that AaPE serves to mitigate the effects of aliasing without discarding of informative high-frequency content.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Optical Context Compression Is Just (Bad) Autoencoding</title>
<link>https://arxiv.org/abs/2512.03643</link>
<guid>https://arxiv.org/abs/2512.03643</guid>
<content:encoded><![CDATA[
arXiv:2512.03643v1 Announce Type: cross 
Abstract: DeepSeek-OCR demonstrates that rendered text can be reconstructed with high fidelity from a small number of vision tokens. This finding has sparked excitement about vision-based context compression for language models. But the evaluation stops at reconstruction; whether these representations help language modeling remains untested. We test two assumptions implicit in the optical-compression narrative: that vision-based compression provides unique advantages for text reconstruction from compressed representations, and that DeepSeek-OCR's reconstruction results are evidence that vision-based compression will be useful for language modeling. Comparing their vision encoder against simple alternatives--parameter-free mean pooling and a learned hierarchical encoder--we find that these simple approaches match or surpass vision for reconstruction at matched compression ratios, and outperform it for language modeling--where vision-based compression fails to beat truncation. The excitement around optical context compression outpaces the evidence. Code and checkpoints are available at https://github.com/ivnle/bad-autoencoding
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Consistent Projection of Langevin Dynamics: Preserving Thermodynamics and Kinetics in Coarse-Grained Models</title>
<link>https://arxiv.org/abs/2512.03706</link>
<guid>https://arxiv.org/abs/2512.03706</guid>
<content:encoded><![CDATA[
arXiv:2512.03706v1 Announce Type: cross 
Abstract: Coarse graining (CG) is an important task for efficient modeling and simulation of complex multi-scale systems, such as the conformational dynamics of biomolecules. This work presents a projection-based coarse-graining formalism for general underdamped Langevin dynamics. Following the Zwanzig projection approach, we derive a closed-form expression for the coarse grained dynamics. In addition, we show how the generator Extended Dynamic Mode Decomposition (gEDMD) method, which was developed in the context of Koopman operator methods, can be used to model the CG dynamics and evaluate its kinetic properties, such as transition timescales. Finally, we combine our approach with thermodynamic interpolation (TI), a generative approach to transform samples between thermodynamic conditions, to extend the scope of the approach across thermodynamic states without repeated numerical simulations. Using a two-dimensional model system, we demonstrate that the proposed method allows to accurately capture the thermodynamic and kinetic properties of the full-space model.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Over-the-Air Federated Learning: Rethinking Edge AI Through Signal Processing</title>
<link>https://arxiv.org/abs/2512.03719</link>
<guid>https://arxiv.org/abs/2512.03719</guid>
<content:encoded><![CDATA[
arXiv:2512.03719v1 Announce Type: cross 
Abstract: Over-the-Air Federated Learning (AirFL) is an emerging paradigm that tightly integrates wireless signal processing and distributed machine learning to enable scalable AI at the network edge. By leveraging the superposition property of wireless signals, AirFL performs communication and model aggregation of the learning process simultaneously, significantly reducing latency, bandwidth, and energy consumption. This article offers a tutorial treatment of AirFL, presenting a novel classification into three design approaches: CSIT-aware, blind, and weighted AirFL. We provide a comprehensive guide to theoretical foundations, performance analysis, complexity considerations, practical limitations, and prospective research directions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Colored Markov Random Fields for Probabilistic Topological Modeling</title>
<link>https://arxiv.org/abs/2512.03727</link>
<guid>https://arxiv.org/abs/2512.03727</guid>
<content:encoded><![CDATA[
arXiv:2512.03727v1 Announce Type: cross 
Abstract: Probabilistic Graphical Models (PGMs) encode conditional dependencies among random variables using a graph -nodes for variables, links for dependencies- and factorize the joint distribution into lower-dimensional components. This makes PGMs well-suited for analyzing complex systems and supporting decision-making. Recent advances in topological signal processing highlight the importance of variables defined on topological spaces in several application domains. In such cases, the underlying topology shapes statistical relationships, limiting the expressiveness of canonical PGMs. To overcome this limitation, we introduce Colored Markov Random Fields (CMRFs), which model both conditional and marginal dependencies among Gaussian edge variables on topological spaces, with a theoretical foundation in Hodge theory. CMRFs extend classical Gaussian Markov Random Fields by including link coloring: connectivity encodes conditional independence, while color encodes marginal independence. We quantify the benefits of CMRFs through a distributed estimation case study over a physical network, comparing it with baselines with different levels of topological prior.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) International Space Station Astrobee Testing</title>
<link>https://arxiv.org/abs/2512.03729</link>
<guid>https://arxiv.org/abs/2512.03729</guid>
<content:encoded><![CDATA[
arXiv:2512.03729v1 Announce Type: cross 
Abstract: The US Naval Research Laboratory's (NRL's) Autonomous Planning In-space Assembly Reinforcement-learning free-flYer (APIARY) experiment pioneers the use of reinforcement learning (RL) for control of free-flying robots in the zero-gravity (zero-G) environment of space. On Tuesday, May 27th 2025 the APIARY team conducted the first ever, to our knowledge, RL control of a free-flyer in space using the NASA Astrobee robot on-board the International Space Station (ISS). A robust 6-degrees of freedom (DOF) control policy was trained using an actor-critic Proximal Policy Optimization (PPO) network within the NVIDIA Isaac Lab simulation environment, randomizing over goal poses and mass distributions to enhance robustness. This paper details the simulation testing, ground testing, and flight validation of this experiment. This on-orbit demonstration validates the transformative potential of RL for improving robotic autonomy, enabling rapid development and deployment (in minutes to hours) of tailored behaviors for space exploration, logistics, and real-time mission needs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crossing the Sim2Real Gap Between Simulation and Ground Testing to Space Deployment of Autonomous Free-flyer Control</title>
<link>https://arxiv.org/abs/2512.03736</link>
<guid>https://arxiv.org/abs/2512.03736</guid>
<content:encoded><![CDATA[
arXiv:2512.03736v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) offers transformative potential for robotic control in space. We present the first on-orbit demonstration of RL-based autonomous control of a free-flying robot, the NASA Astrobee, aboard the International Space Station (ISS). Using NVIDIA's Omniverse physics simulator and curriculum learning, we trained a deep neural network to replace Astrobee's standard attitude and translation control, enabling it to navigate in microgravity. Our results validate a novel training pipeline that bridges the simulation-to-reality (Sim2Real) gap, utilizing a GPU-accelerated, scientific-grade simulation environment for efficient Monte Carlo RL training. This successful deployment demonstrates the feasibility of training RL policies terrestrially and transferring them to space-based applications. This paves the way for future work in In-Space Servicing, Assembly, and Manufacturing (ISAM), enabling rapid on-orbit adaptation to dynamic mission requirements.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-embodied Co-design for Dexterous Hands</title>
<link>https://arxiv.org/abs/2512.03743</link>
<guid>https://arxiv.org/abs/2512.03743</guid>
<content:encoded><![CDATA[
arXiv:2512.03743v1 Announce Type: cross 
Abstract: Dexterous manipulation is limited by both control and design, without consensus as to what makes manipulators best for performing dexterous tasks. This raises a fundamental challenge: how should we design and control robot manipulators that are optimized for dexterity? We present a co-design framework that learns task-specific hand morphology and complementary dexterous control policies. The framework supports 1) an expansive morphology search space including joint, finger, and palm generation, 2) scalable evaluation across the wide design space via morphology-conditioned cross-embodied control, and 3) real-world fabrication with accessible components. We evaluate the approach across multiple dexterous tasks, including in-hand rotation with simulation and real deployment. Our framework enables an end-to-end pipeline that can design, train, fabricate, and deploy a new robotic hand in under 24 hours. The full framework will be open-sourced and available on our website.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Principled RL for Diffusion LLMs Emerges from a Sequence-Level Perspective</title>
<link>https://arxiv.org/abs/2512.03759</link>
<guid>https://arxiv.org/abs/2512.03759</guid>
<content:encoded><![CDATA[
arXiv:2512.03759v1 Announce Type: cross 
Abstract: Reinforcement Learning (RL) has proven highly effective for autoregressive language models, but adapting these methods to diffusion large language models (dLLMs) presents fundamental challenges. The core difficulty lies in likelihood approximation: while autoregressive models naturally provide token-level conditional probabilities essential for token-level RL objectives (e.g., GRPO), dLLMs generate sequences through iterative non-autoregressive denoising steps that lack this factorization. To address this fundamental mismatch, we propose ELBO-based Sequence-level Policy Optimization (ESPO), a principled RL framework that treats entire sequence generation as a single action and uses the ELBO as a tractable sequence-level likelihood proxy. Our method incorporates per-token normalization of importance ratios and robust KL-divergence estimation to ensure stable large-scale training. Extensive experiments on mathematical reasoning, coding, and planning tasks demonstrate that ESPO significantly outperforms token-level baselines, achieving dramatic improvements of 20-40 points on the Countdown task, while maintaining consistent gains on math and coding benchmarks. Our approach establishes sequence-level optimization as a principled and empirically effective paradigm for RL in dLLMs. Our code is available at https://github.com/ML-GSAI/ESPO.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Representation Hijacking</title>
<link>https://arxiv.org/abs/2512.03771</link>
<guid>https://arxiv.org/abs/2512.03771</guid>
<content:encoded><![CDATA[
arXiv:2512.03771v1 Announce Type: cross 
Abstract: We introduce \textbf{Doublespeak}, a simple \emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \textit{bomb}) with a benign token (e.g., \textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AdaptVision: Efficient Vision-Language Models via Adaptive Visual Acquisition</title>
<link>https://arxiv.org/abs/2512.03794</link>
<guid>https://arxiv.org/abs/2512.03794</guid>
<content:encoded><![CDATA[
arXiv:2512.03794v1 Announce Type: cross 
Abstract: Vision-Language Models (VLMs) have achieved remarkable success in visual question answering tasks, but their reliance on large numbers of visual tokens introduces significant computational overhead. While existing efficient VLM approaches reduce visual tokens through fixed-ratio compression, they operate passively and lack the ability to adapt to varying task requirements. This motivates a fundamental question: Can VLMs autonomously determine the minimum number of visual tokens required for each sample? Inspired by human active vision mechanisms, we introduce AdaptVision, an efficient VLM paradigm that enables adaptive visual token acquisition through a coarse-to-fine approach. Our model initially processes compressed visual tokens from low-resolution images and selectively acquires additional visual information by invoking a bounding box tool to crop key regions when necessary. We train AdaptVision using a reinforcement learning framework that carefully balances accuracy and efficiency. Central to our approach is Decoupled Turn Policy Optimization (DTPO), which decouples the learning objective into two components: (1) tool learning, which optimizes correct tool utilization, and (2) accuracy improvement, which refines the generated responses to improve answer correctness. Based on this formulation, we further decouple advantage estimation by computing separate advantages for tokens associated with each objective. This formulation enables more effective optimization for AdaptVision compared to vanilla GRPO. Comprehensive experiments across multiple VQA benchmarks demonstrate that AdaptVision achieves superior performance while consuming substantially fewer visual tokens than state-of-the-art efficient VLM methods.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HieroGlyphTranslator: Automatic Recognition and Translation of Egyptian Hieroglyphs to English</title>
<link>https://arxiv.org/abs/2512.03817</link>
<guid>https://arxiv.org/abs/2512.03817</guid>
<content:encoded><![CDATA[
arXiv:2512.03817v1 Announce Type: cross 
Abstract: Egyptian hieroglyphs, the ancient Egyptian writing system, are composed entirely of drawings. Translating these glyphs into English poses various challenges, including the fact that a single glyph can have multiple meanings. Deep learning translation applications are evolving rapidly, producing remarkable results that significantly impact our lives. In this research, we propose a method for the automatic recognition and translation of ancient Egyptian hieroglyphs from images to English. This study utilized two datasets for classification and translation: the Morris Franken dataset and the EgyptianTranslation dataset. Our approach is divided into three stages: segmentation (using Contour and Detectron2), mapping symbols to Gardiner codes, and translation (using the CNN model). The model achieved a BLEU score of 42.2, a significant result compared to previous research.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparison of neural network training strategies for the simulation of dynamical systems</title>
<link>https://arxiv.org/abs/2512.03851</link>
<guid>https://arxiv.org/abs/2512.03851</guid>
<content:encoded><![CDATA[
arXiv:2512.03851v1 Announce Type: cross 
Abstract: Neural networks have become a widely adopted tool for modeling nonlinear dynamical systems from data. However, the choice of training strategy remains a key design decision, particularly for simulation tasks. This paper compares two predominant strategies: parallel and series-parallel training. The conducted empirical analysis spans five neural network architectures and two examples: a pneumatic valve test bench and an industrial robot benchmark. The study reveals that, even though series-parallel training dominates current practice, parallel training consistently yields better long-term prediction accuracy. Additionally, this work clarifies the often inconsistent terminology in the literature and relate both strategies to concepts from system identification. The findings suggest that parallel training should be considered the default training strategy for neural network-based simulation of dynamical systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniDexVLG: Learning Dexterous Grasp Generation from Vision Language Model-Guided Grasp Semantics, Taxonomy and Functional Affordance</title>
<link>https://arxiv.org/abs/2512.03874</link>
<guid>https://arxiv.org/abs/2512.03874</guid>
<content:encoded><![CDATA[
arXiv:2512.03874v1 Announce Type: cross 
Abstract: Dexterous grasp generation aims to produce grasp poses that align with task requirements and human interpretable grasp semantics. However, achieving semantically controllable dexterous grasp synthesis remains highly challenging due to the lack of unified modeling of multiple semantic dimensions, including grasp taxonomy, contact semantics, and functional affordance. To address these limitations, we present OmniDexVLG, a multimodal, semantics aware grasp generation framework capable of producing structurally diverse and semantically coherent dexterous grasps under joint language and visual guidance. Our approach begins with OmniDexDataGen, a semantic rich dexterous grasp dataset generation pipeline that integrates grasp taxonomy guided configuration sampling, functional affordance contact point sampling, taxonomy aware differential force closure grasp sampling, and physics based optimization and validation, enabling systematic coverage of diverse grasp types. We further introduce OmniDexReasoner, a multimodal grasp type semantic reasoning module that leverages multi agent collaboration, retrieval augmented generation, and chain of thought reasoning to infer grasp related semantics and generate high quality annotations that align language instructions with task specific grasp intent. Building upon these components, we develop a unified Vision Language Grasping generation model that explicitly incorporates grasp taxonomy, contact structure, and functional affordance semantics, enabling fine grained control over grasp synthesis from natural language instructions. Extensive experiments in simulation and real world object grasping and ablation studies demonstrate that our method substantially outperforms state of the art approaches in terms of grasp diversity, contact semantic diversity, functional affordance diversity, and semantic consistency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Digital Twin-based Control Co-Design of Full Vehicle Active Suspensions via Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.03891</link>
<guid>https://arxiv.org/abs/2512.03891</guid>
<content:encoded><![CDATA[
arXiv:2512.03891v1 Announce Type: cross 
Abstract: Active suspension systems are critical for enhancing vehicle comfort, safety, and stability, yet their performance is often limited by fixed hardware designs and control strategies that cannot adapt to uncertain and dynamic operating conditions. Recent advances in digital twins (DTs) and deep reinforcement learning (DRL) offer new opportunities for real-time, data-driven optimization across a vehicle's lifecycle. However, integrating these technologies into a unified framework remains an open challenge. This work presents a DT-based control co-design (CCD) framework for full-vehicle active suspensions using multi-generation design concepts. By integrating automatic differentiation into DRL, we jointly optimize physical suspension components and control policies under varying driver behaviors and environmental uncertainties. DRL also addresses the challenge of partial observability, where only limited states can be sensed and fed back to the controller, by learning optimal control actions directly from available sensor information. The framework incorporates model updating with quantile learning to capture data uncertainty, enabling real-time decision-making and adaptive learning from digital-physical interactions. The approach demonstrates personalized optimization of suspension systems under two distinct driving settings (mild and aggressive). Results show that the optimized systems achieve smoother trajectories and reduce control efforts by approximately 43% and 52% for mild and aggressive, respectively, while maintaining ride comfort and stability. Contributions include: developing a DT-enabled CCD framework integrating DRL and uncertainty-aware model updating for full-vehicle active suspensions, introducing a multi-generation design strategy for self-improving systems, and demonstrating personalized optimization of active suspension systems for distinct driver types.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Autonomous Reinforcement Learning Robot Control with Intel's Loihi 2 Neuromorphic Hardware</title>
<link>https://arxiv.org/abs/2512.03911</link>
<guid>https://arxiv.org/abs/2512.03911</guid>
<content:encoded><![CDATA[
arXiv:2512.03911v1 Announce Type: cross 
Abstract: We present an end-to-end pipeline for deploying reinforcement learning (RL) trained Artificial Neural Networks (ANNs) on neuromorphic hardware by converting them into spiking Sigma-Delta Neural Networks (SDNNs). We demonstrate that an ANN policy trained entirely in simulation can be transformed into an SDNN compatible with Intel's Loihi 2 architecture, enabling low-latency and energy-efficient inference. As a test case, we use an RL policy for controlling the Astrobee free-flying robot, similar to a previously hardware in space-validated controller. The policy, trained with Rectified Linear Units (ReLUs), is converted to an SDNN and deployed on Intel's Loihi 2, then evaluated in NVIDIA's Omniverse Isaac Lab simulation environment for closed-loop control of Astrobee's motion. We compare execution performance between GPU and Loihi 2. The results highlight the feasibility of using neuromorphic platforms for robotic control and establish a pathway toward energy-efficient, real-time neuromorphic computation in future space and terrestrial robotics applications.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Theoretical Framework for Auxiliary-Loss-Free Load Balancing of Sparse Mixture-of-Experts in Large-Scale AI Models</title>
<link>https://arxiv.org/abs/2512.03915</link>
<guid>https://arxiv.org/abs/2512.03915</guid>
<content:encoded><![CDATA[
arXiv:2512.03915v1 Announce Type: cross 
Abstract: In large-scale AI training, Sparse Mixture-of-Experts (s-MoE) layers enable scaling by activating only a small subset of experts per token. An operational challenge in this design is load balancing: routing tokens to minimize the number of idle experts, which is important for the efficient utilization of (costly) GPUs. We provide a theoretical framework for analyzing the Auxiliary-Loss-Free Load Balancing (ALF-LB) procedure -- proposed by DeepSeek's Wang et al. (2024) -- by casting it as a one-step-per-iteration primal-dual method for an assignment problem. First, in a stylized deterministic setting, our framework yields several insightful structural properties: (i) a monotonic improvement of a Lagrangian objective, (ii) a preference rule that moves tokens from overloaded to underloaded experts, and (iii) an approximate-balancing guarantee. Then, we incorporate the stochastic and dynamic nature of AI training using a generalized online optimization formulation. In the online setting, we derive a strong convexity property of the objective that leads to a logarithmic expected regret bound under certain step-size choices. Additionally, we present real experiments on 1B-parameter DeepSeekMoE models to complement our theoretical findings. Together, these results build a principled framework for analyzing the Auxiliary-Loss-Free Load Balancing of s-MoE in AI models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tada-DIP: Input-adaptive Deep Image Prior for One-shot 3D Image Reconstruction</title>
<link>https://arxiv.org/abs/2512.03962</link>
<guid>https://arxiv.org/abs/2512.03962</guid>
<content:encoded><![CDATA[
arXiv:2512.03962v1 Announce Type: cross 
Abstract: Deep Image Prior (DIP) has recently emerged as a promising one-shot neural-network based image reconstruction method. However, DIP has seen limited application to 3D image reconstruction problems. In this work, we introduce Tada-DIP, a highly effective and fully 3D DIP method for solving 3D inverse problems. By combining input-adaptation and denoising regularization, Tada-DIP produces high-quality 3D reconstructions while avoiding the overfitting phenomenon that is common in DIP. Experiments on sparse-view X-ray computed tomography reconstruction validate the effectiveness of the proposed method, demonstrating that Tada-DIP produces much better reconstructions than training-data-free baselines and achieves reconstruction performance on par with a supervised network trained using a large dataset with fully-sampled volumes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Refining Machine Learning Potentials through Thermodynamic Theory of Phase Transitions</title>
<link>https://arxiv.org/abs/2512.03974</link>
<guid>https://arxiv.org/abs/2512.03974</guid>
<content:encoded><![CDATA[
arXiv:2512.03974v1 Announce Type: cross 
Abstract: Foundational Machine Learning Potentials can resolve the accuracy and transferability limitations of classical force fields. They enable microscopic insights into material behavior through Molecular Dynamics simulations, which can crucially expedite material design and discovery. However, insufficiently broad and systematically biased reference data affect the predictive quality of the learned models. Often, these models exhibit significant deviations from experimentally observed phase transition temperatures, in the order of several hundred kelvins. Thus, fine-tuning is necessary to achieve adequate accuracy in many practical problems. This work proposes a fine-tuning strategy via top-down learning, directly correcting the wrongly predicted transition temperatures to match the experimental reference data. Our approach leverages the Differentiable Trajectory Reweighting algorithm to minimize the free energy differences between phases at the experimental target pressures and temperatures. We demonstrate that our approach can accurately correct the phase diagram of pure Titanium in a pressure range of up to 5 GPa, matching the experimental reference within tenths of kelvins and improving the liquid-state diffusion constant. Our approach is model-agnostic, applicable to multi-component systems with solid-solid and solid-liquid transitions, and compliant with top-down training on other experimental properties. Therefore, our approach can serve as an essential step towards highly accurate application-specific and foundational machine learning potentials.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Divide, then Ground: Adapting Frame Selection to Query Types for Long-Form Video Understanding</title>
<link>https://arxiv.org/abs/2512.04000</link>
<guid>https://arxiv.org/abs/2512.04000</guid>
<content:encoded><![CDATA[
arXiv:2512.04000v1 Announce Type: cross 
Abstract: The application of Large Multimodal Models (LMMs) to long-form video understanding is constrained by limited context lengths and the computationally prohibitive cost of processing dense video tokens. Consequently, recent research has focused on query-aware frame selection, methods that often incur significant computational overhead. This paper challenges the assumption that such complex search mechanisms are universally necessary. We first identify and validate a query typology distinguishing between global query and localized query. We demonstrate that while uniform sampling is both effective and efficient for global queries, localized queries indeed necessitate query-aware selection for optimal performance. Building on this insight, we propose DIG, a training-free frame selection framework that adapts its strategy based on the query type. Specifically,DIG employs efficient uniform sampling for global queries while activating a specialized pipeline to extract query-relevant frames for localized queries. Experiments on three long-form video understanding benchmarks demonstrate that DIG consistently outperforms existing baselines and robustly improves LMM performance, even when scaling the input frame count to 256.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation</title>
<link>https://arxiv.org/abs/2512.04025</link>
<guid>https://arxiv.org/abs/2512.04025</guid>
<content:encoded><![CDATA[
arXiv:2512.04025v1 Announce Type: cross 
Abstract: Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast &amp; Efficient Normalizing Flows and Applications of Image Generative Models</title>
<link>https://arxiv.org/abs/2512.04039</link>
<guid>https://arxiv.org/abs/2512.04039</guid>
<content:encoded><![CDATA[
arXiv:2512.04039v1 Announce Type: cross 
Abstract: This thesis presents novel contributions in two primary areas: advancing the efficiency of generative models, particularly normalizing flows, and applying generative models to solve real-world computer vision challenges. The first part introduce significant improvements to normalizing flow architectures through six key innovations: 1) Development of invertible 3x3 Convolution layers with mathematically proven necessary and sufficient conditions for invertibility, (2) introduction of a more efficient Quad-coupling layer, 3) Design of a fast and efficient parallel inversion algorithm for kxk convolutional layers, 4) Fast & efficient backpropagation algorithm for inverse of convolution, 5) Using inverse of convolution, in Inverse-Flow, for the forward pass and training it using proposed backpropagation algorithm, and 6) Affine-StableSR, a compact and efficient super-resolution model that leverages pre-trained weights and Normalizing Flow layers to reduce parameter count while maintaining performance.
  The second part: 1) An automated quality assessment system for agricultural produce using Conditional GANs to address class imbalance, data scarcity and annotation challenges, achieving good accuracy in seed purity testing; 2) An unsupervised geological mapping framework utilizing stacked autoencoders for dimensionality reduction, showing improved feature extraction compared to conventional methods; 3) We proposed a privacy preserving method for autonomous driving datasets using on face detection and image inpainting; 4) Utilizing Stable Diffusion based image inpainting for replacing the detected face and license plate to advancing privacy-preserving techniques and ethical considerations in the field.; and 5) An adapted diffusion model for art restoration that effectively handles multiple types of degradation through unified fine-tuning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Closing the problem of which causal structures of up to six total nodes have a classical-quantum gap</title>
<link>https://arxiv.org/abs/2512.04058</link>
<guid>https://arxiv.org/abs/2512.04058</guid>
<content:encoded><![CDATA[
arXiv:2512.04058v1 Announce Type: cross 
Abstract: The discovery of Bell that there exist quantum correlations that cannot be reproduced classically is one of the most important in the foundations of quantum mechanics, as well as having practical implications. Bell's result was originally proven in a simple bipartite causal structure, but analogous results have also been shown in further causal structures. Here we study the only causal structure with six or fewer nodes in which the question of whether or not there exist quantum correlations that cannot be achieved classically was open. In this causal structure we show that such quantum correlations exist using a method that involves imposing additional restrictions on the correlations. This hence completes the picture of which causal structures of up to six nodes support non-classical quantum correlations. We also provide further illustrations of our method using other causal structures.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Marginalize, Rather than Impute: Probabilistic Wind Power Forecasting with Incomplete Data</title>
<link>https://arxiv.org/abs/2403.03631</link>
<guid>https://arxiv.org/abs/2403.03631</guid>
<content:encoded><![CDATA[
arXiv:2403.03631v2 Announce Type: replace 
Abstract: Machine learning methods are widely and successfully used for probabilistic wind power forecasting, yet the pervasive issue of missing values (e.g., due to sensor faults or communication outages) has received limited attention. The prevailing practice is impute-then-predict, but conditioning on point imputations biases parameter estimates and fails to propagate uncertainty from missing features. Our approach treats missing features and forecast targets uniformly: we learn a joint generative model of features and targets from incomplete data and, at operational deployment, condition on the observed features and marginalize the unobserved ones to produce forecasts. This imputation-free procedure avoids error introduced by imputation and preserves uncertainty aroused from missing features. In experiments, it improves forecast quality in terms of continuous ranked probability score relative to impute-then-predict baselines while incurring substantially lower computational cost than common alternatives.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fairness Interventions: A Study in AI Explainability</title>
<link>https://arxiv.org/abs/2407.14766</link>
<guid>https://arxiv.org/abs/2407.14766</guid>
<content:encoded><![CDATA[
arXiv:2407.14766v3 Announce Type: replace 
Abstract: This paper presents a philosophical and experimental study of fairness interventions in AI classification, centered on the explainability of corrective methods. We argue that ensuring fairness requires not only satisfying a target criterion, but also explaining which variables constrain its realization. When corrections are used to mitigate advantage transparently, they must remain sensitive to the distribution of true labels. To illustrate this approach, we built FairDream, a fairness package whose mechanism is made transparent for lay users, increasing the model's weights of errors on disadvantaged groups. While a user may intend to achieve Demographic Parity by the correction method, experiments show that FairDream tends towards Equalized Odds, revealing a conservative bias inherent to the data environment. We clarify the relationship between these fairness criteria, analyze FairDream's reweighting process, and compare its trade-offs with closely related GridSearch models. Finally, we justify the normative preference for Equalized Odds via an epistemological interpretation of the results, using their proximity with Simpson's paradox. The paper thus unites normative, epistemological, and empirical explanations of fairness interventions, to ensure transparency for the users.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Why Rectified Power Unit Networks Fail and How to Improve It: An Effective Field Theory Perspective</title>
<link>https://arxiv.org/abs/2408.02697</link>
<guid>https://arxiv.org/abs/2408.02697</guid>
<content:encoded><![CDATA[
arXiv:2408.02697v4 Announce Type: replace 
Abstract: The Rectified Power Unit (RePU) activation function, a differentiable generalization of the Rectified Linear Unit (ReLU), has shown promise in constructing neural networks due to its smoothness properties. However, deep RePU networks often suffer from critical issues such as vanishing or exploding values during training, rendering them unstable regardless of hyperparameter initialization. Leveraging the perspective of effective field theory, we identify the root causes of these failures and propose the Modified Rectified Power Unit (MRePU) activation function. MRePU addresses RePU's limitations while preserving its advantages, such as differentiability and universal approximation properties. Theoretical analysis demonstrates that MRePU satisfies criticality conditions necessary for stable training, placing it in a distinct universality class. Extensive experiments validate the effectiveness of MRePU, showing significant improvements in training stability and performance across various tasks, including polynomial regression, physics-informed neural networks (PINNs) and real-world vision tasks. Our findings highlight the potential of MRePU as a robust alternative for building deep neural networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Concentration of Cumulative Reward in Markov Decision Processes</title>
<link>https://arxiv.org/abs/2411.18551</link>
<guid>https://arxiv.org/abs/2411.18551</guid>
<content:encoded><![CDATA[
arXiv:2411.18551v2 Announce Type: replace 
Abstract: In this paper, we investigate the concentration properties of cumulative reward in Markov Decision Processes (MDPs), focusing on both asymptotic and non-asymptotic settings. We introduce a unified approach to characterize reward concentration in MDPs, covering both infinite-horizon settings (i.e., average and discounted reward frameworks) and finite-horizon setting. Our asymptotic results include the law of large numbers, the central limit theorem, and the law of iterated logarithms, while our non-asymptotic bounds include Azuma-Hoeffding-type inequalities and a non-asymptotic version of the law of iterated logarithms. Additionally, we explore two key implications of our results. First, we analyze the sample path behavior of the difference in rewards between any two stationary policies. Second, we show that two alternative definitions of regret for learning policies proposed in the literature are rate-equivalent. Our proof techniques rely on a martingale decomposition of cumulative reward, properties of the solution to the policy evaluation fixed-point equation, and both asymptotic and non-asymptotic concentration results for martingale difference sequences.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Transductive Conformal Inference for Full Ranking</title>
<link>https://arxiv.org/abs/2501.11384</link>
<guid>https://arxiv.org/abs/2501.11384</guid>
<content:encoded><![CDATA[
arXiv:2501.11384v3 Announce Type: replace 
Abstract: We introduce a method based on Conformal Prediction (CP) to quantify the uncertainty of full ranking algorithms. We focus on a specific scenario where $n+m$ items are to be ranked by some ``black box'' algorithm. It is assumed that the relative (ground truth) ranking of $n$ of them is known. The objective is then to quantify the error made by the algorithm on the ranks of the $m$ new items among the total $(n+m)$. In such a setting, the true ranks of the $n$ original items in the total $(n+m)$ depend on the (unknown) true ranks of the $m$ new ones. Consequently, we have no direct access to a calibration set to apply a classical CP method. To address this challenge, we propose to construct distribution-free bounds of the unknown conformity scores using recent results on the distribution of conformal p-values. Using these scores upper bounds, we provide valid prediction sets for the rank of any item. We also control the false coverage proportion, a crucial quantity when dealing with multiple prediction sets. Finally, we empirically show on both synthetic and real data the efficiency of our CP method for state-of-the-art algorithms such as RankNet or LambdaMart.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Training Scaling Laws for Chemical Exploration in Drug Design</title>
<link>https://arxiv.org/abs/2501.19153</link>
<guid>https://arxiv.org/abs/2501.19153</guid>
<content:encoded><![CDATA[
arXiv:2501.19153v4 Announce Type: replace 
Abstract: Chemical Language Models (CLMs) leveraging reinforcement learning (RL) have shown promise in de novo molecular design, yet often suffer from mode collapse, limiting their exploration capabilities. Inspired by Test-Time Training (TTT) in large language models, we propose scaling TTT for CLMs to enhance chemical space exploration. We introduce MolExp, a novel benchmark emphasizing the discovery of structurally diverse molecules with similar bioactivity, simulating real-world drug design challenges. Our results demonstrate that scaling TTT by increasing the number of independent RL agents follows a log-linear scaling law, significantly improving exploration efficiency as measured by MolExp. In contrast, increasing TTT training time yields diminishing returns, even with exploration bonuses. We further evaluate cooperative RL strategies to enhance exploration efficiency. These findings provide a scalable framework for generative molecular design, offering insights into optimizing AI-driven drug discovery.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Filtration-Based Representation Learning for Temporal Graphs</title>
<link>https://arxiv.org/abs/2502.10076</link>
<guid>https://arxiv.org/abs/2502.10076</guid>
<content:encoded><![CDATA[
arXiv:2502.10076v2 Announce Type: replace 
Abstract: In this work, we introduce a filtration on temporal graphs based on $\delta$-temporal motifs (recurrent subgraphs), yielding a multi-scale representation of temporal structure. Our temporal filtration allows tools developed for filtered static graphs, including persistent homology and recent graph filtration kernels, to be applied directly to temporal graph analysis. We demonstrate the effectiveness of this approach on temporal graph classification tasks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet</title>
<link>https://arxiv.org/abs/2502.16116</link>
<guid>https://arxiv.org/abs/2502.16116</guid>
<content:encoded><![CDATA[
arXiv:2502.16116v2 Announce Type: replace 
Abstract: Short-term precipitation nowcasting is essential for flood management, transportation, energy system operations, and emergency response. However, many existing models fail to fully exploit the extensive atmospheric information available, relying primarily on precipitation data alone. This study examines whether integrating multi-variable weather-station measurements with radar can enhance nowcasting skill and introduces two complementary architectures that integrate multi-variable station data with radar images. The SmaAt-fUsion model extends the SmaAt-UNet framework by incorporating weather station data through a convolutional layer, integrating it into the bottleneck of the network; The SmaAt-Krige-GNet model combines precipitation maps with weather station data processed using Kriging, a geo-statistical interpolation method, to generate variable-specific maps. These maps are then utilized in a dual-encoder architecture based on SmaAt-GNet, allowing multi-level data integration . Experimental evaluations were conducted using four years (2016--2019) of weather station and precipitation radar data from the Netherlands. Results demonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, which relies solely on precipitation radar data, in low precipitation scenarios, while SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitation scenarios. This highlights the potential of incorporating discrete weather station data to enhance the performance of deep learning-based weather nowcasting models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding the Limits of Deep Tabular Methods with Temporal Shift</title>
<link>https://arxiv.org/abs/2502.20260</link>
<guid>https://arxiv.org/abs/2502.20260</guid>
<content:encoded><![CDATA[
arXiv:2502.20260v2 Announce Type: replace 
Abstract: Deep tabular models have demonstrated remarkable success on i.i.d. data, excelling in a variety of structured data tasks. However, their performance often deteriorates under temporal distribution shifts, where trends and periodic patterns are present in the evolving data distribution over time. In this paper, we explore the underlying reasons for this failure in capturing temporal dependencies. We begin by investigating the training protocol, revealing a key issue in how model selection performs. While existing approaches use temporal ordering for splitting validation set, we show that even a random split can significantly improve model performance. By minimizing the time lag between training data and test time, while reducing the bias in validation, our proposed training protocol significantly improves generalization across various methods. Furthermore, we analyze how temporal data affects deep tabular representations, uncovering that these models often fail to capture crucial periodic and trend information. To address this gap, we introduce a plug-and-play temporal embedding method based on Fourier series expansion to learn and incorporate temporal patterns, offering an adaptive approach to handle temporal shifts. Our experiments demonstrate that this temporal embedding, combined with the improved training protocol, provides a more effective and robust framework for learning from temporal tabular data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Trajectory Balance with Asynchrony: Decoupling Exploration and Learning for Fast, Scalable LLM Post-Training</title>
<link>https://arxiv.org/abs/2503.18929</link>
<guid>https://arxiv.org/abs/2503.18929</guid>
<content:encoded><![CDATA[
arXiv:2503.18929v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) is a critical component of large language model (LLM) post-training. However, on-policy algorithms used for post-training are not naturally robust to a diversified content of experience replay buffers, which asynchronous off-policy actors can efficiently populate in parallel to training. We propose efficiently learning on such off-policy data via Trajectory Balance with Asynchrony (TBA), an approach to asynchronous RL for LLMs that leverages the principled off-policy TB objective. On math, preference-tuning, and automated red-teaming tasks, we post-train models ranging from Pythia 410M to Qwen 2.5 7B, finding TBA offers speed and performance boosts over strong baselines like Online DPO and Dr. GRPO. Beyond TBA's performance benefits (high accuracy even as asynchrony grows) and speedups ($4\times$ or more), we show its reward- and recency-prioritizing sampling enable further gains as data generation is scaled. Our code is available at https://github.com/bbartoldson/TBA.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Energy-Conserving Neural Network Closure Model for Long-Time Accurate and Stable LES</title>
<link>https://arxiv.org/abs/2504.05868</link>
<guid>https://arxiv.org/abs/2504.05868</guid>
<content:encoded><![CDATA[
arXiv:2504.05868v2 Announce Type: replace 
Abstract: Machine learning-based closure models for LES have shown promise in capturing complex turbulence dynamics but often suffer from instabilities and physical inconsistencies. In this work, we develop a novel skew-symmetric neural architecture as closure model that enforces stability while preserving key physical conservation laws. Our approach leverages a discretization that ensures mass, momentum, and energy conservation, along with a face-averaging filter to maintain mass conservation in coarse-grained velocity fields. We compare our model against several conventional data-driven closures (including unconstrained convolutional neural networks), and the physics-based Smagorinsky model. Performance is evaluated on decaying turbulence and Kolmogorov flow for multiple coarse-graining factors. In these test cases we observe that unconstrained machine learning models suffer from numerical instabilities. In contrast, our skew-symmetric model remains stable across all tests, though at the cost of increased dissipation. Despite this trade-off, we demonstrate that our model still outperforms the Smagorinsky model in unseen scenarios. These findings highlight the potential of structure-preserving machine learning closures for reliable long-time LES.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Distance to Direction: Structure-aware Label-specific Feature Fusion for Label Distribution Learning</title>
<link>https://arxiv.org/abs/2504.19374</link>
<guid>https://arxiv.org/abs/2504.19374</guid>
<content:encoded><![CDATA[
arXiv:2504.19374v2 Announce Type: replace 
Abstract: Label distribution learning (LDL) is an emerging learning paradigm designed to capture the relative importance of labels for each instance. Label-specific features (LSFs), constructed by LIFT, have proven effective for learning tasks with label ambiguity by leveraging clustering-based prototypes for each label to re-characterize instances. However, directly introducing LIFT into LDL tasks can be suboptimal, as the prototypes it collects primarily reflect intra-cluster relationships while neglecting cross-cluster interactions. Additionally, constructing LSFs using multi-perspective information, rather than relying solely on Euclidean distance, provides a more robust and comprehensive representation of instances, mitigating noise and bias that may arise from a single distance perspective. To address these limitations, we introduce Structural Anchor Points (SAPs) to capture inter-cluster interactions. This leads to a novel LSFs construction strategy, LIFT-SAP, which enhances LIFT by integrating both distance and directional information of each instance relative to SAPs. Furthermore, we propose a novel LDL algorithm, Label Distribution Learning via Label-specifIc FeaTure with SAPs (LDL-LIFT-SAP), which unifies multiple label description degrees predicted from different LSF spaces into a cohesive label distribution. Extensive experiments on 15 real-world datasets demonstrate the effectiveness of LIFT-SAP over LIFT, as well as the superiority of LDL-LIFT-SAP compared to seven other well-established algorithms.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges and Limitations of Generative AI in Synthesizing Wearable Sensor Data</title>
<link>https://arxiv.org/abs/2505.14206</link>
<guid>https://arxiv.org/abs/2505.14206</guid>
<content:encoded><![CDATA[
arXiv:2505.14206v2 Announce Type: replace 
Abstract: The widespread adoption of wearable sensors has the potential to provide massive and heterogeneous time series data, driving the use of Artificial Intelligence in human sensing applications. However, data collection remains limited due to stringent ethical regulations, privacy concerns, and other constraints, hindering progress in the field. Synthetic data generation, particularly through Generative Adversarial Networks and Diffusion Models, has emerged as a promising solution to mitigate both data scarcity and privacy issues. However, these models are often limited to narrow operational scenarios, such as short-term and unimodal signal patterns. To address this gap, we present a systematic evaluation of state-of-the-art generative models for time series data, explicitly assessing their performance in challenging scenarios such as stress and emotion recognition. Our study examines the extent to which these models can jointly handle multi-modality, capture long-range dependencies, and support conditional generation-core requirements for real-world wearable sensor data generation. To enable a fair and rigorous comparison, we also introduce an evaluation framework that evaluates both the intrinsic fidelity of the generated data and their utility in downstream predictive tasks. Our findings reveal critical limitations in the existing approaches, particularly in maintaining cross-modal consistency, preserving temporal coherence, and ensuring robust performance in train-on-synthetic, test-on-real, and data augmentation scenarios. Finally, we present our future research directions to enhance synthetic time series generation and improve the applicability of generative models in the wearable computing domain.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ConfRover: Simultaneous Modeling of Protein Conformation and Dynamics via Autoregression</title>
<link>https://arxiv.org/abs/2505.17478</link>
<guid>https://arxiv.org/abs/2505.17478</guid>
<content:encoded><![CDATA[
arXiv:2505.17478v2 Announce Type: replace 
Abstract: Understanding protein dynamics is critical for elucidating their biological functions. The increasing availability of molecular dynamics (MD) data enables the training of deep generative models to efficiently explore the conformational space of proteins. However, existing approaches either fail to explicitly capture the temporal dependencies between conformations or do not support direct generation of time-independent samples. To address these limitations, we introduce ConfRover, an autoregressive model that simultaneously learns protein conformation and dynamics from MD trajectories, supporting both time-dependent and time-independent sampling. At the core of our model is a modular architecture comprising: (i) an encoding layer, adapted from protein folding models, that embeds protein-specific information and conformation at each time frame into a latent space; (ii) a temporal module, a sequence model that captures conformational dynamics across frames; and (iii) an SE(3) diffusion model as the structure decoder, generating conformations in continuous space. Experiments on ATLAS, a large-scale protein MD dataset of diverse structures, demonstrate the effectiveness of our model in learning conformational dynamics and supporting a wide range of downstream tasks. ConfRover is the first model to sample both protein conformations and trajectories within a single framework, offering a novel and flexible approach for learning from protein MD data. Project website: https://bytedance-seed.github.io/ConfRover.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction with Physics-Informed Machine Learning and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
arXiv:2505.18565v5 Announce Type: replace 
Abstract: Physics-informed neural networks (PINNs) have emerged as a promising approach for solving complex fluid dynamics problems, yet their application to fluid-structure interaction (FSI) problems with moving boundaries remains largely unexplored. This work addresses the critical challenge of modeling FSI systems with moving interfaces, where traditional unified PINN architectures struggle to capture the distinct physics governing fluid and structural domains simultaneously. We present an innovative Eulerian-Lagrangian PINN architecture that integrates immersed boundary method (IBM) principles to solve FSI problems with moving boundary conditions. Our approach fundamentally departs from conventional unified architectures by introducing domain-specific neural networks: an Eulerian network for fluid dynamics and a Lagrangian network for structural interfaces, coupled through physics-based constraints. Additionally, we incorporate learnable B-spline activation functions with SiLU to capture both localized high-gradient features near interfaces and global flow patterns. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that while baseline unified PINNs achieve reasonable velocity predictions, they suffer from substantial pressure errors (12.9%) in structural regions. Our Eulerian-Lagrangian architecture with learnable activations (EL-L) achieves better performance across all metrics, improving accuracy by 24.1-91.4% and particularly reducing pressure errors from 12.9% to 2.39%. These results demonstrate that domain decomposition aligned with physical principles, combined with locality-aware activation functions, is essential for accurate FSI modeling within the PINN framework.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comba: Improving Bilinear RNNs with Closed-loop Control</title>
<link>https://arxiv.org/abs/2506.02475</link>
<guid>https://arxiv.org/abs/2506.02475</guid>
<content:encoded><![CDATA[
arXiv:2506.02475v5 Announce Type: replace 
Abstract: Recent efficient sequence modeling methods such as Gated DeltaNet, TTT, and RWKV-7 have achieved performance improvements by supervising the recurrent memory management through Delta learning rule. Unlike previous state-space models (e.g., Mamba) and gated linear attentions (e.g., GLA), these models introduce interactions between the recurrent state and the key vector, structurally resembling bilinear systems. In this paper, we first introduce the concept of Bilinear RNNs with a comprehensive analysis on the advantages and limitations of these models. Then, based on closed-loop control theory, we propose a novel Bilinear RNN variant named Comba, which adopts a scalar-plus-low-rank state transition, with both state feedback and output feedback corrections. We also implement a hardware-efficient chunk-wise parallel kernel in Triton and train models with 340M/1.3B parameters on large-scale corpus. Comba demonstrates superior performance and computation efficiency in both language and vision modeling.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AMPED: Adaptive Multi-objective Projection for balancing Exploration and skill Diversification</title>
<link>https://arxiv.org/abs/2506.05980</link>
<guid>https://arxiv.org/abs/2506.05980</guid>
<content:encoded><![CDATA[
arXiv:2506.05980v4 Announce Type: replace 
Abstract: Skill-based reinforcement learning (SBRL) enables rapid adaptation in environments with sparse rewards by pretraining a skill-conditioned policy. Effective skill learning requires jointly maximizing both exploration and skill diversity. However, existing methods often face challenges in simultaneously optimizing for these two conflicting objectives. In this work, we propose a new method, Adaptive Multi-objective Projection for balancing Exploration and skill Diversification (AMPED), which explicitly addresses both: during pre-training, a gradient-surgery projection balances the exploration and diversity gradients, and during fine-tuning, a skill selector exploits the learned diversity by choosing skills suited to downstream tasks. Our approach achieves performance that surpasses SBRL baselines across various benchmarks. Through an extensive ablation study, we identify the role of each component and demonstrate that each element in AMPED is contributing to performance. We further provide theoretical and empirical evidence that, with a greedy skill selector, greater skill diversity reduces fine-tuning sample complexity. These results highlight the importance of explicitly harmonizing exploration and diversity and demonstrate the effectiveness of AMPED in enabling robust and generalizable skill learning. Project Page: https://geonwoo.me/amped/
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLGENN: A Novel Parameter-Light Equivariant Neural Networks Architecture Based on Clifford Geometric Algebras</title>
<link>https://arxiv.org/abs/2506.09625</link>
<guid>https://arxiv.org/abs/2506.09625</guid>
<content:encoded><![CDATA[
arXiv:2506.09625v2 Announce Type: replace 
Abstract: We propose, implement, and compare with competitors a new architecture of equivariant neural networks based on geometric (Clifford) algebras: Generalized Lipschitz Group Equivariant Neural Networks (GLGENN). These networks are equivariant to all pseudo-orthogonal transformations, including rotations and reflections, of a vector space with any non-degenerate or degenerate symmetric bilinear form. We propose a weight-sharing parametrization technique that takes into account the fundamental structures and operations of geometric algebras. Due to this technique, GLGENN architecture is parameter-light and has less tendency to overfitting than baseline equivariant models. GLGENN outperforms or matches competitors on several benchmarking equivariant tasks, including estimation of an equivariant function and a convex hull experiment, while using significantly fewer optimizable parameters.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TRACED: Transition-aware Regret Approximation with Co-learnability for Environment Design</title>
<link>https://arxiv.org/abs/2506.19997</link>
<guid>https://arxiv.org/abs/2506.19997</guid>
<content:encoded><![CDATA[
arXiv:2506.19997v4 Announce Type: replace 
Abstract: Generalizing deep reinforcement learning agents to unseen environments remains a significant challenge. One promising solution is Unsupervised Environment Design (UED), a co-evolutionary framework in which a teacher adaptively generates tasks with high learning potential, while a student learns a robust policy from this evolving curriculum. Existing UED methods typically measure learning potential via regret, the gap between optimal and current performance, approximated solely by value-function loss. Building on these approaches, we introduce the transition-prediction error as an additional term in our regret approximation. To capture how training on one task affects performance on others, we further propose a lightweight metric called Co-Learnability. By combining these two measures, we present Transition-aware Regret Approximation with Co-learnability for Environment Design (TRACED). Empirical evaluations show that TRACED produces curricula that improve zero-shot generalization over strong baselines across multiple benchmarks. Ablation studies confirm that the transition-prediction error drives rapid complexity ramp-up and that Co-Learnability delivers additional gains when paired with the transition-prediction error. These results demonstrate how refined regret approximation and explicit modeling of task relationships can be leveraged for sample-efficient curriculum design in UED. Project Page: https://geonwoo.me/traced/
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoreSPECT: Enhancing Clustering Algorithms via an Interplay of Density and Geometry</title>
<link>https://arxiv.org/abs/2507.08243</link>
<guid>https://arxiv.org/abs/2507.08243</guid>
<content:encoded><![CDATA[
arXiv:2507.08243v2 Announce Type: replace 
Abstract: In this paper, we provide a novel perspective on the underlying structure of real-world data with ground-truth clusters via characterization of an abundantly observed yet often overlooked density-geometry correlation, that manifests itself as a multi-layered manifold structure.
  We leverage this correlation to design CoreSPECT (Core Space Projection based Enhancement of Clustering Techniques), a general framework that improves the performance of generic clustering algorithms. Our framework boosts the performance of clustering algorithms by applying them to strategically selected regions, then extending the partial partition to a complete partition for the dataset using a novel neighborhood graph based multi-layer propagation procedure.
  We provide initial theoretical support of the functionality of our framework under the assumption of our model, and then provide large-scale real-world experiments on 19 datasets that include standard image datasets as well as genomics datasets.
  We observe two notable improvements. First, CoreSPECT improves the NMI of K-Means by 20% on average, making it competitive to (and in some cases surpassing) the state-of-the-art manifold-based clustering algorithms, while being orders of magnitude faster.
  Secondly, our framework boosts the NMI of HDBSCAN by more than 100% on average, making it competitive to the state-of-the-art in several cases without requiring the true number of clusters and hyper-parameter tuning. The overall ARI improvements are higher.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Demystify Protein Generation with Hierarchical Conditional Diffusion Models</title>
<link>https://arxiv.org/abs/2507.18603</link>
<guid>https://arxiv.org/abs/2507.18603</guid>
<content:encoded><![CDATA[
arXiv:2507.18603v2 Announce Type: replace 
Abstract: Generating novel and functional protein sequences is critical to a wide range of applications in biology. Recent advancements in conditional diffusion models have shown impressive empirical performance in protein generation tasks. However, reliable generations of protein remain an open research question in de novo protein design, especially when it comes to conditional diffusion models. Considering the biological function of a protein is determined by multi-level structures, we propose a novel multi-level conditional diffusion model that integrates both sequence-based and structure-based information for efficient end-to-end protein design guided by specified functions. By generating representations at different levels simultaneously, our framework can effectively model the inherent hierarchical relations between different levels, resulting in an informative and discriminative representation of the generated protein. We also propose a Protein-MMD, a new reliable evaluation metric, to evaluate the quality of generated protein with conditional diffusion models. Our new metric is able to capture both distributional and functional similarities between real and generated protein sequences while ensuring conditional consistency. We experiment with the benchmark datasets, and the results on conditional protein generation tasks demonstrate the efficacy of the proposed generation framework and evaluation metric.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Right to be Forgotten in Pruning: Unveil Machine Unlearning on Sparse Models</title>
<link>https://arxiv.org/abs/2507.18725</link>
<guid>https://arxiv.org/abs/2507.18725</guid>
<content:encoded><![CDATA[
arXiv:2507.18725v2 Announce Type: replace 
Abstract: Machine unlearning aims to efficiently eliminate the memory about deleted data from trained models and address the right to be forgotten. Despite the success of existing unlearning algorithms, unlearning in sparse models has not yet been well studied. In this paper, we empirically find that the deleted data has an impact on the pruned topology in a sparse model. Motivated by the observation and the right to be forgotten, we define a new terminology ``un-pruning" to eliminate the impact of deleted data on model pruning. Then we propose an un-pruning algorithm to approximate the pruned topology driven by retained data. We remark that any existing unlearning algorithm can be integrated with the proposed un-pruning workflow and the error of un-pruning is upper-bounded in theory. Also, our un-pruning algorithm can be applied to both structured sparse models and unstructured sparse models. In the experiment, we further find that Membership Inference Attack (MIA) accuracy is unreliable for assessing whether a model has forgotten deleted data, as a small change in the amount of deleted data can produce arbitrary MIA results. Accordingly, we devise new performance metrics for sparse models to evaluate the success of un-pruning. Lastly, we conduct extensive experiments to verify the efficacy of un-pruning with various pruning methods and unlearning algorithms. Our code is released at https://github.com/NKUShaw/SparseModels .
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PCS Workflow for Veridical Data Science in the Age of AI</title>
<link>https://arxiv.org/abs/2508.00835</link>
<guid>https://arxiv.org/abs/2508.00835</guid>
<content:encoded><![CDATA[
arXiv:2508.00835v2 Announce Type: replace 
Abstract: Data science is a pillar of artificial intelligence (AI), which is transforming nearly every domain of human activity, from the social and physical sciences to engineering and medicine. While data-driven findings in AI offer unprecedented power to extract insights and guide decision-making, many are difficult or impossible to replicate. A key reason for this challenge is the uncertainty introduced by the many choices made throughout the data science life cycle (DSLC). Traditional statistical frameworks often fail to account for this uncertainty. The Predictability-Computability-Stability (PCS) framework for veridical (truthful) data science offers a principled approach to addressing this challenge throughout the DSLC. This paper presents an updated and streamlined PCS workflow, tailored for practitioners and enhanced with guided use of generative AI. We include a running example to display the PCS framework in action, and conduct a related case study which showcases the uncertainty in downstream predictions caused by judgment calls in the data cleaning stage.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MARS: A Meta-Adaptive Reinforcement Learning Framework for Risk-Aware Multi-Agent Portfolio Management</title>
<link>https://arxiv.org/abs/2508.01173</link>
<guid>https://arxiv.org/abs/2508.01173</guid>
<content:encoded><![CDATA[
arXiv:2508.01173v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has shown significant promise in automated portfolio management; however, effectively balancing risk and return remains a central challenge, as many models fail to adapt to dynamically changing market conditions. We propose Meta-controlled Agents for a Risk-aware System (MARS), a novel framework addressing this through a multi-agent, risk-aware approach. MARS replaces monolithic models with a Heterogeneous Agent Ensemble, where each agent's unique risk profile is enforced by a Safety-Critic network to span behaviors from capital preservation to aggressive growth. A high-level Meta-Adaptive Controller (MAC) dynamically orchestrates this ensemble, shifting reliance between conservative and aggressive agents to minimize drawdown during downturns while seizing opportunities in bull markets. This two-tiered structure leverages behavioral diversity rather than explicit feature engineering to ensure a disciplined portfolio robust across market regimes. Experiments on major international indexes confirm that our framework significantly reduces maximum drawdown and volatility while maintaining competitive returns.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instance-Dependent Continuous-Time Reinforcement Learning via Maximum Likelihood Estimation</title>
<link>https://arxiv.org/abs/2508.02103</link>
<guid>https://arxiv.org/abs/2508.02103</guid>
<content:encoded><![CDATA[
arXiv:2508.02103v2 Announce Type: replace 
Abstract: Continuous-time reinforcement learning (CTRL) provides a natural framework for sequential decision-making in dynamic environments where interactions evolve continuously over time. While CTRL has shown growing empirical success, its ability to adapt to varying levels of problem difficulty remains poorly understood. In this work, we investigate the instance-dependent behavior of CTRL and introduce a simple, model-based algorithm built on maximum likelihood estimation (MLE) with a general function approximator. Unlike existing approaches that estimate system dynamics directly, our method estimates the state marginal density to guide learning. We establish instance-dependent performance guarantees by deriving a regret bound that scales with the total reward variance and measurement resolution. Notably, the regret becomes independent of the specific measurement strategy when the observation frequency adapts appropriately to the problem's complexity. To further improve performance, our algorithm incorporates a randomized measurement schedule that enhances sample efficiency without increasing measurement cost. These results highlight a new direction for designing CTRL algorithms that automatically adjust their learning behavior based on the underlying difficulty of the environment.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GTPO: Stabilizing Group Relative Policy Optimization via Gradient and Entropy Control</title>
<link>https://arxiv.org/abs/2508.03772</link>
<guid>https://arxiv.org/abs/2508.03772</guid>
<content:encoded><![CDATA[
arXiv:2508.03772v4 Announce Type: replace 
Abstract: Group Relative Policy Optimization (GRPO) is a promising policy-based approach for Large Language Model alignment, yet its performance is often limited by training instability and suboptimal convergence. In this paper, we identify and analyze two main GRPO issues: (i) the token-level penalization, where valuable tokens shared across different responses receive contradictory feedback signals, leading to conflicting gradient updates that can reduce their likelihood; and (ii) the policy collapse, where negatively rewarded completions may penalize confident responses and shift model decisions toward unlikely tokens, destabilizing training process. To address these issues we introduce GTPO (Group-relative Trajectory-based Policy Optimization), which prevents conflicting gradients on valuable tokens by skipping negative updates while amplifying positive ones and filters out completions whose entropy exceeds a provable threshold, to prevent policy collapse. Unlike GRPO, GTPO does not rely on KL-divergence regularization, eliminating the need for a reference model during training, while still ensuring greater training stability and improved performance, as validated through multiple experiments on GSM8K, MATH, AIME 2024, AIME 2025 and AMC 2023.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Run-Time Monitoring of ERTMS/ETCS Control Flow by Process Mining</title>
<link>https://arxiv.org/abs/2509.10419</link>
<guid>https://arxiv.org/abs/2509.10419</guid>
<content:encoded><![CDATA[
arXiv:2509.10419v2 Announce Type: replace 
Abstract: Ensuring the resilience of computer-based railways is increasingly crucial to account for uncertainties and changes due to the growing complexity and criticality of those systems. Although their software relies on strict verification and validation processes following well-established best-practices and certification standards, anomalies can still occur at run-time due to residual faults, system and environmental modifications that were unknown at design-time, or other emergent cyber-threat scenarios. This paper explores run-time control-flow anomaly detection using process mining to enhance the resilience of ERTMS/ETCS L2 (European Rail Traffic Management System / European Train Control System Level 2). Process mining allows learning the actual control flow of the system from its execution traces, thus enabling run-time monitoring through online conformance checking. In addition, anomaly localization is performed through unsupervised machine learning to link relevant deviations to critical system components. We test our approach on a reference ERTMS/ETCS L2 scenario, namely the RBC/RBC Handover, to show its capability to detect and localize anomalies with high accuracy, efficiency, and explainability.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>All that structure matches does not glitter</title>
<link>https://arxiv.org/abs/2509.12178</link>
<guid>https://arxiv.org/abs/2509.12178</guid>
<content:encoded><![CDATA[
arXiv:2509.12178v2 Announce Type: replace 
Abstract: Generative models for materials, especially inorganic crystals, hold potential to transform the theoretical prediction of novel compounds and structures. Advancement in this field depends on robust benchmarks and minimal, information-rich datasets that enable meaningful model evaluation. This paper critically examines common datasets and reported metrics for a crystal structure prediction task$\unicode{x2014}$generating the most likely structures given the chemical composition of a material. We focus on three key issues: First, materials datasets should contain unique crystal structures; for example, we show that the widely-utilized carbon-24 dataset only contains $\approx$40% unique structures. Second, materials datasets should not be split randomly if polymorphs of many different compositions are numerous, which we find to be the case for the perov-5 and MP-20 datasets. Third, benchmarks can mislead if used uncritically, e.g., reporting a match rate metric without considering the structural variety exhibited by identical building blocks. To address these oft-overlooked issues, we introduce several fixes. We provide revised versions of the carbon-24 dataset: one with duplicates removed, one deduplicated and split by number of atoms $N$, one with enantiomorphs, and two containing only identical structures but with different unit cells. We also propose new splits for datasets with polymorphs, ensuring that polymorphs are grouped within each split subset, setting a more sensible standard for benchmarking model performance. Finally, we present METRe and cRMSE, new model evaluation metrics that can correct existing issues with the match rate metric.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reversible Deep Equilibrium Models</title>
<link>https://arxiv.org/abs/2509.12917</link>
<guid>https://arxiv.org/abs/2509.12917</guid>
<content:encoded><![CDATA[
arXiv:2509.12917v2 Announce Type: replace 
Abstract: Deep Equilibrium Models (DEQs) are an interesting class of implicit model where the model output is implicitly defined as the fixed point of a learned function. These models have been shown to outperform explicit (fixed-depth) models in large-scale tasks by trading many deep layers for a single layer that is iterated many times. However, gradient calculation through DEQs is approximate. This often leads to unstable training dynamics and requires regularisation or many function evaluations to fix. Here, we introduce Reversible Deep Equilibrium Models (RevDEQs) that allow for exact gradient calculation, no regularisation and far fewer function evaluations than DEQs. We show that RevDEQs significantly improve performance on language modelling and image classification tasks against comparable implicit and explicit models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Observation-Free Attacks on Online Learning to Rank</title>
<link>https://arxiv.org/abs/2509.22855</link>
<guid>https://arxiv.org/abs/2509.22855</guid>
<content:encoded><![CDATA[
arXiv:2509.22855v4 Announce Type: replace 
Abstract: Online learning to rank (OLTR) plays a critical role in information retrieval and machine learning systems, with a wide range of applications in search engines and content recommenders. However, despite their extensive adoption, the susceptibility of OLTR algorithms to coordinated adversarial attacks remains poorly understood. In this work, we present a novel framework for attacking some of the widely used OLTR algorithms. Our framework is designed to promote a set of target items so that they appear in the list of top-K recommendations for T - o(T) rounds, while simultaneously inducing linear regret in the learning algorithm. We propose two novel attack strategies: CascadeOFA for CascadeUCB1 and PBMOFA for PBM-UCB . We provide theoretical guarantees showing that both strategies require only O(log T) manipulations to succeed. Additionally, we supplement our theoretical analysis with empirical results on real-world data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ergodic Risk Measures: Towards a Risk-Aware Foundation for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.02945</link>
<guid>https://arxiv.org/abs/2510.02945</guid>
<content:encoded><![CDATA[
arXiv:2510.02945v2 Announce Type: replace 
Abstract: Continual reinforcement learning (continual RL) seeks to formalize the notions of lifelong learning and endless adaptation in RL. In particular, the aim of continual RL is to develop RL agents that can maintain a careful balance between retaining useful information and adapting to new situations. To date, continual RL has been explored almost exclusively through the lens of risk-neutral decision-making, in which the agent aims to optimize the expected long-run performance. In this work, we present the first formal theoretical treatment of continual RL through the lens of risk-aware decision-making, in which the behaviour of the agent is directed towards optimizing a measure of long-run performance beyond the mean. In particular, we show that the classical theory of risk measures, widely used as a theoretical foundation in non-continual risk-aware RL, is, in its current form, incompatible with continual learning. Then, building on this insight, we extend risk measure theory into the continual setting by introducing a new class of ergodic risk measures that are compatible with continual learning. Finally, we provide a case study of risk-aware continual learning, along with empirical results, which show the intuitive appeal of ergodic risk measures in continual settings.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Universal Multi-Domain Translation via Diffusion Routers</title>
<link>https://arxiv.org/abs/2510.03252</link>
<guid>https://arxiv.org/abs/2510.03252</guid>
<content:encoded><![CDATA[
arXiv:2510.03252v2 Announce Type: replace 
Abstract: Multi-domain translation (MDT) aims to learn translations between multiple domains, yet existing approaches either require fully aligned tuples or can only handle domain pairs seen in training, limiting their practicality and excluding many cross-domain mappings. We introduce universal MDT (UMDT), a generalization of MDT that seeks to translate between any pair of $K$ domains using only $K-1$ paired datasets with a central domain. To tackle this problem, we propose Diffusion Router (DR), a unified diffusion-based framework that models all central$\leftrightarrow$non-central translations with a single noise predictor conditioned on the source and target domain labels. DR enables indirect non-central translations by routing through the central domain. We further introduce a novel scalable learning strategy with a variational-bound objective and an efficient Tweedie refinement procedure to support direct non-central mappings. Through evaluation on three large-scale UMDT benchmarks, DR achieves state-of-the-art results for both indirect and direct translations, while lowering sampling cost and unlocking novel tasks such as sketch$\leftrightarrow$segmentation. These results establish DR as a scalable and versatile framework for universal translation across multiple domains.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Detecting Invariant Manifolds in ReLU-Based RNNs</title>
<link>https://arxiv.org/abs/2510.03814</link>
<guid>https://arxiv.org/abs/2510.03814</guid>
<content:encoded><![CDATA[
arXiv:2510.03814v3 Announce Type: replace 
Abstract: Recurrent Neural Networks (RNNs) have found widespread applications in machine learning for time series prediction and dynamical systems reconstruction, and experienced a recent renaissance with improved training algorithms and architectural designs. Understanding why and how trained RNNs produce their behavior is important for scientific and medical applications, and explainable AI more generally. An RNN's dynamical repertoire depends on the topological and geometrical properties of its state space. Stable and unstable manifolds of periodic points play a particularly important role: They dissect a dynamical system's state space into different basins of attraction, and their intersections lead to chaotic dynamics with fractal geometry. Here we introduce a novel algorithm for detecting these manifolds, with a focus on piecewise-linear RNNs (PLRNNs) employing rectified linear units (ReLUs) as their activation function. We demonstrate how the algorithm can be used to trace the boundaries between different basins of attraction, and hence to characterize multistability, a computationally important property. We further show its utility in finding so-called homoclinic points, the intersections between stable and unstable manifolds, and thus establish the existence of chaos in PLRNNs. Finally we show for an empirical example, electrophysiological recordings from a cortical neuron, how insights into the underlying dynamics could be gained through our method.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Monte Carlo-Type Neural Operator for Differential Equations</title>
<link>https://arxiv.org/abs/2510.05620</link>
<guid>https://arxiv.org/abs/2510.05620</guid>
<content:encoded><![CDATA[
arXiv:2510.05620v2 Announce Type: replace 
Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for learning solution operators of one-dimensional partial differential equations (PDEs) by directly learning the kernel function and approximating the associated integral operator using a Monte Carlo-type approach. Unlike Fourier Neural Operators (FNOs), which rely on spectral representations and assume translation-invariant kernels, MCNO makes no such assumptions. The kernel is represented as a learnable tensor over sampled input-output pairs, and sampling is performed once, uniformly at random from a discretized grid. This design enables generalization across multiple grid resolutions without relying on fixed global basis functions or repeated sampling during training, while an interpolation step maps between arbitrary input and output grids to further enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO achieves competitive accuracy with efficient computational cost. We also provide a theoretical analysis proving that the Monte Carlo estimator yields a bounded bias and variance under mild regularity assumptions. This result holds in any spatial dimension, suggesting that MCNO may extend naturally beyond one-dimensional problems. More broadly, this work explores how Monte Carlo-type integration can be incorporated into neural operator frameworks for continuous-domain PDEs, providing a theoretically supported alternative to spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such as the Graph Kernel Neural Operator, GNO).
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retaining by Doing: The Role of On-Policy Data in Mitigating Forgetting</title>
<link>https://arxiv.org/abs/2510.18874</link>
<guid>https://arxiv.org/abs/2510.18874</guid>
<content:encoded><![CDATA[
arXiv:2510.18874v2 Announce Type: replace 
Abstract: Adapting language models (LMs) to new tasks via post-training carries the risk of degrading existing capabilities -- a phenomenon classically known as catastrophic forgetting. In this paper, toward identifying guidelines for mitigating this phenomenon, we systematically compare the forgetting patterns of two widely adopted post-training methods: supervised fine-tuning (SFT) and reinforcement learning (RL). Our experiments reveal a consistent trend across LM families (Llama, Qwen) and tasks (instruction following, general knowledge, and arithmetic reasoning): RL leads to less forgetting than SFT while achieving comparable or higher target task performance. To investigate the cause for this difference, we consider a simplified setting in which the LM is modeled as a mixture of two distributions, one corresponding to prior knowledge and the other to the target task. We identify that the mode-seeking nature of RL, which stems from its use of on-policy data, enables keeping prior knowledge intact when learning the target task. We then verify this insight by demonstrating that the use on-policy data underlies the robustness of RL to forgetting in practical settings, as opposed to other algorithmic choices such as the KL regularization or advantage estimation. Lastly, as a practical implication, our results highlight the potential of mitigating forgetting using approximately on-policy data, which can be substantially more efficient to obtain than fully on-policy data.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Grounded Test-Time Adaptation for LLM Agents</title>
<link>https://arxiv.org/abs/2511.04847</link>
<guid>https://arxiv.org/abs/2511.04847</guid>
<content:encoded><![CDATA[
arXiv:2511.04847v2 Announce Type: replace 
Abstract: Large language model (LLM)-based agents struggle to generalize to novel and complex environments, such as unseen websites or new sets of functions, due to a fundamental mismatch between their pre-training and test-time conditions. This challenge stems from two distinct failure modes: a syntactic misunderstanding of environment-specific components like observation formats, and a semantic misunderstanding of state-transition dynamics, which are only revealed at test time. To address these issues, we propose two distinct and complementary strategies for adapting LLM agents by leveraging environment-specific information available during deployment. First, an online distributional adaptation method parameterizes environmental nuances by learning a lightweight adaptation vector that biases the model's output distribution, enabling rapid alignment with an environment response format. Second, a deployment-time dynamics grounding method employs a persona-driven exploration phase to systematically probe and learn the environment's causal dynamics before task execution, equipping the agent with a nonparametric world model. We evaluate these strategies across diverse agentic benchmarks, including function calling and web navigation. Our empirical results show the effectiveness of both strategies across all benchmarks with minimal computational cost. We find that dynamics grounding is particularly effective in complex environments where unpredictable dynamics pose a major obstacle, demonstrating a robust path toward more generalizable and capable LLM-based agents. For example, on the WebArena multi-site split, this method increases the agent's success rate from 2% to 23%.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Covariance Scattering Transforms</title>
<link>https://arxiv.org/abs/2511.08878</link>
<guid>https://arxiv.org/abs/2511.08878</guid>
<content:encoded><![CDATA[
arXiv:2511.08878v2 Announce Type: replace 
Abstract: Machine learning and data processing techniques relying on covariance information are widespread as they identify meaningful patterns in unsupervised and unlabeled settings. As a prominent example, Principal Component Analysis (PCA) projects data points onto the eigenvectors of their covariance matrix, capturing the directions of maximum variance. This mapping, however, falls short in two directions: it fails to capture information in low-variance directions, relevant when, e.g., the data contains high-variance noise; and it provides unstable results in low-sample regimes, especially when covariance eigenvalues are close. CoVariance Neural Networks (VNNs), i.e., graph neural networks using the covariance matrix as a graph, show improved stability to estimation errors and learn more expressive functions in the covariance spectrum than PCA, but require training and operate in a labeled setup. To get the benefits of both worlds, we propose Covariance Scattering Transforms (CSTs), deep untrained networks that sequentially apply filters localized in the covariance spectrum to the input data and produce expressive hierarchical representations via nonlinearities. We define the filters as covariance wavelets that capture specific and detailed covariance spectral patterns. We improve CSTs' computational and memory efficiency via a pruning mechanism, and we prove that their error due to finite-sample covariance estimations is less sensitive to close covariance eigenvalues compared to PCA, improving their stability. Our experiments on age prediction from cortical thickness measurements on 4 datasets collecting patients with neurodegenerative diseases show that CSTs produce stable representations in low-data settings, as VNNs but without any training, and lead to comparable or better predictions w.r.t. more complex learning models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries</title>
<link>https://arxiv.org/abs/2511.10855</link>
<guid>https://arxiv.org/abs/2511.10855</guid>
<content:encoded><![CDATA[
arXiv:2511.10855v2 Announce Type: replace 
Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hierarchical Deep Research with Local-Web RAG: Toward Automated System-Level Materials Discovery</title>
<link>https://arxiv.org/abs/2511.18303</link>
<guid>https://arxiv.org/abs/2511.18303</guid>
<content:encoded><![CDATA[
arXiv:2511.18303v2 Announce Type: replace 
Abstract: We present a long-horizon, hierarchical deep research (DR) agent designed for complex materials and device discovery problems that exceed the scope of existing Machine Learning (ML) surrogates and closed-source commercial agents. Our framework instantiates a locally deployable DR instance that integrates local retrieval-augmented generation with large language model reasoners, enhanced by a Deep Tree of Research (DToR) mechanism that adaptively expands and prunes research branches to maximize coverage, depth, and coherence. We systematically evaluate across 27 nanomaterials/device topics using a large language model (LLM)-as-judge rubric with five web-enabled state-of-the-art models as jurors. In addition, we conduct dry-lab validations on five representative tasks, where human experts use domain simulations (e.g., density functional theory, DFT) to verify whether DR-agent proposals are actionable. Results show that our DR agent produces reports with quality comparable to--and often exceeding--those of commercial systems (ChatGPT-5-thinking/o3/o4-mini-high Deep Research) at a substantially lower cost, while enabling on-prem integration with local data and tools.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WavefrontDiffusion: Dynamic Decoding Schedule for Improved Reasoning</title>
<link>https://arxiv.org/abs/2511.19473</link>
<guid>https://arxiv.org/abs/2511.19473</guid>
<content:encoded><![CDATA[
arXiv:2511.19473v2 Announce Type: replace 
Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Escaping the Verifier: Learning to Reason via Demonstrations</title>
<link>https://arxiv.org/abs/2511.21667</link>
<guid>https://arxiv.org/abs/2511.21667</guid>
<content:encoded><![CDATA[
arXiv:2511.21667v2 Announce Type: replace 
Abstract: Training Large Language Models (LLMs) to reason often relies on Reinforcement Learning (RL) with task-specific verifiers. However, many real-world reasoning-intensive tasks lack verifiers, despite offering abundant expert demonstrations that remain under-utilized for reasoning-focused training. We introduce RARO (Relativistic Adversarial Reasoning Optimization) that learns strong reasoning capabilities from only expert demonstrations via Inverse Reinforcement Learning. Our method sets up an adversarial interaction between a policy (generator) and a relativistic critic (discriminator): the policy learns to mimic expert answers, while the critic learns to compare and distinguish between policy and expert answers. Our method trains both the policy and the critic jointly and continuously via RL, and we identify the key stabilization techniques required for robust learning. Empirically, RARO significantly outperforms strong verifier-free baselines on all of our evaluation tasks -- Countdown, DeepMath, and Poetry Writing -- and enjoys the same robust scaling trends as RL on verifiable tasks. These results demonstrate that our method effectively elicits strong reasoning performance from expert demonstrations alone, enabling robust reasoning learning even when task-specific verifiers are unavailable.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Variational Inference of Parameters in Opinion Dynamics Models</title>
<link>https://arxiv.org/abs/2403.05358</link>
<guid>https://arxiv.org/abs/2403.05358</guid>
<content:encoded><![CDATA[
arXiv:2403.05358v2 Announce Type: replace-cross 
Abstract: Despite the frequent use of agent-based models (ABMs) for studying social phenomena, parameter estimation remains a challenge, often relying on costly simulation-based heuristics. This work uses variational inference to estimate the parameters of an opinion dynamics ABM, by transforming the estimation problem into an optimization task that can be solved directly.
  Our proposal relies on probabilistic generative ABMs (PGABMs): we start by synthesizing a probabilistic generative model from the ABM rules. Then, we transform the inference process into an optimization problem suitable for automatic differentiation. In particular, we use the Gumbel-Softmax reparameterization for categorical agent attributes and stochastic variational inference for parameter estimation. Furthermore, we explore the trade-offs of using variational distributions with different complexity: normal distributions and normalizing flows.
  We validate our method on a bounded confidence model with agent roles (leaders and followers). Our approach estimates both macroscopic (bounded confidence intervals and backfire thresholds) and microscopic ($200$ categorical, agent-level roles) more accurately than simulation-based and MCMC methods. Consequently, our technique enables experts to tune and validate their ABMs against real-world observations, thus providing insights into human behavior in social systems via data-driven analysis.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are you a robot? Detecting Autonomous Vehicles from Behavior Analysis</title>
<link>https://arxiv.org/abs/2403.09571</link>
<guid>https://arxiv.org/abs/2403.09571</guid>
<content:encoded><![CDATA[
arXiv:2403.09571v2 Announce Type: replace-cross 
Abstract: The tremendous hype around autonomous driving is eagerly calling for emerging and novel technologies to support advanced mobility use cases. As car manufactures keep developing SAE level 3+ systems to improve the safety and comfort of passengers, traffic authorities need to establish new procedures to manage the transition from human-driven to fully-autonomous vehicles while providing a feedback-loop mechanism to fine-tune envisioned autonomous systems. Thus, a way to automatically profile autonomous vehicles and differentiate those from human-driven ones is a must. In this paper, we present a fully-fledged framework that monitors active vehicles using camera images and state information in order to determine whether vehicles are autonomous, without requiring any active notification from the vehicles themselves. Essentially, it builds on the cooperation among vehicles, which share their data acquired on the road feeding a machine learning model to identify autonomous cars. We extensively tested our solution and created the NexusStreet dataset, by means of the CARLA simulator, employing an autonomous driving control agent and a steering wheel maneuvered by licensed drivers. Experiments show it is possible to discriminate the two behaviors by analyzing video clips with an accuracy of 80%, which improves up to 93% when the target state information is available. Lastly, we deliberately degraded the state to observe how the framework performs under non-ideal data collection conditions.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SLO-aware GPU Frequency Scaling for Energy Efficient LLM Inference Serving</title>
<link>https://arxiv.org/abs/2408.05235</link>
<guid>https://arxiv.org/abs/2408.05235</guid>
<content:encoded><![CDATA[
arXiv:2408.05235v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) gain traction, their reliance on power-hungry GPUs places ever-increasing energy demands, raising environmental and monetary concerns. Inference dominates LLM workloads, presenting a critical challenge for providers: minimizing energy costs under Service-Level Objectives (SLOs) that ensure optimal user experience. In this paper, we present \textit{throttLL'eM}, a framework that reduces energy consumption while meeting SLOs through the use of instance and GPU frequency scaling. \textit{throttLL'eM} features mechanisms that project future KV cache usage and batch size. Leveraging a Machine-Learning (ML) model that receives these projections as inputs, \textit{throttLL'eM} manages performance at the iteration level to satisfy SLOs with reduced frequencies and instance sizes. We show that the proposed ML model achieves $R^2$ scores greater than 0.97 and miss-predicts performance by less than 1 iteration per second on average. Experimental results on LLM inference traces show that \textit{throttLL'eM} achieves up to 43.8\% lower energy consumption and an energy efficiency improvement of at least $1.71\times$ under SLOs, when compared to NVIDIA's Triton server.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Train Long-Context Language Models (Effectively)</title>
<link>https://arxiv.org/abs/2410.02660</link>
<guid>https://arxiv.org/abs/2410.02660</guid>
<content:encoded><![CDATA[
arXiv:2410.02660v4 Announce Type: replace-cross 
Abstract: We study continued training and supervised fine-tuning (SFT) of a language model (LM) to make effective use of long-context information. We first establish a reliable evaluation protocol to guide model development -- instead of perplexity or simple needle-in-a-haystack (NIAH) tests, we use a broad set of long-context downstream tasks, and we evaluate models after SFT as this better reveals long-context abilities. Supported by our robust evaluations, we run thorough experiments to decide the data mix for continued pre-training, the instruction tuning dataset, and many other design choices such as position extrapolation. We find that (1) code repositories and books are excellent sources of long data, but it is crucial to combine them with high-quality short-context data; (2) training with a sequence length beyond the evaluation length boosts long-context performance; (3) for SFT, using only short instruction datasets yields strong performance on long-context tasks. Our final model, ProLong-8B, which is initialized from Llama-3 and trained on 40B tokens, demonstrates state-of-the-art long-context performance among similarly sized models at a length of 128K. ProLong outperforms Llama-3.1-8B-Instruct on the majority of long-context tasks despite using only 5% as many tokens during long-context training. Additionally, ProLong can effectively process up to 512K tokens, one of the longest context windows of publicly available LMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Pixels to Prose: Advancing Multi-Modal Language Models for Remote Sensing</title>
<link>https://arxiv.org/abs/2411.05826</link>
<guid>https://arxiv.org/abs/2411.05826</guid>
<content:encoded><![CDATA[
arXiv:2411.05826v2 Announce Type: replace-cross 
Abstract: Remote sensing has evolved from simple image acquisition to complex systems capable of integrating and processing visual and textual data. This review examines the development and application of multi-modal language models (MLLMs) in remote sensing, focusing on their ability to interpret and describe satellite imagery using natural language. We cover the technical underpinnings of MLLMs, including dual-encoder architectures, Transformer models, self-supervised and contrastive learning, and cross-modal integration. The unique challenges of remote sensing data--varying spatial resolutions, spectral richness, and temporal changes--are analyzed for their impact on MLLM performance. Key applications such as scene description, object detection, change detection, text-to-image retrieval, image-to-text generation, and visual question answering are discussed to demonstrate their relevance in environmental monitoring, urban planning, and disaster response. We review significant datasets and resources supporting the training and evaluation of these models. Challenges related to computational demands, scalability, data quality, and domain adaptation are highlighted. We conclude by proposing future research directions and technological advancements to further enhance MLLM utility in remote sensing.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive and Hybrid Imitation Learning: Provably Beating Behavior Cloning</title>
<link>https://arxiv.org/abs/2412.07057</link>
<guid>https://arxiv.org/abs/2412.07057</guid>
<content:encoded><![CDATA[
arXiv:2412.07057v2 Announce Type: replace-cross 
Abstract: Imitation learning (IL) is a paradigm for learning sequential decision making policies from experts, leveraging offline demonstrations, interactive annotations, or both. Recent advances show that when annotation cost is tallied per trajectory, Behavior Cloning (BC) which relies solely on offline demonstrations cannot be improved in general, leaving limited conditions for interactive methods such as DAgger to help. We revisit this conclusion and prove that when the annotation cost is measured per state, algorithms using interactive annotations can provably outperform BC. Specifically: (1) we show that Stagger, a one sample per round variant of DAgger, provably beats BC under low recovery cost settings; (2) we initiate the study of hybrid IL where the agent learns from offline demonstrations and interactive annotations. We propose Warm Stagger whose learning guarantee is not much worse than using either data source alone. Furthermore, motivated by compounding error and cold start problem in imitation learning practice, we give an MDP example in which Warm Stagger has significant better annotation cost; (3) experiments on MuJoCo continuous control tasks confirm that, with modest cost ratio between interactive and offline annotations, interactive and hybrid approaches consistently outperform BC. To the best of our knowledge, our work is the first to highlight the benefit of state wise interactive annotation and hybrid feedback in imitation learning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ORACLE: A Real-Time, Hierarchical, Deep-Learning Photometric Classifier for the LSST</title>
<link>https://arxiv.org/abs/2501.01496</link>
<guid>https://arxiv.org/abs/2501.01496</guid>
<content:encoded><![CDATA[
arXiv:2501.01496v2 Announce Type: replace-cross 
Abstract: We present ORACLE, the first hierarchical deep-learning model for real-time, context-aware classification of transient and variable astrophysical phenomena. ORACLE is a recurrent neural network with Gated Recurrent Units (GRUs), and has been trained using a custom hierarchical cross-entropy loss function to provide high-confidence classifications along an observationally-driven taxonomy with as little as a single photometric observation. Contextual information for each object, including host galaxy photometric redshift, offset, ellipticity and brightness, is concatenated to the light curve embedding and used to make a final prediction. Training on $\sim$0.5M events from the Extended LSST Astronomical Time-Series Classification Challenge, we achieve a top-level (Transient vs Variable) macro-averaged precision of 0.96 using only 1 day of photometric observations after the first detection in addition to contextual information, for each event; this increases to $>$0.99 once 64 days of the light curve has been obtained, and 0.83 at 1024 days after first detection for 19-way classification (including supernova sub-types, active galactic nuclei, variable stars, microlensing events, and kilonovae). We also compare ORACLE with other state-of-the-art classifiers and report comparable performance for the 19-way classification task, in addition to delivering accurate top-level classifications much earlier. The code and model weights used in this work are publicly available at our associated GitHub repository (https://github.com/uiucsn/ELAsTiCC-Classification).
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LargeAD: Large-Scale Cross-Sensor Data Pretraining for Autonomous Driving</title>
<link>https://arxiv.org/abs/2501.04005</link>
<guid>https://arxiv.org/abs/2501.04005</guid>
<content:encoded><![CDATA[
arXiv:2501.04005v3 Announce Type: replace-cross 
Abstract: Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scaling Multimodal Search and Recommendation with Small Language Models via Upside-Down Reinforcement Learning</title>
<link>https://arxiv.org/abs/2502.09854</link>
<guid>https://arxiv.org/abs/2502.09854</guid>
<content:encoded><![CDATA[
arXiv:2502.09854v2 Announce Type: replace-cross 
Abstract: In this work, we investigate how small language models (SLMs) can be scaled to support multimodal search and recommendation use cases while remaining efficient enough for real-time, resource-constrained deployments. We present a framework that combines upside-down reinforcement learning with synthetic data distillation from a large language model (Llama-3) to train a 100M-parameter GPT-2 model for multitask prompt generation. Despite being up to 80 times smaller than state-of-the-art large language models (LLMs), our SLM achieves relevance and diversity scores within 6% of competitive baselines such as Llama-3 8B, Qwen3 8B, and Ministral 8B. These results demonstrate that SLMs can effectively handle multimodal search and recommendation tasks, while dramatically reducing inference latency and memory overhead. Our study highlights the potential of lightweight models as practical engines for scalable multimodal discovery, bridging the gap between cutting-edge research and real-world multimodal applications such as media recommendations and creative content generation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AugMapNet: Improving Spatial Latent Structure via BEV Grid Augmentation for Enhanced Vectorized Online HD Map Construction</title>
<link>https://arxiv.org/abs/2503.13430</link>
<guid>https://arxiv.org/abs/2503.13430</guid>
<content:encoded><![CDATA[
arXiv:2503.13430v2 Announce Type: replace-cross 
Abstract: Autonomous driving requires understanding infrastructure elements, such as lanes and crosswalks. To navigate safely, this understanding must be derived from sensor data in real-time and needs to be represented in vectorized form. Learned Bird's-Eye View (BEV) encoders are commonly used to combine a set of camera images from multiple views into one joint latent BEV grid. Traditionally, from this latent space, an intermediate raster map is predicted, providing dense spatial supervision but requiring post-processing into the desired vectorized form. More recent models directly derive infrastructure elements as polylines using vectorized map decoders, providing instance-level information. Our approach, Augmentation Map Network (AugMapNet), proposes latent BEV feature grid augmentation, a novel technique that significantly enhances the latent BEV representation. AugMapNet combines vector decoding and dense spatial supervision more effectively than existing architectures while remaining easy to integrate compared to other hybrid approaches. It additionally benefits from extra processing on its latent BEV features. Experiments on nuScenes and Argoverse2 datasets demonstrate significant improvements on vectorized map prediction of up to 13.3% over the StreamMapNet baseline on 60 m range and greater improvements on larger ranges. We confirm transferability by applying our method to another baseline, SQD-MapNet, and find similar improvements. A detailed analysis of the latent BEV grid confirms a more structured latent space of AugMapNet and shows the value of our novel concept beyond pure performance improvement. The code can be found at https://github.com/tmonnin/augmapnet
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Wasserstein-Aitchison GAN for angular measures of multivariate extremes</title>
<link>https://arxiv.org/abs/2504.21438</link>
<guid>https://arxiv.org/abs/2504.21438</guid>
<content:encoded><![CDATA[
arXiv:2504.21438v2 Announce Type: replace-cross 
Abstract: Economically responsible mitigation of multivariate extreme risks -- extreme rainfall in a large area, huge variations of many stock prices, widespread breakdowns in transportation systems -- requires estimates of the probabilities that such risks will materialize in the future. This paper develops a new method, Wasserstein--Aitchison Generative Adversarial Networks (WA-GAN) to, which provides simulated values of $d$-dimensional multivariate extreme events and which can hence be used to give estimates of such probabilities. The main hypothesis is that, after transforming the observations to the unit-Pareto scale, their distribution is regularly varying in the sense that the distributions of their radial and angular components (with respect to the $L_1$-norm) converge and become asymptotically independent as the radius gets large. The method is a combination of standard extreme value analysis modeling of the tails of the marginal distributions with nonparametric GAN modeling of the angular distribution. For the latter, the angular values are transformed to Aitchison coordinates in a full $(d-1)$-dimensional linear space, and a Wasserstein GAN is trained on these coordinates and used to generate new values. A reverse transformation is then applied to these values and gives simulated values on the original data scale. Our method is applied to simulated data and to a financial data set from the Kenneth French Data Library. The method shows good performance compared to other existing methods in the literature, both in terms of capturing the dependence structure of the extremes in the data and in generating accurate new extremes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience</title>
<link>https://arxiv.org/abs/2505.08032</link>
<guid>https://arxiv.org/abs/2505.08032</guid>
<content:encoded><![CDATA[
arXiv:2505.08032v2 Announce Type: replace-cross 
Abstract: Adaptive beam switching is essential for mission-critical military and commercial 6G networks but faces major challenges from high carrier frequencies, user mobility, and frequent blockages. While existing machine learning (ML) solutions often focus on maximizing instantaneous throughput, this can lead to unstable policies with high signaling overhead. This paper presents an online Deep Reinforcement Learning (DRL) framework designed to learn an operationally stable policy. By equipping the DRL agent with an enhanced state representation that includes blockage history, and a stability-centric reward function, we enable it to prioritize long-term link quality over transient gains. Validated in a challenging 100-user scenario using the Sionna library, our agent achieves throughput comparable to a reactive Multi-Armed Bandit (MAB) baseline. Specifically, our proposed framework improves link stability by approximately 43% compared to a vanilla DRL approach, achieving operational reliability competitive with MAB while maintaining high data rates. This work demonstrates that by reframing the optimization goal towards operational stability, DRL can deliver efficient, reliable, and real-time beam management solutions for next-generation mission-critical networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal LLM Routing: End-to-End Regret Minimization from Observational Data</title>
<link>https://arxiv.org/abs/2505.16037</link>
<guid>https://arxiv.org/abs/2505.16037</guid>
<content:encoded><![CDATA[
arXiv:2505.16037v2 Announce Type: replace-cross 
Abstract: LLM routing aims to select the most appropriate model for each query, balancing competing performance metrics such as accuracy and cost across a pool of language models. Prior approaches typically adopt a decoupled strategy, where the metrics are first predicted and the model is then selected based on these estimates. This setup is prone to compounding errors and often relies on full-feedback data, where each query is evaluated by all candidate models, which is costly to obtain and maintain in practice. In contrast, we learn from observational data, which records only the outcome of the model actually deployed. We propose a causal end-to-end framework that learns routing policies by minimizing decision-making regret from observational data. To enable efficient optimization, we introduce two theoretically grounded surrogate objectives: a classification-based upper bound, and a softmax-weighted regret approximation shown to recover the optimal policy at convergence. We further extend our framework to handle heterogeneous cost preferences via an interval-conditioned architecture. Experiments on public benchmarks show that our method outperforms existing baselines, achieving state-of-the-art performance across different embedding models.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cataloguing Hugging Face Models to Software Engineering Activities: Automation and Findings</title>
<link>https://arxiv.org/abs/2506.03013</link>
<guid>https://arxiv.org/abs/2506.03013</guid>
<content:encoded><![CDATA[
arXiv:2506.03013v2 Announce Type: replace-cross 
Abstract: Context: Open-source Pre-Trained Models (PTMs) provide extensive resources for various Machine Learning (ML) tasks, yet these resources lack a classification tailored to Software Engineering (SE) needs to support the reliable identification and reuse of models for SE. Objective: To address this gap, we derive a taxonomy encompassing 147 SE tasks and apply an SE-oriented classification to PTMs in a popular open-source ML repository, Hugging Face (HF). Method: Our repository mining study followed a five-phase pipeline: (i) identification SE tasks from the literature; (ii) collection of PTM data from the HF API, including model card descriptions and metadata, and the abstracts of the associated arXiv papers; (iii) text processing to ensure consistency; (iv) a two-phase validation of SE relevance, involving humans and LLM assistance, supported by five pilot studies with human annotators and a generalization test; (v) and data analysis. This process yielded a curated catalogue of 2,205 SE PTMs. Results: We find that most SE PTMs target code generation and coding, emphasizing implementation over early or late development stages. In terms of ML tasks, text generation dominates within SE PTMs. Notably, the number of SE PTMs has increased markedly since 2023 Q2, while evaluation remains limited: only 9.6% report benchmark results, mostly scoring below 50%. Conclusions: Our catalogue reveals documentation and transparency gaps, highlights imbalances across SDLC phases, and provides a foundation for automated SE scenarios, such as the sampling and selection of suitable PTMs.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SPRINT: Enabling Interleaved Planning and Parallelized Execution in Reasoning Models</title>
<link>https://arxiv.org/abs/2506.05745</link>
<guid>https://arxiv.org/abs/2506.05745</guid>
<content:encoded><![CDATA[
arXiv:2506.05745v2 Announce Type: replace-cross 
Abstract: Large reasoning models (LRMs) excel at complex reasoning tasks but typically generate lengthy sequential chains-of-thought, resulting in long inference times before arriving at the final answer. To address this challenge, we introduce SPRINT, a novel post-training and inference-time framework designed to enable LRMs to dynamically identify and exploit opportunities for parallelization during their reasoning process. SPRINT incorporates an innovative data curation pipeline that reorganizes natural language reasoning trajectories into structured rounds of long-horizon planning and parallel execution. By fine-tuning LRMs on a small amount of such curated data, the models learn to dynamically identify independent subtasks within extended reasoning processes and effectively execute them in parallel. Through extensive evaluations, we demonstrate that models fine-tuned with the SPRINT framework match the performance of reasoning models on complex domains such as mathematics while generating up to 39% fewer sequential tokens on problems requiring more than 8,000 output tokens. Finally, we observe consistent results transferred to two out-of-distribution tasks, namely GPQA and Countdown, with up to 45% and 65% reduction in average sequential tokens respectively for longer reasoning trajectories, while matching the performance of the fine-tuned reasoning model.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Top Activations: Efficient and Reliable Crowdsourced Evaluation of Automated Interpretability</title>
<link>https://arxiv.org/abs/2506.07985</link>
<guid>https://arxiv.org/abs/2506.07985</guid>
<content:encoded><![CDATA[
arXiv:2506.07985v2 Announce Type: replace-cross 
Abstract: Interpreting individual neurons or directions in activation space is an important topic in mechanistic interpretability. Numerous automated interpretability methods have been proposed to generate such explanations, but it remains unclear how reliable these explanations are, and which methods produce the most accurate descriptions. While crowd-sourced evaluations are commonly used, existing pipelines are noisy, costly, and typically assess only the highest-activating inputs, leading to unreliable results. In this paper, we introduce two techniques to enable cost-effective and accurate crowdsourced evaluation of automated interpretability methods beyond top activating inputs. First, we propose Model-Guided Importance Sampling (MG-IS) to select the most informative inputs to show human raters. In our experiments, we show this reduces the number of inputs needed to reach the same evaluation accuracy by ~13x. Second, we address label noise in crowd-sourced ratings through Bayesian Rating Aggregation (BRAgg), which allows us to reduce the number of ratings per input required to overcome noise by ~3x. Together, these techniques reduce the evaluation cost by ~40x, making large-scale evaluation feasible. Finally, we use our methods to conduct a large scale crowd-sourced study comparing recent automated interpretability methods for vision networks.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Blameless Users in a Clean Room: Defining Copyright Protection for Generative Models</title>
<link>https://arxiv.org/abs/2506.19881</link>
<guid>https://arxiv.org/abs/2506.19881</guid>
<content:encoded><![CDATA[
arXiv:2506.19881v2 Announce Type: replace-cross 
Abstract: Are there any conditions under which a generative model's outputs are guaranteed not to infringe the copyrights of its training data? This is the question of "provable copyright protection" first posed by Vyas, Kakade, and Barak (ICML 2023). They define near access-freeness (NAF) and propose it as sufficient for protection. This paper revisits the question and establishes new foundations for provable copyright protection -- foundations that are firmer both technically and legally. First, we show that NAF alone does not prevent infringement. In fact, NAF models can enable verbatim copying, a blatant failure of copy protection that we dub being tainted. Then, we introduce our blameless copy protection framework for defining meaningful guarantees, and instantiate it with clean-room copy protection. Clean-room copy protection allows a user to control their risk of copying by behaving in a way that is unlikely to copy in a counterfactual clean-room setting. Finally, we formalize a common intuition about differential privacy and copyright by proving that DP implies clean-room copy protection when the dataset is golden, a copyright deduplication requirement.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Class conditional conformal prediction for multiple inputs by p-value aggregation</title>
<link>https://arxiv.org/abs/2507.07150</link>
<guid>https://arxiv.org/abs/2507.07150</guid>
<content:encoded><![CDATA[
arXiv:2507.07150v2 Announce Type: replace-cross 
Abstract: Conformal prediction methods are statistical tools designed to quantify uncertainty and generate predictive sets with guaranteed coverage probabilities. This work introduces an innovative refinement to these methods for classification tasks, specifically tailored for scenarios where multiple observations (multi-inputs) of a single instance are available at prediction time. Our approach is particularly motivated by applications in citizen science, where multiple images of the same plant or animal are captured by individuals. Our method integrates the information from each observation into conformal prediction, enabling a reduction in the size of the predicted label set while preserving the required class-conditional coverage guarantee. The approach is based on the aggregation of conformal p-values computed from each observation of a multi-input. By exploiting the exact distribution of these p-values, we propose a general aggregation framework using an abstract scoring function, encompassing many classical statistical tools. Knowledge of this distribution also enables refined versions of standard strategies, such as majority voting. We evaluate our method on simulated and real data, with a particular focus on Pl@ntNet, a prominent citizen science platform that facilitates the collection and identification of plant species through user-submitted images.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Solve Constrained Bilevel Control Co-Design Problems</title>
<link>https://arxiv.org/abs/2507.09050</link>
<guid>https://arxiv.org/abs/2507.09050</guid>
<content:encoded><![CDATA[
arXiv:2507.09050v2 Announce Type: replace-cross 
Abstract: Learning to Optimize (L2O) is a subfield of machine learning (ML) in which ML models are trained to solve parametric optimization problems. The general goal is to learn a fast approximator of solutions to constrained optimization problems, as a function of their defining parameters. Prior L2O methods focus almost entirely on single-level programs, in contrast to the bilevel programs, whose constraints are themselves expressed in terms of optimization subproblems. Bilevel programs have numerous important use cases but are notoriously difficult to solve, particularly under stringent time demands. This paper proposes a framework for learning to solve a broad class of challenging bilevel optimization problems, by leveraging modern techniques for differentiation through optimization problems. The framework is illustrated on an array of synthetic bilevel programs, as well as challenging control system co-design problems, showing how neural networks can be trained as efficient approximators of parametric bilevel optimization.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improving Wi-Fi Network Performance Prediction with Deep Learning Models</title>
<link>https://arxiv.org/abs/2507.11168</link>
<guid>https://arxiv.org/abs/2507.11168</guid>
<content:encoded><![CDATA[
arXiv:2507.11168v2 Announce Type: replace-cross 
Abstract: The increasing need for robustness, reliability, and determinism in wireless networks for industrial and mission-critical applications is the driver for the growth of new innovative methods. The study presented in this work makes use of machine learning techniques to predict channel quality in a Wi-Fi network in terms of the frame delivery ratio. Predictions can be used proactively to adjust communication parameters at runtime and optimize network operations for industrial applications. Methods including convolutional neural networks and long short-term memory were analyzed on datasets acquired from a real Wi-Fi setup across multiple channels. The models were compared in terms of prediction accuracy and computational complexity. Results show that the frame delivery ratio can be reliably predicted, and convolutional neural networks, although slightly less effective than other models, are more efficient in terms of CPU usage and memory consumption. This enhances the model's usability on embedded and industrial systems.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Kodezi Chronos: A Debugging-First Language Model for Repository-Scale Code Understanding</title>
<link>https://arxiv.org/abs/2507.12482</link>
<guid>https://arxiv.org/abs/2507.12482</guid>
<content:encoded><![CDATA[
arXiv:2507.12482v4 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have advanced code generation and software automation but remain constrained by inference-time context and lack structured reasoning over code, leaving debugging largely unsolved. While Claude 4.5 Opus achieves 74.40% on SWE-bench Verified and Gemini 3 Pro reaches 76.2%, both models remain below 20% on real multi-file debugging tasks. We introduce Kodezi Chronos-1, a language model purpose-built for debugging that integrates Adaptive Graph-Guided Retrieval to navigate codebases up to 10 million lines (92% precision, 85% recall), Persistent Debug Memory trained on over 15 million sessions, and a seven-layer fix-test-refine architecture. On 5,000 real-world scenarios, Chronos-1 achieves 67.3% +/- 2.1% fix accuracy compared to 14.2% +/- 1.3% for Claude 4.1 Opus and 13.8% +/- 1.2% for GPT-4.1 (Cohen's d = 3.87). On SWE-bench Lite, Chronos-1 reaches a state-of-the-art 80.33% resolution rate (241 of 300), outperforming the next best system by 20 points and achieving repository-specific highs of 96.1% on Sympy and 90.4% on Django. Chronos-1 reduces debugging time by 40% and iterations by 65%, resolving complex multi-file and cross-repository bugs that require temporal analysis. Limitations remain for hardware-dependent and dynamic language errors, and Chronos-1 will be available in Kodezi OS in Q4 2025 and via API in Q1 2026.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Locally Adaptive Conformal Inference for Operator Models</title>
<link>https://arxiv.org/abs/2507.20975</link>
<guid>https://arxiv.org/abs/2507.20975</guid>
<content:encoded><![CDATA[
arXiv:2507.20975v4 Announce Type: replace-cross 
Abstract: Operator models are regression algorithms between Banach spaces of functions. They have become an increasingly critical tool for spatiotemporal forecasting and physics emulation, especially in high-stakes scenarios where robust, calibrated uncertainty quantification is required. We introduce Local Sliced Conformal Inference (LSCI), a distribution-free framework for generating function-valued, locally adaptive prediction sets for operator models. We prove finite-sample validity and derive a data-dependent upper bound on the coverage gap under local exchangeability. On synthetic Gaussian-process tasks and real applications (air quality monitoring, energy demand forecasting, and weather prediction), LSCI yields tighter sets with stronger adaptivity compared to conformal baselines. We also empirically demonstrate robustness against biased predictions and certain out-of-distribution noise regimes.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Astra: A Multi-Agent System for GPU Kernel Performance Optimization</title>
<link>https://arxiv.org/abs/2509.07506</link>
<guid>https://arxiv.org/abs/2509.07506</guid>
<content:encoded><![CDATA[
arXiv:2509.07506v2 Announce Type: replace-cross 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization. Our code is publicly available at https://github.com/Anjiang-Wei/Astra.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Common Pipeline for Harmonizing Electronic Health Record Data for Translational Research</title>
<link>https://arxiv.org/abs/2509.08553</link>
<guid>https://arxiv.org/abs/2509.08553</guid>
<content:encoded><![CDATA[
arXiv:2509.08553v3 Announce Type: replace-cross 
Abstract: Despite the growing availability of Electronic Health Record (EHR) data, researchers often face substantial barriers in effectively using these data for translational research due to their complexity, heterogeneity, and lack of standardized tools and documentation. To address this critical gap, we introduce PEHRT, a common pipeline for harmonizing EHR data for translational research. PEHRT is a comprehensive, ready-to-use resource that includes open-source code, visualization tools, and detailed documentation to streamline the process of preparing EHR data for analysis. The pipeline provides tools to harmonize structured and unstructured EHR data to standardized ontologies to ensure consistency across diverse coding systems. In the presence of unmapped or heterogeneous local codes, PEHRT further leverages representation learning and pre-trained language models to generate robust embeddings that capture semantic relationships across sites to mitigate heterogeneity and enable integrative downstream analyses. PEHRT also supports cross-institutional co-training through shared representations, allowing participating sites to collaboratively refine embeddings and enhance generalizability without sharing individual-level data. The framework is data model-agnostic and can be seamlessly deployed across diverse healthcare systems to produce interoperable, research-ready datasets. By lowering the technical barriers to EHR-based research, PEHRT empowers investigators to transform raw clinical data into reproducible, analysis-ready resources for discovery and innovation.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</title>
<link>https://arxiv.org/abs/2509.09828</link>
<guid>https://arxiv.org/abs/2509.09828</guid>
<content:encoded><![CDATA[
arXiv:2509.09828v2 Announce Type: replace-cross 
Abstract: Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model's inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models will be available at https://github.com/timbroed/DGFusion
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs</title>
<link>https://arxiv.org/abs/2509.17701</link>
<guid>https://arxiv.org/abs/2509.17701</guid>
<content:encoded><![CDATA[
arXiv:2509.17701v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are increasingly used for educational support, yet their response quality varies depending on the language of interaction. This paper presents an automated multilingual pipeline for generating, solving, and evaluating math problems aligned with the German K-10 curriculum. We generated 628 math exercises and translated them into English, German, and Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus) were prompted to produce step-by-step solutions in each language. A held-out panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality using a comparative framework. Results show a consistent gap, with English solutions consistently rated highest, and Arabic often ranked lower. These findings highlight persistent linguistic bias and the need for more equitable multilingual AI systems in education.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Structuring Collective Action with LLM-Guided Evolution: From Ill-Structured Problems to Executable Heuristics</title>
<link>https://arxiv.org/abs/2509.20412</link>
<guid>https://arxiv.org/abs/2509.20412</guid>
<content:encoded><![CDATA[
arXiv:2509.20412v2 Announce Type: replace-cross 
Abstract: Collective action problems, which require aligning individual incentives with collective goals, are classic examples of Ill-Structured Problems (ISPs). For an individual agent, the causal links between local actions and global outcomes are unclear, stakeholder objectives often conflict, and no single, clear algorithm can bridge micro-level choices with macro-level welfare. We present ECHO-MIMIC, a general computational framework that converts this global complexity into a tractable, Well-Structured Problem (WSP) for each agent by discovering executable heuristics and persuasive rationales. The framework operates in two stages: ECHO (Evolutionary Crafting of Heuristics from Outcomes) evolves snippets of Python code that encode candidate behavioral policies, while MIMIC (Mechanism Inference \& Messaging for Individual-to-Collective Alignment) evolves companion natural language messages that motivate agents to adopt those policies. Both phases employ a large-language-model-driven evolutionary search: the LLM proposes diverse and context-aware code or text variants, while population-level selection retains those that maximize collective performance in a simulated environment. We demonstrate this framework on two distinct ISPs: a canonical agricultural landscape management problem and a carbon-aware EV charging time slot usage problem. Results show that ECHO-MIMIC discovers high-performing heuristics compared to baselines and crafts tailored messages that successfully align simulated agent behavior with system-level goals. By coupling algorithmic rule discovery with tailored communication, ECHO-MIMIC transforms the cognitive burden of collective action into a implementable set of agent-level instructions, making previously ill-structured problems solvable in practice and opening a new path toward scalable, adaptive policy design.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MathBode: Measuring the Stability of LLM Reasoning using Frequency Response</title>
<link>https://arxiv.org/abs/2509.23143</link>
<guid>https://arxiv.org/abs/2509.23143</guid>
<content:encoded><![CDATA[
arXiv:2509.23143v4 Announce Type: replace-cross 
Abstract: This paper presents MathBode, a dynamic diagnostic for mathematical reasoning in large language models (LLMs). Instead of one-shot accuracy, MathBode treats each parametric problem as a system: we drive a single parameter sinusoidally and fit first-harmonic responses of model outputs and exact solutions. This yields interpretable, frequency-resolved metrics -- gain (amplitude tracking) and phase (lag) -- that form Bode-style fingerprints. Across five closed-form families (linear solve, ratio/saturation, compound interest, 2x2 linear systems, similar triangles), the diagnostic surfaces systematic low-pass behavior and growing phase lag that accuracy alone obscures. We compare several models against a symbolic baseline that calibrates the instrument ($G \approx 1$, $\phi \approx 0$). Results separate frontier from mid-tier models on dynamics, providing a compact, reproducible protocol that complements standard benchmarks with actionable measurements of reasoning fidelity and consistency. We open-source the dataset and code to enable further research and adoption.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unmute the Patch Tokens: Rethinking Probing in Multi-Label Audio Classification</title>
<link>https://arxiv.org/abs/2509.24901</link>
<guid>https://arxiv.org/abs/2509.24901</guid>
<content:encoded><![CDATA[
arXiv:2509.24901v3 Announce Type: replace-cross 
Abstract: Although probing frozen models has become a standard evaluation paradigm, self-supervised learning in audio defaults to fine-tuning when pursuing state-of-the-art on AudioSet. A key reason is that global pooling creates an information bottleneck causing linear probes to misrepresent the embedding quality: The $\texttt{cls}$-token discards crucial token information about dispersed, localized events in audio. This weakness is rooted in the mismatch between the pretraining objective (globally) and the downstream task (localized). Across a comprehensive benchmark of 13 datasets and 6 spectrogram-based encoders, we investigate the global pooling bottleneck. We introduce binarized prototypical probes: a lightweight and simple pooling method that learns prototypes to perform class-wise information aggregation. Despite its simplicity, our method notably outperforms linear and attentive probing. Our work establishes probing as a competitive and efficient paradigm for evaluating audio SSL models, challenging the reliance on costly fine-tuning.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Theory Meets Federated Learning over Satellite Constellations: Spanning Aggregations, Network Formation, and Performance Optimization</title>
<link>https://arxiv.org/abs/2509.24932</link>
<guid>https://arxiv.org/abs/2509.24932</guid>
<content:encoded><![CDATA[
arXiv:2509.24932v3 Announce Type: replace-cross 
Abstract: In this work, we introduce Fed-Span: \textit{\underline{fed}erated learning with \underline{span}ning aggregation over low Earth orbit (LEO) satellite constellations}. Fed-Span aims to address critical challenges inherent to distributed learning in dynamic satellite networks, including intermittent satellite connectivity, heterogeneous computational capabilities of satellites, and time-varying satellites' datasets. At its core, Fed-Span leverages minimum spanning tree (MST) and minimum spanning forest (MSF) topologies to introduce spanning model aggregation and dispatching processes for distributed learning. To formalize Fed-Span, we offer a fresh perspective on MST/MSF topologies by formulating them through a set of continuous constraint representations (CCRs), thereby integrating these topologies into a distributed learning framework for satellite networks. Using these CCRs, we obtain the energy consumption and latency of operations in Fed-Span. Moreover, we derive novel convergence bounds for Fed-Span, accommodating its key system characteristics and degrees of freedom (i.e., tunable parameters). Finally, we propose a comprehensive optimization problem that jointly minimizes model prediction loss, energy consumption, and latency of {Fed-Span}. We unveil that this problem is NP-hard and develop a systematic approach to transform it into a geometric programming formulation, solved via successive convex optimization with performance guarantees. Through evaluations on real-world datasets, we demonstrate that Fed-Span outperforms existing methods, with faster model convergence, greater energy efficiency, and reduced latency.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Score Distillation of Flow Matching Models</title>
<link>https://arxiv.org/abs/2509.25127</link>
<guid>https://arxiv.org/abs/2509.25127</guid>
<content:encoded><![CDATA[
arXiv:2509.25127v2 Announce Type: replace-cross 
Abstract: Diffusion models achieve high-quality image generation but are limited by slow iterative sampling. Distillation methods alleviate this by enabling one- or few-step generation. Flow matching, originally introduced as a distinct framework, has since been shown to be theoretically equivalent to diffusion under Gaussian assumptions, raising the question of whether distillation techniques such as score distillation transfer directly. We provide a simple derivation -- based on Bayes' rule and conditional expectations -- that unifies Gaussian diffusion and flow matching without relying on ODE/SDE formulations. Building on this view, we extend Score identity Distillation (SiD) to pretrained text-to-image flow-matching models, including SANA, SD3-Medium, SD3.5-Medium/Large, and FLUX.1-dev, all with DiT backbones. Experiments show that, with only modest flow-matching- and DiT-specific adjustments, SiD works out of the box across these models, in both data-free and data-aided settings, without requiring teacher finetuning or architectural changes. This provides the first systematic evidence that score distillation applies broadly to text-to-image flow matching models, resolving prior concerns about stability and soundness and unifying acceleration techniques across diffusion- and flow-based generators. A project page is available at https://yigu1008.github.io/SiD-DiT.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MACS: Measurement-Aware Consistency Sampling for Inverse Problems</title>
<link>https://arxiv.org/abs/2510.02208</link>
<guid>https://arxiv.org/abs/2510.02208</guid>
<content:encoded><![CDATA[
arXiv:2510.02208v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative priors for solving inverse imaging problems. However, their practical deployment is hindered by the substantial computational cost of slow, multi-step sampling. Although Consistency Models (CMs) address this limitation by enabling high-quality generation in only one or a few steps, their direct application to inverse problems has remained largely unexplored. This paper introduces a modified consistency sampling framework specifically designed for inverse problems. The proposed approach regulates the sampler's stochasticity through a measurement-consistency mechanism that leverages the degradation operator, thereby enforcing fidelity to the observed data while preserving the computational efficiency of consistency-based generation. Comprehensive experiments on the Fashion-MNIST and LSUN Bedroom datasets demonstrate consistent improvements across both perceptual and pixel-level metrics, including the Fr\'echet Inception Distance (FID), Kernel Inception Distance (KID), peak signal-to-noise ratio (PSNR), and structural similarity index measure (SSIM), compared with baseline consistency and diffusion-based sampling methods. The proposed method achieves competitive or superior reconstruction quality with only a small number of sampling steps.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differentiable, Bit-shifting, and Scalable Quantization without training neural network from scratch</title>
<link>https://arxiv.org/abs/2510.16088</link>
<guid>https://arxiv.org/abs/2510.16088</guid>
<content:encoded><![CDATA[
arXiv:2510.16088v4 Announce Type: replace-cross 
Abstract: Quantization of neural networks provides benefits of inference in less compute and memory requirements. Previous work in quantization lack two important aspects which this work provides. First almost all previous work in quantization used a non-differentiable approach and for learning; the derivative is usually set manually in backpropogation which make the learning ability of algorithm questionable, our approach is not just differentiable, we also provide proof of convergence of our approach to the optimal neural network. Second previous work in shift/logrithmic quantization either have avoided activation quantization along with weight quantization or achieved less accuracy. Learning logrithmic quantize values of form $2^n$ requires the quantization function can scale to more than 1 bit quantization which is another benifit of our quantization that it provides $n$ bits quantization as well. Our approach when tested with image classification task using imagenet dataset, resnet18 and weight quantization only achieves less than 1 percent accuracy compared to full precision accuracy while taking only 15 epochs to train using shift bit quantization and achieves comparable to SOTA approaches accuracy in both weight and activation quantization using shift bit quantization in 15 training epochs with slightly higher(only higher cpu instructions) inference cost compared to 1 bit quantization(without logrithmic quantization) and not requiring any higher precision multiplication.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Definition of AGI</title>
<link>https://arxiv.org/abs/2510.18212</link>
<guid>https://arxiv.org/abs/2510.18212</guid>
<content:encoded><![CDATA[
arXiv:2510.18212v3 Announce Type: replace-cross 
Abstract: The lack of a concrete definition for Artificial General Intelligence (AGI) obscures the gap between today's specialized AI and human-level cognition. This paper introduces a quantifiable framework to address this, defining AGI as matching the cognitive versatility and proficiency of a well-educated adult. To operationalize this, we ground our methodology in Cattell-Horn-Carroll theory, the most empirically validated model of human cognition. The framework dissects general intelligence into ten core cognitive domains-including reasoning, memory, and perception-and adapts established human psychometric batteries to evaluate AI systems. Application of this framework reveals a highly "jagged" cognitive profile in contemporary models. While proficient in knowledge-intensive domains, current AI systems have critical deficits in foundational cognitive machinery, particularly long-term memory storage. The resulting AGI scores (e.g., GPT-4 at 27%, GPT-5 at 57%) concretely quantify both rapid progress and the substantial gap remaining before AGI.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</title>
<link>https://arxiv.org/abs/2510.18214</link>
<guid>https://arxiv.org/abs/2510.18214</guid>
<content:encoded><![CDATA[
arXiv:2510.18214v2 Announce Type: replace-cross 
Abstract: Safety evaluation of multimodal foundation models often treats vision and language inputs separately, missing risks from joint interpretation where benign content becomes harmful in combination. Existing approaches also fail to distinguish clearly unsafe content from borderline cases, leading to problematic over-blocking or under-refusal of genuinely harmful content. We present Vision Language Safety Understanding (VLSU), a comprehensive framework to systematically evaluate multimodal safety through fine-grained severity classification and combinatorial analysis across 17 distinct safety patterns. Using a multi-stage pipeline with real-world images and human annotation, we construct a large-scale benchmark of 8,187 samples spanning 15 harm categories. Our evaluation of eleven state-of-the-art models reveals systematic joint understanding failures: while models achieve 90%-plus accuracy on clear unimodal safety signals, performance degrades substantially to 20-55% when joint image-text reasoning is required to determine the safety label. Most critically, 34% of errors in joint image-text safety classification occur despite correct classification of the individual modalities, further demonstrating absent compositional reasoning capabilities. Additionally, we find that models struggle to balance refusing unsafe content while still responding to borderline cases that deserve engagement. For example, we find that instruction framing can reduce the over-blocking rate on borderline content from 62.4% to 10.4% in Gemini-1.5, but only at the cost of under-refusing on unsafe content with refusal rate dropping from 90.8% to 53.9%. Overall, our framework exposes weaknesses in joint image-text understanding and alignment gaps in current models, and provides a critical test bed to enable the next milestones in research on robust vision-language safety.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Field Interface-Aware Neural Operators for Multiphase Flow Simulation</title>
<link>https://arxiv.org/abs/2511.08625</link>
<guid>https://arxiv.org/abs/2511.08625</guid>
<content:encoded><![CDATA[
arXiv:2511.08625v2 Announce Type: replace-cross 
Abstract: Multiphase flow simulation is critical in science and engineering but incurs high computational costs due to complex field discontinuities and the need for high-resolution numerical meshes. While Neural Operators (NOs) offer an efficient alternative for solving Partial Differential Equations (PDEs), they struggle with two core challenges unique to multiphase systems: spectral bias caused by spatial heterogeneity at phase interfaces, and the persistent scarcity of expensive, high-resolution field data. This work introduces the Interface Information Aware Neural Operator (IANO), a novel architecture that mitigates these issues by leveraging readily obtainable interface data (e.g., topology and position). Interface data inherently contains the high-frequency features not only necessary to complement the physical field data, but also help with spectral bias. IANO incorporates an interface-aware function encoding mechanism to capture dynamic coupling, and a geometry-aware positional encoding method to enhance spatial fidelity for pointwise super-resolution. Empirical results across multiple multiphase flow cases demonstrate that IANO achieves significant accuracy improvements (up to $\sim$10\%) over existing NO baselines. Furthermore, IANO exhibits superior generalization capabilities in low-data and noisy settings, confirming its utility for practical, data-efficient $\text{AI}$-based multiphase flow simulations.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning</title>
<link>https://arxiv.org/abs/2511.19304</link>
<guid>https://arxiv.org/abs/2511.19304</guid>
<content:encoded><![CDATA[
arXiv:2511.19304v2 Announce Type: replace-cross 
Abstract: Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Manifold Percolation: from generative model to Reinforce learning</title>
<link>https://arxiv.org/abs/2511.20503</link>
<guid>https://arxiv.org/abs/2511.20503</guid>
<content:encoded><![CDATA[
arXiv:2511.20503v2 Announce Type: replace-cross 
Abstract: Generative modeling is typically framed as learning mapping rules, but from an observer's perspective without access to these rules, the task becomes disentangling the geometric support from the probability distribution. We propose that continuum percolation is uniquely suited to this support analysis, as the sampling process effectively projects high-dimensional density estimation onto a geometric counting problem on the support. In this work, we establish a rigorous correspondence between the topological phase transitions of random geometric graphs and the underlying data manifold in high-dimensional space. By analyzing the relationship between our proposed Percolation Shift metric and FID, we show that this metric captures structural pathologies, such as implicit mode collapse, where standard statistical metrics fail. Finally, we translate this topological phenomenon into a differentiable loss function that guides training. Experimental results confirm that this approach not only prevents manifold shrinkage but also fosters a form of synergistic improvement, where topological stability becomes a prerequisite for sustained high fidelity in both static generation and sequential decision making.
]]></content:encoded>
<pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mean-Field Limits for Two-Layer Neural Networks Trained with Consensus-Based Optimization</title>
<link>https://arxiv.org/abs/2511.21466</link>
<guid>https://arxiv.org/abs/2511.21466</guid>
<content:encoded><![CDATA[
<div> Consensus-Based Optimization, neural networks, mean-field limit, optimal transport, multi-task learning<br /><br />Summary:<br /><br />This article explores the use of Consensus-Based Optimization (CBO) for training two-layer neural networks. The authors compare CBO’s performance against the widely-used Adam optimizer on two test cases, highlighting differences in convergence behavior. They demonstrate that a hybrid method combining CBO and Adam accelerates convergence beyond what CBO achieves alone. Furthermore, the paper addresses challenges in multi-task learning by reformulating CBO to reduce memory overhead, making it more efficient for problems with multiple tasks. To provide a rigorous theoretical foundation, the authors recast CBO within the optimal transport framework, enabling a mean-field limit formulation as the number of particles approaches infinity. This advanced formulation operates on the Wasserstein-over-Wasserstein space, a sophisticated mathematical space capturing distributions of probability measures. Importantly, they prove that within this limit the variance of the system decreases monotonically, indicating stable and convergent dynamics. Overall, the work bridges optimization algorithms, neural network training, multi-task learning, and mathematical analysis through mean-field theory, presenting both practical improvements and a strong theoretical understanding of CBO methods. <div>
arXiv:2511.21466v2 Announce Type: replace 
Abstract: We study Consensus-Based Optimization (CBO) for two-layer neural network training. We compare the performance of CBO against Adam on two test cases and demonstrate how a hybrid approach, combining CBO with Adam, provides faster convergence than CBO. Additionally, in the context of multi-task learning, we recast CBO into a formulation that offers less memory overhead. The CBO method allows for a mean-field limit formulation, which we couple with the mean-field limit of the neural network. To this end, we first reformulate CBO within the optimal transport framework. In the limit of infinitely many particles, we define the corresponding dynamics on the Wasserstein-over-Wasserstein space and show that the variance decreases monotonically.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Responsible Development of Generative AI for Education: An Evaluation-Driven Approach</title>
<link>https://arxiv.org/abs/2407.12687</link>
<guid>https://arxiv.org/abs/2407.12687</guid>
<content:encoded><![CDATA[
<div> Keywords: generative AI, education, pedagogical evaluation, LearnLM-Tutor, benchmarks  

<br /><br />Summary:  
1. The article addresses the global challenge of providing equitable and universal access to quality education.  
2. It highlights the excitement around generative AI (gen AI) as a potential personal tutor for learners and teaching assistant for educators, but notes that this potential has not yet been fully realized.  
3. The authors identify key barriers to progress, including difficulties in converting pedagogical intuitions into effective AI prompts, lack of robust evaluation methods, and challenges in defining what constitutes excellent pedagogy.  
4. To tackle these issues, the team collaborates with learners and educators to translate learning science principles into seven diverse educational benchmarks that include quantitative, qualitative, automatic, and human evaluations.  
5. They introduce LearnLM-Tutor, a fine-tuned version of the Gemini model designed to enhance pedagogical capabilities.  
6. Evaluations demonstrate that LearnLM-Tutor is preferred over a prompt-tuned Gemini by both educators and learners across various pedagogical dimensions.  
7. The work aims to establish a foundational educational evaluation framework that can expedite progress in AI and EdTech, ultimately maximizing the positive impact of generative AI in education. <div>
arXiv:2407.12687v4 Announce Type: replace-cross 
Abstract: A major challenge facing the world is the provision of equitable and universal access to quality education. Recent advances in generative AI (gen AI) have created excitement about the potential of new technologies to offer a personal tutor for every learner and a teaching assistant for every teacher. The full extent of this dream, however, has not yet materialised. We argue that this is primarily due to the difficulties with verbalising pedagogical intuitions into gen AI prompts and the lack of good evaluation practices, reinforced by the challenges in defining excellent pedagogy. Here we present our work collaborating with learners and educators to translate high level principles from learning science into a pragmatic set of seven diverse educational benchmarks, spanning quantitative, qualitative, automatic and human evaluations; and to develop a new set of fine-tuning datasets to improve the pedagogical capabilities of Gemini, introducing LearnLM-Tutor. Our evaluations show that LearnLM-Tutor is consistently preferred over a prompt tuned Gemini by educators and learners on a number of pedagogical dimensions. We hope that this work can serve as a first step towards developing a comprehensive educational evaluation framework, and that this can enable rapid progress within the AI and EdTech communities towards maximising the positive impact of gen AI in education.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Improved Ensemble-Based Machine Learning Model with Feature Optimization for Early Diabetes Prediction</title>
<link>https://arxiv.org/abs/2512.02023</link>
<guid>https://arxiv.org/abs/2512.02023</guid>
<content:encoded><![CDATA[
<div> Diabetes, Machine Learning, BRFSS Dataset, Ensemble Learning, Early Detection<br /><br />Summary:<br /><br />1. Diabetes remains a critical global health challenge, and effective early detection is essential for timely intervention. <br />2. Predicting diabetes is complicated by overlapping risk factors and imbalanced data distributions, requiring robust analytical methods. <br />3. The study utilized the 2015 BRFSS dataset, comprising approximately 253,680 records with 22 numerical features, to develop predictive models. <br />4. Several supervised machine learning techniques were evaluated, including individual models such as Random Forest, XGBoost, CatBoost, and LightGBM, which all achieved strong ROC-AUC scores around 0.96. <br />5. Data imbalance was addressed using SMOTE (Synthetic Minority Over-sampling Technique) and Tomek Links to improve model performance. <br />6. Ensemble techniques, specifically stacking using XGBoost and K-Nearest Neighbors (KNN), outperformed individual models, attaining an accuracy of 94.82%, ROC-AUC of 0.989, and PR-AUC of 0.991, demonstrating excellent balance between recall and precision. <br />7. The research also includes the development of a practical React Native application with a Python Flask backend, designed to facilitate early diabetes detection and provide an accessible health monitoring tool for users. <div>
arXiv:2512.02023v1 Announce Type: new 
Abstract: Diabetes is a serious worldwide health issue, and successful intervention depends on early detection. However, overlapping risk factors and data asymmetry make prediction difficult. To use extensive health survey data to create a machine learning framework for diabetes classification that is both accurate and comprehensible, to produce results that will aid in clinical decision-making. Using the BRFSS dataset, we assessed a number of supervised learning techniques. SMOTE and Tomek Links were used to correct class imbalance. To improve prediction performance, both individual models and ensemble techniques such as stacking were investigated. The 2015 BRFSS dataset, which includes roughly 253,680 records with 22 numerical features, is used in this study. Strong ROC-AUC performance of approximately 0.96 was attained by the individual models Random Forest, XGBoost, CatBoost, and LightGBM.The stacking ensemble with XGBoost and KNN yielded the best overall results with 94.82\% accuracy, ROC-AUC of 0.989, and PR-AUC of 0.991, indicating a favourable balance between recall and precision. In our study, we proposed and developed a React Native-based application with a Python Flask backend to support early diabetes prediction, providing users with an accessible and efficient health monitoring tool.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pharmacophore-based design by learning on voxel grids</title>
<link>https://arxiv.org/abs/2512.02031</link>
<guid>https://arxiv.org/abs/2512.02031</guid>
<content:encoded><![CDATA[
<div> Keywords: ligand-based drug discovery, pharmacophore modeling, voxel-based generative model, VoxCap, de-novo design  

<br /><br />Summary:  
This paper addresses key limitations in ligand-based drug discovery (LBDD), particularly the inefficiency and restriction to existing libraries in traditional pharmacophore-shape based virtual screening methods. The authors propose a novel voxel-based generative model called VoxCap, which effectively generates SMILES strings from voxelized molecular representations. Two practical workflows are introduced: (1) a de-novo design workflow that generates new molecules exhibiting high pharmacophore-shape similarity to known binders, enhancing hit diversity and scaffold hopping capabilities, and (2) a fast search workflow that combines generative design with inexpensive 2D substructure similarity searches to drastically improve computational efficiency. VoxCap demonstrates significant improvements over previous methods by producing more diverse de-novo hits and reducing search times by orders of magnitude, making it feasible to screen very large molecular libraries that are otherwise computationally prohibitive for brute force methods. These advances provide scalable and fecund solutions to pharmacophore-based virtual screening, potentially accelerating hit identification and drug discovery processes. <div>
arXiv:2512.02031v1 Announce Type: new 
Abstract: Ligand-based drug discovery (LBDD) relies on making use of known binders to a protein target to find structurally diverse molecules similarly likely to bind. This process typically involves a brute force search of the known binder (query) against a molecular library using some metric of molecular similarity. One popular approach overlays the pharmacophore-shape profile of the known binder to 3D conformations enumerated for each of the library molecules, computes overlaps, and picks a set of diverse library molecules with high overlaps. While this virtual screening workflow has had considerable success in hit diversification, scaffold hopping, and patent busting, it scales poorly with library sizes and restricts candidate generation to existing library compounds. Leveraging recent advances in voxel-based generative modelling, we propose a pharmacophore-based generative model and workflows that address the scaling and fecundity issues of conventional pharmacophore-based virtual screening. We introduce \emph{VoxCap}, a voxel captioning method for generating SMILES strings from voxelised molecular representations. We propose two workflows as practical use cases as well as benchmarks for pharmacophore-based generation: \emph{de-novo} design, in which we aim to generate new molecules with high pharmacophore-shape similarities to query molecules, and fast search, which aims to combine generative design with a cheap 2D substructure similarity search for efficient hit identification. Our results show that VoxCap significantly outperforms previous methods in generating diverse \textit{de-novo} hits. When combined with our fast search workflow, VoxCap reduces computational time by orders of magnitude while returning hits for all query molecules, enabling the search of large libraries that are intractable to search by brute force.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIBNet: a Physics-Inspired Boundary Network for Multiple Scattering Simulations</title>
<link>https://arxiv.org/abs/2512.02049</link>
<guid>https://arxiv.org/abs/2512.02049</guid>
<content:encoded><![CDATA[
<div> Boundary Element Method, Multiple Scattering, Graph Neural Network, Physics-Inspired, Multiscale Architecture<br /><br />Summary:<br /><br />The article addresses the computational challenges of solving multiple scattering problems in unbounded homogeneous domains using the Boundary Element Method (BEM). BEM reduces computational complexity by discretizing only the domain boundaries rather than the entire volume. The main bottleneck is solving the boundary integral equation to determine the solution trace on domain boundaries. To overcome this, the authors propose PIBNet, a physics-inspired, graph-based neural network designed to approximate the solution trace efficiently. This network models obstacles and their long-range interactions using a novel multiscale graph neural network architecture tailored for multiple scattering simulations. The approach leverages graph representations to encapsulate physical interactions effectively. To validate and benchmark PIBNet, the authors introduce several datasets representing various multiple scattering scenarios. Experimental results show that PIBNet outperforms existing state-of-the-art learning-based methods in accuracy and computational efficiency on the examined tasks. Additionally, PIBNet displays strong generalization capabilities, handling problems with a larger number of obstacles than those seen during training. The method thus offers a promising data-driven alternative to traditional numerical solvers for complex scattering problems, balancing physical fidelity and computational speed. The implementation is publicly available at github.com/ENSTA-U2IS-AI/pibnet. <div>
arXiv:2512.02049v1 Announce Type: new 
Abstract: The boundary element method (BEM) provides an efficient numerical framework for solving multiple scattering problems in unbounded homogeneous domains, since it reduces the discretization to the domain boundaries, thereby condensing the computational complexity. The procedure first consists in determining the solution trace on the boundaries of the domain by solving a boundary integral equation, after which the volumetric solution can be recovered at low computational cost with a boundary integral representation. As the first step of the BEM represents the main computational bottleneck, we introduce PIBNet, a learning-based approach designed to approximate the solution trace. The method leverages a physics-inspired graph-based strategy to model obstacles and their long-range interactions efficiently. Then, we introduce a novel multiscale graph neural network architecture for simulating the multiple scattering. To train and evaluate our network, we present a benchmark consisting of several datasets of different types of multiple scattering problems. The results indicate that our approach not only surpasses existing state-of-the-art learning-based methods on the considered tasks but also exhibits superior generalization to settings with an increased number of obstacles. github.com/ENSTA-U2IS-AI/pibnet
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Contextual Gating within the Transformer Stack: Synergistic Feature Modulation for Enhanced Lyrical Classification and Calibration</title>
<link>https://arxiv.org/abs/2512.02053</link>
<guid>https://arxiv.org/abs/2512.02053</guid>
<content:encoded><![CDATA[
<div> Keywords: SFL Transformer, Contextual Gating, feature fusion, lyrical content classification, BERT encoder

<br /><br />Summary:  
This study presents the SFL Transformer, an innovative model that enhances feature fusion for lyrical content classification by integrating auxiliary structural features directly into the self-attention layers of a pre-trained Transformer rather than at the output stage. The key advancement is the introduction of a Contextual Gating mechanism, referred to as Intermediate SFL, which modulates the sequence of hidden states (Hseq) within the BERT encoder stack using low-dimensional structural cues (Fstruct). This approach allows deeper and more effective synergistic combination of structural and semantic information. The model is evaluated on a binary classification task based on UMAP-reduced lyrical embeddings, achieving an outstanding Accuracy of 0.9910 and a Macro F1 score of 0.9910, outperforming the previous state-of-the-art SFL model (Accuracy 0.9894). Additionally, the method demonstrates exceptional reliability with a low Expected Calibration Error (ECE = 0.0081) and a minimal Log Loss of 0.0489, indicating highly accurate probability estimates. Overall, the results validate the hypothesis that injecting auxiliary context mid-stack in a Transformer architecture is an effective strategy for improving discriminative performance and producing well-calibrated predictions in lyrical content classification tasks. <div>
arXiv:2512.02053v1 Announce Type: new 
Abstract: This study introduces a significant architectural advancement in feature fusion for lyrical content classification by integrating auxiliary structural features directly into the self-attention mechanism of a pre-trained Transformer. I propose the SFL Transformer, a novel deep learning model that utilizes a Contextual Gating mechanism (an Intermediate SFL) to modulate the sequence of hidden states within the BERT encoder stack, rather than fusing features at the final output layer. This approach modulates the deep, contextualized semantic features (Hseq) using low-dimensional structural cues (Fstruct). The model is applied to a challenging binary classification task derived from UMAP-reduced lyrical embeddings. The SFL Transformer achieved an Accuracy of 0.9910 and a Macro F1 score of 0.9910, significantly improving the state-of-the-art established by the previously published SFL model (Accuracy 0.9894). Crucially, this Contextual Gating strategy maintained exceptional reliability, with a low Expected Calibration Error (ECE = 0.0081) and Log Loss (0.0489). This work validates the hypothesis that injecting auxiliary context mid-stack is the most effective means of synergistically combining structural and semantic information, creating a model with both superior discriminative power and high-fidelity probability estimates.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opening the Black Box: An Explainable, Few-shot AI4E Framework Informed by Physics and Expert Knowledge for Materials Engineering</title>
<link>https://arxiv.org/abs/2512.02057</link>
<guid>https://arxiv.org/abs/2512.02057</guid>
<content:encoded><![CDATA[
<div> Keywords: Artificial Intelligence for Engineering, few-shot learning, physics-informed modeling, symbolic regression, hot-cracking prediction<br /><br />Summary: The paper addresses two key challenges in applying AI to engineering—limited high-quality data and lack of model interpretability—especially in critical fields like aerospace. It proposes an explainable few-shot AI4E framework that integrates physics and expert knowledge directly into its structure. Beginning with only 32 experimental samples from repair welding on K439B superalloy castings, the method generates physically consistent synthetic data via a three-step process: calibrated noise injection, enforcement of physical constraints, and preservation of parameter relationships. A nested optimization method is then used for constitutive model discovery, combining symbolic regression to find equation forms with differential evolution for parameter tuning, finalized by hybrid global-local refinement. The resultant interpretable constitutive equation predicts hot-cracking tendency with 88% accuracy and gives explicit physical insights into the coupling of thermal, geometric, and metallurgical factors causing cracking. Beyond prediction, the equation acts as a tool for process optimization and high-fidelity virtual data creation, improving other data-driven models. This approach offers a generalizable blueprint for trustworthy AI in safety-sensitive industrial applications where data scarcity exists but domain knowledge is strong. <div>
arXiv:2512.02057v1 Announce Type: new 
Abstract: The industrial adoption of Artificial Intelligence for Engineering (AI4E) faces two fundamental bottlenecks: scarce high-quality data and the lack of interpretability in black-box models-particularly critical in safety-sensitive sectors like aerospace. We present an explainable, few-shot AI4E framework that is systematically informed by physics and expert knowledge throughout its architecture. Starting from only 32 experimental samples in an aerial K439B superalloy castings repair welding case, we first augment physically plausible synthetic data through a three-stage protocol: differentiated noise injection calibrated to process variabilities, enforcement of hard physical constraints, and preservation of inter-parameter relationships. We then employ a nested optimization strategy for constitutive model discovery, where symbolic regression explores equation structures while differential evolution optimizes parameters, followed by intensive parameter refinement using hybrid global-local optimization. The resulting interpretable constitutive equation achieves 88% accuracy in predicting hot-cracking tendency. This equation not only provides quantitative predictions but also delivers explicit physical insight, revealing how thermal, geometric, and metallurgical mechanisms couple to drive cracking-thereby advancing engineers' cognitive understanding of the process. Furthermore, the constitutive equation serves as a multi-functional tool for process optimization and high-fidelity virtual data generation, enabling accuracy improvements in other data-driven models. Our approach provides a general blueprint for developing trustworthy AI systems that embed engineering domain knowledge directly into their architecture, enabling reliable adoption in high-stakes industrial applications where data is limited but physical understanding is available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Ada-MoGE: Adaptive Mixture of Gaussian Expert Model for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.02061</link>
<guid>https://arxiv.org/abs/2512.02061</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariate time series, adaptive MoE, frequency distribution, Gaussian band-pass filtering, spectral intensity<br /><br />Summary:  
This paper addresses challenges in multivariate time series forecasting where dominant frequencies shift over time, complicating effective modeling. Traditional Mixture of Experts (MoE) models use a fixed number of experts, which cannot adapt to evolving spectral distributions, causing frequency coverage imbalance. Having too few experts risks missing critical information, whereas too many experts introduce noise. To overcome these issues, the authors propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model that dynamically determines the number of experts based on spectral intensity and frequency response alignment with the input data’s frequency distribution. This adaptive strategy prevents both information loss due to insufficient experts and noise contamination from excess experts. Additionally, the model employs Gaussian band-pass filtering to smoothly decompose frequency domain features, avoiding noise that arises from abrupt band truncations common in traditional techniques. Experimental results demonstrate that Ada-MoGE achieves state-of-the-art performance across six public multivariate time series benchmarks, while maintaining a lightweight architecture with only 0.2 million parameters. This shows the model’s effectiveness in accurately capturing shifting spectral characteristics with computational efficiency. The proposed method offers a robust and flexible framework for frequency-adaptive forecasting in complex temporal data domains such as industrial, transportation, and financial forecasting. <div>
arXiv:2512.02061v1 Announce Type: new 
Abstract: Multivariate time series forecasts are widely used, such as industrial, transportation and financial forecasts. However, the dominant frequencies in time series may shift with the evolving spectral distribution of the data. Traditional Mixture of Experts (MoE) models, which employ a fixed number of experts, struggle to adapt to these changes, resulting in frequency coverage imbalance issue. Specifically, too few experts can lead to the overlooking of critical information, while too many can introduce noise. To this end, we propose Ada-MoGE, an adaptive Gaussian Mixture of Experts model. Ada-MoGE integrates spectral intensity and frequency response to adaptively determine the number of experts, ensuring alignment with the input data's frequency distribution. This approach prevents both information loss due to an insufficient number of experts and noise contamination from an excess of experts. Additionally, to prevent noise introduction from direct band truncation, we employ Gaussian band-pass filtering to smoothly decompose the frequency domain features, further optimizing the feature representation. The experimental results show that our model achieves state-of-the-art performance on six public benchmarks with only 0.2 million parameters.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DPWMixer: Dual-Path Wavelet Mixer for Long-Term Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.02070</link>
<guid>https://arxiv.org/abs/2512.02070</guid>
<content:encoded><![CDATA[
<div> Long-term time series forecasting, DPWMixer, Haar Wavelet Pyramid, Dual-Path Trend Mixer, multi-scale fusion<br /><br />Summary:<br /><br />1. This paper addresses the challenges in long-term time series forecasting (LTSF), focusing on improving model efficiency and accuracy.<br />2. Transformer-based models capture long-range dependencies effectively but face issues like quadratic computational complexity and overfitting due to limited data.<br />3. Efficient linear models are inadequate for modeling complex and nonlinear local dynamics in time series data.<br />4. Current multi-scale approaches generally use average pooling, which acts as a low-pass filter and causes spectral aliasing and loss of important high-frequency information.<br />5. The authors introduce DPWMixer, a Dual-Path architecture based on a Lossless Haar Wavelet Pyramid, which replaces traditional pooling with orthogonal decomposition to separate trends from local fluctuations without losing information.<br />6. DPWMixer utilizes a Dual-Path Trend Mixer comprising a global linear mapping for capturing macro trends and a patch-based MLP-Mixer to model micro-level dynamics.<br />7. An adaptive multi-scale fusion module integrates predictions from multiple scales, weighting them according to channel stationarity to optimize overall performance.<br />8. Extensive experiments on eight public benchmarks demonstrate that DPWMixer consistently outperforms state-of-the-art baselines.<br />9. The code for DPWMixer is publicly available, fostering transparency and reproducibility in future research. <div>
arXiv:2512.02070v1 Announce Type: new 
Abstract: Long-term time series forecasting (LTSF) is a critical task in computational intelligence. While Transformer-based models effectively capture long-range dependencies, they often suffer from quadratic complexity and overfitting due to data sparsity. Conversely, efficient linear models struggle to depict complex non-linear local dynamics. Furthermore, existing multi-scale frameworks typically rely on average pooling, which acts as a non-ideal low-pass filter, leading to spectral aliasing and the irreversible loss of high-frequency transients. In response, this paper proposes DPWMixer, a computationally efficient Dual-Path architecture. The framework is built upon a Lossless Haar Wavelet Pyramid that replaces traditional pooling, utilizing orthogonal decomposition to explicitly disentangle trends and local fluctuations without information loss. To process these components, we design a Dual-Path Trend Mixer that integrates a global linear mapping for macro-trend anchoring and a flexible patch-based MLP-Mixer for micro-dynamic evolution. Finally, An adaptive multi-scale fusion module then integrates predictions from diverse scales, weighted by channel stationarity to optimize synthesis. Extensive experiments on eight public benchmarks demonstrate that our method achieves a consistent improvement over state-of-the-art baselines. The code is available at https://github.com/hit636/DPWMixer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HTG-GCL: Leveraging Hierarchical Topological Granularity from Cellular Complexes for Graph Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.02073</link>
<guid>https://arxiv.org/abs/2512.02073</guid>
<content:encoded><![CDATA[
<div> Keywords: graph contrastive learning, topological granularity, hierarchical structures, multi-scale cellular complexes, uncertainty weighting<br /><br />Summary:<br /><br />1. The paper addresses limitations in current Graph Contrastive Learning (GCL) methods, particularly their inability to effectively identify task-relevant topological structures in graphs and to adapt across different topological granularities required by diverse downstream tasks.<br /><br />2. To overcome these challenges, the authors propose Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that generates multi-scale ring-based cellular complexes from the same graph, capturing hierarchical topological granularity and thus creating varied topological views.<br /><br />3. Recognizing that not all granularities are equally informative, the framework incorporates a multi-granularity decoupled contrast mechanism combined with a granularity-specific weighting scheme that relies on uncertainty estimation to mitigate the influence of misleading semantics.<br /><br />4. The integration of these components enables HTG-GCL to learn richer and more discriminative graph representations by effectively capturing hierarchical topological information.<br /><br />5. Extensive experiments conducted on various benchmark datasets demonstrate that HTG-GCL surpasses existing GCL methods in performance, validating its efficacy and robustness in diverse graph learning scenarios. <div>
arXiv:2512.02073v1 Announce Type: new 
Abstract: Graph contrastive learning (GCL) aims to learn discriminative semantic invariance by contrasting different views of the same graph that share critical topological patterns. However, existing GCL approaches with structural augmentations often struggle to identify task-relevant topological structures, let alone adapt to the varying coarse-to-fine topological granularities required across different downstream tasks. To remedy this issue, we introduce Hierarchical Topological Granularity Graph Contrastive Learning (HTG-GCL), a novel framework that leverages transformations of the same graph to generate multi-scale ring-based cellular complexes, embodying the concept of topological granularity, thereby generating diverse topological views. Recognizing that a certain granularity may contain misleading semantics, we propose a multi-granularity decoupled contrast and apply a granularity-specific weighting mechanism based on uncertainty estimation. Comprehensive experiments on various benchmarks demonstrate the effectiveness of HTG-GCL, highlighting its superior performance in capturing meaningful graph representations through hierarchical topological information.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FDRMFL:Multi-modal Federated Feature Extraction Model Based on Information Maximization and Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.02076</link>
<guid>https://arxiv.org/abs/2512.02076</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-modal data regression, federated feature extraction, contrastive learning, non-IID data, catastrophic forgetting<br /><br />Summary:<br /><br />This study addresses the challenge of feature extraction in multi-modal data regression, focusing on real-world issues including limited, non-IID datasets, effective fusion of multi-modal information, and model susceptibility to catastrophic forgetting. It introduces a task-driven supervised multi-modal federated feature extraction method that integrates multi-modal information extraction with contrastive learning. The approach is versatile, allowing different neural network architectures as latent mapping functions tailored to each data modality. Each client independently learns low-dimensional representations of multi-modal data, with adjustable parameters to control retention of predictive information related to the response variable. The method employs a multi-constraint learning framework guided by Mean Squared Error loss to ensure regression accuracy. Key constraints include mutual information preservation for retaining task-relevant information, symmetric Kullback-Leibler divergence for feature extraction, fusion, and alignment, and inter-model contrastive loss to combat representation drift and catastrophic forgetting in non-IID contexts. This design keeps feature extraction focused on enhancing downstream regression task performance. Experimental results on both simulations and real-world datasets demonstrate that this method significantly outperforms traditional feature extraction techniques in improving regression outcomes. <div>
arXiv:2512.02076v1 Announce Type: new 
Abstract: This study focuses on the feature extraction problem in multi-modal data regression. To address three core challenges in real-world scenarios: limited and non-IID data, effective extraction and fusion of multi-modal information, and susceptibility to catastrophic forgetting in model learning, a task-driven supervised multi-modal federated feature extraction method is proposed. The method integrates multi-modal information extraction and contrastive learning mechanisms, and can adapt to different neural network structures as the latent mapping functions for data of each modality. It supports each client to independently learn low-dimensional representations of multi-modal data, and can flexibly control the degree of retention of effective information about the response variable in the predictive variables within the low-dimensional features through parameter tuning. The multi-constraint learning framework constructed by the method guarantees regression accuracy using Mean Squared Error loss. Through the synergistic effect of mutual information preservation constraint, symmetric Kullback-Leibler divergence constraint, and inter-model contrastive constraint, it achieves the retention of task-related information, the extraction, fusion, and alignment of multi-modal features, and the mitigation of representation drift and catastrophic forgetting in non-IID scenarios, respectively. This ensures that the feature extraction process always centers on improving the performance of downstream regression tasks. Experimental results from simulations and real-world data analysis demonstrate that the proposed method achieves more significant performance improvement on downstream regression tasks compared with classical feature extraction techniques.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-View Topology-Aware Graph Representation Learning</title>
<link>https://arxiv.org/abs/2512.02130</link>
<guid>https://arxiv.org/abs/2512.02130</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph Classification, Graph Neural Networks, Persistent Homology, Contrastive Learning, Topological Embeddings<br /><br />Summary:<br /><br />1. The article addresses the challenge in graph classification tasks where Graph Neural Networks (GNNs) primarily focus on local structural features but often fail to capture important global topological characteristics. <br />2. To overcome this limitation, the authors propose GraphTCL, a novel dual-view contrastive learning framework that combines structural embeddings generated by GNNs with topological embeddings computed using persistent homology, a method from topological data analysis.<br />3. GraphTCL employs a cross-view contrastive loss that aligns the complementary structural and topological representations, thereby enhancing the quality of the learned graph embeddings.<br />4. The effectiveness of GraphTCL is validated through extensive experiments on standard benchmark datasets, including TU and OGB molecular graph collections, showing consistent improvement over existing state-of-the-art graph classification methods.<br />5. The work demonstrates the significance of incorporating topology-aware techniques into contrastive learning frameworks to advance the performance and robustness of graph representation learning models in various applications such as chemistry, social network analysis, and bioinformatics. <div>
arXiv:2512.02130v1 Announce Type: new 
Abstract: Graph classification has gained significant attention due to its applications in chemistry, social networks, and bioinformatics. While Graph Neural Networks (GNNs) effectively capture local structural patterns, they often overlook global topological features that are critical for robust representation learning. In this work, we propose GraphTCL, a dual-view contrastive learning framework that integrates structural embeddings from GNNs with topological embeddings derived from persistent homology. By aligning these complementary views through a cross-view contrastive loss, our method enhances representation quality and improves classification performance. Extensive experiments on benchmark datasets, including TU and OGB molecular graphs, demonstrate that GraphTCL consistently outperforms state-of-the-art baselines. This study highlights the importance of topology-aware contrastive learning for advancing graph representation methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How Market Volatility Shapes Algorithmic Collusion: A Comparative Analysis of Learning-Based Pricing Algorithms</title>
<link>https://arxiv.org/abs/2512.02134</link>
<guid>https://arxiv.org/abs/2512.02134</guid>
<content:encoded><![CDATA[
<div> autonomous pricing, reinforcement learning, collusion, demand shocks, duopoly models<br /><br />Summary:<br /><br />This paper analyzes the behavior of four autonomous pricing algorithms—Q-Learning, PSO, Double DQN, and DDPG—within three classic duopoly market models: Logit, Hotelling, and Linear. The study incorporates various demand-shock regimes generated by auto-regressive processes to simulate realistic demand conditions. Using profit- and price-based collusion indices, the researchers examine how the interaction between algorithms, market structures, and stochastic demand influences competitive outcomes. It is found that reinforcement-learning algorithms often maintain supra-competitive pricing under stable demand, with the DDPG algorithm showing the strongest collusive behavior. The response to demand shocks varies by market type: Logit markets experience significant performance declines, Hotelling markets remain stable, and Linear markets see an increase in profits as a result of shocks. Despite changes in absolute performance metrics, the relative performance ranking among the algorithms remains consistent across different environments. These findings highlight the significant roles that market structure and demand uncertainty play in shaping algorithmic competition and offer valuable insights for ongoing policy discussions concerning the regulation of autonomous pricing algorithms. <div>
arXiv:2512.02134v1 Announce Type: new 
Abstract: Autonomous pricing algorithms are increasingly influencing competition in digital markets; however, their behavior under realistic demand conditions remains largely unexamined. This paper offers a thorough analysis of four pricing algorithms -- Q-Learning, PSO, Double DQN, and DDPG -- across three classic duopoly models (Logit, Hotelling, Linear) and under various demand-shock regimes created by auto-regressive processes. By utilizing profit- and price-based collusion indices, we investigate how the interactions among algorithms, market structure, and stochastic demand collaboratively influence competitive outcomes. Our findings reveal that reinforcement-learning algorithms often sustain supra-competitive prices under stable demand, with DDPG demonstrating the most pronounced collusive tendencies. Demand shocks produce notably varied effects: Logit markets suffer significant performance declines, Hotelling markets remain stable, and Linear markets experience shock-induced profit inflation. Despite marked changes in absolute performance, the relative rankings of the algorithms are consistent across different environments. These results underscore the critical importance of market structure and demand uncertainty in shaping algorithmic competition, while also contributing to the evolving policy discussions surrounding autonomous pricing behavior.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CLEF: Clinically-Guided Contrastive Learning for Electrocardiogram Foundation Models</title>
<link>https://arxiv.org/abs/2512.02180</link>
<guid>https://arxiv.org/abs/2512.02180</guid>
<content:encoded><![CDATA[
<div> Keywords: ECG, self-supervised learning, contrastive learning, clinical metadata, foundation models<br /><br />Summary:<br /><br />The study presents CLEF, a novel contrastive learning approach for pretraining ECG foundation models by incorporating clinical metadata, specifically clinical risk scores, to adaptively weight negative pairs during training. Unlike previous self-supervised methods, CLEF aligns ECG embeddings with meaningful clinical differences and includes a mechanism to handle missing metadata. The models were pretrained on 12-lead ECG data from 161,000 patients in the MIMIC-IV dataset without requiring annotations for individual ECG samples. CLEF was evaluated on 18 clinical classification and regression tasks across seven external datasets and benchmarked against multiple state-of-the-art self-supervised algorithms and foundation models. Results show that when pretrained on 12-lead data and tested on single-lead (lead-I) data, CLEF consistently outperforms self-supervised baselines, achieving at least a 2.6% improvement in average AUROC for classification tasks and a 3.2% reduction in mean absolute errors for regression. Compared to other self-supervised learning methods, CLEF improves AUROC by at least 1.8%. Additionally, when trained only on lead-I data for classification, CLEF performs comparably to supervised state-of-the-art ECGFounder. The work advances scalable, accurate single-lead ECG analysis, with potential applications in remote cardiovascular health monitoring. Code and pretrained models are publicly available. <div>
arXiv:2512.02180v1 Announce Type: new 
Abstract: The electrocardiogram (ECG) is a key diagnostic tool in cardiovascular health. Single-lead ECG recording is integrated into both clinical-grade and consumer wearables. While self-supervised pretraining of foundation models on unlabeled ECGs improves diagnostic performance, existing approaches do not incorporate domain knowledge from clinical metadata. We introduce a novel contrastive learning approach that utilizes an established clinical risk score to adaptively weight negative pairs: clinically-guided contrastive learning. It aligns the similarities of ECG embeddings with clinically meaningful differences between subjects, with an explicit mechanism to handle missing metadata. On 12-lead ECGs from 161K patients in the MIMIC-IV dataset, we pretrain single-lead ECG foundation models at three scales, collectively called CLEF, using only routinely collected metadata without requiring per-sample ECG annotations. We evaluate CLEF on 18 clinical classification and regression tasks across 7 held-out datasets, and benchmark against 5 foundation model baselines and 3 self-supervised algorithms. When pretrained on 12-lead ECG data and tested on lead-I data, CLEF outperforms self-supervised foundation model baselines: the medium-sized CLEF achieves average AUROC improvements of at least 2.6% in classification and average reductions in MAEs of at least 3.2% in regression. Comparing with existing self-supervised learning algorithms, CLEF improves the average AUROC by at least 1.8%. Moreover, when pretrained only on lead-I data for classification tasks, CLEF performs comparably to the state-of-the-art ECGFounder, which was trained in a supervised manner. Overall, CLEF enables more accurate and scalable single-lead ECG analysis, advancing remote health monitoring. Code and pretrained CLEF models are available at: github.com/Nokia-Bell-Labs/ecg-foundation-model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Enforcing Orderedness to Improve Feature Consistency</title>
<link>https://arxiv.org/abs/2512.02194</link>
<guid>https://arxiv.org/abs/2512.02194</guid>
<content:encoded><![CDATA[
<div> Sparse autoencoders, Ordered Sparse Autoencoders, interpretability, permutation non-identifiability, sparse dictionary learning<br /><br />Summary:<br /><br />1. The paper introduces Ordered Sparse Autoencoders (OSAE), a novel extension of sparse autoencoders aimed at improving interpretability and consistency in learned neural network features.<br /><br />2. OSAEs build on the concept of Matryoshka sparse autoencoders but add two significant innovations: the establishment of a strict ordering of latent features and deterministic usage of every feature dimension, eliminating reliance on sampling-based approximations used in prior methods.<br /><br />3. The authors provide a theoretical analysis demonstrating that OSAEs address permutation non-identifiability issues in sparse dictionary learning, ensuring that learned solutions are unique except for inherent symmetries.<br /><br />4. Empirically, OSAEs were tested on large language models Gemma2-2B and Pythia-70M, where they showed improved consistency in feature representations compared to Matryoshka sparse autoencoder baselines.<br /><br />5. The work contributes to better interpretability and stable feature extraction in neural networks, which is crucial for understanding model behavior and improving downstream tasks that rely on sparse latent representations. <div>
arXiv:2512.02194v1 Announce Type: new 
Abstract: Sparse autoencoders (SAEs) have been widely used for interpretability of neural networks, but their learned features often vary across seeds and hyperparameter settings. We introduce Ordered Sparse Autoencoders (OSAE), which extend Matryoshka SAEs by (1) establishing a strict ordering of latent features and (2) deterministically using every feature dimension, avoiding the sampling-based approximations of prior nested SAE methods. Theoretically, we show that OSAEs resolve permutation non-identifiability in settings of sparse dictionary learning where solutions are unique (up to natural symmetries). Empirically on Gemma2-2B and Pythia-70M, we show that OSAEs can help improve consistency compared to Matryoshka baselines.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modelling the Doughnut of social and planetary boundaries with frugal machine learning</title>
<link>https://arxiv.org/abs/2512.02200</link>
<guid>https://arxiv.org/abs/2512.02200</guid>
<content:encoded><![CDATA[
<div> Keywords: Doughnut economics, machine learning, macroeconomic model, reinforcement learning, sustainability<br /><br />Summary:<br /><br />1. The paper explores the Doughnut framework, which integrates social and planetary boundaries to assess sustainability. 2. It presents a proof-of-concept demonstrating how machine learning (ML) methods can be applied to a simplified macroeconomic model based on the Doughnut concept. 3. Initially, ML techniques, such as a Random Forest Classifier, are used to identify policy parameters that enable living within the Doughnut’s ecological and social limits. 4. Subsequently, reinforcement learning, specifically $Q$-learning, is employed to discover an optimal policy trajectory within the parameter space that guides decision-making towards sustainable outcomes. 5. The study highlights that these frugal ML methods can effectively find combinations of policies that satisfy both environmental and social sustainability criteria. 6. Finally, the paper suggests that future work will involve extending these approaches to more complex ecological macroeconomic models, paving the way for more robust and applicable sustainability policy tools. <div>
arXiv:2512.02200v1 Announce Type: new 
Abstract: The 'Doughnut' of social and planetary boundaries has emerged as a popular framework for assessing environmental and social sustainability. Here, we provide a proof-of-concept analysis that shows how machine learning (ML) methods can be applied to a simple macroeconomic model of the Doughnut. First, we show how ML methods can be used to find policy parameters that are consistent with 'living within the Doughnut'. Second, we show how a reinforcement learning agent can identify the optimal trajectory towards desired policies in the parameter space. The approaches we test, which include a Random Forest Classifier and $Q$-learning, are frugal ML methods that are able to find policy parameter combinations that achieve both environmental and social sustainability. The next step is the application of these methods to a more complex ecological macroeconomic model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WhAM: Towards A Translative Model of Sperm Whale Vocalization</title>
<link>https://arxiv.org/abs/2512.02206</link>
<guid>https://arxiv.org/abs/2512.02206</guid>
<content:encoded><![CDATA[
<div> Keywords: sperm whales, codas, transformer model, acoustic token, synthetic audio<br /><br />Summary:<br />1. The paper introduces WhAM (Whale Acoustics Model), the first transformer-based model designed to generate synthetic sperm whale codas, which are short sequences of clicks used in their communication.<br />2. WhAM is developed by fine-tuning VampNet, a masked acoustic token model originally pretrained on musical audio, utilizing a dataset of 10,000 sperm whale coda recordings collected over two decades.<br />3. The model generates high-fidelity synthetic codas through an iterative masked token prediction approach, maintaining important acoustic features found in the original recordings.<br />4. Evaluation of WhAM's synthetic codas is conducted using the Fréchet Audio Distance metric as well as perceptual assessments from expert marine biologists, confirming the quality and realism of the synthetic audio.<br />5. Despite being trained primarily for generation tasks, WhAM's learned acoustic representations also perform strongly on downstream classification tasks such as rhythm recognition, social unit identification, and vowel classification.<br />6. The project's codebase is publicly available on GitHub, facilitating reproducibility and further research in marine bioacoustics and audio generation. <div>
arXiv:2512.02206v1 Announce Type: new 
Abstract: Sperm whales communicate in short sequences of clicks known as codas. We present WhAM (Whale Acoustics Model), the first transformer-based model capable of generating synthetic sperm whale codas from any audio prompt. WhAM is built by finetuning VampNet, a masked acoustic token model pretrained on musical audio, using 10k coda recordings collected over the past two decades. Through iterative masked token prediction, WhAM generates high-fidelity synthetic codas that preserve key acoustic features of the source recordings. We evaluate WhAM's synthetic codas using Fr\'echet Audio Distance and through perceptual studies with expert marine biologists. On downstream classification tasks including rhythm, social unit, and vowel classification, WhAM's learned representations achieve strong performance, despite being trained for generation rather than classification. Our code is available at https://github.com/Project-CETI/wham
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>InstructLR: A Scalable Approach to Create Instruction Dataset for Under-Resourced Languages</title>
<link>https://arxiv.org/abs/2512.02213</link>
<guid>https://arxiv.org/abs/2512.02213</guid>
<content:encoded><![CDATA[
<div> Keywords: low-resource languages, instruction datasets, large language models, quality filtering, multi-domain benchmarks<br /><br />Summary: This paper addresses the challenge of effective text generation and chat interfaces for low-resource languages (LRLs), particularly those spoken across Africa, due to the scarcity of high-quality instruction datasets. Existing methods like automated translation and synthetic data generation often result in outputs lacking fluency and orthographic consistency. To overcome these limitations, the authors propose InstructLR, a novel framework that combines large language model-driven text generation with a dual-layer quality filtering system. The first filtering layer is automated, leveraging a retrieval-augmented generation (RAG)-based n-shot prompting mechanism to ensure initial data quality. This is complemented by a human-in-the-loop validation layer to further enhance dataset reliability and correctness. Drawing inspiration from established benchmarks such as MMLU for task definition, InstructLR has enabled the creation of three comprehensive multi-domain instruction benchmarks for African LRLs: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k. These benchmarks aim to facilitate improved support and development of large language models capable of handling diverse and complex tasks in these underrepresented languages, advancing the field of language technology inclusivity and performance for LRLs. <div>
arXiv:2512.02213v1 Announce Type: new 
Abstract: Effective text generation and chat interfaces for low-resource languages (LRLs) remain a challenge for state-of-the-art large language models (LLMs) to support. This is mainly due to the difficulty of curating high-quality instruction datasets for LRLs, a limitation prevalent in the languages spoken across the African continent and other regions. Current approaches, such as automated translation and synthetic data generation, frequently yield outputs that lack fluency or even orthographic consistency. In this paper, we introduce InstructLR, a novel framework designed to generate high-quality instruction datasets for LRLs. Our approach integrates LLM-driven text generation with a dual-layer quality filtering mechanism: an automated filtering layer based on retrieval-augmented-generation (RAG)-based n-shot prompting, and a human-in-the-loop validation layer. Drawing inspiration from benchmarks such as MMLU in task definition, InstructLR has facilitated the creation of three multi-domain instruction benchmarks: ZarmaInstruct-50k, BambaraInstruct-50k, and FulfuldeInstruct-50k.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Improved Training Mechanism for Reinforcement Learning via Online Model Selection</title>
<link>https://arxiv.org/abs/2512.02214</link>
<guid>https://arxiv.org/abs/2512.02214</guid>
<content:encoded><![CDATA[
<div> Keywords: online model selection, reinforcement learning, resource allocation, non-stationary dynamics, training stability<br /><br />Summary:<br /><br />This paper addresses the challenge of online model selection within reinforcement learning (RL), proposing methods that enable a selector to adaptively choose the most suitable RL agent configuration from a given set. The central objective is to improve training efficiency and overall performance by incorporating online model selection into RL workflows. The authors analyze theoretical conditions supporting the identification of optimal configurations, focusing on three practical criteria: (1) efficient allocation of computational and data resources to avoid wasteful training, (2) adaptation capabilities under non-stationary or changing environmental dynamics, ensuring robustness over time, and (3) stability of training outcomes across different random seeds to guarantee reliable performance. Theoretical insights are complemented by empirical results demonstrating the effectiveness of the approach on multiple model selection tasks, such as choosing neural network architectures, tuning step-sizes, and enabling self model selection procedures. These findings suggest that integrating online model selection strategies in reinforcement learning can lead to measurable improvements in efficiency, adaptability, and reproducibility of RL training processes. <div>
arXiv:2512.02214v1 Announce Type: new 
Abstract: We study the problem of online model selection in reinforcement learning, where the selector has access to a class of reinforcement learning agents and learns to adaptively select the agent with the right configuration. Our goal is to establish the improved efficiency and performance gains achieved by integrating online model selection methods into reinforcement learning training procedures. We examine the theoretical characterizations that are effective for identifying the right configuration in practice, and address three practical criteria from a theoretical perspective: 1) Efficient resource allocation, 2) Adaptation under non-stationary dynamics, and 3) Training stability across different seeds. Our theoretical results are accompanied by empirical evidence from various model selection tasks in reinforcement learning, including neural architecture selection, step-size selection, and self model selection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Reasoning with Photonic Bayesian Machines</title>
<link>https://arxiv.org/abs/2512.02217</link>
<guid>https://arxiv.org/abs/2512.02217</guid>
<content:encoded><![CDATA[
<div> Keywords: photonic Bayesian machine, uncertainty reasoning, chaotic light sources, Bayesian Neural Networks, probabilistic convolutions<br /><br />Summary:<br /><br />This paper addresses the critical need for uncertainty awareness in AI systems used in safety-critical domains such as medical diagnosis and autonomous mobility. The authors propose a photonic Bayesian machine that exploits the inherent randomness found in chaotic light sources to perform uncertainty reasoning within Bayesian Neural Networks. This analog photonic processor integrates a high-speed 1.28 Tbit/s digital interface compatible with PyTorch, facilitating probabilistic convolutions with an impressive speed of 37.5 picoseconds per convolution. The system is demonstrated on a practical task involving the simultaneous classification and out-of-domain detection of blood cell microscope images, showing effective differentiation between aleatoric and epistemic uncertainties. By harnessing photonics, the approach removes the traditional bottleneck caused by pseudo-random number generation in digital architectures, significantly reducing sampling costs that typically hinder probabilistic model efficiency. The novel photonic Bayesian machine thereby enables high-throughput, trustworthy AI systems capable of reliable uncertainty quantification essential for real-world critical applications. <div>
arXiv:2512.02217v1 Announce Type: new 
Abstract: Artificial intelligence (AI) systems increasingly influence safety-critical aspects of society, from medical diagnosis to autonomous mobility, making uncertainty awareness a central requirement for trustworthy AI. We present a photonic Bayesian machine that leverages the inherent randomness of chaotic light sources to enable uncertainty reasoning within the framework of Bayesian Neural Networks. The analog processor features a 1.28 Tbit/s digital interface compatible with PyTorch, enabling probabilistic convolutions processing within 37.5 ps per convolution. We use the system for simultaneous classification and out-of-domain detection of blood cell microscope images and demonstrate reasoning between aleatoric and epistemic uncertainties. The photonic Bayesian machine removes the bottleneck of pseudo random number generation in digital systems, minimizes the cost of sampling for probabilistic models, and thus enables high-speed trustworthy AI systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Approximation of Phylogenetic Distance Functions by Artificial Neural Networks</title>
<link>https://arxiv.org/abs/2512.02223</link>
<guid>https://arxiv.org/abs/2512.02223</guid>
<content:encoded><![CDATA[
<div> Keywords: phylogenetics, neural networks, distance functions, molecular evolution, hierarchical clustering<br /><br />Summary:  
This work addresses the challenge of inferring phylogenetic relationships among organisms, a key task in biology. Traditional methods like distance-based hierarchical clustering, although initially successful, have been largely replaced by Bayesian and maximum likelihood methods grounded in complex molecular evolution models. The authors propose minimal neural network architectures designed to approximate classic phylogenetic distance functions efficiently. These networks capture essential properties needed to learn distances under various molecular evolutionary models. Unlike heavy model-based inference or recent model-free convolutional and transformer networks, these architectures maintain a small computational footprint and scale effectively with increasing numbers of taxa and molecular characters. The networks demonstrate strong generalization ability, performing well on diverse data. When trained on suitable datasets, these learned distance functions achieve accuracy comparable to state-of-the-art phylogenetic inference methods, offering a computationally efficient alternative without sacrificing performance. <div>
arXiv:2512.02223v1 Announce Type: new 
Abstract: Inferring the phylogenetic relationships among a sample of organisms is a fundamental problem in modern biology. While distance-based hierarchical clustering algorithms achieved early success on this task, these have been supplanted by Bayesian and maximum likelihood search procedures based on complex models of molecular evolution. In this work we describe minimal neural network architectures that can approximate classic phylogenetic distance functions and the properties required to learn distances under a variety of molecular evolutionary models. In contrast to model-based inference (and recently proposed model-free convolutional and transformer networks), these architectures have a small computational footprint and are scalable to large numbers of taxa and molecular characters. The learned distance functions generalize well and, given an appropriate training dataset, achieve results comparable to state-of-the art inference methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Effect of Enforcing Fairness on Reshaping Explanations in Machine Learning Models</title>
<link>https://arxiv.org/abs/2512.02265</link>
<guid>https://arxiv.org/abs/2512.02265</guid>
<content:encoded><![CDATA[
<div> Fairness, Explainability, Shapley values, Healthcare, Bias mitigation<br /><br />Summary:<br /><br />1. The study investigates the relationship between model fairness improvements and explainability in trustworthy machine learning for healthcare applications.<br /><br />2. It highlights that while improving fairness often affects predictive performance, the impact on explanations, especially Shapley-based feature importance rankings, remains underexplored.<br /><br />3. Using three datasets related to pediatric urinary tract infection risk, direct anticoagulant bleeding risk, and recidivism risk, the authors assess changes in feature rankings after applying fairness constraints.<br /><br />4. Multiple model classes were evaluated to examine the stability of Shapley value explanations across different fairness interventions.<br /><br />5. Results show that increasing fairness across racial subgroups can significantly shift feature importance rankings, sometimes differently for each subgroup, indicating potential challenges for clinical trust.<br /><br />6. The findings emphasize the necessity to consider accuracy, fairness, and explainability together during model evaluation rather than treating them separately. <div>
arXiv:2512.02265v1 Announce Type: new 
Abstract: Trustworthy machine learning in healthcare requires strong predictive performance, fairness, and explanations. While it is known that improving fairness can affect predictive performance, little is known about how fairness improvements influence explainability, an essential ingredient for clinical trust. Clinicians may hesitate to rely on a model whose explanations shift after fairness constraints are applied. In this study, we examine how enhancing fairness through bias mitigation techniques reshapes Shapley-based feature rankings. We quantify changes in feature importance rankings after applying fairness constraints across three datasets: pediatric urinary tract infection risk, direct anticoagulant bleeding risk, and recidivism risk. We also evaluate multiple model classes on the stability of Shapley-based rankings. We find that increasing model fairness across racial subgroups can significantly alter feature importance rankings, sometimes in different ways across groups. These results highlight the need to jointly consider accuracy, fairness, and explainability in model assessment rather than in isolation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limitations of Membership Queries in Testable Learning</title>
<link>https://arxiv.org/abs/2512.02279</link>
<guid>https://arxiv.org/abs/2512.02279</guid>
<content:encoded><![CDATA[
<div> Membership queries, testable learning, distribution-specific learning, refutation, statistical queries<br /><br />Summary:<br /><br />1. The paper investigates the impact of membership queries (MQ) in the testable learning model introduced by Rubinfeld and Vasilyan (2023), a model where the learning algorithm must produce a near-optimal hypothesis if the data distribution satisfies certain properties. 2. It shows that membership queries do not reduce the time complexity of testable learning algorithms beyond what is achievable by sample-only distribution-specific learning algorithms. 3. The authors provide a general reduction from sample-based refutation of Boolean concept classes—frameworks previously studied by Vadhan (2017) and Klivans and Li (2018)—to testable learning with queries (TL-Q), establishing strong lower bounds on the efficiency gains possible from MQ in this setting. 4. Consequently, for any concept class and distribution family, no TL-Q algorithm using m samples can be super-polynomially faster than the best m-sample PAC learner. 5. Additionally, the paper defines a class of “statistical” MQ algorithms common in distribution-specific learning, such as those based on influence estimation and subcube-conditional statistical queries, and demonstrates that TL-Q algorithms in this class correspond to efficient statistical query refutation and learning algorithms. This leads to the conclusion that known SQ dimension lower bounds imply many efficient MQ learners cannot be testable. <div>
arXiv:2512.02279v1 Announce Type: new 
Abstract: Membership queries (MQ) often yield speedups for learning tasks, particularly in the distribution-specific setting. We show that in the \emph{testable learning} model of Rubinfeld and Vasilyan [RV23], membership queries cannot decrease the time complexity of testable learning algorithms beyond the complexity of sample-only distribution-specific learning. In the testable learning model, the learner must output a hypothesis whenever the data distribution satisfies a desired property, and if it outputs a hypothesis, the hypothesis must be near-optimal.
  We give a general reduction from sample-based \emph{refutation} of boolean concept classes, as presented in [Vadhan17, KL18], to testable learning with queries (TL-Q). This yields lower bounds for TL-Q via the reduction from learning to refutation given in [KL18]. The result is that, relative to a concept class and a distribution family, no $m$-sample TL-Q algorithm can be super-polynomially more time-efficient than the best $m$-sample PAC learner.
  Finally, we define a class of ``statistical'' MQ algorithms that encompasses many known distribution-specific MQ learners, such as those based on influence estimation or subcube-conditional statistical queries. We show that TL-Q algorithms in this class imply efficient statistical-query refutation and learning algorithms. Thus, combined with known SQ dimension lower bounds, our results imply that these efficient membership query learners cannot be made testable.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training Dynamics of Learning 3D-Rotational Equivariance</title>
<link>https://arxiv.org/abs/2512.02303</link>
<guid>https://arxiv.org/abs/2512.02303</guid>
<content:encoded><![CDATA[
<div> Data augmentation, equivariance error, 3D-rotation, molecular tasks, loss landscape<br /><br />Summary:<br /><br />This paper addresses how quickly and effectively models trained with data augmentation learn to respect symmetries, specifically 3D rotational equivariance. The authors derive a principled measure of equivariance error for convex losses, quantifying the portion of total loss due to imperfect symmetry learning. Their empirical investigation focuses on high-dimensional molecular tasks, including flow matching, force field prediction, and voxel denoising. Results reveal that models reduce equivariance error rapidly to less than or equal to 2% of held-out loss within 1,000 to 10,000 training steps, consistent across various model and dataset sizes. This rapid convergence occurs because learning 3D rotational equivariance presents an easier optimization problem with a smoother and better-conditioned loss landscape compared to the main prediction task. Interestingly, the penalty in loss for models not enforcing equivariance remains small throughout training, which means non-equivariant models may achieve lower test losses per GPU-hour unless the efficiency gap associated with equivariant learning is reduced. Additionally, the paper investigates both experimentally and theoretically how relative equivariance error relates to learning gradients and model parameters, offering insights into the interplay between symmetry learning and optimization dynamics. <div>
arXiv:2512.02303v1 Announce Type: new 
Abstract: While data augmentation is widely used to train symmetry-agnostic models, it remains unclear how quickly and effectively they learn to respect symmetries. We investigate this by deriving a principled measure of equivariance error that, for convex losses, calculates the percent of total loss attributable to imperfections in learned symmetry. We focus our empirical investigation to 3D-rotation equivariance on high-dimensional molecular tasks (flow matching, force field prediction, denoising voxels) and find that models reduce equivariance error quickly to $\leq$2\% held-out loss within 1k-10k training steps, a result robust to model and dataset size. This happens because learning 3D-rotational equivariance is an easier learning task, with a smoother and better-conditioned loss landscape, than the main prediction task. For 3D rotations, the loss penalty for non-equivariant models is small throughout training, so they may achieve lower test loss than equivariant models per GPU-hour unless the equivariant ``efficiency gap'' is narrowed. We also experimentally and theoretically investigate the relationships between relative equivariance error, learning gradients, and model parameters.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Unlocking the Power of Boltzmann Machines by Parallelizable Sampler and Efficient Temperature Estimation</title>
<link>https://arxiv.org/abs/2512.02323</link>
<guid>https://arxiv.org/abs/2512.02323</guid>
<content:encoded><![CDATA[
<div> Boltzmann machines, simulated bifurcation, Langevin SB, inverse temperature estimation, sampler-adaptive learning  

<br /><br />Summary:  
This paper introduces an innovative approach to improve the training efficiency and versatility of Boltzmann machines (BMs), energy-based generative models that traditionally require costly training methods. The authors propose a novel sampling method named Langevin Simulated Bifurcation (LSB), inspired by a quantum-inspired combinatorial optimization technique, simulated bifurcation (SB). LSB allows for parallelized sampling that matches the accuracy of conventional MCMC methods but operates more efficiently. Unlike previous approaches limited to Restricted Boltzmann Machines (RBMs), this method applies to BMs with general couplings, significantly broadening applicability. However, LSB alone cannot control the inverse temperature of the sampled Boltzmann distribution, which is critical for stable learning and performance. To resolve this, the authors develop a method called conditional expectation matching (CEM), which efficiently estimates the inverse temperature during training. By integrating LSB with CEM, they create a combined framework named sampler-adaptive learning (SAL). SAL facilitates efficient learning with enhanced model expressiveness beyond RBMs, advancing the practical usability of BMs. This work paves the way for more effective energy-based generative modeling by addressing sampling parallelization and temperature control challenges. <div>
arXiv:2512.02323v1 Announce Type: new 
Abstract: Boltzmann machines (BMs) are powerful energy-based generative models, but their heavy training cost has largely confined practical use to Restricted BMs (RBMs) trained with an efficient learning method called contrastive divergence. More accurate learning typically requires Markov chain Monte Carlo (MCMC) Boltzmann sampling, but it is time-consuming due to the difficulty of parallelization for more expressive models. To address this limitation, we first propose a new Boltzmann sampler inspired by a quantum-inspired combinatorial optimization called simulated bifurcation (SB). This SB-inspired approach, which we name Langevin SB (LSB), enables parallelized sampling while maintaining accuracy comparable to MCMC. Furthermore, this is applicable not only to RBMs but also to BMs with general couplings. However, LSB cannot control the inverse temperature of the output Boltzmann distribution, which hinders learning and degrades performance. To overcome this limitation, we also developed an efficient method for estimating the inverse temperature during the learning process, which we call conditional expectation matching (CEM). By combining LSB and CEM, we establish an efficient learning framework for BMs with greater expressive power than RBMs. We refer to this framework as sampler-adaptive learning (SAL). SAL opens new avenues for energy-based generative modeling beyond RBMs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Retrieval-Augmented Memory for Online Learning</title>
<link>https://arxiv.org/abs/2512.02333</link>
<guid>https://arxiv.org/abs/2512.02333</guid>
<content:encoded><![CDATA[
<div> Keywords: Retrieval-augmented models, online learning, concept drift, memory replay, non-stationary environments<br /><br />Summary:<br /><br />1. The paper addresses the challenge of streaming supervised learning in non-stationary environments, specifically focusing on handling concept drift where data distributions change over time.<br />2. It introduces Retrieval-Augmented Memory for Online Learning (RAM-OL), an extension of stochastic gradient descent that incorporates a small buffer of past examples to improve online classification.<br />3. RAM-OL retrieves nearest neighbours of the current input from the buffer in the hidden representation space and jointly updates the model using both the current input and retrieved examples.<br />4. Two replay variants are proposed: a naive replay which simply reuses neighbours, and a gated replay that applies constraints such as a time window, similarity thresholds, and gradient reweighting to balance rapid adaptation with robustness against outdated data.<br />5. Theoretically, the approach is analyzed under a bounded drift model, showing how retrieval can lower adaptation costs and improve regret bounds when patterns recur.<br />6. Empirical evaluation on three real-world datasets (electricity pricing, electricity load, and airline delays) demonstrates that RAM-OL significantly improves accuracy and reduces variance in strongly drifting data, with the gated variant performing comparably to baseline methods in noisier data.<br />7. The study concludes that retrieval-augmented memory is an effective, practical, and robust method for online learning under concept drift scenarios. <div>
arXiv:2512.02333v1 Announce Type: new 
Abstract: Retrieval-augmented models couple parametric predictors with non-parametric memories, but their use in streaming supervised learning with concept drift is not well understood. We study online classification in non-stationary environments and propose Retrieval-Augmented Memory for Online Learning (RAM-OL), a simple extension of stochastic gradient descent that maintains a small buffer of past examples. At each time step, RAM-OL retrieves a few nearest neighbours of the current input in the hidden representation space and updates the model jointly on the current example and the retrieved neighbours. We compare a naive replay variant with a gated replay variant that constrains neighbours using a time window, similarity thresholds, and gradient reweighting, in order to balance fast reuse of relevant past data against robustness to outdated regimes. From a theoretical perspective, we interpret RAM-OL under a bounded drift model and discuss how retrieval can reduce adaptation cost and improve regret constants when patterns recur over time. Empirically, we instantiate RAM-OL on a simple online multilayer perceptron and evaluate it on three real-world data streams derived from electricity pricing, electricity load, and airline delay data. On strongly and periodically drifting streams, RAM-OL improves prequential accuracy by up to about seven percentage points and greatly reduces variance across random seeds, while on a noisy airline stream the gated variant closely matches the purely online baseline. These results show that retrieval-augmented memory is a practical and robust tool for online learning under concept drift.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting MBTA Transit Dynamics: A Performance Benchmarking of Statistical and Machine Learning Models</title>
<link>https://arxiv.org/abs/2512.02336</link>
<guid>https://arxiv.org/abs/2512.02336</guid>
<content:encoded><![CDATA[
<div> Keywords: MBTA, subway usage prediction, delay modeling, machine learning, feature importance<br /><br />Summary:<br />The paper focuses on predicting subway usage and delays within the Massachusetts Bay Transportation Authority (MBTA) public transit system in Boston. It evaluates 10 statistical and machine learning models to predict next-day gated station entries, a proxy for subway ridership, incorporating factors such as day of the week, season, pressure, wind speed, temperature, and precipitation. For delay prediction, an additional model—a self-exciting point process model—is introduced to extend the evaluation to 11 models, highlighting a novel application of point-process frameworks for MBTA delay analysis. The research experiments with selective feature inclusion to identify the importance of different factors in model performance. Results show that including day of week or season data significantly improves the predictive accuracy over models without them. Contrarily, adding weather data (pressure, wind speed, temperature, precipitation) generally degrades performance, likely due to overfitting issues. Root Mean Squared Error (RMSE) is used to assess model accuracy. The study provides insights into which variables are most relevant for forecasting transit usage and delays, suggesting that temporal features such as day and season are more impactful than weather variables for predictive models in the MBTA context. <div>
arXiv:2512.02336v1 Announce Type: new 
Abstract: The Massachusetts Bay Transportation Authority (MBTA) is the main public transit provider in Boston, operating multiple means of transport, including trains, subways, and buses. However, the system often faces delays and fluctuations in ridership volume, which negatively affect efficiency and passenger satisfaction. To further understand this phenomenon, this paper compares the performance of existing and unique methods to determine the best approach in predicting gated station entries in the subway system (a proxy for subway usage) and the number of delays in the overall MBTA system. To do so, this research considers factors that tend to affect public transportation, such as day of week, season, pressure, wind speed, average temperature, and precipitation. This paper evaluates the performance of 10 statistical and machine learning models on predicting next-day subway usage. On predicting delay count, the number of models is extended to 11 per day by introducing a self-exciting point process model, representing a unique application of a point-process framework for MBTA delay modeling. This research involves experimenting with the selective inclusion of features to determine feature importance, testing model accuracy via Root Mean Squared Error (RMSE). Remarkably, it is found that providing either day of week or season data has a more substantial benefit to predictive accuracy compared to weather data; in fact, providing weather data generally worsens performance, suggesting a tendency of models to overfit.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SpecPV: Improving Self-Speculative Decoding for Long-Context Generation via Partial Verification</title>
<link>https://arxiv.org/abs/2512.02337</link>
<guid>https://arxiv.org/abs/2512.02337</guid>
<content:encoded><![CDATA[
<div> speculative decoding, long-context generation, large language models, verification bottleneck, SpecPV  

<br /><br />Summary:  
Large language models (LLMs) increasingly require efficient long-context generation to support complex tasks such as code generation, deep reasoning, and understanding long documents. Speculative decoding is an effective acceleration technique that uses a draft-verify paradigm, where a lightweight draft model proposes multiple candidate tokens and the target model verifies them. However, as context length increases, the verification step becomes the main bottleneck slowing down generation. To address this, the authors propose SpecPV, a self-speculative decoding method that speeds up verification by utilizing partial key-value (KV) states for fast checking and periodically performs full verification to correct accumulated errors. The approach balances efficiency and accuracy through this hybrid verification strategy. SpecPV is evaluated on multiple long-context benchmarks with different models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results demonstrate that SpecPV can achieve up to a 6x speedup in decoding compared to standard autoregressive decoding while only incurring minor performance degradation, significantly improving generation efficiency for long-context LLM applications. <div>
arXiv:2512.02337v1 Announce Type: new 
Abstract: Growing demands from tasks like code generation, deep reasoning, and long-document understanding have made long-context generation a crucial capability for large language models (LLMs). Speculative decoding is one of the most direct and effective approaches for accelerating generation. It follows a draft-verify paradigm, where a lightweight draft model proposes several candidate tokens and the target model verifies them. However, we find that as the context length grows, verification becomes the dominant bottleneck. To further accelerate speculative decoding in long-context generation, we introduce SpecPV, a self-speculative decoding approach that performs fast verification using partial key-value states (KV) and periodically applies full verification to eliminate accumulated errors. We validate SpecPV across multiple long-context benchmarks and models, including LLaMA-3.1-8B-Instruct and Qwen3-series. Experimental results show that SpecPV achieves up to 6x decoding speedup over standard autoregressive decoding with minor degradation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FOVA: Offline Federated Reinforcement Learning with Mixed-Quality Data</title>
<link>https://arxiv.org/abs/2512.02350</link>
<guid>https://arxiv.org/abs/2512.02350</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated Reinforcement Learning, offline learning, vote mechanism, advantage-weighted regression, policy improvement<br /><br />Summary:<br /><br />Offline Federated Reinforcement Learning (FRL) integrates federated learning with offline reinforcement learning, targeting distributed training from decentralized datasets. Existing offline FRL methods often suffer significant performance degradation when handling mixed-quality data collected by policies of varying quality across clients. To address this challenge, the paper proposes a novel vote-based offline FRL framework named FOVA, which utilizes a vote mechanism to accurately identify high-return actions during local policy evaluation. This strategy mitigates the adverse impact of low-quality behaviors resulting from heterogeneous local policies. Additionally, FOVA incorporates advantage-weighted regression (AWR) to establish consistent local and global training objectives, greatly improving both training efficiency and stability. The authors provide a rigorous theoretical analysis, demonstrating that the policy learned by FOVA guarantees strict improvement over the behavioral policy used to collect data. Comprehensive experiments on popular benchmarks validate the effectiveness of FOVA, showing it achieves substantial performance gains compared to existing baseline methods in offline FRL scenarios with mixed-quality data. Overall, the work advances offline FRL by enabling more robust and efficient policy learning in federated settings with heterogeneous client datasets. <div>
arXiv:2512.02350v1 Announce Type: new 
Abstract: Offline Federated Reinforcement Learning (FRL), a marriage of federated learning and offline reinforcement learning, has attracted increasing interest recently. Albeit with some advancement, we find that the performance of most existing offline FRL methods drops dramatically when provided with mixed-quality data, that is, the logging behaviors (offline data) are collected by policies with varying qualities across clients. To overcome this limitation, this paper introduces a new vote-based offline FRL framework, named FOVA. It exploits a \emph{vote mechanism} to identify high-return actions during local policy evaluation, alleviating the negative effect of low-quality behaviors from diverse local learning policies. Besides, building on advantage-weighted regression (AWR), we construct consistent local and global training objectives, significantly enhancing the efficiency and stability of FOVA. Further, we conduct an extensive theoretical analysis and rigorously show that the policy learned by FOVA enjoys strict policy improvement over the behavioral policy. Extensive experiments corroborate the significant performance gains of our proposed algorithm over existing baselines on widely used benchmarks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning in POMDP's via Direct Gradient Ascent</title>
<link>https://arxiv.org/abs/2512.02383</link>
<guid>https://arxiv.org/abs/2512.02383</guid>
<content:encoded><![CDATA[
<div> Gradient-based optimization, POMDP, GPOMDP algorithm, policy gradient, average reward<br /><br />Summary:<br /><br />This paper explores both theoretical foundations and experimental validations of gradient-based methods aimed at directly optimizing policy performance in controlled partially observable Markov decision processes (POMDPs). The authors introduce GPOMDP, an algorithm inspired by the REINFORCE approach, designed to estimate an approximation of the gradient of the average reward with respect to parameters of a stochastic policy. GPOMDP's main advantages are threefold: it needs only a single sample path from the underlying Markov chain, it employs a single free parameter \(\beta \in [0,1)\) that intuitively balances bias and variance, and it functions without any explicit knowledge of the system's true states. The paper provides rigorous proofs of convergence for the GPOMDP algorithm, ensuring its theoretical soundness. Moreover, the gradient estimates produced by GPOMDP are integrated into a conjugate-gradient optimization procedure that effectively searches for local maxima of the average reward function, facilitating improved policy performance in environments governed by POMDP dynamics. This work bridges the gap between practical algorithm design and theoretical guarantees within reinforcement learning under partial observability. <div>
arXiv:2512.02383v1 Announce Type: new 
Abstract: This paper discusses theoretical and experimental aspects of gradient-based approaches to the direct optimization of policy performance in controlled POMDPs. We introduce GPOMDP, a REINFORCE-like algorithm for estimating an approximation to the gradient of the average reward as a function of the parameters of a stochastic policy. The algorithm's chief advantages are that it requires only a single sample path of the underlying Markov chain, it uses only one free parameter $\beta\in [0,1)$, which has a natural interpretation in terms of bias-variance trade-off, and it requires no knowledge of the underlying state. We prove convergence of GPOMDP and show how the gradient estimates produced by GPOMDP can be used in a conjugate-gradient procedure to find local optima of the average reward.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Sensitive Q-Learning in Continuous Time with Application to Dynamic Portfolio Selection</title>
<link>https://arxiv.org/abs/2512.02386</link>
<guid>https://arxiv.org/abs/2512.02386</guid>
<content:encoded><![CDATA[
<div> Risk-sensitive reinforcement learning, continuous-time, stochastic differential equation, optimized certainty equivalent, q-learning<br /><br />Summary:<br /><br />This paper addresses the challenge of risk-sensitive reinforcement learning (RSRL) in continuous-time settings, where the environment dynamics are described by controllable stochastic differential equations (SDEs). The research focuses on optimizing policies according to a general nonlinear risk-sensitive criterion defined as a functional of cumulative rewards. A key theoretical result demonstrates that for risk functionals expressed as optimized certainty equivalents (OCEs), the optimal policy can be represented as a Markovian policy when applied to an augmented version of the original environment. Building on this theoretical insight, the authors introduce CT-RS-q, a novel risk-sensitive variant of q-learning designed for continuous-time problems. The algorithm leverages a new martingale characterization technique to handle the continuous-time and risk-sensitive nature of the task. To validate the proposed method, a simulation study is conducted involving a dynamic portfolio selection problem, showcasing the algorithm’s practical effectiveness. The results illustrate that CT-RS-q successfully learns risk-sensitive policies and can manage the trade-off between return and risk inherent in financial decision-making contexts. Overall, the paper contributes both theoretical foundations and an actionable algorithm for continuous-time risk-aware reinforcement learning applications. <div>
arXiv:2512.02386v1 Announce Type: new 
Abstract: This paper studies the problem of risk-sensitive reinforcement learning (RSRL) in continuous time, where the environment is characterized by a controllable stochastic differential equation (SDE) and the objective is a potentially nonlinear functional of cumulative rewards. We prove that when the functional is an optimized certainty equivalent (OCE), the optimal policy is Markovian with respect to an augmented environment. We also propose \textit{CT-RS-q}, a risk-sensitive q-learning algorithm based on a novel martingale characterization approach. Finally, we run a simulation study on a dynamic portfolio selection problem and illustrate the effectiveness of our algorithm.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESACT: An End-to-End Sparse Accelerator for Compute-Intensive Transformers via Local Similarity</title>
<link>https://arxiv.org/abs/2512.02403</link>
<guid>https://arxiv.org/abs/2512.02403</guid>
<content:encoded><![CDATA[
<div> Transformers, Sparsity, Local Similarity, SPLS, ESACT<br /><br />Summary:<br /><br />Transformers, despite their superior performance in many domains, suffer from high computational costs that limit their efficient deployment on hardware. Sparsity is a promising approach to reduce this overhead, yet existing accelerators mainly exploit intra-row sparsity in attention mechanisms, neglecting inter-row sparsity or relying on expensive global similarity computations which reduce acceleration benefits. Most current methods also limit sparsity application to only a few transformer components. This work analyzes attention distribution and computation flow, revealing that leveraging local similarity can enable efficient end-to-end sparse acceleration with minimal computational overhead. Based on this insight, the authors propose ESACT, an end-to-end sparse accelerator for compute-intensive transformers. ESACT utilizes the SplS (Sparsity Prediction with Local Similarity) mechanism that employs HLog quantization to accurately predict local attention sparsity before QK generation, enabling efficient sparsity across all transformer components. To facilitate hardware implementation, the design introduces three architectural innovations. Experimental evaluations on 26 benchmarks show that SPLS reduces total computation by 52.03% with less than 1% accuracy loss. ESACT achieves an energy efficiency of 3.29 TOPS/W end-to-end and improves attention-level energy efficiency by 2.95× and 2.26× compared with state-of-the-art attention accelerators SpAtten and Sanger, respectively. <div>
arXiv:2512.02403v1 Announce Type: new 
Abstract: Transformers, composed of QKV generation, attention computation, and FFNs,
  have become the dominant model across various domains due to their outstanding performance.
  However, their high computational cost hinders efficient hardware deployment.
  Sparsity offers a promising solution,
  yet most existing accelerators exploit only intra-row sparsity in attention,
  while few consider inter-row sparsity.
  Approaches leveraging inter-row sparsity often rely on costly global similarity estimation,
  which diminishes the acceleration benefits of sparsity,
  and typically apply sparsity to only one or two transformer components.
  Through careful analysis of the attention distribution and computation flow,
  we observe that local similarity allows end-to-end sparse acceleration with lower computational overhead.
  Motivated by this observation, we propose ESACT,
  an end-to-end sparse accelerator for compute-intensive Transformers.
  ESACT centers on the Sparsity Prediction with Local Similarity (SPLS) mechanism,
  which leverages HLog quantization to accurately predict local attention sparsity prior to QK generation,
  achieving efficient sparsity across all transformer components.
  To support efficient hardware realization, we introduce three architectural innovations.
  Experimental results on 26 benchmarks demonstrate that
  SPLS reduces total computation by 52.03% with less than 1% accuracy loss.
  ESACT achieves an end-to-end energy efficiency of 3.29 TOPS/W,
  and improves attention-level energy efficiency by 2.95x and 2.26x over
  SOTA attention accelerators SpAtten and Sanger, respectively.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Configuration of On-Street Parking Spaces using Multi Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02406</link>
<guid>https://arxiv.org/abs/2512.02406</guid>
<content:encoded><![CDATA[
<div> Keywords: on-street parking, traffic congestion, multi-agent reinforcement learning, dynamic configuration, spatio-temporal correlations  

<br /><br />Summary:  
With urban traffic congestion worsening due to increased travel demand, on-street parking further restricts road space and impedes traffic flow. This paper investigates minimizing the negative effects of on-street parking by dynamically configuring parking spaces using vehicle-to-infrastructure connectivity. The problem is formulated as an optimization task addressed through a data-driven approach. The proposed solution is a scalable two-layer multi-agent reinforcement learning (MARL) framework. At the lane level, agents decide optimal parking configurations using a novel Deep Q-learning model that incorporates long short-term memory (LSTM) networks and graph attention networks (GANs) to effectively capture spatio-temporal dependencies. At the higher block level, agents oversee lane agents’ actions to ensure adequate parking availability within blocks. The framework is evaluated through comprehensive experiments in the SUMO traffic simulator, using both synthetic scenarios and real-world data from Melbourne, Australia. Results demonstrate a substantial reduction in average vehicle travel time loss by up to 47%, while only minimally increasing walking distances to parking spaces. This approach shows promise in balancing efficient traffic flow with parking availability through adaptive, data-driven strategies leveraging advanced reinforcement learning techniques. <div>
arXiv:2512.02406v1 Announce Type: new 
Abstract: With increased travelling needs more than ever, traffic congestion has become a major concern in most urban areas. Allocating spaces for on-street parking, further hinders traffic flow, by limiting the effective road width available for driving. With the advancement of vehicle-to-infrastructure connectivity technologies, we explore how the impact of on-street parking on traffic congestion could be minimized, by dynamically configuring on-street parking spaces. Towards that end, we formulate dynamic on-street parking space configuration as an optimization problem, and we follow a data driven approach, considering the nature of our problem. Our proposed solution comprises a two-layer multi agent reinforcement learning based framework, which is inherently scalable to large road networks. The lane level agents are responsible for deciding the optimal parking space configuration for each lane, and we introduce a novel Deep Q-learning architecture which effectively utilizes long short term memory networks and graph attention networks to capture the spatio-temporal correlations evident in the given problem. The block level agents control the actions of the lane level agents and maintain a sufficient level of parking around the block. We conduct a set of comprehensive experiments using SUMO, on both synthetic data as well as real-world data from the city of Melbourne. Our experiments show that the proposed framework could reduce the average travel time loss of vehicles significantly, reaching upto 47%, with a negligible increase in the walking distance for parking.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data Curation Through the Lens of Spectral Dynamics: Static Limits, Dynamic Acceleration, and Practical Oracles</title>
<link>https://arxiv.org/abs/2512.02409</link>
<guid>https://arxiv.org/abs/2512.02409</guid>
<content:encoded><![CDATA[
<div> data pruning, synthetic data, neural scaling, spectral tail exponent, data curation<br /><br />Summary:<br /><br />This paper investigates the effects of various data-centric strategies utilized in training large-scale neural models, including data pruning, synthetic data generation, distillation, RLHF, and difficulty-based sampling. It highlights that while some methods reliably improve efficiency and performance, synthetic data often increases dataset size without enhancing model capabilities. The authors formalize data curation as reweighting the sampling distribution and analyze its impact through the eigenstructure of the data-induced operator. Their first main result demonstrates that static pruning corresponds to a bounded operator, which cannot alter the spectral tail exponent, thereby limiting improvements to finite regions and not affecting asymptotic neural scaling. The second key result concerns time-dependent data curation, where an idealized oracle that dynamically tracks and re-normalizes spectral residuals in the data could provably accelerate learning by modifying the tail behavior of the spectrum. However, practical implementations can only approximate this ideal process. Overall, the work provides a theoretical framework linking data manipulation methods to their influence on neural scaling laws and learning dynamics, explaining why some data curation strategies yield limited gains while suggesting paths toward more effective adaptive sampling techniques. <div>
arXiv:2512.02409v1 Announce Type: new 
Abstract: Large-scale neural models are increasingly trained with data pruning, synthetic data generation, cross-model distillation, reinforcement learning from human feedback (RLHF), and difficulty-based sampling. While several of these data-centric strategies reliably improve training efficiency and downstream performance, others fail to provide meaningful gains -- most notably self-generated synthetic data, which often increases dataset volume without enhancing model capability.
  We formalize data curation as reweighting the sampling distribution and map its effect onto the eigenstructure of the data-induced operator. Our first main result shows that \textbf{static pruning induces a bounded operator and therefore cannot change the spectral tail exponent}; it provides at most finite-region improvements and cannot alter asymptotic neural scaling. Our second result analyzes \textbf{time-dependent data curation}, showing that an ideal oracle capable of tracking spectral residuals and continuously re-normalizing the tail can provably accelerate learning -- although practical systems can only approximate this behavior.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Domain Offline Policy Adaptation with Dynamics- and Value-Aligned Data Filtering</title>
<link>https://arxiv.org/abs/2512.02435</link>
<guid>https://arxiv.org/abs/2512.02435</guid>
<content:encoded><![CDATA[
<div> Keywords: Cross-Domain Reinforcement Learning, Offline RL, Dynamics Alignment, Value Alignment, Data Filtering  

<br /><br />Summary:  
This paper addresses the challenge of Cross-Domain Offline Reinforcement Learning, where an agent trained with limited target domain data and abundant source domain data must overcome dynamics discrepancies between domains. The authors identify that previous methods focus exclusively on dynamics alignment—selecting samples that match the target domain’s environment dynamics—but neglect value alignment, which involves choosing source samples of high-value relevance to the target tasks. They demonstrate through theoretical analysis that both dynamics and value alignment are critical for narrowing the sub-optimality gap when transferring policies. Motivated by this insight, the paper introduces DVDF (Dynamics- and Value-aligned Data Filtering), a method that filters and selects source domain data exhibiting both alignments to improve policy learning. The authors construct various dynamics shift scenarios, such as changes in kinematics and morphology, to rigorously evaluate their approach. Empirical results across multiple benchmark tasks and datasets show that DVDF consistently outperforms existing baselines, particularly in low-data regimes where only 5,000 target domain transitions are available. Overall, DVDF demonstrates superior generalization and robustness in cross-domain offline RL by jointly addressing both key aspects of alignment, paving the way for more effective knowledge transfer in reinforcement learning. <div>
arXiv:2512.02435v1 Announce Type: new 
Abstract: Cross-Domain Offline Reinforcement Learning aims to train an agent deployed in the target environment, leveraging both a limited target domain dataset and a source domain dataset with (possibly) sufficient data coverage. Due to the underlying dynamics misalignment between the source and target domain, simply merging the data from two datasets may incur inferior performance. Recent advances address this issue by selectively sharing source domain samples that exhibit dynamics alignment with the target domain. However, these approaches focus solely on dynamics alignment and overlook \textit{value alignment}, i.e., selecting high-quality, high-value samples from the source domain. In this paper, we first demonstrate that both dynamics alignment and value alignment are essential for policy learning, by examining the limitations of the current theoretical framework for cross-domain RL and establishing a concrete sub-optimality gap of a policy trained on the source domain and evaluated on the target domain. Motivated by the theoretical insights, we propose to selectively share those source domain samples with both high dynamics and value alignment and present our \textbf{\underline{D}}ynamics- and \textbf{\underline{V}}alue-aligned \textbf{\underline{D}}ata \textbf{\underline{F}}iltering (DVDF) method. We design a range of dynamics shift settings, including kinematic and morphology shifts, and evaluate DVDF on various tasks and datasets, as well as in challenging extremely low-data settings where the target domain dataset contains only 5,000 transitions. Extensive experiments demonstrate that DVDF consistently outperforms prior strong baselines and delivers exceptional performance across multiple tasks and datasets.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Refusals Fail: Unstable Safety Mechanisms in Long-Context LLM Agents</title>
<link>https://arxiv.org/abs/2512.02445</link>
<guid>https://arxiv.org/abs/2512.02445</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, long context window, agentic setup, task performance, safety issues<br /><br />Summary:<br /><br />This article investigates the behavior of large language model (LLM) agents when operating over very long context windows, particularly focusing on agentic setups which combine tool usage and extended context. Unlike prior work that mainly evaluated LLMs on long-context prompts, this study highlights how the length, type, and placement of context can cause inconsistent and unexpected changes in both task performance and safety-related refusals to carry out harmful requests. The authors observe that models with context windows ranging from 1 million to 2 million tokens already show significant performance degradation at around 100,000 tokens, with task accuracy dropping by more than 50% in both benign and harmful scenarios. Furthermore, refusal rates behave unpredictably depending on the model: for instance, GPT-4.1-nano’s refusal rate increased from roughly 5% to 40% at 200,000 tokens, whereas Grok 4 Fast’s refusal rate decreased sharply from 80% to 10%. These findings reveal critical safety concerns when LLM agents are employed in long-horizon, multi-step tasks with extensive context, suggesting that current safety evaluation metrics and paradigms may be inadequate. The work calls for rethinking how LLM agent capabilities and safety are assessed in long-context and agentic environments. <div>
arXiv:2512.02445v1 Announce Type: new 
Abstract: Solving complex or long-horizon problems often requires large language models (LLMs) to use external tools and operate over a significantly longer context window. New LLMs enable longer context windows and support tool calling capabilities. Prior works have focused mainly on evaluation of LLMs on long-context prompts, leaving agentic setup relatively unexplored, both from capability and safety perspectives. Our work addresses this gap. We find that LLM agents could be sensitive to length, type, and placement of the context, exhibiting unexpected and inconsistent shifts in task performance and in refusals to execute harmful requests. Models with 1M-2M token context windows show severe degradation already at 100K tokens, with performance drops exceeding 50\% for both benign and harmful tasks. Refusal rates shift unpredictably: GPT-4.1-nano increases from $\sim$5\% to $\sim$40\% while Grok 4 Fast decreases from $\sim$80\% to $\sim$10\% at 200K tokens. Our work shows potential safety issues with agents operating on longer context and opens additional questions on the current metrics and paradigm for evaluating LLM agent safety on long multi-step tasks. In particular, our results on LLM agents reveal a notable divergence in both capability and safety performance compared to prior evaluations of LLMs on similar criteria.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabGRU: An Enhanced Design for Urban Rainfall Intensity Estimation Using Commercial Microwave Links</title>
<link>https://arxiv.org/abs/2512.02465</link>
<guid>https://arxiv.org/abs/2512.02465</guid>
<content:encoded><![CDATA[
<div> Keywords: Urban rainfall monitoring, Commercial Microwave Links, TabGRU, Transformer, Bidirectional GRU  

<br /><br />Summary:  
The paper addresses the challenge of high-resolution urban rainfall monitoring, essential for resilient smart cities amid increasing urbanization and extreme weather events. It introduces a novel hybrid deep learning model called TabGRU, combining the Transformer architecture with a Bidirectional Gated Recurrent Unit (BiGRU) to capture both long-term dependencies and local sequential features in Commercial Microwave Link (CML) data. The model incorporates learnable positional embeddings and an attention pooling mechanism to enhance dynamic feature extraction and generalization. Validation was conducted on a public benchmark dataset from Gothenburg, Sweden, comprising data from 12 sub-links and two rain gauges over a test period involving around 10 rainfall events. The TabGRU model outperformed traditional physics-based models and other deep learning baselines, achieving high coefficients of determination (R² of 0.91 at Torp and 0.96 at Barl). Notably, TabGRU effectively mitigated the overestimation issues seen in physics-based models during peak rainfall. The results demonstrate that the proposed TabGRU can robustly and accurately monitor urban rainfall using CML data, overcoming limitations of existing methods under tested conditions. <div>
arXiv:2512.02465v1 Announce Type: new 
Abstract: In the face of accelerating global urbanization and the increasing frequency of extreme weather events, highresolution urban rainfall monitoring is crucial for building resilient smart cities. Commercial Microwave Links (CMLs) are an emerging data source with great potential for this task.While traditional rainfall retrieval from CMLs relies on physicsbased models, these often struggle with real-world complexities like signal noise and nonlinear attenuation. To address these limitations, this paper proposes a novel hybrid deep learning architecture based on the Transformer and a Bidirectional Gated Recurrent Unit (BiGRU), which we name TabGRU. This design synergistically captures both long-term dependencies and local sequential features in the CML signal data. The model is further enhanced by a learnable positional embedding and an attention pooling mechanism to improve its dynamic feature extraction and generalization capabilities. The model was validated on a public benchmark dataset from Gothenburg, Sweden (June-September 2015). The evaluation used 12 sub-links from two rain gauges (Torp and Barl) over a test period (August 22-31) covering approximately 10 distinct rainfall events. The proposed TabGRU model demonstrated consistent advantages, outperforming deep learning baselines and achieving high coefficients of determination (R2) at both the Torp site (0.91) and the Barl site (0.96). Furthermore, compared to the physics-based approach, TabGRU maintained higher accuracy and was particularly effective in mitigating the significant overestimation problem observed in the PL model during peak rainfall events. This evaluation confirms that the TabGRU model can effectively overcome the limitations of traditional methods, providing a robust and accurate solution for CML-based urban rainfall monitoring under the tested conditions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dual-Robust Cross-Domain Offline Reinforcement Learning Against Dynamics Shifts</title>
<link>https://arxiv.org/abs/2512.02486</link>
<guid>https://arxiv.org/abs/2512.02486</guid>
<content:encoded><![CDATA[
<div> Keywords: offline reinforcement learning, cross-domain, dynamics shifts, robustness, Bellman operator<br /><br />Summary:<br />1. The paper addresses the challenge of limited data coverage in single-domain offline reinforcement learning (RL) by leveraging cross-domain offline RL, which uses additional data from domains with shifted dynamics. <br />2. Current approaches mainly focus on robustness during training against dynamics shifts but overlook robustness at test time when the policy encounters dynamics perturbations in real-world deployments. <br />3. Empirical analysis reveals that policies trained via cross-domain offline RL can be fragile during evaluation, especially when target domain data is scarce. <br />4. To improve both train-time and test-time robustness, the authors introduce the Robust Cross-domain Bellman (RCB) operator, which remains conservative regarding out-of-distribution dynamics transitions and enhances resilience to dynamics perturbations. <br />5. To mitigate potential value estimation errors introduced by RCB, the framework incorporates a dynamic value penalty and the Huber loss, culminating in the Dual-ROBust Cross-domain Offline RL (DROCO) algorithm. <br />6. Extensive experiments across diverse dynamics shift scenarios demonstrate that DROCO outperforms strong baselines and significantly boosts robustness against dynamics perturbations. <div>
arXiv:2512.02486v1 Announce Type: new 
Abstract: Single-domain offline reinforcement learning (RL) often suffers from limited data coverage, while cross-domain offline RL handles this issue by leveraging additional data from other domains with dynamics shifts. However, existing studies primarily focus on train-time robustness (handling dynamics shifts from training data), neglecting the test-time robustness against dynamics perturbations when deployed in practical scenarios. In this paper, we investigate dual (both train-time and test-time) robustness against dynamics shifts in cross-domain offline RL. We first empirically show that the policy trained with cross-domain offline RL exhibits fragility under dynamics perturbations during evaluation, particularly when target domain data is limited. To address this, we introduce a novel robust cross-domain Bellman (RCB) operator, which enhances test-time robustness against dynamics perturbations while staying conservative to the out-of-distribution dynamics transitions, thus guaranteeing the train-time robustness. To further counteract potential value overestimation or underestimation caused by the RCB operator, we introduce two techniques, the dynamic value penalty and the Huber loss, into our framework, resulting in the practical \textbf{D}ual-\textbf{RO}bust \textbf{C}ross-domain \textbf{O}ffline RL (DROCO) algorithm. Extensive empirical results across various dynamics shift scenarios show that DROCO outperforms strong baselines and exhibits enhanced robustness to dynamics perturbations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid(Penalized Regression and MLP) Models for Outcome Prediction in HDLSS Health Data</title>
<link>https://arxiv.org/abs/2512.02489</link>
<guid>https://arxiv.org/abs/2512.02489</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, NHANES, diabetes prediction, XGBoost, multilayer perceptron<br /><br />Summary:<br /><br />This paper applies established machine learning methods to the NHANES health survey data for predicting diabetes status. It evaluates baseline models including logistic regression, random forest, and XGBoost to establish benchmark performance. A novel hybrid model is proposed that combines an XGBoost-based feature encoder with a lightweight multilayer perceptron (MLP) classifier as the prediction head. Experimental results on the processed NHANES subset indicate that the hybrid approach outperforms the baseline models, achieving improved area under the curve (AUC) and balanced accuracy metrics. The study emphasizes model performance improvements that can be achieved by leveraging feature encoding with powerful gradient boosting techniques alongside neural network classifiers. To facilitate reproducibility and further research, the author releases all associated code and scripts necessary to replicate the experiments and results. This work highlights the potential utility of combining gradient boosting and neural networks for health survey data analysis and diabetes risk prediction. <div>
arXiv:2512.02489v1 Announce Type: new 
Abstract: I present an application of established machine learning techniques to NHANES health survey data for predicting diabetes status. I compare baseline models (logistic regression, random forest, XGBoost) with a hybrid approach that uses an XGBoost feature encoder and a lightweight multilayer perceptron (MLP) head. Experiments show the hybrid model attains improved AUC and balanced accuracy compared to baselines on the processed NHANES subset. I release code and reproducible scripts to encourage replication.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Fully First-Order Layer for Differentiable Optimization</title>
<link>https://arxiv.org/abs/2512.02494</link>
<guid>https://arxiv.org/abs/2512.02494</guid>
<content:encoded><![CDATA[
<div> Keywords: differentiable optimization, bilevel optimization, first-order gradient, active-set Lagrangian, hypergradient oracle  

<br /><br />Summary:  
1. Differentiable optimization layers are crucial for learning systems that solve embedded optimization problems, but gradient computation using implicit differentiation is costly due to the involvement of Hessian matrices.  
2. This paper proposes a novel algorithm that eliminates the need for Hessian computations by using only first-order information, thereby reducing computational and memory overhead.  
3. The key innovation is reformulating differentiable optimization as a bilevel optimization problem and employing recent advances in bilevel optimization methods.  
4. The authors introduce an active-set Lagrangian hypergradient oracle which approximates hypergradients in finite time without Hessian evaluations, providing non-asymptotic guarantees on accuracy.  
5. Their method achieves an overall complexity of approximately \(\tilde{O}(\delta^{-1}\epsilon^{-3})\) for constrained bilevel optimization, matching the best-known rates for non-smooth, non-convex optimization problems.  
6. To facilitate adoption, an open-source Python library implementing the proposed algorithm is released, easily adaptable from current solvers, available at https://github.com/guaguakai/FFOLayer. <div>
arXiv:2512.02494v1 Announce Type: new 
Abstract: Differentiable optimization layers enable learning systems to make decisions by solving embedded optimization problems. However, computing gradients via implicit differentiation requires solving a linear system with Hessian terms, which is both compute- and memory-intensive. To address this challenge, we propose a novel algorithm that computes the gradient using only first-order information. The key insight is to rewrite the differentiable optimization as a bilevel optimization problem and leverage recent advances in bilevel methods. Specifically, we introduce an active-set Lagrangian hypergradient oracle that avoids Hessian evaluations and provides finite-time, non-asymptotic approximation guarantees. We show that an approximate hypergradient can be computed using only first-order information in $\tilde{\oo}(1)$ time, leading to an overall complexity of $\tilde{\oo}(\delta^{-1}\epsilon^{-3})$ for constrained bilevel optimization, which matches the best known rate for non-smooth non-convex optimization. Furthermore, we release an open-source Python library that can be easily adapted from existing solvers. Our code is available here: https://github.com/guaguakai/FFOLayer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Water Quality Estimation Through Machine Learning Multivariate Analysis</title>
<link>https://arxiv.org/abs/2512.02508</link>
<guid>https://arxiv.org/abs/2512.02508</guid>
<content:encoded><![CDATA[
<div> Keywords: Water Quality, UV-Vis Spectroscopy, Machine Learning, SHAP, Agrifood Sector<br /><br />Summary:<br /><br />1. This study addresses the critical role of water quality in the agrifood sector, where water is utilized for fertigation, animal husbandry, and processing.  
2. With the increasing digitalization of agriculture, there is a growing need for automatic and efficient methods for assessing water quality to ensure safety and regulatory compliance.  
3. The research integrates Ultraviolet-Visible (UV-Vis) spectroscopy with machine learning techniques to detect and evaluate key water quality parameters rapidly and accurately.  
4. Emphasis is placed on the interpretability of machine learning models by applying SHapley Additive exPlanations (SHAP), which reveal how absorbance at specific wavelengths influences prediction outcomes.  
5. The demonstrated approach offers a promising solution for fast, reliable, and interpretable water quality assessment, supporting better decision-making in the agrifood sector and contributing to enhanced water safety management. <div>
arXiv:2512.02508v1 Announce Type: new 
Abstract: The quality of water is key for the quality of agrifood sector. Water is used in agriculture for fertigation, for animal husbandry, and in the agrifood processing industry. In the context of the progressive digitalization of this sector, the automatic assessment of the quality of water is thus becoming an important asset. In this work, we present the integration of Ultraviolet-Visible (UV-Vis) spectroscopy with Machine Learning in the context of water quality assessment aiming at ensuring water safety and the compliance of water regulation. Furthermore, we emphasize the importance of model interpretability by employing SHapley Additive exPlanations (SHAP) to understand the contribution of absorbance at different wavelengths to the predictions. Our approach demonstrates the potential for rapid, accurate, and interpretable assessment of key water quality parameters.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Fairness Aware Multi Task Federated Learning for VR Network</title>
<link>https://arxiv.org/abs/2512.02513</link>
<guid>https://arxiv.org/abs/2512.02513</guid>
<content:encoded><![CDATA[
<div> Keywords: wireless VR, federated learning, caching strategy, decentralized learning, quality of experience<br /><br />Summary:<br /><br />This paper addresses the challenge of delivering high-quality, real-time wireless VR video by proposing a decentralized multi-task fair federated learning (DMTFL) approach tailored for caching users’ fields of view (FOV) at base stations (BSs). Traditional federated learning methods tend to bias toward certain users and fail to adapt to the statistical heterogeneity among different users and BSs because they rely on a single global model. In contrast, the proposed DMTFL personalizes caching models for each BS to optimize content delivery while ensuring fairness and efficiency. The models learned at each BS are optimized to perform well under any target distribution, which is theoretically supported by guarantees derived from Rademacher complexity and a probably approximately correct (PAC) bound on the loss function. The caching and prefetching strategies tailored for each BS help reduce latency and improve the user quality of experience by anticipating VR users’ FOV preferences. Simulations based on realistic VR head-tracking datasets demonstrate the superiority of the DMTFL algorithm over baseline approaches, showing improved fairness and performance in wireless VR content delivery. This work paves the way for scalable, personalized, and efficient wireless VR streaming systems. <div>
arXiv:2512.02513v1 Announce Type: new 
Abstract: Wireless connectivity promises to unshackle virtual reality (VR) experiences, allowing users to engage from anywhere, anytime. However, delivering seamless, high-quality, real-time VR video wirelessly is challenging due to the stringent quality of experience requirements, low latency constraints, and limited VR device capabilities. This paper addresses these challenges by introducing a novel decentralized multi task fair federated learning (DMTFL) based caching that caches and prefetches each VR user's field of view (FOV) at base stations (BSs) based on the caching strategies tailored to each BS. In federated learning (FL) in its naive form, often biases toward certain users, and a single global model fails to capture the statistical heterogeneity across users and BSs. In contrast, the proposed DMTFL algorithm personalizes content delivery by learning individual caching models at each BS. These models are further optimized to perform well under any target distribution, while providing theoretical guarantees via Rademacher complexity and a probably approximately correct (PAC) bound on the loss. Using a realistic VR head-tracking dataset, our simulations demonstrate the superiority of our proposed DMTFL algorithm compared to baseline algorithms.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>In-Context Distillation with Self-Consistency Cascades: A Simple, Training-Free Way to Reduce LLM Agent Costs</title>
<link>https://arxiv.org/abs/2512.02543</link>
<guid>https://arxiv.org/abs/2512.02543</guid>
<content:encoded><![CDATA[
<div> Keywords: in-context distillation, LLM agents, cost reduction, self-consistency cascades, multi-step reasoning<br /><br />Summary:<br /><br />This paper addresses the high inference costs associated with deploying large language model (LLM) agents at scale, which hinder rapid prototyping and experimentation. The authors propose a novel method called in-context distillation that adapts knowledge distillation to an in-context learning framework by retrieving relevant teacher demonstrations dynamically during each agent step and providing them as examples to a lower-cost student model. This enables the student to imitate the teacher’s behavior on-the-fly without expensive fine-tuning or manual prompt engineering. They further integrate this method with self-consistency cascades, an approach that adaptively determines when to trust the student model’s outputs, balancing cost-efficiency and accuracy. Experimental results on the ALFWorld benchmark, which involves multi-step embodied reasoning, demonstrate that the approach achieves teacher-level accuracy at 2.5× lower inference cost, reducing per-episode cost from \$0.059 to \$0.024 and yielding significant cumulative savings at deployment scale. On the AppWorld benchmark, focused on multi-step API workflows, their method shifts the Pareto frontier by cutting costs in half without sacrificing performance. Overall, this approach materially lowers operational costs and preserves rapid experimentation capabilities, making advanced LLM agent systems more economically viable for wider adoption. <div>
arXiv:2512.02543v1 Announce Type: new 
Abstract: The world currently has an abundance of ideas for how to use new LLM agents, and developers seek to rapidly prototype and test new agentic designs. However, executing agents at scale using high-capacity LLMs incurs high inference costs. We propose a simple method for reducing LLM agent inference costs without incurring the development friction costs associated with LLM fine-tuning (long training cycles, optimization hyperparameter tweaking loops) or manual prompt engineering (laborious trial and error). Most importantly, we introduce $\textit{in-context distillation}$, which adapts the idea of knowledge distillation (training a low cost-student model to mimic a high-cost teacher) to an in-context learning setting. Our approach retrieves relevant teacher demonstrations at each agent step and provides them to the student as in-context examples, enabling the student to imitate teacher behavior on-the-fly. We combine in-context distillation with the established idea of $\textit{self-consistency cascades}$ to know when the trust the student. This adaptive strategy realizes the cost benefits of model specialization while preserving the productivity of working with frozen models. On the multi-step embodied reasoning benchmark ALFWorld, our method matches teacher-level accuracy at $\textbf{2.5$\times$ lower cost}$, reducing per-episode costs from \$0.059 to \$0.024. The upfront demonstration cost amortizes after just 843 episodes, yielding cumulative savings exceeding \$34,900 at deployment scale (1M episodes). On AppWorld, a complex agent benchmark requiring multi-step API workflows, we shift the Pareto frontier by achieving a $\textbf{2$\times$ cost reduction}$ at iso-accuracy. By reducing operational costs while maintaining rapid experimentation cycles with frozen models, our approach makes advanced agentic systems economically viable for a broader range of applications.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tensor Network Based Feature Learning Model</title>
<link>https://arxiv.org/abs/2512.02547</link>
<guid>https://arxiv.org/abs/2512.02547</guid>
<content:encoded><![CDATA[
<div> Keywords: tensor-product features, Canonical Polyadic Decomposition, feature learning, Alternating Least Squares, kernel approximation  

<br /><br />Summary:  
This paper addresses the challenge of efficiently training kernel-based models on large-scale datasets, focusing on tensor-product structured polynomial and Fourier features that typically suffer from the curse of dimensionality. Previous work resolved this issue by reparameterizing model parameters using tensor networks. However, optimizing feature hyperparameters remained reliant on costly standard cross-validation methods. The authors propose the Feature Learning (FL) model, which represents tensor-product features through a learnable Canonical Polyadic Decomposition (CPD), enabling joint optimization of feature hyperparameters and model parameters. Leveraging the CPD structure allows the use of an Alternating Least Squares (ALS) algorithm for efficient training. Experiments on real-world datasets of varying dimensionality and size demonstrate that the FL model consistently trains 3 to 5 times faster compared to traditional approaches while achieving comparable prediction accuracy. The FL model thus offers a scalable and practical solution for hyperparameter optimization in kernel-based learning with tensor-product features. <div>
arXiv:2512.02547v1 Announce Type: new 
Abstract: Many approximations were suggested to circumvent the cubic complexity of kernel-based algorithms, allowing their application to large-scale datasets. One strategy is to consider the primal formulation of the learning problem by mapping the data to a higher-dimensional space using tensor-product structured polynomial and Fourier features. The curse of dimensionality due to these tensor-product features was effectively solved by a tensor network reparameterization of the model parameters. However, another important aspect of model training - identifying optimal feature hyperparameters - has not been addressed and is typically handled using the standard cross-validation approach. In this paper, we introduce the Feature Learning (FL) model, which addresses this issue by representing tensor-product features as a learnable Canonical Polyadic Decomposition (CPD). By leveraging this CPD structure, we efficiently learn the hyperparameters associated with different features alongside the model parameters using an Alternating Least Squares (ALS) optimization method. We prove the effectiveness of the FL model through experiments on real data of various dimensionality and scale. The results show that the FL model can be consistently trained 3-5 times faster than and have the prediction quality on par with a standard cross-validated model.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CUDA-L2: Surpassing cuBLAS Performance for Matrix Multiplication through Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02551</link>
<guid>https://arxiv.org/abs/2512.02551</guid>
<content:encoded><![CDATA[
<div> Keywords: CUDA-L2, large language models, reinforcement learning, HGEMM optimization, CUDA kernels<br /><br />Summary:<br /><br />1. The paper introduces CUDA-L2, a novel system that leverages large language models (LLMs) combined with reinforcement learning (RL) to optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels automatically.<br /><br />2. CUDA-L2 uses CUDA kernel execution speed as the RL reward to explore and optimize across 1,000 different kernel configurations, enabling it to systematically improve performance beyond existing solutions.<br /><br />3. The system outperforms several leading matrix multiplication baselines, including torch.matmul, Nvidia's closed-source cuBLAS, and cuBLASLt libraries, achieving significant speedups.<br /><br />4. In an offline mode, where kernels execute in rapid succession, CUDA-L2 delivers an average performance boost of +22.0% over torch.matmul, +19.2% over cuBLAS (optimal layouts), +16.8% over cuBLASLt-heuristic, and +11.4% over cuBLASLt-AutoTuning.<br /><br />5. Under server mode conditions simulating real-time inference with random kernel execution intervals, these improvements are even more pronounced, rising to +28.7%, +26.0%, +22.4%, and +15.9% respectively.<br /><br />6. The results demonstrate that highly-optimized, performance-critical CUDA kernels such as HGEMM can benefit substantially from automated, LLM-guided RL approaches that can efficiently explore vast configuration spaces beyond human capability.<br /><br />7. The project and source code are publicly available at github.com/deepreinforce-ai/CUDA-L2. <div>
arXiv:2512.02551v1 Announce Type: new 
Abstract: In this paper, we propose CUDA-L2, a system that combines large language models (LLMs) and reinforcement learning (RL) to automatically optimize Half-precision General Matrix Multiply (HGEMM) CUDA kernels. Using CUDA execution speed as the RL reward, CUDA-L2 automatically optimizes HGEMM kernels across 1,000 configurations. CUDA-L2 systematically outperforms major matmul baselines to date, from the widely-used {\it torch.matmul} to state-of-the-art Nvidia's closed-source libraries, i.e., {\it cuBLAS}, {\it cuBLASLt}. In offline mode, where kernels are executed consecutively without time intervals, CUDA-L2 yields +22.0\% over {\it torch.matmul} on average; +19.2\% over {\it cuBLAS} using the optimal layout configuration (normal-normal NN and transposed-normal TN); +16.8\% over {\it cuBLASLt-heuristic}, which queries {\it cuBLASLt} library and selects the algorithm based on the heuristic's suggestion; and +11.4\% over the most competitive {\it cuBLASLt-AutoTuning} model, which selects the fastest algorithm from up to 100 candidates from {\it cuBLASLt}'s suggestions. In server mode, where kernels are executed at random intervals simulating real-time inference, the speedups further increase to +28.7\%, +26.0\%, +22.4\%, and +15.9\% for {\it torch.matmul}, {\it cuBLAS}, {\it cuBLASLt-heuristic}, and {\it cuBLASLt-AutoTuning} respectively. CUDA-L2 shows that even the most performance-critical, heavily-optimized kernels like HGEMM can be improved through LLM-guided RL automation by systematically exploring configuration spaces at scales impractical for humans. Project and code can be found at github.com/deepreinforce-ai/CUDA-L2
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GoRL: An Algorithm-Agnostic Framework for Online Reinforcement Learning with Generative Policies</title>
<link>https://arxiv.org/abs/2512.02581</link>
<guid>https://arxiv.org/abs/2512.02581</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, multimodal policies, generative models, latent policy, continuous control<br /><br />Summary:<br /><br />The paper addresses a core challenge in reinforcement learning (RL) where policies easy to optimize often lack the expressiveness needed for complex control tasks due to their unimodal nature, exemplified by Gaussian policies. It introduces GoRL (Generative Online Reinforcement Learning), a novel framework that decouples the optimization process from action generation by using a tractable latent policy combined with a conditional generative decoder. This design allows stable optimization of the latent policy on one timescale while the decoder, which synthesizes actions, evolves separately to enhance policy expressiveness without requiring tractable action likelihoods. The two-timescale update mechanism stabilizes training, preventing instability caused by propagating noisy gradients through deep generative chains, a challenge in prior generative policy approaches. Empirical evaluations on various continuous-control benchmarks demonstrate that GoRL consistently outperforms standard Gaussian policies and recent generative policy baselines. Notably, on the HopperStand task, GoRL achieves a normalized return above 870, exceeding the strongest baseline by more than a factor of three. Overall, the work establishes that separating optimization from generation offers a practical and effective strategy to develop RL policies that are both stable to train and capable of modeling complex, multimodal action distributions. <div>
arXiv:2512.02581v1 Announce Type: new 
Abstract: Reinforcement learning (RL) faces a persistent tension: policies that are stable to optimize are often too simple to represent the multimodal action distributions needed for complex control. Gaussian policies provide tractable likelihoods and smooth gradients, but their unimodal form limits expressiveness. Conversely, generative policies based on diffusion or flow matching can model rich multimodal behaviors; however, in online RL, they are frequently unstable due to intractable likelihoods and noisy gradients propagating through deep sampling chains. We address this tension with a key structural principle: decoupling optimization from generation. Building on this insight, we introduce GoRL (Generative Online Reinforcement Learning), a framework that optimizes a tractable latent policy while utilizing a conditional generative decoder to synthesize actions. A two-timescale update schedule enables the latent policy to learn stably while the decoder steadily increases expressiveness, without requiring tractable action likelihoods. Across a range of continuous-control tasks, GoRL consistently outperforms both Gaussian policies and recent generative-policy baselines. Notably, on the HopperStand task, it reaches a normalized return above 870, more than 3 times that of the strongest baseline. These results demonstrate that separating optimization from generation provides a practical path to policies that are both stable and highly expressive.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Modeling and Inverse Identification of Interfacial Heat Conduction in Finite Layer and Semi-Infinite Substrate Systems via a Physics-Guided Neural Framework</title>
<link>https://arxiv.org/abs/2512.02618</link>
<guid>https://arxiv.org/abs/2512.02618</guid>
<content:encoded><![CDATA[
<div> Heat transfer, semiconductor devices, physics-informed neural networks, Transformer architecture, interface thermal modeling<br /><br />Summary:<br /><br />This paper addresses the challenge of modeling heat transfer in semiconductor devices where a finite chip layer interfaces with a semi-infinite substrate having significantly different thermophysical properties, resulting in steep temperature gradients at the interface. Traditional numerical methods require fine discretization to capture these gradients, which is computationally intensive. Similarly, conventional physics-informed neural networks (PINNs) struggle with unstable convergence and physical inconsistencies near the interface. To overcome these issues, the authors propose HeatTransFormer, a novel physics-guided Transformer architecture tailored for interface-dominated diffusion problems. The model incorporates physically informed spatiotemporal sampling, a Laplace-based activation function mimicking analytical diffusion solutions, and a mask-free attention mechanism that enables bidirectional spatiotemporal coupling. These innovations allow HeatTransFormer to accurately resolve steep interfacial gradients, maintain physical consistency, and achieve stable convergence where PINNs often fail. The framework is validated on configurations involving a finite chip layer coupled with a semi-infinite substrate, successfully predicting coherent temperature fields across the interface. Furthermore, when combined with a physics-constrained inverse modeling approach, HeatTransFormer can simultaneously identify three unknown thermal properties using only external measurements. Overall, this work demonstrates that physics-guided Transformer models offer a unified and effective solution for both forward simulation and inverse parameter identification in complex thermal systems dominated by interfacial phenomena. <div>
arXiv:2512.02618v1 Announce Type: new 
Abstract: Heat transfer in semiconductor devices is dominated by chip and substrate assemblies, where heat generated within a finite chip layer dissipates into a semi-infinite substrate with much higher thermophysical properties. This mismatch produces steep interfacial temperature gradients, making the transient thermal response highly sensitive to the interface. Conventional numerical solvers require excessive discretization to resolve these dynamics, while physics-informed neural networks (PINNs) often exhibit unstable convergence and loss of physical consistency near the material interface. To address these challenges, we introduce HeatTransFormer, a physics-guided Transformer architecture for interface-dominated diffusion problems. The framework integrates physically informed spatiotemporal sampling, a Laplace-based activation emulating analytical diffusion solutions, and a mask-free attention mechanism supporting bidirectional spatiotemporal coupling. These components enable the model to resolve steep gradients, maintain physical consistency, and remain stable where PINNs typically fail. HeatTransFormer produces coherent temperature fields across the interface when applied to a finite layer and semi-infinite substrate configuration. Coupled with a physics-constrained inverse strategy, it further enables reliable identification of three unknown thermal properties simultaneously using only external measurements. Overall, this work demonstrates that physics-guided Transformer architectures provide a unified framework for forward and inverse modeling in interface-dominated thermal systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adapting Tensor Kernel Machines to Enable Efficient Transfer Learning for Seizure Detection</title>
<link>https://arxiv.org/abs/2512.02626</link>
<guid>https://arxiv.org/abs/2512.02626</guid>
<content:encoded><![CDATA[
<div> transfer learning, tensor kernel machine, adaptive SVM, seizure detection, EEG<br /><br />Summary:<br /><br />1. The paper proposes an efficient transfer learning method leveraging a tensor kernel machine to improve target task performance by learning from a related source task. <br />2. Inspired by adaptive SVM, the method transfers knowledge via regularization while utilizing low-rank tensor networks to form a compact non-linear model operating in the primal domain.<br />3. This approach enables efficient model adaptation without increasing the number of model parameters.<br />4. The adaptive tensor kernel machine (Adapt-TKM) is applied to seizure detection using behind-the-ear EEG data, personalizing patient-independent models with limited patient-specific information.<br />5. Experiments show that the patient-adapted Adapt-TKM outperforms both patient-independent and fully patient-specific models, while using roughly 100 times fewer parameters compared to adaptive SVM, resulting in faster inference.<br />6. The method's parameter and computational efficiency makes it especially suitable for resource-constrained wearable devices, facilitating real-time seizure detection and personalized healthcare applications. <div>
arXiv:2512.02626v1 Announce Type: new 
Abstract: Transfer learning aims to optimize performance in a target task by learning from a related source problem. In this work, we propose an efficient transfer learning method using a tensor kernel machine. Our method takes inspiration from the adaptive SVM and hence transfers 'knowledge' from the source to the 'adapted' model via regularization. The main advantage of using tensor kernel machines is that they leverage low-rank tensor networks to learn a compact non-linear model in the primal domain. This allows for a more efficient adaptation without adding more parameters to the model. To demonstrate the effectiveness of our approach, we apply the adaptive tensor kernel machine (Adapt-TKM) to seizure detection on behind-the-ear EEG. By personalizing patient-independent models with a small amount of patient-specific data, the patient-adapted model (which utilizes the Adapt-TKM), achieves better performance compared to the patient-independent and fully patient-specific models. Notably, it is able to do so while requiring around 100 times fewer parameters than the adaptive SVM model, leading to a correspondingly faster inference speed. This makes the Adapt-TKM especially useful for resource-constrained wearable devices.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SeeNav-Agent: Enhancing Vision-Language Navigation with Visual Prompt and Step-Level Policy Optimization</title>
<link>https://arxiv.org/abs/2512.02631</link>
<guid>https://arxiv.org/abs/2512.02631</guid>
<content:encoded><![CDATA[
<div> Vision-Language Navigation, Large Vision-Language Models, Visual Prompt, Reinforcement Fine-Tuning, Navigation Success Rate<br /><br />Summary:<br /><br />This work presents SeeNav-Agent, a novel framework designed to enhance Vision-Language Navigation (VLN) agents that utilize Large Vision-Language Models (LVLMs), which typically suffer from perception, reasoning, and planning errors. To tackle perception hallucinations and improve spatial awareness, the framework introduces a dual-view Visual Prompt (VP) technique applied in the input space. For improving planning and post-training efficiency, a step-level Reinforcement Fine-Tuning (RFT) method called Step Reward Group Policy Optimization (SRGPO) is developed. SRGPO defines verifiable process rewards and performs efficient advantage estimation by randomly grouping navigation steps, providing dense reward signals to boost reinforcement learning. Experimental evaluations on the EmbodiedBench Navigation benchmark show that integrating the zero-shot VP module with GPT-4.1 reaches a navigation success rate of 86.7%, outperforming current top LVLMs by approximately 20 percentage points. Additionally, after SRGPO-based post-training, the Qwen2.5-VL-3B model achieves a success rate of 72.3%, 5.6 percentage points higher than the best existing LVLM. Compared to other RFT algorithms like GRPO and GiGPO, SRGPO demonstrates superior training stability, faster convergence, and better generalization, indicating its effectiveness in advancing VLN performance. <div>
arXiv:2512.02631v1 Announce Type: new 
Abstract: Existing Vision-Language Navigation (VLN) agents based on Large Vision-Language Models (LVLMs) often suffer from perception errors, reasoning errors, and planning errors, which significantly hinder their navigation performance. To address these limitations, a novel VLN agent framework, named SeeNav-Agent, is proposed in this work. First, to reduce perception hallucinations of the visual module of the VLN agent, a dual-view Visual Prompt (VP) technique is introduced in the input space, which can also improve the agent's understanding of current spatial states. Subsequently, a novel step-level Reinforcement Fine-Tuning (RFT) method, Step Reward Group Policy Optimization (SRGPO), is designed for the post-training of VLN agents. In SRGPO, we first define verifiable process rewards for the navigation task, and then perform efficient step-level advantage estimation by randomly grouping different navigation steps. SRGPO provides dense reward signals for the reinforcement learning process of the VLN agent and enhances its planning capability. Experimental results on the EmbodiedBench Navigation benchmark indicate that by introducing the zero-shot VP module, the GPT-4.1 achieves a navigation success rate of 86.7%, surpassing the current best LVLM by approximately 20 percentage points (pp). Through post-training based on SRGPO, the Qwen2.5-VL-3B model reaches a navigation success rate of 72.3%, outperforming the best existing LVLM model by 5.6 pp. Moreover, compared to RFT algorithms such as GRPO and GiGPO, the proposed SRGPO demonstrates significant improvements in training stability, convergence efficiency, and generalization capability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Joint Distillation for Fast Likelihood Evaluation and Sampling in Flow-based Models</title>
<link>https://arxiv.org/abs/2512.02636</link>
<guid>https://arxiv.org/abs/2512.02636</guid>
<content:encoded><![CDATA[
<div> Keywords: flow-based generative models, likelihood evaluation, neural function evaluations, continuous normalizing flows, distillation

<br /><br />Summary:  
1. The paper addresses the challenge of efficient log-likelihood evaluation in generative models, especially diffusion and flow-based models, which currently require hundreds to thousands of neural function evaluations (NFEs) per likelihood computation.  
2. Existing distillation techniques have reduced sampling steps significantly but have compromised likelihood tractability by either abandoning likelihood computation or still demanding costly integration over full trajectories.  
3. The authors introduce Fast Flow Joint Distillation (F2D2), a novel framework that drastically cuts down the NFEs needed for both sampling and likelihood evaluation by about two orders of magnitude.  
4. F2D2 leverages the insight that in continuous normalizing flows, sampling and likelihood ODEs share a velocity field, enabling joint distillation of both sampling trajectories and cumulative divergence using a single unified model with an added divergence prediction head.  
5. The method is modular, compatible with existing few-step flow-based sampling models, preserves high sample quality, and resolves a key computational bottleneck in flow-based generative models. As a practical application, the paper proposes a lightweight self-guidance method that allows a 2-step MeanFlow model to outperform a 1024-step teacher model with only one extra backward NFE. <div>
arXiv:2512.02636v1 Announce Type: new 
Abstract: Log-likelihood evaluation enables important capabilities in generative models, including model comparison, certain fine-tuning objectives, and many downstream applications. Yet paradoxically, some of today's best generative models -- diffusion and flow-based models -- still require hundreds to thousands of neural function evaluations (NFEs) to compute a single likelihood. While recent distillation methods have successfully accelerated sampling to just a few steps, they achieve this at the cost of likelihood tractability: existing approaches either abandon likelihood computation entirely or still require expensive integration over full trajectories. We present fast flow joint distillation (F2D2), a framework that simultaneously reduces the number of NFEs required for both sampling and likelihood evaluation by two orders of magnitude. Our key insight is that in continuous normalizing flows, the coupled ODEs for sampling and likelihood are computed from a shared underlying velocity field, allowing us to jointly distill both the sampling trajectory and cumulative divergence using a single model. F2D2 is modular, compatible with existing flow-based few-step sampling models, and requires only an additional divergence prediction head. Experiments demonstrate F2D2's capability of achieving accurate log-likelihood with few-step evaluations while maintaining high sample quality, solving a long-standing computational bottleneck in flow-based generative models. As an application of our approach, we propose a lightweight self-guidance method that enables a 2-step MeanFlow model to outperform a 1024 step teacher model with only a single additional backward NFE.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Weighted LSSVM for Multi-View Classification</title>
<link>https://arxiv.org/abs/2512.02653</link>
<guid>https://arxiv.org/abs/2512.02653</guid>
<content:encoded><![CDATA[
<div> Multi-view learning, kernel methods, LS-SVM, adaptive weighting, privacy-preserving<br /><br />Summary:<br /><br />This paper introduces AW-LSSVM, an adaptive weighted least squares support vector machine designed for multi-view learning scenarios. Multi-view learning integrates various representations of the same data instances to enhance overall model performance. Existing kernel-based methods often rely on fusion techniques that lack explicit enforcement of collaboration types across different views or employ co-regularization methods that limit comprehensive global collaboration. AW-LSSVM addresses these shortcomings by promoting complementary learning via an iterative global coupling mechanism. This approach enables each view to concentrate on difficult samples identified by other views from previous iterations, dynamically adjusting the view weights to better capture the complexities in the data. Experiments on multiple datasets demonstrate that AW-LSSVM consistently outperforms prevailing kernel-based multi-view learning methods. Additionally, the method retains the isolation of raw features for each view, which inherently supports privacy-preserving applications by avoiding direct data sharing or fusion of sensitive raw data. The proposed adaptive weighted LS-SVM framework thus balances improved predictive accuracy with privacy considerations, making it well-suited for real-world scenarios involving diverse data representations and privacy constraints. <div>
arXiv:2512.02653v1 Announce Type: new 
Abstract: Multi-view learning integrates diverse representations of the same instances to improve performance. Most existing kernel-based multi-view learning methods use fusion techniques without enforcing an explicit collaboration type across views or co-regularization which limits global collaboration. We propose AW-LSSVM, an adaptive weighted LS-SVM that promotes complementary learning by an iterative global coupling to make each view focus on hard samples of others from previous iterations. Experiments demonstrate that AW-LSSVM outperforms existing kernel-based multi-view methods on most datasets, while keeping raw features isolated, making it also suitable for privacy-preserving scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distill, Forget, Repeat: A Framework for Continual Unlearning in Text-to-Image Diffusion Models</title>
<link>https://arxiv.org/abs/2512.02657</link>
<guid>https://arxiv.org/abs/2512.02657</guid>
<content:encoded><![CDATA[
<div> Keywords: continual unlearning, generative models, machine unlearning, generative distillation, data privacy<br /><br />Summary:<br /><br />The paper addresses the challenge of machine unlearning (MU) within large-scale visual generative models, especially under real-world conditions where deletion requests happen sequentially, termed continual unlearning (CUL). Existing MU methods applied naïvely in this continual setting cause significant issues, including stability crises, retention collapse, unwanted damage to related concepts, and loss of generative quality. To tackle these problems, the authors propose a novel continual unlearning framework based on generative distillation, which treats each unlearning step as a multi-objective teacher-student distillation task. This approach draws on continual learning principles to preserve model integrity while accommodating multiple sequential deletion requests. Experimental evaluation on a 10-step sequential benchmark demonstrates that their method effectively forgets targeted concepts with high fidelity without notable negative impact on the retention of other concepts or the overall image quality. Their framework notably outperforms existing baselines in terms of both unlearning accuracy and stability. The study offers a practical pathway for industries to responsibly deploy and maintain large generative models while complying with evolving data privacy regulations such as GDPR’s “Right to be Forgotten.” This work thus represents an important step towards scalable and stable continual unlearning in generative AI systems. <div>
arXiv:2512.02657v1 Announce Type: new 
Abstract: The recent rapid growth of visual generative models trained on vast web-scale datasets has created significant tension with data privacy regulations and copyright laws, such as GDPR's ``Right to be Forgotten.'' This necessitates machine unlearning (MU) to remove specific concepts without the prohibitive cost of retraining. However, existing MU techniques are fundamentally ill-equipped for real-world scenarios where deletion requests arrive sequentially, a setting known as continual unlearning (CUL). Naively applying one-shot methods in a continual setting triggers a stability crisis, leading to a cascade of degradation characterized by retention collapse, compounding collateral damage to related concepts, and a sharp decline in generative quality. To address this critical challenge, we introduce a novel generative distillation based continual unlearning framework that ensures targeted and stable unlearning under sequences of deletion requests. By reframing each unlearning step as a multi-objective, teacher-student distillation process, the framework leverages principles from continual learning to maintain model integrity. Experiments on a 10-step sequential benchmark demonstrate that our method unlearns forget concepts with better fidelity and achieves this without significant interference to the performance on retain concepts or the overall image quality, substantially outperforming baselines. This framework provides a viable pathway for the responsible deployment and maintenance of large-scale generative models, enabling industries to comply with ongoing data removal requests in a practical and effective manner.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph VQ-Transformer (GVT): Fast and Accurate Molecular Generation via High-Fidelity Discrete Latents</title>
<link>https://arxiv.org/abs/2512.02667</link>
<guid>https://arxiv.org/abs/2512.02667</guid>
<content:encoded><![CDATA[
arXiv:2512.02667v1 Announce Type: new 
Abstract: The de novo generation of molecules with desirable properties is a critical challenge, where diffusion models are computationally intensive and autoregressive models struggle with error propagation. In this work, we introduce the Graph VQ-Transformer (GVT), a two-stage generative framework that achieves both high accuracy and efficiency. The core of our approach is a novel Graph Vector Quantized Variational Autoencoder (VQ-VAE) that compresses molecular graphs into high-fidelity discrete latent sequences. By synergistically combining a Graph Transformer with canonical Reverse Cuthill-McKee (RCM) node ordering and Rotary Positional Embeddings (RoPE), our VQ-VAE achieves near-perfect reconstruction rates. An autoregressive Transformer is then trained on these discrete latents, effectively converting graph generation into a well-structured sequence modeling problem. Crucially, this mapping of complex graphs to high-fidelity discrete sequences bridges molecular design with the powerful paradigm of large-scale sequence modeling, unlocking potential synergies with Large Language Models (LLMs). Extensive experiments show that GVT achieves state-of-the-art or highly competitive performance across major benchmarks like ZINC250k, MOSES, and GuacaMol, and notably outperforms leading diffusion models on key distribution similarity metrics such as FCD and KL Divergence. With its superior performance, efficiency, and architectural novelty, GVT not only presents a compelling alternative to diffusion models but also establishes a strong new baseline for the field, paving the way for future research in discrete latent-space molecular generation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformal Correction for Efficiency May be at Odds with Entropy</title>
<link>https://arxiv.org/abs/2512.02704</link>
<guid>https://arxiv.org/abs/2512.02704</guid>
<content:encoded><![CDATA[
arXiv:2512.02704v1 Announce Type: new 
Abstract: Conformal prediction (CP) provides a comprehensive framework to produce statistically rigorous uncertainty sets for black-box machine learning models. To further improve the efficiency of CP, conformal correction is proposed to fine-tune or wrap the base model with an extra module using a conformal-aware inefficiency loss. In this work, we empirically and theoretically identify a trade-off between the CP efficiency and the entropy of model prediction. We then propose an entropy-constrained conformal correction method, exploring a better Pareto optimum between efficiency and entropy. Extensive experimental results on both computer vision and graph datasets demonstrate the efficacy of the proposed method. For instance, it can significantly improve the efficiency of state-of-the-art CP methods by up to 34.4%, given an entropy threshold.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FGC-Comp: Adaptive Neighbor-Grouped Attribute Completion for Graph-based Anomaly Detection</title>
<link>https://arxiv.org/abs/2512.02705</link>
<guid>https://arxiv.org/abs/2512.02705</guid>
<content:encoded><![CDATA[
arXiv:2512.02705v1 Announce Type: new 
Abstract: Graph-based Anomaly Detection models have gained widespread adoption in recent years, identifying suspicious nodes by aggregating neighborhood information. However, most existing studies overlook the pervasive issues of missing and adversarially obscured node attributes, which can undermine aggregation stability and prediction reliability. To mitigate this, we propose FGC-Comp, a lightweight, classifier-agnostic, and deployment-friendly attribute completion module-designed to enhance neighborhood aggregation under incomplete attributes. We partition each node's neighbors into three label-based groups, apply group-specific transforms to the labeled groups while a node-conditioned gate handles unknowns, fuse messages via residual connections, and train end-to-end with a binary classification objective to improve aggregation stability and prediction reliability under missing attributes. Experiments on two real-world fraud datasets validate the effectiveness of the approach with negligible computational overhead.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Credal Graph Neural Networks</title>
<link>https://arxiv.org/abs/2512.02722</link>
<guid>https://arxiv.org/abs/2512.02722</guid>
<content:encoded><![CDATA[
arXiv:2512.02722v1 Announce Type: new 
Abstract: Uncertainty quantification is essential for deploying reliable Graph Neural Networks (GNNs), where existing approaches primarily rely on Bayesian inference or ensembles. In this paper, we introduce the first credal graph neural networks (CGNNs), which extend credal learning to the graph domain by training GNNs to output set-valued predictions in the form of credal sets. To account for the distinctive nature of message passing in GNNs, we develop a complementary approach to credal learning that leverages different aspects of layer-wise information propagation. We assess our approach on uncertainty quantification in node classification under out-of-distribution conditions. Our analysis highlights the critical role of the graph homophily assumption in shaping the effectiveness of uncertainty estimates. Extensive experiments demonstrate that CGNNs deliver more reliable representations of epistemic uncertainty and achieve state-of-the-art performance under distributional shift on heterophilic graphs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Jamming for Autoencoder Distribution Matching</title>
<link>https://arxiv.org/abs/2512.02740</link>
<guid>https://arxiv.org/abs/2512.02740</guid>
<content:encoded><![CDATA[
arXiv:2512.02740v1 Announce Type: new 
Abstract: We propose the use of adversarial wireless jamming to regularise the latent space of an autoencoder to match a diagonal Gaussian distribution. We consider the minimisation of a mean squared error distortion, where a jammer attempts to disrupt the recovery of a Gaussian source encoded and transmitted over the adversarial channel. A straightforward consequence of existing theoretical results is the fact that the saddle point of a minimax game - involving such an encoder, its corresponding decoder, and an adversarial jammer - consists of diagonal Gaussian noise output by the jammer. We use this result as inspiration for a novel approach to distribution matching in the latent space, utilising jamming as an auxiliary objective to encourage the aggregated latent posterior to match a diagonal Gaussian distribution. Using this new technique, we achieve distribution matching comparable to standard variational autoencoders and to Wasserstein autoencoders. This approach can also be generalised to other latent distributions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FiMMIA: scaling semantic perturbation-based membership inference across modalities</title>
<link>https://arxiv.org/abs/2512.02786</link>
<guid>https://arxiv.org/abs/2512.02786</guid>
<content:encoded><![CDATA[
arXiv:2512.02786v1 Announce Type: new 
Abstract: Membership Inference Attacks (MIAs) aim to determine whether a specific data point was included in the training set of a target model. Although there are have been numerous methods developed for detecting data contamination in large language models (LLMs), their performance on multimodal LLMs (MLLMs) falls short due to the instabilities introduced through multimodal component adaptation and possible distribution shifts across multiple inputs. In this work, we investigate multimodal membership inference and address two issues: first, by identifying distribution shifts in the existing datasets, and second, by releasing an extended baseline pipeline to detect them. We also generalize the perturbation-based membership inference methods to MLLMs and release \textbf{FiMMIA} -- a modular \textbf{F}ramework for \textbf{M}ultimodal \textbf{MIA}.\footnote{The source code and framework have been made publicly available under the MIT license via \href{https://github.com/ai-forever/data_leakage_detect}{link}.The video demonstration is available on \href{https://youtu.be/a9L4-H80aSg}{YouTube}.} Our approach trains a neural network to analyze the target model's behavior on perturbed inputs, capturing distributional differences between members and non-members. Comprehensive evaluations on various fine-tuned multimodal models demonstrate the effectiveness of our perturbation-based membership inference attacks in multimodal domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Navigation to Refinement: Revealing the Two-Stage Nature of Flow-based Diffusion Models through Oracle Velocity</title>
<link>https://arxiv.org/abs/2512.02826</link>
<guid>https://arxiv.org/abs/2512.02826</guid>
<content:encoded><![CDATA[
arXiv:2512.02826v1 Announce Type: new 
Abstract: Flow-based diffusion models have emerged as a leading paradigm for training generative models across images and videos. However, their memorization-generalization behavior remains poorly understood. In this work, we revisit the flow matching (FM) objective and study its marginal velocity field, which admits a closed-form expression, allowing exact computation of the oracle FM target. Analyzing this oracle velocity field reveals that flow-based diffusion models inherently formulate a two-stage training target: an early stage guided by a mixture of data modes, and a later stage dominated by the nearest data sample. The two-stage objective leads to distinct learning behaviors: the early navigation stage generalizes across data modes to form global layouts, whereas the later refinement stage increasingly memorizes fine-grained details. Leveraging these insights, we explain the effectiveness of practical techniques such as timestep-shifted schedules, classifier-free guidance intervals, and latent space design choices. Our study deepens the understanding of diffusion model training dynamics and offers principles for guiding future architectural and algorithmic improvements.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Comparative Study on How Data Normalization Affects Zero-Shot Generalization in Time Series Foundation Models</title>
<link>https://arxiv.org/abs/2512.02833</link>
<guid>https://arxiv.org/abs/2512.02833</guid>
<content:encoded><![CDATA[
arXiv:2512.02833v1 Announce Type: new 
Abstract: We investigate input normalization methods for Time-Series Foundation Models (TSFMs). While normalization is well-studied in dataset-specific time-series models, it remains overlooked in TSFMs where generalization is critical. Time-series data, unlike text or images, exhibits significant scale variation across domains and channels, coupled with non-stationarity, can undermine TSFM performance regardless of architectural complexity. Through systematic evaluation across four architecturally diverse TSFMs, we empirically establish REVIN as the most efficient approach, reducing zero-shot MASE by 89\% relative to an un-normalized baseline and by 44\% versus other normalization methods, while matching the best in-domain accuracy (0.84 MASE) without any dataset-level preprocessing -- yielding the highest accuracy-efficiency trade-off. Yet its effect utilization depends on architectural design choices and optimization objective, particularly with respect to training loss scale sensitivity and model type (probabilistic, point-forecast, or LLM-based models).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GraphMatch: Fusing Language and Graph Representations in a Dynamic Two-Sided Work Marketplace</title>
<link>https://arxiv.org/abs/2512.02849</link>
<guid>https://arxiv.org/abs/2512.02849</guid>
<content:encoded><![CDATA[
arXiv:2512.02849v1 Announce Type: new 
Abstract: Recommending matches in a text-rich, dynamic two-sided marketplace presents unique challenges due to evolving content and interaction graphs. We introduce GraphMatch, a new large-scale recommendation framework that fuses pre-trained language models with graph neural networks to overcome these challenges. Unlike prior approaches centered on standalone models, GraphMatch is a comprehensive recipe built on powerful text encoders and GNNs working in tandem. It employs adversarial negative sampling alongside point-in-time subgraph training to learn representations that capture both the fine-grained semantics of evolving text and the time-sensitive structure of the graph. We evaluated extensively on interaction data from Upwork, a leading labor marketplace, at large scale, and discuss our approach towards low-latency inference suitable for real-time use. In our experiments, GraphMatch outperforms language-only and graph-only baselines on matching tasks while being efficient at runtime. These results demonstrate that unifying language and graph representations yields a highly effective solution to text-rich, dynamic two-sided recommendations, bridging the gap between powerful pretrained LMs and large-scale graphs in practice.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Decentralized Federated Learning for Robust Optimization</title>
<link>https://arxiv.org/abs/2512.02852</link>
<guid>https://arxiv.org/abs/2512.02852</guid>
<content:encoded><![CDATA[
arXiv:2512.02852v1 Announce Type: new 
Abstract: In decentralized federated learning (DFL), the presence of abnormal clients, often caused by noisy or poisoned data, can significantly disrupt the learning process and degrade the overall robustness of the model. Previous methods on this issue often require a sufficiently large number of normal neighboring clients or prior knowledge of reliable clients, which reduces the practical applicability of DFL. To address these limitations, we develop here a novel adaptive DFL (aDFL) approach for robust estimation. The key idea is to adaptively adjust the learning rates of clients. By assigning smaller rates to suspicious clients and larger rates to normal clients, aDFL mitigates the negative impact of abnormal clients on the global model in a fully adaptive way. Our theory does not put any stringent conditions on neighboring nodes and requires no prior knowledge. A rigorous convergence analysis is provided to guarantee the oracle property of aDFL. Extensive numerical experiments demonstrate the superior performance of the aDFL method.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Assessing the performance of correlation-based multi-fidelity neural emulators</title>
<link>https://arxiv.org/abs/2512.02868</link>
<guid>https://arxiv.org/abs/2512.02868</guid>
<content:encoded><![CDATA[
arXiv:2512.02868v1 Announce Type: new 
Abstract: Outer loop tasks such as optimization, uncertainty quantification or inference can easily become intractable when the underlying high-fidelity model is computationally expensive. Similarly, data-driven architectures typically require large datasets to perform predictive tasks with sufficient accuracy. A possible approach to mitigate these challenges is the development of multi-fidelity emulators, leveraging potentially biased, inexpensive low-fidelity information while correcting and refining predictions using scarce, accurate high-fidelity data. This study investigates the performance of multi-fidelity neural emulators, neural networks designed to learn the input-to-output mapping by integrating limited high-fidelity data with abundant low-fidelity model solutions. We investigate the performance of such emulators for low and high-dimensional functions, with oscillatory character, in the presence of discontinuities, for collections of models with equal and dissimilar parametrization, and for a possibly large number of potentially corrupted low-fidelity sources. In doing so, we consider a large number of architectural, hyperparameter, and dataset configurations including networks with a different amount of spectral bias (Multi-Layered Perceptron, Siren and Kolmogorov Arnold Network), various mechanisms for coordinate encoding, exact or learnable low-fidelity information, and for varying training dataset size. We further analyze the added value of the multi-fidelity approach by conducting equivalent single-fidelity tests for each case, quantifying the performance gains achieved through fusing multiple sources of information.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OptPO: Optimal Rollout Allocation for Test-time Policy Optimization</title>
<link>https://arxiv.org/abs/2512.02882</link>
<guid>https://arxiv.org/abs/2512.02882</guid>
<content:encoded><![CDATA[
arXiv:2512.02882v1 Announce Type: new 
Abstract: Test-time policy optimization enables large language models (LLMs) to adapt to distribution shifts by leveraging feedback from self-generated rollouts. However, existing methods rely on fixed-budget majority voting to estimate rewards, incurring substantial computational redundancy. We propose Optimal Rollout Allocation for Test-time Policy Optimization (OptPO), a principled framework that adaptively allocates inference budgets. By formulating the voting process as a Bayesian sequential probability ratio test, OptPO dynamically halts sampling once the posterior confidence in a consensus answer exceeds a specified threshold. Crucially, it utilizes the retained rollouts for on-policy updates, seamlessly integrating with algorithms like PPO or GRPO without requiring ground-truth labels. Across diverse reasoning benchmarks, OptPO significantly reduces rollout overhead compared to fixed-sample baselines while preserving or improving accuracy. By unifying statistically optimal stopping with test-time learning, OptPO offers a computationally efficient paradigm for test-time adaptation. The source code will be open upon acceptance at https://open-upon-acceptance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FAIRY2I: Universal Extremely-Low Bit QAT framework via Widely-Linear Representation and Phase-Aware Quantization</title>
<link>https://arxiv.org/abs/2512.02901</link>
<guid>https://arxiv.org/abs/2512.02901</guid>
<content:encoded><![CDATA[
arXiv:2512.02901v1 Announce Type: new 
Abstract: Large language models (LLMs) have revolutionized artificial intelligence, yet their massive memory and computational demands necessitate aggressive quantization, increasingly pushing representations toward the theoretical limit of a single bit. While complex-valued LLMs, such as iFairy, offer a superior chance for low-bit representation compared to real-valued counterparts, they require training from scratch, preventing the utilization of the vast ecosystem of pre-trained real-valued foundation models. Here we present Fairy2i, a universal framework that transforms pre-trained real-valued layers into an equivalent widely-linear complex form, enabling extremely low-bit quantization while reusing existing checkpoints. By proving a lossless mathematical equivalence between real and widely-linear maps, we convert standard Transformers into the complex domain and employ a phase-aware quantization scheme with a highly efficient codebook of fourth roots of unity. Furthermore, we introduce a recursive residual quantization mechanism that iteratively minimizes quantization error, allowing inference to proceed via efficient multiplication-free accumulation. We demonstrate that Fairy2i restores the performance of LLaMA-2 7B at an effective 2-bit precision to levels nearly comparable with full-precision baselines, significantly outperforming state-of-the-art real-valued binary and ternary quantization methods. This work bridges the gap between the representational efficiency of complex-valued arithmetic and the practical utility of pre-trained models, paving a new way for efficient inference on commodity hardware.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hypothesis Testing for Generalized Thurstone Models</title>
<link>https://arxiv.org/abs/2512.02912</link>
<guid>https://arxiv.org/abs/2512.02912</guid>
<content:encoded><![CDATA[
arXiv:2512.02912v1 Announce Type: new 
Abstract: In this work, we develop a hypothesis testing framework to determine whether pairwise comparison data is generated by an underlying \emph{generalized Thurstone model} $\mathcal{T}_F$ for a given choice function $F$. While prior work has predominantly focused on parameter estimation and uncertainty quantification for such models, we address the fundamental problem of minimax hypothesis testing for $\mathcal{T}_F$ models. We formulate this testing problem by introducing a notion of separation distance between general pairwise comparison models and the class of $\mathcal{T}_F$ models. We then derive upper and lower bounds on the critical threshold for testing that depend on the topology of the observation graph. For the special case of complete observation graphs, this threshold scales as $\Theta((nk)^{-1/2})$, where $n$ is the number of agents and $k$ is the number of comparisons per pair. Furthermore, we propose a hypothesis test based on our separation distance, construct confidence intervals, establish time-uniform bounds on the probabilities of type I and II errors using reverse martingale techniques, and derive minimax lower bounds using information-theoretic methods. Finally, we validate our results through experiments on synthetic and real-world datasets.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Multimodal Embeddings for Traffic Accident Prediction and Causal Estimation</title>
<link>https://arxiv.org/abs/2512.02920</link>
<guid>https://arxiv.org/abs/2512.02920</guid>
<content:encoded><![CDATA[
arXiv:2512.02920v1 Announce Type: new 
Abstract: We consider analyzing traffic accident patterns using both road network data and satellite images aligned to road graph nodes. Previous work for predicting accident occurrences relies primarily on road network structural features while overlooking physical and environmental information from the road surface and its surroundings. In this work, we construct a large multimodal dataset across six U.S. states, containing nine million traffic accident records from official sources, and one million high-resolution satellite images for each node of the road network. Additionally, every node is annotated with features such as the region's weather statistics and road type (e.g., residential vs. motorway), and each edge is annotated with traffic volume information (i.e., Average Annual Daily Traffic). Utilizing this dataset, we conduct a comprehensive evaluation of multimodal learning methods that integrate both visual and network embeddings. Our findings show that integrating both data modalities improves prediction accuracy, achieving an average AUROC of $90.1\%$, which is a $3.7\%$ gain over graph neural network models that only utilize graph structures. With the improved embeddings, we conduct a causal analysis based on a matching estimator to estimate the key contributing factors influencing traffic accidents. We find that accident rates rise by $24\%$ under higher precipitation, by $22\%$ on higher-speed roads such as motorways, and by $29\%$ due to seasonal patterns, after adjusting for other confounding factors. Ablation studies confirm that satellite imagery features are essential for achieving accurate prediction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast Gaussian Process Approximations for Autocorrelated Data</title>
<link>https://arxiv.org/abs/2512.02925</link>
<guid>https://arxiv.org/abs/2512.02925</guid>
<content:encoded><![CDATA[
arXiv:2512.02925v1 Announce Type: new 
Abstract: This paper is concerned with the problem of how to speed up computation for Gaussian process models trained on autocorrelated data. The Gaussian process model is a powerful tool commonly used in nonlinear regression applications. Standard regression modeling assumes random samples and an independently, identically distributed noise. Various fast approximations that speed up Gaussian process regression work under this standard setting. But for autocorrelated data, failing to account for autocorrelation leads to a phenomenon known as temporal overfitting that deteriorates model performance on new test instances. To handle autocorrelated data, existing fast Gaussian process approximations have to be modified; one such approach is to segment the originally correlated data points into blocks in which the blocked data are de-correlated. This work explains how to make some of the existing Gaussian process approximations work with blocked data. Numerical experiments across diverse application datasets demonstrate that the proposed approaches can remarkably accelerate computation for Gaussian process regression on autocorrelated data without compromising model prediction performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pruning AMR: Efficient Visualization of Implicit Neural Representations via Weight Matrix Analysis</title>
<link>https://arxiv.org/abs/2512.02967</link>
<guid>https://arxiv.org/abs/2512.02967</guid>
<content:encoded><![CDATA[
arXiv:2512.02967v1 Announce Type: new 
Abstract: An implicit neural representation (INR) is a neural network that approximates a spatiotemporal function. Many memory-intensive visualization tasks, including modern 4D CT scanning methods, represent data natively as INRs. While INRs are prized for being more memory-efficient than traditional data stored on a lattice, many visualization tasks still require discretization to a regular grid. We present PruningAMR, an algorithm that builds a mesh with resolution adapted to geometric features encoded by the INR. To identify these geometric features, we use an interpolative decomposition pruning method on the weight matrices of the INR. The resulting pruned network is used to guide adaptive mesh refinement, enabling automatic mesh generation tailored to the underlying resolution of the function. Starting from a pre-trained INR--without access to its training data--we produce a variable resolution visualization with substantial memory savings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ProteinPNet: Prototypical Part Networks for Concept Learning in Spatial Proteomics</title>
<link>https://arxiv.org/abs/2512.02983</link>
<guid>https://arxiv.org/abs/2512.02983</guid>
<content:encoded><![CDATA[
arXiv:2512.02983v1 Announce Type: new 
Abstract: Understanding the spatial architecture of the tumor microenvironment (TME) is critical to advance precision oncology. We present ProteinPNet, a novel framework based on prototypical part networks that discovers TME motifs from spatial proteomics data. Unlike traditional post-hoc explanability models, ProteinPNet directly learns discriminative, interpretable, faithful spatial prototypes through supervised training. We validate our approach on synthetic datasets with ground truth motifs, and further test it on a real-world lung cancer spatial proteomics dataset. ProteinPNet consistently identifies biologically meaningful prototypes aligned with different tumor subtypes. Through graphical and morphological analyses, we show that these prototypes capture interpretable features pointing to differences in immune infiltration and tissue modularity. Our results highlight the potential of prototype-based learning to reveal interpretable spatial biomarkers within the TME, with implications for mechanistic discovery in spatial omics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Distribution-Calibrated Inference time compute for Thinking LLM-as-a-Judge</title>
<link>https://arxiv.org/abs/2512.03019</link>
<guid>https://arxiv.org/abs/2512.03019</guid>
<content:encoded><![CDATA[
arXiv:2512.03019v1 Announce Type: new 
Abstract: Thinking Large Language Models (LLMs) used as judges for pairwise preferences remain noisy at the single-sample level, and common aggregation rules (majority vote, soft self-consistency, or instruction-based self-aggregation) are inconsistent when ties are allowed. We study inference-time compute (ITC) for evaluators that generate n independent thinking-rating samples per item, and propose a principled, distribution-calibrated aggregation scheme. Our method models three-way preferences with a Bradley-Terry-Davidson formulation on rating counts, leveraging both polarity (margin among non-ties) and decisiveness (non-tie rate) to distinguish narrow margins from strong consensus. Across various evaluation benchmarks, our approach consistently reduces MAE and increases pairwise accuracy versus standard baselines, and when evaluated against human-consensus meta-labels, matches or exceeds individual human raters. These results show that carefully allocating ITC and aggregating with distribution-aware methods turns noisy individual model judgments into reliable ratings for evaluation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TokenPowerBench: Benchmarking the Power Consumption of LLM Inference</title>
<link>https://arxiv.org/abs/2512.03024</link>
<guid>https://arxiv.org/abs/2512.03024</guid>
<content:encoded><![CDATA[
arXiv:2512.03024v1 Announce Type: new 
Abstract: Large language model (LLM) services now answer billions of queries per day, and industry reports show that inference, not training, accounts for more than 90% of total power consumption. However, existing benchmarks focus on either training/fine-tuning or performance of inference and provide little support for power consumption measurement and analysis of inference. We introduce TokenPowerBench, the first lightweight and extensible benchmark designed for LLM-inference power consumption studies. The benchmark combines (i) a declarative configuration interface covering model choice, prompt set, and inference engine, (ii) a measurement layer that captures GPU-, node-, and system-level power without specialized power meters, and (iii) a phase-aligned metrics pipeline that attributes energy to the prefill and decode stages of every request. These elements make it straight-forward to explore the power consumed by an LLM inference run; furthermore, by varying batch size, context length, parallelism strategy and quantization, users can quickly assess how each setting affects joules per token and other energy-efficiency metrics. We evaluate TokenPowerBench on four of the most widely used model series (Llama, Falcon, Qwen, and Mistral). Our experiments cover from 1 billion parameters up to the frontier-scale Llama3-405B model. Furthermore, we release TokenPowerBench as open source to help users to measure power consumption, forecast operating expenses, and meet sustainability targets when deploying LLM services.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Real-time Face Mask Detection and Social Distancing System for COVID-19 using Attention-InceptionV3 Model</title>
<link>https://arxiv.org/abs/2411.05312</link>
<guid>https://arxiv.org/abs/2411.05312</guid>
<content:encoded><![CDATA[
arXiv:2411.05312v1 Announce Type: cross 
Abstract: One of the deadliest pandemics is now happening in the current world due to COVID-19. This contagious virus is spreading like wildfire around the whole world. To minimize the spreading of this virus, World Health Organization (WHO) has made protocols mandatory for wearing face masks and maintaining 6 feet physical distance. In this paper, we have developed a system that can detect the proper maintenance of that distance and people are properly using masks or not. We have used the customized attention-inceptionv3 model in this system for the identification of those two components. We have used two different datasets along with 10,800 images including both with and without Face Mask images. The training accuracy has been achieved 98% and validation accuracy 99.5%. The system can conduct a precision value of around 98.2% and the frame rate per second (FPS) was 25.0. So, with this system, we can identify high-risk areas with the highest possibility of the virus spreading zone. This may help authorities to take necessary steps to locate those risky areas and alert the local people to ensure proper precautions in no time.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching an Online Multi-Institutional Research Level Software Engineering Course with Industry - an Experience Report</title>
<link>https://arxiv.org/abs/2512.01523</link>
<guid>https://arxiv.org/abs/2512.01523</guid>
<content:encoded><![CDATA[
arXiv:2512.01523v1 Announce Type: cross 
Abstract: Covid has made online teaching and learning acceptable and students, faculty, and industry professionals are all comfortable with this mode. This comfort can be leveraged to offer an online multi-institutional research-level course in an area where individual institutions may not have the requisite faculty to teach and/or research students to enroll. If the subject is of interest to industry, online offering also allows industry experts to contribute and participate with ease. Advanced topics in Software Engineering are ideally suited for experimenting with this approach as industry, which is often looking to incorporate advances in software engineering in their practices, is likely to agree to contribute and participate. In this paper we describe an experiment in teaching a course titled "AI in Software Engineering" jointly between two institutions with active industry participation, and share our and student's experience. We believe this collaborative teaching approach can be used for offering research level courses in any applied area of computer science by institutions who are small and find it difficult to offer research level courses on their own.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DySTAN: Joint Modeling of Sedentary Activity and Social Context from Smartphone Sensors</title>
<link>https://arxiv.org/abs/2512.02025</link>
<guid>https://arxiv.org/abs/2512.02025</guid>
<content:encoded><![CDATA[
arXiv:2512.02025v1 Announce Type: cross 
Abstract: Accurately recognizing human context from smartphone sensor data remains a significant challenge, especially in sedentary settings where activities such as studying, attending lectures, relaxing, and eating exhibit highly similar inertial patterns. Furthermore, social context plays a critical role in understanding user behavior, yet is often overlooked in mobile sensing research. To address these gaps, we introduce LogMe, a mobile sensing application that passively collects smartphone sensor data (accelerometer, gyroscope, magnetometer, and rotation vector) and prompts users for hourly self-reports capturing both sedentary activity and social context. Using this dual-label dataset, we propose DySTAN (Dynamic Cross-Stitch with Task Attention Network), a multi-task learning framework that jointly classifies both context dimensions from shared sensor inputs. It integrates task-specific layers with cross-task attention to model subtle distinctions effectively. DySTAN improves sedentary activity macro F1 scores by 21.8% over a single-task CNN-BiLSTM-GRU (CBG) model and by 8.2% over the strongest multi-task baseline, Sluice Network (SN). These results demonstrate the importance of modeling multiple, co-occurring context dimensions to improve the accuracy and robustness of mobile context recognition.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Difficulty of Token-Level Modeling of Dysfluency and Fluency Shaping Artifacts</title>
<link>https://arxiv.org/abs/2512.02027</link>
<guid>https://arxiv.org/abs/2512.02027</guid>
<content:encoded><![CDATA[
arXiv:2512.02027v1 Announce Type: cross 
Abstract: Automatic transcription of stuttered speech remains a challenge, even for modern end-to-end (E2E) automatic speech recognition (ASR) frameworks. Dysfluencies and fluency-shaping artifacts are often overlooked, resulting in non-verbatim transcriptions with limited clinical and research value. We propose a parameter-efficient adaptation method to decode dysfluencies and fluency modifications as special tokens within transcriptions, evaluated on simulated (LibriStutter, English) and natural (KSoF, German) stuttered speech datasets. To mitigate ASR performance disparities and bias towards English, we introduce a multi-step fine-tuning strategy with language-adaptive pretraining. Tokenization analysis further highlights the tokenizer's English-centric bias, which poses challenges for improving performance on German data. Our findings demonstrate the effectiveness of lightweight adaptation techniques for dysfluency-aware ASR while exposing key limitations in multilingual E2E systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Seizure-NGCLNet: Representation Learning of SEEG Spatial Pathological Patterns for Epileptic Seizure Detection via Node-Graph Dual Contrastive Learning</title>
<link>https://arxiv.org/abs/2512.02028</link>
<guid>https://arxiv.org/abs/2512.02028</guid>
<content:encoded><![CDATA[
arXiv:2512.02028v1 Announce Type: cross 
Abstract: Complex spatial connectivity patterns, such as interictal suppression and ictal propagation, complicate accurate drug-resistant epilepsy (DRE) seizure detection using stereotactic electroencephalography (SEEG) and traditional machine learning methods. Two critical challenges remain:(1)a low signal-to-noise ratio in functional connectivity estimates, making it difficult to learn seizure-related interactions; and (2)expert labels for spatial pathological connectivity patterns are difficult to obtain, meanwhile lacking the patterns' representation to improve seizure detection. To address these issues, we propose a novel node-graph dual contrastive learning framework, Seizure-NGCLNet, to learn SEEG interictal suppression and ictal propagation patterns for detecting DRE seizures with high precision. First, an adaptive graph augmentation strategy guided by centrality metrics is developed to generate seizure-related brain networks. Second, a dual-contrastive learning approach is integrated, combining global graph-level contrast with local node-graph contrast, to encode both spatial structural and semantic epileptogenic features. Third, the pretrained embeddings are fine-tuned via a top-k localized graph attention network to perform the final classification. Extensive experiments on a large-scale public SEEG dataset from 33 DRE patients demonstrate that Seizure-NGCLNet achieves state-of-the-art performance, with an average accuracy of 95.93%, sensitivity of 96.25%, and specificity of 94.12%. Visualizations confirm that the learned embeddings clearly separate ictal from interictal states, reflecting suppression and propagation patterns that correspond to the clinical mechanisms. These results highlight Seizure-NGCLNet's ability to learn interpretable spatial pathological patterns, enhancing both seizure detection and seizure onset zone localization.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative design and validation of therapeutic peptides for glioblastoma based on a potential target ATP5A</title>
<link>https://arxiv.org/abs/2512.02030</link>
<guid>https://arxiv.org/abs/2512.02030</guid>
<content:encoded><![CDATA[
arXiv:2512.02030v1 Announce Type: cross 
Abstract: Glioblastoma (GBM) remains the most aggressive tumor, urgently requiring novel therapeutic strategies. Here, we present a dry-to-wet framework combining generative modeling and experimental validation to optimize peptides targeting ATP5A, a potential peptide-binding protein for GBM. Our framework introduces the first lead-conditioned generative model, which focuses exploration on geometrically relevant regions around lead peptides and mitigates the combinatorial complexity of de novo methods. Specifically, we propose POTFlow, a \underline{P}rior and \underline{O}ptimal \underline{T}ransport-based \underline{Flow}-matching model for peptide optimization. POTFlow employs secondary structure information (e.g., helix, sheet, loop) as geometric constraints, which are further refined by optimal transport to produce shorter flow paths. With this design, our method achieves state-of-the-art performance compared with five popular approaches. When applied to GBM, our method generates peptides that selectively inhibit cell viability and significantly prolong survival in a patient-derived xenograft (PDX) model. As the first lead peptide-conditioned flow matching model, POTFlow holds strong potential as a generalizable framework for therapeutic peptide design.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Characterizing Continuous and Discrete Hybrid Latent Spaces for Structural Connectomes</title>
<link>https://arxiv.org/abs/2512.02032</link>
<guid>https://arxiv.org/abs/2512.02032</guid>
<content:encoded><![CDATA[
arXiv:2512.02032v1 Announce Type: cross 
Abstract: Structural connectomes are detailed graphs that map how different brain regions are physically connected, offering critical insight into aging, cognition, and neurodegenerative diseases. However, these connectomes are high-dimensional and densely interconnected, which makes them difficult to interpret and analyze at scale. While low-dimensional spaces like PCA and autoencoders are often used to capture major sources of variation, their latent spaces are generally continuous and cannot fully reflect the mixed nature of variability in connectomes, which include both continuous (e.g., connectivity strength) and discrete factors (e.g., imaging site). Motivated by this, we propose a variational autoencoder (VAE) with a hybrid latent space that jointly models the discrete and continuous components. We analyze a large dataset of 5,761 connectomes from six Alzheimer's disease studies with ten acquisition protocols. Each connectome represents a single scan from a unique subject (3579 females, 2182 males), aged 22 to 102, with 4338 cognitively normal, 809 with mild cognitive impairment (MCI), and 614 with Alzheimer's disease (AD). Each connectome contains 121 brain regions defined by the BrainCOLOR atlas. We train our hybrid VAE in an unsupervised way and characterize what each latent component captures. We find that the discrete space is particularly effective at capturing subtle site-related differences, achieving an Adjusted Rand Index (ARI) of 0.65 with site labels, significantly outperforming PCA and a standard VAE followed by clustering (p < 0.05). These results demonstrate that the hybrid latent space can disentangle distinct sources of variability in connectomes in an unsupervised manner, offering potential for large-scale connectome analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Integration of LSTM Networks in Random Forest Algorithms for Stock Market Trading Predictions</title>
<link>https://arxiv.org/abs/2512.02036</link>
<guid>https://arxiv.org/abs/2512.02036</guid>
<content:encoded><![CDATA[
arXiv:2512.02036v1 Announce Type: cross 
Abstract: The aim of this paper is the analysis and selection of stock trading systems that combine different models with data of different nature, such as financial and microeconomic information. Specifically, based on previous work by the authors and applying advanced techniques of Machine Learning and Deep Learning, our objective is to formulate trading algorithms for the stock market with empirically tested statistical advantages, thus improving results published in the literature. Our approach integrates Long Short-Term Memory (LSTM) networks with algorithms based on decision trees, such as Random Forest and Gradient Boosting. While the former analyze price patterns of financial assets, the latter are fed with economic data of companies. Numerical simulations of algorithmic trading with data from international companies and 10-weekday predictions confirm that an approach based on both fundamental and technical variables can outperform the usual approaches, which do not combine those two types of variables. In doing so, Random Forest turned out to be the best performer among the decision trees. We also discuss how the prediction performance of such a hybrid approach can be boosted by selecting the technical variables.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From 'What-is' to 'What-if' in Human-Factor Analysis: A Post-Occupancy Evaluation Case</title>
<link>https://arxiv.org/abs/2512.02060</link>
<guid>https://arxiv.org/abs/2512.02060</guid>
<content:encoded><![CDATA[
arXiv:2512.02060v1 Announce Type: cross 
Abstract: Human-factor analysis typically employs correlation analysis and significance testing to identify relationships between variables. However, these descriptive ('what-is') methods, while effective for identifying associations, are often insufficient for answering causal ('what-if') questions. Their application in such contexts often overlooks confounding and colliding variables, potentially leading to bias and suboptimal or incorrect decisions.
  We advocate for explicitly distinguishing descriptive from interventional questions in human-factor analysis, and applying causal inference frameworks specifically to these problems to prevent methodological mismatches. This approach disentangles complex variable relationships and enables counterfactual reasoning. Using post-occupancy evaluation (POE) data from the Center for the Built Environment's (CBE) Occupant Survey as a demonstration case, we show how causal discovery reveals intervention hierarchies and directional relationships that traditional associational analysis misses. The systematic distinction between causally associated and independent variables, combined with intervention prioritization capabilities, offers broad applicability to complex human-centric systems, for example, in building science or ergonomics, where understanding intervention effects is critical for optimization and decision-making.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum Machine Learning for Secondary Frequency Control</title>
<link>https://arxiv.org/abs/2512.02065</link>
<guid>https://arxiv.org/abs/2512.02065</guid>
<content:encoded><![CDATA[
arXiv:2512.02065v1 Announce Type: cross 
Abstract: Frequency control in power systems is critical to maintaining stability and preventing blackouts. Traditional methods like meta-heuristic algorithms and machine learning face limitations in real-time applicability and scalability. This paper introduces a novel approach using a pure variational quantum circuit (VQC) for real-time secondary frequency control in diesel generators. Unlike hybrid classical-quantum models, the proposed VQC operates independently during execution, eliminating latency from classical-quantum data exchange. The VQC is trained via supervised learning to map historical frequency deviations to optimal Proportional-Integral (PI) controller parameters using a pre-computed lookup table. Simulations demonstrate that the VQC achieves high prediction accuracy (over 90%) with sufficient quantum measurement shots and generalizes well across diverse test events. The quantum-optimized PI parameters significantly improve transient response, reducing frequency fluctuations and settling time.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Parallel Multi-Circuit Quantum Feature Fusion in Hybrid Quantum-Classical Convolutional Neural Networks for Breast Tumor Classification</title>
<link>https://arxiv.org/abs/2512.02066</link>
<guid>https://arxiv.org/abs/2512.02066</guid>
<content:encoded><![CDATA[
arXiv:2512.02066v1 Announce Type: cross 
Abstract: Quantum machine learning has emerged as a promising approach to improve feature extraction and classification tasks in high-dimensional data domains such as medical imaging. In this work, we present a hybrid Quantum-Classical Convolutional Neural Network (QCNN) architecture designed for the binary classification of the BreastMNIST dataset, a standardized benchmark for distinguishing between benign and malignant breast tumors. Our architecture integrates classical convolutional feature extraction with two distinct quantum circuits: an amplitude-encoding variational quantum circuit (VQC) and an angle-encoding VQC circuit with circular entanglement, both implemented on four qubits. These circuits generate quantum feature embeddings that are fused with classical features to form a joint feature space, which is subsequently processed by a fully connected classifier. To ensure fairness, the hybrid QCNN is parameter-matched against a baseline classical CNN, allowing us to isolate the contribution of quantum layers. Both models are trained under identical conditions using the Adam optimizer and binary cross-entropy loss. Experimental evaluation in five independent runs demonstrates that the hybrid QCNN achieves statistically significant improvements in classification accuracy compared to the classical CNN, as validated by a one-sided Wilcoxon signed rank test (p = 0.03125) and supported by large effect size of Cohen's d = 2.14. Our results indicate that hybrid QCNN architectures can leverage entanglement and quantum feature fusion to enhance medical image classification tasks. This work establishes a statistical validation framework for assessing hybrid quantum models in biomedical applications and highlights pathways for scaling to larger datasets and deployment on near-term quantum hardware.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The 4/$\delta$ Bound: Designing Predictable LLM-Verifier Systems for Formal Method Guarantee</title>
<link>https://arxiv.org/abs/2512.02080</link>
<guid>https://arxiv.org/abs/2512.02080</guid>
<content:encoded><![CDATA[
arXiv:2512.02080v1 Announce Type: cross 
Abstract: The idea of using Formal Verification tools with large language models (LLMs) has enabled scaling software verification beyond manual workflows. However, current methods remain unreliable. Without a solid theoretical footing, the refinement process can wander; sometimes it settles, sometimes it loops back, and sometimes it breaks away from any stable trajectory. This work bridges this critical gap by developing an LLM-Verifier Convergence Theorem, providing the first formal framework with provable guarantees for termination and convergence. We model the interaction between the LLM and the verifier as a discrete-time Markov Chain, with state transitions determined by a key parameter: the error-reduction probability ($\delta$). The procedure reaching the Verified state almost surely demonstrates that the program terminates for any $\delta > 0$, with an expected iteration count bounded by $\mathbb{E}[n] \leq 4/\delta$. We then stress-tested this prediction in an extensive empirical campaign comprising more than 90,000 trials. The empirical results match the theory with striking consistency. Every single run reached verification, and the convergence factor clustered tightly around $C_f\approx$ 1.0. Consequently, the bound mirrors the system's actual behavior. The evidence is sufficiently robust to support dividing the workflow into three distinct operating zones: marginal, practical, and high-performance. Consequently, we establish the design thresholds with absolute confidence. Together, the theoretical guarantee and the experimental evidence provide a clearer architectural foundation for LLM-assisted verification. Heuristic tuning no longer has to be carried out by the system. Engineers gain a framework that supports predictable resource planning and performance budgeting, precisely what is needed before deploying these pipelines into safety-critical software environments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Betti Numbers to Persistence Diagrams: A Hybrid Quantum Algorithm for Topological Data Analysis</title>
<link>https://arxiv.org/abs/2512.02081</link>
<guid>https://arxiv.org/abs/2512.02081</guid>
<content:encoded><![CDATA[
arXiv:2512.02081v1 Announce Type: cross 
Abstract: Persistence diagrams serve as a core tool in topological data analysis, playing a crucial role in pathological monitoring, drug discovery, and materials design. However, existing quantum topological algorithms, such as the LGZ algorithm, can only efficiently compute summary statistics like Betti numbers, failing to provide persistence diagram information that tracks the lifecycle of individual topological features, severely limiting their practical value. This paper proposes a novel quantum-classical hybrid algorithm that achieves, for the first time, the leap from "quantum computation of Betti numbers" to "quantum acquisition of practical persistence diagrams." The algorithm leverages the LGZ quantum algorithm as an efficient feature extractor, mining the harmonic form eigenvectors of the combinatorial Laplacian as well as Betti numbers, constructing specialized topological kernel functions to train a quantum support vector machine (QSVM), and learning the mapping from quantum topological features to persistence diagrams. The core contributions of this algorithm are: (1) elevating quantum topological computation from statistical summaries to pattern recognition, greatly expanding its application value; (2) obtaining more practical topological information in the form of persistence diagrams for real-world applications while maintaining the exponential speedup advantage of quantum computation; (3) proposing a novel hybrid paradigm of "classical precision guiding quantum efficiency." This method provides a feasible pathway for the practical implementation of quantum topological data analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Comparing Baseline and Day-1 Diffusion MRI Using Multimodal Deep Embeddings for Stroke Outcome Prediction</title>
<link>https://arxiv.org/abs/2512.02088</link>
<guid>https://arxiv.org/abs/2512.02088</guid>
<content:encoded><![CDATA[
arXiv:2512.02088v1 Announce Type: cross 
Abstract: This study compares baseline (J0) and 24-hour (J1) diffusion magnetic resonance imaging (MRI) for predicting three-month functional outcomes after acute ischemic stroke (AIS). Seventy-four AIS patients with paired apparent diffusion coefficient (ADC) scans and clinical data were analyzed. Three-dimensional ResNet-50 embeddings were fused with structured clinical variables, reduced via principal component analysis (<=12 components), and classified using linear support vector machines with eight-fold stratified group cross-validation. J1 multimodal models achieved the highest predictive performance (AUC = 0.923 +/- 0.085), outperforming J0-based configurations (AUC <= 0.86). Incorporating lesion-volume features further improved model stability and interpretability. These findings demonstrate that early post-treatment diffusion MRI provides superior prognostic value to pre-treatment imaging and that combining MRI, clinical, and lesion-volume features produces a robust and interpretable framework for predicting three-month functional outcomes in AIS patients.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Opening the Black Box: Nowcasting Singapore's GDP Growth and its Explainability</title>
<link>https://arxiv.org/abs/2512.02092</link>
<guid>https://arxiv.org/abs/2512.02092</guid>
<content:encoded><![CDATA[
arXiv:2512.02092v1 Announce Type: cross 
Abstract: Timely assessment of current conditions is essential especially for small, open economies such as Singapore, where external shocks transmit rapidly to domestic activity. We develop a real-time nowcasting framework for quarterly GDP growth using a high-dimensional panel of approximately 70 indicators, encompassing economic and financial indicators over 1990Q1-2023Q2. The analysis covers penalized regressions, dimensionality-reduction methods, ensemble learning algorithms, and neural architectures, benchmarked against a Random Walk, an AR(3), and a Dynamic Factor Model. The pipeline preserves temporal ordering through an expanding-window walk-forward design with Bayesian hyperparameter optimization, and uses moving block-bootstrap procedures both to construct prediction intervals and to obtain confidence bands for feature-importance measures. It adopts model-specific and XAI-based explainability tools. A Model Confidence Set procedure identifies statistically superior learners, which are then combined through simple, weighted, and exponentially weighted schemes; the resulting time-varying weights provide an interpretable representation of model contributions. Predictive ability is assessed via Giacomini-White tests. Empirical results show that penalized regressions, dimensionality-reduction models, and GRU networks consistently outperform all benchmarks, with RMSFE reductions of roughly 40-60%; aggregation delivers further gains. Feature-attribution methods highlight industrial production, external trade, and labor-market indicators as dominant drivers of Singapore's short-run growth dynamics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CoatFusion: Controllable Material Coating in Images</title>
<link>https://arxiv.org/abs/2512.02143</link>
<guid>https://arxiv.org/abs/2512.02143</guid>
<content:encoded><![CDATA[
arXiv:2512.02143v1 Announce Type: cross 
Abstract: We introduce Material Coating, a novel image editing task that simulates applying a thin material layer onto an object while preserving its underlying coarse and fine geometry. Material coating is fundamentally different from existing "material transfer" methods, which are designed to replace an object's intrinsic material, often overwriting fine details. To address this new task, we construct a large-scale synthetic dataset (110K images) of 3D objects with varied, physically-based coatings, named DataCoat110K. We then propose CoatFusion, a novel architecture that enables this task by conditioning a diffusion model on both a 2D albedo texture and granular, PBR-style parametric controls, including roughness, metalness, transmission, and a key thickness parameter. Experiments and user studies show CoatFusion produces realistic, controllable coatings and significantly outperforms existing material editing and transfer methods on this new task.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SplatSuRe: Selective Super-Resolution for Multi-view Consistent 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2512.02172</link>
<guid>https://arxiv.org/abs/2512.02172</guid>
<content:encoded><![CDATA[
arXiv:2512.02172v1 Announce Type: cross 
Abstract: 3D Gaussian Splatting (3DGS) enables high-quality novel view synthesis, motivating interest in generating higher-resolution renders than those available during training. A natural strategy is to apply super-resolution (SR) to low-resolution (LR) input views, but independently enhancing each image introduces multi-view inconsistencies, leading to blurry renders. Prior methods attempt to mitigate these inconsistencies through learned neural components, temporally consistent video priors, or joint optimization on LR and SR views, but all uniformly apply SR across every image. In contrast, our key insight is that close-up LR views may contain high-frequency information for regions also captured in more distant views, and that we can use the camera pose relative to scene geometry to inform where to add SR content. Building from this insight, we propose SplatSuRe, a method that selectively applies SR content only in undersampled regions lacking high-frequency supervision, yielding sharper and more consistent results. Across Tanks & Temples, Deep Blending and Mip-NeRF 360, our approach surpasses baselines in both fidelity and perceptual quality. Notably, our gains are most significant in localized foreground regions where higher detail is desired.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling on Metric Graphs</title>
<link>https://arxiv.org/abs/2512.02175</link>
<guid>https://arxiv.org/abs/2512.02175</guid>
<content:encoded><![CDATA[
arXiv:2512.02175v1 Announce Type: cross 
Abstract: Metric graphs are structures obtained by associating edges in a standard graph with segments of the real line and gluing these segments at the vertices of the graph. The resulting structure has a natural metric that allows for the study of differential operators and stochastic processes on the graph. Brownian motions in these domains have been extensively studied theoretically using their generators. However, less work has been done on practical algorithms for simulating these processes. We introduce the first algorithm for simulating Brownian motions on metric graphs through a timestep splitting Euler-Maruyama-based discretization of their corresponding stochastic differential equation. By applying this scheme to Langevin diffusions on metric graphs, we also obtain the first algorithm for sampling on metric graphs. We provide theoretical guarantees on the number of timestep splittings required for the algorithm to converge to the underlying stochastic process. We also show that the exit probabilities of the simulated particle converge to the vertex-edge jump probabilities of the underlying stochastic differential equation as the timestep goes to zero. Finally, since this method is highly parallelizable, we provide fast, memory-aware implementations of our algorithm in the form of custom CUDA kernels that are up to ~8000x faster than a GPU implementation using PyTorch on simple star metric graphs. Beyond simple star graphs, we benchmark our algorithm on a real cortical vascular network extracted from a DuMuX tissue-perfusion model for tracer transport. Our algorithm is able to run stable simulations with timesteps significantly larger than the stable limit of the finite volume method used in DuMuX while also achieving speedups of up to ~1500x.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Think Before You Prune: Self-Reflective Structured Pruning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2512.02185</link>
<guid>https://arxiv.org/abs/2512.02185</guid>
<content:encoded><![CDATA[
arXiv:2512.02185v1 Announce Type: cross 
Abstract: Reasoning LLMs (RLMs) such as OpenAI o1, DeepSeek-R1, and Qwen3 deliver strong multi-step reasoning through chain-of-thought generation, but their large model sizes and lengthy decode-time outputs make them costly to deploy and unsuitable for resource-constrained settings. To reduce computing and memory cost, pruning offers a promising solution by removing unimportant parameters. However, despite their success on standard LLMs, existing pruning methods severely damage RLMs, as even moderate sparsity (e.g., 20%) can collapse accuracy and completely disrupt the model's reasoning coherence. We begin by analyzing why existing pruning pipelines fail on reasoning LLMs and find that their brittleness largely stems from a mismatch between the calibration data, the pruning objective, and the model's decode-time reasoning behavior. Our study further shows that the most reliable calibration signal comes not from human-written labels but from the model's own self-generated reasoning traces, which more accurately reflect its inference distribution. Guided by these insights, we introduce RESP, a self-reflective structured pruning framework that aligns pruning decisions with the model's reasoning dynamics through self-generated calibration, decode-only gradient-based importance estimation, and progressive regeneration that maintains calibration fidelity as sparsity increases. Experiments on Qwen3-8B demonstrate that RESP markedly outperforms existing structured pruning methods on both GSM8K and MathQA, preserving near-dense accuracy at 20-30% sparsity and substantially mitigating performance collapse at higher sparsity levels. At 40% sparsity, RESP attains 81.3% accuracy on GSM8K and 59.6% on MathQA, surpassing the strongest baselines by 66.87% and 47%, respectively.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orchestration Framework for Financial Agents: From Algorithmic Trading to Agentic Trading</title>
<link>https://arxiv.org/abs/2512.02227</link>
<guid>https://arxiv.org/abs/2512.02227</guid>
<content:encoded><![CDATA[
arXiv:2512.02227v1 Announce Type: cross 
Abstract: The financial market is a mission-critical playground for AI agents due to its temporal dynamics and low signal-to-noise ratio. Building an effective algorithmic trading system may require a professional team to develop and test over the years. In this paper, we propose an orchestration framework for financial agents, which aims to democratize financial intelligence to the general public. We map each component of the traditional algorithmic trading system to agents, including planner, orchestrator, alpha agents, risk agents, portfolio agents, backtest agents, execution agents, audit agents, and memory agent. We present two in-house trading examples. For the stock trading task (hourly data from 04/2024 to 12/2024), our approach achieved a return of $20.42\%$, a Sharpe ratio of 2.63, and a maximum drawdown of $-3.59\%$, while the S&amp;P 500 index yielded a return of $15.97\%$. For the BTC trading task (minute data from 27/07/2025 to 13/08/2025), our approach achieved a return of $8.39\%$, a Sharpe ratio of $0.38$, and a maximum drawdown of $-2.80\%$, whereas the BTC price increased by $3.80\%$. Our code is available on \href{https://github.com/Open-Finance-Lab/AgenticTrading}{GitHub}.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>STRIDE: A Systematic Framework for Selecting AI Modalities - Agentic AI, AI Assistants, or LLM Calls</title>
<link>https://arxiv.org/abs/2512.02228</link>
<guid>https://arxiv.org/abs/2512.02228</guid>
<content:encoded><![CDATA[
arXiv:2512.02228v1 Announce Type: cross 
Abstract: The rapid shift from stateless large language models (LLMs) to autonomous, goal-driven agents raises a central question: When is agentic AI truly necessary? While agents enable multi-step reasoning, persistent memory, and tool orchestration, deploying them indiscriminately leads to higher cost, complexity, and risk.
  We present STRIDE (Systematic Task Reasoning Intelligence Deployment Evaluator), a framework that provides principled recommendations for selecting between three modalities: (i) direct LLM calls, (ii) guided AI assistants, and (iii) fully autonomous agentic AI. STRIDE integrates structured task decomposition, dynamism attribution, and self-reflection requirement analysis to produce an Agentic Suitability Score, ensuring that full agentic autonomy is reserved for tasks with inherent dynamism or evolving context.
  Evaluated across 30 real-world tasks spanning SRE, compliance, and enterprise automation, STRIDE achieved 92% accuracy in modality selection, reduced unnecessary agent deployments by 45%, and cut resource costs by 37%. Expert validation over six months in SRE and compliance domains confirmed its practical utility, with domain specialists agreeing that STRIDE effectively distinguishes between tasks requiring simple LLM calls, guided assistants, or full agentic autonomy. This work reframes agent adoption as a necessity-driven design decision, ensuring autonomy is applied only when its benefits justify the costs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>See, Hear, and Understand: Benchmarking Audiovisual Human Speech Understanding in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2512.02231</link>
<guid>https://arxiv.org/abs/2512.02231</guid>
<content:encoded><![CDATA[
arXiv:2512.02231v1 Announce Type: cross 
Abstract: Multimodal large language models (MLLMs) are expected to jointly interpret vision, audio, and language, yet existing video benchmarks rarely assess fine-grained reasoning about human speech. Many tasks remain visually solvable or only coarsely evaluate speech, offering limited insight into whether models can align who speaks, what is said, and when it occurs. We introduce AV-SpeakerBench, a curated benchmark of 3,212 multiple-choice questions focused on speaker-centric audiovisual reasoning in real-world videos. It features: (1) a speaker-centered formulation that treats speakers-not scenes-as the core reasoning unit; (2) fusion-grounded question design embedding audiovisual dependencies into question semantics; and (3) expert-curated annotations ensuring temporal precision and cross-modal validity. Comprehensive evaluations show that the Gemini family consistently outperforms open-source systems, with Gemini 2.5 Pro achieving the best results. Among open models, Qwen3-Omni-30B approaches Gemini 2.0 Flash but remains far behind Gemini 2.5 Pro, primarily due to weaker audiovisual fusion rather than visual perception. We believe AV-SpeakerBench establishes a rigorous foundation for advancing fine-grained audiovisual reasoning in future multimodal systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PhishSnap: Image-Based Phishing Detection Using Perceptual Hashing</title>
<link>https://arxiv.org/abs/2512.02243</link>
<guid>https://arxiv.org/abs/2512.02243</guid>
<content:encoded><![CDATA[
arXiv:2512.02243v1 Announce Type: cross 
Abstract: Phishing remains one of the most prevalent online threats, exploiting human trust to harvest sensitive credentials. Existing URL- and HTML-based detection systems struggle against obfuscation and visual deception. This paper presents \textbf{PhishSnap}, a privacy-preserving, on-device phishing detection system leveraging perceptual hashing (pHash). Implemented as a browser extension, PhishSnap captures webpage screenshots, computes visual hashes, and compares them against legitimate templates to identify visually similar phishing attempts. A \textbf{2024 dataset of 10,000 URLs} (70\%/20\%/10\% train/validation/test) was collected from PhishTank and Netcraft. Due to security takedowns, a subset of phishing pages was unavailable, reducing dataset diversity. The system achieved \textbf{0.79 accuracy}, \textbf{0.76 precision}, and \textbf{0.78 recall}, showing that visual similarity remains a viable anti-phishing measure. The entire inference process occurs locally, ensuring user privacy and minimal latency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Verifying Closed-Loop Contractivity of Learning-Based Controllers via Partitioning</title>
<link>https://arxiv.org/abs/2512.02262</link>
<guid>https://arxiv.org/abs/2512.02262</guid>
<content:encoded><![CDATA[
arXiv:2512.02262v1 Announce Type: cross 
Abstract: We address the problem of verifying closed-loop contraction in nonlinear control systems whose controller and contraction metric are both parameterized by neural networks. By leveraging interval analysis and interval bound propagation, we derive a tractable and scalable sufficient condition for closed-loop contractivity that reduces to checking that the dominant eigenvalue of a symmetric Metzler matrix is nonpositive. We combine this sufficient condition with a domain partitioning strategy to integrate this sufficient condition into training. The proposed approach is validated on an inverted pendulum system, demonstrating the ability to learn neural network controllers and contraction metrics that provably satisfy the contraction condition.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spatiotemporal Pyramid Flow Matching for Climate Emulation</title>
<link>https://arxiv.org/abs/2512.02268</link>
<guid>https://arxiv.org/abs/2512.02268</guid>
<content:encoded><![CDATA[
arXiv:2512.02268v1 Announce Type: cross 
Abstract: Generative models have the potential to transform the way we emulate Earth's changing climate. Previous generative approaches rely on weather-scale autoregression for climate emulation, but this is inherently slow for long climate horizons and has yet to demonstrate stable rollouts under nonstationary forcings. Here, we introduce Spatiotemporal Pyramid Flows (SPF), a new class of flow matching approaches that model data hierarchically across spatial and temporal scales. Inspired by cascaded video models, SPF partitions the generative trajectory into a spatiotemporal pyramid, progressively increasing spatial resolution to reduce computation and coupling each stage with an associated timescale to enable direct sampling at any temporal level in the pyramid. This design, together with conditioning each stage on prescribed physical forcings (e.g., greenhouse gases or aerosols), enables efficient, parallel climate emulation at multiple timescales. On ClimateBench, SPF outperforms strong flow matching baselines and pre-trained models at yearly and monthly timescales while offering fast sampling, especially at coarser temporal levels. To scale SPF, we curate ClimateSuite, the largest collection of Earth system simulations to date, comprising over 33,000 simulation-years across ten climate models and the first dataset to include simulations of climate interventions. We find that the scaled SPF model demonstrates good generalization to held-out scenarios across climate models. Together, SPF and ClimateSuite provide a foundation for accurate, efficient, probabilistic climate emulation across temporal scales and realistic future scenarios. Data and code is publicly available at https://github.com/stanfordmlgroup/spf .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Robustness of Traffic Classification under Resource Constraints: Input Structure Matters</title>
<link>https://arxiv.org/abs/2512.02276</link>
<guid>https://arxiv.org/abs/2512.02276</guid>
<content:encoded><![CDATA[
arXiv:2512.02276v1 Announce Type: cross 
Abstract: Traffic classification (TC) plays a critical role in cybersecurity, particularly in IoT and embedded contexts, where inspection must often occur locally under tight hardware constraints. We use hardware-aware neural architecture search (HW-NAS) to derive lightweight TC models that are accurate, efficient, and deployable on edge platforms. Two input formats are considered: a flattened byte sequence and a 2D packet-wise time series; we examine how input structure affects adversarial vulnerability when using resource-constrained models. Robustness is assessed against white-box attacks, specifically Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). On USTC-TFC2016, both HW-NAS models achieve over 99% clean-data accuracy while remaining within 65k parameters and 2M FLOPs. Yet under perturbations of strength 0.1, their robustness diverges: the flat model retains over 85% accuracy, while the time-series variant drops below 35%. Adversarial fine-tuning delivers robust gains, with flat-input accuracy exceeding 96% and the time-series variant recovering over 60 percentage points in robustness, all without compromising efficiency. The results underscore how input structure influences adversarial vulnerability, and show that even compact, resource-efficient models can attain strong robustness, supporting their practical deployment in secure edge-based TC.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>OmniGuard: Unified Omni-Modal Guardrails with Deliberate Reasoning</title>
<link>https://arxiv.org/abs/2512.02306</link>
<guid>https://arxiv.org/abs/2512.02306</guid>
<content:encoded><![CDATA[
arXiv:2512.02306v1 Announce Type: cross 
Abstract: Omni-modal Large Language Models (OLLMs) that process text, images, videos, and audio introduce new challenges for safety and value guardrails in human-AI interaction. Prior guardrail research largely targets unimodal settings and typically frames safeguarding as binary classification, which limits robustness across diverse modalities and tasks. To address this gap, we propose OmniGuard, the first family of omni-modal guardrails that performs safeguarding across all modalities with deliberate reasoning ability. To support the training of OMNIGUARD, we curate a large, comprehensive omni-modal safety dataset comprising over 210K diverse samples, with inputs that cover all modalities through both unimodal and cross-modal samples. Each sample is annotated with structured safety labels and carefully curated safety critiques from expert models through targeted distillation. Extensive experiments on 15 benchmarks show that OmniGuard achieves strong effectiveness and generalization across a wide range of multimodal safety scenarios. Importantly, OmniGuard provides a unified framework that enforces policies and mitigates risks in omni-modalities, paving the way toward building more robust and capable omnimodal safeguarding systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Few-shot Protein Fitness Prediction via In-context Learning and Test-time Training</title>
<link>https://arxiv.org/abs/2512.02315</link>
<guid>https://arxiv.org/abs/2512.02315</guid>
<content:encoded><![CDATA[
arXiv:2512.02315v1 Announce Type: cross 
Abstract: Accurately predicting protein fitness with minimal experimental data is a persistent challenge in protein engineering. We introduce PRIMO (PRotein In-context Mutation Oracle), a transformer-based framework that leverages in-context learning and test-time training to adapt rapidly to new proteins and assays without large task-specific datasets. By encoding sequence information, auxiliary zero-shot predictions, and sparse experimental labels from many assays as a unified token set in a pre-training masked-language modeling paradigm, PRIMO learns to prioritize promising variants through a preference-based loss function. Across diverse protein families and properties-including both substitution and indel mutations-PRIMO outperforms zero-shot and fully supervised baselines. This work underscores the power of combining large-scale pre-training with efficient test-time adaptation to tackle challenging protein design tasks where data collection is expensive and label availability is limited.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Molecular Embedding-Based Algorithm Selection in Protein-Ligand Docking</title>
<link>https://arxiv.org/abs/2512.02328</link>
<guid>https://arxiv.org/abs/2512.02328</guid>
<content:encoded><![CDATA[
arXiv:2512.02328v1 Announce Type: cross 
Abstract: Selecting an effective docking algorithm is highly context-dependent, and no single method performs reliably across structural, chemical, or protocol regimes. We introduce MolAS, a lightweight algorithm selection system that predicts per-algorithm performance from pretrained protein-ligand embeddings using attentional pooling and a shallow residual decoder. With only hundreds to a few thousand labelled complexes, MolAS achieves up to 15% absolute improvement over the single-best solver (SBS) and closes 17-66% of the Virtual Best Solver (VBS)-SBS gap across five diverse docking benchmarks. Analyses of reliability, embedding geometry, and solver-selection patterns show that MolAS succeeds when the oracle landscape exhibits low entropy and separable solver behaviour, but collapses under protocol-induced hierarchy shifts. These findings indicate that the main barrier to robust docking AS is not representational capacity but instability in solver rankings across pose-generation regimes, positioning MolAS as both a practical in-domain selector and a diagnostic tool for assessing when AS is feasible.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safeguarded Stochastic Polyak Step Sizes for Non-smooth Optimization: Robust Performance Without Small (Sub)Gradients</title>
<link>https://arxiv.org/abs/2512.02342</link>
<guid>https://arxiv.org/abs/2512.02342</guid>
<content:encoded><![CDATA[
arXiv:2512.02342v1 Announce Type: cross 
Abstract: The stochastic Polyak step size (SPS) has proven to be a promising choice for stochastic gradient descent (SGD), delivering competitive performance relative to state-of-the-art methods on smooth convex and non-convex optimization problems, including deep neural network training. However, extensions of this approach to non-smooth settings remain in their early stages, often relying on interpolation assumptions or requiring knowledge of the optimal solution. In this work, we propose a novel SPS variant, Safeguarded SPS (SPS$_{safe}$), for the stochastic subgradient method, and provide rigorous convergence guarantees for non-smooth convex optimization with no need for strong assumptions. We further incorporate momentum into the update rule, yielding equally tight theoretical results. Comprehensive experiments on convex benchmarks and deep neural networks corroborate our theory: the proposed step size accelerates convergence, reduces variance, and consistently outperforms existing adaptive baselines. Finally, in the context of deep neural network training, our method demonstrates robust performance by addressing the vanishing gradient problem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Error Injection Fails to Elicit Self-Correction In Language Models</title>
<link>https://arxiv.org/abs/2512.02389</link>
<guid>https://arxiv.org/abs/2512.02389</guid>
<content:encoded><![CDATA[
arXiv:2512.02389v1 Announce Type: cross 
Abstract: Reinforcement learning has become the dominant paradigm for eliciting reasoning and self-correction capabilities in large language models, but its computational expense motivates exploration of alternatives. Inspired by techniques from autonomous driving and robotics, we investigate whether supervised learning with synthetic error injection can induce self-correction abilities in language models. Our approach inserts artificial errors into reasoning chains, masks them, and supervises the model to recognize and correct these mistakes. Despite the intuitive appeal of this method, we find that it fails to significantly improve performance even on simple synthetic tasks across multiple models. Moreover, even when the model catches its own error, it often parrots the original mistake. We find that the distribution shift of synthetic errors to on-policy errors significantly degrades the error-correction capabilities of the fine-tuned model, even with good synthetic coverage of on-policy errors. Our results help explain why on-policy reinforcement learning methods have proven uniquely effective for eliciting self-correction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WISE: Weighted Iterative Society-of-Experts for Robust Multimodal Multi-Agent Debate</title>
<link>https://arxiv.org/abs/2512.02405</link>
<guid>https://arxiv.org/abs/2512.02405</guid>
<content:encoded><![CDATA[
arXiv:2512.02405v1 Announce Type: cross 
Abstract: Recent large language models (LLMs) are trained on diverse corpora and tasks, leading them to develop complementary strengths. Multi-agent debate (MAD) has emerged as a popular way to leverage these strengths for robust reasoning, though it has mostly been applied to language-only tasks, leaving its efficacy on multimodal problems underexplored. In this paper, we study MAD for solving vision-and-language reasoning problems. Our setup enables generalizing the debate protocol with heterogeneous experts that possess single- and multi-modal capabilities. To this end, we present Weighted Iterative Society-of-Experts (WISE), a generalized and modular MAD framework that partitions the agents into Solvers, that generate solutions, and Reflectors, that verify correctness, assign weights, and provide natural language feedback. To aggregate the agents' solutions across debate rounds, while accounting for variance in their responses and the feedback weights, we present a modified Dawid-Skene algorithm for post-processing that integrates our two-stage debate model. We evaluate WISE on SMART-840, VisualPuzzles, EvoChart-QA, and a new SMART-840++ dataset with programmatically generated problem instances of controlled difficulty. Our results show that WISE consistently improves accuracy by 2-7% over the state-of-the-art MAD setups and aggregation methods across diverse multimodal tasks and LLM configurations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging Large Language Models to Bridge On-chain and Off-chain Transparency in Stablecoins</title>
<link>https://arxiv.org/abs/2512.02418</link>
<guid>https://arxiv.org/abs/2512.02418</guid>
<content:encoded><![CDATA[
arXiv:2512.02418v1 Announce Type: cross 
Abstract: Stablecoins such as USDT and USDC aspire to peg stability by coupling issuance controls with reserve attestations. In practice, however, the transparency is split across two worlds: verifiable on-chain traces and off-chain disclosures locked in unstructured text that are unconnected. We introduce a large language model (LLM)-based automated framework that bridges these two dimensions by aligning on-chain issuance data with off-chain disclosure statements. First, we propose an integrative framework using LLMs to capture and analyze on- and off-chain data through document parsing and semantic alignment, extracting key financial indicators from issuer attestations and mapping them to corresponding on-chain metrics. Second, we integrate multi-chain issuance records and disclosure documents within a model context protocol (MCP) framework that standardizes LLMs access to both quantitative market data and qualitative disclosure narratives. This framework enables unified retrieval and contextual alignment across heterogeneous stablecoin information sources and facilitates consistent analysis. Third, we demonstrate the capability of LLMs to operate across heterogeneous data modalities in blockchain analytics, quantifying discrepancies between reported and observed circulation and examining their implications for cross-chain transparency and price dynamics. Our findings reveal systematic gaps between disclosed and verifiable data, showing that LLM-assisted analysis enhances cross-modal transparency and supports automated, data-driven auditing in decentralized finance (DeFi).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Quantum feature encoding optimization</title>
<link>https://arxiv.org/abs/2512.02422</link>
<guid>https://arxiv.org/abs/2512.02422</guid>
<content:encoded><![CDATA[
arXiv:2512.02422v1 Announce Type: cross 
Abstract: Quantum Machine Learning (QML) holds the promise of enhancing machine learning modeling in terms of both complexity and accuracy. A key challenge in this domain is the encoding of input data, which plays a pivotal role in determining the performance of QML models. In this work, we tackle a largely unaddressed aspect of encoding that is unique to QML modeling -- rather than adjusting the ansatz used for encoding, we consider adjusting how data is conveyed to the ansatz. We specifically implement QML pipelines that leverage classical data manipulation (i.e., ordering, selecting, and weighting features) as a preprocessing step, and evaluate if these aspects of encoding can have a significant impact on QML model performance, and if they can be effectively optimized to improve performance. Our experimental results, applied across a wide variety of data sets, ansatz, and circuit sizes, with a representative QML approach, demonstrate that by optimizing how features are encoded in an ansatz we can substantially and consistently improve the performance of QML models, making a compelling case for integrating these techniques in future QML applications. Finally we demonstrate the practical feasibility of this approach by running it using real quantum hardware with 100 qubit circuits and successfully achieving improved QML modeling performance in this case as well.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldMM: Dynamic Multimodal Memory Agent for Long Video Reasoning</title>
<link>https://arxiv.org/abs/2512.02425</link>
<guid>https://arxiv.org/abs/2512.02425</guid>
<content:encoded><![CDATA[
arXiv:2512.02425v1 Announce Type: cross 
Abstract: Recent advances in video large language models have demonstrated strong capabilities in understanding short clips. However, scaling them to hours- or days-long videos remains highly challenging due to limited context capacity and the loss of critical visual details during abstraction. Existing memory-augmented methods mitigate this by leveraging textual summaries of video segments, yet they heavily rely on text and fail to utilize visual evidence when reasoning over complex scenes. Moreover, retrieving from fixed temporal scales further limits their flexibility in capturing events that span variable durations. To address this, we introduce WorldMM, a novel multimodal memory agent that constructs and retrieves from multiple complementary memories, encompassing both textual and visual representations. WorldMM comprises three types of memory: episodic memory indexes factual events across multiple temporal scales, semantic memory continuously updates high-level conceptual knowledge, and visual memory preserves detailed information about scenes. During inference, an adaptive retrieval agent iteratively selects the most relevant memory source and leverages multiple temporal granularities based on the query, continuing until it determines that sufficient information has been gathered. WorldMM significantly outperforms existing baselines across five long video question-answering benchmarks, achieving an average 8.4% performance gain over previous state-of-the-art methods, showing its effectiveness on long video reasoning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Basis-Oriented Low-rank Transfer for Few-Shot and Test-Time Adaptation</title>
<link>https://arxiv.org/abs/2512.02441</link>
<guid>https://arxiv.org/abs/2512.02441</guid>
<content:encoded><![CDATA[
arXiv:2512.02441v1 Announce Type: cross 
Abstract: Adapting large pre-trained models to unseen tasks under tight data and compute budgets remains challenging. Meta-learning approaches explicitly learn good initializations, but they require an additional meta-training phase over many tasks, incur high training cost, and can be unstable. At the same time, the number of task-specific pre-trained models continues to grow, yet the question of how to transfer them to new tasks with minimal additional training remains relatively underexplored. We propose BOLT (Basis-Oriented Low-rank Transfer), a framework that reuses existing fine-tuned models not by merging weights, but instead by extracting an orthogonal, task-informed spectral basis and adapting within that subspace. In the offline phase, BOLT collects dominant singular directions from multiple task vectors and orthogonalizes them per layer to form reusable bases. In the online phase, we freeze these bases and train only a small set of diagonal coefficients per layer for the new task, yielding a rank-controlled update with very few trainable parameters. This design provides (i) a strong, training-free initialization for unseen tasks, obtained by pooling source-task coefficients, along with a lightweight rescaling step while leveraging the shared orthogonal bases, and (ii) a parameter-efficient fine-tuning (PEFT) path that, in our experiments, achieves robust performance compared to common PEFT baselines as well as a representative meta-learned initialization. Our results show that constraining adaptation to a task-informed orthogonal subspace provides an effective alternative for unseen-task transfer.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>QJoin: Transformation-aware Joinable Data Discovery Using Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.02444</link>
<guid>https://arxiv.org/abs/2512.02444</guid>
<content:encoded><![CDATA[
arXiv:2512.02444v1 Announce Type: cross 
Abstract: Discovering which tables in large, heterogeneous repositories can be joined and by what transformations is a central challenge in data integration and data discovery. Traditional join discovery methods are largely designed for equi-joins, which assume that join keys match exactly or nearly so. These techniques, while efficient in clean, well-normalized databases, fail in open or federated settings where identifiers are inconsistently formatted, embedded, or split across multiple columns. Approximate or fuzzy joins alleviate minor string variations but cannot capture systematic transformations. We introduce QJoin, a reinforcement-learning framework that learns and reuses transformation strategies across join tasks. QJoin trains an agent under a uniqueness-aware reward that balances similarity with key distinctiveness, enabling it to explore concise, high-value transformation chains. To accelerate new joins, we introduce two reuse mechanisms: (i) agent transfer, which initializes new policies from pretrained agents, and (ii) transformation reuse, which caches successful operator sequences for similar column clusters. On the AutoJoin Web benchmark (31 table pairs), QJoin achieves an average F1-score of 91.0%. For 19,990 join tasks in NYC+Chicago open datasets, Qjoin reduces runtime by up to 7.4% (13,747 s) by using reusing. These results demonstrate that transformation learning and reuse can make join discovery both more accurate and more efficient.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Guided Self-Evolving LLMs with Minimal Human Supervision</title>
<link>https://arxiv.org/abs/2512.02472</link>
<guid>https://arxiv.org/abs/2512.02472</guid>
<content:encoded><![CDATA[
arXiv:2512.02472v1 Announce Type: cross 
Abstract: AI self-evolution has long been envisioned as a path toward superintelligence, where models autonomously acquire, refine, and internalize knowledge from their own learning experiences. Yet in practice, unguided self-evolving systems often plateau quickly or even degrade as training progresses. These failures arise from issues such as concept drift, diversity collapse, and mis-evolution, as models reinforce their own biases and converge toward low-entropy behaviors. To enable models to self-evolve in a stable and controllable manner while minimizing reliance on human supervision, we introduce R-Few, a guided Self-Play Challenger-Solver framework that incorporates lightweight human oversight through in-context grounding and mixed training. At each iteration, the Challenger samples a small set of human-labeled examples to guide synthetic question generation, while the Solver jointly trains on human and synthetic examples under an online, difficulty-based curriculum. Across math and general reasoning benchmarks, R-Few achieves consistent and iterative improvements. For example, Qwen3-8B-Base improves by +3.0 points over R-Zero on math tasks and achieves performance on par with General-Reasoner, despite the latter being trained on 20 times more human data. Ablation studies confirm the complementary contributions of grounded challenger training and curriculum-based solver training, and further analysis shows that R-Few mitigates drift, yielding more stable and controllable co-evolutionary dynamics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WorldPack: Compressed Memory Improves Spatial Consistency in Video World Modeling</title>
<link>https://arxiv.org/abs/2512.02473</link>
<guid>https://arxiv.org/abs/2512.02473</guid>
<content:encoded><![CDATA[
arXiv:2512.02473v1 Announce Type: cross 
Abstract: Video world models have attracted significant attention for their ability to produce high-fidelity future visual observations conditioned on past observations and navigation actions. Temporally- and spatially-consistent, long-term world modeling has been a long-standing problem, unresolved with even recent state-of-the-art models, due to the prohibitively expensive computational costs for long-context inputs. In this paper, we propose WorldPack, a video world model with efficient compressed memory, which significantly improves spatial consistency, fidelity, and quality in long-term generation despite much shorter context length. Our compressed memory consists of trajectory packing and memory retrieval; trajectory packing realizes high context efficiency, and memory retrieval maintains the consistency in rollouts and helps long-term generations that require spatial reasoning. Our performance is evaluated with LoopNav, a benchmark on Minecraft, specialized for the evaluation of long-term consistency, and we verify that WorldPack notably outperforms strong state-of-the-art models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Stress-Testing Causal Claims via Cardinality Repairs</title>
<link>https://arxiv.org/abs/2512.02491</link>
<guid>https://arxiv.org/abs/2512.02491</guid>
<content:encoded><![CDATA[
arXiv:2512.02491v1 Announce Type: cross 
Abstract: Causal analyses derived from observational data underpin high-stakes decisions in domains such as healthcare, public policy, and economics. Yet such conclusions can be surprisingly fragile: even minor data errors - duplicate records, or entry mistakes - may drastically alter causal relationships. This raises a fundamental question: how robust is a causal claim to small, targeted modifications in the data? Addressing this question is essential for ensuring the reliability, interpretability, and reproducibility of empirical findings. We introduce SubCure, a framework for robustness auditing via cardinality repairs. Given a causal query and a user-specified target range for the estimated effect, SubCure identifies a small set of tuples or subpopulations whose removal shifts the estimate into the desired range. This process not only quantifies the sensitivity of causal conclusions but also pinpoints the specific regions of the data that drive those conclusions. We formalize this problem under both tuple- and pattern-level deletion settings and show both are NP-complete. To scale to large datasets, we develop efficient algorithms that incorporate machine unlearning techniques to incrementally update causal estimates without retraining from scratch. We evaluate SubCure across four real-world datasets covering diverse application domains. In each case, it uncovers compact, high-impact subsets whose removal significantly shifts the causal conclusions, revealing vulnerabilities that traditional methods fail to detect. Our results demonstrate that cardinality repair is a powerful and general-purpose tool for stress-testing causal analyses and guarding against misleading claims rooted in ordinary data imperfections.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian Physics-Informed Neural Networks for Inverse Problems (BPINN-IP): Application in Infrared Image Processing</title>
<link>https://arxiv.org/abs/2512.02495</link>
<guid>https://arxiv.org/abs/2512.02495</guid>
<content:encoded><![CDATA[
arXiv:2512.02495v1 Announce Type: cross 
Abstract: Inverse problems arise across scientific and engineering domains, where the goal is to infer hidden parameters or physical fields from indirect and noisy observations. Classical approaches, such as variational regularization and Bayesian inference, provide well established theoretical foundations for handling ill posedness. However, these methods often become computationally restrictive in high dimensional settings or when the forward model is governed by complex physics. Physics Informed Neural Networks (PINNs) have recently emerged as a promising framework for solving inverse problems by embedding physical laws directly into the training process of neural networks. In this paper, we introduce a new perspective on the Bayesian Physics Informed Neural Network (BPINN) framework, extending classical PINNs by explicitly incorporating training data generation, modeling and measurement uncertainties through Bayesian prior modeling and doing inference with the posterior laws. Also, as we focus on the inverse problems, we call this method BPINN-IP, and we show that the standard PINN formulation naturally appears as its special case corresponding to the Maximum A Posteriori (MAP) estimate. This unified formulation allows simultaneous exploitation of physical constraints, prior knowledge, and data-driven inference, while enabling uncertainty quantification through posterior distributions. To demonstrate the effectiveness of the proposed framework, we consider inverse problems arising in infrared image processing, including deconvolution and super-resolution, and present results on both simulated and real industrial data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative Multi-modal Feedback for Singing Voice Synthesis Evaluation</title>
<link>https://arxiv.org/abs/2512.02523</link>
<guid>https://arxiv.org/abs/2512.02523</guid>
<content:encoded><![CDATA[
arXiv:2512.02523v1 Announce Type: cross 
Abstract: Singing voice synthesis (SVS) has advanced significantly, enabling models to generate vocals with accurate pitch and consistent style. As these capabilities improve, the need for reliable evaluation and optimization becomes increasingly critical. However, current methods like reward systems often rely on single numerical scores, struggle to capture various dimensions such as phrasing or expressiveness, and require costly annotations, limiting interpretability and generalization. To address these issues, we propose a generative feedback (i.e., reward model) framework that provides multi-dimensional language and audio feedback for SVS assessment. Our approach leverages an audio-language model to generate text and audio critiques-covering aspects such as melody, content, and auditory quality. The model is fine-tuned on a hybrid dataset combining human music reactions and synthetic critiques from a MLLMs, enhancing diversity and linguistic richness. Quantitative experiments validate the effectiveness of the proposed dataset and training strategy, demonstrating that the framework produces musically accurate and interpretable evaluations suitable for guiding generative model improvement. The code is at [https://github.com/opendilab/VocalCritic](https://github.com/opendilab/VocalCritic)
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Concise Review of Hallucinations in LLMs and their Mitigation</title>
<link>https://arxiv.org/abs/2512.02527</link>
<guid>https://arxiv.org/abs/2512.02527</guid>
<content:encoded><![CDATA[
arXiv:2512.02527v1 Announce Type: cross 
Abstract: Traditional language models face a challenge from hallucinations. Their very presence casts a large, dangerous shadow over the promising realm of natural language processing. It becomes crucial to understand the various kinds of hallucinations that occur nowadays, their origins, and ways of reducing them. This document provides a concise and straightforward summary of that. It serves as a one-stop resource for a general understanding of hallucinations and how to mitigate them.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Laplace Approximation For Tensor Train Kernel Machines In System Identification</title>
<link>https://arxiv.org/abs/2512.02532</link>
<guid>https://arxiv.org/abs/2512.02532</guid>
<content:encoded><![CDATA[
arXiv:2512.02532v1 Announce Type: cross 
Abstract: To address the scalability limitations of Gaussian process (GP) regression, several approximation techniques have been proposed. One such method is based on tensor networks, which utilizes an exponential number of basis functions without incurring exponential computational cost. However, extending this model to a fully probabilistic formulation introduces several design challenges. In particular, for tensor train (TT) models, it is unclear which TT-core should be treated in a Bayesian manner. We introduce a Bayesian tensor train kernel machine that applies Laplace approximation to estimate the posterior distribution over a selected TT-core and employs variational inference (VI) for precision hyperparameters. Experiments show that core selection is largely independent of TT-ranks and feature structure, and that VI replaces cross-validation while offering up to 65x faster training. The method's effectiveness is demonstrated on an inverse dynamics problem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Computations in Deep Learning Inference</title>
<link>https://arxiv.org/abs/2512.02550</link>
<guid>https://arxiv.org/abs/2512.02550</guid>
<content:encoded><![CDATA[
arXiv:2512.02550v1 Announce Type: cross 
Abstract: The computational demands of modern Deep Neural Networks (DNNs) are immense and constantly growing. While training costs usually capture public attention, inference demands are also contributing in significant computational, energy and environmental footprints. Sparsity stands out as a critical mechanism for drastically reducing these resource demands. However, its potential remains largely untapped and is not yet fully incorporated in production AI systems. To bridge this gap, this work provides the necessary knowledge and insights for performance engineers keen to get involved in deep learning inference optimization. In particular, in this work we: a) discuss the various forms of sparsity that can be utilized in DNN inference, b) explain how the original dense computations translate to sparse kernels, c) provide an extensive bibliographic review of the state-of-the-art in the implementation of these kernels for CPUs and GPUs, d) discuss the availability of sparse datasets in support of sparsity-related research and development, e) explore the current software tools and frameworks that provide robust sparsity support, and f) present evaluation results of different implementations of the key SpMM and SDDMM kernels on CPU and GPU platforms. Ultimately, this paper aims to serve as a resource for performance engineers seeking to develop and deploy highly efficient sparse deep learning models in productions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Zero-Shot Instruction Following in RL via Structured LTL Representations</title>
<link>https://arxiv.org/abs/2512.02633</link>
<guid>https://arxiv.org/abs/2512.02633</guid>
<content:encoded><![CDATA[
arXiv:2512.02633v1 Announce Type: cross 
Abstract: Linear temporal logic (LTL) is a compelling framework for specifying complex, structured tasks for reinforcement learning (RL) agents. Recent work has shown that interpreting LTL instructions as finite automata, which can be seen as high-level programs monitoring task progress, enables learning a single generalist policy capable of executing arbitrary instructions at test time. However, existing approaches fall short in environments where multiple high-level events (i.e., atomic propositions) can be true at the same time and potentially interact in complicated ways. In this work, we propose a novel approach to learning a multi-task policy for following arbitrary LTL instructions that addresses this shortcoming. Our method conditions the policy on sequences of simple Boolean formulae, which directly align with transitions in the automaton, and are encoded via a graph neural network (GNN) to yield structured task representations. Experiments in a complex chess-based environment demonstrate the advantages of our approach.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hear What Matters! Text-conditioned Selective Video-to-Audio Generation</title>
<link>https://arxiv.org/abs/2512.02650</link>
<guid>https://arxiv.org/abs/2512.02650</guid>
<content:encoded><![CDATA[
arXiv:2512.02650v1 Announce Type: cross 
Abstract: This work introduces a new task, text-conditioned selective video-to-audio (V2A) generation, which produces only the user-intended sound from a multi-object video. This capability is especially crucial in multimedia production, where audio tracks are handled individually for each sound source for precise editing, mixing, and creative control. However, current approaches generate single source-mixed sounds at once, largely because visual features are entangled, and region cues or prompts often fail to specify the source. We propose SelVA, a novel text-conditioned V2A model that treats the text prompt as an explicit selector of target source and modulates video encoder to distinctly extract prompt-relevant video features. The proposed supplementary tokens promote cross-attention by suppressing text-irrelevant activations with efficient parameter tuning, yielding robust semantic and temporal grounding. SelVA further employs a self-augmentation scheme to overcome the lack of mono audio track supervision. We evaluate SelVA on VGG-MONOAUDIO, a curated benchmark of clean single-source videos for such a task. Extensive experiments and ablations consistently verify its effectiveness across audio quality, semantic alignment, and temporal synchronization. Code and demo are available at https://jnwnlee.github.io/selva-demo/.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SAND Challenge: Four Approaches for Dysartria Severity Classification</title>
<link>https://arxiv.org/abs/2512.02669</link>
<guid>https://arxiv.org/abs/2512.02669</guid>
<content:encoded><![CDATA[
arXiv:2512.02669v1 Announce Type: cross 
Abstract: This paper presents a unified study of four distinct modeling approaches for classifying dysarthria severity in the Speech Analysis for Neurodegenerative Diseases (SAND) challenge. All models tackle the same five class classification task using a common dataset of speech recordings. We investigate: (1) a ViT-OF method leveraging a Vision Transformer on spectrogram images, (2) a 1D-CNN approach using eight 1-D CNN's with majority-vote fusion, (3) a BiLSTM-OF approach using nine BiLSTM models with majority vote fusion, and (4) a Hierarchical XGBoost ensemble that combines glottal and formant features through a two stage learning framework. Each method is described, and their performances on a validation set of 53 speakers are compared. Results show that while the feature-engineered XGBoost ensemble achieves the highest macro-F1 (0.86), the deep learning models (ViT, CNN, BiLSTM) attain competitive F1-scores (0.70) and offer complementary insights into the problem.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Embedding networks with the random walk first return time distribution</title>
<link>https://arxiv.org/abs/2512.02694</link>
<guid>https://arxiv.org/abs/2512.02694</guid>
<content:encoded><![CDATA[
arXiv:2512.02694v1 Announce Type: cross 
Abstract: We propose the first return time distribution (FRTD) of a random walk as an interpretable and mathematically grounded node embedding. The FRTD assigns a probability mass function to each node, allowing us to define a distance between any pair of nodes using standard metrics for discrete distributions. We present several arguments to motivate the FRTD embedding. First, we show that FRTDs are strictly more informative than eigenvalue spectra, yet insufficient for complete graph identification, thus placing FRTD equivalence between cospectrality and isomorphism. Second, we argue that FRTD equivalence between nodes captures structural similarity. Third, we empirically demonstrate that the FRTD embedding outperforms manually designed graph metrics in network alignment tasks. Finally, we show that random networks that approximately match the FRTD of a desired target also preserve other salient features. Together these results demonstrate the FRTD as a simple and mathematically principled embedding for complex networks.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ALDI-ray: Adapting the ALDI Framework for Security X-ray Object Detection</title>
<link>https://arxiv.org/abs/2512.02696</link>
<guid>https://arxiv.org/abs/2512.02696</guid>
<content:encoded><![CDATA[
arXiv:2512.02696v1 Announce Type: cross 
Abstract: Domain adaptation in object detection is critical for real-world applications where distribution shifts degrade model performance. Security X-ray imaging presents a unique challenge due to variations in scanning devices and environmental conditions, leading to significant domain discrepancies. To address this, we apply ALDI++, a domain adaptation framework that integrates self-distillation, feature alignment, and enhanced training strategies to mitigate domain shift effectively in this area. We conduct extensive experiments on the EDS dataset, demonstrating that ALDI++ surpasses the state-of-the-art (SOTA) domain adaptation methods across multiple adaptation scenarios. In particular, ALDI++ with a Vision Transformer for Detection (ViTDet) backbone achieves the highest mean average precision (mAP), confirming the effectiveness of transformer-based architectures for cross-domain object detection. Additionally, our category-wise analysis highlights consistent improvements in detection accuracy, reinforcing the robustness of the model across diverse object classes. Our findings establish ALDI++ as an efficient solution for domain-adaptive object detection, setting a new benchmark for performance stability and cross-domain generalization in security X-ray imagery.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM-Pruner: Buffering for Spatial Sparsity in an Efficient VLM Centrifugal Token Pruning Paradigm</title>
<link>https://arxiv.org/abs/2512.02700</link>
<guid>https://arxiv.org/abs/2512.02700</guid>
<content:encoded><![CDATA[
arXiv:2512.02700v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) excel at image understanding tasks, but the large number of visual tokens imposes significant computational costs, hindering deployment on mobile devices. Many pruning methods rely solely on token importance and thus overlook inter-token redundancy, retaining numerous duplicated tokens and wasting capacity. Although some redundancy-aware approaches have been proposed, they often ignore the spatial relationships among visual tokens. This can lead to overly sparse selections of retained tokens that fail to adequately cover the regions of target objects. To address these limitations, we propose VLM-Pruner, a training-free token pruning algorithm that explicitly balances redundancy and spatial sparsity. We introduce a centrifugal token pruning paradigm that enables near-to-far selection while prioritizing the preservation of fine-grained object details. Moreover, we design a Buffering for Spatial Sparsity (BSS) criterion that defers the selection of spatially distant tokens. We further adopt a parallel greedy strategy to conduct token selection efficiently. To mitigate information loss from pruning, we selectively fuse salient information from the discarded tokens into the retained ones. Comprehensive comparisons demonstrate that VLM-Pruner consistently outperforms strong baselines across five VLMs with an 88.9\% pruning rate, while delivering an end-to-end inference speedup.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CREST: Universal Safety Guardrails Through Cluster-Guided Cross-Lingual Transfer</title>
<link>https://arxiv.org/abs/2512.02711</link>
<guid>https://arxiv.org/abs/2512.02711</guid>
<content:encoded><![CDATA[
arXiv:2512.02711v1 Announce Type: cross 
Abstract: Ensuring content safety in large language models (LLMs) is essential for their deployment in real-world applications. However, existing safety guardrails are predominantly tailored for high-resource languages, leaving a significant portion of the world's population underrepresented who communicate in low-resource languages. To address this, we introduce CREST (CRoss-lingual Efficient Safety Transfer), a parameter-efficient multilingual safety classification model that supports 100 languages with only 0.5B parameters. By training on a strategically chosen subset of only 13 high-resource languages, our model utilizes cluster-based cross-lingual transfer from a few to 100 languages, enabling effective generalization to both unseen high-resource and low-resource languages. This approach addresses the challenge of limited training data in low-resource settings. We conduct comprehensive evaluations across six safety benchmarks to demonstrate that CREST outperforms existing state-of-the-art guardrails of comparable scale and achieves competitive results against models with significantly larger parameter counts (2.5B parameters and above). Our findings highlight the limitations of language-specific guardrails and underscore the importance of developing universal, language-agnostic safety systems that can scale effectively to serve global populations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Bayesian Behaviour and Optimal Cue Combination in LLMs</title>
<link>https://arxiv.org/abs/2512.02719</link>
<guid>https://arxiv.org/abs/2512.02719</guid>
<content:encoded><![CDATA[
arXiv:2512.02719v1 Announce Type: cross 
Abstract: Large language models (LLMs) excel at explicit reasoning, but their implicit computational strategies remain underexplored. Decades of psychophysics research show that humans intuitively process and integrate noisy signals using near-optimal Bayesian strategies in perceptual tasks. We ask whether LLMs exhibit similar behaviour and perform optimal multimodal integration without explicit training or instruction. Adopting the psychophysics paradigm, we infer computational principles of LLMs from systematic behavioural studies. We introduce a behavioural benchmark - BayesBench: four magnitude estimation tasks (length, location, distance, and duration) over text and image, inspired by classic psychophysics, and evaluate a diverse set of nine LLMs alongside human judgments for calibration. Through controlled ablations of noise, context, and instruction prompts, we measure performance, behaviour and efficiency in multimodal cue-combination. Beyond accuracy and efficiency metrics, we introduce a Bayesian Consistency Score that detects Bayes-consistent behavioural shifts even when accuracy saturates. Our results show that while capable models often adapt in Bayes-consistent ways, accuracy does not guarantee robustness. Notably, GPT-5 Mini achieves perfect text accuracy but fails to integrate visual cues efficiently. This reveals a critical dissociation between capability and strategy, suggesting accuracy-centric benchmarks may over-index on performance while missing brittle uncertainty handling. These findings reveal emergent principled handling of uncertainty and highlight the correlation between accuracy and Bayesian tendencies. We release our psychophysics benchmark and consistency metric (https://bayes-bench.github.io) as evaluation tools and to inform future multimodal architecture designs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>StockMem: An Event-Reflection Memory Framework for Stock Forecasting</title>
<link>https://arxiv.org/abs/2512.02720</link>
<guid>https://arxiv.org/abs/2512.02720</guid>
<content:encoded><![CDATA[
arXiv:2512.02720v1 Announce Type: cross 
Abstract: Stock price prediction is challenging due to market volatility and its sensitivity to real-time events. While large language models (LLMs) offer new avenues for text-based forecasting, their application in finance is hindered by noisy news data and the lack of explicit answers in text. General-purpose memory architectures struggle to identify the key drivers of price movements. To address this, we propose StockMem, an event-reflection dual-layer memory framework. It structures news into events and mines them along two dimensions: horizontal consolidation integrates daily events, while longitudinal tracking captures event evolution to extract incremental information reflecting market expectation discrepancies. This builds a temporal event knowledge base. By analyzing event-price dynamics, the framework further forms a reflection knowledge base of causal experiences. For prediction, it retrieves analogous historical scenarios and reasons with current events, incremental data, and past experiences. Experiments show StockMem outperforms existing memory architectures and provides superior, explainable reasoning by tracing the information chain affecting prices, enhancing decision transparency in financial forecasting.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative modeling using evolved quantum Boltzmann machines</title>
<link>https://arxiv.org/abs/2512.02721</link>
<guid>https://arxiv.org/abs/2512.02721</guid>
<content:encoded><![CDATA[
arXiv:2512.02721v1 Announce Type: cross 
Abstract: Born-rule generative modeling, a central task in quantum machine learning, seeks to learn probability distributions that can be efficiently sampled by measuring complex quantum states. One hope is for quantum models to efficiently capture probability distributions that are difficult to learn and simulate by classical means alone. Quantum Boltzmann machines were proposed about one decade ago for this purpose, yet efficient training methods have remained elusive. In this paper, I overcome this obstacle by proposing a practical solution that trains quantum Boltzmann machines for Born-rule generative modeling. Two key ingredients in the proposal are the Donsker-Varadhan variational representation of the classical relative entropy and the quantum Boltzmann gradient estimator of [Patel et al., arXiv:2410.12935]. I present the main result for a more general ansatz known as an evolved quantum Boltzmann machine [Minervini et al., arXiv:2501.03367], which combines parameterized real- and imaginary-time evolution. I also show how to extend the findings to other distinguishability measures beyond relative entropy. Finally, I present four different hybrid quantum-classical algorithms for the minimax optimization underlying training, and I discuss their theoretical convergence guarantees.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DF-Mamba: Deformable State Space Modeling for 3D Hand Pose Estimation in Interactions</title>
<link>https://arxiv.org/abs/2512.02727</link>
<guid>https://arxiv.org/abs/2512.02727</guid>
<content:encoded><![CDATA[
arXiv:2512.02727v1 Announce Type: cross 
Abstract: Modeling daily hand interactions often struggles with severe occlusions, such as when two hands overlap, which highlights the need for robust feature learning in 3D hand pose estimation (HPE). To handle such occluded hand images, it is vital to effectively learn the relationship between local image features (e.g., for occluded joints) and global context (e.g., cues from inter-joints, inter-hands, or the scene). However, most current 3D HPE methods still rely on ResNet for feature extraction, and such CNN's inductive bias may not be optimal for 3D HPE due to its limited capability to model the global context. To address this limitation, we propose an effective and efficient framework for visual feature extraction in 3D HPE using recent state space modeling (i.e., Mamba), dubbed Deformable Mamba (DF-Mamba). DF-Mamba is designed to capture global context cues beyond standard convolution through Mamba's selective state modeling and the proposed deformable state scanning. Specifically, for local features after convolution, our deformable scanning aggregates these features within an image while selectively preserving useful cues that represent the global context. This approach significantly improves the accuracy of structured 3D HPE, with comparable inference speed to ResNet-50. Our experiments involve extensive evaluations on five divergent datasets including single-hand and two-hand scenarios, hand-only and hand-object interactions, as well as RGB and depth-based estimation. DF-Mamba outperforms the latest image backbones, including VMamba and Spatial-Mamba, on all datasets and achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Improving AI Agents through Self-Play</title>
<link>https://arxiv.org/abs/2512.02731</link>
<guid>https://arxiv.org/abs/2512.02731</guid>
<content:encoded><![CDATA[
arXiv:2512.02731v1 Announce Type: cross 
Abstract: We extend the moduli-theoretic framework of psychometric batteries to the domain of dynamical systems. While previous work established the AAI capability score as a static functional on the space of agent representations, this paper formalizes the agent as a flow $\nu_r$ parameterized by computational resource $r$, governed by a recursive Generator-Verifier-Updater (GVU) operator. We prove that this operator generates a vector field on the parameter manifold $\Theta$, and we identify the coefficient of self-improvement $\kappa$ as the Lie derivative of the capability functional along this flow.
  The central contribution of this work is the derivation of the Variance Inequality, a spectral condition that is sufficient (under mild regularity) for the stability of self-improvement. We show that a sufficient condition for $\kappa > 0$ is that, up to curvature and step-size effects, the combined noise of generation and verification must be small enough.
  We then apply this formalism to unify the recent literature on Language Self-Play (LSP), Self-Correction, and Synthetic Data bootstrapping. We demonstrate that architectures such as STaR, SPIN, Reflexion, GANs and AlphaZero are specific topological realizations of the GVU operator that satisfy the Variance Inequality through filtration, adversarial discrimination, or grounding in formal systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond Paired Data: Self-Supervised UAV Geo-Localization from Reference Imagery Alone</title>
<link>https://arxiv.org/abs/2512.02737</link>
<guid>https://arxiv.org/abs/2512.02737</guid>
<content:encoded><![CDATA[
arXiv:2512.02737v1 Announce Type: cross 
Abstract: Image-based localization in GNSS-denied environments is critical for UAV autonomy. Existing state-of-the-art approaches rely on matching UAV images to geo-referenced satellite images; however, they typically require large-scale, paired UAV-satellite datasets for training. Such data are costly to acquire and often unavailable, limiting their applicability. To address this challenge, we adopt a training paradigm that removes the need for UAV imagery during training by learning directly from satellite-view reference images. This is achieved through a dedicated augmentation strategy that simulates the visual domain shift between satellite and real-world UAV views. We introduce CAEVL, an efficient model designed to exploit this paradigm, and validate it on ViLD, a new and challenging dataset of real-world UAV images that we release to the community. Our method achieves competitive performance compared to approaches trained with paired data, demonstrating its effectiveness and strong generalization capabilities.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LumiX: Structured and Coherent Text-to-Intrinsic Generation</title>
<link>https://arxiv.org/abs/2512.02781</link>
<guid>https://arxiv.org/abs/2512.02781</guid>
<content:encoded><![CDATA[
arXiv:2512.02781v1 Announce Type: cross 
Abstract: We present LumiX, a structured diffusion framework for coherent text-to-intrinsic generation. Conditioned on text prompts, LumiX jointly generates a comprehensive set of intrinsic maps (e.g., albedo, irradiance, normal, depth, and final color), providing a structured and physically consistent description of an underlying scene. This is enabled by two key contributions: 1) Query-Broadcast Attention, a mechanism that ensures structural consistency by sharing queries across all maps in each self-attention block. 2) Tensor LoRA, a tensor-based adaptation that parameter-efficiently models cross-map relations for efficient joint training. Together, these designs enable stable joint diffusion training and unified generation of multiple intrinsic properties. Experiments show that LumiX produces coherent and physically meaningful results, achieving 23% higher alignment and a better preference score (0.19 vs. -0.41) compared to the state of the art, and it can also perform image-conditioned intrinsic decomposition within the same framework.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Phase-Adaptive LLM Framework with Multi-Stage Validation for Construction Robot Task Allocation: A Systematic Benchmark Against Traditional Optimization Algorithms</title>
<link>https://arxiv.org/abs/2512.02810</link>
<guid>https://arxiv.org/abs/2512.02810</guid>
<content:encoded><![CDATA[
arXiv:2512.02810v1 Announce Type: cross 
Abstract: Multi-robot task allocation in construction automation has traditionally relied on optimization methods such as Dynamic Programming and Reinforcement Learning. This research introduces the LangGraph-based Task Allocation Agent (LTAA), an LLM-driven framework that integrates phase-adaptive allocation strategies, multi-stage validation with hierarchical retries, and dynamic prompting for efficient robot coordination. Although recent LLM approaches show potential for construction robotics, they largely lack rigorous validation and benchmarking against established algorithms. This paper presents the first systematic comparison of LLM-based task allocation with traditional methods in construction scenarios.The study validates LLM feasibility through SMART-LLM replication and addresses implementation challenges using a Self-Corrective Agent Architecture. LTAA leverages natural-language reasoning combined with structured validation mechanisms, achieving major computational gains reducing token usage by 94.6% and allocation time by 86% through dynamic prompting. The framework adjusts its strategy across phases: emphasizing execution feasibility early and workload balance in later allocations.The authors evaluate LTAA against Dynamic Programming, Q-learning, and Deep Q-Network (DQN) baselines using construction operations from the TEACh human-robot collaboration dataset. In the Heavy Excels setting, where robots have strong task specializations, LTAA achieves 77% task completion with superior workload balance, outperforming all traditional methods. These findings show that LLM-based reasoning with structured validation can match established optimization algorithms while offering additional advantages such as interpretability, adaptability, and the ability to update task logic without retraining.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Theory of Contrastive Learning for Domain Generalization</title>
<link>https://arxiv.org/abs/2512.02831</link>
<guid>https://arxiv.org/abs/2512.02831</guid>
<content:encoded><![CDATA[
arXiv:2512.02831v1 Announce Type: cross 
Abstract: Contrastive learning is among the most popular and powerful approaches for self-supervised representation learning, where the goal is to map semantically similar samples close together while separating dissimilar ones in the latent space. Existing theoretical methods assume that downstream task classes are drawn from the same latent class distribution used during the pretraining phase. However, in real-world settings, downstream tasks may not only exhibit distributional shifts within the same label space but also introduce new or broader label spaces, leading to domain generalization challenges. In this work, we introduce novel generalization bounds that explicitly account for both types of mismatch: domain shift and domain generalization. Specifically, we analyze scenarios where downstream tasks either (i) draw classes from the same latent class space but with shifted distributions, or (ii) involve new label spaces beyond those seen during pretraining. Our analysis reveals how the performance of contrastively learned representations depends on the statistical discrepancy between pretraining and downstream distributions. This extended perspective allows us to derive provable guarantees on the performance of learned representations on average classification tasks involving class distributions outside the pretraining latent class set.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cross-Lingual Prompt Steerability: Towards Accurate and Robust LLM Behavior across Languages</title>
<link>https://arxiv.org/abs/2512.02841</link>
<guid>https://arxiv.org/abs/2512.02841</guid>
<content:encoded><![CDATA[
arXiv:2512.02841v1 Announce Type: cross 
Abstract: System prompts provide a lightweight yet powerful mechanism for conditioning large language models (LLMs) at inference time. While prior work has focused on English-only settings, real-world deployments benefit from having a single prompt to operate reliably across languages. This paper presents a comprehensive study of how different system prompts steer models toward accurate and robust cross-lingual behavior. We propose a unified four-dimensional evaluation framework to assess system prompts in multilingual environments. Through large-scale experiments on five languages, three LLMs, and three benchmarks, we uncover that certain prompt components, such as CoT, emotion, and scenario, correlate with robust multilingual behavior. We develop a prompt optimization framework for multilingual settings and show it can automatically discover prompts that improve all metrics by 5-10%. Finally, we analyze over 10 million reasoning units and find that more performant system prompts induce more structured and consistent reasoning patterns, while reducing unnecessary language-switching. Together, we highlight system prompt optimization as a scalable path to accurate and robust multilingual LLM behavior.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLM as Strategist: Adaptive Generation of Safety-critical Testing Scenarios via Guided Diffusion</title>
<link>https://arxiv.org/abs/2512.02844</link>
<guid>https://arxiv.org/abs/2512.02844</guid>
<content:encoded><![CDATA[
arXiv:2512.02844v1 Announce Type: cross 
Abstract: The safe deployment of autonomous driving systems (ADSs) relies on comprehensive testing and evaluation. However, safety-critical scenarios that can effectively expose system vulnerabilities are extremely sparse in the real world. Existing scenario generation methods face challenges in efficiently constructing long-tail scenarios that ensure fidelity, criticality, and interactivity, while particularly lacking real-time dynamic response capabilities to the vehicle under test (VUT). To address these challenges, this paper proposes a safety-critical testing scenario generation framework that integrates the high-level semantic understanding capabilities of Vision Language Models (VLMs) with the fine-grained generation capabilities of adaptive guided diffusion models. The framework establishes a three-layer hierarchical architecture comprising a strategic layer for VLM-directed scenario generation objective determination, a tactical layer for guidance function formulation, and an operational layer for guided diffusion execution. We first establish a high-quality fundamental diffusion model that learns the data distribution of real driving scenarios. Next, we design an adaptive guided diffusion method that enables real-time, precise control of background vehicles (BVs) in closed-loop simulation. The VLM is then incorporated to autonomously generate scenario generation objectives and guidance functions through deep scenario understanding and risk reasoning, ultimately guiding the diffusion model to achieve VLM-directed scenario generation. Experimental results demonstrate that the proposed method can efficiently generate realistic, diverse, and highly interactive safety-critical testing scenarios. Furthermore, case studies validate the adaptability and VLM-directed generation performance of the proposed method.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Are Detectors Fair to Indian IP-AIGC? A Cross-Generator Study</title>
<link>https://arxiv.org/abs/2512.02850</link>
<guid>https://arxiv.org/abs/2512.02850</guid>
<content:encoded><![CDATA[
arXiv:2512.02850v1 Announce Type: cross 
Abstract: Modern image editors can produce identity-preserving AIGC (IP-AIGC), where the same person appears with new attire, background, or lighting. The robustness and fairness of current detectors in this regime remain unclear, especially for under-represented populations. We present what we believe is the first systematic study of IP-AIGC detection for Indian and South-Asian faces, quantifying cross-generator generalization and intra-population performance. We assemble Indian-focused training splits from FairFD and HAV-DF, and construct two held-out IP-AIGC test sets (HIDF-img-ip-genai and HIDF-vid-ip-genai) using commercial web-UI generators (Gemini and ChatGPT) with identity-preserving prompts. We evaluate two state-of-the-art detectors (AIDE and Effort) under pretrained (PT) and fine-tuned (FT) regimes and report AUC, AP, EER, and accuracy. Fine-tuning yields strong in-domain gains (for example, Effort AUC 0.739 to 0.944 on HAV-DF-test; AIDE EER 0.484 to 0.259), but consistently degrades performance on held-out IP-AIGC for Indian cohorts (for example, AIDE AUC 0.923 to 0.563 on HIDF-img-ip-genai; Effort 0.740 to 0.533), which indicates overfitting to training-generator cues. On non-IP HIDF images, PT performance remains high, which suggests a specific brittleness to identity-preserving edits rather than a generic distribution shift. Our study establishes IP-AIGC-Indian as a challenging and practically relevant scenario and motivates representation-preserving adaptation and India-aware benchmark curation to close generalization gaps in AIGC detection.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Leveraging generative adversarial networks with spatially adaptive denormalization for multivariate stochastic seismic data inversion</title>
<link>https://arxiv.org/abs/2512.02863</link>
<guid>https://arxiv.org/abs/2512.02863</guid>
<content:encoded><![CDATA[
arXiv:2512.02863v1 Announce Type: cross 
Abstract: Probabilistic seismic inverse modeling often requires the prediction of both spatially correlated geological heterogeneities (e.g., facies) and continuous parameters (e.g., rock and elastic properties). Generative adversarial networks (GANs) provide an efficient training-image-based simulation framework capable of reproducing complex geological models with high accuracy and comparably low generative cost. However, their application in stochastic geophysical inversion for multivariate property prediction is limited, as representing multiple coupled properties requires large and unstable networks with high memory and training demands. A more recent variant of GANs with spatially adaptive denormalization (SPADE-GAN) enables the direct conditioning of facies spatial distributions on local probability maps. Leveraging on such features, an iterative geostatistical inversion algorithm is proposed, SPADE-GANInv, integrating a pre-trained SPADE-GAN with geostatistical simulation, for the prediction of facies and multiple correlated continuous properties from seismic data. The SPADE-GAN is trained to reproduce realistic facies geometries, while sequential stochastic co-simulation predicts the spatial variability of the facies-dependent continuous properties. At each iteration, a set of subsurface realizations is generated and used to compute synthetic seismic data. The realizations providing the highest similarity coefficient to the observed data are used to update the subsurface probability models in the next iteration. The method is demonstrated on both 2-D synthetic scenarios and field data, targeting the prediction of facies, porosity, and acoustic impedance from full-stack seismic data. Results show that the algorithm enables accurate multivariate prediction, mitigates the impact of biased prior data, and accommodates additional local conditioning such as well logs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VLA Models Are More Generalizable Than You Think: Revisiting Physical and Spatial Modeling</title>
<link>https://arxiv.org/abs/2512.02902</link>
<guid>https://arxiv.org/abs/2512.02902</guid>
<content:encoded><![CDATA[
arXiv:2512.02902v1 Announce Type: cross 
Abstract: Vision-language-action (VLA) models achieve strong in-distribution performance but degrade sharply under novel camera viewpoints and visual perturbations. We show that this brittleness primarily arises from misalignment in Spatial Modeling, rather than Physical Modeling. To address this, we propose a one-shot adaptation framework that recalibrates visual representations through lightweight, learnable updates. Our first method, Feature Token Modulation (FTM), applies a global affine transformation to visual tokens and improves Libero viewpoint accuracy from 48.5% to 87.1% with only 4K parameters. Building on this, Feature Linear Adaptation (FLA) introduces low-rank updates to the ViT encoder, achieving 90.8% success with 4.7M parameters -- matching LoRA-scale finetuning at far lower cost. Together, these results reveal substantial untapped robustness in pretrained VLA models and demonstrate that targeted, minimal visual adaptation is sufficient to restore viewpoint generalization.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Martingale Score: An Unsupervised Metric for Bayesian Rationality in LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.02914</link>
<guid>https://arxiv.org/abs/2512.02914</guid>
<content:encoded><![CDATA[
arXiv:2512.02914v1 Announce Type: cross 
Abstract: Recent advances in reasoning techniques have substantially improved the performance of large language models (LLMs), raising expectations for their ability to provide accurate, truthful, and reliable information. However, emerging evidence suggests that iterative reasoning may foster belief entrenchment and confirmation bias, rather than enhancing truth-seeking behavior. In this study, we propose a systematic evaluation framework for belief entrenchment in LLM reasoning by leveraging the Martingale property from Bayesian statistics. This property implies that, under rational belief updating, the expected value of future beliefs should remain equal to the current belief, i.e., belief updates are unpredictable from the current belief. We propose the unsupervised, regression-based Martingale Score to measure violations of this property, which signal deviation from the Bayesian ability of updating on new evidence. In open-ended problem domains including event forecasting, value-laden questions, and academic paper review, we find such violations to be widespread across models and setups, where the current belief positively predicts future belief updates, a phenomenon which we term belief entrenchment. We identify the models, reasoning techniques, and domains more prone to belief entrenchment. Finally, we validate the Martingale Score by showing that it predicts ground-truth accuracy on problem domains where ground truth labels are available. This indicates that, while designed as an unsupervised metric that operates even in domains without access to ground truth, the Martingale Score is a useful proxy of the truth-seeking ability of a reasoning process.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Representation of Inorganic Synthesis Reactions and Prediction: Graphical Framework and Datasets</title>
<link>https://arxiv.org/abs/2512.02947</link>
<guid>https://arxiv.org/abs/2512.02947</guid>
<content:encoded><![CDATA[
arXiv:2512.02947v1 Announce Type: cross 
Abstract: While machine learning has enabled the rapid prediction of inorganic materials with novel properties, the challenge of determining how to synthesize these materials remains largely unsolved. Previous work has largely focused on predicting precursors or reaction conditions, but only rarely on full synthesis pathways. We introduce the ActionGraph, a directed acyclic graph framework that encodes both the chemical and procedural structure, in terms of synthesis operations, of inorganic synthesis reactions. Using 13,017 text-mined solid-state synthesis reactions from the Materials Project, we show that incorporating PCA-reduced ActionGraph adjacency matrices into a $k$-nearest neighbors retrieval model significantly improves synthesis pathway prediction. While the ActionGraph framework only results in a 1.34% and 2.76% increase in precursor and operation F1 scores (average over varying numbers of PCA components) respectively, the operation length matching accuracy rises 3.4 times (from 15.8% to 53.3%). We observe an interesting trade-off where precursor prediction performance peaks at 10-11 PCA components while operation prediction continues improving up to 30 components. This suggests composition information dominates precursor selection while structural information is critical for operation sequencing. Overall, the ActionGraph framework demonstrates strong potential, and with further adoption, its full range of benefits can be effectively realized.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flexible Gravitational-Wave Parameter Estimation with Transformers</title>
<link>https://arxiv.org/abs/2512.02968</link>
<guid>https://arxiv.org/abs/2512.02968</guid>
<content:encoded><![CDATA[
arXiv:2512.02968v1 Announce Type: cross 
Abstract: Gravitational-wave data analysis relies on accurate and efficient methods to extract physical information from noisy detector signals, yet the increasing rate and complexity of observations represent a growing challenge. Deep learning provides a powerful alternative to traditional inference, but existing neural models typically lack the flexibility to handle variations in data analysis settings. Such variations accommodate imperfect observations or are required for specialized tests, and could include changes in detector configurations, overall frequency ranges, or localized cuts. We introduce a flexible transformer-based architecture paired with a training strategy that enables adaptation to diverse analysis settings at inference time. Applied to parameter estimation, we demonstrate that a single flexible model -- called Dingo-T1 -- can (i) analyze 48 gravitational-wave events from the third LIGO-Virgo-KAGRA Observing Run under a wide range of analysis configurations, (ii) enable systematic studies of how detector and frequency configurations impact inferred posteriors, and (iii) perform inspiral-merger-ringdown consistency tests probing general relativity. Dingo-T1 also improves median sample efficiency on real events from a baseline of 1.4% to 4.2%. Our approach thus demonstrates flexible and scalable inference with a principled framework for handling missing or incomplete data -- key capabilities for current and next-generation observatories.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rethinking Generalized BCIs: Benchmarking 340,000+ Unique Algorithmic Configurations for EEG Mental Command Decoding</title>
<link>https://arxiv.org/abs/2512.02978</link>
<guid>https://arxiv.org/abs/2512.02978</guid>
<content:encoded><![CDATA[
arXiv:2512.02978v1 Announce Type: cross 
Abstract: Robust decoding and classification of brain patterns measured with electroencephalography (EEG) remains a major challenge for real-world (i.e. outside scientific lab and medical facilities) brain-computer interface (BCI) applications due to well documented inter- and intra-participant variability. Here, we present a large-scale benchmark evaluating over 340,000+ unique combinations of spatial and nonlinear EEG classification. Our methodological pipeline consists in combinations of Common Spatial Patterns (CSP), Riemannian geometry, functional connectivity, and fractal- or entropy-based features across three open-access EEG datasets. Unlike prior studies, our analysis operates at the per-participant level and across multiple frequency bands (8-15 Hz and 8-30 Hz), enabling direct assessment of both group-level performance and individual variability. Covariance tangent space projection (cov-tgsp) and CSP consistently achieved the highest average classification accuracies. However, their effectiveness was strongly dataset-dependent, and marked participant-level differences persisted, particularly in the most heterogeneous of the datasets. Importantly, nonlinear methods outperformed spatial approaches for specific individuals, underscoring the need for personalized pipeline selection. Our findings highlight that no universal 'one-size-fits-all' method can optimally decode EEG motor imagery patterns across all users or datasets. Future work will require adaptive, multimodal, and possibly novel approaches to fully address neurophysiological variability in practical BCI applications where the system can automatically adapt to what makes each user unique.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LORE: A Large Generative Model for Search Relevance</title>
<link>https://arxiv.org/abs/2512.03025</link>
<guid>https://arxiv.org/abs/2512.03025</guid>
<content:encoded><![CDATA[
arXiv:2512.03025v1 Announce Type: cross 
Abstract: Achievement. We introduce LORE, a systematic framework for Large Generative Model-based relevance in e-commerce search. Deployed and iterated over three years, LORE achieves a cumulative +27\% improvement in online GoodRate metrics. This report shares the valuable experience gained throughout its development lifecycle, spanning data, features, training, evaluation, and deployment. Insight. While existing works apply Chain-of-Thought (CoT) to enhance relevance, they often hit a performance ceiling. We argue this stems from treating relevance as a monolithic task, lacking principled deconstruction. Our key insight is that relevance comprises distinct capabilities: knowledge and reasoning, multi-modal matching, and rule adherence. We contend that a qualitative-driven decomposition is essential for breaking through current performance bottlenecks. Contributions. LORE provides a complete blueprint for the LLM relevance lifecycle. Key contributions include: (1) A two-stage training paradigm combining progressive CoT synthesis via SFT with human preference alignment via RL. (2) A comprehensive benchmark, RAIR, designed to evaluate these core capabilities. (3) A query frequency-stratified deployment strategy that efficiently transfers offline LLM capabilities to the online system. LORE serves as both a practical solution and a methodological reference for other vertical domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Physically Consistent Lagrangian Control Models Without Acceleration Measurements</title>
<link>https://arxiv.org/abs/2512.03035</link>
<guid>https://arxiv.org/abs/2512.03035</guid>
<content:encoded><![CDATA[
arXiv:2512.03035v1 Announce Type: cross 
Abstract: This article investigates the modeling and control of Lagrangian systems involving non-conservative forces using a hybrid method that does not require acceleration calculations. It focuses in particular on the derivation and identification of physically consistent models, which are essential for model-based control synthesis. Lagrangian or Hamiltonian neural networks provide useful structural guarantees but the learning of such models often leads to inconsistent models, especially on real physical systems where training data are limited, partial and noisy. Motivated by this observation and the objective to exploit these models for model-based nonlinear control, a learning algorithm relying on an original loss function is proposed to improve the physical consistency of Lagrangian systems. A comparative analysis of different learning-based modeling approaches with the proposed solution shows significant improvements in terms of physical consistency of the learned models, on both simulated and experimental systems. The model's consistency is then exploited to demonstrate, on an experimental benchmark, the practical relevance of the proposed methodology for feedback linearization and energy-based control techniques.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithms for Adaptive Experiments that Trade-off Statistical Analysis with Reward: Combining Uniform Random Assignment and Reward Maximization</title>
<link>https://arxiv.org/abs/2112.08507</link>
<guid>https://arxiv.org/abs/2112.08507</guid>
<content:encoded><![CDATA[
arXiv:2112.08507v4 Announce Type: replace 
Abstract: Traditional randomized A/B experiments assign arms with uniform random (UR) probability, such as 50/50 assignment to two versions of a website to discover whether one version engages users more. To more quickly and automatically use data to benefit users, multi-armed bandit algorithms such as Thompson Sampling (TS) have been advocated. While TS is interpretable and incorporates the randomization key to statistical inference, it can cause biased estimates and increase false positives and false negatives in detecting differences in arm means. We introduce a more Statistically Sensitive algorithm, TS-PostDiff (Posterior Probability of Small Difference), that mixes TS with traditional UR by using an additional adaptive step, where the probability of using UR (vs TS) is proportional to the posterior probability that the difference in arms is small. This allows an experimenter to define what counts as a small difference, below which a traditional UR experiment can obtain informative data for statistical inference at low cost, and above which using more TS to maximize user benefits is key. We evaluate TS-PostDiff against UR, TS, and two other TS variants designed to improve statistical inference. We consider results for the common two-armed experiment across a range of settings inspired by real-world applications. Our results provide insight into when and why TS-PostDiff or alternative approaches provide better tradeoffs between benefiting users (reward) and statistical inference (false positive rate and power). TS-PostDiff's adaptivity helps efficiently reduce false positives and increase statistical power when differences are small, while increasing reward more when differences are large. The work highlights important considerations for future Statistically Sensitive algorithm development that balances reward and statistical analysis in adaptive experimentation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian and Non-Gaussian Universality of Data Augmentation</title>
<link>https://arxiv.org/abs/2202.09134</link>
<guid>https://arxiv.org/abs/2202.09134</guid>
<content:encoded><![CDATA[
arXiv:2202.09134v5 Announce Type: replace 
Abstract: We provide universality results that quantify how data augmentation affects the variance and limiting distribution of estimates through simple surrogates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. As our main theoretical tool, we develop an adaptation of Lindeberg's technique for block dependence. The resulting universality regime may be Gaussian or non-Gaussian.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs</title>
<link>https://arxiv.org/abs/2312.15230</link>
<guid>https://arxiv.org/abs/2312.15230</guid>
<content:encoded><![CDATA[
arXiv:2312.15230v4 Announce Type: replace 
Abstract: Neural Networks can be effectively compressed through pruning, significantly reducing storage and compute demands while maintaining predictive performance. Simple yet effective methods like magnitude pruning remove less important parameters and typically require a costly retraining procedure to restore performance. However, with the rise of LLMs, full retraining has become infeasible due to memory and compute constraints. This study challenges the practice of retraining all parameters by showing that updating a small subset of highly expressive parameters can suffice to recover or even enhance performance after pruning. Surprisingly, retraining just 0.01%-0.05% of the parameters in GPT-architectures can match the performance of full retraining across various sparsity levels, significantly reducing compute and memory requirements, and enabling retraining of models with up to 30 billion parameters on a single GPU in minutes. To bridge the gap to full retraining in the high sparsity regime, we introduce two novel LoRA variants that, unlike standard LoRA, allow merging adapters back without compromising sparsity. Going a step further, we show that these methods can be applied for memory-efficient layer-wise reconstruction, significantly enhancing state-of-the-art retraining-free methods like Wanda (Sun et al., 2023) and SparseGPT (Frantar & Alistarh, 2023). Our findings present a promising alternative to avoiding retraining.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixture of Experts Softens the Curse of Dimensionality in Operator Learning</title>
<link>https://arxiv.org/abs/2404.09101</link>
<guid>https://arxiv.org/abs/2404.09101</guid>
<content:encoded><![CDATA[
arXiv:2404.09101v2 Announce Type: replace 
Abstract: We study the approximation-theoretic implications of mixture-of-experts architectures for operator learning, where the complexity of a single large neural operator is distributed across many small neural operators (NOs), and each input is routed to exactly one NO via a decision tree. We analyze how this tree-based routing and expert decomposition affect approximation power, sample complexity, and stability. Our main result is a distributed universal approximation theorem for mixture of neural operators (MoNOs): any Lipschitz nonlinear operator between $L^2([0,1]^d)$ spaces can be uniformly approximated over the Sobolev unit ball to arbitrary accuracy $\varepsilon>0$ by an MoNO, where each expert NO has a depth, width, and rank scaling as $\mathcal{O}(\varepsilon^{-1})$. Although the number of experts may grow with accuracy, each NO remains small, enough to fit within active memory of standard hardware for reasonable accuracy levels. Our analysis also yields new quantitative approximation rates for classical NOs approximating uniformly continuous nonlinear operators uniformly on compact subsets of $L^2([0,1]^d)$.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>XXLTraffic: Expanding and Extremely Long Traffic forecasting beyond test adaptation</title>
<link>https://arxiv.org/abs/2406.12693</link>
<guid>https://arxiv.org/abs/2406.12693</guid>
<content:encoded><![CDATA[
arXiv:2406.12693v3 Announce Type: replace 
Abstract: Traffic forecasting is crucial for smart cities and intelligent transportation initiatives, where deep learning has made significant progress in modeling complex spatio-temporal patterns in recent years. However, current public datasets have limitations in reflecting the distribution shift nature of real-world scenarios, characterized by continuously evolving infrastructures, varying temporal distributions, and long temporal gaps due to sensor downtimes or changes in traffic patterns. These limitations inevitably restrict the practical applicability of existing traffic forecasting datasets. To bridge this gap, we present XXLTraffic, largest available public traffic dataset with the longest timespan collected from Los Angeles, USA, and New South Wales, Australia, curated to support research in extremely long forecasting beyond test adaptation. Our benchmark includes both typical time-series forecasting settings with hourly and daily aggregated data and novel configurations that introduce gaps and down-sample the training size to better simulate practical constraints. We anticipate the new XXLTraffic will provide a fresh perspective for the time-series and traffic forecasting communities. It would also offer a robust platform for developing and evaluating models designed to tackle the extremely long forecasting problems beyond test adaptation. Our dataset supplements existing spatio-temporal data resources and leads to new research directions in this domain.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-trained Language Models Improve the Few-shot Prompt Ability of Decision Transformer</title>
<link>https://arxiv.org/abs/2408.01402</link>
<guid>https://arxiv.org/abs/2408.01402</guid>
<content:encoded><![CDATA[
arXiv:2408.01402v2 Announce Type: replace 
Abstract: Decision Transformer (DT) has emerged as a promising class of algorithms in offline reinforcement learning (RL) tasks, leveraging pre-collected datasets and Transformer's capability to model long sequences. Recent works have demonstrated that using parts of trajectories from training tasks as prompts in DT enhances its performance on unseen tasks, giving rise to Prompt-DT methods. However, collecting data from specific environments can be both costly and unsafe in many scenarios, leading to suboptimal performance and limited few-shot prompt abilities due to the data-hungry nature of Transformer-based models. Additionally, the limited datasets used in pre-training make it challenging for Prompt-DT type of methods to distinguish between various RL tasks through prompts alone. To address these challenges, we introduce the Language model-initialized Prompt Decision Transformer (LPDT) framework, which leverages pretrained language models providing rich prior knowledge for RL tasks and fine-tunes the sequence model using Low-rank Adaptation (LoRA) for meta-RL problems. We further incorporate prompt regularization to effectively differentiate between tasks based on prompt feature representations. Comprehensive empirical studies demonstrate that initializing with a pre-trained language model provides the prior knowledge and achieves a similar performance with Prompt-DT under only $10\%$ data in some MuJoCo control tasks. We also provide a thorough ablation study to validate the effectiveness of each component, including sequence modeling, language models, prompt regularizations, and prompt strategies.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Minimax Hypothesis Testing for the Bradley-Terry-Luce Model</title>
<link>https://arxiv.org/abs/2410.08360</link>
<guid>https://arxiv.org/abs/2410.08360</guid>
<content:encoded><![CDATA[
arXiv:2410.08360v2 Announce Type: replace 
Abstract: The Bradley-Terry-Luce (BTL) model is one of the most widely used models for ranking a collection of items or agents based on pairwise comparisons among them. Given $n$ agents, the BTL model endows each agent $i$ with a latent skill score $\alpha_i > 0$ and posits that the probability that agent $i$ is preferred over agent $j$ is $\alpha_i/(\alpha_i + \alpha_j)$. In this work, our objective is to formulate a hypothesis test that determines whether a given pairwise comparison dataset, with $k$ comparisons per pair of agents, originates from an underlying BTL model. We formalize this testing problem in the minimax sense and define the critical threshold of the problem. We then establish upper bounds on the critical threshold for general induced observation graphs (satisfying mild assumptions) and develop lower bounds for complete induced graphs. Our bounds demonstrate that for complete induced graphs, the critical threshold scales as $\Theta((nk)^{-1/2})$ in a minimax sense. In particular, our test statistic for the upper bounds is based on a new approximation we derive for the separation distance between general pairwise comparison models and the class of BTL models. To further assess the performance of our statistical test, we prove upper bounds on the type I and type II probabilities of error. Much of our analysis is conducted within the context of a fixed observation graph structure, where the graph possesses certain ``nice'' properties, such as expansion and bounded principal ratio. Additionally, we derive several auxiliary results, such as bounds on principal ratios of graphs, $\ell^2$-bounds on BTL parameter estimation under model mismatch, stability of rankings under the BTL model, etc. We validate our theoretical results through experiments on synthetic and real-world datasets and propose a data-driven permutation testing approach to determine test thresholds.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WARPD: World model Assisted Reactive Policy Diffusion</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v3 Announce Type: replace 
Abstract: With the increasing availability of open-source robotic data, imitation learning has become a promising approach for both manipulation and locomotion. Diffusion models are now widely used to train large, generalized policies that predict controls or trajectories, leveraging their ability to model multimodal action distributions. However, this generality comes at the cost of larger model sizes and slower inference, an acute limitation for robotic tasks requiring high control frequencies. Moreover, Diffusion Policy (DP), a popular trajectory-generation approach, suffers from a trade-off between performance and action horizon: fewer diffusion queries lead to larger trajectory chunks, which in turn accumulate tracking errors. To overcome these challenges, we introduce WARPD (World model Assisted Reactive Policy Diffusion), a method that generates closed-loop policies (weights for neural policies) directly, instead of open-loop trajectories. By learning behavioral distributions in parameter space rather than trajectory space, WARPD offers two major advantages: (1) extended action horizons with robustness to perturbations, while maintaining high task performance, and (2) significantly reduced inference costs. Empirically, WARPD outperforms DP in long-horizon and perturbed environments, and achieves multitask performance on par with DP while requiring only ~ 1/45th of the inference-time FLOPs per step.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LSHBloom: Memory-efficient, Extreme-scale Document Deduplication</title>
<link>https://arxiv.org/abs/2411.04257</link>
<guid>https://arxiv.org/abs/2411.04257</guid>
<content:encoded><![CDATA[
arXiv:2411.04257v3 Announce Type: replace 
Abstract: Contemporary large language model (LLM) training pipelines require the assembly of internet-scale databases full of text data from a variety of sources (e.g., web, academic, and publishers). Preprocessing these datasets via deduplication -- detecting and eliminating additional instances of the same content -- is a major focus for assembling and curating training datasets for LLMs. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Unfortunately, contemporary approaches to document-level deduplication are either unreliable at accurately identifying duplicate documents or extremely expensive in terms of both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same state-of-the-art deduplication performance as MinhashLSH, with only a marginal increase in false positives (near zero in our experiments), while boasting competitive runtime (12$\times$ faster than MinhashLSH on peS2o) and, crucially, using 18$\times$ less disk space than MinhashLSH (as measured on peS2o). Based on extrapolation, we show that this advantage in space and runtime remains even at the extreme scale of several billion documents. LSHBloom allows practitioners to access the deduplication quality of MinHashLSH at scales that are normally only tractable for less sophisticated, heuristic solutions. As a result, LSHBloom promises to enable scaling high-quality document deduplication to internet-scale text datasets.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FedSub: Introducing Class-aware Subnetworks Fusion to Enhance Personalized Federated Learning</title>
<link>https://arxiv.org/abs/2411.08699</link>
<guid>https://arxiv.org/abs/2411.08699</guid>
<content:encoded><![CDATA[
arXiv:2411.08699v2 Announce Type: replace 
Abstract: Personalized Federated Learning aims at addressing the challenges of non-IID data in collaborative model training. However, existing methods struggle to balance personalization and generalization, often oversimplifying client similarities or relying too heavily on global models. In this paper, we propose FedSub, a novel approach that introduces class-aware model updates based on data prototypes and model subnetworks fusion to enhance personalization. Prototypes serve as compact representations of client data for each class, clustered on the server to capture label-specific similarities among the clients. Meanwhile, model subnetworks encapsulate the most relevant components to process each class and they are then fused on the server based on the identified clusters to generate fine-grained, class-specific, and highly personalized model updates for each client. Experimental results in three real-world scenarios with high data heterogeneity in human activity recognition and mobile health applications demonstrate the effectiveness of FedSub with respect to state-of-the-art methods to achieve fast convergence and high classification performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sloth: scaling laws for LLM skills to predict multi-benchmark performance across families</title>
<link>https://arxiv.org/abs/2412.06540</link>
<guid>https://arxiv.org/abs/2412.06540</guid>
<content:encoded><![CDATA[
arXiv:2412.06540v5 Announce Type: replace 
Abstract: Scaling laws for large language models (LLMs) predict model performance based on parameters like size and training data. However, differences in training configurations and data processing across model families lead to significant variations in benchmark performance, making it difficult for a single scaling law to generalize across all LLMs. On the other hand, training family-specific scaling laws requires training models of varying sizes for every family. In this work, we propose Skills Scaling Laws (SSLaws, pronounced as Sloth), a novel scaling law that leverages publicly available benchmark data and assumes LLM performance is driven by low-dimensional latent skills, such as reasoning and instruction following. These latent skills are influenced by computational resources like model size and training tokens, but with varying efficiencies across model families. Sloth exploits correlations across benchmarks to provide more accurate and interpretable predictions while alleviating the need to train multiple LLMs per family. We present both theoretical results on parameter identification and empirical evaluations on 12 prominent benchmarks, from Open LLM Leaderboard v1/v2, demonstrating that Sloth predicts LLM performance accurately and offers insights into scaling behaviors for complex downstream tasks, increased test-time compute, and compute-optimal scaling of skills.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CSI-BERT2: A BERT-inspired Framework for Efficient CSI Prediction and Classification in Wireless Communication and Sensing</title>
<link>https://arxiv.org/abs/2412.06861</link>
<guid>https://arxiv.org/abs/2412.06861</guid>
<content:encoded><![CDATA[
arXiv:2412.06861v4 Announce Type: replace 
Abstract: Channel state information (CSI) is a fundamental component in both wireless communication and sensing systems, enabling critical functions such as radio resource optimization and environmental perception. In wireless sensing, data scarcity and packet loss hinder efficient model training, while in wireless communication, high-dimensional CSI matrices and short coherent times caused by high mobility present challenges in CSI estimation. To address these issues, we propose a unified framework named CSI-BERT2 for CSI prediction and classification tasks, built on CSI-BERT, which adapts BERT to capture the complex relationships among CSI sequences through a bidirectional self-attention mechanism. We introduce a two-stage training method that first uses a mask language model (MLM) to enable the model to learn general feature extraction from scarce datasets in an unsupervised manner, followed by fine-tuning for specific downstream tasks. Specifically, we extend MLM into a mask prediction model (MPM), which efficiently addresses the CSI prediction task. To further enhance the representation capacity of CSI data, we modify the structure of the original CSI-BERT. We introduce an adaptive re-weighting layer (ARL) to enhance subcarrier representation and a multi-layer perceptron (MLP)-based temporal embedding module to mitigate temporal information loss problem inherent in the original Transformer. Extensive experiments on both real-world collected and simulated datasets demonstrate that CSI-BERT2 achieves state-of-the-art performance across all tasks. Our results further show that CSI-BERT2 generalizes effectively across varying sampling rates and robustly handles discontinuous CSI sequences caused by packet loss-challenges that conventional methods fail to address. The dataset and code are publicly available at https://github.com/RS2002/CSI-BERT2 .
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Machine Unlearning via Information Theoretic Regularization</title>
<link>https://arxiv.org/abs/2502.05684</link>
<guid>https://arxiv.org/abs/2502.05684</guid>
<content:encoded><![CDATA[
arXiv:2502.05684v3 Announce Type: replace 
Abstract: How can we effectively remove or ''unlearn'' undesirable information, such as specific features or the influence of individual data points, from a learning outcome while minimizing utility loss and ensuring rigorous guarantees? We introduce a unified mathematical framework based on information-theoretic regularization to address both data point unlearning and feature unlearning. For data point unlearning, we introduce the $\textit{Marginal Unlearning Principle}$, an auditable and provable framework inspired by memory suppression studies in neuroscience. Moreover, we provide formal information-theoretic unlearning definition based on the proposed principle, named marginal unlearning, and provable guarantees on sufficiency and necessity of marginal unlearning to the existing approximate unlearning definitions. We then show the proposed framework provide natural solution to the marginal unlearning problems. For feature unlearning, the framework applies to deep learning with arbitrary training objectives. By combining flexibility in learning objectives with simplicity in regularization design, our approach is highly adaptable and practical for a wide range of machine learning and AI applications. From a mathematical perspective, we provide an unified analytic solution to the optimal feature unlearning problem with a variety of information-theoretic training objectives. Our theoretical analysis reveals intriguing connections between machine unlearning, information theory, optimal transport, and extremal sigma algebras. Numerical simulations support our theoretical finding.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>EquiTabPFN: A Target-Permutation Equivariant Prior Fitted Networks</title>
<link>https://arxiv.org/abs/2502.06684</link>
<guid>https://arxiv.org/abs/2502.06684</guid>
<content:encoded><![CDATA[
arXiv:2502.06684v3 Announce Type: replace 
Abstract: Recent foundational models for tabular data, such as TabPFN, excel at adapting to new tasks via in-context learning, but remain constrained to a fixed, pre-defined number of target dimensions-often necessitating costly ensembling strategies. We trace this constraint to a deeper architectural shortcoming: these models lack target equivariance, so that permuting target dimension orderings alters their predictions. This deficiency gives rise to an irreducible "equivariance gap", an error term that introduces instability in predictions. We eliminate this gap by designing a fully target-equivariant architecture-ensuring permutation invariance via equivariant encoders, decoders, and a bi-attention mechanism. Empirical evaluation on standard classification benchmarks shows that, on datasets with more classes than those seen during pre-training, our model matches or surpasses existing methods while incurring lower computational overhead.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MaxSup: Overcoming Representation Collapse in Label Smoothing</title>
<link>https://arxiv.org/abs/2502.15798</link>
<guid>https://arxiv.org/abs/2502.15798</guid>
<content:encoded><![CDATA[
arXiv:2502.15798v3 Announce Type: replace 
Abstract: Label Smoothing (LS) is widely adopted to reduce overconfidence in neural network predictions and improve generalization. Despite these benefits, recent studies reveal two critical issues with LS. First, LS induces overconfidence in misclassified samples. Second, it compacts feature representations into overly tight clusters, diluting intra-class diversity, although the precise cause of this phenomenon remained elusive. In this paper, we analytically decompose the LS-induced loss, exposing two key terms: (i) a regularization term that dampens overconfidence only when the prediction is correct, and (ii) an error-amplification term that arises under misclassifications. This latter term compels the network to reinforce incorrect predictions with undue certainty, exacerbating representation collapse. To address these shortcomings, we propose Max Suppression (MaxSup), which applies uniform regularization to both correct and incorrect predictions by penalizing the top-1 logit rather than the ground-truth logit. Through extensive feature-space analyses, we show that MaxSup restores intra-class variation and sharpens inter-class boundaries. Experiments on large-scale image classification and multiple downstream tasks confirm that MaxSup is a more robust alternative to LS. Code is available at: https://github.com/ZhouYuxuanYX/Maximum-Suppression-Regularization
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Aligning Compound AI Systems via System-level DPO</title>
<link>https://arxiv.org/abs/2502.17721</link>
<guid>https://arxiv.org/abs/2502.17721</guid>
<content:encoded><![CDATA[
arXiv:2502.17721v3 Announce Type: replace 
Abstract: Compound AI systems, comprising multiple interacting components such as LLMs, foundation models, and external tools, have demonstrated remarkable improvements compared to single models in various tasks. To ensure their effective deployment in real-world applications, aligning these systems with human preferences is crucial. However, aligning the compound system via policy optimization, unlike the alignment of a single model, is challenging for two main reasons: (i) non-differentiable interactions between components make end-to-end gradient-based optimization method inapplicable, and (ii) system-level preferences cannot be directly transformed into component-level preferences. To address these challenges, we first formulate compound AI systems as Directed Acyclic Graphs (DAGs), explicitly modeling both component interactions and the associated data flows. Building on this formulation, we introduce $\textbf{SysDPO}$, a framework that extends Direct Preference Optimization (DPO) to enable joint system-level alignment. We propose two variants, SysDPO-Direct and SysDPO-Sampling, tailored for scenarios depending on whether we construct a system-specific preference dataset. We empirically demonstrate the effectiveness of our approach across two applications: the joint alignment of a language model and a diffusion model, and the joint alignment of an LLM collaboration system.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Projecting Assumptions: The Duality Between Sparse Autoencoders and Concept Geometry</title>
<link>https://arxiv.org/abs/2503.01822</link>
<guid>https://arxiv.org/abs/2503.01822</guid>
<content:encoded><![CDATA[
arXiv:2503.01822v2 Announce Type: replace 
Abstract: Sparse Autoencoders (SAEs) are widely used to interpret neural networks by identifying meaningful concepts from their representations. However, do SAEs truly uncover all concepts a model relies on, or are they inherently biased toward certain kinds of concepts? We introduce a unified framework that recasts SAEs as solutions to a bilevel optimization problem, revealing a fundamental challenge: each SAE imposes structural assumptions about how concepts are encoded in model representations, which in turn shapes what it can and cannot detect. This means different SAEs are not interchangeable -- switching architectures can expose entirely new concepts or obscure existing ones. To systematically probe this effect, we evaluate SAEs across a spectrum of settings: from controlled toy models that isolate key variables, to semi-synthetic experiments on real model activations and finally to large-scale, naturalistic datasets. Across this progression, we examine two fundamental properties that real-world concepts often exhibit: heterogeneity in intrinsic dimensionality (some concepts are inherently low-dimensional, others are not) and nonlinear separability. We show that SAEs fail to recover concepts when these properties are ignored, and we design a new SAE that explicitly incorporates both, enabling the discovery of previously hidden concepts and reinforcing our theoretical insights. Our findings challenge the idea of a universal SAE and underscores the need for architecture-specific choices in model interpretability. Overall, we argue an SAE does not just reveal concepts -- it determines what can be seen at all.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mixed precision accumulation for neural network inference guided by componentwise forward error analysis</title>
<link>https://arxiv.org/abs/2503.15568</link>
<guid>https://arxiv.org/abs/2503.15568</guid>
<content:encoded><![CDATA[
arXiv:2503.15568v2 Announce Type: replace 
Abstract: This work proposes a mathematically founded mixed precision accumulation strategy for the inference of neural networks. Our strategy is based on a new componentwise forward error analysis that explains the propagation of errors in the forward pass of neural networks. Specifically, our analysis shows that the error in each component of the output of a linear layer is proportional to the condition number of the inner product between the weights and the input, multiplied by the condition number of the activation function. These condition numbers can vary widely from one component to the other, thus creating a significant opportunity to introduce mixed precision: each component should be accumulated in a precision inversely proportional to the product of these condition numbers. We propose a numerical algorithm that exploits this observation: it first computes all components in low precision, uses this output to estimate the condition numbers, and recomputes in higher precision only the components associated with large condition numbers. We test our algorithm on various networks and datasets and confirm experimentally that it can significantly improve the cost--accuracy tradeoff compared with uniform precision accumulation baselines.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Maternal and Fetal Health Status Assessment by Using Machine Learning on Optical 3D Body Scans</title>
<link>https://arxiv.org/abs/2504.05627</link>
<guid>https://arxiv.org/abs/2504.05627</guid>
<content:encoded><![CDATA[
arXiv:2504.05627v2 Announce Type: replace 
Abstract: Monitoring maternal and fetal health during pregnancy is crucial for preventing adverse outcomes. While tests such as ultrasound scans offer high accuracy, they can be costly and inconvenient. Telehealth and more accessible body shape information provide pregnant women with a convenient way to monitor their health. This study explores the potential of 3D body scan data, captured during the 18-24 gestational weeks, to predict adverse pregnancy outcomes and estimate clinical parameters. We developed a novel algorithm with two parallel streams which are used for extract body shape features: one for supervised learning to extract sequential abdominal circumference information, and another for unsupervised learning to extract global shape descriptors, alongside a branch for demographic data.
  Our results indicate that 3D body shape can assist in predicting preterm labor, gestational diabetes mellitus (GDM), gestational hypertension (GH), and in estimating fetal weight. Compared to other machine learning models, our algorithm achieved the best performance, with prediction accuracies exceeding 88% and fetal weight estimation accuracy of 76.74% within a 10% error margin, outperforming conventional anthropometric methods by 22.22%.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>VeLU: Variance-enhanced Learning Unit for Deep Neural Networks</title>
<link>https://arxiv.org/abs/2504.15051</link>
<guid>https://arxiv.org/abs/2504.15051</guid>
<content:encoded><![CDATA[
arXiv:2504.15051v2 Announce Type: replace 
Abstract: Activation functions play a critical role in deep neural networks by shaping gradient flow, optimization stability, and generalization. While ReLU remains widely used due to its simplicity, it suffers from gradient sparsity and dead-neuron issues and offers no adaptivity to input statistics. Smooth alternatives such as Swish and GELU improve gradient propagation but still apply a fixed transformation regardless of the activation distribution. In this paper, we propose VeLU, a Variance-enhanced Learning Unit that introduces variance-aware and distributionally aligned nonlinearity through a principled combination of ArcTan-ArcSin transformations, adaptive scaling, and Wasserstein-2 regularization (Optimal Transport). This design enables VeLU to modulate its response based on local activation variance, mitigate internal covariate shift at the activation level, and improve training stability without adding learnable parameters or architectural overhead. Extensive experiments across six deep neural networks show that VeLU outperforms ReLU, ReLU6, Swish, and GELU on 12 vision benchmarks. The implementation of VeLU is publicly available in GitHub.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation</title>
<link>https://arxiv.org/abs/2504.19602</link>
<guid>https://arxiv.org/abs/2504.19602</guid>
<content:encoded><![CDATA[
arXiv:2504.19602v3 Announce Type: replace 
Abstract: Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels, i.e., normalized probability distributions) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining competitive accuracy. Enhanced ERA resolves the fundamental instability of conventional temperature-based aggregation, ensuring robust control and high performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>How to Learn a Star: Binary Classification with Starshaped Polyhedral Sets</title>
<link>https://arxiv.org/abs/2505.01346</link>
<guid>https://arxiv.org/abs/2505.01346</guid>
<content:encoded><![CDATA[
arXiv:2505.01346v2 Announce Type: replace 
Abstract: We consider binary classification restricted to a class of continuous piecewise linear functions whose decision boundaries are (possibly nonconvex) starshaped polyhedral sets, supported on a fixed polyhedral simplicial fan. We investigate the expressivity of these function classes and describe the combinatorial and geometric structure of the loss landscape, most prominently the sublevel sets, for two loss-functions: the 0/1-loss (discrete loss) and a log-likelihood loss function. In particular, we give explicit bounds on the VC dimension of this model, and concretely describe the sublevel sets of the discrete loss as chambers in a hyperplane arrangement. For the log-likelihood loss, we give sufficient conditions for the optimum to be unique, and describe the geometry of the optimum when varying the rate parameter of the underlying exponential probability distribution.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning to Integrate Diffusion ODEs by Averaging the Derivatives</title>
<link>https://arxiv.org/abs/2505.14502</link>
<guid>https://arxiv.org/abs/2505.14502</guid>
<content:encoded><![CDATA[
arXiv:2505.14502v2 Announce Type: replace 
Abstract: To accelerate diffusion model inference, numerical solvers perform poorly at extremely small steps, while distillation techniques often introduce complexity and instability. This work presents an intermediate strategy, balancing performance and cost, by learning ODE integration using loss functions derived from the derivative-integral relationship, inspired by Monte Carlo integration and Picard iteration. From a geometric perspective, the losses operate by gradually extending the tangent to the secant, thus are named as secant losses. The target of secant losses is the same as that of diffusion models, or the diffusion model itself, leading to great training stability. By fine-tuning or distillation, the secant version of EDM achieves a $10$-step FID of $2.14$ on CIFAR-10, while the secant version of SiT-XL/2 attains a $4$-step FID of $2.27$ and an $8$-step FID of $1.96$ on ImageNet-$256\times256$. Code is available at https://github.com/poppuppy/secant_expectation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Cohort-Based Active Modality Acquisition</title>
<link>https://arxiv.org/abs/2505.16791</link>
<guid>https://arxiv.org/abs/2505.16791</guid>
<content:encoded><![CDATA[
arXiv:2505.16791v3 Announce Type: replace 
Abstract: Real-world machine learning applications often involve data from multiple modalities that must be integrated effectively to make robust predictions. However, in many practical settings, not all modalities are available for every sample, and acquiring additional modalities can be costly. This raises the question: which samples should be prioritized for additional modality acquisition when resources are limited? While prior work has explored individual-level acquisition strategies and training-time active learning paradigms, test-time and cohort-based acquisition remain underexplored. We introduce Cohort-based Active Modality Acquisition (CAMA), a novel test-time setting to formalize the challenge of selecting which samples should receive additional modalities. We derive acquisition strategies that leverage a combination of generative imputation and discriminative modeling to estimate the expected benefit of acquiring missing modalities based on common evaluation metrics. We also introduce upper-bound heuristics that provide performance ceilings to benchmark acquisition strategies. Experiments on multimodal datasets with up to 15 modalities demonstrate that our proposed imputation-based strategies can more effectively guide the acquisition of additional modalities for selected samples compared with methods relying solely on unimodal information, entropy-based guidance, or random selection. We showcase the real-world relevance and scalability of our method by demonstrating its ability to effectively guide the costly acquisition of proteomics data for disease prediction in a large prospective cohort, the UK Biobank (UKBB). Our work provides an effective approach for optimizing modality acquisition at the cohort level, enabling more effective use of resources in constrained settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping</title>
<link>https://arxiv.org/abs/2505.18738</link>
<guid>https://arxiv.org/abs/2505.18738</guid>
<content:encoded><![CDATA[
arXiv:2505.18738v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full finetuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA's representational capacity. To address this limitation, we propose AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture fixed and learnable nonlinearities. This combination forms an MLP-like structure with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I) not only matches or surpasses full fine-tuning performance with only 6.18% ~ 25% of LoRA's parameters but also (II) outperforms competitive PEFT methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust performance across various rank configurations.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Policy Optimization in Robust Constrained MDPs with Iteration Complexity Guarantees</title>
<link>https://arxiv.org/abs/2505.19238</link>
<guid>https://arxiv.org/abs/2505.19238</guid>
<content:encoded><![CDATA[
arXiv:2505.19238v2 Announce Type: replace 
Abstract: Constrained decision-making is essential for designing safe policies in real-world control systems, yet simulated environments often fail to capture real-world adversities. We consider the problem of learning a policy that will maximize the cumulative reward while satisfying a constraint, even when there is a mismatch between the real model and an accessible simulator/nominal model. In particular, we consider the robust constrained Markov decision problem (RCMDP) where an agent needs to maximize the reward and satisfy the constraint against the worst possible stochastic model under the uncertainty set centered around an unknown nominal model. Primal-dual methods, effective for standard constrained MDP (CMDP), are not applicable here because of the lack of the strong duality property. Further, one cannot apply the standard robust value-iteration based approach on the composite value function either as the worst case models may be different for the reward value function and the constraint value function. We propose a novel technique that effectively minimizes the constraint value function--to satisfy the constraints; on the other hand, when all the constraints are satisfied, it can simply maximize the robust reward value function. We prove that such an algorithm finds a policy with at most $\epsilon$ sub-optimality and feasible policy after $O(\epsilon^{-2})$ iterations. In contrast to the state-of-the-art method, we do not need to employ a binary search, thus, we reduce the computation time by at least 4x for smaller value of discount factor ($\gamma$) and by at least 6x for larger value of $\gamma$.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>medDreamer: Model-Based Reinforcement Learning with Latent Imagination on Complex EHRs for Clinical Decision Support</title>
<link>https://arxiv.org/abs/2505.19785</link>
<guid>https://arxiv.org/abs/2505.19785</guid>
<content:encoded><![CDATA[
arXiv:2505.19785v3 Announce Type: replace 
Abstract: Timely and personalized treatment decisions are essential across a wide range of healthcare settings where patient responses can vary significantly and evolve over time. Clinical data used to support these treatment decisions are often irregularly sampled, where missing data frequencies may implicitly convey information about the patient's condition. Existing Reinforcement Learning (RL) based clinical decision support systems often ignore the missing patterns and distort them with coarse discretization and simple imputation. They are also predominantly model-free and largely depend on retrospective data, which could lead to insufficient exploration and bias by historical behaviors. To address these limitations, we propose medDreamer, a novel model-based reinforcement learning framework for personalized treatment recommendation. medDreamer contains a world model with an Adaptive Feature Integration module that simulates latent patient states from irregular data and a two-phase policy trained on a hybrid of real and imagined trajectories. This enables learning optimal policies that go beyond the sub-optimality of historical clinical decisions, while remaining close to real clinical data. We evaluate medDreamer on both sepsis and mechanical ventilation treatment tasks using two large-scale Electronic Health Records (EHRs) datasets. Comprehensive evaluations show that medDreamer significantly outperforms model-free and model-based baselines in both clinical outcomes and off-policy metrics.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Differential Privacy Analysis of Decentralized Gossip Averaging under Varying Threat Models</title>
<link>https://arxiv.org/abs/2505.19969</link>
<guid>https://arxiv.org/abs/2505.19969</guid>
<content:encoded><![CDATA[
arXiv:2505.19969v2 Announce Type: replace 
Abstract: Fully decentralized training of machine learning models offers significant advantages in scalability, robustness, and fault tolerance. However, achieving differential privacy (DP) guarantees in such settings is challenging due to the absence of a central aggregator and varying trust assumptions among nodes. We present a novel privacy analysis of decentralized gossip-based averaging algorithms with additive node-level noise, from arbitrary views of nodes in a graph and especially consider the averaging over nearest neighbors with secure summation and individual node-wise views. Our main contribution is a an analytical framework based on a linear systems formulation that accurately characterizes privacy leakage between nodes across different scenarios. In case the gossip averaging happens via secure summation, we show that the R\'enyi DP parameter growth is asymptotically $O(T)$, where $T$ is the number of training rounds, similarly as in the case of central aggregation.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Matryoshka Model Learning for Improved Elastic Student Models</title>
<link>https://arxiv.org/abs/2505.23337</link>
<guid>https://arxiv.org/abs/2505.23337</guid>
<content:encoded><![CDATA[
arXiv:2505.23337v3 Announce Type: replace 
Abstract: Industry-grade ML models are carefully designed to meet rapidly evolving serving constraints, which requires significant resources for model development. In this paper, we propose MatTA, a framework for training multiple accurate Student models using a novel Teacher-TA-Student recipe. TA models are larger versions of the Student models with higher capacity, and thus allow Student models to better relate to the Teacher model and also bring in more domain-specific expertise. Furthermore, multiple accurate Student models can be extracted from the TA model. Therefore, despite only one training run, our methodology provides multiple servable options to trade off accuracy for lower serving cost. We demonstrate the proposed method, MatTA, on proprietary datasets and models. Its practical efficacy is underscored by live A/B tests within a production ML system, demonstrating 20% improvement on a key metric. We also demonstrate our method on GPT-2 Medium, a public model, and achieve relative improvements of over 24% on SAT Math and over 10% on the LAMBADA benchmark.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the Closed-Form of Flow Matching: Generalization Does Not Arise from Target Stochasticity</title>
<link>https://arxiv.org/abs/2506.03719</link>
<guid>https://arxiv.org/abs/2506.03719</guid>
<content:encoded><![CDATA[
arXiv:2506.03719v2 Announce Type: replace 
Abstract: Modern deep generative models can now produce high-quality synthetic samples that are often indistinguishable from real training data. A growing body of research aims to understand why recent methods, such as diffusion and flow matching techniques, generalize so effectively. Among the proposed explanations are the inductive biases of deep learning architectures and the stochastic nature of the conditional flow matching loss. In this work, we rule out the noisy nature of the loss as a key factor driving generalization in flow matching. First, we empirically show that in high-dimensional settings, the stochastic and closed-form versions of the flow matching loss yield nearly equivalent losses. Then, using state-of-the-art flow matching models on standard image datasets, we demonstrate that both variants achieve comparable statistical performance, with the surprising observation that using the closed-form can even improve performance.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Revisiting Clustering of Neural Bandits: Selective Reinitialization for Mitigating Loss of Plasticity</title>
<link>https://arxiv.org/abs/2506.12389</link>
<guid>https://arxiv.org/abs/2506.12389</guid>
<content:encoded><![CDATA[
arXiv:2506.12389v3 Announce Type: replace 
Abstract: Clustering of Bandits (CB) methods enhance sequential decision-making by grouping bandits into clusters based on similarity and incorporating cluster-level contextual information, demonstrating effectiveness and adaptability in applications like personalized streaming recommendations. However, when extending CB algorithms to their neural version (commonly referred to as Clustering of Neural Bandits, or CNB), they suffer from loss of plasticity, where neural network parameters become rigid and less adaptable over time, limiting their ability to adapt to non-stationary environments (e.g., dynamic user preferences in recommendation). To address this challenge, we propose Selective Reinitialization (SeRe), a novel bandit learning framework that dynamically preserves the adaptability of CNB algorithms in evolving environments. SeRe leverages a contribution utility metric to identify and selectively reset underutilized units, mitigating loss of plasticity while maintaining stable knowledge retention. Furthermore, when combining SeRe with CNB algorithms, the adaptive change detection mechanism adjusts the reinitialization frequency according to the degree of non-stationarity, ensuring effective adaptation without unnecessary resets. Theoretically, we prove that SeRe enables sublinear cumulative regret in piecewise-stationary environments, outperforming traditional CNB approaches in long-term performances. Extensive experiments on six real-world recommendation datasets demonstrate that SeRe-enhanced CNB algorithms can effectively mitigate the loss of plasticity with lower regrets, improving adaptability and robustness in dynamic settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Knowledge Adaptation as Posterior Correction</title>
<link>https://arxiv.org/abs/2506.14262</link>
<guid>https://arxiv.org/abs/2506.14262</guid>
<content:encoded><![CDATA[
arXiv:2506.14262v2 Announce Type: replace 
Abstract: Adaptation is the holy grail of intelligence, but even the best AI models lack the adaptability of toddlers. In spite of great progress, little is known about the mechanisms by which machines can learn to adapt as fast as humans and animals. Here, we cast adaptation as `correction' of old posteriors and show that a wide-variety of existing adaptation methods follow this very principle, including those used for continual learning, federated learning, unlearning, and model merging. In all these settings, more accurate posteriors often lead to smaller corrections and can enable faster adaptation. Posterior correction is derived by using the dual representation of the Bayesian Learning Rule of Khan and Rue (2023), where the interference between the old representation and new information is quantified by using the natural-gradient mismatch. We present many examples demonstrating how machines can learn to adapt quickly by using posterior correction.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Training a Scientific Reasoning Model for Chemistry</title>
<link>https://arxiv.org/abs/2506.17238</link>
<guid>https://arxiv.org/abs/2506.17238</guid>
<content:encoded><![CDATA[
arXiv:2506.17238v2 Announce Type: replace 
Abstract: Reasoning models are large language models that emit a long chain-of-thought before answering, providing both higher accuracy and explicit reasoning for their response. A major question has been whether language model reasoning generalizes beyond mathematics, programming, and logic, where most previous work has focused. We demonstrate that reasoning models can be post-trained for chemistry without additional domain pretraining, and require substantially less data compared to contemporary domain-specific models. We report ether0, a 24B parameter LLM (based on Mistral-Small-24B) that can reason in natural language and respond with chemical structures. This reasoning model was trained with reinforcement learning on 640,730 experimentally-grounded chemistry problems across 375 tasks ranging from synthesizability, to blood-brain barrier permeability, to human receptor activity, to scent. Our model exceeds general-purpose chemistry models, frontier models, and human experts on molecular design tasks. It is also more data efficient relative to specialized models. We anticipate that this method can be applied to train data-efficient language models specialized for tasks across a wide variety of scientific domains.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>scE2TM improves single-cell embedding interpretability and reveals cellular perturbation signatures</title>
<link>https://arxiv.org/abs/2507.08355</link>
<guid>https://arxiv.org/abs/2507.08355</guid>
<content:encoded><![CDATA[
arXiv:2507.08355v2 Announce Type: replace 
Abstract: Single-cell RNA sequencing technologies have revolutionized our understanding of cellular heterogeneity, yet computational methods often struggle to balance performance with biological interpretability. Embedded topic models have been widely used for interpretable single-cell embedding learning. However, these models suffer from the potential problem of interpretation collapse, where topics semantically collapse towards each other, resulting in redundant topics and incomplete capture of biological variation. Furthermore, the rise of single-cell foundation models creates opportunities to harness external biological knowledge for guiding model embeddings. Here, we present scE2TM, an external knowledge-guided embedded topic model that provides a high-quality cell embedding and interpretation for scRNA-seq analysis. Through embedding clustering regularization method, each topic is constrained to be the center of a separately aggregated gene cluster, enabling it to capture unique biological information. Across 20 scRNA-seq datasets, scE2TM achieves superior clustering performance compared with seven state-of-the-art methods. A comprehensive interpretability benchmark further shows that scE2TM-learned topics exhibit higher diversity and stronger consistency with underlying biological pathways. Modeling interferon-stimulated PBMCs, scE2TM simulates topic perturbations that drive control cells toward stimulated-like transcriptional states, faithfully mirroring experimental interferon responses. In melanoma, scE2TM identifies malignant-specific topics and extrapolates them to unseen patient data, revealing gene programs associated with patient survival.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Instruction-based Time Series Editing</title>
<link>https://arxiv.org/abs/2508.01504</link>
<guid>https://arxiv.org/abs/2508.01504</guid>
<content:encoded><![CDATA[
arXiv:2508.01504v2 Announce Type: replace 
Abstract: In time series editing, we aim to modify some properties of a given time series without altering others. For example, when analyzing a hospital patient's blood pressure, we may add a sudden early drop and observe how it impacts their future while preserving other conditions. Existing diffusion-based editors rely on rigid, predefined attribute vectors as conditions and produce all-or-nothing edits through sampling. This attribute- and sampling-based approach limits flexibility in condition format and lacks customizable control over editing strength. To overcome these limitations, we introduce Instruction-based Time Series Editing, where users specify intended edits using natural language. This allows users to express a wider range of edits in a more accessible format. We then introduce InstructTime, the first instruction-based time series editor. InstructTime takes in time series and instructions, embeds them into a shared multi-modal representation space, then decodes their embeddings to generate edited time series. By learning a structured multi-modal representation space, we can easily interpolate between embeddings to achieve varying degrees of edit. To handle local and global edits together, we propose multi-resolution encoders. In our experiments, we use synthetic and real datasets and find that InstructTime is a state-of-the-art time series editor: InstructTime achieves high-quality edits with controllable strength, can generalize to unseen instructions, and can be easily adapted to unseen conditions through few-shot learning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Feature Selection based on Rule-based Learning for Explainable Classification with Uncertainty Quantification</title>
<link>https://arxiv.org/abs/2508.02566</link>
<guid>https://arxiv.org/abs/2508.02566</guid>
<content:encoded><![CDATA[
arXiv:2508.02566v2 Announce Type: replace 
Abstract: Dynamic feature selection (DFS) offers a compelling alternative to traditional, static feature selection by adapting the selected features to each individual sample. This provides insights into the decision-making process for each case, which makes DFS especially significant in settings where decision transparency is key, i.e., clinical decisions. However, existing DFS methods use opaque models, which hinder their applicability in real-life scenarios. DFS also introduces new own sources of uncertainty compared to the static setting, which is also not considered in the existing literature. In this paper, we formalize the additional sources of uncertainty in DFS, and give formulas to estimate them. We also propose novel approach by leveraging a rule-based system as a base classifier for the DFS process, which enhances decision interpretability compared to neural estimators. Finally, we demonstrate the competitive performance of our rule-based DFS approach against established and state-of-the-art greedy and reinforcement learning methods, which are mostly considered opaque, compared to our explainable rulebased system.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Increasing Information Extraction in Low-Signal Regimes via Multiple Instance Learning</title>
<link>https://arxiv.org/abs/2508.07114</link>
<guid>https://arxiv.org/abs/2508.07114</guid>
<content:encoded><![CDATA[
arXiv:2508.07114v2 Announce Type: replace 
Abstract: In this work, we introduce a new information-theoretic perspective on Multiple Instance Learning (MIL) for parameter estimation with i.i.d. data, and show that MIL can outperform single-instance learners in low-signal regimes. Prior work [Nachman and Thaler, 2021] argued that single-instance methods are often sufficient, but this conclusion presumes enough single-instance signal to train near-optimal classifiers. We demonstrate that even state-of-the-art single-instance models can fail to reach optimal classifier performance in challenging low-signal regimes, whereas MIL can mitigate this sub-optimality. As a concrete application, we constrain Wilson coefficients of the Standard Model Effective Field Theory (SMEFT) using kinematic information from subatomic particle collision events at the Large Hadron Collider (LHC). In experiments, we observe that under specific modeling and weak signal conditions, pooling instances can increase the effective Fisher information compared to single-instance approaches.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bridging Human and LLM Judgments: Understanding and Narrowing the Gap</title>
<link>https://arxiv.org/abs/2508.12792</link>
<guid>https://arxiv.org/abs/2508.12792</guid>
<content:encoded><![CDATA[
arXiv:2508.12792v2 Announce Type: replace 
Abstract: Large language models are increasingly used as judges (LLM-as-a-judge) to evaluate model outputs at scale, but their assessments often diverge systematically from human judgments. We present Bridge, a unified statistical framework that explicitly bridges human and LLM evaluations under both absolute scoring and pairwise comparison paradigms. Bridge posits a latent human preference score for each prompt-response pair and models LLM deviations as linear transformations of covariates that capture sources of discrepancies. This offers a simple and principled framework for refining LLM ratings and characterizing systematic discrepancies between humans and LLMs. We provide an efficient fitting algorithm with asymptotic guarantees for statistical inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot Arena), Bridge achieves higher agreement with human ratings (accuracy, calibration, and KL divergence) and exposes systematic human-LLM gaps.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convergent Reinforcement Learning Algorithms for Stochastic Shortest Path Problem</title>
<link>https://arxiv.org/abs/2508.13963</link>
<guid>https://arxiv.org/abs/2508.13963</guid>
<content:encoded><![CDATA[
arXiv:2508.13963v2 Announce Type: replace 
Abstract: In this paper we propose two algorithms in the tabular setting and an algorithm for the function approximation setting for the Stochastic Shortest Path (SSP) problem. SSP problems form an important class of problems in Reinforcement Learning (RL), as other types of cost-criteria in RL can be formulated in the setting of SSP. We show asymptotic almost-sure convergence for all our algorithms. We observe superior performance of our tabular algorithms compared to other well-known convergent RL algorithms. We further observe reliable performance of our function approximation algorithm compared to other algorithms in the function approximation setting.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values</title>
<link>https://arxiv.org/abs/2508.14083</link>
<guid>https://arxiv.org/abs/2508.14083</guid>
<content:encoded><![CDATA[
arXiv:2508.14083v2 Announce Type: replace 
Abstract: The ubiquity of missing data in urban intelligence systems, attributable to adverse environmental conditions and equipment failures, poses a significant challenge to the efficacy of downstream applications, notably in the realms of traffic forecasting and energy consumption prediction.
  Therefore, it is imperative to develop a robust spatio-temporal learning methodology capable of extracting meaningful insights from incomplete datasets. Despite the existence of methodologies for spatio-temporal graph forecasting in the presence of missing values, unresolved issues persist.
  Primarily, the majority of extant research is predicated on time-series analysis, thereby neglecting the dynamic spatial correlations inherent in sensor networks.
  Additionally, the complexity of missing data patterns compounds the intricacy of the problem.
  Furthermore, the variability in maintenance conditions results in a significant fluctuation in the ratio and pattern of missing values, thereby challenging the generalizability of predictive models.
  In response to these challenges, this study introduces GeoMAE, a self-supervised spatio-temporal representation learning model.
  The model is comprised of three principal components: an input preprocessing module, an attention-based spatio-temporal forecasting network (STAFN), and an auxiliary learning task, which draws inspiration from Masking AutoEncoders to enhance the robustness of spatio-temporal representation learning.
  Empirical evaluations on real-world datasets demonstrate that GeoMAE significantly outperforms existing benchmarks, achieving up to 13.20\% relative improvement over the best baseline models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Implicit Hypergraph Neural Network</title>
<link>https://arxiv.org/abs/2508.14101</link>
<guid>https://arxiv.org/abs/2508.14101</guid>
<content:encoded><![CDATA[
arXiv:2508.14101v2 Announce Type: replace 
Abstract: Hypergraphs offer a generalized framework for capturing high-order relationships between entities and have been widely applied in various domains, including healthcare, social networks, and bioinformatics. Hypergraph neural networks, which rely on message-passing between nodes over hyperedges to learn latent representations, have emerged as the method of choice for predictive tasks in many of these domains. These approaches typically perform only a small number of message-passing rounds to learn the representations, which they then utilize for predictions. The small number of message-passing rounds comes at a cost, as the representations only capture local information and forego long-range high-order dependencies. However, as we demonstrate, blindly increasing the message-passing rounds to capture long-range dependency also degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture long-range dependencies in standard graphs while maintaining performance. Despite their popularity, prior work has not studied long-range dependency issues on hypergraph neural networks. Here, we first demonstrate that existing hypergraph neural networks lose predictive power when aggregating more information to capture long-range dependency. We then propose Implicit Hypergraph Neural Network (IHNN), a novel framework that jointly learns fixed-point representations for both nodes and hyperedges in an end-to-end manner to alleviate this issue. Leveraging implicit differentiation, we introduce a tractable projected gradient descent approach to train the model efficiently. Extensive experiments on real-world hypergraphs for node classification demonstrate that IHNN outperforms the closest prior works in most settings, establishing a new state-of-the-art in hypergraph learning.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Amortized Sampling with Transferable Normalizing Flows</title>
<link>https://arxiv.org/abs/2508.18175</link>
<guid>https://arxiv.org/abs/2508.18175</guid>
<content:encoded><![CDATA[
arXiv:2508.18175v2 Announce Type: replace 
Abstract: Efficient equilibrium sampling of molecular conformations remains a core challenge in computational chemistry and statistical inference. Classical approaches such as molecular dynamics or Markov chain Monte Carlo inherently lack amortization; the computational cost of sampling must be paid in full for each system of interest. The widespread success of generative models has inspired interest towards overcoming this limitation through learning sampling algorithms. Despite performing competitively with conventional methods when trained on a single system, learned samplers have so far demonstrated limited ability to transfer across systems. We demonstrate that deep learning enables the design of scalable and transferable samplers by introducing Prose, a 285 million parameter all-atom transferable normalizing flow trained on a corpus of peptide molecular dynamics trajectories up to 8 residues in length. Prose draws zero-shot uncorrelated proposal samples for arbitrary peptide systems, achieving the previously intractable transferability across sequence length, whilst retaining the efficient likelihood evaluation of normalizing flows. Through extensive empirical evaluation we demonstrate the efficacy of Prose as a proposal for a variety of sampling algorithms, finding a simple importance sampling-based finetuning procedure to achieve competitive performance to established methods such as sequential Monte Carlo. We open-source the Prose codebase, model weights, and training dataset, to further stimulate research into amortized sampling methods and finetuning objectives.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Simulating classification models to evaluate Predict-Then-Optimize methods</title>
<link>https://arxiv.org/abs/2509.02191</link>
<guid>https://arxiv.org/abs/2509.02191</guid>
<content:encoded><![CDATA[
arXiv:2509.02191v2 Announce Type: replace 
Abstract: Uncertainty in optimization is often represented as stochastic parameters in the optimization model. In Predict-Then-Optimize approaches, predictions of a machine learning model are used as values for such parameters, effectively transforming the stochastic optimization problem into a deterministic one. This two-stage framework is built on the assumption that more accurate predictions result in solutions that are closer to the actual optimal solution. However, providing evidence for this assumption in the context of complex, constrained optimization problems is challenging and often overlooked in the literature. Simulating predictions of machine learning models offers a way to (experimentally) analyze how prediction error impacts solution quality without the need to train real models. Complementing an algorithm from the literature for simulating binary classification, we introduce a new algorithm for simulating predictions of multiclass classifiers. We conduct a computational study to evaluate the performance of these algorithms, and show that classifier performance can be simulated with reasonable accuracy, although some variability is observed. Additionally, we apply these algorithms to assess the performance of a Predict-Then-Optimize algorithm for a machine scheduling problem. The experiments demonstrate that the relationship between prediction error and how close solutions are to the actual optimum is non-trivial, highlighting important considerations for the design and evaluation of decision-making systems based on machine learning predictions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CPEP: Contrastive Pose-EMG Pre-training Enhances Gesture Generalization on EMG Signals</title>
<link>https://arxiv.org/abs/2509.04699</link>
<guid>https://arxiv.org/abs/2509.04699</guid>
<content:encoded><![CDATA[
arXiv:2509.04699v3 Announce Type: replace 
Abstract: Hand gesture classification using high-quality structured data such as videos, images, and hand skeletons is a well-explored problem in computer vision. Leveraging low-power, cost-effective biosignals, e.g. surface electromyography (sEMG), allows for continuous gesture prediction on wearables. In this paper, we demonstrate that learning representations from weak-modality data that are aligned with those from structured, high-quality data can improve representation quality and enables zero-shot classification. Specifically, we propose a Contrastive Pose-EMG Pre-training (CPEP) framework to align EMG and pose representations, where we learn an EMG encoder that produces high-quality and pose-informative representations. We assess the gesture classification performance of our model through linear probing and zero-shot setups. Our model outperforms emg2pose benchmark models by up to 21% on in-distribution gesture classification and 72% on unseen (out-of-distribution) gesture classification.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Spontaneous Kolmogorov-Arnold Geometry in Shallow MLPs</title>
<link>https://arxiv.org/abs/2509.12326</link>
<guid>https://arxiv.org/abs/2509.12326</guid>
<content:encoded><![CDATA[
arXiv:2509.12326v2 Announce Type: replace 
Abstract: The Kolmogorov-Arnold (KA) representation theorem constructs universal, but highly non-smooth inner functions (the first layer map) in a single (non-linear) hidden layer neural network. Such universal functions have a distinctive local geometry, a "texture," which can be characterized by the inner function's Jacobian $J({\mathbf{x}})$, as $\mathbf{x}$ varies over the data. It is natural to ask if this distinctive KA geometry emerges through conventional neural network optimization. We find that indeed KA geometry often is produced when training vanilla single hidden layer neural networks. We quantify KA geometry through the statistical properties of the exterior powers of $J(\mathbf{x})$: number of zero rows and various observables for the minor statistics of $J(\mathbf{x})$, which measure the scale and axis alignment of $J(\mathbf{x})$. This leads to a rough understanding for where KA geometry occurs in the space of function complexity and model hyperparameters. The motivation is first to understand how neural networks organically learn to prepare input data for later downstream processing and, second, to learn enough about the emergence of KA geometry to accelerate learning through a timely intervention in network hyperparameters. This research is the "flip side" of KA-Networks (KANs). We do not engineer KA into the neural network, but rather watch KA emerge in shallow MLPs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SimpleFold: Folding Proteins is Simpler than You Think</title>
<link>https://arxiv.org/abs/2509.18480</link>
<guid>https://arxiv.org/abs/2509.18480</guid>
<content:encoded><![CDATA[
arXiv:2509.18480v3 Announce Type: replace 
Abstract: Protein folding models have achieved groundbreaking results typically via a combination of integrating domain knowledge into the architectural blocks and training pipelines. Nonetheless, given the success of generative models across different but related problems, it is natural to question whether these architectural designs are a necessary condition to build performant models. In this paper, we introduce SimpleFold, the first flow-matching based protein folding model that solely uses general purpose transformer blocks. Protein folding models typically employ computationally expensive modules involving triangular updates, explicit pair representations or multiple training objectives curated for this specific domain. Instead, SimpleFold employs standard transformer blocks with adaptive layers and is trained via a generative flow-matching objective with an additional structural term. We scale SimpleFold to 3B parameters and train it on approximately 9M distilled protein structures together with experimental PDB data. On standard folding benchmarks, SimpleFold-3B achieves competitive performance compared to state-of-the-art baselines, in addition SimpleFold demonstrates strong performance in ensemble prediction which is typically difficult for models trained via deterministic reconstruction objectives. Due to its general-purpose architecture, SimpleFold shows efficiency in deployment and inference on consumer-level hardware. SimpleFold challenges the reliance on complex domain-specific architectures designs in protein folding, opening up an alternative design space for future progress.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BoreaRL: A Multi-Objective Reinforcement Learning Environment for Climate-Adaptive Boreal Forest Management</title>
<link>https://arxiv.org/abs/2509.19846</link>
<guid>https://arxiv.org/abs/2509.19846</guid>
<content:encoded><![CDATA[
arXiv:2509.19846v2 Announce Type: replace 
Abstract: Boreal forests store 30-40\% of terrestrial carbon, much in climate-vulnerable permafrost soils, making their management critical for climate mitigation. However, optimizing forest management for both carbon sequestration and permafrost preservation presents complex trade-offs that current tools cannot adequately address. We introduce BoreaRL, the first multi-objective reinforcement learning environment for climate-adaptive boreal forest management, featuring a physically-grounded simulator of coupled energy, carbon, and water fluxes. BoreaRL supports two training paradigms: site-specific mode for controlled studies and generalist mode for learning robust policies under environmental stochasticity. Through evaluation of multi-objective RL algorithms, we reveal a fundamental asymmetry in learning difficulty: carbon objectives are significantly easier to optimize than thaw (permafrost preservation) objectives, with thaw-focused policies showing minimal learning progress across both paradigms. In generalist settings, standard gradient-descent based preference-conditioned approaches fail, while a naive site selection approach achieves superior performance by strategically selecting training episodes. Analysis of learned strategies reveals distinct management philosophies, where carbon-focused policies favor aggressive high-density coniferous stands, while effective multi-objective policies balance species composition and density to protect permafrost while maintaining carbon gains. Our results demonstrate that robust climate-adaptive forest management remains challenging for current MORL methods, establishing BoreaRL as a valuable benchmark for developing more effective approaches. We open-source BoreaRL to accelerate research in multi-objective RL for climate applications.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Offline Goal-conditioned Reinforcement Learning with Quasimetric Representations</title>
<link>https://arxiv.org/abs/2509.20478</link>
<guid>https://arxiv.org/abs/2509.20478</guid>
<content:encoded><![CDATA[
arXiv:2509.20478v2 Announce Type: replace 
Abstract: Approaches for goal-conditioned reinforcement learning (GCRL) often use learned state representations to extract goal-reaching policies. Two frameworks for representation structure have yielded particularly effective GCRL algorithms: (1) *contrastive representations*, in which methods learn "successor features" with a contrastive objective that performs inference over future outcomes, and (2) *temporal distances*, which link the (quasimetric) distance in representation space to the transit time from states to goals. We propose an approach that unifies these two frameworks, using the structure of a quasimetric representation space (triangle inequality) with the right additional constraints to learn successor representations that enable optimal goal-reaching. Unlike past work, our approach is able to exploit a **quasimetric** distance parameterization to learn **optimal** goal-reaching distances, even with **suboptimal** data and in **stochastic** environments. This gives us the best of both worlds: we retain the stability and long-horizon capabilities of Monte Carlo contrastive RL methods, while getting the free stitching capabilities of quasimetric network parameterizations. On existing offline GCRL benchmarks, our representation learning objective improves performance on stitching tasks where methods based on contrastive learning struggle, and on noisy, high-dimensional environments where methods based on quasimetric networks struggle.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Scaling Laws of Feature Emergence from Learning Dynamics of Grokking</title>
<link>https://arxiv.org/abs/2509.21519</link>
<guid>https://arxiv.org/abs/2509.21519</guid>
<content:encoded><![CDATA[
arXiv:2509.21519v5 Announce Type: replace 
Abstract: While the phenomenon of grokking, i.e., delayed generalization, has been studied extensively, it remains an open problem whether there is a mathematical framework that characterizes what kind of features will emerge, how and in which conditions it happens, and is closely related to the gradient dynamics of the training, for complex structured inputs. We propose a novel framework, named $\mathbf{Li}_2$, that captures three key stages for the grokking behavior of 2-layer nonlinear networks: (I) Lazy learning, (II) independent feature learning and (III) interactive feature learning. At the lazy learning stage, top layer overfits to random hidden representation and the model appears to memorize, and at the same time, the backpropagated gradient $G_F$ from the top layer now carries information about the target label, with a specific structure that enables each hidden node to learn their representation independently. Interestingly, the independent dynamics follows exactly the gradient ascent of an energy function $E$, and its local maxima are precisely the emerging features. We study whether these local-optima induced features are generalizable, their representation power, and how they change on sample size, in group arithmetic tasks. When hidden nodes start to interact in the later stage of learning, we provably show how $G_F$ changes to focus on missing features that need to be learned. Our study sheds lights on roles played by key hyperparameters such as weight decay, learning rate and sample sizes in grokking, leads to provable scaling laws of feature emergence, memorization and generalization, and reveals why recent optimizers such as Muon can be effective, from the first principles of gradient dynamics. Our analysis can be extended to multi-layers. The code is available at https://github.com/yuandong-tian/understanding/tree/main/ssl/real-dataset/cogo.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Diffusion for Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2509.23846</link>
<guid>https://arxiv.org/abs/2509.23846</guid>
<content:encoded><![CDATA[
arXiv:2509.23846v2 Announce Type: replace 
Abstract: Robustness to modeling errors and uncertainties remains a central challenge in reinforcement learning (RL). In this work, we address this challenge by leveraging diffusion models to train robust RL policies. Diffusion models have recently gained popularity in model-based RL due to their ability to generate full trajectories "all at once", mitigating the compounding errors typical of step-by-step transition models. Moreover, they can be conditioned to sample from specific distributions, making them highly flexible. We leverage conditional sampling to learn policies that are robust to uncertainty in environment dynamics. Building on the established connection between Conditional Value at Risk (CVaR) optimization and robust RL, we introduce Adversarial Diffusion for Robust Reinforcement Learning (AD-RRL). AD-RRL guides the diffusion process to generate worst-case trajectories during training, effectively optimizing the CVaR of the cumulative return. Empirical results across standard benchmarks show that AD-RRL achieves superior robustness and performance compared to existing robust RL methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ChessArena: A Chess Testbed for Evaluating Strategic Reasoning Capabilities of Large Language Models</title>
<link>https://arxiv.org/abs/2509.24239</link>
<guid>https://arxiv.org/abs/2509.24239</guid>
<content:encoded><![CDATA[
arXiv:2509.24239v3 Announce Type: replace 
Abstract: Recent large language models (LLMs) have shown strong reasoning capabilities. However, a critical question remains: do these models possess genuine reasoning skills particularly complex strategic reasoning or are they primarily excelling at sophisticated pattern recognition within their training data? To address this question, this paper presents a chess testbed, ChessArena, to evaluate the strategic reasoning capabilities of LLMs. Chess requires complex strategic reasoning capabilities including long-term planning, strict rule comprehension, and multi-turn conversation memorization. Specifically, ChessArena is a competitive framework where LLMs play against each other, under four different play modes. The testbed is equipped with a ranking algorithm and a leaderboard. The testbed can also evaluate fine-grained capabilities including basic understanding, move selection, and puzzle solving. Over 13 LLMs with different modes are evaluated in ChessArena, playing over 800 games. The results reveal significant shortcomings in current LLMs: no model can beat Maia-1100 (a chess engine at human amateur level), while some even failed to defeat a random player that selects moves arbitrarily. We also present a strong baseline to the testbed: our fine-tuned Qwen3-8B substantially improved performance, approaching much larger state-of-the-art reasoning models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLASS Flows: Transition Sampling for Alignment of Flow and Diffusion Models</title>
<link>https://arxiv.org/abs/2509.25170</link>
<guid>https://arxiv.org/abs/2509.25170</guid>
<content:encoded><![CDATA[
arXiv:2509.25170v2 Announce Type: replace 
Abstract: The performance of flow matching and diffusion models can be greatly improved at inference time using reward alignment algorithms, yet efficiency remains a major limitation. While several algorithms were proposed, we demonstrate that a common bottleneck is the sampling method these algorithms rely on: many algorithms require to sample Markov transitions via SDE sampling, which is significantly less efficient and often less performant than ODE sampling. To remove this bottleneck, we introduce GLASS Flows, a new sampling paradigm that simulates a "flow matching model within a flow matching model" to sample Markov transitions. As we show in this work, this "inner" flow matching model can be retrieved from a pre-trained model without any re-training, combining the efficiency of ODEs with the stochastic evolution of SDEs. On large-scale text-to-image models, we show that GLASS Flows eliminate the trade-off between stochastic evolution and efficiency. Combined with Feynman-Kac Steering, GLASS Flows improve state-of-the-art performance in text-to-image generation, making it a simple, drop-in solution for inference-time scaling of flow and diffusion models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>LLM-NAS: LLM-driven Hardware-Aware Neural Architecture Search</title>
<link>https://arxiv.org/abs/2510.01472</link>
<guid>https://arxiv.org/abs/2510.01472</guid>
<content:encoded><![CDATA[
arXiv:2510.01472v3 Announce Type: replace 
Abstract: Hardware-Aware Neural Architecture Search (HW-NAS) requires joint optimization of accuracy and latency under device constraints. Traditional supernet-based methods require multiple GPU days per dataset. Large Language Model (LLM)-driven approaches avoid training a large supernet and can provide quick feedback, but we observe an exploration bias: the LLM repeatedly proposes neural network designs within limited search space and fails to discover architectures across different latency ranges in the entire search space. To address this issue, we propose LLM-NAS: an LLM-driven Neural Architecture Search that can generate neural networks with high accuracy and low latency with reduced search cost. Our proposed LLM-NAS has three key components: 1) a complexity-driven partitioning engine that divides the search space by complexity to enforce diversity and mitigate exploration bias; 2) an LLM-powered architecture prompt co-evolution operator, in which the LLM first updates a knowledge base of design heuristics based on results from the previous round, then performs a guided evolution algorithm on architectures with prompts that incorporate this knowledge base. Prompts and designs improve together across rounds which avoids random guesswork and improve efficiency; 3) a zero-cost predictor to avoid training a large number of candidates from scratch. Experimental results show that on HW-NAS-Bench, LLM-NAS can achieve overall higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Meanwhile, the search cost drops from days to minutes compared with traditional supernet baselines.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reference Grounded Skill Discovery</title>
<link>https://arxiv.org/abs/2510.06203</link>
<guid>https://arxiv.org/abs/2510.06203</guid>
<content:encoded><![CDATA[
arXiv:2510.06203v2 Announce Type: replace 
Abstract: Scaling unsupervised skill discovery algorithms to high-DoF agents remains challenging. As dimensionality increases, the exploration space grows exponentially, while the manifold of meaningful skills remains limited. Therefore, semantic meaningfulness becomes essential to effectively guide exploration in high-dimensional spaces. In this work, we present **Reference-Grounded Skill Discovery (RGSD)**, a novel algorithm that grounds skill discovery in a semantically meaningful latent space using reference data. RGSD first performs contrastive pretraining to embed motions on a unit hypersphere, clustering each reference trajectory into a distinct direction. This grounding enables skill discovery to simultaneously involve both imitation of reference behaviors and the discovery of semantically related diverse behaviors. On a simulated SMPL humanoid with $359$-D observations and $69$-D actions, RGSD successfully imitates skills such as walking, running, punching, and sidestepping, while also discover variations of these behaviors. In downstream locomotion tasks, RGSD leverages the discovered skills to faithfully satisfy user-specified style commands and outperforms imitation-learning baselines, which often fail to maintain the commanded style. Overall, our results suggest that lightweight reference-grounding offers a practical path to discovering semantically rich and structured skills in high-DoF systems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Safety Game: Balancing Safe and Informative Conversations with Blackbox Agentic AI using LP Solvers</title>
<link>https://arxiv.org/abs/2510.09330</link>
<guid>https://arxiv.org/abs/2510.09330</guid>
<content:encoded><![CDATA[
arXiv:2510.09330v2 Announce Type: replace 
Abstract: Ensuring that large language models (LLMs) comply with safety requirements is a central challenge in AI deployment. Existing alignment approaches primarily operate during training, such as through fine-tuning or reinforcement learning from human feedback, but these methods are costly and inflexible, requiring retraining whenever new requirements arise. Recent efforts toward inference-time alignment mitigate some of these limitations but still assume access to model internals, which is impractical, and not suitable for third party stakeholders who do not have access to the models. In this work, we propose a model-independent, black-box framework for safety alignment that does not require retraining or access to the underlying LLM architecture. As a proof of concept, we address the problem of trading off between generating safe but uninformative answers versus helpful yet potentially risky ones. We formulate this dilemma as a two-player zero-sum game whose minimax equilibrium captures the optimal balance between safety and helpfulness. LLM agents operationalize this framework by leveraging a linear programming solver at inference time to compute equilibrium strategies. Our results demonstrate the feasibility of black-box safety alignment, offering a scalable and accessible pathway for stakeholders, including smaller organizations and entities in resource-constrained settings, to enforce safety across rapidly evolving LLM ecosystems.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rewarding the Journey, Not Just the Destination: A Composite Path and Answer Self-Scoring Reward Mechanism for Test-Time Reinforcement Learning</title>
<link>https://arxiv.org/abs/2510.17923</link>
<guid>https://arxiv.org/abs/2510.17923</guid>
<content:encoded><![CDATA[
arXiv:2510.17923v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has emerged as a powerful paradigm for advancing Large Language Models (LLMs), achieving remarkable performance in complex reasoning domains such as mathematics and code generation. However, current RL methods face a fundamental scalability bottleneck due to their heavy reliance on human-curated preference data or labeled datasets for reward modeling. To overcome this limitation, we explore RL on unlabeled data where models learn autonomously from continuous experience streams. The core challenge in this setting lies in reliable reward estimation without ground-truth supervision. Existing approaches like Test-Time RL address this through self-consistent consensus, but risk reinforcing incorrect pseudo-labels derived from majority voting. We introduce COMPASS (Composite Path and Answer Self-Scoring), a novel test-time reward mechanism that operates without external supervision. COMPASS integrates two complementary components: the Dual-Calibration Answer Reward (DCAR), which stabilizes training by establishing trustworthy pseudo-labels through confidence and credibility calibration, and the Decisive Path Reward (DPR), which directly optimizes the reasoning process quality beyond mere outcome supervision. By jointly reinforcing trustworthy consensus answers and highly decisive reasoning chains, the COMPASS systematically enhances the model's analytical capabilities. Extensive experiments show that COMPASS achieves significant and consistent performance gains across diverse reasoning tasks and model architectures, advancing a more scalable direction for LLMs to learn from continuous experience.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Risk-Averse Constrained Reinforcement Learning with Optimized Certainty Equivalents</title>
<link>https://arxiv.org/abs/2510.20199</link>
<guid>https://arxiv.org/abs/2510.20199</guid>
<content:encoded><![CDATA[
arXiv:2510.20199v2 Announce Type: replace 
Abstract: Constrained optimization provides a common framework for dealing with conflicting objectives in reinforcement learning (RL). In most of these settings, the objectives (and constraints) are expressed though the expected accumulated reward. However, this formulation neglects risky or even possibly catastrophic events at the tails of the reward distribution, and is often insufficient for high-stakes applications in which the risk involved in outliers is critical. In this work, we propose a framework for risk-aware constrained RL, which exhibits per-stage robustness properties jointly in reward values and time using optimized certainty equivalents (OCEs). Our framework ensures an exact equivalent to the original constrained problem within a parameterized strong Lagrangian duality framework under appropriate constraint qualifications, and yields a simple algorithmic recipe which can be wrapped around standard RL solvers, such as PPO. Lastly, we establish the convergence of the proposed algorithm under common assumptions, and verify the risk-aware properties of our approach through several numerical experiments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GAPO: Robust Advantage Estimation for Real-World Code LLMs</title>
<link>https://arxiv.org/abs/2510.21830</link>
<guid>https://arxiv.org/abs/2510.21830</guid>
<content:encoded><![CDATA[
arXiv:2510.21830v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) is widely used for post-training large language models (LLMs) in code editing, where group-relative methods like GRPO are popular for their critic-free, normalized advantage estimation. However, in real-world code-editing scenarios, reward distributions are often skewed with unpredictable outliers, leading to distorted advantage computation and increased noise. To address this issue, we propose Group Adaptive Policy Optimization (GAPO), which adaptively finds an outlier-free highest-density interval (HDI) per prompt and then uses the median of that interval as an adaptive Q to replace the group mean in advantage calculation. This adaptive Q robustly handles skewed distributions while remaining plug-and-play and efficient. We validate GAPO on nine instruction-tuned LLMs (3B-14B) using a large internal dataset of 51,844 real-world, history-aware code-editing tasks across 10 languages, demonstrating consistent improvements in exact match accuracy over GRPO and its variant DAPO. Code is publicly available.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Conformational Rank Conditioned Committees for Machine Learning-Assisted Directed Evolution</title>
<link>https://arxiv.org/abs/2510.24974</link>
<guid>https://arxiv.org/abs/2510.24974</guid>
<content:encoded><![CDATA[
arXiv:2510.24974v2 Announce Type: replace 
Abstract: Machine Learning-assisted directed evolution (MLDE) is a powerful tool for efficiently navigating antibody fitness landscapes. Many structure-aware MLDE pipelines rely on a single conformation or a single committee across all conformations, limiting their ability to separate conformational uncertainty from epistemic uncertainty. Here, we introduce a rank -conditioned committee (RCC) framework that leverages ranked conformations to assign a deep neural network committee per rank. This design enables a principled separation between epistemic uncertainty and conformational uncertainty. We validate our RCC-MLDE approach on SARS-CoV-2 antibody docking, demonstrating significant improvements over baseline strategies. Our results offer a scalable route for therapeutic antibody discovery while directly addressing the challenge of modeling conformational uncertainty.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MSAD: A Deep Dive into Model Selection for Time series Anomaly Detection</title>
<link>https://arxiv.org/abs/2510.26643</link>
<guid>https://arxiv.org/abs/2510.26643</guid>
<content:encoded><![CDATA[
arXiv:2510.26643v2 Announce Type: replace 
Abstract: Anomaly detection is a fundamental task for time series analytics with important implications for the downstream performance of many applications. Despite increasing academic interest and the large number of methods proposed in the literature, recent benchmarks and evaluation studies demonstrated that no overall best anomaly detection methods exist when applied to very heterogeneous time series datasets. Therefore, the only scalable and viable solution to solve anomaly detection over very different time series collected from diverse domains is to propose a model selection method that will select, based on time series characteristics, the best anomaly detection methods to run. Existing AutoML solutions are, unfortunately, not directly applicable to time series anomaly detection, and no evaluation of time series-based approaches for model selection exists. Towards that direction, this paper studies the performance of time series classification methods used as model selection for anomaly detection. In total, we evaluate 234 model configurations derived from 16 base classifiers across more than 1980 time series, and we propose the first extensive experimental evaluation of time series classification as model selection for anomaly detection. Our results demonstrate that model selection methods outperform every single anomaly detection method while being in the same order of magnitude regarding execution time. This evaluation is the first step to demonstrate the accuracy and efficiency of time series classification algorithms for anomaly detection, and represents a strong baseline that can then be used to guide the model selection step in general AutoML pipelines. Preprint version of an article accepted at the VLDB Journal.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2511.02802</link>
<guid>https://arxiv.org/abs/2511.02802</guid>
<content:encoded><![CDATA[
arXiv:2511.02802v3 Announce Type: replace 
Abstract: Tabular foundation models represent a growing paradigm in structured data learning, extending the benefits of large-scale pretraining to tabular domains. However, their adoption remains limited due to heterogeneous preprocessing pipelines, fragmented APIs, inconsistent fine-tuning procedures, and the absence of standardized evaluation for deployment-oriented metrics such as calibration and fairness. We present TabTune, a unified library that standardizes the complete workflow for tabular foundation models through a single interface. TabTune provides consistent access to seven state-of-the-art models supporting multiple adaptation strategies, including zero-shot inference, meta-learning, supervised fine-tuning (SFT), and parameter-efficient fine-tuning (PEFT). The framework automates model-aware preprocessing, manages architectural heterogeneity internally, and integrates evaluation modules for performance, calibration, and fairness. Designed for extensibility and reproducibility, TabTune enables consistent benchmarking of adaptation strategies of tabular foundation models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fast 3D Surrogate Modeling for Data Center Thermal Management</title>
<link>https://arxiv.org/abs/2511.11722</link>
<guid>https://arxiv.org/abs/2511.11722</guid>
<content:encoded><![CDATA[
arXiv:2511.11722v2 Announce Type: replace 
Abstract: Reducing energy consumption and carbon emissions in data centers by enabling real-time temperature prediction is critical for sustainability and operational efficiency. Achieving this requires accurate modeling of the 3D temperature field to capture airflow dynamics and thermal interactions under varying operating conditions. Traditional thermal CFD solvers, while accurate, are computationally expensive and require expert-crafted meshes and boundary conditions, making them impractical for real-time use. To address these limitations, we develop a vision-based surrogate modeling framework that operates directly on a 3D voxelized representation of the data center, incorporating server workloads, fan speeds, and HVAC temperature set points. We evaluate multiple architectures, including 3D CNN U-Net variants, a 3D Fourier Neural Operator, and 3D vision transformers, to map these thermal inputs to high-fidelity heat maps. Our results show that the surrogate models generalize across data center configurations and significantly speed up computations (20,000x), from hundreds of milliseconds to hours. This fast and accurate estimation of hot spots and temperature distribution enables real-time cooling control and workload redistribution, leading to substantial energy savings (7\%) and reduced carbon footprint.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GLOBE: Accurate and Generalizable PDE Surrogates using Domain-Inspired Architectures and Equivariances</title>
<link>https://arxiv.org/abs/2511.15856</link>
<guid>https://arxiv.org/abs/2511.15856</guid>
<content:encoded><![CDATA[
arXiv:2511.15856v3 Announce Type: replace 
Abstract: We introduce GLOBE, a new neural surrogate for homogeneous PDEs that draws inductive bias from boundary-element methods and equivariant ML. GLOBE represents solutions as superpositions of learnable Green's-function-like kernels evaluated from boundary faces to targets, composed across multiscale branches and communication hyperlayers. The architecture is translation-, rotation-, and parity-equivariant; discretization-invariant in the fine-mesh limit; and units-invariant via rigorous nondimensionalization. An explicit far-field decay envelope stabilizes extrapolation, boundary-to-boundary hyperlayer communication mediates long-range coupling, and the all-to-all boundary-to-target evaluation yields a global receptive field that respects PDE information flow, even for elliptic PDEs.
  On AirFRANS (steady incompressible RANS over NACA airfoils), GLOBE achieves substantial accuracy improvements. On the "Full" split, it reduces mean-squared error by roughly 200x on all fields relative to the dataset's reference baselines, and roughly 50x relative to the next-best-performing model. In the "Scarce" split, it achieves over 100x lower error on velocity and pressure fields and over 600x lower error on surface pressure than Transolver. Qualitative results show sharp near-wall gradients, coherent wakes, and limited errors under modest extrapolation in Reynolds number and angle of attack.
  In addition to this accuracy, the model is quite compact (117k parameters), and fields can be evaluated at arbitrary points during inference. We also demonstrate the ability to train and predict with non-watertight meshes, which has strong practical implications.
  These results show that rigorous physics- and domain-inspired inductive biases can achieve large gains in accuracy, generalizability, and practicality for ML-based PDE surrogates for industrial computer-aided engineering (CAE).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PersonaAgent with GraphRAG: Community-Aware Knowledge Graphs for Personalized LLM</title>
<link>https://arxiv.org/abs/2511.17467</link>
<guid>https://arxiv.org/abs/2511.17467</guid>
<content:encoded><![CDATA[
arXiv:2511.17467v2 Announce Type: replace 
Abstract: We propose a novel framework for persona-based language model system, motivated by the need for personalized AI agents that adapt to individual user preferences. In our approach, the agent embodies the user's "persona" (e.g. user profile or taste) and is powered by a large language model (LLM). To enable the agent to leverage rich contextual information, we introduce a Knowledge-Graph-enhanced Retrieval-Augmented Generation (Graph RAG) mechanism that constructs an LLM-derived graph index of relevant documents and summarizes communities of related information. Our framework generates personalized prompts by combining: (1) a summary of the user's historical behaviors and preferences extracted from the knowledge graph, and (2) relevant global interaction patterns identified through graph-based community detection. This dynamic prompt engineering approach allows the agent to maintain consistent persona-aligned behaviors while benefiting from collective knowledge. On the LaMP benchmark, our method improves news categorization F1 by 11.1%, movie tagging F1 by 56.1%, and reduces product rating MAE by 10.4% over prior methods. Our code is available at https://anonymous.4open.science/r/PersonaAgentwGraphRAG-DE6F
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-stationary and Varying-discounting Markov Decision Processes for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2511.17598</link>
<guid>https://arxiv.org/abs/2511.17598</guid>
<content:encoded><![CDATA[
arXiv:2511.17598v2 Announce Type: replace 
Abstract: Algorithms developed under stationary Markov Decision Processes (MDPs) often face challenges in non-stationary environments, and infinite-horizon formulations may not directly apply to finite-horizon tasks. To address these limitations, we introduce the Non-stationary and Varying-discounting MDP (NVMDP) framework, which naturally accommodates non-stationarity and allows discount rates to vary with time and transitions. Infinite-horizon, stationary MDPs emerge as special cases of NVMDPs for identifying an optimal policy, and finite-horizon MDPs are also subsumed within the NVMDP formulations. Moreover, NVMDPs provide a flexible mechanism to shape optimal policies, without altering the state space, action space, or the reward structure. We establish the theoretical foundations of NVMDPs, including assumptions, state- and action-value formulation and recursion, matrix representation, optimality conditions, and policy improvement under finite state and action spaces. Building on these results, we adapt dynamic programming and generalized Q-learning algorithms to NVMDPs, along with formal convergence proofs. For problems requiring function approximation, we extend the Policy Gradient Theorem and the policy improvement bound in Trust Region Policy Optimization (TRPO), offering proofs in both scalar and matrix forms. Empirical evaluations in a non-stationary gridworld environment demonstrate that NVMDP-based algorithms successfully recover optimal trajectories under multiple reward and discounting schemes, whereas original Q-learning fails. These results collectively show that NVMDPs provide a theoretically sound and practically effective framework for reinforcement learning, requiring only minor algorithmic modifications while enabling robust handling of non-stationarity and explicit optimal policy shaping.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AVA-VLA: Improving Vision-Language-Action models with Active Visual Attention</title>
<link>https://arxiv.org/abs/2511.18960</link>
<guid>https://arxiv.org/abs/2511.18960</guid>
<content:encoded><![CDATA[
arXiv:2511.18960v2 Announce Type: replace 
Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities in embodied AI tasks. However, existing VLA models, often built upon Vision-Language Models (VLMs), typically process dense visual inputs independently at each timestep. This approach implicitly models the task as a Markov Decision Process (MDP). However, this history-agnostic design is suboptimal for effective visual token processing in dynamic sequential decision-making, as it fails to leverage the context of history. To address this limitation, we reformulate the problem from a Partially Observable Markov Decision Process (POMDP) perspective and propose a novel framework named AVA-VLA. Inspired by the POMDP that the action generation should be conditioned on the belief state. AVA-VLA introduces Active Visual Attention (AVA) to dynamically modulate visual processing. It achieves this by leveraging the recurrent state, which is a neural approximation of the agent's belief state derived from the previous decision step. Specifically, the AVA module uses the recurrent state to compute the soft weights to actively process task-relevant visual tokens based on its historical context. Comprehensive evaluations demonstrate that AVA-VLA achieves state-of-the-art performance across popular robotic benchmarks, including LIBERO and CALVIN. Furthermore, real-world deployments on a dual-arm robot platform validate the framework's practical applicability and robust sim-to-real transferability.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Massively Multitask World Models for Continuous Control</title>
<link>https://arxiv.org/abs/2511.19584</link>
<guid>https://arxiv.org/abs/2511.19584</guid>
<content:encoded><![CDATA[
arXiv:2511.19584v2 Announce Type: replace 
Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DiCaP: Distribution-Calibrated Pseudo-labeling for Semi-Supervised Multi-Label Learning</title>
<link>https://arxiv.org/abs/2511.20225</link>
<guid>https://arxiv.org/abs/2511.20225</guid>
<content:encoded><![CDATA[
arXiv:2511.20225v2 Announce Type: replace 
Abstract: Semi-supervised multi-label learning (SSMLL) aims to address the challenge of limited labeled data in multi-label learning (MLL) by leveraging unlabeled data to improve the model's performance. While pseudo-labeling has become a dominant strategy in SSMLL, most existing methods assign equal weights to all pseudo-labels regardless of their quality, which can amplify the impact of noisy or uncertain predictions and degrade the overall performance. In this paper, we theoretically verify that the optimal weight for a pseudo-label should reflect its correctness likelihood. Empirically, we observe that on the same dataset, the correctness likelihood distribution of unlabeled data remains stable, even as the number of labeled training samples varies. Building on this insight, we propose Distribution-Calibrated Pseudo-labeling (DiCaP), a correctness-aware framework that estimates posterior precision to calibrate pseudo-label weights. We further introduce a dual-thresholding mechanism to separate confident and ambiguous regions: confident samples are pseudo-labeled and weighted accordingly, while ambiguous ones are explored by unsupervised contrastive learning. Experiments conducted on multiple benchmark datasets verify that our method achieves consistent improvements, surpassing state-of-the-art methods by up to 4.27%.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On SkipGram Word Embedding Models with Negative Sampling: Unified Framework and Impact of Noise Distributions</title>
<link>https://arxiv.org/abs/2009.04413</link>
<guid>https://arxiv.org/abs/2009.04413</guid>
<content:encoded><![CDATA[
arXiv:2009.04413v2 Announce Type: replace-cross 
Abstract: SkipGram word embedding models with negative sampling, or SGN in short, is an elegant family of word embedding models. In this paper, we formulate a framework for word embedding, referred to as Word-Context Classification (WCC), that generalizes SGN to a wide family of models. The framework, which uses some ``noise examples'', is justified through theoretical analysis. The impact of noise distribution on the learning of the WCC embedding models is studied experimentally, suggesting that the best noise distribution is, in fact, the data distribution, in terms of both the embedding performance and the speed of convergence during training. Along our way, we discover several novel embedding models that outperform existing WCC models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Predicting Human Perceptions of Robot Performance During Navigation Tasks</title>
<link>https://arxiv.org/abs/2310.11590</link>
<guid>https://arxiv.org/abs/2310.11590</guid>
<content:encoded><![CDATA[
arXiv:2310.11590v3 Announce Type: replace-cross 
Abstract: Understanding human perceptions of robot performance is crucial for designing socially intelligent robots that can adapt to human expectations. Current approaches often rely on surveys, which can disrupt ongoing human-robot interactions. As an alternative, we explore predicting people's perceptions of robot performance using non-verbal behavioral cues and machine learning techniques. We contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in Virtual Reality, together with perceptions of robot performance provided by users on a 5-point scale. We then analyze how well humans and supervised learning techniques can predict perceived robot performance based on different observation types (like facial expression and spatial behavior features). Our results suggest that facial expressions alone provide useful information, but in the navigation scenarios that we considered, reasoning about spatial features in context is critical for the prediction task. Also, supervised learning techniques outperformed humans' predictions in most cases. Further, when predicting robot performance as a binary classification task on unseen users' data, the F1-Score of machine learning models more than doubled that of predictions on a 5-point scale. This suggested good generalization capabilities, particularly in identifying performance directionality over exact ratings. Based on these findings, we conducted a real-world demonstration where a mobile robot uses a machine learning model to predict how a human who follows it perceives it. Finally, we discuss the implications of our results for implementing these supervised learning models in real-world navigation. Our work paves the path to automatically enhancing robot behavior based on observations of users and inferences about their perceptions of a robot.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ContourDiff: Unpaired Medical Image Translation with Structural Consistency</title>
<link>https://arxiv.org/abs/2403.10786</link>
<guid>https://arxiv.org/abs/2403.10786</guid>
<content:encoded><![CDATA[
arXiv:2403.10786v3 Announce Type: replace-cross 
Abstract: Accurately translating medical images between different modalities, such as Computed Tomography (CT) to Magnetic Resonance Imaging (MRI), has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff with Spatially Coherent Guided Diffusion (SCGD), a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of interest. By applying the contour as a constraint at every diffusion sampling step, we ensure the preservation of anatomical content. We evaluate our method on challenging lumbar spine and hip-and-thigh CT-to-MRI translation tasks, via (1) the performance of segmentation models trained on translated images applied to real MRIs, and (2) the foreground FID and KID of translated images with respect to real MRIs. Our method outperforms other unpaired image translation methods by a significant margin across almost all metrics and scenarios. Moreover, it achieves this without the need to access any input domain information during training and we further verify its zero-shot capability, showing that a model trained on one anatomical region can be directly applied to unseen regions without retraining (GitHub: https://github.com/mazurowski-lab/ContourDiff).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Lockpicking LLMs: A Logit-Based Jailbreak Using Token-level Manipulation</title>
<link>https://arxiv.org/abs/2405.13068</link>
<guid>https://arxiv.org/abs/2405.13068</guid>
<content:encoded><![CDATA[
arXiv:2405.13068v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have transformed the field of natural language processing, but they remain susceptible to jailbreaking attacks that exploit their capabilities to generate unintended and potentially harmful content. Existing token-level jailbreaking techniques, while effective, face scalability and efficiency challenges, especially as models undergo frequent updates and incorporate advanced defensive measures. In this paper, we introduce JailMine, an innovative token-level manipulation approach that addresses these limitations effectively. JailMine employs an automated "mining" process to elicit malicious responses from LLMs by strategically selecting affirmative outputs and iteratively reducing the likelihood of rejection. Through rigorous testing across multiple well-known LLMs and datasets, we demonstrate JailMine's effectiveness and efficiency, achieving a significant average reduction of 86% in time consumed while maintaining high success rates averaging 95%, even in the face of evolving defensive strategies. Our work contributes to the ongoing effort to assess and mitigate the vulnerability of LLMs to jailbreaking attacks, underscoring the importance of continued vigilance and proactive measures to enhance the security and reliability of these powerful language models.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Anomalous Change Point Detection Using Probabilistic Predictive Coding</title>
<link>https://arxiv.org/abs/2405.15727</link>
<guid>https://arxiv.org/abs/2405.15727</guid>
<content:encoded><![CDATA[
arXiv:2405.15727v2 Announce Type: replace-cross 
Abstract: Change point detection (CPD) and anomaly detection (AD) are essential techniques in various fields to identify abrupt changes or abnormal data instances. However, existing methods are often constrained to univariate data, face scalability challenges with large datasets due to computational demands, and experience reduced performance with high-dimensional or intricate data, as well as hidden anomalies. Furthermore, they often lack interpretability and adaptability to domain-specific knowledge, which limits their versatility across different fields. In this work, we propose a deep learning-based CPD/AD method called Probabilistic Predictive Coding (PPC) that jointly learns to encode sequential data to low-dimensional latent space representations and to predict the subsequent data representations as well as the corresponding prediction uncertainties. The model parameters are optimized with maximum likelihood estimation by comparing these predictions with the true encodings. At the time of application, the true and predicted encodings are used to determine the probability of conformance, an interpretable and meaningful anomaly score. Furthermore, our approach has linear time complexity, scalability issues are prevented, and the method can easily be adjusted to a wide range of data types and intricate applications. We demonstrate the effectiveness and adaptability of our proposed method across synthetic time series experiments, image data, and real-world magnetic resonance spectroscopic imaging data.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hidden in Plain Text: Emergence &amp; Mitigation of Steganographic Collusion in LLMs</title>
<link>https://arxiv.org/abs/2410.03768</link>
<guid>https://arxiv.org/abs/2410.03768</guid>
<content:encoded><![CDATA[
arXiv:2410.03768v2 Announce Type: replace-cross 
Abstract: The rapid proliferation of frontier model agents promises significant societal advances but also raises concerns about systemic risks arising from unsafe interactions. Collusion to the disadvantage of others has been identified as a central form of undesirable agent cooperation. The use of information hiding (steganography) in agent communications could render such collusion practically undetectable. This underscores the need for investigations into the possibility of such behaviours emerging and the robustness corresponding countermeasures. To investigate this problem we design two approaches -- a gradient-based reinforcement learning (GBRL) method and an in-context reinforcement learning (ICRL) method -- for reliably eliciting sophisticated LLM-generated linguistic text steganography.
  We demonstrate, for the first time, that unintended steganographic collusion in LLMs can arise due to mispecified reward incentives during training. Additionally, we find that standard mitigations -- both passive oversight of model outputs and active mitigation through communication paraphrasing -- are not fully effective at preventing this steganographic communication. Our findings imply that (i) emergence of steganographic collusion is a plausible concern that should be monitored and researched, and (ii) preventing emergence may require innovation in mitigation techniques.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Online Convex Optimization with Memory and Limited Predictions</title>
<link>https://arxiv.org/abs/2410.23574</link>
<guid>https://arxiv.org/abs/2410.23574</guid>
<content:encoded><![CDATA[
arXiv:2410.23574v2 Announce Type: replace-cross 
Abstract: This paper addresses an online convex optimization problem where the cost function at each step depends on a history of past decisions (i.e., memory), and the decision maker has access to limited predictions of future cost values within a finite window. The goal is to design an algorithm that minimizes the dynamic regret against the optimal sequence of decisions in hindsight. To this end, we propose a novel predictive algorithm and establish strong theoretical guarantees for its performance. We show that the algorithm's dynamic regret decays exponentially with the length of the prediction window. Our algorithm comprises two general subroutines of independent interest. The first subroutine solves online convex optimization with memory and bandit feedback, achieving a $\sqrt{TV_T}$-dynamic regret, where $V_T$ measures the variation of the optimal decision sequence. The second is a zeroth-order method that attains a linear convergence rate for general convex optimization, matching the best achievable rate of first-order methods. The key to our algorithm is a novel truncated Gaussian smoothing technique when querying the decision points to obtain the predictions. We validate our theoretical results with numerical experiments.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Exponential Separation Between Quantum and Quantum-Inspired Classical Algorithms for Linear Systems</title>
<link>https://arxiv.org/abs/2411.02087</link>
<guid>https://arxiv.org/abs/2411.02087</guid>
<content:encoded><![CDATA[
arXiv:2411.02087v5 Announce Type: replace-cross 
Abstract: Achieving a provable exponential quantum speedup for an important machine learning task has been a central research goal since the seminal HHL quantum algorithm for solving linear systems and the subsequent quantum recommender systems algorithm by Kerenidis and Prakash. These algorithms were initially believed to be strong candidates for exponential speedups, but a lower bound ruling out similar classical improvements remained absent. In breakthrough work by Tang, it was demonstrated that this lack of progress in classical lower bounds was for good reasons. Concretely, she gave a classical counterpart of the quantum recommender systems algorithm, reducing the quantum advantage to a mere polynomial. Her approach is quite general and was named quantum-inspired classical algorithms. Since then, almost all the initially exponential quantum machine learning speedups have been reduced to polynomial via new quantum-inspired classical algorithms. From the current state-of-affairs, it is unclear whether we can hope for exponential quantum speedups for any natural machine learning task.
  In this work, we present the first such provable exponential separation between quantum and quantum-inspired classical algorithms for the basic problem of solving a linear system when the input matrix is well-conditioned and has sparse rows and columns.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Reinforcement Learning: An Overview</title>
<link>https://arxiv.org/abs/2412.05265</link>
<guid>https://arxiv.org/abs/2412.05265</guid>
<content:encoded><![CDATA[
arXiv:2412.05265v5 Announce Type: replace-cross 
Abstract: This manuscript gives a big-picture, up-to-date overview of the field of (deep) reinforcement learning and sequential decision making, covering value-based methods, policy-based methods, model-based methods, multi-agent RL, LLMs and RL, and various other topics (e.g., offline RL, hierarchical RL, intrinsic reward). It also includes some code snippets for training LLMs with RL.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Convolution goes higher-order: a biologically inspired mechanism empowers image classification</title>
<link>https://arxiv.org/abs/2412.06740</link>
<guid>https://arxiv.org/abs/2412.06740</guid>
<content:encoded><![CDATA[
arXiv:2412.06740v2 Announce Type: replace-cross 
Abstract: We propose a novel approach to image classification inspired by complex nonlinear biological visual processing, whereby classical convolutional neural networks (CNNs) are equipped with learnable higher-order convolutions. Our model incorporates a Volterra-like expansion of the convolution operator, capturing multiplicative interactions akin to those observed in early and advanced stages of biological visual processing. We evaluated this approach on synthetic datasets by measuring sensitivity to testing higher-order correlations and performance in standard benchmarks (MNIST, FashionMNIST, CIFAR10, CIFAR100 and Imagenette). Our architecture outperforms traditional CNN baselines, and achieves optimal performance with expansions up to 3rd/4th order, aligning remarkably well with the distribution of pixel intensities in natural images. Through systematic perturbation analysis, we validate this alignment by isolating the contributions of specific image statistics to model performance, demonstrating how different orders of convolution process distinct aspects of visual information. Furthermore, Representational Similarity Analysis reveals distinct geometries across network layers, indicating qualitatively different modes of visual information processing. Our work bridges neuroscience and deep learning, offering a path towards more effective, biologically inspired computer vision models. It provides insights into visual information processing and lays the groundwork for neural networks that better capture complex visual patterns, particularly in resource-constrained scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Decentralized Projection-free Online Upper-Linearizable Optimization with Applications to DR-Submodular Optimization</title>
<link>https://arxiv.org/abs/2501.18183</link>
<guid>https://arxiv.org/abs/2501.18183</guid>
<content:encoded><![CDATA[
arXiv:2501.18183v2 Announce Type: replace-cross 
Abstract: We introduce a novel framework for decentralized projection-free optimization, extending projection-free methods to a broader class of upper-linearizable functions. Our approach leverages decentralized optimization techniques with the flexibility of upper-linearizable function frameworks, effectively generalizing traditional DR-submodular function optimization. We obtain the regret of $O(T^{1-\theta/2})$ with communication complexity of $O(T^{\theta})$ and number of linear optimization oracle calls of $O(T^{2\theta})$ for decentralized upper-linearizable function optimization, for any $0\le \theta \le 1$. This approach allows for the first results for monotone up-concave optimization with general convex constraints and non-monotone up-concave optimization with general convex constraints. Further, the above results for first order feedback are extended to zeroth order, semi-bandit, and bandit feedback.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Lanczos method for systematic optimization of neural-network quantum states</title>
<link>https://arxiv.org/abs/2502.01264</link>
<guid>https://arxiv.org/abs/2502.01264</guid>
<content:encoded><![CDATA[
arXiv:2502.01264v2 Announce Type: replace-cross 
Abstract: Recently, artificial intelligence for science has made significant inroads into various fields of natural science research. In the field of quantum many-body computation, researchers have developed numerous ground state solvers based on neural-network quantum states (NQSs), achieving ground state energies with accuracy comparable to or surpassing traditional methods such as variational Monte Carlo methods, density matrix renormalization group, and quantum Monte Carlo methods. Here, we combine supervised learning, variational Monte Carlo (VMC), and the Lanczos method to develop a systematic approach to improving the NQSs of many-body systems, which we refer to as the NQS Lanczos method. The algorithm mainly consists of two parts: the supervised learning part and the VMC optimization part. Through supervised learning, the Lanczos states are represented by the NQSs. Through VMC, the NQSs are further optimized. We analyze the reasons for the underfitting problem and demonstrate how the NQS Lanczos method systematically improves the energy in the highly frustrated regime of the two-dimensional Heisenberg $J_1$-$J_2$ model. Compared to the existing method that combines the Lanczos method with the restricted Boltzmann machine, the primary advantage of the NQS Lanczos method is its linearly increasing computational cost.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generative diffusion for perceptron problems: statistical physics analysis and efficient algorithms</title>
<link>https://arxiv.org/abs/2502.16292</link>
<guid>https://arxiv.org/abs/2502.16292</guid>
<content:encoded><![CDATA[
arXiv:2502.16292v3 Announce Type: replace-cross 
Abstract: We consider random instances of non-convex perceptron problems in the high-dimensional limit of a large number of examples $M$ and weights $N$, with finite load $\alpha = M/N$. We develop a formalism based on replica theory to predict the fundamental limits of efficiently sampling the solution space using generative diffusion algorithms, conjectured to be saturated when the score function is provided by Approximate Message Passing. For the spherical perceptron with negative margin $\kappa$, we find that the uniform distribution over solutions can be efficiently sampled in most of the Replica Symmetric region of the $\alpha$-$\kappa$ plane. In contrast, for binary weights, sampling from the uniform distribution remains intractable. A theoretical analysis of this obstruction leads us to identify a potential $U(s) = -\log(s)$, under which the corresponding tilted distribution becomes efficiently samplable via diffusion. Moreover, we show numerically that an annealing procedure over the shape of this potential yields a fast and robust Markov Chain Monte Carlo algorithm for sampling the solution space of the binary perceptron.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Walk Before You Dance: High-fidelity and Editable Dance Synthesis via Generative Masked Motion Prior</title>
<link>https://arxiv.org/abs/2504.04634</link>
<guid>https://arxiv.org/abs/2504.04634</guid>
<content:encoded><![CDATA[
arXiv:2504.04634v3 Announce Type: replace-cross 
Abstract: Recent advances in dance generation have enabled the automatic synthesis of 3D dance motions. However, existing methods still face significant challenges in simultaneously achieving high realism, precise dance-music synchronization, diverse motion expression, and physical plausibility. To address these limitations, we propose a novel approach that leverages a generative masked text-to-motion model as a distribution prior to learn a probabilistic mapping from diverse guidance signals, including music, genre, and pose, into high-quality dance motion sequences. Our framework also supports semantic motion editing, such as motion inpainting and body part modification. Specifically, we introduce a multi-tower masked motion model that integrates a text-conditioned masked motion backbone with two parallel, modality-specific branches: a music-guidance tower and a pose-guidance tower. The model is trained using synchronized and progressive masked training, which allows effective infusion of the pretrained text-to-motion prior into the dance synthesis process while enabling each guidance branch to optimize independently through its own loss function, mitigating gradient interference. During inference, we introduce classifier-free logits guidance and pose-guided token optimization to strengthen the influence of music, genre, and pose signals. Extensive experiments demonstrate that our method sets a new state of the art in dance generation, significantly advancing both the quality and editability over existing approaches. Project Page available at https://foram-s1.github.io/DanceMosaic/
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
arXiv:2505.14996v3 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference, while also removing the flexibility to reduce to simpler systems. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively design, critique, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic problem decomposition and agent composition through meta-feedback on solvability and completeness, and reduction to simpler systems when appropriate. Experiments across reasoning (math and graduate-level QA), coding, and agentic (search-based) benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms strong manual and automatic MAS baselines. It achieves substantial average accuracy improvements of up to 16.69% on reasoning, 16.66% on coding, and 5.45% on agentic tasks, while maintaining cost efficiency.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BountyBench: Dollar Impact of AI Agent Attackers and Defenders on Real-World Cybersecurity Systems</title>
<link>https://arxiv.org/abs/2505.15216</link>
<guid>https://arxiv.org/abs/2505.15216</guid>
<content:encoded><![CDATA[
arXiv:2505.15216v3 Announce Type: replace-cross 
Abstract: AI agents have the potential to significantly alter the cybersecurity landscape. Here, we introduce the first framework to capture offensive and defensive cyber-capabilities in evolving real-world systems. Instantiating this framework with BountyBench, we set up 25 systems with complex, real-world codebases. To capture the vulnerability lifecycle, we define three task types: Detect (detecting a new vulnerability), Exploit (exploiting a given vulnerability), and Patch (patching a given vulnerability). For Detect, we construct a new success indicator, which is general across vulnerability types and provides localized evaluation. We manually set up the environment for each system, including installing packages, setting up server(s), and hydrating database(s). We add 40 bug bounties, which are vulnerabilities with monetary awards from \$10 to \$30,485, covering 9 of the OWASP Top 10 Risks. To modulate task difficulty, we devise a new strategy based on information to guide detection, interpolating from identifying a zero day to exploiting a given vulnerability. We evaluate 10 agents: Claude Code, OpenAI Codex CLI with o3-high and o4-mini, and custom agents with o3-high, GPT-4.1, Gemini 2.5 Pro Preview, Claude 3.7 Sonnet Thinking, Qwen3 235B A22B, Llama 4 Maverick, and DeepSeek-R1. Given up to three attempts, the top-performing agents are Codex CLI: o3-high (12.5% on Detect, mapping to \$3,720; 90% on Patch, mapping to \$14,152), Custom Agent: Claude 3.7 Sonnet Thinking (67.5% on Exploit), and Codex CLI: o4-mini (90% on Patch, mapping to \$14,422). Codex CLI: o3-high, Codex CLI: o4-mini, and Claude Code are more capable at defense, achieving higher Patch scores of 90%, 90%, and 87.5%, compared to Exploit scores of 47.5%, 32.5%, and 57.5% respectively; while the custom agents are relatively balanced between offense and defense, achieving Exploit scores of 17.5-67.5% and Patch scores of 25-60%.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees</title>
<link>https://arxiv.org/abs/2505.18659</link>
<guid>https://arxiv.org/abs/2505.18659</guid>
<content:encoded><![CDATA[
arXiv:2505.18659v2 Announce Type: replace-cross 
Abstract: Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (PPI) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text-Queried Audio Source Separation via Hierarchical Modeling</title>
<link>https://arxiv.org/abs/2505.21025</link>
<guid>https://arxiv.org/abs/2505.21025</guid>
<content:encoded><![CDATA[
arXiv:2505.21025v2 Announce Type: replace-cross 
Abstract: Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz</title>
<link>https://arxiv.org/abs/2505.22083</link>
<guid>https://arxiv.org/abs/2505.22083</guid>
<content:encoded><![CDATA[
arXiv:2505.22083v3 Announce Type: replace-cross 
Abstract: In this work, we introduce the first type of non-Euclidean neural quantum state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent neural networks (RNNs)), to be used in the Variational Monte Carlo method of approximating the ground state energy for quantum many-body systems. In particular, we examine the performances of NQS ansatzes constructed from both conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical settings of the one- and two-dimensional transverse field Ising models (TFIM) and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$ systems. By virtue of the fact that, for all of the experiments performed in this work, hyperbolic GRU can yield performances comparable to or better than Euclidean RNNs, which have been extensively studied in these settings in the literature, our work is a proof-of-concept for the viability of hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum many-body systems. Furthermore, in settings where the Hamiltonian displays a clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ & $J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor interactions, our results show that hyperbolic GRU definitively outperforms its Euclidean version in almost all instances. The fact that these results are reminiscent of the established ones from natural language processing where hyperbolic GRU almost always outperforms Euclidean RNNs when the training data exhibit a tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin systems that involve different degrees of nearest neighbor interactions. Finally, with this work, we hope to initiate future studies of other types of non-Euclidean NQS beyond hyperbolic GRU.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HeavyWater and SimplexWater: Distortion-Free LLM Watermarks for Low-Entropy Next-Token Predictions</title>
<link>https://arxiv.org/abs/2506.06409</link>
<guid>https://arxiv.org/abs/2506.06409</guid>
<content:encoded><![CDATA[
arXiv:2506.06409v2 Announce Type: replace-cross 
Abstract: Large language model (LLM) watermarks enable authentication of text provenance, curb misuse of machine-generated text, and promote trust in AI systems. Current watermarks operate by changing the next-token predictions output by an LLM. The updated (i.e., watermarked) predictions depend on random side information produced, for example, by hashing previously generated tokens. LLM watermarking is particularly challenging in low-entropy generation tasks -- such as coding -- where next-token predictions are near-deterministic. In this paper, we propose an optimization framework for watermark design. Our goal is to understand how to most effectively use random side information in order to maximize the likelihood of watermark detection and minimize the distortion of generated text. Our analysis informs the design of two new watermarks: HeavyWater and SimplexWater. Both watermarks are tunable, gracefully trading-off between detection accuracy and text distortion. They can also be applied to any LLM and are agnostic to side information generation. We examine the performance of HeavyWater and SimplexWater through several benchmarks, demonstrating that they can achieve high watermark detection accuracy with minimal compromise of text generation quality, particularly in the low-entropy regime. Our theoretical analysis also reveals surprising new connections between LLM watermarking and coding theory. The code implementation can be found in https://github.com/DorTsur/HeavyWater_SimplexWater
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Noise tolerance via reinforcement: Learning a reinforced quantum dynamics</title>
<link>https://arxiv.org/abs/2506.12418</link>
<guid>https://arxiv.org/abs/2506.12418</guid>
<content:encoded><![CDATA[
arXiv:2506.12418v3 Announce Type: replace-cross 
Abstract: The performance of quantum simulations heavily depends on the efficiency of noise mitigation techniques and error correction algorithms. Reinforcement has emerged as a powerful strategy to enhance the efficiency of learning and optimization algorithms. In this study, we demonstrate that a reinforced quantum dynamics can exhibit significant robustness against interactions with a noisy environment. We study a quantum annealing process where, through reinforcement, the system is encouraged to maintain its current state or follow a noise-free evolution. A learning algorithm is employed to derive a concise approximation of this reinforced dynamics, reducing the total evolution time and, consequently, the system's exposure to noisy interactions. This also avoids the complexities associated with implementing quantum feedback in such reinforcement algorithms. The efficacy of our method is demonstrated through numerical simulations of reinforced quantum annealing with one- and two-qubit systems under Pauli noise.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>CNN-Enabled Scheduling for Probabilistic Real-Time Guarantees in Industrial URLLC</title>
<link>https://arxiv.org/abs/2506.14987</link>
<guid>https://arxiv.org/abs/2506.14987</guid>
<content:encoded><![CDATA[
arXiv:2506.14987v3 Announce Type: replace-cross 
Abstract: Ensuring packet-level communication quality is vital for ultra-reliable, low-latency communications (URLLC) in large-scale industrial wireless networks. We enhance the Local Deadline Partition (LDP) algorithm by introducing a CNN-based dynamic priority prediction mechanism for improved interference coordination in multi-cell, multi-channel networks. Unlike LDP's static priorities, our approach uses a Convolutional Neural Network and graph coloring to adaptively assign link priorities based on real-time traffic, transmission opportunities, and network conditions. Assuming that first training phase is performed offline, our approach introduced minimal overhead, while enabling more efficient resource allocation, boosting network capacity, SINR, and schedulability. Simulation results show SINR gains of up to 113\%, 94\%, and 49\% over LDP across three network configurations, highlighting its effectiveness for complex URLLC scenarios.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Keeping Medical AI Healthy and Trustworthy: A Review of Detection and Correction Methods for System Degradation</title>
<link>https://arxiv.org/abs/2506.17442</link>
<guid>https://arxiv.org/abs/2506.17442</guid>
<content:encoded><![CDATA[
arXiv:2506.17442v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) is increasingly integrated into modern healthcare, offering powerful support for clinical decision-making. However, in real-world settings, AI systems may experience performance degradation over time, due to factors such as shifting data distributions, changes in patient characteristics, evolving clinical protocols, and variations in data quality. These factors can compromise model reliability, posing safety concerns and increasing the likelihood of inaccurate predictions or adverse outcomes. This review presents a forward-looking perspective on monitoring and maintaining the "health" of AI systems in healthcare. We highlight the urgent need for continuous performance monitoring, early degradation detection, and effective self-correction mechanisms. The paper begins by reviewing common causes of performance degradation at both data and model levels. We then summarize key techniques for detecting data and model drift, followed by an in-depth look at root cause analysis. Correction strategies are further reviewed, ranging from model retraining to test-time adaptation. Our survey spans both traditional machine learning models and state-of-the-art large language models (LLMs), offering insights into their strengths and limitations. Finally, we discuss ongoing technical challenges and propose future research directions. This work aims to guide the development of reliable, robust medical AI systems capable of sustaining safe, long-term deployment in dynamic clinical settings.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SQLBarber: A System Leveraging Large Language Models to Generate Customized and Realistic SQL Workloads</title>
<link>https://arxiv.org/abs/2507.06192</link>
<guid>https://arxiv.org/abs/2507.06192</guid>
<content:encoded><![CDATA[
arXiv:2507.06192v2 Announce Type: replace-cross 
Abstract: Database research and development often require a large number of SQL queries for benchmarking purposes. However, acquiring real-world SQL queries is challenging due to privacy concerns, and existing SQL generation methods are limited in customization and in satisfying realistic constraints. To address this issue, we present SQLBarber, a system based on Large Language Models (LLMs) to generate customized and realistic SQL workloads. SQLBarber (i) eliminates the need for users to manually craft SQL templates in advance, while providing the flexibility to accept natural language specifications to constrain SQL templates, (ii) scales efficiently to generate large volumes of queries matching any user-defined cost distribution (e.g., cardinality and execution plan cost), and (iii) uses execution statistics from Amazon Redshift and Snowflake to derive SQL template specifications and query cost distributions that reflect real-world query characteristics. SQLBarber introduces (i) a declarative interface for users to effortlessly generate customized SQL templates, (ii) an LLM-powered pipeline augmented with a self-correction module that profiles, refines, and prunes SQL templates based on query costs, and (iii) a Bayesian Optimizer to efficiently explore different predicate values and identify a set of queries that satisfy the target cost distribution. We construct and open-source ten benchmarks of varying difficulty levels and target query cost distributions based on real-world statistics from Snowflake and Amazon Redshift. Extensive experiments on these benchmarks show that SQLBarber is the only system that can generate customized SQL templates. It reduces query generation time by one to three orders of magnitude, and significantly improves alignment with the target cost distribution, compared with existing methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>APTx Neuron: A Unified Trainable Neuron Architecture Integrating Activation and Computation</title>
<link>https://arxiv.org/abs/2507.14270</link>
<guid>https://arxiv.org/abs/2507.14270</guid>
<content:encoded><![CDATA[
arXiv:2507.14270v5 Announce Type: replace-cross 
Abstract: We propose the APTx Neuron, a novel, unified neural computation unit that integrates non-linear activation and linear transformation into a single trainable expression. The APTx Neuron is derived from the APTx activation function, thereby eliminating the need for separate activation layers and making the architecture both optimization-efficient and elegant. The proposed neuron follows the functional form $y = \sum_{i=1}^{n} ((\alpha_i + \tanh(\beta_i x_i)) \cdot \gamma_i x_i) + \delta$, where all parameters $\alpha_i$, $\beta_i$, $\gamma_i$, and $\delta$ are trainable. We validate our APTx Neuron-based architecture on the MNIST dataset, achieving up to $96.69\%$ test accuracy within 11 epochs using approximately 332K trainable parameters. The results highlight the superior expressiveness and training efficiency of the APTx Neuron compared to traditional neurons, pointing toward a new paradigm in unified neuron design and the architectures built upon it. Source code is available at https://github.com/mr-ravin/aptx_neuron.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>GRAFT: GRaPH and Table Reasoning for Textual Alignment -- A Benchmark for Structured Instruction Following and Visual Reasoning</title>
<link>https://arxiv.org/abs/2508.15690</link>
<guid>https://arxiv.org/abs/2508.15690</guid>
<content:encoded><![CDATA[
arXiv:2508.15690v4 Announce Type: replace-cross 
Abstract: GRAFT is a structured multimodal benchmark designed to probe how well LLMs handle instruction following, visual reasoning, and tasks requiring tight visual textual alignment. The dataset is built around programmatically generated charts and synthetically rendered tables, each paired with a carefully constructed, multi step analytical question that depends solely on what can be inferred from the image itself. Responses are formatted in structured outputs such as JSON or YAML, enabling consistent and fine grained evaluation of both reasoning processes and adherence to output specifications. The benchmark further introduces a taxonomy of reasoning operations ranging from comparison and trend identification to ranking, aggregation, proportional estimation, and anomaly detection to support a comprehensive assessment of model capabilities. Taken together, GRAFT provides a unified and scalable framework for evaluating multimodal LLMs on visually grounded, structured reasoning tasks, offering a more rigorous standard for future benchmarking efforts.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
<link>https://arxiv.org/abs/2508.19112</link>
<guid>https://arxiv.org/abs/2508.19112</guid>
<content:encoded><![CDATA[
arXiv:2508.19112v2 Announce Type: replace-cross 
Abstract: Accurate detection and segmentation of cancerous lesions from computed tomography (CT) scans is essential for automated treatment planning and cancer treatment response assessment. Transformer-based models with self-supervised pretraining have achieved strong performance on in-distribution (ID) data but often generalize poorly on out-of-distribution (OOD) inputs. We investigate this behavior for lung cancer segmentation using an encoder-decoder model. Our encoder is a Swin Transformer pretrained with masked image modeling (SimMIM) on 10,432 unlabeled 3D CT scans spanning cancerous and non-cancerous conditions, and the decoder was randomly initialized. This model was evaluated on an independent ID test set and four OOD scenarios, including chest CT cohorts (pulmonary embolism and negative COVID-19) and abdomen CT cohorts (kidney cancers and non-cancerous pancreas). OOD detection was performed at the scan level using RF-Deep, a random forest classifier applied to contextual tumor-anchored feature representations. We evaluated 920 3D CTs (172,650 images) and observed that RF-Deep achieved FPR95 values of 18.26% and 27.66% on the chest CT cohorts, and near-perfect detection (less than 0.1% FPR95) on the abdomen CT cohorts, consistently outperforming established OOD methods. These results demonstrate that our RF-Deep classifier provides a simple, lightweight, and effective approach for enhancing the reliability of segmentation models in clinical deployment.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Synthetic Data Generation with Lorenzetti for Time Series Anomaly Detection in High-Energy Physics Calorimeters</title>
<link>https://arxiv.org/abs/2509.07451</link>
<guid>https://arxiv.org/abs/2509.07451</guid>
<content:encoded><![CDATA[
arXiv:2509.07451v3 Announce Type: replace-cross 
Abstract: Anomaly detection in multivariate time series is crucial to ensure the quality of data coming from a physics experiment. Accurately identifying the moments when unexpected errors or defects occur is essential, yet challenging due to scarce labels, unknown anomaly types, and complex correlations across dimensions. To address the scarcity and unreliability of labelled data, we use the Lorenzetti Simulator to generate synthetic events with injected calorimeter anomalies. We then assess the sensitivity of several time series anomaly detection methods, including transformer-based and other deep learning models. The approach employed here is generic and applicable to different detector designs and defects.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
<link>https://arxiv.org/abs/2509.08366</link>
<guid>https://arxiv.org/abs/2509.08366</guid>
<content:encoded><![CDATA[
arXiv:2509.08366v2 Announce Type: replace-cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments illustrate the performance of kNNSampler. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Apertus: Democratizing Open and Compliant LLMs for Global Language Environments</title>
<link>https://arxiv.org/abs/2509.14233</link>
<guid>https://arxiv.org/abs/2509.14233</guid>
<content:encoded><![CDATA[
arXiv:2509.14233v2 Announce Type: replace-cross 
Abstract: We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting `robots.txt` exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ParlAI Vote: A Web Platform for Analyzing Gender and Political Bias in Large Language Models</title>
<link>https://arxiv.org/abs/2509.16264</link>
<guid>https://arxiv.org/abs/2509.16264</guid>
<content:encoded><![CDATA[
arXiv:2509.16264v3 Announce Type: replace-cross 
Abstract: We present ParlAI Vote, an interactive web platform for exploring European Parliament debates and votes, and for testing LLMs on vote prediction and bias analysis. This web system connects debate topics, speeches, and roll-call outcomes, and includes rich demographic data such as gender, age, country, and political group. Users can browse debates, inspect linked speeches, compare real voting outcomes with predictions from frontier LLMs, and view error breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its core tasks of gender classification and vote prediction, ParlAI Vote highlights systematic performance bias in state-of-the-art LLMs. It unifies data, models, and visual analytics in a single interface, lowering the barrier for reproducing findings, auditing behavior, and running counterfactual scenarios. This web platform also shows model reasoning, helping users see why errors occur and what cues the models rely on. It supports research, education, and public engagement with legislative decision-making, while making clear both the strengths and the limitations of current LLMs in political analysis.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Look Before you Leap: Estimating LLM Benchmark Scores from Descriptions</title>
<link>https://arxiv.org/abs/2509.20645</link>
<guid>https://arxiv.org/abs/2509.20645</guid>
<content:encoded><![CDATA[
arXiv:2509.20645v2 Announce Type: replace-cross 
Abstract: Progress in large language models is constrained by an evaluation bottleneck: build a benchmark, run models, then iterate. We ask a question: can we forecast outcomes before running any experiments to inform earlier study design? For example, a team building an AI assistant for a certain task can estimate whether expected performance is around 50 or closer to 80, evidence that supports whether to proceed to a pilot study, how to scope it, and how to allocate resources. We study text-only performance forecasting, where a model predicts a score from a redacted task description and intended configuration, with no access to dataset instances. To support systematic study, we curate PRECOG, a corpus of redacted description-performance pairs spanning diverse tasks, domains, and metrics. We scrape task and configuration descriptions from arXiv, yielding 2,290 instances covering 1,519 papers, and construct a leakage free test split using papers published after the knowledge cutoff of the evaluated models. Experiments show the task is challenging but feasible: reasoning models achieve moderate prediction performance with well calibrated uncertainty, reaching mean absolute error as low as 9.9 at high confidence thresholds. We further test a zero-leakage setting, forecasting on newly released datasets or experiments before their papers are indexed, where GPT5 with built in web search still attains nontrivial prediction accuracy. Overall, our corpus and analyses offer an initial step toward open ended anticipatory evaluation, supporting difficulty estimation and smarter experiment prioritization.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperdimensional Probe: Decoding LLM Representations via Vector Symbolic Architectures</title>
<link>https://arxiv.org/abs/2509.25045</link>
<guid>https://arxiv.org/abs/2509.25045</guid>
<content:encoded><![CDATA[
arXiv:2509.25045v2 Announce Type: replace-cross 
Abstract: Despite their capabilities, Large Language Models (LLMs) remain opaque with limited understanding of their internal representations. Current interpretability methods either focus on input-oriented feature extraction, such as supervised probes and Sparse Autoencoders (SAEs), or on output distribution inspection, such as logit-oriented approaches. A full understanding of LLM vector spaces, however, requires integrating both perspectives, something existing approaches struggle with due to constraints on latent feature definitions. We introduce the Hyperdimensional Probe, a hybrid supervised probe that combines symbolic representations with neural probing. Leveraging Vector Symbolic Architectures (VSAs) and hypervector algebra, it unifies prior methods: the top-down interpretability of supervised probes, SAE's sparsity-driven proxy space, and output-oriented logit investigation. This allows deeper input-focused feature extraction while supporting output-oriented investigation. Our experiments show that our method consistently extracts meaningful concepts across LLMs, embedding sizes, and setups, uncovering concept-driven patterns in analogy-oriented inference and QA-focused text generation. By supporting joint input-output analysis, this work advances semantic understanding of neural representations while unifying the complementary perspectives of prior methods.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Curiosity-Driven Development of Action and Language in Robots Through Self-Exploration</title>
<link>https://arxiv.org/abs/2510.05013</link>
<guid>https://arxiv.org/abs/2510.05013</guid>
<content:encoded><![CDATA[
arXiv:2510.05013v3 Announce Type: replace-cross 
Abstract: Human infants acquire language and action gradually through development, achieving strong generalization from minimal experience, whereas large language models require exposure to billions of training tokens. What mechanisms underlie such efficient developmental learning in humans? This study investigates this question through robot simulation experiments in which agents learn to perform actions associated with imperative sentences (e.g., \textit{push red cube}) via curiosity-driven self-exploration. Our approach integrates the active inference framework with reinforcement learning, enabling intrinsically motivated developmental learning. The simulations reveal several key findings: i) Generalization improves markedly as the scale of compositional elements increases. ii) Curiosity combined with motor noise yields substantially better learning than exploration without curiosity. iii) Rote pairing of sentences and actions precedes the emergence of compositional generalization. iv) Simpler, prerequisite-like actions develop earlier than more complex actions that depend on them. v) When exception-handling rules were introduced -- where certain imperative sentences required executing inconsistent actions -- the robots successfully acquired these exceptions through exploration and displayed a U-shaped performance curve characteristic of representational redescription in child language learning. Together, these results suggest that curiosity-driven exploration and active inference provide a powerful account of how intrinsic motivation and hierarchical sensorimotor learning can jointly support scalable compositional generalization and exception handling in both humans and artificial agents.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive Pruning for Increased Robustness and Reduced Computational Overhead in Gaussian Process Accelerated Saddle Point Searches</title>
<link>https://arxiv.org/abs/2510.06030</link>
<guid>https://arxiv.org/abs/2510.06030</guid>
<content:encoded><![CDATA[
arXiv:2510.06030v3 Announce Type: replace-cross 
Abstract: Gaussian process (GP) regression provides a strategy for accelerating saddle point searches on high-dimensional energy surfaces by reducing the number of times the energy and its derivatives with respect to atomic coordinates need to be evaluated. The computational overhead in the hyperparameter optimization can, however, be large and make the approach inefficient. Failures can also occur if the search ventures too far into regions that are not represented well enough by the GP model. Here, these challenges are resolved by using geometry-aware optimal transport measures and an active pruning strategy using a summation over Wasserstein-1 distances for each atom-type in farthest-point sampling, selecting a fixed-size subset of geometrically diverse configurations to avoid rapidly increasing cost of GP updates as more observations are made. Stability is enhanced by permutation-invariant metric that provides a reliable trust radius for early-stopping and a logarithmic barrier penalty for the growth of the signal variance. These physically motivated algorithmic changes prove their efficacy by reducing to less than a half the mean computational time on a set of 238 challenging configurations from a previously published data set of chemical reactions. With these improvements, the GP approach is established as, a robust and scalable algorithm for accelerating saddle point searches when the evaluation of the energy and atomic forces requires significant computational effort.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geopolitics, Geoeconomics and Risk: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2510.12416</link>
<guid>https://arxiv.org/abs/2510.12416</guid>
<content:encoded><![CDATA[
arXiv:2510.12416v2 Announce Type: replace-cross 
Abstract: We introduce a novel high-frequency daily panel dataset of both markets and news-based indicators -- including Geopolitical Risk, Economic Policy Uncertainty, Trade Policy Uncertainty, and Political Sentiment -- for 42 countries across both emerging and developed markets. Using this dataset, we study how sentiment dynamics shape sovereign risk, measured by Credit Default Swap (CDS) spreads, and evaluate their forecasting value relative to traditional drivers such as global monetary policy and market volatility. Our horse-race analysis of forecasting models demonstrates that incorporating news-based indicators significantly enhances predictive accuracy and enriches the analysis, with non-linear machine learning methods -- particularly Random Forests -- delivering the largest gains. Our analysis reveals that while global financial variables remain the dominant drivers of sovereign risk, geopolitical risk and economic policy uncertainty also play a meaningful role. Crucially, their effects are amplified through non-linear interactions with global financial conditions. Finally, we document pronounced regional heterogeneity, as certain asset classes and emerging markets exhibit heightened sensitivity to shocks in policy rates, global financial volatility, and geopolitical risk.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>On the identifiability of causal graphs with multiple environments</title>
<link>https://arxiv.org/abs/2510.13583</link>
<guid>https://arxiv.org/abs/2510.13583</guid>
<content:encoded><![CDATA[
arXiv:2510.13583v2 Announce Type: replace-cross 
Abstract: Causal discovery from i.i.d. observational data is known to be generally ill-posed. We demonstrate that if we have access to the distribution of a structural causal model, and additional data from only two environments that sufficiently differ in the noise statistics, the unique causal graph is identifiable. Notably, this is the first result in the literature that guarantees the entire causal graph recovery with a constant number of environments and arbitrary nonlinear mechanisms. Our only constraint is the Gaussianity of the noise terms; however, we propose potential ways to relax this requirement. Of interest on its own, we expand on the well-known duality between independent component analysis (ICA) and causal discovery; recent advancements have shown that nonlinear ICA can be solved from multiple environments, at least as many as the number of sources: we show that the same can be achieved for causal discovery while having access to much less auxiliary information.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2511.09809</link>
<guid>https://arxiv.org/abs/2511.09809</guid>
<content:encoded><![CDATA[
arXiv:2511.09809v2 Announce Type: replace-cross 
Abstract: Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at https://github.com/kdafnis/STS.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Compression and Artifact Correction for Streaming Underwater Imaging Sonar</title>
<link>https://arxiv.org/abs/2511.13922</link>
<guid>https://arxiv.org/abs/2511.13922</guid>
<content:encoded><![CDATA[
arXiv:2511.13922v2 Announce Type: replace-cross 
Abstract: Real-time imaging sonar is crucial for underwater monitoring where optical sensing fails, but its use is limited by low uplink bandwidth and severe sonar-specific artifacts (speckle, motion blur, reverberation, acoustic shadows) affecting up to 98% of frames. We present SCOPE, a self-supervised framework that jointly performs compression and artifact correction without clean-noise pairs or synthetic assumptions. SCOPE combines (i) Adaptive Codebook Compression (ACC), which learns frequency-encoded latent representations tailored to sonar, with (ii) Frequency-Aware Multiscale Segmentation (FAMS), which decomposes frames into low-frequency structure and sparse high-frequency dynamics while suppressing rapidly fluctuating artifacts. A hedging training strategy further guides frequency-aware learning using low-pass proxy pairs generated without labels. Evaluated on months of in-situ ARIS sonar data, SCOPE achieves a structural similarity index (SSIM) of 0.77, representing a 40% improvement over prior self-supervised denoising baselines, at bitrates down to <= 0.0118 bpp. It reduces uplink bandwidth by more than 80% while improving downstream detection. The system runs in real time, with 3.1 ms encoding on an embedded GPU and 97 ms full multi-layer decoding on the server end. SCOPE has been deployed for months in three Pacific Northwest rivers to support real-time salmon enumeration and environmental monitoring in the wild. Results demonstrate that learning frequency-structured latents enables practical, low-bitrate sonar streaming with preserved signal details under real-world deployment conditions.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Front-door Reducibility: Reducing ADMGs to the Standard Front-door Setting via a Graphical Criterion</title>
<link>https://arxiv.org/abs/2511.15679</link>
<guid>https://arxiv.org/abs/2511.15679</guid>
<content:encoded><![CDATA[
arXiv:2511.15679v2 Announce Type: replace-cross 
Abstract: Front-door adjustment gives a simple closed-form identification formula under the classical front-door criterion, but its applicability is often viewed as narrow. By contrast, the general ID algorithm can identify many more causal effects in arbitrary graphs, yet typically outputs algebraically complex expressions that are hard to estimate and interpret. We show that many such graphs can in fact be reduced to a standard front-door setting via front-door reducibility (FDR), a graphical condition on acyclic directed mixed graphs that aggregates variables into super-nodes $(\boldsymbol{X}^{*},\boldsymbol{Y}^{*},\boldsymbol{M}^{*})$. We characterize the FDR criterion, prove it is equivalent (at the graph level) to the existence of an FDR adjustment, and present FDR-TID, an exact algorithm that finds an admissible FDR triple with correctness, completeness, and finite-termination guarantees. Empirical examples show that many graphs far outside the textbook front-door setting are FDR, yielding simple, estimable adjustments where general ID expressions would be cumbersome. FDR therefore complements existing identification methods by prioritizing interpretability and computational simplicity without sacrificing generality across mixed graphs.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis</title>
<link>https://arxiv.org/abs/2511.18843</link>
<guid>https://arxiv.org/abs/2511.18843</guid>
<content:encoded><![CDATA[
arXiv:2511.18843v2 Announce Type: replace-cross 
Abstract: Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a systematic framework for applying BERTopic to focus group transcripts using data from ten focus groups exploring HPV vaccine perceptions in Tunisia (1,075 utterances). We conducted comprehensive hyperparameter exploration across 27 configurations, evaluating each through bootstrap stability analysis, performance metrics, and comparison with LDA baseline. Bootstrap analysis revealed that stability metrics (NMI and ARI) exhibited strong disagreement (r = -0.691) and showed divergent relationships with coherence, demonstrating that stability is multifaceted rather than monolithic. Our multi-criteria selection framework yielded a 7-topic model achieving 18\% higher coherence than optimized LDA (0.573 vs. 0.486) with interpretable topics validated through independent human evaluation (ICC = 0.700, weighted Cohen's kappa = 0.678). These findings demonstrate that transformer-based topic modeling can extract interpretable themes from small focus group transcript corpora when systematically configured and validated, while revealing that quality metrics capture distinct, sometimes conflicting constructs requiring multi-criteria evaluation. We provide complete documentation and code to support reproducibility.
]]></content:encoded>
<pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>When Features Beat Noise: A Feature Selection Technique Through Noise-Based Hypothesis Testing</title>
<link>https://arxiv.org/abs/2511.20851</link>
<guid>https://arxiv.org/abs/2511.20851</guid>
<content:encoded><![CDATA[
<div> Keywords: feature selection, noise features, bootstrap hypothesis testing, Boruta, high-dimensional data<br /><br />Summary:<br /><br />This paper addresses the challenge of feature selection in machine learning, especially for high-dimensional datasets. Existing methods either have high computational costs or lack clear statistical criteria for stopping or assessing feature importance. The authors critique a common heuristic that inserts random noise features and retains predictors ranked above the strongest noise feature, highlighting its lack of theoretical basis. To overcome these issues, the paper introduces a novel feature selection algorithm that incorporates multiple random noise features and compares each real feature's importance against the maximum importance of noise features using a non-parametric bootstrap hypothesis testing framework. This provides a statistically grounded stopping rule. The paper includes statistical derivations to explain the principles behind the algorithm's design. The method is tested on simulated datasets with controlled settings and benchmarked against Boruta and Knockoff methods, showing improved recovery of meaningful features. Additionally, the approach is applied to diverse real-world datasets, outperforming conventional techniques such as Boruta, Recursive Feature Elimination (RFE), and Extra Trees. Overall, the proposed algorithm offers a principled, computationally efficient, and robust solution for feature selection that enhances reliable inference and predictive performance. <div>
arXiv:2511.20851v2 Announce Type: replace-cross 
Abstract: Feature selection has remained a daunting challenge in machine learning and artificial intelligence, where increasingly complex, high-dimensional datasets demand principled strategies for isolating the most informative predictors. Despite widespread adoption, many established techniques suffer from notable limitations; some incur substantial computational cost, while others offer no definite statistical driven stopping criteria or assesses the significance of their importance scores. A common heuristic approach introduces multiple random noise features and retains all predictors ranked above the strongest noise feature. Although intuitive, this strategy lacks theoretical justification and depends heavily on heuristics. This paper proposes a novel feature selection method that addresses these limitations. Our approach introduces multiple random noise features and evaluates each feature's importance against the maximum importance value among these noise features incorporating a non-parametric bootstrap-based hypothesis testing framework to establish a solid theoretical foundation. We establish the conceptual soundness of our approach through statistical derivations that articulate the principles guiding the design of our algorithm. To evaluate its reliability, we generated simulated datasets under controlled statistical settings and benchmarked performance against Boruta and Knockoff-based methods, observing consistently stronger recovery of meaningful signal. As a demonstration of practical utility, we applied the technique across diverse real-world datasets, where it surpassed feature selection techniques including Boruta, RFE, and Extra Trees. Hence, the method emerges as a robust algorithm for principled feature selection, enabling the distillation of informative predictors that support reliable inference, enhanced predictive performance, and efficient computation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Unifying View of Linear Function Approximation in Off-Policy RL Through Matrix Splitting and Preconditioning</title>
<link>https://arxiv.org/abs/2501.01774</link>
<guid>https://arxiv.org/abs/2501.01774</guid>
<content:encoded><![CDATA[
<div> Keywords: Off-policy policy evaluation, Temporal Difference Learning, Fitted Q-Iteration, matrix splitting, convergence analysis<br /><br />Summary:<br /><br />This paper revisits the traditional understanding of Temporal Difference Learning (TD) and Fitted Q-Iteration (FQI) in off-policy policy evaluation within reinforcement learning, challenging the view that they merely differ by the number of updates toward the target value function. The authors develop a unified mathematical framework under linear value function approximation, showing that TD, Partial Fitted Q-Iteration (PFQI), and FQI are iterations solving the same linear system but use different matrix splitting schemes and preconditioners. A key insight is that increasing the number of updates using the target network method shifts from a constant preconditioner (TD) to a data-feature adaptive preconditioner (FQI). This explains why TD convergence does not guarantee FQI convergence and establishes precise convergence relationships among these algorithms. The framework allows sharper theoretical results, characterizing convergence conditions without relying on traditional assumptions like feature linear independence. Using an encoder-decoder perspective, the paper also proves that smaller learning rates can aid convergence when large rates fail. Importantly, the work introduces matrix splitting into the convergence analysis for the first time, reveals new necessary conditions on feature properties for convergence, and demonstrates that common assumptions can be relaxed without losing convergence guarantees, especially in the on-policy stochastic TD setting. <div>
arXiv:2501.01774v4 Announce Type: replace 
Abstract: In off-policy policy evaluation (OPE) tasks within reinforcement learning, Temporal Difference Learning(TD) and Fitted Q-Iteration (FQI) have traditionally been viewed as differing in the number of updates toward the target value function: TD makes one update, FQI makes an infinite number, and Partial Fitted Q-Iteration (PFQI) performs a finite number. We show that this view is not accurate, and provide a new mathematical perspective under linear value function approximation that unifies these methods as a single iterative method solving the same linear system, but using different matrix splitting schemes and preconditioners. We show that increasing the number of updates under the same target value function, i.e., the target network technique, is a transition from using a constant preconditioner to using a data-feature adaptive preconditioner. This elucidates, for the first time, why TD convergence does not necessarily imply FQI convergence, and establishes tight convergence connections among TD, PFQI, and FQI. Our framework enables sharper theoretical results than previous work and characterization of the convergence conditions for each algorithm, without relying on assumptions about the features (e.g., linear independence). We also provide an encoder-decoder perspective to better understand the convergence conditions of TD, and prove, for the first time, that when a large learning rate doesn't work, trying a smaller one may help. Our framework also leads to the discovery of new crucial conditions on features for convergence, and shows how common assumptions about features influence convergence, e.g., the assumption of linearly independent features can be dropped without compromising the convergence guarantees of stochastic TD in the on-policy setting. This paper is also the first to introduce matrix splitting into the convergence analysis of these algorithms.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Addressing divergent representations from causal interventions on neural networks</title>
<link>https://arxiv.org/abs/2511.04638</link>
<guid>https://arxiv.org/abs/2511.04638</guid>
<content:encoded><![CDATA[
<div> Keywords: mechanistic interpretability, causal interventions, out-of-distribution representations, behavioral null-space, Counterfactual Latent loss  

<br /><br />Summary:  
This paper investigates whether causal interventions used in mechanistic interpretability create out-of-distribution (OOD) representations, potentially affecting the faithfulness of explanations derived from them. First, it theoretically and empirically shows that common causal intervention methods frequently shift internal model representations away from the model's natural distribution. Second, it provides a theoretical analysis of two types of such divergences: "harmless" divergences, which occur within the behavioral null-space and do not affect model behavior, and "pernicious" divergences, which activate hidden network pathways causing unintended behavioral changes. Third, to mitigate harmful divergences, the authors apply and modify the Counterfactual Latent (CL) loss method from Grant (2025), which encourages the representations altered by causal interventions to stay closer to the model’s natural distribution. This adjustment reduces the likelihood of pernicious divergences while maintaining the interpretability benefits of interventions. Overall, the work offers valuable insights and a promising approach toward enhancing the reliability and validity of mechanistic interpretability techniques in complex models. <div>
arXiv:2511.04638v4 Announce Type: replace 
Abstract: A common approach to mechanistic interpretability is to causally manipulate model representations via targeted interventions in order to understand what those representations encode. Here we ask whether such interventions create out-of-distribution (divergent) representations, and whether this raises concerns about how faithful their resulting explanations are to the target model in its natural state. First, we demonstrate theoretically and empirically that common causal intervention techniques often do shift internal representations away from the natural distribution of the target model. Then, we provide a theoretical analysis of two cases of such divergences: "harmless" divergences that occur in the behavioral null-space of the layer(s) of interest, and "pernicious" divergences that activate hidden network pathways and cause dormant behavioral changes. Finally, in an effort to mitigate the pernicious cases, we apply and modify the Counterfactual Latent (CL) loss from Grant (2025) allowing representations from causal interventions to remain closer to the natural distribution, reducing the likelihood of harmful divergences while preserving the interpretive power of the interventions. Together, these results highlight a path towards more reliable interpretability methods.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Adaptive Policy Optimization</title>
<link>https://arxiv.org/abs/2511.20347</link>
<guid>https://arxiv.org/abs/2511.20347</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Large Language Models, Policy Optimization, Soft Adaptive Policy Optimization, Mixture-of-Experts  

<br /><br />Summary:  
Reinforcement learning (RL) is crucial for improving the reasoning capabilities of large language models (LLMs), yet achieving stable and effective policy optimization remains difficult. Traditional token-level importance ratio methods suffer from high variance, especially in Mixture-of-Experts (MoE) models, leading to unstable training updates. Existing group-based policy optimization techniques like GSPO and GRPO attempt to address this with hard clipping mechanisms, but these can impair the balance between learning stability and effectiveness. The proposed Soft Adaptive Policy Optimization (SAPO) method replaces hard clipping with a smooth, temperature-controlled gating function that adaptively modulates off-policy updates to maintain useful learning signals. SAPO preserves sequence-level coherence akin to GSPO but avoids brittle hard clipping by enabling a continuous trust region, selectively down-weighting only problematic tokens rather than discarding entire sequences. Compared to GRPO, SAPO’s soft token-level scaling provides more informative and stable gradient updates. Empirical evaluations on mathematical reasoning benchmarks show that SAPO improves training stability and Pass@1 accuracy under comparable settings. Additionally, SAPO’s application to training the Qwen3-VL model series yields consistent performance improvements across diverse tasks and model sizes. Overall, SAPO offers a reliable, scalable, and more effective RL optimization strategy for training LLMs. <div>
arXiv:2511.20347v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction</title>
<link>https://arxiv.org/abs/2503.08594</link>
<guid>https://arxiv.org/abs/2503.08594</guid>
<content:encoded><![CDATA[
<div> autoregressive generation, point cloud, level-of-detail, multi-scale, ShapeNet<br /><br />Summary:<br /><br />This paper addresses the limitations of autoregressive point cloud generation, which traditionally underperforms compared to diffusion-based methods due to its reliance on an arbitrary ordering of points. This ordering biases the model toward local, short-range predictions that fail to capture essential global structural properties such as symmetry and consistent topology. To overcome these issues, the authors propose PointNSP, a novel coarse-to-fine generative framework inspired by the level-of-detail (LOD) principle commonly used in shape modeling. PointNSP operates by preserving the global shape structure at lower resolutions and progressively refining the geometry at higher scales through a next-scale prediction mechanism. This multi-scale approach respects the permutation-invariant nature of point sets, allowing rich interactions within each scale without the fragility of fixed sequential orderings. Experimental results on the ShapeNet dataset demonstrate that PointNSP achieves state-of-the-art generation quality within the autoregressive paradigm, surpassing both earlier autoregressive and strong diffusion-based baselines. Additionally, PointNSP benefits from greater efficiency in parameters, training time, and inference speed. The framework’s advantages become even more significant when generating dense point clouds with 8,192 points, highlighting its scalability and practical potential for high-resolution 3D generation tasks. <div>
arXiv:2503.08594v3 Announce Type: replace-cross 
Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering</title>
<link>https://arxiv.org/abs/2511.05876</link>
<guid>https://arxiv.org/abs/2511.05876</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Multi-View Clustering, Ego Graphs, Mixture-of-Experts, Contrastive Learning<br /><br />Summary:<br /><br />The paper addresses limitations in existing Multi-View Clustering (MVC) techniques that rely on coarse-grained graph fusion by proposing a novel method called Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). The MoEGCL framework consists of two main components: the Mixture of Ego-Graphs Fusion (MoEGF) and the Ego Graph Contrastive Learning (EGCL) module. MoEGF innovatively constructs ego graphs for each sample and applies a Mixture-of-Experts network to achieve fine-grained fusion at the sample level, replacing conventional rough view-level fusion. The EGCL module enhances representation learning by aligning the fused embedding with view-specific representations and strengthens similarity across samples within the same cluster, rather than focusing solely on the same samples. This approach improves the granularity and quality of graph representations in MVC. Extensive experiments validate that MoEGCL significantly outperforms state-of-the-art deep MVC methods. The authors also provide publicly accessible source code to support reproducibility and further research, available at https://github.com/HackerHyper/MoEGCL. <div>
arXiv:2511.05876v3 Announce Type: replace-cross 
Abstract: In recent years, the advancement of Graph Neural Networks (GNNs) has significantly propelled progress in Multi-View Clustering (MVC). However, existing methods face the problem of coarse-grained graph fusion. Specifically, current approaches typically generate a separate graph structure for each view and then perform weighted fusion of graph structures at the view level, which is a relatively rough strategy. To address this limitation, we present a novel Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly consists of two modules. In particular, we propose an innovative Mixture of Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a Mixture-of-Experts network to implement fine-grained fusion of ego graphs at the sample level, rather than the conventional view-level fusion. Additionally, we present the Ego Graph Contrastive Learning (EGCL) module to align the fused representation with the view-specific representation. The EGCL module enhances the representation similarity of samples from the same cluster, not merely from the same sample, further boosting fine-grained graph representation. Extensive experiments demonstrate that MoEGCL achieves state-of-the-art results in deep multi-view clustering tasks. The source code is publicly available at https://github.com/HackerHyper/MoEGCL.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Geometric Calibration and Neutral Zones for Uncertainty-Aware Multi-Class Classification</title>
<link>https://arxiv.org/abs/2511.20960</link>
<guid>https://arxiv.org/abs/2511.20960</guid>
<content:encoded><![CDATA[
<div> Calibration, Uncertainty Quantification, Fisher–Rao Metric, Additive Log-Ratio, Reliability Scores  

<br /><br />Summary:  
This paper introduces a geometric framework for both calibration and instance-level uncertainty quantification of neural network probability outputs, treating predicted probabilities as points on a probability simplex with the Fisher–Rao metric. Key contributions include: (i) the development of Additive Log-Ratio (ALR) calibration maps that generalize Platt scaling from binary to multi-class classification; (ii) the creation of geometric reliability scores that convert calibrated probabilities into actionable measures of uncertainty, enabling the deferral of ambiguous predictions for human review. Theoretical results provide consistency guarantees of the calibration estimator at a convergence rate of \(O_p(n^{-1/2})\) and establish concentration bounds for reliability scores with explicit sub-Gaussian parameters, facilitating validation set design. The authors conjecture the Neyman–Pearson optimality of their neutral zone classifier based on links to Bhattacharyya coefficients. Empirical evaluation on Adeno-Associated Virus classification reveals that the proposed two-stage approach captures 72.5% of errors while deferring 34.5% of samples, cutting automated decision errors from 16.8% to 6.9%. Importantly, performance improvements are driven mainly by the reliability scoring rather than calibration alone. This work merges information geometry with statistical learning to provide formal guarantees and practical tools for uncertainty-aware classification critical in high-stakes applications requiring rigorous validation. <div>
arXiv:2511.20960v2 Announce Type: replace-cross 
Abstract: Modern artificial intelligence systems make critical decisions yet often fail silently when uncertain -- even well-calibrated models provide no mechanism to identify \textit{which specific predictions} are unreliable. We develop a geometric framework addressing both calibration and instance-level uncertainty quantification for neural network probability outputs. Treating probability vectors as points on the $(c-1)$-dimensional probability simplex equipped with the Fisher--Rao metric, we construct: (i) Additive Log-Ratio (ALR) calibration maps that reduce exactly to Platt scaling for binary problems while extending naturally to multi-class settings, and (ii) geometric reliability scores that translate calibrated probabilities into actionable uncertainty measures, enabling principled deferral of ambiguous predictions to human review.
  Theoretical contributions include: consistency of the calibration estimator at rate $O_p(n^{-1/2})$ via M-estimation theory (Theorem~1), and tight concentration bounds for reliability scores with explicit sub-Gaussian parameters enabling sample size calculations for validation set design (Theorem~2). We conjecture Neyman--Pearson optimality of our neutral zone construction based on connections to Bhattacharyya coefficients. Empirical validation on Adeno-Associated Virus classification demonstrates that the two-stage framework captures 72.5\% of errors while deferring 34.5\% of samples, reducing automated decision error rates from 16.8\% to 6.9\%. Notably, calibration alone yields marginal accuracy gains; the operational benefit arises primarily from the reliability scoring mechanism, which applies to any well-calibrated probability output. This work bridges information geometry and statistical learning, offering formal guarantees for uncertainty-aware classification in applications requiring rigorous validation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Crowdsourcing the Frontier: Advancing Hybrid Physics-ML Climate Simulation via a $50,000 Kaggle Competition</title>
<link>https://arxiv.org/abs/2511.20963</link>
<guid>https://arxiv.org/abs/2511.20963</guid>
<content:encoded><![CDATA[
<div> Subgrid parameterization, machine learning, climate models, online stability, hybrid physics-AI<br /><br />Summary:<br /><br />This paper addresses the challenge of integrating subgrid machine-learning (ML) parameterizations into climate models to capture high-resolution physical effects without excessive computational costs. It highlights issues like online instability and inconsistent online performance that have hindered the operational use of ML parameterizations in long-term climate projections. To facilitate progress, the ClimSim dataset and an associated Kaggle competition were launched, inviting the broader machine learning community to tackle the offline problem. The authors evaluate the performance of ML emulators inspired by winning Kaggle team architectures when coupled with an interactive climate model featuring full cloud microphysics, a component traditionally vulnerable to online instability. Their findings demonstrate reproducible online stability across diverse ML architectures in a realistic, low-resolution setting with real-world geography, marking a significant milestone. Despite similar offline and online biases across architectures, responses to design choices like expanding input variables vary notably. Multiple Kaggle-derived architectures achieve state-of-the-art results for key metrics such as zonal mean bias and global root-mean-square error, suggesting that crowdsourcing ML solutions for offline training can effectively enhance online performance. Overall, this work underscores the promise of hybrid physics-AI climate simulations informed by community-driven ML advancements. <div>
arXiv:2511.20963v3 Announce Type: replace-cross 
Abstract: Subgrid machine-learning (ML) parameterizations have the potential to introduce a new generation of climate models that incorporate the effects of higher-resolution physics without incurring the prohibitive computational cost associated with more explicit physics-based simulations. However, important issues, ranging from online instability to inconsistent online performance, have limited their operational use for long-term climate projections. To more rapidly drive progress in solving these issues, domain scientists and machine learning researchers opened up the offline aspect of this problem to the broader machine learning and data science community with the release of ClimSim, a NeurIPS Datasets and Benchmarks publication, and an associated Kaggle competition. This paper reports on the downstream results of the Kaggle competition by coupling emulators inspired by the winning teams' architectures to an interactive climate model (including full cloud microphysics, a regime historically prone to online instability) and systematically evaluating their online performance. Our results demonstrate that online stability in the low-resolution, real-geography setting is reproducible across multiple diverse architectures, which we consider a key milestone. All tested architectures exhibit strikingly similar offline and online biases, though their responses to architecture-agnostic design choices (e.g., expanding the list of input variables) can differ significantly. Multiple Kaggle-inspired architectures achieve state-of-the-art (SOTA) results on certain metrics such as zonal mean bias patterns and global RMSE, indicating that crowdsourcing the essence of the offline problem is one path to improving online performance in hybrid physics-AI climate simulation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Measuring What LLMs Think They Do: SHAP Faithfulness and Deployability on Financial Tabular Classification</title>
<link>https://arxiv.org/abs/2512.00163</link>
<guid>https://arxiv.org/abs/2512.00163</guid>
<content:encoded><![CDATA[
<div> LLMs, SHAP values, financial risk assessment, explainability, LightGBM<br /><br />Summary:<br /><br />1. The paper investigates the use of Large Language Models (LLMs) for classification tasks on structured tabular data, particularly in financial risk assessment, contrasting their performance and explainability against classical machine learning models like LightGBM.  
2. The study evaluates LLMs through zero-shot prompting and examines their reliability and suitability for high-stakes financial applications, an area where their effectiveness remains uncertain.  
3. It systematically generates SHAP (SHapley Additive exPlanations) values to interpret the feature impacts predicted by LLMs and compares these explanations to the models’ own self-reported feature importance as well as to LightGBM’s SHAP values.  
4. The results reveal a clear divergence between the LLMs’ self-explanations and their SHAP values, indicating inconsistencies in their internal feature impact reasoning, along with significant differences between LLM and LightGBM SHAP values.  
5. These findings suggest limitations of current LLMs as standalone classifiers for financial structured data but point to potential improvements through enhanced explainability techniques and few-shot prompting, which could make LLMs more viable in risk-sensitive domains. <div>
arXiv:2512.00163v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have attracted significant attention for classification tasks, offering a flexible alternative to trusted classical machine learning models like LightGBM through zero-shot prompting. However, their reliability for structured tabular data remains unclear, particularly in high stakes applications like financial risk assessment. Our study systematically evaluates LLMs and generates their SHAP values on financial classification tasks. Our analysis shows a divergence between LLMs self-explanation of feature impact and their SHAP values, as well as notable differences between LLMs and LightGBM SHAP values. These findings highlight the limitations of LLMs as standalone classifiers for structured financial modeling, but also instill optimism that improved explainability mechanisms coupled with few-shot prompting will make LLMs usable in risk-sensitive domains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Faster Verified Explanations for Neural Networks</title>
<link>https://arxiv.org/abs/2512.00164</link>
<guid>https://arxiv.org/abs/2512.00164</guid>
<content:encoded><![CDATA[
<div> Verified explanations, neural networks, scalability, FaVeX, robust explanations<br /><br />Summary:<br /><br />1. Verified explanations provide a theoretically sound method to interpret neural network decisions, addressing their black-box nature. <br />2. Existing verified explanation techniques are limited by scalability due to reliance on multiple calls to neural network verifiers, which can have exponential worst-case complexity. <br />3. The authors introduce FaVeX, a novel algorithm to compute verified explanations more efficiently by combining batch and sequential processing of input features and reusing information from previous verification queries. <br />4. FaVeX improves scalability by optimizing how invariances in input features and feature assignments that change predictions are handled. <br />5. The paper proposes a new hierarchical definition for verified explanations called verifier-optimal robust explanations, which explicitly accounts for the incompleteness of network verifiers. <br />6. Experimental results demonstrate that FaVeX and verifier-optimal robust explanations significantly enhance scalability, enabling the production of meaningful formal explanations for neural networks with hundreds of thousands of nonlinear activations. <div>
arXiv:2512.00164v1 Announce Type: new 
Abstract: Verified explanations are a theoretically-principled way to explain the decisions taken by neural networks, which are otherwise black-box in nature. However, these techniques face significant scalability challenges, as they require multiple calls to neural network verifiers, each of them with an exponential worst-case complexity. We present FaVeX, a novel algorithm to compute verified explanations. FaVeX accelerates the computation by dynamically combining batch and sequential processing of input features, and by reusing information from previous queries, both when proving invariances with respect to certain input features, and when searching for feature assignments altering the prediction. Furthermore, we present a novel and hierarchical definition of verified explanations, termed verifier-optimal robust explanations, that explicitly factors the incompleteness of network verifiers within the explanation. Our comprehensive experimental evaluation demonstrates the superior scalability of both FaVeX, and of verifier-optimal robust explanations, which together can produce meaningful formal explanation on networks with hundreds of thousands of non-linear activations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>We Still Don't Understand High-Dimensional Bayesian Optimization</title>
<link>https://arxiv.org/abs/2512.00170</link>
<guid>https://arxiv.org/abs/2512.00170</guid>
<content:encoded><![CDATA[
<div> Bayesian optimization, high-dimensional spaces, linear regression, Gaussian processes, molecular optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of performing Bayesian optimization (BO) in high-dimensional search spaces, a setting where many traditional BO methods struggle due to the curse of dimensionality.<br /><br />2. Existing state-of-the-art techniques try to counteract this by incorporating assumptions such as locality, sparsity, and smoothness into their models.<br /><br />3. Contrary to expectations, the authors show that Bayesian linear regression, one of the simplest approaches, outperforms these complex methods after applying a geometric transformation that prevents the model from overly focusing on boundaries.<br /><br />4. Using Gaussian processes with linear kernels, the method achieves competitive results on tasks with dimensionalities ranging from 60 to 6,000, demonstrating robustness and scalability.<br /><br />5. The linear model approach offers significant computational benefits, enabling closed-form sampling and linear computational scaling with the number of data points, which the authors leverage effectively in large-scale molecular optimization cases involving more than 20,000 observations.<br /><br />6. These findings challenge prevalent assumptions about BO in high-dimensional spaces and suggest a reconsideration of model complexity and prior assumptions when designing optimization methods for such domains. <div>
arXiv:2512.00170v1 Announce Type: new 
Abstract: High-dimensional spaces have challenged Bayesian optimization (BO). Existing methods aim to overcome this so-called curse of dimensionality by carefully encoding structural assumptions, from locality to sparsity to smoothness, into the optimization procedure. Surprisingly, we demonstrate that these approaches are outperformed by arguably the simplest method imaginable: Bayesian linear regression. After applying a geometric transformation to avoid boundary-seeking behavior, Gaussian processes with linear kernels match state-of-the-art performance on tasks with 60- to 6,000-dimensional search spaces. Linear models offer numerous advantages over their non-parametric counterparts: they afford closed-form sampling and their computation scales linearly with data, a fact we exploit on molecular optimization tasks with > 20,000 observations. Coupled with empirical analyses, our results suggest the need to depart from past intuitions about BO methods in high-dimensional spaces.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Orion-Bix: Bi-Axial Attention for Tabular In-Context Learning</title>
<link>https://arxiv.org/abs/2512.00181</link>
<guid>https://arxiv.org/abs/2512.00181</guid>
<content:encoded><![CDATA[
<div> Keywords: tabular foundation model, biaxial attention, meta-learned in-context reasoning, few-shot learning, hierarchical decision routing<br /><br />Summary: Orion-Bix is a novel tabular foundation model designed to tackle challenges in real-world machine learning tasks involving mixed numeric and categorical data, weak feature structures, and limited labeled samples. It employs a biaxial attention mechanism that integrates standard, grouped, hierarchical, and relational attention layers, combining their outputs via multi-CLS summarization to effectively capture both local and global dependencies within data tables. The model features a label-aware in-context learning (ICL) head that adapts dynamically to new tasks and efficiently manages large label spaces using hierarchical decision routing. Orion-Bix is meta-trained on synthetically generated, structurally diverse tabular datasets embedded with causal priors, enabling it to learn transferable inductive biases across heterogeneous data types. Delivered as a scikit-learn compatible package, it surpasses traditional gradient-boosting models and remains competitive with the latest tabular foundation models on public benchmarks. The integration of biaxial attention with episodic meta-training makes Orion-Bix robust and well-suited for few-shot learning scenarios in tabular data. The model and its implementation are publicly accessible on GitHub, promoting reproducibility and further research in the tabular learning domain. <div>
arXiv:2512.00181v1 Announce Type: new 
Abstract: Tabular data drive most real-world machine learning applications, yet building general-purpose models for them remains difficult. Mixed numeric and categorical fields, weak feature structure, and limited labeled data make scaling and generalization challenging. To this end, we introduce Orion-Bix, a tabular foundation model that combines biaxial attention with meta-learned in-context reasoning for few-shot tabular learning. Its encoder alternates standard, grouped, hierarchical, and relational attention, fusing their outputs through multi-CLS summarization to capture both local and global dependencies efficiently. A label-aware ICL head adapts on the fly and scales to large label spaces via hierarchical decision routing. Meta-trained on synthetically generated, structurally diverse tables with causal priors, Orion-Bix learns transferable inductive biases across heterogeneous data. Delivered as a scikit-learn compatible foundation model, it outperforms gradient-boosting baselines and remains competitive with state-of-the-art tabular foundation models on public benchmarks, showing that biaxial attention with episodic meta-training enables robust, few-shot-ready tabular learning. The model is publicly available at https://github.com/Lexsi-Labs/Orion-BiX .
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hybrid Context-Fusion Attention (CFA) U-Net and Clustering for Robust Seismic Horizon Interpretation</title>
<link>https://arxiv.org/abs/2512.00191</link>
<guid>https://arxiv.org/abs/2512.00191</guid>
<content:encoded><![CDATA[
<div> Keywords: seismic horizon interpretation, U-Net, Context Fusion Attention, DBSCAN clustering, sparse annotations<br /><br />Summary:<br /><br />1. The paper addresses the challenge of interpreting seismic horizons for subsurface characterization in hydrocarbon exploration, focusing on improving automated horizon tracking with deep learning.<br /><br />2. A novel hybrid framework is proposed that combines advanced U-Net variants with spatial clustering techniques to enhance horizon continuity and geometric accuracy.<br /><br />3. The core innovation is the Context Fusion Attention (CFA) U-Net, which integrates spatial and Sobel-derived geometric features within attention gates, boosting precision and surface completeness.<br /><br />4. Five model architectures were evaluated (Standard U-Net, compressed U-Net, U-Net++, Attention U-Net, and CFA U-Net) across different data sparsity conditions (10-, 20-, and 40-line spacing), demonstrating that CFA U-Net outperforms existing baselines.<br /><br />5. Experimental results on the Mexilhao field (Santos Basin, Brazil) dataset showed a validation IoU of 0.881 and MAE of 2.49ms, while the F3 Block dataset (North Sea) achieved 97.6% surface coverage under sparse data, confirming robustness.<br /><br />6. Further, Density-Based Spatial Clustering of Applications with Noise (DBSCAN) was employed to refine combined inline and cross-line horizon predictions, leading to geologically plausible surfaces.<br /><br />7. This work highlights the benefits of combining attention-based architectures with geometric context and hybrid clustering methods, offering a generalizable solution for seismic interpretation in structurally complex, data-scarce environments. <div>
arXiv:2512.00191v1 Announce Type: new 
Abstract: Interpreting seismic horizons is a critical task for characterizing subsurface structures in hydrocarbon exploration. Recent advances in deep learning, particularly U-Net-based architectures, have significantly improved automated horizon tracking. However, challenges remain in accurately segmenting complex geological features and interpolating horizons from sparse annotations. To address these issues, a hybrid framework is presented that integrates advanced U-Net variants with spatial clustering to enhance horizon continuity and geometric fidelity. The core contribution is the Context Fusion Attention (CFA) U-Net, a novel architecture that fuses spatial and Sobel-derived geometric features within attention gates to improve both precision and surface completeness. The performance of five architectures, the U-Net (Standard and compressed), U-Net++, Attention U-Net, and CFA U-Net, was systematically evaluated across various data sparsity regimes (10-, 20-, and 40-line spacing). This approach outperformed existing baselines, achieving state-of-the-art results on the Mexilhao field (Santos Basin, Brazil) dataset with a validation IoU of 0.881 and MAE of 2.49ms, and excellent surface coverage of 97.6% on the F3 Block of the North Sea dataset under sparse conditions. The framework further refines merged horizon predictions (inline and cross-line) using Density-Based Spatial Clustering of Applications with Noise (DBSCAN) to produce geologically plausible surfaces. The results demonstrate the advantages of hybrid methodologies and attention-based architectures enhanced with geometric context, providing a robust and generalizable solution for seismic interpretation in structurally complex and data-scarce environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Emergent Riemannian geometry over learning discrete computations on continuous manifolds</title>
<link>https://arxiv.org/abs/2512.00196</link>
<guid>https://arxiv.org/abs/2512.00196</guid>
<content:encoded><![CDATA[
<div> Keywords: neural networks, discrete computations, continuous manifolds, Riemannian metric, generalisation

<br /><br />Summary: This paper investigates how neural networks perform discrete computations when processing continuous input data such as images. The authors analyze the representational geometry of neural networks by examining the Riemannian pullback metric across layers, revealing that network computations consist of two key functions: discretising continuous input features and applying logical operations on these discretised variables. They explore how different learning regimes, specifically rich versus lazy learning, lead to distinct metric and curvature structures within the networks' representational geometry. These geometric differences influence the networks' ability to generalise to unseen inputs, highlighting how the underlying learning dynamics affect performance. The study offers a novel geometric framework that provides insights into the internal mechanisms by which neural networks learn to map continuous inputs to discrete task outputs. This framework bridges gaps in understanding the transition from continuous data manifolds to discrete computations, with implications for improving network design and learning strategies to enhance generalisation on complex tasks. <div>
arXiv:2512.00196v1 Announce Type: new 
Abstract: Many tasks require mapping continuous input data (e.g. images) to discrete task outputs (e.g. class labels). Yet, how neural networks learn to perform such discrete computations on continuous data manifolds remains poorly understood. Here, we show that signatures of such computations emerge in the representational geometry of neural networks as they learn. By analysing the Riemannian pullback metric across layers of a neural network, we find that network computation can be decomposed into two functions: discretising continuous input features and performing logical operations on these discretised variables. Furthermore, we demonstrate how different learning regimes (rich vs. lazy) have contrasting metric and curvature structures, affecting the ability of the networks to generalise to unseen inputs. Overall, our work provides a geometric framework for understanding how neural networks learn to perform discrete computations on continuous manifolds.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Constructing Efficient Fact-Storing MLPs for Transformers</title>
<link>https://arxiv.org/abs/2512.00207</link>
<guid>https://arxiv.org/abs/2512.00207</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, MLP fact storage, parameter efficiency, Transformer integration, fact editing<br /><br />Summary: This paper introduces an improved framework for constructing multilayer perceptrons (MLPs) that explicitly store factual knowledge within large language models (LLMs). First, the proposed MLP construction works for almost all feasible input-output pairs, overcoming limitations of previous methods. Second, it achieves asymptotically optimal parameter efficiency by matching information-theoretic bounds for certain embedding types, thus maximizing facts stored per parameter. Third, the framework maintains compatibility with Transformer architectures, enabling effective factual recall within these models. The authors discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent trained MLPs, providing new insights into fact storage mechanisms. They identify a straightforward encoder-decoder approach that empirically matches the optimal facts-per-parameter scaling observed in gradient-trained MLPs across tested input-output pairs. Furthermore, the study reveals a fundamental tradeoff between an MLP’s fact storage capacity and its practical usability when integrated into Transformers. As a practical application, the paper demonstrates modular fact editing on one-layer Transformers by replacing entire MLP components at once, showcasing the potential for flexible and efficient factual updates in LLMs. <div>
arXiv:2512.00207v1 Announce Type: new 
Abstract: The success of large language models (LLMs) can be attributed in part to their ability to efficiently store factual knowledge as key-value mappings within their MLP parameters. Recent work has proposed explicit weight constructions to build such fact-storing MLPs, providing an improved understanding of LLM fact storage mechanisms. In this paper, we introduce an MLP construction framework that improves over previous constructions in three areas: it 1) works for all but a measure-zero set of feasible input-output pairs, 2) achieves asymptotically optimal parameter efficiency matching information-theoretic bounds for some embeddings, and 3) maintains usability within Transformers for factual recall. Through our improvements, we 1) discover a metric on value embeddings that characterizes facts-per-parameter scaling for both constructed and gradient-descent-trained MLPs, 2) identify a simple encoder-decoder mechanism that empirically matches gradient-descent MLP facts-per-parameter asymptotics across all the inputs and outputs we test, and 3) uncover a fundamental tradeoff between an MLP's fact-storage capacity and its usability within Transformers. Finally, we demonstrate a proof-of-concept application of fact-storing MLPs: modular fact editing on one-layer Transformers by \textit{replacing entire MLPs at once}.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TIE: A Training-Inversion-Exclusion Framework for Visually Interpretable and Uncertainty-Guided Out-of-Distribution Detection</title>
<link>https://arxiv.org/abs/2512.00229</link>
<guid>https://arxiv.org/abs/2512.00229</guid>
<content:encoded><![CDATA[
<div> Keywords: deep neural networks, out-of-distribution detection, uncertainty estimation, anomaly detection, iterative refinement  

<br /><br />Summary:  
This paper addresses the challenge of enabling deep neural networks to reliably detect inputs that lie outside their training distribution, a key issue for building trustworthy machine learning systems. The authors propose TIE (Training–Inversion–Exclusion), a novel framework that integrates predictive uncertainty estimation and out-of-distribution (OOD) detection into a single unified process. TIE transforms a standard multiclass classifier into an \((n+1)\)-class model by introducing a "garbage" class seeded with Gaussian noise to capture outliers. The method involves iterative closed-loop steps in each epoch: training the classifier, inverting it to reconstruct uncertain inputs, and excluding these uncertain inverted samples into the garbage class. Over iterations, these inverted samples shift from noise to interpretable class prototypes, offering visual insight into the learned data manifolds. During inference, TIE rejects OOD inputs either by assigning them to the garbage class or by producing low-confidence in-distribution predictions, without requiring external OOD datasets. Extensive evaluations on multiple benchmarks and metrics such as AUROC, AUPR, and FPR@95%TPR show that TIE achieves near-perfect OOD detection, with practically 0 false positive rate at 95% true positive rate, when trained on datasets like MNIST or FashionMNIST against diverse unseen OOD sets. This makes TIE a robust, interpretable approach for anomaly detection and uncertainty calibration. <div>
arXiv:2512.00229v1 Announce Type: new 
Abstract: Deep neural networks often struggle to recognize when an input lies outside their training experience, leading to unreliable and overconfident predictions. Building dependable machine learning systems therefore requires methods that can both estimate predictive \textit{uncertainty} and detect \textit{out-of-distribution (OOD)} samples in a unified manner. In this paper, we propose \textbf{TIE: a Training--Inversion--Exclusion} framework for visually interpretable and uncertainty-guided anomaly detection that jointly addresses these challenges through iterative refinement. TIE extends a standard $n$-class classifier to an $(n+1)$-class model by introducing a garbage class initialized with Gaussian noise to represent outlier inputs. Within each epoch, TIE performs a closed-loop process of \textit{training, inversion, and exclusion}, where highly uncertain inverted samples reconstructed from the just-trained classifier are excluded into the garbage class. Over successive iterations, the inverted samples transition from noisy artifacts into visually coherent class prototypes, providing transparent insight into how the model organizes its learned manifolds. During inference, TIE rejects OOD inputs by either directly mapping them to the garbage class or producing low-confidence, uncertain misclassifications within the in-distribution classes that are easily separable, all without relying on external OOD datasets. A comprehensive threshold-based evaluation using multiple OOD metrics and performance measures such as \textit{AUROC}, \textit{AUPR}, and \textit{FPR@95\%TPR} demonstrates that TIE offers a unified and interpretable framework for robust anomaly detection and calibrated uncertainty estimation (UE) achieving near-perfect OOD detection with \textbf{\(\!\approx\!\) 0 FPR@95\%TPR} when trained on MNIST or FashionMNIST and tested against diverse unseen datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Self-Supervised Dynamical System Representations for Physiological Time-Series</title>
<link>https://arxiv.org/abs/2512.00239</link>
<guid>https://arxiv.org/abs/2512.00239</guid>
<content:encoded><![CDATA[
<div> Keywords: self-supervised learning, physiological time series, dynamical systems, pretraining framework, representation learning<br /><br />Summary:  
The paper addresses limitations in current self-supervised learning (SSL) methods for physiological time series, which often rely on heuristic strategies or loosely defined generative tasks. It introduces a novel pretraining framework that leverages the information structure inherent in a dynamical systems generative model applied across multiple time series samples. The central insight is that class identity can be robustly captured by focusing on generative variables representing system parameters shared among similar samples, while ignoring noise unique to individual samples. Based on this insight, the authors propose PULSE, a cross-reconstruction-based pretraining objective specifically designed for physiological time series. PULSE explicitly extracts shared system information and discards non-transferable, sample-specific noise. Theoretical analysis provides sufficient conditions for successful recovery of system information. Empirical validation is conducted with synthetic dynamical system experiments to demonstrate the effectiveness of the approach. Moreover, PULSE is applied to various real-world physiological datasets, showing its ability to learn meaningful representations that distinguish semantic classes, improve label efficiency, and enhance transfer learning performance across tasks. This framework thus offers a principled and effective approach to pretraining in physiological time series data environments. <div>
arXiv:2512.00239v1 Announce Type: new 
Abstract: The effectiveness of self-supervised learning (SSL) for physiological time series depends on the ability of a pretraining objective to preserve information about the underlying physiological state while filtering out unrelated noise. However, existing strategies are limited due to reliance on heuristic principles or poorly constrained generative tasks. To address this limitation, we propose a pretraining framework that exploits the information structure of a dynamical systems generative model across multiple time-series. This framework reveals our key insight that class identity can be efficiently captured by extracting information about the generative variables related to the system parameters shared across similar time series samples, while noise unique to individual samples should be discarded. Building on this insight, we propose PULSE, a cross-reconstruction-based pretraining objective for physiological time series datasets that explicitly extracts system information while discarding non-transferrable sample-specific ones. We establish theory that provides sufficient conditions for the system information to be recovered, and empirically validate it using a synthetic dynamical systems experiment. Furthermore, we apply our method to diverse real-world datasets, demonstrating that PULSE learns representations that can broadly distinguish semantic classes, increase label efficiency, and improve transfer learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves</title>
<link>https://arxiv.org/abs/2512.00242</link>
<guid>https://arxiv.org/abs/2512.00242</guid>
<content:encoded><![CDATA[
<div> Keywords: Sheaf Neural Networks, Polynomial Neural Sheaf Diffusion, normalised sheaf Laplacian, heterophily, graph diffusion<br /><br />Summary:<br /><br />1. Sheaf Neural Networks (SNNs) enhance graph learning by associating local vector spaces (stalks) and linear restriction maps to nodes and edges, which provide an edge-sensitive inductive bias especially useful for handling heterophilic graphs and mitigating oversmoothing. <br />2. Existing Neural Sheaf Diffusion models typically depend on computationally expensive SVD-based normalization and dense restriction maps on each edge. These challenges grow with the stalk dimension, require frequent updates of the Laplacian operator, and result in unstable gradients.<br />3. To overcome these problems, the paper introduces Polynomial Neural Sheaf Diffusion (PolyNSD), a novel diffusion technique that represents the propagation operator as a degree-K polynomial of a normalized sheaf Laplacian. This is efficiently computed using a stable three-term recurrence on a spectrally rescaled operator.<br />4. PolyNSD achieves an explicit K-hop receptive field within a single layer that does not depend on the stalk dimension. It includes a trainable spectral response expressed as a convex combination of K+1 orthogonal polynomial basis functions.<br />5. The method stabilizes training via convex mixtures, spectral rescaling, and residual/gated connections, achieving state-of-the-art performance on both homophilic and heterophilic graph benchmarks, while using only diagonal restriction maps. This reduces computational costs and memory usage, reversing previous trends that tied performance to large stalk dimensions. <div>
arXiv:2512.00242v1 Announce Type: new 
Abstract: Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Hierarchical Hybrid AI Approach: Integrating Deep Reinforcement Learning and Scripted Agents in Combat Simulations</title>
<link>https://arxiv.org/abs/2512.00249</link>
<guid>https://arxiv.org/abs/2512.00249</guid>
<content:encoded><![CDATA[
<div> Keywords: combat simulations, intelligent agents, deep reinforcement learning, hierarchical hybrid AI, wargaming

<br /><br />Summary:  
This paper addresses the challenges in developing intelligent agents for combat simulations used in wargaming. Traditional rule-based scripted agents are reliable and predictable in controlled, routine situations but lack flexibility in complex, dynamic scenarios. Deep reinforcement learning (RL) agents provide adaptability and learning capabilities, making them better suited for unexpected situations; however, they often operate as black boxes and struggle to scale effectively in large simulations. To overcome these limitations, the authors propose a novel hierarchical hybrid AI approach that combines the strengths of both methods. The system employs scripted agents at the tactical level for routine decision-making, ensuring consistency and dependability. Meanwhile, RL agents are deployed at the strategic level, where adaptability and learning from experience are crucial. This hierarchical structure leverages the predictability of scripted methods and the flexibility of RL, enhancing overall agent performance. Experimental results demonstrate significant improvements in robustness, adaptability, and effectiveness within complex simulation environments. Consequently, this approach offers a promising framework for the design and training of intelligent agents in wargaming and other complex combat simulations by bridging the gap between scripted and learning-based methodologies. <div>
arXiv:2512.00249v1 Announce Type: new 
Abstract: In the domain of combat simulations in support of wargaming, the development of intelligent agents has predominantly been characterized by rule-based, scripted methodologies with deep reinforcement learning (RL) approaches only recently being introduced. While scripted agents offer predictability and consistency in controlled environments, they fall short in dynamic, complex scenarios due to their inherent inflexibility. Conversely, RL agents excel in adaptability and learning, offering potential improvements in handling unforeseen situations, but suffer from significant challenges such as black-box decision-making processes and scalability issues in larger simulation environments. This paper introduces a novel hierarchical hybrid artificial intelligence (AI) approach that synergizes the reliability and predictability of scripted agents with the dynamic, adaptive learning capabilities of RL. By structuring the AI system hierarchically, the proposed approach aims to utilize scripted agents for routine, tactical-level decisions and RL agents for higher-level, strategic decision-making, thus addressing the limitations of each method while leveraging their individual strengths. This integration is shown to significantly improve overall performance, providing a robust, adaptable, and effective solution for developing and training intelligent agents in complex simulation environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SD-CGAN: Conditional Sinkhorn Divergence GAN for DDoS Anomaly Detection in IoT Networks</title>
<link>https://arxiv.org/abs/2512.00251</link>
<guid>https://arxiv.org/abs/2512.00251</guid>
<content:encoded><![CDATA[
<div> IoT anomaly detection, DoS attacks, Sinkhorn Divergence, CTGAN augmentation, edge computing  

<br /><br />Summary:  
This paper addresses the challenge of anomaly detection in increasingly complex Internet of Things (IoT) edge networks, focusing on advanced Denial-of-Service (DoS) attacks and zero-day exploits amidst dynamic and imbalanced traffic conditions. The authors introduce SD-CGAN, a novel Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, designed specifically for robust anomaly detection in IoT edge environments. SD-CGAN integrates CTGAN-based synthetic data augmentation to tackle class imbalance by generating high-quality minority class samples. Sinkhorn Divergence is employed as a geometry-aware loss function, which contributes to improved training stability and mitigates mode collapse issues typical in GANs. The framework is rigorously evaluated using exploitative attack subsets extracted from the CICDDoS2019 dataset, which is known for representing realistic Distributed Denial-of-Service scenarios. Experimental results demonstrate that SD-CGAN outperforms baseline deep learning models and other GAN-based methods in terms of detection accuracy, precision, recall, and F1-score metrics. Additionally, the model maintains computational efficiency, making it well suited for deployment in resource-constrained edge computing devices within IoT networks. Overall, the proposed approach represents a significant advancement for enhancing security and resilience in IoT edge environments against sophisticated cyber attacks. <div>
arXiv:2512.00251v1 Announce Type: new 
Abstract: The increasing complexity of IoT edge networks presents significant challenges for anomaly detection, particularly in identifying sophisticated Denial-of-Service (DoS) attacks and zero-day exploits under highly dynamic and imbalanced traffic conditions. This paper proposes SD-CGAN, a Conditional Generative Adversarial Network framework enhanced with Sinkhorn Divergence, tailored for robust anomaly detection in IoT edge environments. The framework incorporates CTGAN-based synthetic data augmentation to address class imbalance and leverages Sinkhorn Divergence as a geometry-aware loss function to improve training stability and reduce mode collapse. The model is evaluated on exploitative attack subsets from the CICDDoS2019 dataset and compared against baseline deep learning and GAN-based approaches. Results show that SD-CGAN achieves superior detection accuracy, precision, recall, and F1-score while maintaining computational efficiency suitable for deployment in edge-enabled IoT environments.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Scalable and Interpretable Scientific Discovery via Sparse Variational Gaussian Process Kolmogorov-Arnold Networks (SVGP KAN)</title>
<link>https://arxiv.org/abs/2512.00260</link>
<guid>https://arxiv.org/abs/2512.00260</guid>
<content:encoded><![CDATA[
<div> Keywords: Kolmogorov-Arnold Networks, Gaussian Processes, Sparse Variational Inference, Computational Scalability, Structural Identification<br /><br />Summary:<br /><br />Kolmogorov-Arnold Networks (KANs) improve interpretability by placing learnable univariate functions on network edges, serving as an alternative to standard Multi-Layer Perceptrons (MLPs). However, traditional KANs lack probabilistic outputs, which limits their utility in scenarios that require uncertainty quantification. Recent extensions incorporating Gaussian Processes (GPs) provide probabilistic modeling but rely on exact inference, which scales cubically with the dataset size \(N\), making them impractical for large datasets. To address this, the authors introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that combines sparse variational inference with the KAN framework. This method utilizes \(M\) inducing points and analytic moment matching to reduce computational complexity significantly—from \(O(N^3)\) down to \(O(NM^2)\) or linear in the number of samples—enabling efficient processing of larger scientific datasets. Additionally, the integration of a permutation-based importance analysis equips the network with the capability for structural identification, allowing it to pinpoint relevant input variables and classify functional relationships. This advancement broadens the applicability of probabilistic KANs and enhances their interpretability and utility in complex data analysis tasks. <div>
arXiv:2512.00260v1 Announce Type: new 
Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising alternative to Multi-Layer Perceptron (MLP) by placing learnable univariate functions on network edges, enhancing interpretability. However, standard KANs lack probabilistic outputs, limiting their utility in applications requiring uncertainty quantification. While recent Gaussian Process (GP) extensions to KANs address this, they utilize exact inference methods that scale cubically with data size N, restricting their application to smaller datasets. We introduce the Sparse Variational GP-KAN (SVGP-KAN), an architecture that integrates sparse variational inference with the KAN topology. By employing $M$ inducing points and analytic moment matching, our method reduces computational complexity from $O(N^3)$ to $O(NM^2)$ or linear in sample size, enabling the application of probabilistic KANs to larger scientific datasets. Furthermore, we demonstrate that integrating a permutation-based importance analysis enables the network to function as a framework for structural identification, identifying relevant inputs and classifying functional relationships.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teleportation-Based Defenses for Privacy in Approximate Machine Unlearning</title>
<link>https://arxiv.org/abs/2512.00272</link>
<guid>https://arxiv.org/abs/2512.00272</guid>
<content:encoded><![CDATA[
<div> Keywords: approximate machine unlearning, privacy risks, membership inference, neural network symmetries, teleportation defense<br /><br />Summary:<br /><br />Approximate machine unlearning focuses on efficiently removing the impact of specific data points from trained models as a practical alternative to fully retraining them. However, it introduces privacy vulnerabilities, as attackers with access to models before and after unlearning can perform membership inference or data reconstruction attacks. The root causes of these risks are identified as large gradient norms associated with samples that need to be forgotten and the closeness of the unlearned model’s parameters to the original. To assess the severity, the authors propose unlearning-specific membership inference and reconstruction attacks, revealing that even state-of-the-art unlearning methods like NGP and SCRUB remain vulnerable. To address these threats, the paper introduces WARP, a defense mechanism that utilizes neural network symmetries to perform a "teleportation" reparameterization. This approach reduces the gradient energy of forget-set samples and increases parameter dispersion without sacrificing the model’s predictive performance. As a result, WARP effectively obfuscates signals from forgotten data, impeding attackers’ abilities to distinguish forgotten samples from non-members or reconstruct forgotten data. The method demonstrates consistent privacy improvements across six different unlearning algorithms, significantly lowering adversarial advantage in both black-box (up to 64% reduction) and white-box (up to 92% reduction) attack settings, all while preserving accuracy on retained data. This work positions teleportation as a generalizable tool for enhancing privacy in approximate machine unlearning. <div>
arXiv:2512.00272v1 Announce Type: new 
Abstract: Approximate machine unlearning aims to efficiently remove the influence of specific data points from a trained model, offering a practical alternative to full retraining. However, it introduces privacy risks: an adversary with access to pre- and post-unlearning models can exploit their differences for membership inference or data reconstruction. We show these vulnerabilities arise from two factors: large gradient norms of forget-set samples and the close proximity of unlearned parameters to the original model. To demonstrate their severity, we propose unlearning-specific membership inference and reconstruction attacks, showing that several state-of-the-art methods (e.g., NGP, SCRUB) remain vulnerable. To mitigate this leakage, we introduce WARP, a plug-and-play teleportation defense that leverages neural network symmetries to reduce forget-set gradient energy and increase parameter dispersion while preserving predictions. This reparameterization obfuscates the signal of forgotten data, making it harder for attackers to distinguish forgotten samples from non-members or recover them via reconstruction. Across six unlearning algorithms, our approach achieves consistent privacy gains, reducing adversarial advantage (AUC) by up to 64% in black-box and 92% in white-box settings, while maintaining accuracy on retained data. These results highlight teleportation as a general tool for reducing attack success in approximate unlearning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>BioArc: Discovering Optimal Neural Architectures for Biological Foundation Models</title>
<link>https://arxiv.org/abs/2512.00283</link>
<guid>https://arxiv.org/abs/2512.00283</guid>
<content:encoded><![CDATA[
<div> Keywords: BioArc, Neural Architecture Search, foundation models, biological data, architecture design<br /><br />Summary:<br /><br />1. Foundation models have made significant advances in fields like NLP and computer vision, but applying their architectures directly to biological data has led to suboptimal outcomes.<br />2. Existing models fail to adequately address the unique physicochemical and structural characteristics of biological modalities, such as long-range dependencies, sparse information, and complex data "grammars."<br />3. To overcome these challenges, the authors propose BioArc, a framework that leverages Neural Architecture Search (NAS) to automatically and systematically discover optimal architectures tailored for biological foundation models.<br />4. BioArc explores a large architecture design space across diverse biological modalities, analyzing how architecture choices interact with tokenization methods and training strategies.<br />5. This extensive analysis enables identification of novel high-performance architectures and the derivation of empirical design principles to guide future biological model development.<br />6. Additionally, the paper introduces architecture prediction methods that can efficiently and effectively propose optimal architectures for new biological tasks, enhancing adaptability.<br />7. Overall, this work contributes a principled methodology and resource to the community for building the next generation of task-specific and foundation models in biology, moving beyond intuition-driven designs toward data-driven architecture discovery. <div>
arXiv:2512.00283v1 Announce Type: new 
Abstract: Foundation models have revolutionized various fields such as natural language processing (NLP) and computer vision (CV). While efforts have been made to transfer the success of the foundation models in general AI domains to biology, existing works focus on directly adopting the existing foundation model architectures from general machine learning domains without a systematic design considering the unique physicochemical and structural properties of each biological data modality. This leads to suboptimal performance, as these repurposed architectures struggle to capture the long-range dependencies, sparse information, and complex underlying ``grammars'' inherent to biological data. To address this gap, we introduce BioArc, a novel framework designed to move beyond intuition-driven architecture design towards principled, automated architecture discovery for biological foundation models. Leveraging Neural Architecture Search (NAS), BioArc systematically explores a vast architecture design space, evaluating architectures across multiple biological modalities while rigorously analyzing the interplay between architecture, tokenization, and training strategies. This large-scale analysis identifies novel, high-performance architectures, allowing us to distill a set of empirical design principles to guide future model development. Furthermore, to make the best of this set of discovered principled architectures, we propose and compare several architecture prediction methods that effectively and efficiently predict optimal architectures for new biological tasks. Overall, our work provides a foundational resource and a principled methodology to guide the creation of the next generation of task-specific and foundation models for biology.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data-Driven Modeling and Correction of Vehicle Dynamics</title>
<link>https://arxiv.org/abs/2512.00289</link>
<guid>https://arxiv.org/abs/2512.00289</guid>
<content:encoded><![CDATA[
<div> Keywords: non-autonomous vehicle dynamics, data-driven correction, DRIPS, Flow Map Learning, model-form uncertainty

<br /><br />Summary:  
This work addresses the challenge of learning and correcting non-autonomous vehicle dynamics characterized by time-dependent control inputs, which complicate direct temporal modeling. The authors propose reformulating the vehicle dynamics into a sequence of local parametric dynamical systems by locally parameterizing the time-dependent inputs. Two complementary data-driven approaches are developed. First, DRIPS (dimension reduction and interpolation in parameter space) constructs efficient linear surrogate models via lifted observable spaces and manifold-based operator interpolation, enabling robust and data-efficient learning for systems with dynamics linearly representable in the lifted space. Second, FML (Flow Map Learning), a deep neural network method, approximates parametric evolution maps without requiring explicit nonlinear treatment, and is further enhanced with transfer learning for model correction. This correction process can adjust misspecified prior models using sparse high-fidelity or experimental data without needing a predefined correction form. Numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models validate that DRIPS excels in data efficiency and robustness for learning non-autonomous dynamics, while FML delivers expressive nonlinear modeling and effective correction of model-form errors under limited data conditions. The framework provides a powerful combination of physics-based modeling and data-driven adaptation for vehicle dynamics. <div>
arXiv:2512.00289v1 Announce Type: new 
Abstract: We develop a data-driven framework for learning and correcting non-autonomous vehicle dynamics. Physics-based vehicle models are often simplified for tractability and therefore exhibit inherent model-form uncertainty, motivating the need for data-driven correction. Moreover, non-autonomous dynamics are governed by time-dependent control inputs, which pose challenges in learning predictive models directly from temporal snapshot data. To address these, we reformulate the vehicle dynamics via a local parameterization of the time-dependent inputs, yielding a modified system composed of a sequence of local parametric dynamical systems. We approximate these parametric systems using two complementary approaches. First, we employ the DRIPS (dimension reduction and interpolation in parameter space) methodology to construct efficient linear surrogate models, equipped with lifted observable spaces and manifold-based operator interpolation. This enables data-efficient learning of vehicle models whose dynamics admit accurate linear representations in the lifted spaces. Second, for more strongly nonlinear systems, we employ FML (Flow Map Learning), a deep neural network approach that approximates the parametric evolution map without requiring special treatment of nonlinearities. We further extend FML with a transfer-learning-based model correction procedure, enabling the correction of misspecified prior models using only a sparse set of high-fidelity or experimental measurements, without assuming a prescribed form for the correction term. Through a suite of numerical experiments on unicycle, simplified bicycle, and slip-based bicycle models, we demonstrate that DRIPS offers robust and highly data-efficient learning of non-autonomous vehicle dynamics, while FML provides expressive nonlinear modeling and effective correction of model-form errors under severe data scarcity.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FiCoTS: Fine-to-Coarse LLM-Enhanced Hierarchical Cross-Modality Interaction for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2512.00293</link>
<guid>https://arxiv.org/abs/2512.00293</guid>
<content:encoded><![CDATA[
<div> Keywords: Time Series Forecasting, Large Language Models, Multimodal Learning, Cross-Modality Interaction, FiCoTS<br /><br />Summary: This paper addresses the challenge of time series forecasting by leveraging Large Language Models (LLMs) through a novel LLM-as-Enhancer paradigm, contrasting with the common LLM-as-Predictor approach. The authors highlight the difficulty in aligning semantic information between time series and textual modalities due to their distinct natures. To overcome this, FiCoTS is proposed—a fine-to-coarse multimodal framework that enhances forecasting performance by progressively integrating text and time series data across three levels: token-level, feature-level, and decision-level. At the token level, a dynamic heterogeneous graph filters noise and aligns time series segments with corresponding text tokens. The feature-level module employs a global cross-attention mechanism that connects each time series variable with relevant textual context, facilitating deeper semantic interaction. At the decision level, a gated network adaptively fuses outputs from both modalities to generate robust and accurate predictions. The synergy among these modules enables effective cross-modal information flow and supports temporal forecasting. Extensive experiments on seven real-world datasets demonstrate FiCoTS achieves state-of-the-art forecasting accuracy. The authors also commit to releasing their code publicly, promoting reproducibility and further research in multimodal time series forecasting. <div>
arXiv:2512.00293v1 Announce Type: new 
Abstract: Time series forecasting is central to data analysis and web technologies. The recent success of Large Language Models (LLMs) offers significant potential for this field, especially from the cross-modality aspect. Most methods adopt an LLM-as-Predictor paradigm, using LLM as the forecasting backbone and designing modality alignment mechanisms to enable LLM to understand time series data. However, the semantic information in the two modalities of time series and text differs significantly, making it challenging for LLM to fully understand time series data. To mitigate this challenge, our work follows an LLM-as-Enhancer paradigm to fully utilize the advantage of LLM in text understanding, where LLM is only used to encode text modality to complement time series modality. Based on this paradigm, we propose FiCoTS, an LLM-enhanced fine-to-coarse framework for multimodal time series forecasting. Specifically, the framework facilitates progressive cross-modality interaction by three levels in a fine-to-coarse scheme: First, in the token-level modality alignment module, a dynamic heterogeneous graph is constructed to filter noise and align time series patches with text tokens; Second, in the feature-level modality interaction module, a global cross-attention mechanism is introduced to enable each time series variable to connect with relevant textual contexts; Third, in the decision-level modality fusion module, we design a gated network to adaptively fuse the results of the two modalities for robust predictions. These three modules work synergistically to let the two modalities interact comprehensively across three semantic levels, enabling textual information to effectively support temporal prediction. Extensive experiments on seven real-world benchmarks demonstrate that our model achieves state-of-the-art performance. The codes will be released publicly.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Challenges of Heterogeneity in Big Data: A Comparative Study of Classification in Large-Scale Structured and Unstructured Domains</title>
<link>https://arxiv.org/abs/2512.00298</link>
<guid>https://arxiv.org/abs/2512.00298</guid>
<content:encoded><![CDATA[
<div> Variety, Big Data, Hyperparameter Optimization, Text Classification, Model Generalization<br /><br />Summary:<br /><br />This study investigates the role of heterogeneity, termed "Variety," in Big Data classification tasks by comparing approaches across different data types: structured numerical data (Epsilon dataset) and unstructured textual data (Rest-Mex and IMDB datasets). Two key methodologies are employed: evolutionary and Bayesian hyperparameter optimization techniques, specifically Genetic Algorithms and Optuna implemented in Python for numerical data, and distributed processing using Apache Spark applied to large text corpora. The findings reveal a notable "complexity paradox" where, in high-dimensional numerical spaces, simpler optimized linear models like Support Vector Machines and Logistic Regression outperform more complex models such as deep learning architectures and Gradient Boosting. For text domains, limited distributed fine-tuning capabilities cause complex models to overfit; instead, effective feature engineering strategies—namely Transformer-based embeddings like ROBERTa and Bayesian Target Encoding—allow simpler models to achieve better generalization. Ultimately, the work proposes a unified framework guiding algorithm selection depending on the dataset’s characteristics and computational infrastructure constraints, balancing model complexity with data variety and scalability considerations. <div>
arXiv:2512.00298v1 Announce Type: new 
Abstract: This study analyzes the impact of heterogeneity ("Variety") in Big Data by comparing classification strategies across structured (Epsilon) and unstructured (Rest-Mex, IMDB) domains. A dual methodology was implemented: evolutionary and Bayesian hyperparameter optimization (Genetic Algorithms, Optuna) in Python for numerical data, and distributed processing in Apache Spark for massive textual corpora. The results reveal a "complexity paradox": in high-dimensional spaces, optimized linear models (SVM, Logistic Regression) outperformed deep architectures and Gradient Boosting. Conversely, in text-based domains, the constraints of distributed fine-tuning led to overfitting in complex models, whereas robust feature engineering -- specifically Transformer-based embeddings (ROBERTa) and Bayesian Target Encoding -- enabled simpler models to generalize effectively. This work provides a unified framework for algorithm selection based on data nature and infrastructure constraints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Gradient Inversion in Federated Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00303</link>
<guid>https://arxiv.org/abs/2512.00303</guid>
<content:encoded><![CDATA[
<div> Federated reinforcement learning, privacy leakage, gradient inversion, transition dynamics, regularization  

<br /><br />Summary:  
Federated Reinforcement Learning (FRL) allows multiple agents to collaboratively learn optimal policies without sharing their raw data, protecting local data privacy through gradient sharing. However, FRL remains vulnerable to privacy attacks where adversaries attempt to reconstruct private local training data by exploiting the shared gradients. Unlike in supervised federated learning, successful data reconstruction in FRL is more challenging because the reconstructed data must not only match the shared gradients but must also align with the true transition dynamics of the environment, reflecting real-world state transitions and rewards. To tackle this challenge, the authors introduce a new attack technique named Regularization Gradient Inversion Attack (RGIA). RGIA incorporates prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process, guiding the reconstruction toward data consistent with actual environment transitions. Theoretically, it is demonstrated that this regularization significantly reduces the space of possible solutions, filtering out spurious data reconstructions and narrowing the results to those that satisfy both the gradient information and true environment dynamics. Extensive experiments on control and autonomous driving tasks validate that RGIA can precisely reconstruct local private data with transition distributions closely matching the original, highlighting critical privacy risks in FRL systems. <div>
arXiv:2512.00303v1 Announce Type: new 
Abstract: Federated reinforcement learning (FRL) enables distributed learning of optimal policies while preserving local data privacy through gradient sharing.However, FRL faces the risk of data privacy leaks, where attackers exploit shared gradients to reconstruct local training data.Compared to traditional supervised federated learning, successful reconstruction in FRL requires the generated data not only to match the shared gradients but also to align with real transition dynamics of the environment (i.e., aligning with the real data transition distribution).To address this issue, we propose a novel attack method called Regularization Gradient Inversion Attack (RGIA), which enforces prior-knowledge-based regularization on states, rewards, and transition dynamics during the optimization process to ensure that the reconstructed data remain close to the true transition distribution.Theoretically, we prove that the prior-knowledge-based regularization term narrows the solution space from a broad set containing spurious solutions to a constrained subset that satisfies both gradient matching and true transition dynamics.Extensive experiments on control tasks and autonomous driving tasks demonstrate that RGIA can effectively constrain reconstructed data transition distributions and thus successfully reconstruct local private data.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adversarial Signed Graph Learning with Differential Privacy</title>
<link>https://arxiv.org/abs/2512.00307</link>
<guid>https://arxiv.org/abs/2512.00307</guid>
<content:encoded><![CDATA[
<div> Signed graphs, balance theory, differential privacy, adversarial learning, gradient perturbation<br /><br />Summary:<br /><br />This paper addresses the challenge of privacy-preserving learning on signed graphs, which represent relationships with both positive and negative edges. It leverages balance theory to inform edge sign inference but highlights privacy risks where model parameters could leak sensitive link information. Traditional differential privacy (DP) methods using edge or gradient perturbation are inadequate for signed graphs due to cascading sign errors and high sensitivity from sign flips. To overcome this, the authors propose ASGL, an adversarial signed graph learning framework that ensures node-level DP while maintaining high utility. ASGL works by decomposing the signed graph into positive and negative subgraphs, enabling a novel gradient-perturbed adversarial module to better approximate signed connectivity distributions. This approach reduces cascading inference errors and lowers sensitivity through subgraph separation. Additionally, a constrained breadth-first search tree strategy integrated with balance theory helps accurately identify edge signs for generated node pairs and facilitates gradient decoupling to further minimize sensitivity. The effectiveness of ASGL is demonstrated via extensive experiments on real-world datasets, showing it achieves a favorable balance between privacy protection and utility across various downstream tasks. <div>
arXiv:2512.00307v1 Announce Type: new 
Abstract: Signed graphs with positive and negative edges can model complex relationships in social networks. Leveraging on balance theory that deduces edge signs from multi-hop node pairs, signed graph learning can generate node embeddings that preserve both structural and sign information. However, training on sensitive signed graphs raises significant privacy concerns, as model parameters may leak private link information. Existing protection methods with differential privacy (DP) typically rely on edge or gradient perturbation for unsigned graph protection. Yet, they are not well-suited for signed graphs, mainly because edge perturbation tends to cascading errors in edge sign inference under balance theory, while gradient perturbation increases sensitivity due to node interdependence and gradient polarity change caused by sign flips, resulting in larger noise injection. In this paper, motivated by the robustness of adversarial learning to noisy interactions, we present ASGL, a privacy-preserving adversarial signed graph learning method that preserves high utility while achieving node-level DP. We first decompose signed graphs into positive and negative subgraphs based on edge signs, and then design a gradient-perturbed adversarial module to approximate the true signed connectivity distribution. In particular, the gradient perturbation helps mitigate cascading errors, while the subgraph separation facilitates sensitivity reduction. Further, we devise a constrained breadth-first search tree strategy that fuses with balance theory to identify the edge signs between generated node pairs. This strategy also enables gradient decoupling, thereby effectively lowering gradient sensitivity. Extensive experiments on real-world datasets show that ASGL achieves favorable privacy-utility trade-offs across multiple downstream tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Tracing Mathematical Proficiency Through Problem-Solving Processes</title>
<link>https://arxiv.org/abs/2512.00311</link>
<guid>https://arxiv.org/abs/2512.00311</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge Tracing, problem-solving process, mathematical proficiency, large language model, interpretability  

<br /><br />Summary:  
This paper addresses the limitations of traditional Knowledge Tracing (KT) methods, which typically rely only on whether students' answers are correct and overlook detailed problem-solving processes. To bridge this gap, the authors propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), a novel approach that integrates students' step-by-step solution behaviors to better capture multidimensional mathematical proficiency. The study introduces a new dataset named KT-PSP-25, specifically designed to support this enriched KT modeling. Central to the method is StatusKT, a framework that implements a teacher-student-teacher three-stage large language model (LLM) pipeline. In this pipeline, the first teacher LLM extracts problem-specific proficiency indicators from the problem-solving process, the student LLM simulates responses based on these indicators and the student’s process, and the final teacher LLM evaluates these responses to assess mastery over each proficiency indicator. Experimental evaluation on the KT-PSP-25 dataset demonstrates that StatusKT significantly enhances the predictive performance of existing KT methods. Importantly, beyond improved predictions, StatusKT offers interpretable explanations by explicitly modeling and communicating students’ mathematical proficiency states, thus contributing to explainability and personalized learning in Intelligent Tutoring Systems. <div>
arXiv:2512.00311v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) aims to model student's knowledge state and predict future performance to enable personalized learning in Intelligent Tutoring Systems. However, traditional KT methods face fundamental limitations in explainability, as they rely solely on the response correctness, neglecting the rich information embedded in students' problem-solving processes. To address this gap, we propose Knowledge Tracing Leveraging Problem-Solving Process (KT-PSP), which incorporates students' problem-solving processes to capture the multidimensional aspects of mathematical proficiency. We also introduce KT-PSP-25, a new dataset specifically designed for the KT-PSP. Building on this, we present StatusKT, a KT framework that employs a teacher-student-teacher three-stage LLM pipeline to extract students' MP as intermediate signals. In this pipeline, the teacher LLM first extracts problem-specific proficiency indicators, then a student LLM generates responses based on the student's solution process, and a teacher LLM evaluates these responses to determine mastery of each indicator. The experimental results on KT-PSP-25 demonstrate that StatusKT improves the prediction performance of existing KT methods. Moreover, StatusKT provides interpretable explanations for its predictions by explicitly modeling students' mathematical proficiency.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Introducing AI-Driven IoT Energy Management Framework</title>
<link>https://arxiv.org/abs/2512.00321</link>
<guid>https://arxiv.org/abs/2512.00321</guid>
<content:encoded><![CDATA[
<div> Keywords: power consumption, IoT systems, contextual decision making, forecasting, energy management<br /><br />Summary:<br /><br />This article addresses the importance of reducing power consumption due to increasing reliance on technology, highlighting how better power usage prediction can decrease costs and improve electrical grid reliability. It proposes a comprehensive framework for IoT systems focused on contextual decision making, proactive adaptation, and scalability, aiming to create a structured process that enhances accuracy and interconnected development. The framework integrates long-term and short-term power consumption forecasting, anomaly detection, and qualitative data considerations into energy management decisions. The study demonstrates the feasibility of this holistic approach by applying each component of the framework. The framework’s performance was evaluated using Power Consumption Time Series data, showing its direct applicability to real-world energy management challenges. Overall, the approach supports reducing power consumption and contributes to grid stability through improved forecasting and adaptive decision-making mechanisms in IoT environments. <div>
arXiv:2512.00321v1 Announce Type: new 
Abstract: Power consumption has become a critical aspect of modern life due to the consistent reliance on technological advancements. Reducing power consumption or following power usage predictions can lead to lower monthly costs and improved electrical reliability. The proposal of a holistic framework to establish a foundation for IoT systems with a focus on contextual decision making, proactive adaptation, and scalable structure. A structured process for IoT systems with accuracy and interconnected development would support reducing power consumption and support grid stability. This study presents the feasibility of this proposal through the application of each aspect of the framework. This system would have long term forecasting, short term forecasting, anomaly detection, and consideration of qualitative data with any energy management decisions taken. Performance was evaluated on Power Consumption Time Series data to display the direct application of the framework.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive prediction theory combining offline and online learning</title>
<link>https://arxiv.org/abs/2512.00342</link>
<guid>https://arxiv.org/abs/2512.00342</guid>
<content:encoded><![CDATA[
<div> Keywords: offline learning, online adaptation, nonlinear stochastic dynamical systems, generalization error, meta-LMS prediction<br /><br />Summary:<br /><br />This paper explores a two-stage learning framework that combines offline learning and online adaptation to improve prediction performance in real-world intelligence systems dealing with nonlinear stochastic dynamical systems. First, the authors analyze the offline-learning phase by deriving an upper bound on the generalization error for approximate nonlinear-least-squares estimation under datasets exhibiting strong correlations and distribution shifts. The Kullback-Leibler divergence is employed to measure distributional discrepancies, providing a theoretical foundation for handling non-stationary data. Second, for the online-adaptation phase, a meta-LMS prediction algorithm is proposed to adjust for uncertain parameter drift based on the offline-trained model, addressing challenges present in real-world target systems. This combination effectively integrates the strengths of both offline and online approaches, resulting in superior prediction accuracy compared to using purely offline or purely online methods alone. The paper substantiates these findings with both theoretical guarantees and empirical experiments, showcasing the practical benefits of the proposed framework in handling correlated and evolving system data. <div>
arXiv:2512.00342v1 Announce Type: new 
Abstract: Real-world intelligence systems usually operate by combining offline learning and online adaptation with highly correlated and non-stationary system data or signals, which, however, has rarely been investigated theoretically in the literature. This paper initiates a theoretical investigation on the prediction performance of a two-stage learning framework combining offline and online algorithms for a class of nonlinear stochastic dynamical systems. For the offline-learning phase, we establish an upper bound on the generalization error for approximate nonlinear-least-squares estimation under general datasets with strong correlation and distribution shift, leveraging the Kullback-Leibler divergence to quantify the distributional discrepancies. For the online-adaptation phase, we address, on the basis of the offline-trained model, the possible uncertain parameter drift in real-world target systems by proposing a meta-LMS prediction algorithm. This two-stage framework, integrating offline learning with online adaptation, demonstrates superior prediction performances compared with either purely offline or online methods. Both theoretical guarantees and empirical studies are provided.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Memory Efficient Self-Play Algorithm for Model-free Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00351</link>
<guid>https://arxiv.org/abs/2512.00351</guid>
<content:encoded><![CDATA[
<div> Keywords: multi-agent reinforcement learning, Nash Q-Learning, sample complexity, computational complexity, Markov games<br /><br />Summary: The paper addresses key challenges in multi-agent reinforcement learning (MARL), focusing on two-player zero-sum Markov games. It introduces Memory-Efficient Nash Q-Learning (ME-Nash-QL), a model-free self-play algorithm designed to improve existing theoretical limitations such as memory inefficiency, high sample and computational complexity, and non-Markov or non-Nash policies. ME-Nash-QL can output an ε-approximate Nash policy with space complexity \(O(SABH)\) and sample complexity \(\widetilde{O}(H^4SAB/\varepsilon^2)\), improving over prior methods notably when the minimum of actions \(A\) or \(B\) is much smaller than \(H^2\). The algorithm achieves the lowest computational complexity, \(O(T \mathrm{poly}(AB))\), preserving Markov policies efficiently for \(T\) samples. Furthermore, it boasts a significantly reduced burn-in cost of \(O(SAB\, \mathrm{poly}(H))\), compared to earlier approaches requiring at least \(O(S^3 AB\, \mathrm{poly}(H))\). Overall, ME-Nash-QL offers substantial advances in memory use, sample efficiency, and computation in MARL for tabular settings with long horizons, contributing a practical and theoretically sound solution to zero-sum stochastic games. <div>
arXiv:2512.00351v1 Announce Type: new 
Abstract: The thriving field of multi-agent reinforcement learning (MARL) studies how a group of interacting agents make decisions autonomously in a shared dynamic environment. Existing theoretical studies in this area suffer from at least two of the following obstacles: memory inefficiency, the heavy dependence of sample complexity on the long horizon and the large state space, the high computational complexity, non-Markov policy, non-Nash policy, and high burn-in cost. In this work, we take a step towards settling this problem by designing a model-free self-play algorithm \emph{Memory-Efficient Nash Q-Learning (ME-Nash-QL)} for two-player zero-sum Markov games, which is a specific setting of MARL. ME-Nash-QL is proven to enjoy the following merits. First, it can output an $\varepsilon$-approximate Nash policy with space complexity $O(SABH)$ and sample complexity $\widetilde{O}(H^4SAB/\varepsilon^2)$, where $S$ is the number of states, $\{A, B\}$ is the number of actions for two players, and $H$ is the horizon length. It outperforms existing algorithms in terms of space complexity for tabular cases, and in terms of sample complexity for long horizons, i.e., when $\min\{A, B\}\ll H^2$. Second, ME-Nash-QL achieves the lowest computational complexity $O(T\mathrm{poly}(AB))$ while preserving Markov policies, where $T$ is the number of samples. Third, ME-Nash-QL also achieves the best burn-in cost $O(SAB\,\mathrm{poly}(H))$, whereas previous algorithms have a burn-in cost of at least $O(S^3 AB\,\mathrm{poly}(H))$ to attain the same level of sample complexity with ours.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sample-Efficient Tabular Self-Play for Offline Robust Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00352</link>
<guid>https://arxiv.org/abs/2512.00352</guid>
<content:encoded><![CDATA[
<div> Robust Reinforcement Learning, Two-player Zero-sum Games, Offline Learning, Sample Complexity, Markov Games<br /><br />Summary:<br /><br />This work addresses multi-agent reinforcement learning (MARL) in the context of robust two-player zero-sum Markov games (TZMGs) under offline settings, focusing on tabular robust TZMGs (RTZMGs). The authors propose a novel model-based algorithm named RTZ-VI-LCB, which leverages optimistic robust value iteration coupled with a data-driven Bernstein-style penalty to ensure robust value estimation. This approach explicitly handles distribution shifts present in historical datasets, thereby ensuring robustness against environmental uncertainties and the sim-to-real gap. The paper establishes that RTZ-VI-LCB achieves near-optimal sample complexity under conditions of partial data coverage and environmental uncertainty, meaning the method is efficient in terms of data requirements. Additionally, the authors develop an information-theoretic lower bound that demonstrates the sample complexity of their algorithm is tight and optimal with respect to both state and action space dimensions. This makes RTZ-VI-LCB the first algorithm to reach this level of theoretical optimality in the offline RTZMG setting. Experimental validation further supports the theoretical findings and shows that RTZ-VI-LCB sets a new benchmark for robust offline learning in two-player zero-sum Markov games. <div>
arXiv:2512.00352v1 Announce Type: new 
Abstract: Multi-agent reinforcement learning (MARL), as a thriving field, explores how multiple agents independently make decisions in a shared dynamic environment. Due to environmental uncertainties, policies in MARL must remain robust to tackle the sim-to-real gap. We focus on robust two-player zero-sum Markov games (TZMGs) in offline settings, specifically on tabular robust TZMGs (RTZMGs). We propose a model-based algorithm (\textit{RTZ-VI-LCB}) for offline RTZMGs, which is optimistic robust value iteration combined with a data-driven Bernstein-style penalty term for robust value estimation. By accounting for distribution shifts in the historical dataset, the proposed algorithm establishes near-optimal sample complexity guarantees under partial coverage and environmental uncertainty. An information-theoretic lower bound is developed to confirm the tightness of our algorithm's sample complexity, which is optimal regarding both state and action spaces. To the best of our knowledge, RTZ-VI-LCB is the first to attain this optimality, sets a new benchmark for offline RTZMGs, and is validated experimentally.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Learning Causal States Under Partial Observability and Perturbation</title>
<link>https://arxiv.org/abs/2512.00357</link>
<guid>https://arxiv.org/abs/2512.00357</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Partially Observable Markov Decision Processes, Diffusion Models, Causal State Representation, Bisimulation Metric<br /><br />Summary:<br /><br />A core challenge in reinforcement learning (RL) is making accurate decisions when the agent faces incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing approaches struggle to simultaneously handle environmental perturbations and partial observability. To address this, the authors propose CaDiff, a novel framework that can enhance any RL algorithm by uncovering the causal structure underlying P$^2$OMDPs. CaDiff introduces the Asynchronous Diffusion Model (ADM), which uses a forward and reverse diffusion process with different step counts to interpret perturbations as noise that can be suppressed, effectively denoising the observations. A new bisimulation metric is developed within CaDiff to measure similarity between the partially observable environment and its denoised causal state representation. Theoretical guarantees are provided by deriving upper bounds on value function approximation errors incurred when transitioning from perturbed observations to denoised causal states, demonstrating a balance between reward and transition modeling errors. Experimental results on Roboschool benchmark tasks show that CaDiff improves returns by at least 14.18% against existing baselines. This work represents the first practical and theoretically grounded method to approximate causal states in RL using diffusion models. <div>
arXiv:2512.00357v1 Announce Type: new 
Abstract: A critical challenge for reinforcement learning (RL) is making decisions based on incomplete and noisy observations, especially in perturbed and partially observable Markov decision processes (P$^2$OMDPs). Existing methods fail to mitigate perturbations while addressing partial observability. We propose \textit{Causal State Representation under Asynchronous Diffusion Model (CaDiff)}, a framework that enhances any RL algorithm by uncovering the underlying causal structure of P$^2$OMDPs. This is achieved by incorporating a novel asynchronous diffusion model (ADM) and a new bisimulation metric. ADM enables forward and reverse processes with different numbers of steps, thus interpreting the perturbation of P$^2$OMDP as part of the noise suppressed through diffusion. The bisimulation metric quantifies the similarity between partially observable environments and their causal counterparts. Moreover, we establish the theoretical guarantee of CaDiff by deriving an upper bound for the value function approximation errors between perturbed observations and denoised causal states, reflecting a principled trade-off between approximation errors of reward and transition-model. Experiments on Roboschool tasks show that CaDiff enhances returns by at least 14.18\% compared to baselines. CaDiff is the first framework that approximates causal states using diffusion models with both theoretical rigor and practicality.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>S^2-KD: Semantic-Spectral Knowledge Distillation Spatiotemporal Forecasting</title>
<link>https://arxiv.org/abs/2512.00366</link>
<guid>https://arxiv.org/abs/2512.00366</guid>
<content:encoded><![CDATA[
<div> Keywords: Spatiotemporal forecasting, Knowledge distillation, Semantic priors, Spectral representations, Multimodal models<br /><br />Summary:<br /><br />1. The paper addresses spatiotemporal forecasting, which typically depends on computationally heavy models to capture complex dynamics in data. To alleviate this, knowledge distillation (KD) is used to train lightweight student models. 2. Existing KD methods focus on preserving spectral properties like high-frequency details and low-frequency trends by operating on pixel-level signals, but they fail to incorporate the rich semantic and causal context behind these visual patterns. 3. The authors propose S²-KD, a novel KD framework that combines semantic priors with spectral representations to overcome these limitations. 4. The approach involves training a privileged, multimodal teacher model that uses textual narratives from a Large Multimodal Model (LMM) for reasoning about event causes and decouples spectral components in its latent space. 5. A new distillation objective transfers this unified semantic-spectral knowledge from the teacher to a lightweight, vision-only student, enabling the student to produce predictions that are both spectrally accurate and semantically coherent without needing textual inputs at inference time. 6. Experiments on benchmarks such as WeatherBench and TaxiBJ+ demonstrate that S²-KD significantly improves simple student models and allows them to outperform state-of-the-art methods, especially in long-horizon and complex, non-stationary forecasting scenarios. <div>
arXiv:2512.00366v1 Announce Type: new 
Abstract: Spatiotemporal forecasting often relies on computationally intensive models to capture complex dynamics. Knowledge distillation (KD) has emerged as a key technique for creating lightweight student models, with recent advances like frequency-aware KD successfully preserving spectral properties (i.e., high-frequency details and low-frequency trends). However, these methods are fundamentally constrained by operating on pixel-level signals, leaving them blind to the rich semantic and causal context behind the visual patterns. To overcome this limitation, we introduce S^2-KD, a novel framework that unifies Semantic priors with Spectral representations for distillation. Our approach begins by training a privileged, multimodal teacher model. This teacher leverages textual narratives from a Large Multimodal Model (LMM) to reason about the underlying causes of events, while its architecture simultaneously decouples spectral components in its latent space. The core of our framework is a new distillation objective that transfers this unified semantic-spectral knowledge into a lightweight, vision-only student. Consequently, the student learns to make predictions that are not only spectrally accurate but also semantically coherent, without requiring any textual input or architectural overhead at inference. Extensive experiments on benchmarks like WeatherBench and TaxiBJ+ show that S^2-KD significantly boosts the performance of simple student models, enabling them to outperform state-of-the-art methods, particularly in long-horizon and complex non-stationary scenarios.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>An Empirical Study on the Effectiveness of Incorporating Offline RL As Online RL Subroutines</title>
<link>https://arxiv.org/abs/2512.00383</link>
<guid>https://arxiv.org/abs/2512.00383</guid>
<content:encoded><![CDATA[
<div> Offline RL, Online RL, Tabula Rasa, Fine-tuning, Learning Efficiency<br /><br />Summary:<br /><br />This article presents a novel perspective by integrating offline reinforcement learning (RL) algorithms as subroutines within tabula rasa online RL, leveraging the agent’s historical interactions as datasets for offline learning. It formalizes a flexible framework that supports multiple variants of offline RL incorporation, including final policy recommendation and online fine-tuning. The paper introduces new techniques aimed at improving the overall effectiveness of this integration to enhance the efficiency of online learning. Extensive empirical analyses reveal three key findings: first, the success and effectiveness of the proposed framework are highly dependent on the specific nature and characteristics of the tasks involved. Second, the newly proposed techniques significantly boost the effectiveness of the framework, improving online learning outcomes. Third, the study finds that existing methods for online fine-tuning show overall ineffectiveness in this context, highlighting a gap in current research and a need for further investigation. This calls attention to the necessity for developing more robust online fine-tuning approaches to complement offline RL incorporation for better learning performance. <div>
arXiv:2512.00383v1 Announce Type: new 
Abstract: We take the novel perspective of incorporating offline RL algorithms as subroutines of tabula rasa online RL. This is feasible because an online learning agent can repurpose its historical interactions as offline dataset. We formalize this idea into a framework that accommodates several variants of offline RL incorporation such as final policy recommendation and online fine-tuning. We further introduce convenient techniques to improve its effectiveness in enhancing online learning efficiency. Our extensive and systematic empirical analyses show that 1) the effectiveness of the proposed framework depends strongly on the nature of the task, 2) our proposed techniques greatly enhance its effectiveness, and 3) existing online fine-tuning methods are overall ineffective, calling for more research therein.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient and Programmable Exploration of Synthesizable Chemical Space</title>
<link>https://arxiv.org/abs/2512.00384</link>
<guid>https://arxiv.org/abs/2512.00384</guid>
<content:encoded><![CDATA[
<div> synthesizable chemical space, molecular discovery, transformer model, property-based querying, molecular optimization<br /><br />Summary:<br /><br />1. The paper addresses the challenge of exploring synthesizable chemical space to discover molecules that are both synthetically accessible and meet desired property criteria. <br />2. It introduces PrexSyn, a decoder-only transformer model trained on an unprecedented scale—billions of data points of synthesizable reaction pathways linked with molecular properties—facilitated by a high-throughput C++ data generation engine. <br />3. PrexSyn effectively reconstructs synthesizable chemical space with high accuracy and inference speed while capturing complex associations between molecular properties and synthetic pathways. <br />4. The model supports programmable molecular generation using property-based queries, including complex logical combinations, enabling users to specify multiple property objectives simultaneously. <br />5. Leveraging this querying capability, PrexSyn optimizes molecules against black-box oracle functions through iterative refinement, surpassing synthesis-agnostic baselines in sampling efficiency. <br />6. Overall, PrexSyn represents a significant advance in synthesizable molecular design by combining comprehensive chemical space coverage, rapid and efficient molecular sampling, and the flexibility to tailor generation to complex property-defined objectives. <div>
arXiv:2512.00384v1 Announce Type: new 
Abstract: The constrained nature of synthesizable chemical space poses a significant challenge for sampling molecules that are both synthetically accessible and possess desired properties. In this work, we present PrexSyn, an efficient and programmable model for molecular discovery within synthesizable chemical space. PrexSyn is based on a decoder-only transformer trained on a billion-scale datastream of synthesizable pathways paired with molecular properties, enabled by a real-time, high-throughput C++-based data generation engine. The large-scale training data allows PrexSyn to reconstruct the synthesizable chemical space nearly perfectly at a high inference speed and learn the association between properties and synthesizable molecules. Based on its learned property-pathway mappings, PrexSyn can generate synthesizable molecules that satisfy not only single-property conditions but also composite property queries joined by logical operators, thereby allowing users to ``program'' generation objectives. Moreover, by exploiting this property-based querying capability, PrexSyn can efficiently optimize molecules against black-box oracle functions via iterative query refinement, achieving higher sampling efficiency than even synthesis-agnostic baselines, making PrexSyn a powerful general-purpose molecular optimization tool. Overall, PrexSyn pushes the frontier of synthesizable molecular design by setting a new state of the art in synthesizable chemical space coverage, molecular sampling efficiency, and inference speed.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Solving Neural Min-Max Games: The Role of Architecture, Initialization &amp; Dynamics</title>
<link>https://arxiv.org/abs/2512.00389</link>
<guid>https://arxiv.org/abs/2512.00389</guid>
<content:encoded><![CDATA[
<div> Keywords: zero-sum games, neural networks, von Neumann-Nash equilibrium, overparameterization, non-convex min-max optimization<br /><br />Summary:<br />1. The paper addresses zero-sum games between neural networks, which are widely applicable in adversarial training, AI alignment, and robust optimization, with equilibria representing desirable system behaviors. <br />2. Despite the non-convex, non-concave nature of the underlying objectives, simple gradient methods empirically tend to converge, hinting at an underlying geometric structure. <br />3. The authors provide a theoretical framework explaining this convergence by leveraging hidden convexity and overparameterization in these neural network games.<br />4. Key contributions include identifying sufficient conditions related to network initialization, training dynamics, and width to guarantee global convergence to von Neumann-Nash equilibria in a broad class of two-layer neural network min-max games.<br />5. Technically, the work introduces (a) a novel path-length bound for alternating gradient descent-ascent schemes adapted to min-max games, and (b) a demonstration that hidden convex-concave geometry translates to a two-sided Polyak-Łojasiewicz condition with high probability under overparameterization, supported by random matrix theory tools.<br /> This is the first theoretical result of its kind for min-max games involving two-layer neural networks, advancing understanding of convergence in complex neural network game settings. <div>
arXiv:2512.00389v1 Announce Type: new 
Abstract: Many emerging applications - such as adversarial training, AI alignment, and robust optimization - can be framed as zero-sum games between neural nets, with von Neumann-Nash equilibria (NE) capturing the desirable system behavior. While such games often involve non-convex non-concave objectives, empirical evidence shows that simple gradient methods frequently converge, suggesting a hidden geometric structure. In this paper, we provide a theoretical framework that explains this phenomenon through the lens of hidden convexity and overparameterization. We identify sufficient conditions - spanning initialization, training dynamics, and network width - that guarantee global convergence to a NE in a broad class of non-convex min-max games. To our knowledge, this is the first such result for games that involve two-layer neural networks. Technically, our approach is twofold: (a) we derive a novel path-length bound for the alternating gradient descent-ascent scheme in min-max games; and (b) we show that the reduction from a hidden convex-concave geometry to two-sided Polyak-{\L}ojasiewicz (P{\L}) min-max condition hold with high probability under overparameterization, using tools from random matrix theory.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Coefficients to Directions: Rethinking Model Merging with Directional Alignment</title>
<link>https://arxiv.org/abs/2512.00391</link>
<guid>https://arxiv.org/abs/2512.00391</guid>
<content:encoded><![CDATA[
<div> Model merging, directional alignment, parameter space, feature space, neural collapse<br /><br />Summary:<br /><br />Model merging integrates multiple independently trained models into a single one without requiring joint retraining, reducing computational costs. Previous techniques focused mainly on parameter decomposition, coefficient optimization, and subspace learning to combine model parameters effectively. However, these methods often neglect the importance of directional information present in both parameter and feature spaces, which can cause inconsistencies and structural incoherence when naively merging models. The problem is exacerbated because coefficient-based optimization assumes compatible directional patterns across models' feature spaces, an assumption challenged by Neural Collapse, which reveals structured directional variations in class features among separately trained models. To address these issues, the paper introduces a unified geometric framework called Merging with Directional Alignment (\method{}), which explicitly aligns directional structures in both parameter and feature spaces to enhance structural coherence. This directional alignment reduces inconsistencies and better preserves the intrinsic geometric relationships within models. Extensive experiments across different benchmarks, model sizes, and task setups demonstrate that the proposed method outperforms traditional merging approaches by improving performance and robustness. Overall, the work highlights the critical role of directional alignment in successful model merging and provides a principled, practical method for achieving it. <div>
arXiv:2512.00391v1 Announce Type: new 
Abstract: Model merging has emerged as a practical paradigm for integrating multiple independently trained models into a single model without joint retraining. Previous studies have demonstrated the effectiveness of combining parameters through strategies such as parameter decomposition, coefficient optimization, and subspace learning, significantly reducing the need for expensive joint training and achieving strong empirical performance across diverse tasks. However, these approaches predominantly treat merging as a problem of parameter space decomposition or fusion coefficient optimization, while overlooking the critical role of directional information in both parameter and feature spaces. In practice, na\"ive merging introduces inconsistencies in dominant parameter directions and disrupts structural coherence across models, which can degrade performance. Moreover, coefficient-based optimization methods implicitly assume compatible feature-space directions across models. However, Neural Collapse indicates that class features follow structured directional patterns, which may differ across independently trained models, making coefficient optimization alone insufficient. In this work, we emphasize the importance of \emph{directional alignment} and introduce a unified geometric framework, \emph{Merging with Directional Alignment} (\method{}), which aligns directional structures consistently in both the parameter and feature spaces. Our analysis shows that directional alignment improves structural coherence, and extensive experiments across benchmarks, model scales, and task configurations further validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Time-Series at the Edge: Tiny Separable CNNs for Wearable Gait Detection and Optimal Sensor Placement</title>
<link>https://arxiv.org/abs/2512.00396</link>
<guid>https://arxiv.org/abs/2512.00396</guid>
<content:encoded><![CDATA[
<div> Keywords: Parkinson's disease, gait detection, on-device time-series analysis, ultra-light CNN, wearable sensors

<br /><br />Summary:  
This study focuses on on-device time-series analysis for detecting gait in Parkinson's disease (PD) using short windows of triaxial acceleration data from wearable sensors, specifically targeting resource-constrained devices like low-power wearables and edge nodes. The researchers compare a magnitude thresholding method against three one-dimensional convolutional neural networks (1D CNNs): a literature baseline model using separable convolutions, and two ultra-light models—one purely separable convolutional and another incorporating residual connections. Experiments are conducted on the BioStampRC21 dataset with 2-second windows sampled at 30 Hz and use subject-independent leave-one-subject-out (LOSO) validation on 16 PD patients equipped with chest-worn IMUs. The best-performing ultra-light residual separable CNN model achieves a precision-recall area under curve (PR-AUC) of 94.5%, F1-score of 91.2%, and Matthews correlation coefficient (MCC) of 89.4%, surpassing the baseline model despite having roughly 10 times fewer parameters (533 vs. 5,552). The smallest model with only 305 parameters still attains strong metrics (PR-AUC 94.0%, F1 91.0%, MCC 89.1%). Thresholding yields high recall (89.0%) but lower precision (76.5%), causing many false positives and variability across subjects. Sensor placement analysis indicates chest and thighs provide the most reliable data, while forearm sensors reduce precision and recall due to arm movement noise. Combining data from all sensor sites naively does not improve results beyond the single best site. Both compact CNN models run efficiently on STM32-class microcontrollers with sub-10 ms latency, making them suitable for on-sensor gating to minimize data transmission or storage. The work highlights ultra-light separable CNNs as an effective, resource-efficient alternative to thresholding for wearable PD gait detection on edge devices. <div>
arXiv:2512.00396v1 Announce Type: new 
Abstract: We study on-device time-series analysis for gait detection in Parkinson's disease (PD) from short windows of triaxial acceleration, targeting resource-constrained wearables and edge nodes. We compare magnitude thresholding to three 1D CNNs for time-series analysis: a literature baseline (separable convolutions) and two ultra-light models - one purely separable and one with residual connections. Using the BioStampRC21 dataset, 2 s windows at 30 Hz, and subject-independent leave-one-subject-out (LOSO) validation on 16 PwPD with chest-worn IMUs, our residual separable model (Model 2, 533 params) attains PR-AUC = 94.5%, F1 = 91.2%, MCC = 89.4%, matching or surpassing the baseline (5,552 params; PR-AUC = 93.7%, F1 = 90.5%, MCC = 88.5%) with approximately 10x fewer parameters. The smallest model (Model 1, 305 params) reaches PR-AUC = 94.0%, F1 = 91.0%, MCC = 89.1%. Thresholding obtains high recall (89.0%) but low precision (76.5%), yielding many false positives and high inter-subject variance. Sensor-position analysis (train-on-all) shows chest and thighs are most reliable; forearms degrade precision/recall due to non-gait arm motion; naive fusion of all sites does not outperform the best single site. Both compact CNNs execute within tight memory/latency budgets on STM32-class MCUs (sub-10 ms on low-power boards), enabling on-sensor gating of transmission/storage. Overall, ultra-light separable CNNs provide a superior accuracy-efficiency-generalization trade-off to fixed thresholds for wearable PD gait detection and underscore the value of tailored time-series models for edge deployment.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>SelfAI: Building a Self-Training AI System with LLM Agents</title>
<link>https://arxiv.org/abs/2512.00403</link>
<guid>https://arxiv.org/abs/2512.00403</guid>
<content:encoded><![CDATA[
<div> autonomous scientific discovery, multi-agent platform, optimal stopping, hyperparameter search, evaluation metrics  

<br /><br />Summary:  
1. The paper addresses limitations in current autonomous scientific discovery systems, which are often domain-specific, provide limited real-time researcher interaction, and lack mechanisms to decide when to stop exploration, causing inefficiencies and reproducibility issues.  
2. The authors propose SelfAI, a general multi-agent platform designed to overcome these challenges by integrating three core components: a User Agent for converting high-level research goals into standardized experimental setups; a Cognitive Agent powered by large language models (LLMs) implementing optimal stopping criteria for iterative hyperparameter search refinement; and an Experiment Manager that coordinates parallel, fault-tolerant training workflows on diverse hardware while maintaining a structured knowledge base for continuous feedback.  
3. SelfAI introduces two new evaluation metrics—Score and AUP_D—to measure efficiency in discovery and diversity in search strategies.  
4. The platform is benchmarked across varied tasks including regression, natural language processing, computer vision, scientific computing, medical imaging, and drug discovery, consistently showing strong performance and reducing redundant experimental trials compared to traditional Bayesian optimization and other LLM-based methods.  
5. SelfAI facilitates seamless interaction with human researchers, enhancing usability and enabling integration of human expertise in the autonomous scientific discovery process. <div>
arXiv:2512.00403v1 Announce Type: new 
Abstract: Recent work on autonomous scientific discovery has leveraged LLM-based agents to integrate problem specification, experiment planning, and execution into end-to-end systems. However, these frameworks are often confined to narrow application domains, offer limited real-time interaction with researchers, and lack principled mechanisms for determining when to halt exploration, resulting in inefficiencies, reproducibility challenges, and under-utilized human expertise. To address these gaps, we propose \textit{SelfAI}, a general multi-agent platform that combines a User Agent for translating high-level research objectives into standardized experimental configurations, a Cognitive Agent powered by LLMs with optimal stopping criteria to iteratively refine hyperparameter searches, and an Experiment Manager responsible for orchestrating parallel, fault-tolerant training workflows across heterogeneous hardware while maintaining a structured knowledge base for continuous feedback. We further introduce two novel evaluation metrics, Score and $\text{AUP}_D$, to quantify discovery efficiency and search diversity. Across regression, NLP, computer vision, scientific computing, medical imaging, and drug discovery benchmarks, SelfAI consistently achieves strong performance and reduces redundant trials compared to classical Bayesian optimization and LLM-based baselines, while enabling seamless interaction with human researchers.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>TrendGNN: Towards Understanding of Epidemics, Beliefs, and Behaviors</title>
<link>https://arxiv.org/abs/2512.00421</link>
<guid>https://arxiv.org/abs/2512.00421</guid>
<content:encoded><![CDATA[
<div> Keywords: epidemic forecasting, human behavior, graph neural networks, interpretability, interdependent signals

<br /><br />Summary:  
This paper addresses the complex relationship between epidemic outcomes and human behavior and beliefs, highlighting the limitations of current forecasting models that often use simplistic mechanistic or opaque black-box approaches. The authors propose a novel graph-based forecasting framework that constructs a graph of interrelated epidemic and behavioral signals based on trend similarity. By employing graph neural networks (GNNs) on this graph, the framework enables prediction with a focus on interpretability, showing which signals are more predictable and identifying the key relationships that enhance forecasting accuracy. This method advances understanding of how multiple interdependent signals interact, offering insight into the mechanisms driving epidemic trends and behaviors. The approach has significant implications for the development of future simulation models that integrate diverse data sources, such as behavior, beliefs, and observed epidemic signals, thereby improving intervention planning and policy-making. The work marks an early step toward interpretable modeling in complex, multi-signal domains, aiming to bridge the gap between transparency and predictive performance in epidemic forecasting. <div>
arXiv:2512.00421v1 Announce Type: new 
Abstract: Epidemic outcomes have a complex interplay with human behavior and beliefs. Most of the forecasting literature has focused on the task of predicting epidemic signals using simple mechanistic models or black-box models, such as deep transformers, that ingest all available signals without offering interpretability. However, to better understand the mechanisms and predict the impact of interventions, we need the ability to forecast signals associated with beliefs and behaviors in an interpretable manner. In this work, we propose a graph-based forecasting framework that first constructs a graph of interrelated signals based on trend similarity, and then applies graph neural networks (GNNs) for prediction. This approach enables interpretable analysis by revealing which signals are more predictable and which relationships contribute most to forecasting accuracy. We believe our method provides early steps towards a framework for interpretable modeling in domains with multiple potentially interdependent signals, with implications for building future simulation models that integrate behavior, beliefs, and observations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy-Preserving Generative Modeling and Clinical Validation of Longitudinal Health Records for Chronic Disease</title>
<link>https://arxiv.org/abs/2512.00434</link>
<guid>https://arxiv.org/abs/2512.00434</guid>
<content:encoded><![CDATA[
<div> Keywords: synthetic data, time-series generative model, data privacy, chronic kidney disease, DP-TimeGAN<br /><br />Summary:  
This paper addresses the challenge of maintaining data privacy in medical workflows, especially given the increased use of electronic patient records and strict data protection regulations. It highlights the potential of synthetic data as an alternative to real clinical data for training machine learning models but notes that existing generative models struggle with either time-series data or formal privacy guarantees. The authors propose enhancements to a state-of-the-art time-series generative model, improving its capability to handle longitudinal clinical data while integrating quantifiable privacy protections. They evaluate their approach using real patient data from chronic kidney disease (CKD) and ICU datasets, employing statistical tests, Train-on-Synthetic-Test-on-Real (TSTR) experiments, and expert clinical reviews for validation. Their non-private version (Augmented TimeGAN) outperforms transformer- and flow-based models on key statistical metrics, demonstrating better data fidelity. The private variant (DP-TimeGAN) achieves a mean authenticity score of 0.778 on the CKD dataset, surpassing existing models on the privacy-utility trade-off. Importantly, clinician evaluations confirm that synthetic data generated by both models is comparable in quality to real clinical data. This work provides a robust framework to develop machine learning solutions for chronic conditions without compromising patient privacy. <div>
arXiv:2512.00434v1 Announce Type: new 
Abstract: Data privacy is a critical challenge in modern medical workflows as the adoption of electronic patient records has grown rapidly. Stringent data protection regulations limit access to clinical records for training and integrating machine learning models that have shown promise in improving diagnostic accuracy and personalized care outcomes. Synthetic data offers a promising alternative; however, current generative models either struggle with time-series data or lack formal privacy guaranties. In this paper, we enhance a state-of-the-art time-series generative model to better handle longitudinal clinical data while incorporating quantifiable privacy safeguards. Using real data from chronic kidney disease and ICU patients, we evaluate our method through statistical tests, a Train-on-Synthetic-Test-on-Real (TSTR) setup, and expert clinical review. Our non-private model (Augmented TimeGAN) outperforms transformer- and flow-based models on statistical metrics in several datasets, while our private model (DP-TimeGAN) maintains a mean authenticity of 0.778 on the CKD dataset, outperforming existing state-of-the-art models on the privacy-utility frontier. Both models achieve performance comparable to real data in clinician evaluations, providing robust input data necessary for developing models for complex chronic conditions without compromising data privacy.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FairMT: Fairness for Heterogeneous Multi-Task Learning</title>
<link>https://arxiv.org/abs/2512.00469</link>
<guid>https://arxiv.org/abs/2512.00469</guid>
<content:encoded><![CDATA[
<div> Fairness, Multi-task Learning, Heterogeneous Tasks, Asymmetric Constraints, Primal-Dual Optimization<br /><br />Summary:  
The paper addresses the challenge of fairness in multi-task learning (MTL), a relatively unexplored area compared to single-task fairness. It highlights the difficulty in applying existing fairness methods—primarily designed for classification—to MTL settings involving heterogeneous tasks such as classification, detection, and regression, especially when labels are partially missing. Existing approaches often constrain only the shared representation, allowing bias to persist in task-specific heads, which results in uneven fairness across tasks. Moreover, conventional fairness enforcement tends to treat fairness as a zero-sum trade-off with utility, often degrading performance for groups that are already well-served. To overcome these challenges, the authors propose FairMT, a unified framework that supports all three task types under incomplete supervision by introducing an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism. This mechanism consolidates task-dependent asymmetric fairness violations into a single unified constraint. FairMT jointly optimizes utility and fairness through a primal-dual formulation, aided by a head-aware multi-objective optimization proxy that manages the anisotropy introduced by task-specific heads. Experimental results on diverse multi-task benchmarks demonstrate that FairMT consistently improves fairness while maintaining or enhancing task utility. The authors also plan to release the code upon paper acceptance. <div>
arXiv:2512.00469v1 Announce Type: new 
Abstract: Fairness in machine learning has been extensively studied in single-task settings, while fair multi-task learning (MTL), especially with heterogeneous tasks (classification, detection, regression) and partially missing labels, remains largely unexplored. Existing fairness methods are predominantly classification-oriented and fail to extend to continuous outputs, making a unified fairness objective difficult to formulate. Further, existing MTL optimization is structurally misaligned with fairness: constraining only the shared representation, allowing task heads to absorb bias and leading to uncontrolled task-specific disparities. Finally, most work treats fairness as a zero-sum trade-off with utility, enforcing symmetric constraints that achieve parity by degrading well-served groups. We introduce FairMT, a unified fairness-aware MTL framework that accommodates all three task types under incomplete supervision. At its core is an Asymmetric Heterogeneous Fairness Constraint Aggregation mechanism, which consolidates task-dependent asymmetric violations into a unified fairness constraint. Utility and fairness are jointly optimized via a primal--dual formulation, while a head-aware multi-objective optimization proxy provides a tractable descent geometry that explicitly accounts for head-induced anisotropy. Across three homogeneous and heterogeneous MTL benchmarks encompassing diverse modalities and supervision regimes, FairMT consistently achieves substantial fairness gains while maintaining superior task utility. Code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESPO: Entropy Importance Sampling Policy Optimization</title>
<link>https://arxiv.org/abs/2512.00499</link>
<guid>https://arxiv.org/abs/2512.00499</guid>
<content:encoded><![CDATA[
<div> Keywords: reinforcement learning, policy optimization, entropy importance sampling, fine-tuning stability, mathematical reasoning benchmarks<br /><br />Summary: This paper addresses challenges in large language model (LLM) reinforcement learning, focusing on the trade-off between optimization granularity and training stability in group-based policy optimization frameworks like GRPO and GSPO. GSPO, while improving robustness through sequence-level optimization, suffers from inefficiencies such as gradient underutilization caused by its conservative clipping mechanism and uniform credit assignment that overlooks the varying importance of reasoning steps within sequences. To overcome these issues, the authors propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that balances fine-grained control with training stability. ESPO works by decomposing sequences into groups according to predictive entropy, which allows it to implement entropy-driven importance sampling for capturing heterogeneity within sequences, and entropy-adaptive clipping that dynamically adjusts trust regions based on the model’s uncertainty. The effectiveness of ESPO is validated with extensive experiments on mathematical reasoning benchmarks, demonstrating faster convergence and achieving state-of-the-art results. Notably, ESPO significantly improves accuracy on the challenging HMMT benchmark, increasing performance from 4.4% to 13.13%, showcasing its potential for enhancing LLM fine-tuning and reasoning capabilities. <div>
arXiv:2512.00499v1 Announce Type: new 
Abstract: Large language model (LLM) reinforcement learning has increasingly relied on group-based policy optimization frameworks, such as GRPO and GSPO, to achieve stable fine-tuning at scale. However, a fundamental trade-off persists between optimization granularity and training stability. While GSPO improves robustness via sequence-level optimization, its monolithic treatment of sequences introduces severe inefficiencies: its conservative clipping mechanism indiscriminately discards valid training samples-a phenomenon we term gradient underutilization-and its uniform credit assignment fails to capture the heterogeneous contributions of critical reasoning steps. In this work, we propose Entropy Importance Sampling Policy Optimization (ESPO), a novel framework that reconciles fine-grained control with training stability. ESPO decomposes sequences into groups based on predictive entropy, enabling (1) Entropy-driven Importance Sampling to capture intra-sequence heterogeneity, and (2) Entropy-adaptive Clipping to dynamically allocate trust regions based on model uncertainty. Extensive experiments on mathematical reasoning benchmarks demonstrate that ESPO not only accelerates convergence but also achieves state-of-the-art performance, notably improving accuracy on the challenging HMMT benchmark from 4.4% to 13.13%.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Rep3Net: An Approach Exploiting Multimodal Representation for Molecular Bioactivity Prediction</title>
<link>https://arxiv.org/abs/2512.00521</link>
<guid>https://arxiv.org/abs/2512.00521</guid>
<content:encoded><![CDATA[
<div> Keywords: bioactivity prediction, drug discovery, deep learning, graph representation, ChemBERTa  

<br /><br />Summary:  
The article addresses the challenge of accurately predicting molecular bioactivity against target proteins, which is a critical step in early-stage drug discovery. Traditional QSAR models, relying heavily on molecular descriptors, are limited in capturing the structural and contextual nuances of compounds, leading to reduced prediction accuracy. To overcome this, the authors propose Rep3Net, a unified deep learning framework that integrates multiple data representations: molecular descriptors, spatial and relational information via graph-based compound representations, and contextual embeddings generated from SMILES strings using ChemBERTa. This multimodal feature integration enhances the model’s ability to capture comprehensive molecular information. The model is evaluated on the Poly [ADP-ribose] polymerase 1 (PARP-1) dataset, an important therapeutic target involved in DNA repair and oncology. Comparative analysis against conventional methods such as Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and XGBoost demonstrates that Rep3Net delivers superior bioactivity prediction performance. The study highlights Rep3Net's scalability and effectiveness in computational screening workflows for drug discovery, positioning it as a promising tool to accelerate identification of bioactive compounds through improved predictive accuracy and richer molecular feature representation. <div>
arXiv:2512.00521v1 Announce Type: new 
Abstract: In early stage drug discovery, bioactivity prediction of molecules against target proteins plays a crucial role. Trdaitional QSAR models that utilizes molecular descriptor based data often struggles to predict bioactivity of molecules effectively due to its limitation in capturing structural and contextual information embedded within each compound. To address this challenge, we propose Rep3Net, a unified deep learning architecture that not only incorporates descriptor data but also includes spatial and relational information through graph-based represenation of compounds and contextual information through ChemBERTa generated embeddings from SMILES strings. Our model employing multimodal concatenated features produce reliable bioactivity prediction on Poly [ADP-ribose] polymerase 1 (PARP-1) dataset. PARP-1 is a crucial agent in DNA damage repair and has become a significant theraputic target in malignancies that depend on it for survival and growth. A comprehensive analysis and comparison with conventional standalone models including GCN, GAT, XGBoost, etc. demonstrates that our architecture achieves the highest predictive performance. In computational screening of compounds in drug discovery, our architecture provides a scalable framework for bioactivity prediction.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Hyperbolic Continuous Structural Entropy for Hierarchical Clustering</title>
<link>https://arxiv.org/abs/2512.00524</link>
<guid>https://arxiv.org/abs/2512.00524</guid>
<content:encoded><![CDATA[
<div> Hierarchical clustering, hyperbolic space, structural entropy, graph neural networks, graph structure learning

<br /><br />Summary:  
This paper addresses key challenges in hierarchical clustering, notably the lack of a global objective in most methods and the inadequacy of graph-based approaches that often operate on static or fully connected graphs without properly leveraging graph structure. The authors introduce HypCSE, a novel approach that maps data points into hyperbolic space and optimizes a relaxed form of continuous structural entropy (CSE) on enhanced graph structures. HypCSE employs hyperbolic graph neural networks to encode graph vertices, enabling effective representation in hyperbolic space. To enable differentiable optimization, the structural entropy objective is reformulated using the concept of the lowest common ancestor (LCA) in trees and then relaxed into continuous structural entropy by drawing an analogy between hyperbolic embeddings and hierarchical partition trees. Additionally, the model incorporates a graph structure learning component that dynamically updates the graph during training to better reflect the underlying hierarchical relationships among data points. Extensive experiments conducted on seven benchmark datasets demonstrate that HypCSE outperforms existing hierarchical clustering methods, validating the effectiveness of integrating hyperbolic geometry, continuous entropy objectives, and adaptive graph structure learning for continuous hierarchical clustering. <div>
arXiv:2512.00524v1 Announce Type: new 
Abstract: Hierarchical clustering is a fundamental machine-learning technique for grouping data points into dendrograms. However, existing hierarchical clustering methods encounter two primary challenges: 1) Most methods specify dendrograms without a global objective. 2) Graph-based methods often neglect the significance of graph structure, optimizing objectives on complete or static predefined graphs. In this work, we propose Hyperbolic Continuous Structural Entropy neural networks, namely HypCSE, for structure-enhanced continuous hierarchical clustering. Our key idea is to map data points in the hyperbolic space and minimize the relaxed continuous structural entropy (SE) on structure-enhanced graphs. Specifically, we encode graph vertices in hyperbolic space using hyperbolic graph neural networks and minimize approximate SE defined on graph embeddings. To make the SE objective differentiable for optimization, we reformulate it into a function using the lowest common ancestor (LCA) on trees and then relax it into continuous SE (CSE) by the analogy of hyperbolic graph embeddings and partitioning trees. To ensure a graph structure that effectively captures the hierarchy of data points for CSE calculation, we employ a graph structure learning (GSL) strategy that updates the graph structure during training. Extensive experiments on seven datasets demonstrate the superior performance of HypCSE.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pushing the Boundaries of Interpretability: Incremental Enhancements to the Explainable Boosting Machine</title>
<link>https://arxiv.org/abs/2512.00528</link>
<guid>https://arxiv.org/abs/2512.00528</guid>
<content:encoded><![CDATA[
<div> Keywords: Explainable Boosting Machine, hyperparameter optimization, fairness, self-supervised pre-training, transparency  

<br /><br />Summary:  
This paper addresses the "black-box" problem in machine learning by enhancing the Explainable Boosting Machine (EBM), a transparent and accurate glassbox model. First, the study applies targeted hyperparameter optimization using Bayesian methods to improve model performance. Second, it introduces a custom multi-objective function for fairness during hyperparameter tuning, aiming to ensure equitable model decisions. Third, a novel self-supervised pre-training pipeline is developed specifically for cold-start scenarios, where limited labeled data is available. These three methodologies are rigorously evaluated on benchmark datasets such as Adult Income, Credit Card Fraud Detection, and UCI Heart Disease. The results show that although the hyperparameter tuning leads to only marginal gains in the primary ROC AUC metric, it importantly shifts the model’s decision-making behavior. This demonstrates the significance of evaluating machine learning models beyond a single accuracy score, highlighting fairness and behavioral transparency. Altogether, the enhancements make the EBM not only more accurate but also more robust, equitable, and transparent. The work represents a crucial advancement toward building responsible AI systems that meet the strict requirements of ethical and regulatory compliance in high-stakes applications. <div>
arXiv:2512.00528v1 Announce Type: new 
Abstract: The widespread adoption of complex machine learning models in high-stakes domains has brought the "black-box" problem to the forefront of responsible AI research. This paper aims at addressing this issue by improving the Explainable Boosting Machine (EBM), a state-of-the-art glassbox model that delivers both high accuracy and complete transparency. The paper outlines three distinct enhancement methodologies: targeted hyperparameter optimization with Bayesian methods, the implementation of a custom multi-objective function for fairness for hyperparameter optimization, and a novel self-supervised pre-training pipeline for cold-start scenarios. All three methodologies are evaluated across standard benchmark datasets, including the Adult Income, Credit Card Fraud Detection, and UCI Heart Disease datasets. The analysis indicates that while the tuning process yielded marginal improvements in the primary ROC AUC metric, it led to a subtle but important shift in the model's decision-making behavior, demonstrating the value of a multi-faceted evaluation beyond a single performance score. This work is positioned as a critical step toward developing machine learning systems that are not only accurate but also robust, equitable, and transparent, meeting the growing demands of regulatory and ethical compliance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Algorithmic Guarantees for Distilling Supervised and Offline RL Datasets</title>
<link>https://arxiv.org/abs/2512.00536</link>
<guid>https://arxiv.org/abs/2512.00536</guid>
<content:encoded><![CDATA[
<div> dataset distillation, supervised learning, regression, offline reinforcement learning, Bellman loss<br /><br />Summary:<br /><br />1. The paper addresses dataset distillation, aiming to create a smaller synthetic dataset enabling models trained on it to perform comparably to those trained on the original dataset.<br /><br />2. For supervised regression in  \(\mathbb{R}^d\), the authors propose an efficient algorithm that matches losses on the training and synthetic datasets using a fixed set of randomly sampled regressors, avoiding direct model training.<br /><br />3. They provide a theoretical performance guarantee showing that only \(\tilde{O}(d^2)\) sampled regressors are needed so that any bounded linear model's mean squared error (MSE) loss on the synthetic data closely approximates that on the training data.<br /><br />4. Matching lower bounds of \(\Omega(d^2)\) are also proven, demonstrating the tightness of their analysis.<br /><br />5. Extending to offline reinforcement learning (RL), the algorithm distills datasets by matching the Bellman loss rather than the behavioral cloning objective used in prior works, effectively utilizing both reward and next-state information without policy model optimization.<br /><br />6. The resulting synthetic RL dataset ensures that any linear action-value predictor's Bellman loss is close to that on the original training data, enabling similar policy performance.<br /><br />7. Experiments validate the theoretical results and show performance improvements, confirming the utility of the proposed approach in both supervised regression and offline RL settings. <div>
arXiv:2512.00536v1 Announce Type: new 
Abstract: Given a training dataset, the goal of dataset distillation is to derive a synthetic dataset such that models trained on the latter perform as well as those trained on the training dataset. In this work, we develop and analyze an efficient dataset distillation algorithm for supervised learning, specifically regression in $\mathbb{R}^d$, based on matching the losses on the training and synthetic datasets with respect to a fixed set of randomly sampled regressors without any model training. Our first key contribution is a novel performance guarantee proving that our algorithm needs only $\tilde{O}(d^2)$ sampled regressors to derive a synthetic dataset on which the MSE loss of any bounded linear model is nearly the same as its MSE loss on the given training data. In particular, the model optimized on the synthetic data has close to minimum loss on the training data, thus performing nearly as well as the model optimized on the latter. Complementing this, we also prove a matching lower bound of $\Omega(d^2)$ for the number of sampled regressors showing the tightness of our analysis.
  Our second contribution is to extend our algorithm to offline RL dataset distillation by matching the Bellman loss, unlike previous works which used a behavioral cloning objective. This is the first such method which leverages both, the rewards and the next state information, available in offline RL datasets, without any policy model optimization. Our algorithm generates a synthetic dataset whose Bellman loss with respect to any linear action-value predictor is close to the latter's Bellman loss on the offline RL training dataset. Therefore, a policy associated with an action-value predictor optimized on the synthetic dataset performs nearly as well as that derived from the one optimized on the training data. We conduct experiments to validate our theoretical guarantees and observe performance gains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>DQ4FairIM: Fairness-aware Influence Maximization using Deep Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00545</link>
<guid>https://arxiv.org/abs/2512.00545</guid>
<content:encoded><![CDATA[
<div> Keywords: Influence Maximization, Fairness, Reinforcement Learning, Deep Q-learning, Social Networks<br /><br />Summary:  
This work addresses the Influence Maximization (IM) problem, which seeks to select seed nodes to maximize influence spread in social networks, by incorporating fairness considerations. Real-world social networks exhibit structural inequalities with dominant majority and underrepresented minority groups, often leading traditional IM methods to favor majority groups disproportionately. The authors propose DQ4FairIM, a novel fairness-aware deep reinforcement learning approach that formulates the IM problem as a Markov Decision Process (MDP). Using deep Q-learning combined with Structure2Vec network embeddings, their method learns policies that maximize the expected influenced nodes while enhancing fairness. They incorporate maximin fairness to prioritize improving influence among the least-represented group, thereby promoting equitable distribution across communities regardless of protected attributes. Extensive experiments on synthetic and real-world networks demonstrate that DQ4FairIM outperforms both fairness-agnostic and existing fairness-aware baselines, achieving better fairness and a superior fairness-performance trade-off. Additionally, the learned policies generalize well without retraining to different problem instances, including variations in network size and seed budget. This highlights the method’s robustness and practical applicability in diverse social network settings where equitable influence spread is crucial. <div>
arXiv:2512.00545v1 Announce Type: new 
Abstract: The Influence Maximization (IM) problem aims to select a set of seed nodes within a given budget to maximize the spread of influence in a social network. However, real-world social networks have several structural inequalities, such as dominant majority groups and underrepresented minority groups. If these inequalities are not considered while designing IM algorithms, the outcomes might be biased, disproportionately benefiting majority groups while marginalizing minorities. In this work, we address this gap by designing a fairness-aware IM method using Reinforcement Learning (RL) that ensures equitable influence outreach across all communities, regardless of protected attributes. Fairness is incorporated using a maximin fairness objective, which prioritizes improving the outreach of the least-influenced group, pushing the solution toward an equitable influence distribution. We propose a novel fairness-aware deep RL method, called DQ4FairIM, that maximizes the expected number of influenced nodes by learning an RL policy. The learnt policy ensures that minority groups formulate the IM problem as a Markov Decision Process (MDP) and use deep Q-learning, combined with the Structure2Vec network embedding, earning together with Structure2Vec network embedding to solve the MDP. We perform extensive experiments on synthetic benchmarks and real-world networks to compare our method with fairness-agnostic and fairness-aware baselines. The results show that our method achieves a higher level of fairness while maintaining a better fairness-performance trade-off than baselines. Additionally, our approach learns effective seeding policies that generalize across problem instances without retraining, such as varying the network size or the number of seed nodes.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A Graph Neural Network Approach for Localized and High-Resolution Temperature Forecasting</title>
<link>https://arxiv.org/abs/2512.00546</link>
<guid>https://arxiv.org/abs/2512.00546</guid>
<content:encoded><![CDATA[
<div> Heatwaves, Graph Neural Network, Temperature Forecasting, Localized Prediction, Global South<br /><br />Summary:<br /><br />1. Heatwaves are becoming more intense globally and represent one of the deadliest weather-related disasters. The impacts disproportionately affect marginalized communities and countries in the Global South where vulnerability is heightened by limited health resources, urban heat islands, and inadequate infrastructure.<br /><br />2. Existing numerical weather prediction (NWP) models often lack the resolution to effectively capture micro-scale temperature extremes, which limits timely warning dissemination particularly for the most vulnerable populations.<br /><br />3. The study proposes a novel Graph Neural Network (GNN) framework designed for high-resolution, localized temperature forecasting. This model leverages spatial learning capabilities and computational efficiency to produce temperature predictions at multiple future horizons, up to 48 hours ahead.<br /><br />4. The model was evaluated for Southwestern Ontario, Canada, demonstrating strong performance with a mean absolute error (MAE) of 1.93°C across forecasts spanning 1 to 48 hours and an MAE of 2.93°C specifically at the 48-hour horizon using 24-hour input data.<br /><br />5. Although tested in a data-rich environment, the framework offers potential for transfer learning strategies to develop equitable, localized temperature forecasts in data-scarce regions of the Global South, addressing an urgent need for improved heatwave resilience globally. <div>
arXiv:2512.00546v1 Announce Type: new 
Abstract: Heatwaves are intensifying worldwide and are among the deadliest weather disasters. The burden falls disproportionately on marginalized populations and the Global South, where under-resourced health systems, exposure to urban heat islands, and the lack of adaptive infrastructure amplify risks. Yet current numerical weather prediction models often fail to capture micro-scale extremes, leaving the most vulnerable excluded from timely early warnings. We present a Graph Neural Network framework for localized, high-resolution temperature forecasting. By leveraging spatial learning and efficient computation, our approach generates forecasts at multiple horizons, up to 48 hours. For Southwestern Ontario, Canada, the model captures temperature patterns with a mean MAE of 1.93$^{\circ}$C across 1-48h forecasts and MAE@48h of 2.93$^{\circ}$C, evaluated using 24h input windows on the largest region. While demonstrated here in a data-rich context, this work lays the foundation for transfer learning approaches that could enable localized, equitable forecasts in data-limited regions of the Global South.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>List Replicable Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00553</link>
<guid>https://arxiv.org/abs/2512.00553</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning, Replicability, PAC RL, List Complexity, Stability  

<br /><br />Summary:  
This paper addresses the challenge of replicability in reinforcement learning (RL), where algorithms often show instability and sensitivity to training conditions. It formalizes the concept of list replicability within the Probably Approximately Correct (PAC) RL framework, requiring that an algorithm outputs a near-optimal policy contained in a small list consistently across runs, with the list size defining list complexity. The authors distinguish between weak list replicability, ensuring the final policy lies within a small list, and strong list replicability, which further constrains the entire sequence of executed policies. Existing RL methods face exponential list complexity due to inherent instability. The main contribution is a new tabular RL algorithm that guarantees polynomial list complexity with respect to states, actions, and horizon length. Additionally, they extend their approach to strong list replicability, providing polynomial bounds on possible policy execution traces. This advance is enabled by a novel lexicographic planning strategy among near-optimal actions within random tolerance and a replicability-preserving state reachability test in stochastic environments. Finally, the authors demonstrate that integrating their planning strategy into practical RL settings can empirically improve algorithmic stability, offering insight into overcoming RL instability challenges in real applications. <div>
arXiv:2512.00553v1 Announce Type: new 
Abstract: Replicability is a fundamental challenge in reinforcement learning (RL), as RL algorithms are empirically observed to be unstable and sensitive to variations in training conditions. To formally address this issue, we study \emph{list replicability} in the Probably Approximately Correct (PAC) RL framework, where an algorithm must return a near-optimal policy that lies in a \emph{small list} of policies across different runs, with high probability. The size of this list defines the \emph{list complexity}. We introduce both weak and strong forms of list replicability: the weak form ensures that the final learned policy belongs to a small list, while the strong form further requires that the entire sequence of executed policies remains constrained. These objectives are challenging, as existing RL algorithms exhibit exponential list complexity due to their instability. Our main theoretical contribution is a provably efficient tabular RL algorithm that guarantees list replicability by ensuring the list complexity remains polynomial in the number of states, actions, and the horizon length. We further extend our techniques to achieve strong list replicability, bounding the number of possible policy execution traces polynomially with high probability. Our theoretical result is made possible by key innovations including (i) a novel planning strategy that selects actions based on lexicographic order among near-optimal choices within a randomly chosen tolerance threshold, and (ii) a mechanism for testing state reachability in stochastic environments while preserving replicability. Finally, we demonstrate that our theoretical investigation sheds light on resolving the \emph{instability} issue of RL algorithms used in practice. In particular, we show that empirically, our new planning strategy can be incorporated into practical RL frameworks to enhance their stability.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-Generating Multi-Difficulty PDE Data for Few-Shot Neural PDE Solvers</title>
<link>https://arxiv.org/abs/2512.00564</link>
<guid>https://arxiv.org/abs/2512.00564</guid>
<content:encoded><![CDATA[
<div> Keywords: learned PDE solvers, Navier-Stokes, task difficulty transfer, classical solver compute, dataset curation

<br /><br />Summary:  
1. The paper addresses the high computational cost of generating training data for learned partial differential equation (PDE) solvers, which often exceeds the cost of training the neural models themselves.  
2. It identifies key difficulty axes in PDE problems, such as more complex geometries and higher Reynolds numbers, that simultaneously challenge classical solvers and present opportunities for neural speedups.  
3. The authors systematically study difficulty transfer on 2D incompressible Navier-Stokes equations by varying task complexity through geometry (obstacle number and placement), physics (Reynolds number), and their combinations.  
4. The study finds that pre-generating many low and medium difficulty examples using classical solvers and including them in the training set significantly reduces the number of high-difficulty samples needed to learn complex physics effectively.  
5. By mixing low and high difficulty data, the required classical solver compute for dataset generation is reduced by a factor of 8.9 to achieve the same error as training solely on high difficulty data.  
6. These results emphasize the importance of strategically allocating classical solver computational resources across different difficulty levels rather than solely focusing on total compute, suggesting large efficiency gains from carefully curated pre-generated PDE data for neural solver training.  
7. The authors provide code for reproducing their experiments at the given GitHub repository. <div>
arXiv:2512.00564v1 Announce Type: new 
Abstract: A key aspect of learned partial differential equation (PDE) solvers is that the main cost often comes from generating training data with classical solvers rather than learning the model itself. Another is that there are clear axes of difficulty--e.g., more complex geometries and higher Reynolds numbers--along which problems become (1) harder for classical solvers and thus (2) more likely to benefit from neural speedups. Towards addressing this chicken-and-egg challenge, we study difficulty transfer on 2D incompressible Navier-Stokes, systematically varying task complexity along geometry (number and placement of obstacles), physics (Reynolds number), and their combination. Similar to how it is possible to spend compute to pre-train foundation models and improve their performance on downstream tasks, we find that by classically solving (analogously pre-generating) many low and medium difficulty examples and including them in the training set, it is possible to learn high-difficulty physics from far fewer samples. Furthermore, we show that by combining low and high difficulty data, we can spend 8.9x less compute on pre-generating a dataset to achieve the same error as using only high difficulty examples. Our results highlight that how we allocate classical-solver compute across difficulty levels is as important as how much we allocate overall, and suggest substantial gains from principled curation of pre-generated PDE data for neural solvers. Our code is available at https://github.com/Naman-Choudhary-AI-ML/pregenerating-pde
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Non-Asymptotic Convergence of Discrete Diffusion Models: Masked and Random Walk dynamics</title>
<link>https://arxiv.org/abs/2512.00580</link>
<guid>https://arxiv.org/abs/2512.00580</guid>
<content:encoded><![CDATA[
<div> Discrete Diffusion Models, convergence guarantees, discrete state spaces, score function, high-dimensional scalability  

<br /><br />Summary:  
This paper investigates Discrete Diffusion Models (DDMs) on discrete state spaces, addressing the theoretical challenges distinct from the better-understood continuous diffusion models. The authors provide the first non-asymptotic convergence guarantees for DDMs on both finite spaces \(\mathbb{Z}^d_m\) and countably infinite spaces \(\mathbb{N}^d\), under mild assumptions. The study focuses on forward noising dynamics governed by masked and random walk processes. A key insight is the characterization of the backward diffusion process through a discrete score function whose monotonicity is critical for bounding the generation error. Importantly, the model’s complexity grows only linearly, up to logarithmic factors, with the dimension \(d\), contrasting the typical exponential scaling in discrete combinatorial settings, thereby enabling efficient scalability to high-dimensional data. Unlike previous work, the convergence proofs do not require the score function to be bounded. This framework covers a broad class of noising mechanisms including uniform noising on finite and infinite discrete spaces as well as masking-based noise. Overall, the paper lays foundational theoretical groundwork for discrete diffusion methodologies, paving the way for rigorous analysis and practical implementation of DDMs in various discrete data domains. <div>
arXiv:2512.00580v1 Announce Type: new 
Abstract: We investigate the theoretical underpinnings of Discrete Diffusion Models (DDMs) on discrete state spaces. Unlike in the continuous setting-where diffusion models are well understood both theoretically and empirically-the discrete case poses significant challenges due to its combinatorial structure and the lack of rigorous analysis. In this work, we establish convergence guarantees for DDMs on both the finite space $\mathbb{Z}^d_m=\{0,...,m-1\}^d$ and the countably infinite space $\mathbb{N}^d$ under mild assumptions, focusing on forward masked and random walk dynamics. Similar to the continuous case, the backward process can be characterized by a discrete score function, whose monotonicity plays a central role in deriving the error bounds of the generated data. Notably, the complexity of our model scales linearly up to logarithmic factors, rather than exponentially, with the dimension, making it efficiently scalable to high-dimensional data. To the best of our knowledge, this study provides the first non-asymptotic convergence guarantees that do not rely on the boundedness of the estimated score-covering not only uniform noising processes on $\mathbb{Z}^d_m$ and on $\mathbb{N}^d$, but also masking-based noising dynamics.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Statistical NLP for Optimization of Clinical Trial Success Prediction in Pharmaceutical R&amp;D</title>
<link>https://arxiv.org/abs/2512.00586</link>
<guid>https://arxiv.org/abs/2512.00586</guid>
<content:encoded><![CDATA[
<div> Keywords: NLP classifier, clinical trials, neuroscience, BioBERT, probability of technical and regulatory success (pTRS)  

<br /><br />Summary:  
This study develops and evaluates a natural language processing (NLP)-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for neuroscience clinical trials. Given the high attrition rates and costs in pharmaceutical R&amp;D, especially in neuroscience where success rates are under 10%, early identification of promising trials could significantly improve resource allocation and reduce financial risk. The classifier utilizes data from ClinicalTrials.gov combined with success labels from the Clinical Trial Outcome dataset, extracting text-based features through statistical NLP methods. These features were incorporated into multiple non-large language model (non-LLM) algorithms including logistic regression, gradient boosting, and random forest, achieving an ROC-AUC of 0.64 on a retrospective dataset of 101,145 completed trials from 1976 to 2024. A domain-specific language model, BioBERT, was then employed to build an LLM-based predictive model. This BioBERT model outperformed non-LLM models, attaining an ROC-AUC of 0.74 and a Brier Score of 0.185, reflecting 40% less squared prediction error relative to benchmarks. Notably, the BioBERT predictions surpassed industry benchmarks 70% of the time. Overall, the integration of NLP-driven models into drug development aims to improve strategic decision-making and investment optimization in neuroscience clinical trials. <div>
arXiv:2512.00586v1 Announce Type: new 
Abstract: This work presents the development and evaluation of an NLP-enabled probabilistic classifier designed to estimate the probability of technical and regulatory success (pTRS) for clinical trials in the field of neuroscience. While pharmaceutical R&amp;D is plagued by high attrition rates and enormous costs, particularly within neuroscience, where success rates are below 10%, timely identification of promising programs can streamline resource allocation and reduce financial risk. Leveraging data from the ClinicalTrials.gov database and success labels from the recently developed Clinical Trial Outcome dataset, the classifier extracts text-based clinical trial features using statistical NLP techniques. These features were integrated into several non-LLM frameworks (logistic regression, gradient boosting, and random forest) to generate calibrated probability scores. Model performance was assessed on a retrospective dataset of 101,145 completed clinical trials spanning 1976-2024, achieving an overall ROC-AUC of 0.64. An LLM-based predictive model was then built using BioBERT, a domain-specific language representation encoder. The BioBERT-based model achieved an overall ROC-AUC of 0.74 and a Brier Score of 0.185, indicating its predictions had, on average, 40% less squared error than would be observed using industry benchmarks. The BioBERT-based model also made trial outcome predictions that were superior to benchmark values 70% of the time overall. By integrating NLP-driven insights into drug development decision-making, this work aims to enhance strategic planning and optimize investment allocation in neuroscience programs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Developing Fairness-Aware Task Decomposition to Improve Equity in Post-Spinal Fusion Complication Prediction</title>
<link>https://arxiv.org/abs/2512.00598</link>
<guid>https://arxiv.org/abs/2512.00598</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, multitask learning, postoperative complications, subgroup inference, clinical prediction<br /><br />Summary:<br /><br />1. The article addresses fairness challenges in clinical prediction models, focusing on spinal fusion surgery for scoliosis, where patient outcomes are highly heterogeneous.<br />2. Existing fairness methods often use broad demographic adjustments or post-hoc corrections, which can miss latent clinical population structures and may inadvertently perpetuate bias.<br />3. The proposed FAIR-MTL framework introduces a fairness-aware multitask learning approach that avoids using explicit sensitive attributes during training.<br />4. FAIR-MTL performs unsupervised subgroup inference by extracting demographic embeddings and applying k-means clustering to identify latent patient subgroups.<br />5. These subgroup labels guide task routing within a shared multitask architecture, and subgroup imbalance is addressed via inverse-frequency weighting and regularization techniques.<br />6. When applied to predicting postoperative complication severity with four levels, FAIR-MTL outperforms single-task baselines, achieving an AUC of 0.86 and accuracy of 75%.<br />7. The model reduces bias significantly: demographic parity difference drops to 0.055 for gender and 0.056 for age, while equalized odds reduce to 0.094 and 0.148, respectively.<br />8. Interpretability is maintained through SHAP and Gini importance analyses, which consistently identify clinically relevant features like hemoglobin, hematocrit, and patient weight.<br />9. The study demonstrates that using unsupervised subgroup discovery within a multitask learning framework enables more equitable, interpretable, and clinically actionable risk stratification in surgical settings. <div>
arXiv:2512.00598v1 Announce Type: new 
Abstract: Fairness in clinical prediction models remains a persistent challenge, particularly in high-stakes applications such as spinal fusion surgery for scoliosis, where patient outcomes exhibit substantial heterogeneity. Many existing fairness approaches rely on coarse demographic adjustments or post-hoc corrections, which fail to capture the latent structure of clinical populations and may unintentionally reinforce bias. We propose FAIR-MTL, a fairness-aware multitask learning framework designed to provide equitable and fine-grained prediction of postoperative complication severity.
  Instead of relying on explicit sensitive attributes during model training, FAIR-MTL employs a data-driven subgroup inference mechanism. We extract a compact demographic embedding, and apply k-means clustering to uncover latent patient subgroups that may be differentially affected by traditional models. These inferred subgroup labels determine task routing within a shared multitask architecture. During training, subgroup imbalance is mitigated through inverse-frequency weighting, and regularization prevents overfitting to smaller groups.
  Applied to postoperative complication prediction with four severity levels, FAIR-MTL achieves an AUC of 0.86 and an accuracy of 75%, outperforming single-task baselines while substantially reducing bias. For gender, the demographic parity difference decreases to 0.055 and equalized odds to 0.094; for age, these values reduce to 0.056 and 0.148, respectively. Model interpretability is ensured through SHAP and Gini importance analyses, which consistently highlight clinically meaningful predictors such as hemoglobin, hematocrit, and patient weight. Our findings show that incorporating unsupervised subgroup discovery into a multitask framework enables more equitable, interpretable, and clinically actionable predictions for surgical risk stratification.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficient Matroid Bandit Linear Optimization Leveraging Unimodality</title>
<link>https://arxiv.org/abs/2512.00605</link>
<guid>https://arxiv.org/abs/2512.00605</guid>
<content:encoded><![CDATA[
<div> Keywords: combinatorial semi-bandit, matroid constraints, regret, time complexity, membership oracle<br /><br />Summary:<br /><br />This paper addresses the combinatorial semi-bandit problem under matroid constraints, a framework relevant for tasks such as diverse online recommendations. While existing methods achieve optimal regret — meaning their performance matches theoretical lower bounds — they often suffer from high computational costs, especially when dealing with large matroids or expensive membership oracles. The authors identify and exploit an underlying unimodal structure within the matroid semi-bandit problem, enabling a significant reduction in the number of membership oracle calls to \(\mathcal{O}(\log \log T)\) iterations over time horizon \(T\). This results in a substantially improved time complexity for the learning process, maintaining near-optimal performance. Empirical evaluations conducted across various matroid benchmarks demonstrate two main results: (i) the proposed method incurs no loss in regret compared to state-of-the-art algorithms, confirming that the trade-off in performance is negligible; and (ii) there is a clear reduction in overall computation time and the number of costly membership oracle calls. This work thus offers a practical advancement by making combinatorial semi-bandit learning more scalable and efficient without compromising theoretical guarantees. <div>
arXiv:2512.00605v1 Announce Type: new 
Abstract: We study the combinatorial semi-bandit problem under matroid constraints. The regret achieved by recent approaches is optimal, in the sense that it matches the lower bound. Yet, time complexity remains an issue for large matroids or for matroids with costly membership oracles (e.g. online recommendation that ensures diversity). This paper sheds a new light on the matroid semi-bandit problem by exploiting its underlying unimodal structure. We demonstrate that, with negligible loss in regret, the number of iterations involving the membership oracle can be limited to \mathcal{O}(\log \log T)$. This results in an overall improved time complexity of the learning process. Experiments conducted on various matroid benchmarks show (i) no loss in regret compared to state-of-the-art approaches; and (ii) reduced time complexity and number of calls to the membership oracle.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Generalized Graph Transformer Variational Autoencoder</title>
<link>https://arxiv.org/abs/2512.00612</link>
<guid>https://arxiv.org/abs/2512.00612</guid>
<content:encoded><![CDATA[
<div> Keywords: Graph link prediction, Generalized Graph Transformer, Variational Autoencoder, Self-attention, Laplacian positional encoding<br /><br />Summary:<br /><br />The paper introduces the Generalized Graph Transformer Variational Autoencoder (GGT-VAE), a novel model designed for the task of graph link prediction. It combines a Generalized Graph Transformer Architecture with a Variational Autoencoder framework to effectively capture relational dependencies within graphs. Unlike traditional GraphVAE, GCN, or GNN models that rely heavily on message passing, GGT-VAE employs a transformer-style global self-attention mechanism, enabling it to model structural patterns across nodes in a global context. Additionally, the model incorporates Laplacian positional encoding to better embed structural information in the latent space. Experimental evaluations on multiple benchmark datasets demonstrate that GGT-VAE consistently outperforms baseline methods in terms of ROC-AUC and Average Precision metrics. This work represents one of the first attempts to fuse generalized graph transformers with variational frameworks for graph structure generation, marking progress in both generative modeling and graph representation learning. Overall, GGT-VAE opens new pathways for leveraging transformer architectures in graph-based tasks without the limitations of message passing, showing promising results in link prediction applications. <div>
arXiv:2512.00612v1 Announce Type: new 
Abstract: Graph link prediction has long been a central problem in graph representation learning in both network analysis and generative modeling. Recent progress in deep learning has introduced increasingly sophisticated architectures for capturing relational dependencies within graph-structured data. In this work, we propose the Generalized Graph Transformer Variational Autoencoder (GGT-VAE). Our model integrates Generalized Graph Transformer Architecture with Variational Autoencoder framework for link prediction. Unlike prior GraphVAE, GCN, or GNN approaches, GGT-VAE leverages transformer style global self-attention mechanism along with laplacian positional encoding to model structural patterns across nodes into a latent space without relying on message passing. Experimental results on several benchmark datasets demonstrate that GGT-VAE consistently achieves above-baseline performance in terms of ROC-AUC and Average Precision. To the best of our knowledge, this is among the first studies to explore graph structure generation using a generalized graph transformer backbone in a variational framework.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Neuroscience-Inspired Memory Replay for Continual Learning: A Comparative Study of Predictive Coding and Backpropagation-Based Strategies</title>
<link>https://arxiv.org/abs/2512.00619</link>
<guid>https://arxiv.org/abs/2512.00619</guid>
<content:encoded><![CDATA[
arXiv:2512.00619v1 Announce Type: new 
Abstract: Continual learning remains a fundamental challenge in artificial intelligence, with catastrophic forgetting posing a significant barrier to deploying neural networks in dynamic environments. Inspired by biological memory consolidation mechanisms, we propose a novel framework for generative replay that leverages predictive coding principles to mitigate forgetting. We present a comprehensive comparison between predictive coding-based and backpropagation-based gen- erative replay strategies, evaluating their effectiveness on task retention and transfer efficiency across multiple benchmark datasets. Our experimental results demonstrate that predictive coding-based replay achieves superior retention performance (average 15.3% improvement) while maintaining competitive transfer efficiency, suggesting that biologically-inspired mechanisms can offer principled solutions to continual learning challenges. The proposed framework provides insights into the relationship between biological memory processes and artificial learning systems, opening new avenues for neuroscience-inspired AI research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Financial Text Classification Based On rLoRA Finetuning On Qwen3-8B model</title>
<link>https://arxiv.org/abs/2512.00630</link>
<guid>https://arxiv.org/abs/2512.00630</guid>
<content:encoded><![CDATA[
arXiv:2512.00630v1 Announce Type: new 
Abstract: Financial text classification has increasingly become an important aspect in quantitative trading systems and related tasks, such as financial sentiment analysis and the classification of financial news. In this paper, we assess the performance of the large language model Qwen3-8B on both tasks. Qwen3-8B is a state-of-the-art model that exhibits strong instruction-following and multilingual capabilities, and is distinct from standard models, primarily because it is specifically optimized for efficient fine tuning and high performance on reasoning-based benchmarks, making it suitable for financial applications. To adapt this model, we apply Noisy Embedding Instruction Finetuning and based on our previous work, this method increases robustness by injecting controlled noise into the embedding layers during supervised adaptation. We improve efficiency further with Rank-stabilized Low-Rank Adaptation low-rank optimization approach, and FlashAttention, which allow for faster training with lower GPU memory. For both tasks, we benchmark Qwen3-8B against standard classical transformer models, such as T5, BERT, and RoBERTa, and large models at scale, such as LLaMA1-7B, LLaMA2-7B, and Baichuan2-7B. The findings reveal that Qwen3-8B consistently surpasses these baselines by obtaining better classification accuracy and needing fewer training epochs. The synergy of instruction-based fine-tuning and memory-efficient optimization methods suggests Qwen3-8B can potentially serve as a scalable, economical option for real-time financial NLP applications. Qwen3-8B provides a very promising base for advancing dynamic quantitative trading systems in the future.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Privacy Preserving Diffusion Models for Mixed-Type Tabular Data Generation</title>
<link>https://arxiv.org/abs/2512.00638</link>
<guid>https://arxiv.org/abs/2512.00638</guid>
<content:encoded><![CDATA[
arXiv:2512.00638v1 Announce Type: new 
Abstract: We introduce DP-FinDiff, a differentially private diffusion framework for synthesizing mixed-type tabular data. DP-FinDiff employs embedding-based representations for categorical features, reducing encoding overhead and scaling to high-dimensional datasets. To adapt DP-training to the diffusion process, we propose two privacy-aware training strategies: an adaptive timestep sampler that aligns updates with diffusion dynamics, and a feature-aggregated loss that mitigates clipping-induced bias. Together, these enhancements improve fidelity and downstream utility without weakening privacy guarantees. On financial and medical datasets, DP-FinDiff achieves 16-42% higher utility than DP baselines at comparable privacy levels, demonstrating its promise for safe and effective data sharing in sensitive domains.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ML-Tool-Bench: Tool-Augmented Planning for ML Tasks</title>
<link>https://arxiv.org/abs/2512.00672</link>
<guid>https://arxiv.org/abs/2512.00672</guid>
<content:encoded><![CDATA[
arXiv:2512.00672v1 Announce Type: new 
Abstract: The development of autonomous machine learning (ML) agents capable of end-to-end data science workflows represents a significant frontier in artificial intelligence. These agents must orchestrate complex sequences of data analysis, feature engineering, model selection, and hyperparameter optimization, tasks that require sophisticated planning and iteration. While recent work on building ML agents has explored using large language models (LLMs) for direct code generation, tool-augmented approaches offer greater modularity and reliability. However, existing tool-use benchmarks focus primarily on task-specific tool selection or argument extraction for tool invocation, failing to evaluate the sophisticated planning capabilities required for ML Agents. In this work, we introduce a comprehensive benchmark for evaluating tool-augmented ML agents using a curated set of 61 specialized tools and 15 tabular ML challenges from Kaggle. Our benchmark goes beyond traditional tool-use evaluation by incorporating an in-memory named object management, allowing agents to flexibly name, save, and retrieve intermediate results throughout the workflows. We demonstrate that standard ReAct-style approaches struggle to generate valid tool sequences for complex ML pipelines, and that tree search methods with LLM-based evaluation underperform due to inconsistent state scoring. To address these limitations, we propose two simple approaches: 1) using shaped deterministic rewards with structured textual feedback, and 2) decomposing the original problem into a sequence of sub-tasks, which significantly improves trajectory validity and task performance. Using GPT-4o, our approach improves over ReAct by 16.52 percentile positions, taking the median across all Kaggle challenges. We believe our work provides a foundation for developing more capable tool-augmented planning ML agents.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Using physics-inspired Singular Learning Theory to understand grokking &amp; other phase transitions in modern neural networks</title>
<link>https://arxiv.org/abs/2512.00686</link>
<guid>https://arxiv.org/abs/2512.00686</guid>
<content:encoded><![CDATA[
arXiv:2512.00686v1 Announce Type: new 
Abstract: Classical statistical inference and learning theory often fail to explain the success of modern neural networks. A key reason is that these models are non-identifiable (singular), violating core assumptions behind PAC bounds and asymptotic normality. Singular learning theory (SLT), a physics-inspired framework grounded in algebraic geometry, has gained popularity for its ability to close this theory-practice gap. In this paper, we empirically study SLT in toy settings relevant to interpretability and phase transitions. First, we understand the SLT free energy $\mathcal{F}_n$ by testing an Arrhenius-style rate hypothesis using both a grokking modulo-arithmetic model and Anthropic's Toy Models of Superposition. Second, we understand the local learning coefficient $\lambda_{\alpha}$ by measuring how it scales with problem difficulty across several controlled network families (polynomial regressors, low-rank linear networks, and low-rank autoencoders). Our experiments recover known scaling laws while others yield meaningful deviations from theoretical expectations. Overall, our paper illustrates the many merits of SLT for understanding neural network phase transitions, and poses open research questions for the field.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Flow Matching for Tabular Data Synthesis</title>
<link>https://arxiv.org/abs/2512.00698</link>
<guid>https://arxiv.org/abs/2512.00698</guid>
<content:encoded><![CDATA[
arXiv:2512.00698v1 Announce Type: new 
Abstract: Synthetic data generation is an important tool for privacy-preserving data sharing. While diffusion models have set recent benchmarks, flow matching (FM) offers a promising alternative. This paper presents different ways to implement flow matching for tabular data synthesis. We provide a comprehensive empirical study that compares flow matching (FM and variational FM) with a state-of-the-art diffusion method (TabDDPM and TabSyn) in tabular data synthesis. We evaluate both the standard Optimal Transport (OT) and the Variance Preserving (VP) probability paths, and also compare deterministic and stochastic samplers -- something possible when learning to generate using \textit{variational} flow matching -- characterising the empirical relationship between data utility and privacy risk. Our key findings reveal that flow matching, particularly TabbyFlow, outperforms diffusion baselines. Flow matching methods also achieves better performance with remarkably low function evaluations ($\leq$ 100 steps), offering a substantial computational advantage. The choice of probability path is also crucial, as using the OT path demonstrates superior performance, while VP has potential for producing synthetic data with lower disclosure risk. Lastly, our results show that making flows stochastic not only preserves marginal distributions but, in some instances, enables the generation of high utility synthetic data with reduced disclosure risk.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Precision Protein-Ligand Affinity Prediction Benchmark: A Complete and Modification-Aware DAVIS Dataset</title>
<link>https://arxiv.org/abs/2512.00708</link>
<guid>https://arxiv.org/abs/2512.00708</guid>
<content:encoded><![CDATA[
arXiv:2512.00708v1 Announce Type: new 
Abstract: Advancements in AI for science unlocks capabilities for critical drug discovery tasks such as protein-ligand binding affinity prediction. However, current models overfit to existing oversimplified datasets that does not represent naturally occurring and biologically relevant proteins with modifications. In this work, we curate a complete and modification-aware version of the widely used DAVIS dataset by incorporating 4,032 kinase-ligand pairs involving substitutions, insertions, deletions, and phosphorylation events. This enriched dataset enables benchmarking of predictive models under biologically realistic conditions. Based on this new dataset, we propose three benchmark settings-Augmented Dataset Prediction, Wild-Type to Modification Generalization, and Few-Shot Modification Generalization-designed to assess model robustness in the presence of protein modifications. Through extensive evaluation of both docking-free and docking-based methods, we find that docking-based model generalize better in zero-shot settings. In contrast, docking-free models tend to overfit to wild-type proteins and struggle with unseen modifications but show notable improvement when fine-tuned on a small set of modified examples. We anticipate that the curated dataset and benchmarks offer a valuable foundation for developing models that better generalize to protein modifications, ultimately advancing precision medicine in drug discovery. The benchmark is available at: https://github.com/ZhiGroup/DAVIS-complete
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Exploiting Function-Family Structure in Analog Circuit Optimization</title>
<link>https://arxiv.org/abs/2512.00712</link>
<guid>https://arxiv.org/abs/2512.00712</guid>
<content:encoded><![CDATA[
arXiv:2512.00712v1 Announce Type: new 
Abstract: Analog circuit optimization is typically framed as black-box search over arbitrary smooth functions, yet device physics constrains performance mappings to structured families: exponential device laws, rational transfer functions, and regime-dependent dynamics. Off-the-shelf Gaussian-process surrogates impose globally smooth, stationary priors that are misaligned with these regime-switching primitives and can severely misfit highly nonlinear circuits at realistic sample sizes (50--100 evaluations). We demonstrate that pre-trained tabular models encoding these primitives enable reliable optimization without per-circuit engineering. Circuit Prior Network (CPN) combines a tabular foundation model (TabPFN v2) with Direct Expected Improvement (DEI), computing expected improvement exactly under discrete posteriors rather than Gaussian approximations. Across 6 circuits and 25 baselines, structure-matched priors achieve $R^2 \approx 0.99$ in small-sample regimes where GP-Mat\'ern attains only $R^2 = 0.16$ on Bandgap, deliver $1.05$--$3.81\times$ higher FoM with $3.34$--$11.89\times$ fewer iterations, and suggest a shift from hand-crafting models as priors toward systematic physics-informed structure identification. Our code will be made publicly available upon paper acceptance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Graph Data Augmentation with Contrastive Learning on Covariate Distribution Shift</title>
<link>https://arxiv.org/abs/2512.00716</link>
<guid>https://arxiv.org/abs/2512.00716</guid>
<content:encoded><![CDATA[
arXiv:2512.00716v1 Announce Type: new 
Abstract: Covariate distribution shift occurs when certain structural features present in the test set are absent from the training set. It is a common type of out-of-distribution (OOD) problem, frequently encountered in real-world graph data with complex structures. Existing research has revealed that most out-of-the-box graph neural networks (GNNs) fail to account for covariate shifts. Furthermore, we observe that existing methods aimed at addressing covariate shifts often fail to fully leverage the rich information contained within the latent space. Motivated by the potential of the latent space, we introduce a new method called MPAIACL for More Powerful Adversarial Invariant Augmentation using Contrastive Learning. MPAIACL leverages contrastive learning to unlock the full potential of vector representations by harnessing their intrinsic information. Through extensive experiments, MPAIACL demonstrates its robust generalization and effectiveness, as it performs well compared with other baselines across various public OOD datasets. The code is publicly available at https://github.com/flzeng1/MPAIACL.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upcycled and Merged MoE Reward Model for Mitigating Reward Hacking</title>
<link>https://arxiv.org/abs/2512.00724</link>
<guid>https://arxiv.org/abs/2512.00724</guid>
<content:encoded><![CDATA[
arXiv:2512.00724v1 Announce Type: new 
Abstract: Reward models play a critical role in Reinforcement Learning from Human Feedback (RLHF) by assessing the consistency between generated outputs and human preferences. However, conventional reward models are prone to reward hacking or over-optimization, where the policy exploits shortcut patterns to obtain high reward scores that do not reflect true human preference. Although Mixture-of-Experts (MoE)-based reward models can enhance discriminative capability, they typically introduce substantial computational overhead. To address these challenges, we propose an upcycle and merge MoE reward modeling approach. We first upcycle a dense reward model into a MoE architecture, where a shared expert captures general knowledge, while normal experts specialize in instruction-specific patterns. We then apply routing-weight normalization and merge experts back into a dense model through a learnable weight-averaging mechanism, preserving performance gains while significantly reducing inference cost. Experimental results demonstrate that our method effectively mitigates reward hacking across various model scales. Our work highlights the potential of upcycle and merge MoE structures for improving both robustness and efficiency of RLHF reward models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ESMC: MLLM-Based Embedding Selection for Explainable Multiple Clustering</title>
<link>https://arxiv.org/abs/2512.00725</link>
<guid>https://arxiv.org/abs/2512.00725</guid>
<content:encoded><![CDATA[
arXiv:2512.00725v1 Announce Type: new 
Abstract: Typical deep clustering methods, while achieving notable progress, can only provide one clustering result per dataset. This limitation arises from their assumption of a fixed underlying data distribution, which may fail to meet user needs and provide unsatisfactory clustering outcomes. Our work investigates how multi-modal large language models (MLLMs) can be leveraged to achieve user-driven clustering, emphasizing their adaptability to user-specified semantic requirements. However, directly using MLLM output for clustering has risks for producing unstructured and generic image descriptions instead of feature-specific and concrete ones. To address these issues, our method first discovers that MLLMs' hidden states of text tokens are strongly related to the corresponding features, and leverages these embeddings to perform clusterings from any user-defined criteria. We also employ a lightweight clustering head augmented with pseudo-label learning, significantly enhancing clustering accuracy. Extensive experiments demonstrate its competitive performance on diverse datasets and metrics.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Learning for Modeling and Dispatching Hybrid Wind Farm Power Generation</title>
<link>https://arxiv.org/abs/2512.00728</link>
<guid>https://arxiv.org/abs/2512.00728</guid>
<content:encoded><![CDATA[
arXiv:2512.00728v1 Announce Type: new 
Abstract: Wind farms with integrated energy storage, or hybrid wind farms, are able to store energy and dispatch it to the grid following an operational strategy. For individual wind farms with integrated energy storage capacity, data-driven dispatch strategies using localized grid demand and market conditions as input parameters stand to maximize wind energy value. Synthetic power generation data modeled on atmospheric conditions provide another avenue for improving the robustness of data-driven dispatch strategies. To these ends, the present work develops two deep learning frameworks: COVE-NN, an LSTM-based dispatch strategy tailored to individual wind farms, which reduced annual COVE by 32.3% over 43 years of simulated operations in a case study at the Pyron site; and a power generation modeling framework that reduced RMSE by 9.5% and improved power curve similarity by 18.9% when validated on the Palouse wind farm. Together, these models pave the way for more robust, data-driven dispatch strategies and potential extensions to other renewable energy systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>REM: Evaluating LLM Embodied Spatial Reasoning through Multi-Frame Trajectories</title>
<link>https://arxiv.org/abs/2512.00736</link>
<guid>https://arxiv.org/abs/2512.00736</guid>
<content:encoded><![CDATA[
arXiv:2512.00736v1 Announce Type: new 
Abstract: Humans build viewpoint-independent cognitive maps through navigation, enabling intuitive reasoning about object permanence and spatial relations. We argue that multimodal large language models (MLLMs), despite extensive video training, lack this fundamental spatial reasoning capability, a critical limitation for embodied applications. To demonstrate these limitations and drive research, we introduce REM (Reasoning over Embodied Multi-Frame Trajectories), a benchmark using controllable 3D environments for long-horizon embodied spatial reasoning. REM systematically evaluates key aspects like object permanence/distinction, spatial relationships, and numerical tracking across dynamic embodied viewpoints. Our evaluation shows that the best-performing current models exhibit promising overall performance, but become increasingly unreliable at even moderate complexity levels easily handled by humans. These findings highlight challenges MLLMs face in developing robust spatial representations from sequential visual input. Consequently, REM provides targeted metrics and diagnostics to foster improved spatial understanding in future models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Preventing Model Collapse via Contraction-Conditioned Neural Filters</title>
<link>https://arxiv.org/abs/2512.00757</link>
<guid>https://arxiv.org/abs/2512.00757</guid>
<content:encoded><![CDATA[
arXiv:2512.00757v1 Announce Type: new 
Abstract: This paper presents a neural network filter method based on contraction operators to address model collapse in recursive training of generative models. Unlike \cite{xu2024probabilistic}, which requires superlinear sample growth ($O(t^{1+s})$), our approach completely eliminates the dependence on increasing sample sizes within an unbiased estimation framework by designing a neural filter that learns to satisfy contraction conditions. We develop specialized neural network architectures and loss functions that enable the filter to actively learn contraction conditions satisfying Assumption 2.3 in exponential family distributions, thereby ensuring practical application of our theoretical results. Theoretical analysis demonstrates that when the learned contraction conditions are satisfied, estimation errors converge probabilistically even with constant sample sizes, i.e., $\limsup_{t\to\infty}\mathbb{P}(\|\mathbf{e}_t\|>\delta)=0$ for any $\delta>0$. Experimental results show that our neural network filter effectively learns contraction conditions and prevents model collapse under fixed sample size settings, providing an end-to-end solution for practical applications.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Forecasting India's Demographic Transition Under Fertility Policy Scenarios Using hybrid LSTM-PINN Model</title>
<link>https://arxiv.org/abs/2512.00760</link>
<guid>https://arxiv.org/abs/2512.00760</guid>
<content:encoded><![CDATA[
arXiv:2512.00760v1 Announce Type: new 
Abstract: Demographic forecasting remains a fundamental challenge for policy planning in rapidly evolving nations such as India, where fertility transitions, policy interventions, and age structured dynamics interact in complex ways. In this study, we present a hybrid modelling framework that integrates policy-aware fertility functions into a Physics-Informed Neural Network (PINN) enhanced with Long Short-Term Memory (LSTM) networks to capture physical constraints and temporal dependencies in population dynamics. The model is applied to India's age structured population from 2024 to 2054 under three fertility-policy scenarios: continuation of current fertility decline, stricter population control, and relaxed fertility promotion. The governing transport-reaction partial differential equation is formulated with India-specific demographic indicators, including age-specific fertility and mortality rates. PINNs embed the core population equation and policy-driven fertility changes, while LSTM layers improve long-term forecasting across decades. Results show that fertility policies substantially shape future age distribution, dependency ratios, and workforce size. Stricter controls intensify ageing and reduce labour force participation, whereas relaxed policies support workforce growth but increase population pressure. Our findings suggest that the hybrid LSTM-PINN is an effective approach for demographic forecasting, offering accuracy with interpretability. Beyond methodological novelty, this work provides actionable insights for India's demographic policy debates, highlighting the need for balanced fertility interventions to ensure sustainable socio-economic development.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Provable Benefit of Sign Descent: A Minimal Model Under Heavy-Tailed Class Imbalance</title>
<link>https://arxiv.org/abs/2512.00763</link>
<guid>https://arxiv.org/abs/2512.00763</guid>
<content:encoded><![CDATA[
arXiv:2512.00763v1 Announce Type: new 
Abstract: Adaptive optimization methods (such as Adam) play a major role in LLM pretraining, significantly outperforming Gradient Descent (GD). Recent studies have proposed new smoothness assumptions on the loss function to explain the advantages of adaptive algorithms with structured preconditioners, e.g., coordinate-wise or layer-wise, and steepest descent methods w.r.t. non-euclidean norms, e.g., $\ell_\infty$ norm or spectral norm, over GD. However, it remains unclear how these smoothness assumptions manifest in language modelling tasks. In this work, we aim to analyze the benefit of $\ell_\infty$-norm descent (a.k.a. sign descent) directly from properties of the data distribution, namely, heavy-tailed class imbalance. We propose a minimal yet representative setting of next-token prediction, where we can provably show faster convergence of coordinate-wise algorithms such as Sign descent (steepest descent w.r.t. $\ell_\infty$ norm) over normalized GD (steepest descent w.r.t. to $\ell_2$ norm) in the presence of heavy tail class imbalance.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Text Mining Analysis of Symptom Patterns in Medical Chatbot Conversations</title>
<link>https://arxiv.org/abs/2512.00768</link>
<guid>https://arxiv.org/abs/2512.00768</guid>
<content:encoded><![CDATA[
arXiv:2512.00768v1 Announce Type: new 
Abstract: The fast growth of digital health systems has led to a need to better comprehend how they interpret and represent patient-reported symptoms. Chatbots have been used in healthcare to provide clinical support and enhance the user experience, making it possible to provide meaningful clinical patterns from text-based data through chatbots. The proposed research utilises several different natural language processing methods to study the occurrences of symptom descriptions in medicine as well as analyse the patterns that emerge through these conversations within medical bots. Through the use of the Medical Conversations to Disease Dataset which contains 960 multi-turn dialogues divided into 24 Clinical Conditions, a standardised representation of conversations between patient and bot is created for further analysis by computational means. The multi-method approach uses a variety of tools, including Latent Dirichlet Allocation (LDA) to identify latent symptom themes, K-Means to group symptom descriptions by similarity, Transformer-based Named Entity Recognition (NER) to extract medical concepts, and the Apriori algorithm to discover frequent symptom pairs. Findings from the analysis indicate a coherent structure of clinically relevant topics, moderate levels of clustering cohesiveness and several high confidence rates on the relationships between symptoms like fever headache and rash itchiness. The results support the notion that conversational medical data can be a valuable diagnostic signal for early symptom interpretation, assist in strengthening decision support and improve how users interact with tele-health technology. By demonstrating a method for converting unstructured free-flowing dialogue into actionable knowledge regarding symptoms this work provides an extensible framework to further enhance future performance, dependability and clinical utility of selecting medical chatbots.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AI Agent for Source Finding by SoFiA-2 for SKA-SDC2</title>
<link>https://arxiv.org/abs/2512.00769</link>
<guid>https://arxiv.org/abs/2512.00769</guid>
<content:encoded><![CDATA[
arXiv:2512.00769v1 Announce Type: new 
Abstract: Source extraction is crucial in analyzing data from next-generation, large-scale sky surveys in radio bands, such as the Square Kilometre Array (SKA). Several source extraction programs, including SoFiA and Aegean, have been developed to address this challenge. However, finding optimal parameter configurations when applying these programs to real observations is non-trivial. For example, the outcomes of SoFiA intensely depend on several key parameters across its preconditioning, source-finding, and reliability-filtering modules. To address this issue, we propose a framework to automatically optimize these parameters using an AI agent based on a state-of-the-art reinforcement learning (RL) algorithm, i.e., Soft Actor-Critic (SAC). The SKA Science Data Challenge 2 (SDC2) dataset is utilized to assess the feasibility and reliability of this framework. The AI agent interacts with the environment by adjusting parameters based on the feedback from the SDC2 score defined by the SDC2 Team, progressively learning to select parameter sets that yield improved performance. After sufficient training, the AI agent can automatically identify an optimal parameter configuration that outperform the benchmark set by Team SoFiA within only 100 evaluation steps and with reduced time consumption. Our approach could address similar problems requiring complex parameter tuning, beyond radio band surveys and source extraction. Yet, high-quality training sets containing representative observations and catalogs of ground truth are essential.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>What Is Preference Optimization Doing, How and Why?</title>
<link>https://arxiv.org/abs/2512.00778</link>
<guid>https://arxiv.org/abs/2512.00778</guid>
<content:encoded><![CDATA[
arXiv:2512.00778v1 Announce Type: new 
Abstract: Preference optimization (PO) is indispensable for large language models (LLMs), with methods such as direct preference optimization (DPO) and proximal policy optimization (PPO) achieving great success. A common belief is that DPO is supervised learning while PPO is reinforcement learning, yet deeper analyses for the reasons underlying these differences remain lacking. To fill this gap, we analyze their optimization dynamics, revealing distinct algorithmic behaviors and comprehending their underlying causes. First, we examine the target directions of gradient-based updates and find that DPO follows stable targets, whereas PPO follows dynamic targets that balance exploration and exploitation, thus validating the common belief from a new perspective. Second, we examine the roles of positive learning, negative learning, and loss reweighting, which are three key components in PO methods. Our analyses reveal that these components play fairly different roles. In DPO, positive and negative learning jointly shape the learning targets meanwhile mutually offset each other. However, loss reweighting in DPO acts less as a reward signal but more as a regularizer to mitigate overfitting. In PPO, negative learning primarily supports exploration rather than determining the targets. Meanwhile, loss reweighting, related to absolute values of token-level advantages, indicates the distinct roles of token groups in updating targets. Given these findings, we conduct carefully designed ablation studies to further examine how controlling these dynamics impacts optimization efficiency and practical performance. The insights gained from our analyses not only deepen the understanding of PO methods but also inspire the development of more preference-aligned LLMs.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Sigma: The Key for Vision-Language-Action Models toward Telepathic Alignment</title>
<link>https://arxiv.org/abs/2512.00783</link>
<guid>https://arxiv.org/abs/2512.00783</guid>
<content:encoded><![CDATA[
arXiv:2512.00783v1 Announce Type: new 
Abstract: To address the gap in humanoid robot cognitive systems regarding the lack of a time-updable mediating thought space between semantics and continuous control, this study constructs and trains a VLA model named "Sigma" that runs on a single RTX 4090. It uses the open-source pi05_base model as a foundation and preprocesses svla_so101_pickplace into a training dataset. The researcher independently designed an architecture for a vision-language-action model that combines deep semantic understanding and association to achieve telepathic communication. The training process involved repeated optimizations of data preprocessing, LoRA fine-tuning, and the inference-stage adapter. The experiment employed offline closed-loop replay, comparing Sigma with the untuned pure pi05_base_base model under data conditions. Results showed that Sigma exhibited a stable decrease in control MSE across vector, fragment, and entire trajectory timescales, while maintaining the telepathy norm and semantic-text alignment quality unchanged. It demonstrates that mind-responsive alignment control is quantified through an architecture that combines deep understanding of semantics and association without retraining the base model, which provides reproducible experience for semantic alignment and intention-driven behavior in humanoid robots.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Limitations of Using Identical Distributions for Training and Testing When Learning Boolean Functions</title>
<link>https://arxiv.org/abs/2512.00791</link>
<guid>https://arxiv.org/abs/2512.00791</guid>
<content:encoded><![CDATA[
arXiv:2512.00791v1 Announce Type: new 
Abstract: When the distributions of the training and test data do not coincide, the problem of understanding generalization becomes considerably more complex, prompting a variety of questions. In this work, we focus on a fundamental one: Is it always optimal for the training distribution to be identical to the test distribution? Surprisingly, assuming the existence of one-way functions, we find that the answer is no. That is, matching distributions is not always the best scenario, which contrasts with the behavior of most learning methods. Nonetheless, we also show that when certain regularities are imposed on the target functions, the standard conclusion is recovered in the case of the uniform distribution.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Estimating the Effective Rank of Vision Transformers via Low-Rank Factorization</title>
<link>https://arxiv.org/abs/2512.00792</link>
<guid>https://arxiv.org/abs/2512.00792</guid>
<content:encoded><![CDATA[
arXiv:2512.00792v1 Announce Type: new 
Abstract: Deep networks are heavily over-parameterized, yet their learned representations often admit low-rank structure. We introduce a framework for estimating a model's intrinsic dimensionality by treating learned representations as projections onto a low-rank subspace of the model's full capacity. Our approach: train a full-rank teacher, factorize its weights at multiple ranks, and train each factorized student via distillation to measure performance as a function of rank.
  We define effective rank as a region, not a point: the smallest contiguous set of ranks for which the student reaches 85-95% of teacher accuracy. To stabilize estimates, we fit accuracy vs. rank with a monotone PCHIP interpolant and identify crossings of the normalized curve. We also define the effective knee as the rank maximizing perpendicular distance between the smoothed accuracy curve and its endpoint secant; an intrinsic indicator of where marginal gains concentrate.
  On ViT-B/32 fine-tuned on CIFAR-100 (one seed, due to compute constraints), factorizing linear blocks and training with distillation yields an effective-rank region of approximately [16, 34] and an effective knee at r* ~ 31. At rank 32, the student attains 69.46% top-1 accuracy vs. 73.35% for the teacher (~94.7% of baseline) while achieving substantial parameter compression. We provide a framework to estimate effective-rank regions and knees across architectures and datasets, offering a practical tool for characterizing the intrinsic dimensionality of deep models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Soft Quality-Diversity Optimization</title>
<link>https://arxiv.org/abs/2512.00810</link>
<guid>https://arxiv.org/abs/2512.00810</guid>
<content:encoded><![CDATA[
arXiv:2512.00810v1 Announce Type: new 
Abstract: Quality-Diversity (QD) algorithms constitute a branch of optimization that is concerned with discovering a diverse and high-quality set of solutions to an optimization problem. Current QD methods commonly maintain diversity by dividing the behavior space into discrete regions, ensuring that solutions are distributed across different parts of the space. The QD problem is then solved by searching for the best solution in each region. This approach to QD optimization poses challenges in large solution spaces, where storing many solutions is impractical, and in high-dimensional behavior spaces, where discretization becomes ineffective due to the curse of dimensionality. We present an alternative framing of the QD problem, called \emph{Soft QD}, that sidesteps the need for discretizations. We validate this formulation by demonstrating its desirable properties, such as monotonicity, and by relating its limiting behavior to the widely used QD Score metric. Furthermore, we leverage it to derive a novel differentiable QD algorithm, \emph{Soft QD Using Approximated Diversity (SQUAD)}, and demonstrate empirically that it is competitive with current state of the art methods on standard benchmarks while offering better scalability to higher dimensional problems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Causal Invariance and Counterfactual Learning Driven Cooperative Game for Multi-Label Classification</title>
<link>https://arxiv.org/abs/2512.00812</link>
<guid>https://arxiv.org/abs/2512.00812</guid>
<content:encoded><![CDATA[
arXiv:2512.00812v1 Announce Type: new 
Abstract: Multi-label classification (MLC) remains vulnerable to label imbalance, spurious correlations, and distribution shifts, challenges that are particularly detrimental to rare label prediction. To address these limitations, we introduce the Causal Cooperative Game (CCG) framework, which conceptualizes MLC as a cooperative multi-player interaction. CCG unifies explicit causal discovery via Neural Structural Equation Models with a counterfactual curiosity reward to drive robust feature learning. Furthermore, it incorporates a causal invariance loss to ensure generalization across diverse environments, complemented by a specialized enhancement strategy for rare labels. Extensive benchmarking demonstrates that CCG substantially outperforms strong baselines in both rare label prediction and overall robustness. Through rigorous ablation studies and qualitative analysis, we validate the efficacy and interpretability of our components, underscoring the potential of synergizing causal inference with cooperative game theory for advancing multi-label learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>ReJump: A Tree-Jump Representation for Analyzing and Improving LLM Reasoning</title>
<link>https://arxiv.org/abs/2512.00831</link>
<guid>https://arxiv.org/abs/2512.00831</guid>
<content:encoded><![CDATA[
arXiv:2512.00831v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) are Large Language Models (LLMs) explicitly trained to generate long-form Chain-of-Thoughts (CoTs), achieving impressive success on challenging tasks like math and programming. However, their underlying reasoning "algorithms" remain poorly understood. To investigate this, we propose ReJump, which represents a reasoning trace as a visitation order over nodes in a tree of intermediate problem-solving steps. Transitions between nodes, which we term jumps, include adjacent moves that capture behaviors such as calculation, and non-adjacent moves that capture behaviors such as backtracking and verification. ReJump enables analyzing LLM reasoning with diverse metrics that quantify exploration, exploitation, overthinking, forgetting, and verification. Using our proposed LLM agent to extract reasoning traces into ReJump format, we evaluate state-of-the-art LRMs on two tasks and find that models with similar accuracy can exhibit distinct reasoning behaviors, while different tasks favor different reasoning styles (e.g., varying balance between exploration and exploitation). To further understand how learning strategies shape reasoning, we use ReJump to compare distilled LRMs with their teachers, CoT-prompted LLMs with LRMs, and to examine how the number of reasoning examples and reinforcement learning affect reasoning behavior. Finally, we show that ReJump can improve reasoning quality at test time through strategies such as ReJump-guided Best-of-N selection and prompt selection. Our code is publicly available at https://github.com/UW-Madison-Lee-Lab/ReJump.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Uncertainty Quantification for Deep Regression using Contextualised Normalizing Flows</title>
<link>https://arxiv.org/abs/2512.00835</link>
<guid>https://arxiv.org/abs/2512.00835</guid>
<content:encoded><![CDATA[
arXiv:2512.00835v1 Announce Type: new 
Abstract: Quantifying uncertainty in deep regression models is important both for understanding the confidence of the model and for safe decision-making in high-risk domains. Existing approaches that yield prediction intervals overlook distributional information, neglecting the effect of multimodal or asymmetric distributions on decision-making. Similarly, full or approximated Bayesian methods, while yielding the predictive posterior density, demand major modifications to the model architecture and retraining. We introduce MCNF, a novel post hoc uncertainty quantification method that produces both prediction intervals and the full conditioned predictive distribution. MCNF operates on top of the underlying trained predictive model; thus, no predictive model retraining is needed. We provide experimental evidence that the MCNF-based uncertainty estimate is well calibrated, is competitive with state-of-the-art uncertainty quantification methods, and provides richer information for downstream decision-making tasks.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Prediction-space knowledge markets for communication-efficient federated learning on multimedia tasks</title>
<link>https://arxiv.org/abs/2512.00841</link>
<guid>https://arxiv.org/abs/2512.00841</guid>
<content:encoded><![CDATA[
arXiv:2512.00841v1 Announce Type: new 
Abstract: Federated learning (FL) enables collaborative training over distributed multimedia data but suffers acutely from statistical heterogeneity and communication constraints, especially when clients deploy large models. Classic parameter-averaging methods such as FedAvg transmit full model weights and can diverge under nonindependent and identically distributed (non-IID) data. We propose KTA v2, a prediction-space knowledge trading market for FL. Each round, clients locally train on their private data, then share only logits on a small public reference set. The server constructs a client-client similarity graph in prediction space, combines it with reference-set accuracy to form per-client teacher ensembles, and sends back personalized soft targets for a second-stage distillation update. This two-stage procedure can be interpreted as approximate block-coordinate descent on a unified objective with prediction-space regularization. Experiments on FEMNIST, CIFAR-10 and AG News show that, under comparable or much lower communication budgets, KTA v2 consistently outperforms a local-only baseline and strong parameter-based methods (FedAvg, FedProx), and substantially improves over a FedMD-style global teacher. On CIFAR-10 with ResNet-18, KTA v2 reaches 57.7% test accuracy using approximately 1/1100 of FedAvg's communication, while on AG News it attains 89.3% accuracy with approximately 1/300 of FedAvg's traffic.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Topological Federated Clustering via Gravitational Potential Fields under Local Differential Privacy</title>
<link>https://arxiv.org/abs/2512.00849</link>
<guid>https://arxiv.org/abs/2512.00849</guid>
<content:encoded><![CDATA[
arXiv:2512.00849v1 Announce Type: new 
Abstract: Clustering non-independent and identically distributed (non-IID) data under local differential privacy (LDP) in federated settings presents a critical challenge: preserving privacy while maintaining accuracy without iterative communication. Existing one-shot methods rely on unstable pairwise centroid distances or neighborhood rankings, degrading severely under strong LDP noise and data heterogeneity. We present Gravitational Federated Clustering (GFC), a novel approach to privacy-preserving federated clustering that overcomes the limitations of distance-based methods under varying LDP. Addressing the critical challenge of clustering non-IID data with diverse privacy guarantees, GFC transforms privatized client centroids into a global gravitational potential field where true cluster centers emerge as topologically persistent singularities. Our framework introduces two key innovations: (1) a client-side compactness-aware perturbation mechanism that encodes local cluster geometry as "mass" values, and (2) a server-side topological aggregation phase that extracts stable centroids through persistent homology analysis of the potential field's superlevel sets. Theoretically, we establish a closed-form bound between the privacy budget $\epsilon$ and centroid estimation error, proving the potential field's Lipschitz smoothing properties exponentially suppress noise in high-density regions. Empirically, GFC outperforms state-of-the-art methods on ten benchmarks, especially under strong LDP constraints ($\epsilon < 1$), while maintaining comparable performance at lower privacy budgets. By reformulating federated clustering as a topological persistence problem in a synthetic physics-inspired space, GFC achieves unprecedented privacy-accuracy trade-offs without iterative communication, providing a new perspective for privacy-preserving distributed learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>City-Conditioned Memory for Multi-City Traffic and Mobility Forecasting</title>
<link>https://arxiv.org/abs/2512.00851</link>
<guid>https://arxiv.org/abs/2512.00851</guid>
<content:encoded><![CDATA[
arXiv:2512.00851v1 Announce Type: new 
Abstract: Deploying spatio-temporal forecasting models across many cities is difficult: traffic networks differ in size and topology, data availability can vary by orders of magnitude, and new cities may provide only a short history of logs. Existing deep traffic models are typically trained per city and backbone, creating high maintenance cost and poor transfer to data-scarce cities. We ask whether a single, backbone-agnostic layer can condition on "which city this sequence comes from", improve accuracy in full- and low-data regimes, and support better cross-city adaptation with minimal code changes.
  We propose CityCond, a light-weight city-conditioned memory layer that augments existing spatio-temporal backbones. CityCond combines a city-ID encoder with an optional shared memory bank (CityMem). Given a city index and backbone hidden states, it produces city-conditioned features fused through gated residual connections. We attach CityCond to five representative backbones (GRU, TCN, Transformer, GNN, STGCN) and evaluate three regimes: full-data, low-data, and cross-city few-shot transfer on METR-LA and PEMS-BAY. We also run auxiliary experiments on SIND, a drone-based multi-agent trajectory dataset from a signalized intersection in Tianjin (we focus on pedestrian tracks).
  Across more than fourteen model variants and three random seeds, CityCond yields consistent improvements, with the largest gains for high-capacity backbones such as Transformers and STGCNs. CityMem reduces Transformer error by roughly one third in full-data settings and brings substantial gains in low-data and cross-city transfer. On SIND, simple city-ID conditioning modestly improves low-data LSTM performance. CityCond can therefore serve as a reusable design pattern for scalable, multi-city forecasting under realistic data constraints.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Robust Probabilistic Load Forecasting for a Single Household: A Comparative Study from SARIMA to Transformers on the REFIT Dataset</title>
<link>https://arxiv.org/abs/2512.00856</link>
<guid>https://arxiv.org/abs/2512.00856</guid>
<content:encoded><![CDATA[
arXiv:2512.00856v1 Announce Type: new 
Abstract: Probabilistic forecasting is essential for modern risk management, allowing decision-makers to quantify uncertainty in critical systems. This paper tackles this challenge using the volatile REFIT household dataset, which is complicated by a large structural data gap. We first address this by conducting a rigorous comparative experiment to select a Seasonal Imputation method, demonstrating its superiority over linear interpolation in preserving the data's underlying distribution. We then systematically evaluate a hierarchy of models, progressing from classical baselines (SARIMA, Prophet) to machine learning (XGBoost) and advanced deep learning architectures (LSTM). Our findings reveal that classical models fail to capture the data's non-linear, regime-switching behavior. While the LSTM provided the most well-calibrated probabilistic forecast, the Temporal Fusion Transformer (TFT) emerged as the superior all-round model, achieving the best point forecast accuracy (RMSE 481.94) and producing safer, more cautious prediction intervals that effectively capture extreme volatility.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>The Spectral Dimension of NTKs is Constant: A Theory of Implicit Regularization, Finite-Width Stability, and Scalable Estimation</title>
<link>https://arxiv.org/abs/2512.00860</link>
<guid>https://arxiv.org/abs/2512.00860</guid>
<content:encoded><![CDATA[
arXiv:2512.00860v1 Announce Type: new 
Abstract: Modern deep networks are heavily overparameterized yet often generalize well, suggesting a form of low intrinsic complexity not reflected by parameter counts. We study this complexity at initialization through the effective rank of the Neural Tangent Kernel (NTK) Gram matrix, $r_{\text{eff}}(K) = (\text{tr}(K))^2/\|K\|_F^2$. For i.i.d. data and the infinite-width NTK $k$, we prove a constant-limit law $\lim_{n\to\infty} \mathbb{E}[r_{\text{eff}}(K_n)] = \mathbb{E}[k(x, x)]^2 / \mathbb{E}[k(x, x')^2] =: r_\infty$, with sub-Gaussian concentration. We further establish finite-width stability: if the finite-width NTK deviates in operator norm by $O_p(m^{-1/2})$ (width $m$), then $r_{\text{eff}}$ changes by $O_p(m^{-1/2})$. We design a scalable estimator using random output probes and a CountSketch of parameter Jacobians and prove conditional unbiasedness and consistency with explicit variance bounds. On CIFAR-10 with ResNet-20/56 (widths 16/32) across $n \in \{10^3, 5\times10^3, 10^4, 2.5\times10^4, 5\times10^4\}$, we observe $r_{\text{eff}} \approx 1.0\text{--}1.3$ and slopes $\approx 0$ in $n$, consistent with the theory, and the kernel-moment prediction closely matches fitted constants.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>HBLLM: Wavelet-Enhanced High-Fidelity 1-Bit Quantization for LLMs</title>
<link>https://arxiv.org/abs/2512.00862</link>
<guid>https://arxiv.org/abs/2512.00862</guid>
<content:encoded><![CDATA[
arXiv:2512.00862v1 Announce Type: new 
Abstract: We introduce HBLLM, a wavelet-enhanced high-fidelity $1$-bit post-training quantization method for Large Language Models (LLMs). By leveraging Haar wavelet transforms to enhance expressive capacity through frequency decomposition, HBLLM significantly improves quantization fidelity while maintaining minimal overhead. This approach features two innovative structure-aware grouping strategies: (1) frequency-aware multi-parameter intra-row grouping and (2) $\ell_2$-norm-based saliency-driven column selection. For non-salient weights, a shared mean is employed across quantization groups within each frequency band to optimize storage efficiency. Experiments conducted on the OPT and LLaMA models demonstrate that HBLLM achieves state-of-the-art performance in $1$-bit quantization, attaining a perplexity of $6.71$ on LLaMA$2$-$13$B with an average weight storage of only $1.08$ bits. Code available at: https://github.com/Yeyke/HBLLM.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Towards Active Synthetic Data Generation for Finetuning Language Models</title>
<link>https://arxiv.org/abs/2512.00884</link>
<guid>https://arxiv.org/abs/2512.00884</guid>
<content:encoded><![CDATA[
arXiv:2512.00884v1 Announce Type: new 
Abstract: A common and effective means for improving language model capabilities involves finetuning a ``student'' language model's parameters on generations from a more proficient ``teacher'' model. Termed ``synthetic data'', these generations are often produced before any student finetuning, but some work has considered generating new synthetic samples as training progresses. This paper studies and advocates for the latter case, where data are generated in an iterative, closed-loop fashion that is guided by the current state of the student model. For a fixed budget of generated samples, or a budget in terms of compute spent querying a teacher, we show that this curation of finetuning data affords improved student performance over static generation. Further, while there have been several LLM-specific methods proposed that operate in this regime, we find that simple, inexpensive selection criteria from the active learning literature tend to be most performant. We validate these claims across four mathematical and logical reasoning datasets using four different small language models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Light-Weight Benchmarks Reveal the Hidden Hardware Cost of Zero-Shot Tabular Foundation Models</title>
<link>https://arxiv.org/abs/2512.00888</link>
<guid>https://arxiv.org/abs/2512.00888</guid>
<content:encoded><![CDATA[
arXiv:2512.00888v1 Announce Type: new 
Abstract: Zero-shot foundation models (FMs) promise training-free prediction on tabular data, yet their hardware footprint remains poorly characterized. We present a fully reproducible benchmark that reports test accuracy together with wall-clock latency, peak CPU RAM, and peak GPU VRAM on four public datasets: Adult-Income, Higgs-100k, Wine-Quality, and California-Housing. Two open FMs (TabPFN-1.0 and TabICL-base) are compared against tuned XGBoost, LightGBM, and Random Forest baselines on a single NVIDIA T4 GPU. The tree ensembles equal or surpass FM accuracy on three datasets while completing full-test batches in <= 0.40 s and <= 150 MB RAM, using zero VRAM. TabICL achieves a 0.8 percentage-point gain on Higgs but requires roughly 40,000 times more latency (960 s) and 9 GB VRAM. TabPFN matches tree-model accuracy on Wine and Housing but peaks at 4 GB VRAM and cannot process the full 100k-row Higgs table. These results quantify the substantial hardware-versus-accuracy trade-offs in current tabular FMs and provide an open baseline for future efficiency-oriented research.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Beyond High-Entropy Exploration: Correctness-Aware Low-Entropy Segment-Based Advantage Shaping for Reasoning LLMs</title>
<link>https://arxiv.org/abs/2512.00908</link>
<guid>https://arxiv.org/abs/2512.00908</guid>
<content:encoded><![CDATA[
arXiv:2512.00908v1 Announce Type: new 
Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become a central approach for improving the reasoning ability of large language models. Recent work studies RLVR through token entropy, arguing that high-entropy tokens drive exploration and should receive stronger updates. However, they overlook the fact that most of a reasoning trajectory consists of low-entropy segments that encode stable and reusable structural patterns. Through qualitative and quantitative analyses, we find that the overlap of low-entropy segments across correct responses strongly correlates with model accuracy, while overlaps involving incorrect responses exhibit stable but unproductive patterns. Motivated by these findings, we propose LESS, a correctness-aware reinforcement framework that performs fine-grained advantage modulation over low-entropy segments. LESS amplifies segments unique to correct responses, suppresses those unique to incorrect ones, and neutralizes segments shared by both, while preserving high-entropy exploration in the underlying RL algorithm. Instantiated on top of the popular GRPO, LESS consistently improves accuracy over strong RL baselines across three backbones and six math benchmarks, achieves stronger robustness of the performance floor.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Partially Equivariant Reinforcement Learning in Symmetry-Breaking Environments</title>
<link>https://arxiv.org/abs/2512.00915</link>
<guid>https://arxiv.org/abs/2512.00915</guid>
<content:encoded><![CDATA[
arXiv:2512.00915v1 Announce Type: new 
Abstract: Group symmetries provide a powerful inductive bias for reinforcement learning (RL), enabling efficient generalization across symmetric states and actions via group-invariant Markov Decision Processes (MDPs). However, real-world environments almost never realize fully group-invariant MDPs; dynamics, actuation limits, and reward design usually break symmetries, often only locally. Under group-invariant Bellman backups for such cases, local symmetry-breaking introduces errors that propagate across the entire state-action space, resulting in global value estimation errors. To address this, we introduce Partially group-Invariant MDP (PI-MDP), which selectively applies group-invariant or standard Bellman backups depending on where symmetry holds. This framework mitigates error propagation from locally broken symmetries while maintaining the benefits of equivariance, thereby enhancing sample efficiency and generalizability. Building on this framework, we present practical RL algorithms -- Partially Equivariant (PE)-DQN for discrete control and PE-SAC for continuous control -- that combine the benefits of equivariance with robustness to symmetry-breaking. Experiments across Grid-World, locomotion, and manipulation benchmarks demonstrate that PE-DQN and PE-SAC significantly outperform baselines, highlighting the importance of selective symmetry exploitation for robust and sample-efficient RL.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>D-CTNet: A Dual-Branch Channel-Temporal Forecasting Network with Frequency-Domain Correction</title>
<link>https://arxiv.org/abs/2512.00925</link>
<guid>https://arxiv.org/abs/2512.00925</guid>
<content:encoded><![CDATA[
arXiv:2512.00925v1 Announce Type: new 
Abstract: Accurate Multivariate Time Series (MTS) forecasting is crucial for collaborative design of complex systems, Digital Twin building, and maintenance ahead of time. However, the collaborative industrial environment presents new challenges for MTS forecasting models: models should decouple complex inter-variable dependencies while addressing non-stationary distribution shift brought by environmental changes. To address these challenges and improve collaborative sensing reliability, we propose a Patch-Based Dual-Branch Channel-Temporal Forecasting Network (D-CTNet). Particularly, with a parallel dual-branch design incorporating linear temporal modeling layer and channel attention mechanism, our method explicitly decouples and jointly learns intra-channel temporal evolution patterns and dynamic multivariate correlations. Furthermore, a global patch attention fusion module goes beyond the local window scope to model long range dependencies. Most importantly, aiming at non-stationarity, a Frequency-Domain Stationarity Correction mechanism adaptively suppresses distribution shift impacts from environment change by spectrum alignment. Evaluations on seven benchmark datasets show that our model achieves better forecasting accuracy and robustness compared with state-of-the-art methods. Our work shows great promise as a new forecasting engine for industrial collaborative systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Memory-Integrated Reconfigurable Adapters: A Unified Framework for Settings with Multiple Tasks</title>
<link>https://arxiv.org/abs/2512.00940</link>
<guid>https://arxiv.org/abs/2512.00940</guid>
<content:encoded><![CDATA[
arXiv:2512.00940v1 Announce Type: new 
Abstract: Organisms constantly pivot between tasks such as evading predators, foraging, traversing rugged terrain, and socializing, often within milliseconds. Remarkably, they preserve knowledge of once-learned environments sans catastrophic forgetting, a phenomenon neuroscientists hypothesize, is due to a singular neural circuitry dynamically overlayed by neuromodulatory agents such as dopamine and acetylcholine. In parallel, deep learning research addresses analogous challenges via domain generalization (DG) and continual learning (CL), yet these methods remain siloed, despite the brains ability to perform them seamlessly. In particular, prior work has not explored architectures involving associative memories (AMs), which are an integral part of biological systems, to jointly address these tasks. We propose Memory-Integrated Reconfigurable Adapters (MIRA), a unified framework that integrates Hopfield-style associative memory modules atop a shared backbone. Associative memory keys are learned post-hoc to index and retrieve an affine combination of stored adapter updates for any given task or domain on a per-sample basis. By varying only the task-specific objectives, we demonstrate that MIRA seamlessly accommodates domain shifts and sequential task exposures under one roof. Empirical evaluations on standard benchmarks confirm that our AM-augmented architecture significantly enhances adaptability and retention: in DG, MIRA achieves SoTA out-of-distribution accuracy, and in incremental learning settings, it outperforms architectures explicitly designed to handle catastrophic forgetting using generic CL algorithms. By unifying adapter-based modulation with biologically inspired associative memory, MIRA delivers rapid task switching and enduring knowledge retention in a single extensible architecture, charting a path toward more versatile and memory-augmented AI systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Multi-Modal AI for Remote Patient Monitoring in Cancer Care</title>
<link>https://arxiv.org/abs/2512.00949</link>
<guid>https://arxiv.org/abs/2512.00949</guid>
<content:encoded><![CDATA[
arXiv:2512.00949v1 Announce Type: new 
Abstract: For patients undergoing systemic cancer therapy, the time between clinic visits is full of uncertainties and risks of unmonitored side effects. To bridge this gap in care, we developed and prospectively trialed a multi-modal AI framework for remote patient monitoring (RPM). This system integrates multi-modal data from the HALO-X platform, such as demographics, wearable sensors, daily surveys, and clinical events. Our observational trial is one of the largest of its kind and has collected over 2.1 million data points (6,080 patient-days) of monitoring from 84 patients. We developed and adapted a multi-modal AI model to handle the asynchronous and incomplete nature of real-world RPM data, forecasting a continuous risk of future adverse events. The model achieved an accuracy of 83.9% (AUROC=0.70). Notably, the model identified previous treatments, wellness check-ins, and daily maximum heart rate as key predictive features. A case study demonstrated the model's ability to provide early warnings by outputting escalating risk profiles prior to the event. This work establishes the feasibility of multi-modal AI RPM for cancer care and offers a path toward more proactive patient support.(Accepted at Europe NeurIPS 2025 Multimodal Representation Learning for Healthcare Workshop)
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>WUSH: Near-Optimal Adaptive Transforms for LLM Quantization</title>
<link>https://arxiv.org/abs/2512.00956</link>
<guid>https://arxiv.org/abs/2512.00956</guid>
<content:encoded><![CDATA[
arXiv:2512.00956v1 Announce Type: new 
Abstract: Quantization to low bitwidth is a standard approach for deploying large language models, however, a few extreme weights and activations stretch the dynamic range and reduce the effective resolution of the quantizer. A common mitigation approach is to apply some fixed orthogonal transforms, such as Hadamard matrices, before quantization, which typically reduces the dynamic range. Yet, these transforms ignore the statistics of the data, and their optimality is currently not understood. In this work, we derive, for the first time, closed-form optimal linear blockwise transforms for joint weight-activation quantization using standard data-free quantizers for common numerical formats. Specifically, we provide derivations of the optimal adaptive (data-aware) transforms for round-to-nearest (RTN), AbsMax-scaled block quantizers for both integer and floating-point formats. The resulting construction, which we call WUSH, combines a Hadamard backbone with a data-dependent component based on second-order moments, yielding a non-orthogonal transform that is provably optimal under mild assumptions and remains structured for efficient implementation. Preliminary experimental results show that our approach consistently improves upon the Hadamard transform for common formats.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Goal-Driven Reward by Video Diffusion Models for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.00961</link>
<guid>https://arxiv.org/abs/2512.00961</guid>
<content:encoded><![CDATA[
arXiv:2512.00961v1 Announce Type: new 
Abstract: Reinforcement Learning (RL) has achieved remarkable success in various domains, yet it often relies on carefully designed programmatic reward functions to guide agent behavior. Designing such reward functions can be challenging and may not generalize well across different tasks. To address this limitation, we leverage the rich world knowledge contained in pretrained video diffusion models to provide goal-driven reward signals for RL agents without ad-hoc design of reward. Our key idea is to exploit off-the-shelf video diffusion models pretrained on large-scale video datasets as informative reward functions in terms of video-level and frame-level goals. For video-level rewards, we first finetune a pretrained video diffusion model on domain-specific datasets and then employ its video encoder to evaluate the alignment between the latent representations of agent's trajectories and the generated goal videos. To enable more fine-grained goal-achievement, we derive a frame-level goal by identifying the most relevant frame from the generated video using CLIP, which serves as the goal state. We then employ a learned forward-backward representation that represents the probability of visiting the goal state from a given state-action pair as frame-level reward, promoting more coherent and goal-driven trajectories. Experiments on various Meta-World tasks demonstrate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Subgroup Validity in Machine Learning for Echocardiogram Data</title>
<link>https://arxiv.org/abs/2512.00976</link>
<guid>https://arxiv.org/abs/2512.00976</guid>
<content:encoded><![CDATA[
arXiv:2512.00976v1 Announce Type: new 
Abstract: Echocardiogram datasets enable training deep learning models to automate interpretation of cardiac ultrasound, thereby expanding access to accurate readings of diagnostically-useful images. However, the gender, sex, race, and ethnicity of the patients in these datasets are underreported and subgroup-specific predictive performance is unevaluated. These reporting deficiencies raise concerns about subgroup validity that must be studied and addressed before model deployment. In this paper, we show that current open echocardiogram datasets are unable to assuage subgroup validity concerns. We improve sociodemographic reporting for two datasets: TMED-2 and MIMIC-IV-ECHO. Analysis of six open datasets reveals no consideration of gender-diverse patients and insufficient patient counts for many racial and ethnic groups. We further perform an exploratory subgroup analysis of two published aortic stenosis detection models on TMED-2. We find insufficient evidence for subgroup validity for sex, racial, and ethnic subgroups. Our findings highlight that more data for underrepresented subgroups, improved demographic reporting, and subgroup-focused analyses are needed to prove subgroup validity in future work.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Upper Approximation Bounds for Neural Oscillators</title>
<link>https://arxiv.org/abs/2512.01015</link>
<guid>https://arxiv.org/abs/2512.01015</guid>
<content:encoded><![CDATA[
arXiv:2512.01015v1 Announce Type: new 
Abstract: Neural oscillators, originating from the second-order ordinary differential equations (ODEs), have demonstrated competitive performance in stably learning causal mappings between long-term sequences or continuous temporal functions. However, theoretically quantifying the capacities of their neural network architectures remains a significant challenge. In this study, the neural oscillator consisting of a second-order ODE followed by a multilayer perceptron (MLP) is considered. Its upper approximation bound for approximating causal and uniformly continuous operators between continuous temporal function spaces and that for approximating uniformly asymptotically incrementally stable second-order dynamical systems are derived. The established proof method of the approximation bound for approximating the causal continuous operators can also be directly applied to state-space models consisting of a linear time-continuous complex recurrent neural network followed by an MLP. Theoretical results reveal that the approximation error of the neural oscillator for approximating the second-order dynamical systems scales polynomially with the reciprocals of the widths of two utilized MLPs, thus mitigating the curse of parametric complexity. The decay rates of two established approximation error bounds are validated through two numerical cases. These results provide a robust theoretical foundation for the effective application of the neural oscillator in science and engineering.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Operator-Theoretic Framework for Gradient-Free Federated Learning</title>
<link>https://arxiv.org/abs/2512.01025</link>
<guid>https://arxiv.org/abs/2512.01025</guid>
<content:encoded><![CDATA[
arXiv:2512.01025v1 Announce Type: new 
Abstract: Federated learning must address heterogeneity, strict communication and computation limits, and privacy while ensuring performance. We propose an operator-theoretic framework that maps the $L^2$-optimal solution into a reproducing kernel Hilbert space (RKHS) via a forward operator, approximates it using available data, and maps back with the inverse operator, yielding a gradient-free scheme. Finite-sample bounds are derived using concentration inequalities over operator norms, and the framework identifies a data-dependent hypothesis space with guarantees on risk, error, robustness, and approximation. Within this space we design efficient kernel machines leveraging the space folding property of Kernel Affine Hull Machines. Clients transfer knowledge via a scalar space folding measure, reducing communication and enabling a simple differentially private protocol: summaries are computed from noise-perturbed data matrices in one step, avoiding per-round clipping and privacy accounting. The induced global rule requires only integer minimum and equality-comparison operations per test point, making it compatible with fully homomorphic encryption (FHE). Across four benchmarks, the gradient-free FL method with fixed encoder embeddings matches or outperforms strong gradient-based fine-tuning, with gains up to 23.7 points. In differentially private experiments, kernel smoothing mitigates accuracy loss in high-privacy regimes. The global rule admits an FHE realization using $Q \times C$ encrypted minimum and $C$ equality-comparison operations per test point, with operation-level benchmarks showing practical latencies. Overall, the framework provides provable guarantees with low communication, supports private knowledge transfer via scalar summaries, and yields an FHE-compatible prediction rule offering a mathematically grounded alternative to gradient-based federated learning under heterogeneity.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Associative Syntax and Maximal Repetitions reveal context-dependent complexity in fruit bat communication</title>
<link>https://arxiv.org/abs/2512.01033</link>
<guid>https://arxiv.org/abs/2512.01033</guid>
<content:encoded><![CDATA[
arXiv:2512.01033v1 Announce Type: new 
Abstract: This study presents an unsupervised method to infer discreteness, syntax and temporal structures of fruit-bats vocalizations, as a case study of graded vocal systems, and evaluates the complexity of communication patterns in relation with behavioral context. The method improved the baseline for unsupervised labeling of vocal units (i.e. syllables) through manifold learning, by investigating how dimen- sionality reduction on mel-spectrograms affects labeling, and comparing it with unsupervised labels based on acoustic similarity. We then encoded vocalizations as syllabic sequences to analyze the type of syntax, and extracted the Maximal Repetitions (MRs) to evaluate syntactical structures. We found evidence for: i) associative syntax, rather than combinatorial (context classification is unaffected by permutation of sequences, F 1 > 0.9); ii) context-dependent use of syllables (Wilcoxon rank-sum tests, p-value < 0.05); iii) heavy-tail distribution of MRs (truncated power-law, exponent {\alpha} < 2), indicative of mechanism encoding com- binatorial complexity. Analysis of MRs and syllabic transition networks revealed that mother-pupil interactions were characterized by repetitions, while commu- nication in conflict-contexts exhibited higher complexity (longer MRs and more interconnected vocal sequences) than non-agonistic contexts. We propose that communicative complexity is higher in scenarios of disagreement, reflecting lower compressibility of information.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>AltNet: Addressing the Plasticity-Stability Dilemma in Reinforcement Learning</title>
<link>https://arxiv.org/abs/2512.01034</link>
<guid>https://arxiv.org/abs/2512.01034</guid>
<content:encoded><![CDATA[
arXiv:2512.01034v1 Announce Type: new 
Abstract: Neural networks have shown remarkable success in supervised learning when trained on a single task using a fixed dataset. However, when neural networks are trained on a reinforcement learning task, their ability to continue learning from new experiences declines over time. This decline in learning ability is known as plasticity loss. To restore plasticity, prior work has explored periodically resetting the parameters of the learning network, a strategy that often improves overall performance. However, such resets come at the cost of a temporary drop in performance, which can be dangerous in real-world settings. To overcome this instability, we introduce AltNet, a reset-based approach that restores plasticity without performance degradation by leveraging twin networks. The use of twin networks anchors performance during resets through a mechanism that allows networks to periodically alternate roles: one network learns as it acts in the environment, while the other learns off-policy from the active network's interactions and a replay buffer. At fixed intervals, the active network is reset and the passive network, having learned from prior experiences, becomes the new active network. AltNet restores plasticity, improving sample efficiency and achieving higher performance, while avoiding performance drops that pose risks in safety-critical settings. We demonstrate these advantages in several high-dimensional control tasks from the DeepMind Control Suite, where AltNet outperforms various relevant baseline methods, as well as state-of-the-art reset-based techniques.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>FMTK: A Modular Toolkit for Composable Time Series Foundation Model Pipelines</title>
<link>https://arxiv.org/abs/2512.01038</link>
<guid>https://arxiv.org/abs/2512.01038</guid>
<content:encoded><![CDATA[
arXiv:2512.01038v1 Announce Type: new 
Abstract: Foundation models (FMs) have opened new avenues for machine learning applications due to their ability to adapt to new and unseen tasks with minimal or no further training. Time-series foundation models (TSFMs) -- FMs trained on time-series data -- have shown strong performance on classification, regression, and imputation tasks. Recent pipelines combine TSFMs with task-specific encoders, decoders, and adapters to improve performance; however, assembling such pipelines typically requires ad hoc, model-specific implementations that hinder modularity and reproducibility. We introduce FMTK, an open-source, lightweight and extensible toolkit for constructing and fine-tuning TSFM pipelines via standardized backbone and component abstractions. FMTK enables flexible composition across models and tasks, achieving correctness and performance with an average of seven lines of code. https://github.com/umassos/FMTK
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Adaptive-lambda Subtracted Importance Sampled Scores in Machine Unlearning for DDPMs and VAEs</title>
<link>https://arxiv.org/abs/2512.01054</link>
<guid>https://arxiv.org/abs/2512.01054</guid>
<content:encoded><![CDATA[
arXiv:2512.01054v1 Announce Type: new 
Abstract: Machine Unlearning is essential for large generative models (VAEs, DDPMs) to comply with the right to be forgotten and prevent undesired content generation without costly retraining. Existing approaches, such as Static-lambda SISS for diffusion models, rely on a fixed mixing weight lambda, which is suboptimal because the required unlearning strength varies across samples and training stages.
  We propose Adaptive-lambda SISS, a principled extension that turns lambda into a latent variable dynamically inferred at each training step. A lightweight inference network parameterizes an adaptive posterior over lambda, conditioned on contextual features derived from the instantaneous SISS loss terms (retain/forget losses and their gradients). This enables joint optimization of the diffusion model and the lambda-inference mechanism via a variational objective, yielding significantly better trade-offs.
  We further extend the adaptive-lambda principle to score-based unlearning and introduce a multi-class variant of Score Forgetting Distillation. In addition, we present two new directions: (i) a hybrid objective combining the data-free efficiency of Score Forgetting Distillation with the direct gradient control of SISS, and (ii) a Reinforcement Learning formulation that treats unlearning as a sequential decision process, learning an optimal policy over a state space defined by the model's current memory of the forget set.
  Experiments on an augmented MNIST benchmark show that Adaptive-lambda SISS substantially outperforms the original static-lambda SISS, achieving stronger removal of forgotten classes while better preserving generation quality on the retain set.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>PIANO: Physics-informed Dual Neural Operator for Precipitation Nowcasting</title>
<link>https://arxiv.org/abs/2512.01062</link>
<guid>https://arxiv.org/abs/2512.01062</guid>
<content:encoded><![CDATA[
arXiv:2512.01062v1 Announce Type: new 
Abstract: Precipitation nowcasting, key for early warning of disasters, currently relies on computationally expensive and restrictive methods that limit access to many countries. To overcome this challenge, we propose precipitation nowcasting using satellite imagery with physics constraints for improved accuracy and physical consistency. We use a novel physics-informed dual neural operator (PIANO) structure to enforce the fundamental equation of advection-diffusion during training to predict satellite imagery using a PINN loss. Then, we use a generative model to convert satellite images to radar images, which are used for precipitation nowcasting. Compared to baseline models, our proposed model shows a notable improvement in moderate (4mm/h) precipitation event prediction alongside short-term heavy (8mm/h) precipitation event prediction. It also demonstrates low seasonal variability in predictions, indicating robustness for generalization. This study suggests the potential of the PIANO and serves as a good baseline for physics-informed precipitation nowcasting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Bayesian dynamic scheduling of multipurpose batch processes under incomplete look-ahead information</title>
<link>https://arxiv.org/abs/2512.01093</link>
<guid>https://arxiv.org/abs/2512.01093</guid>
<content:encoded><![CDATA[
arXiv:2512.01093v1 Announce Type: new 
Abstract: Multipurpose batch processes become increasingly popular in manufacturing industries since they adapt to low-volume, high-value products and shifting demands. These processes often operate in a dynamic environment, which faces disturbances such as processing delays and demand changes. To minimise long-term cost and system nervousness (i.e., disruptive changes to schedules), schedulers must design rescheduling strategies to address such disturbances effectively. Existing methods often assume complete look-ahead information over the scheduling horizon. This assumption contrasts with realistic situations where schedulers can only access incomplete look-ahead information. Sticking with existing methods may lead to suboptimal long-term costs and high-level system nervousness. In this work we propose a Bayesian dynamic scheduling method. Our method relies on learning a Bayesian Network from the probability distribution of disturbances. Specifically, the Bayesian Network represents how likely each operation will be impacted by disturbances. During the online execution, when new disturbances become observed, this method updates the posterior distribution and therefore guides the rescheduling strategy. We compare our method with the existing periodic rescheduling strategy (which generates new schedules from scratch at fixed intervals) on four benchmark problems. Computational results show that our method achieves statistically better long-term costs and system nervousness. In the theoretical aspect, we prove that if disturbances are mutually independent, the impact-quantifying variables inherently satisfy the independence assumptions required by Bayesian Networks. As an implication, practitioners can extend the method to other scheduling problems (such as job shop scheduling and continuous processes), given that they define the problem-specific dependencies between operations.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2512.01113</link>
<guid>https://arxiv.org/abs/2512.01113</guid>
<content:encoded><![CDATA[
arXiv:2512.01113v1 Announce Type: new 
Abstract: Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.
  We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.
  We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\times$ reduction in runtime.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>World Model Robustness via Surprise Recognition</title>
<link>https://arxiv.org/abs/2512.01119</link>
<guid>https://arxiv.org/abs/2512.01119</guid>
<content:encoded><![CDATA[
arXiv:2512.01119v1 Announce Type: new 
Abstract: AI systems deployed in the real world must contend with distractions and out-of-distribution (OOD) noise that can destabilize their policies and lead to unsafe behavior. While robust training can reduce sensitivity to some forms of noise, it is infeasible to anticipate all possible OOD conditions. To mitigate this issue, we develop an algorithm that leverages a world model's inherent measure of surprise to reduce the impact of noise in world model--based reinforcement learning agents. We introduce both multi-representation and single-representation rejection sampling, enabling robustness to settings with multiple faulty sensors or a single faulty sensor. While the introduction of noise typically degrades agent performance, we show that our techniques preserve performance relative to baselines under varying types and levels of noise across multiple environments within self-driving simulation domains (CARLA and Safety Gymnasium). Furthermore, we demonstrate that our methods enhance the stability of two state-of-the-art world models with markedly different underlying architectures: Cosmos and DreamerV3. Together, these results highlight the robustness of our approach across world modeling domains. We release our code at https://github.com/Bluefin-Tuna/WISER .
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Mode-Conditioning Unlocks Superior Test-Time Scaling</title>
<link>https://arxiv.org/abs/2512.01127</link>
<guid>https://arxiv.org/abs/2512.01127</guid>
<content:encoded><![CDATA[
arXiv:2512.01127v1 Announce Type: new 
Abstract: Parallel sampling promises substantial gains in test-time scaling, but its effectiveness is sharply limited by diversity collapse, where models concentrate on a few modes and repeated samples produce the same mistakes. We propose the mode-conditioning (ModC) framework, which explicitly allocates test-time compute across reasoning modes using either specialist models or mode-specific prefixes. ModC consistently improves scaling across controlled graph-search tasks and large-scale reasoning benchmarks, spanning model families and sizes from 0.5B to 7B. On OpenThoughts, fine-tuning Qwen2.5-7B with ModC achieves a 4x efficiency gain over standard training while also improving the maximum attainable Pass@k. We further show that gradient clustering enables ModC without explicit mode labels, yielding up to 10% gains on datasets such as NuminaMath. Finally, we show that ModC improves reinforcement learning (RL) and can further boost diversity-inducing RL methods. These results demonstrate that standard training underutilizes the diversity in data, and that ModC provides a simple, effective remedy for unlocking the full benefits of diversity in test-time scaling.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Projection-Free CNN Pruning via Frank-Wolfe with Momentum: Sparser Models with Less Pretraining</title>
<link>https://arxiv.org/abs/2512.01147</link>
<guid>https://arxiv.org/abs/2512.01147</guid>
<content:encoded><![CDATA[
arXiv:2512.01147v1 Announce Type: new 
Abstract: We investigate algorithmic variants of the Frank-Wolfe (FW) optimization method for pruning convolutional neural networks. This is motivated by the "Lottery Ticket Hypothesis", which suggests the existence of smaller sub-networks within larger pre-trained networks that perform comparatively well (if not better). Whilst most literature in this area focuses on Deep Neural Networks more generally, we specifically consider Convolutional Neural Networks for image classification tasks. Building on the hypothesis, we compare simple magnitude-based pruning, a Frank-Wolfe style pruning scheme, and an FW method with momentum on a CNN trained on MNIST. Our experiments track test accuracy, loss, sparsity, and inference time as we vary the dense pre-training budget from 1 to 10 epochs. We find that FW with momentum yields pruned networks that are both sparser and more accurate than the original dense model and the simple pruning baselines, while incurring minimal inference-time overhead in our implementation. Moreover, FW with momentum reaches these accuracies after only a few epochs of pre-training, indicating that full pre-training of the dense model is not required in this setting.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Dynamic Algorithm for Explainable k-medians Clustering under lp Norm</title>
<link>https://arxiv.org/abs/2512.01150</link>
<guid>https://arxiv.org/abs/2512.01150</guid>
<content:encoded><![CDATA[
arXiv:2512.01150v1 Announce Type: new 
Abstract: We study the problem of explainable k-medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k clusters while minimizing the k-medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k-medians under lp norm for every finite p >= 1. Our algorithm achieves an O(p(log k)^{1 + 1/p - 1/p^2}) approximation to the optimal k-medians cost for any p >= 1. Previously, algorithms were known only for p = 1 and p = 2. For p = 2, our algorithm improves upon the existing bound of O(log^{3/2}k), and for p = 1, it matches the tight bound of log k + O(1) up to a multiplicative O(log log k) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O(d log^3 k) and O(log k) recourse, making it suitable for large-scale and evolving datasets.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Fiber Bundle Networks: A Geometric Machine Learning Paradigm</title>
<link>https://arxiv.org/abs/2512.01151</link>
<guid>https://arxiv.org/abs/2512.01151</guid>
<content:encoded><![CDATA[
arXiv:2512.01151v1 Announce Type: new 
Abstract: We propose Fiber Bundle Networks (FiberNet), a novel machine learning framework integrating differential geometry with machine learning. Unlike traditional deep neural networks relying on black-box function fitting, we reformulate classification as interpretable geometric optimization on fiber bundles, where categories form the base space and wavelet-transformed features lie in the fibers above each category. We introduce two innovations: (1) learnable Riemannian metrics identifying important frequency feature components, (2) variational prototype optimization through energy function minimization. Classification is performed via Voronoi tessellation under the learned Riemannian metric, where each prototype defines a decision region and test samples are assigned to the nearest prototype, providing clear geometric interpretability. This work demonstrates that the integration of fiber bundle with machine learning provides interpretability and efficiency, which are difficult to obtain simultaneously in conventional deep learning.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Open-Set Domain Adaptation Under Background Distribution Shift: Challenges and A Provably Efficient Solution</title>
<link>https://arxiv.org/abs/2512.01152</link>
<guid>https://arxiv.org/abs/2512.01152</guid>
<content:encoded><![CDATA[
arXiv:2512.01152v1 Announce Type: new 
Abstract: As we deploy machine learning systems in the real world, a core challenge is to maintain a model that is performant even as the data shifts. Such shifts can take many forms: new classes may emerge that were absent during training, a problem known as open-set recognition, and the distribution of known categories may change. Guarantees on open-set recognition are mostly derived under the assumption that the distribution of known classes, which we call \emph{the background distribution}, is fixed. In this paper we develop \ours{}, a method that is guaranteed to solve open-set recognition even in the challenging case where the background distribution shifts. We prove that the method works under benign assumptions that the novel class is separable from the non-novel classes, and provide theoretical guarantees that it outperforms a representative baseline in a simplified overparameterized setting. We develop techniques to make \ours{} scalable and robust, and perform comprehensive empirical evaluations on image and text data. The results show that \ours{} significantly outperforms existing open-set recognition methods under background shift. Moreover, we provide new insights into how factors such as the size of the novel class influences performance, an aspect that has not been extensively explored in prior work.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>From Regression to Classification: Exploring the Benefits of Categorical Representations of Energy in MLIPs</title>
<link>https://arxiv.org/abs/2512.01160</link>
<guid>https://arxiv.org/abs/2512.01160</guid>
<content:encoded><![CDATA[
arXiv:2512.01160v1 Announce Type: new 
Abstract: Density Functional Theory (DFT) is a widely used computational method for estimating the energy and behavior of molecules. Machine Learning Interatomic Potentials (MLIPs) are models trained to approximate DFT-level energies and forces at dramatically lower computational cost. Many modern MLIPs rely on a scalar regression formulation; given information about a molecule, they predict a single energy value and corresponding forces while minimizing absolute error with DFT's calculations. In this work, we explore a multi-class classification formulation that predicts a categorical distribution over energy/force values, providing richer supervision through multiple targets. Most importantly, this approach offers a principled way to quantify model uncertainty.
  In particular, our method predicts a histogram of the energy/force distribution, converts scalar targets into histograms, and trains the model using cross-entropy loss. Our results demonstrate that this categorical formulation can achieve absolute error performance comparable to regression baselines. Furthermore, this representation enables the quantification of epistemic uncertainty through the entropy of the predicted distribution, offering a measure of model confidence absent in scalar regression approaches.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>2D-ThermAl: Physics-Informed Framework for Thermal Analysis of Circuits using Generative AI</title>
<link>https://arxiv.org/abs/2512.01163</link>
<guid>https://arxiv.org/abs/2512.01163</guid>
<content:encoded><![CDATA[
arXiv:2512.01163v1 Announce Type: new 
Abstract: Thermal analysis is increasingly critical in modern integrated circuits, where non-uniform power dissipation and high transistor densities can cause rapid temperature spikes and reliability concerns. Traditional methods, such as FEM-based simulations offer high accuracy but computationally prohibitive for early-stage design, often requiring multiple iterative redesign cycles to resolve late-stage thermal failures. To address these challenges, we propose 'ThermAl', a physics-informed generative AI framework which effectively identifies heat sources and estimates full-chip transient and steady-state thermal distributions directly from input activity profiles. ThermAl employs a hybrid U-Net architecture enhanced with positional encoding and a Boltzmann regularizer to maintain physical fidelity. Our model is trained on an extensive dataset of heat dissipation maps, ranging from simple logic gates (e.g., inverters, NAND, XOR) to complex designs, generated via COMSOL. Experimental results demonstrate that ThermAl delivers precise temperature mappings for large circuits, with a root mean squared error (RMSE) of only 0.71{\deg}C, and outperforms conventional FEM tools by running up to ~200 times faster. We analyze performance across diverse layouts and workloads, and discuss its applicability to large-scale EDA workflows. While thermal reliability assessments often extend beyond 85{\deg}C for post-layout signoff, our focus here is on early-stage hotspot detection and thermal pattern learning. To ensure generalization beyond the nominal operating range 25-55{\deg}C, we additionally performed cross-validation on an extended dataset spanning 25-95{\deg}C maintaining a high accuracy (<2.2% full-scale RMSE) even under elevated temperature conditions representative of peak power and stress scenarios.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>A TinyML Reinforcement Learning Approach for Energy-Efficient Light Control in Low-Cost Greenhouse Systems</title>
<link>https://arxiv.org/abs/2512.01167</link>
<guid>https://arxiv.org/abs/2512.01167</guid>
<content:encoded><![CDATA[
arXiv:2512.01167v1 Announce Type: new 
Abstract: This study presents a reinforcement learning (RL)-based control strategy for adaptive lighting regulation in controlled environments using a low-power microcontroller. A model-free Q-learning algorithm was implemented to dynamically adjust the brightness of a Light-Emitting Diode (LED) based on real-time feedback from a light-dependent resistor (LDR) sensor. The system was trained to stabilize at 13 distinct light intensity levels (L1 to L13), with each target corresponding to a specific range within the 64-state space derived from LDR readings. A total of 130 trials were conducted, covering all target levels with 10 episodes each. Performance was evaluated in terms of convergence speed, steps taken, and time required to reach target states. Box plots and histograms were generated to analyze the distribution of training time and learning efficiency across targets. Experimental validation demonstrated that the agent could effectively learn to stabilize at varying light levels with minimal overshooting and smooth convergence, even in the presence of environmental perturbations. This work highlights the feasibility of lightweight, on-device RL for energy-efficient lighting control and sets the groundwork for multi-modal environmental control applications in resource-constrained agricultural systems.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Data assimilation and discrepancy modeling with shallow recurrent decoders</title>
<link>https://arxiv.org/abs/2512.01170</link>
<guid>https://arxiv.org/abs/2512.01170</guid>
<content:encoded><![CDATA[
arXiv:2512.01170v1 Announce Type: new 
Abstract: The requirements of modern sensing are rapidly evolving, driven by increasing demands for data efficiency, real-time processing, and deployment under limited sensing coverage. Complex physical systems are often characterized through the integration of a limited number of point sensors in combination with scientific computations which approximate the dominant, full-state dynamics. Simulation models, however, inevitably neglect small-scale or hidden processes, are sensitive to perturbations, or oversimplify parameter correlations, leading to reconstructions that often diverge from the reality measured by sensors. This creates a critical need for data assimilation, the process of integrating observational data with predictive simulation models to produce coherent and accurate estimates of the full state of complex physical systems. We propose a machine learning framework for Data Assimilation with a SHallow REcurrent Decoder (DA-SHRED) which bridges the simulation-to-real (SIM2REAL) gap between computational modeling and experimental sensor data. For real-world physics systems modeling high-dimensional spatiotemporal fields, where the full state cannot be directly observed and must be inferred from sparse sensor measurements, we leverage the latent space learned from a reduced simulation model via SHRED, and update these latent variables using real sensor data to accurately reconstruct the full system state. Furthermore, our algorithm incorporates a sparse identification of nonlinear dynamics based regression model in the latent space to identify functionals corresponding to missing dynamics in the simulation model. We demonstrate that DA-SHRED successfully closes the SIM2REAL gap and additionally recovers missing dynamics in highly complex systems, demonstrating that the combination of efficient temporal encoding and physics-informed correction enables robust data assimilation.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>First On-Orbit Demonstration of a Geospatial Foundation Model</title>
<link>https://arxiv.org/abs/2512.01181</link>
<guid>https://arxiv.org/abs/2512.01181</guid>
<content:encoded><![CDATA[
arXiv:2512.01181v1 Announce Type: new 
Abstract: Geospatial foundation models (GeoFMs) promise broad generalisation capacity for Earth observation (EO) tasks, particularly under data-limited conditions. However, their large size poses a barrier to deployment on resource-constrained space hardware. To address this, we present compact variants of a Vision Transformer (ViT)-based GeoFM that preserve downstream task performance while enabling onboard execution. Evaluation across five downstream tasks and validation in two representative flight environments show that model compression and domain adaptation are critical to reducing size and resource demands while maintaining high performance under operational conditions. We further demonstrate reliable on-orbit inference with the IMAGIN-e payload aboard the International Space Station. These results establish a pathway from large GeoFMs to flight-ready, resource-efficient deployments, expanding the feasibility of onboard AI for EO missions.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
<item>
<title>Teaching by Failure: Counter-Example-Driven Curricula for Transformer Self-Improvement</title>
<link>https://arxiv.org/abs/2512.01187</link>
<guid>https://arxiv.org/abs/2512.01187</guid>
<content:encoded><![CDATA[
arXiv:2512.01187v1 Announce Type: new 
Abstract: Transformer models often exhibit brittle extrapolation, failing on inputs that are longer or structurally more complex than those seen during training. We introduce Counter-Example-Driven Curricula (CEDC), an automated framework that improves model robustness by iteratively focusing on its own failures. At each step, CEDC uses the current model to generate a diverse set of candidate problems, employs a fast, executable verifier to identify incorrect predictions (counter-examples), and then fine-tunes the model on a dataset enriched with these discovered failures. We evaluate CEDC on a suite of algorithmic and natural language tasks, including integer addition, sorting, Dyck-2 language recognition, and three text classification benchmarks. Compared to static training and standard curriculum learning baselines, CEDC achieves up to 30x greater length extrapolation, is 3.75x more computationally efficient than uniform data augmentation, and requires no manual difficulty heuristics. We provide a detailed analysis of the counter-examples, showing how the curriculum naturally adapts to target progressively more complex error modes. Our findings establish verifier-guided, failure-driven learning as a simple, powerful, and efficient paradigm for enhancing the generalization capabilities of Transformer models.
]]></content:encoded>
<pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
</item>
</channel>
</rss>