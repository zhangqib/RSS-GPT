<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom">
<channel>
<title>cs.LG updates on arXiv.org</title>
<link>http://rss.arxiv.org/rss/cs.LG</link>

<item>
<title>VTool-R1: VLMs Learn to Think with Images via Reinforcement Learning on Multimodal Tool Use</title>
<link>https://arxiv.org/abs/2505.19255</link>
<guid>https://arxiv.org/abs/2505.19255</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement Learning Finetuning, large language models, multimodal reasoning, Visual Sketchpad, VTool-R1 <br>
Summary: 
The article introduces VTool-R1, a framework that enhances the reasoning capabilities of large language models (LLMs) by incorporating visual reasoning steps. By interleaving text and visual steps during training, VTool-R1 enables LLMs to generate multimodal chains of thought. This approach integrates Python-based visual editing tools into the Reinforcement Learning Finetuning process, allowing LLMs to strategically utilize visual reasoning in their responses. The framework is trained using outcome-based rewards tied to task accuracy, enabling LLMs to "think with images" and improve reasoning performance. Experiments conducted on structured visual question answering tasks demonstrate that VTool-R1 significantly enhances the ability of LLMs to generate multimodal chains of thoughts by utilizing visual reasoning tools effectively. <div>
arXiv:2505.19255v2 Announce Type: replace 
Abstract: Reinforcement Learning Finetuning (RFT) has significantly advanced the reasoning capabilities of large language models (LLMs) by enabling long chains of thought, self-correction, and effective tool use. While recent works attempt to extend RFT to vision-language models (VLMs), these efforts largely produce text-only reasoning conditioned on static image inputs, falling short of true multimodal reasoning in the response. In contrast, test-time methods like Visual Sketchpad incorporate visual steps but lack training mechanisms.
  We introduce VTool-R1, the first framework that trains VLMs to generate multimodal chains of thought by interleaving text and intermediate visual reasoning steps. VTool-R1 integrates Python-based visual editing tools into the RFT process, enabling VLMs to learn when and how to generate visual reasoning steps that benefit final reasoning. Trained with outcome-based rewards tied to task accuracy, our approach elicits strategic visual tool use for reasoning without relying on process-based supervision. Experiments on structured visual question answering over charts and tables show that VTool-R1 enhances reasoning performance by teaching VLMs to "think with images" and generate multimodal chain of thoughts with tools.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Large Reasoning Models for Agriculture</title>
<link>https://arxiv.org/abs/2505.19259</link>
<guid>https://arxiv.org/abs/2505.19259</guid>
<content:encoded><![CDATA[
<div> Keywords: Agricultural reasoning, Large reasoning models, AgReason, AgThoughts, AgThinker

Summary:<br><br>Agricultural decision-making is a complex process that relies on various factors such as geography, climate, and economics. Traditional large language models struggle to navigate this complexity, leading researchers to explore the use of large reasoning models. The AgReason project introduces an expert-curated benchmark with 100 questions for agricultural reasoning, showing that large reasoning models outperform conventional ones. The AgThoughts dataset, with 44.6K question-answer pairs, is presented to improve agricultural reasoning abilities in language models. The AgThinker suite of small reasoning models, designed for consumer-grade GPUs, demonstrates the effectiveness of the AgThoughts dataset in enhancing agricultural reasoning capabilities in language models. This research highlights the potential of using large reasoning models in agricultural decision-making processes. <br><br>Summary: <div>
arXiv:2505.19259v2 Announce Type: replace 
Abstract: Agricultural decision-making involves complex, context-specific reasoning, where choices about crops, practices, and interventions depend heavily on geographic, climatic, and economic conditions. Traditional large language models (LLMs) often fall short in navigating this nuanced problem due to limited reasoning capacity. We hypothesize that recent advances in large reasoning models (LRMs) can better handle such structured, domain-specific inference. To investigate this, we introduce AgReason, the first expert-curated open-ended science benchmark with 100 questions for agricultural reasoning. Evaluations across thirteen open-source and proprietary models reveal that LRMs outperform conventional ones, though notable challenges persist, with the strongest Gemini-based baseline achieving 36% accuracy. We also present AgThoughts, a large-scale dataset of 44.6K question-answer pairs generated with human oversight and equipped with synthetically generated reasoning traces. Using AgThoughts, we develop AgThinker, a suite of small reasoning models that can be run on consumer-grade GPUs, and show that our dataset can be effective in unlocking agricultural reasoning abilities in LLMs. Our project page is here: https://baskargroup.github.io/Ag_reasoning/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JEDI: Latent End-to-end Diffusion Mitigates Agent-Human Performance Asymmetry in Model-Based Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19698</link>
<guid>https://arxiv.org/abs/2505.19698</guid>
<content:encoded><![CDATA[
<div> Keywords: model-based reinforcement learning, diffusion world models, Atari100k benchmark, Joint Embedding Diffusion, human-level performance

Summary:
Recent advancements in model-based reinforcement learning have led to super-human performance on the Atari100k benchmark. However, there exists a performance asymmetry where MBRL agents excel in some tasks but underperform in others, skewing aggregate metrics. Pixel-based agents trained with diffusion world models exhibit this asymmetry more prominently. To address this issue, tasks are categorized as Agent-Optimal or Human-Optimal, highlighting the need for balanced metrics. The lack of temporally-structured latent space in pixel-based methods is identified as a contributor to the asymmetry. A new model, Joint Embedding Diffusion (JEDI), is proposed to tackle this problem. JEDI outperforms current models in human-optimal tasks, remains competitive in the Atari100k benchmark, and boasts improved efficiency in terms of speed and memory usage. This work challenges the traditional definition of crossing human-level performance in the Atari100k domain.<br><br>Summary: <div>
arXiv:2505.19698v2 Announce Type: replace 
Abstract: Recent advances in model-based reinforcement learning (MBRL) have achieved super-human level performance on the Atari100k benchmark, driven by reinforcement learning agents trained on powerful diffusion world models. However, we identify that the current aggregates mask a major performance asymmetry: MBRL agents dramatically outperform humans in some tasks despite drastically underperforming in others, with the former inflating the aggregate metrics. This is especially pronounced in pixel-based agents trained with diffusion world models. In this work, we address the pronounced asymmetry observed in pixel-based agents as an initial attempt to reverse the worrying upward trend observed in them. We address the problematic aggregates by delineating all tasks as Agent-Optimal or Human-Optimal and advocate for equal importance on metrics from both sets. Next, we hypothesize this pronounced asymmetry is due to the lack of temporally-structured latent space trained with the World Model objective in pixel-based methods. Lastly, to address this issue, we propose Joint Embedding DIffusion (JEDI), a novel latent diffusion world model trained end-to-end with the self-consistency objective. JEDI outperforms SOTA models in human-optimal tasks while staying competitive across the Atari100k benchmark, and runs 3 times faster with 43% lower memory than the latest pixel-based diffusion baseline. Overall, our work rethinks what it truly means to cross human-level performance in Atari100k.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Distill-Net: A Data Distillation Approach Tailored for Reply-based Continual Learning</title>
<link>https://arxiv.org/abs/2505.20135</link>
<guid>https://arxiv.org/abs/2505.20135</guid>
<content:encoded><![CDATA[
<div> Keywords: Replay-based continual learning, memory buffer, dataset distillation, global information, forgetting mitigation

Summary:
Replay-based continual learning methods often struggle with consolidating past knowledge due to limited memory buffer capacity and heuristic data selection. To address this challenge, a new dataset distillation framework for continual learning is proposed in this study. The framework employs a learnable memory buffer to distill global information from current task data and accumulated knowledge from the previous memory buffer. A lightweight distillation module is introduced to generate soft labels for memory buffer data, minimizing computational overhead and overfitting risks. Experimental results demonstrate that the proposed method achieves competitive performance and effectively prevents forgetting across various datasets. The source code for the method will be made publicly available. <br><br>Summary: <div>
arXiv:2505.20135v2 Announce Type: replace 
Abstract: Replay-based continual learning (CL) methods assume that models trained on a small subset can also effectively minimize the empirical risk of the complete dataset. These methods maintain a memory buffer that stores a sampled subset of data from previous tasks to consolidate past knowledge. However, this assumption is not guaranteed in practice due to the limited capacity of the memory buffer and the heuristic criteria used for buffer data selection. To address this issue, we propose a new dataset distillation framework tailored for CL, which maintains a learnable memory buffer to distill the global information from the current task data and accumulated knowledge preserved in the previous memory buffer. Moreover, to avoid the computational overhead and overfitting risks associated with parameterizing the entire buffer during distillation, we introduce a lightweight distillation module that can achieve global information distillation solely by generating learnable soft labels for the memory buffer data. Extensive experiments show that, our method can achieve competitive results and effectively mitigates forgetting across various datasets. The source code will be publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Real-World Assessment of Audio Watermarking Algorithms: Will They Survive Neural Codecs?</title>
<link>https://arxiv.org/abs/2505.19663</link>
<guid>https://arxiv.org/abs/2505.19663</guid>
<content:encoded><![CDATA[
<div> Benchmark, deep learning, audio watermarking, neural compression, audio attacks  
Summary:  
- The article introduces the Robust Audio Watermarking Benchmark (RAW-Bench) for evaluating deep learning-based audio watermarking methods with standardized comparisons.  
- A comprehensive audio attack pipeline is introduced to simulate real-world usage, including various distortions like compression, background noise, and reverberation.  
- Four existing watermarking methods were evaluated on RAW-bench, revealing challenges posed by neural compression techniques and the importance of training with audio attacks for improved robustness.  
- Specific distortions such as polarity inversion, time stretching, or reverb were found to significantly affect certain methods.  
- The evaluation framework is accessible on github.com/SonyResearch/raw_bench.  

<br><br>Summary: <div>
arXiv:2505.19663v2 Announce Type: replace-cross 
Abstract: We introduce the Robust Audio Watermarking Benchmark (RAW-Bench), a benchmark for evaluating deep learning-based audio watermarking methods with standardized and systematic comparisons. To simulate real-world usage, we introduce a comprehensive audio attack pipeline with various distortions such as compression, background noise, and reverberation, along with a diverse test dataset including speech, environmental sounds, and music recordings. Evaluating four existing watermarking methods on RAW-bench reveals two main insights: (i) neural compression techniques pose the most significant challenge, even when algorithms are trained with such compressions; and (ii) training with audio attacks generally improves robustness, although it is insufficient in some cases. Furthermore, we find that specific distortions, such as polarity inversion, time stretching, or reverb, seriously affect certain methods. The evaluation framework is accessible at github.com/SonyResearch/raw_bench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>

<item>
<title>The Role of Visualization in LLM-Assisted Knowledge Graph Systems: Effects on User Trust, Exploration, and Workflows</title>
<link>https://arxiv.org/abs/2505.21512</link>
<guid>https://arxiv.org/abs/2505.21512</guid>
<content:encoded><![CDATA[
<div> Keywords: Knowledge graphs, Large language models, KG exploration, User trust, Visualization

Summary:<br /><br />
The study explores the interaction between users and knowledge graphs (KGs) using large language models (LLMs) for exploration. The LinkQ system was developed to convert natural language questions into structured queries with LLMs. Visual mechanisms were designed to help users assess the accuracy of KG queries and LLM responses. Findings from a qualitative evaluation with practitioners revealed that users tended to overtrust the system's outputs, even when incorrect, due to the helpful visualizations. Users showed distinct workflows based on their familiarity with KGs and LLMs, challenging the assumption of one-size-fits-all systems. The study highlights the risks of false trust in LLM-assisted tools and emphasizes the need for further research on visualization as a mitigation strategy. <div>
arXiv:2505.21512v1 Announce Type: new 
Abstract: Knowledge graphs (KGs) are powerful data structures, but exploring them effectively remains difficult for even expert users. Large language models (LLMs) are increasingly used to address this gap, yet little is known empirically about how their usage with KGs shapes user trust, exploration strategies, or downstream decision-making - raising key design challenges for LLM-based KG visual analysis systems. To study these effects, we developed LinkQ, a KG exploration system that converts natural language questions into structured queries with an LLM. We collaborated with KG experts to design five visual mechanisms that help users assess the accuracy of both KG queries and LLM responses: an LLM-KG state diagram that illustrates which stage of the exploration pipeline LinkQ is in, a query editor displaying the generated query paired with an LLM explanation, an entity-relation ID table showing extracted KG entities and relations with semantic descriptions, a query structure graph that depicts the path traversed in the KG, and an interactive graph visualization of query results. From a qualitative evaluation with 14 practitioners, we found that users - even KG experts - tended to overtrust LinkQ's outputs due to its "helpful" visualizations, even when the LLM was incorrect. Users exhibited distinct workflows depending on their prior familiarity with KGs and LLMs, challenging the assumption that these systems are one-size-fits-all - despite often being designed as if they are. Our findings highlight the risks of false trust in LLM-assisted data analysis tools and the need for further investigation into the role of visualization as a mitigation technique.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SIMCOPILOT: Evaluating Large Language Models for Copilot-Style Code Generation</title>
<link>https://arxiv.org/abs/2505.21514</link>
<guid>https://arxiv.org/abs/2505.21514</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, coding assistants, benchmark, Java, Python

Summary:<br />
The article introduces SIMCOPILOT, a benchmark that simulates the role of large language models as interactive coding assistants, targeting completion and infill tasks in Java and Python. The benchmark provides a comprehensive framework for evaluating LLM coding capabilities in practical scenarios, addressing critical factors like task-specific performance nuances and contextual understanding. Evaluations across different domains highlight model strengths and challenges in maintaining logical consistency within complex code structures. The study emphasizes the limitations of LLM-driven code generation and the evolving role of LLMs as intelligent software development partners. Overall, the benchmark offers detailed insights into the utility and performance of large language models in coding environments.<br /> <div>
arXiv:2505.21514v1 Announce Type: new 
Abstract: We introduce SIMCOPILOT, a benchmark that simulates the role of large language models (LLMs) as interactive, "copilot"-style coding assistants. Targeting both completion (finishing incomplete methods or code blocks) and infill tasks (filling missing segments within existing code), SIMCOPILOT provides a comprehensive framework for evaluating LLM coding capabilities. The benchmark comprises dedicated sub-benchmarks for Java (SIMCOPILOTJ) and Python (SIMCOPILOTP), covering diverse codebases varying in size and complexity. Our key contributions include: (a) establishing a realistic, detailed evaluation environment to assess LLM utility in practical coding scenarios, and (b) providing fine-grained analyses that address critical factors frequently overlooked by existing benchmarks, such as task-specific performance nuances, contextual understanding across code segments, and sensitivity to variable scope. Evaluations conducted across domains-including algorithms, databases, computer vision, and neural networks-offer insights into model strengths and highlight persistent challenges in maintaining logical consistency within complex dependency structures. Beyond benchmarking, our study sheds light on the current limitations of LLM-driven code generation and underscores the ongoing transition of LLMs from merely syntax-aware generators toward reliable, intelligent software development partners.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Restoration and Spatial Rewiring for Source-Free Multivariate Time Series Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.21525</link>
<guid>https://arxiv.org/abs/2505.21525</guid>
<content:encoded><![CDATA[
<div> Spatial-temporal feature encoder, Multivariate time series, Domain adaptation, Data privacy, Temporal restoration<br />
<br />
Summary:
The paper introduces a new Source-Free Domain Adaptation (SFDA) method called Temporal Restoration and Spatial Rewiring (TERSE) designed specifically for multivariate time series (MTS) data. Traditional SFDA methods struggle with MTS due to a lack of consideration for spatial correlations inherent in the data. TERSE addresses this challenge by utilizing a customized spatial-temporal feature encoder and tasks for temporal restoration and spatial rewiring to capture and transfer spatial-temporal dependencies across domains. By guiding the target encoder to produce consistent features with the source domain, TERSE facilitates implicit feature alignment. It can also be easily integrated into existing SFDA methods as a plug-and-play module. Extensive experiments on real-world time series datasets demonstrate the effectiveness and versatility of TERSE in preserving data privacy and accurately adapting pre-trained models to unlabelled target domains. <div>
arXiv:2505.21525v1 Announce Type: new 
Abstract: Source-Free Domain Adaptation (SFDA) aims to adapt a pre-trained model from an annotated source domain to an unlabelled target domain without accessing the source data, thereby preserving data privacy. While existing SFDA methods have proven effective in reducing reliance on source data, they struggle to perform well on multivariate time series (MTS) due to their failure to consider the intrinsic spatial correlations inherent in MTS data. These spatial correlations are crucial for accurately representing MTS data and preserving invariant information across domains. To address this challenge, we propose Temporal Restoration and Spatial Rewiring (TERSE), a novel and concise SFDA method tailored for MTS data. Specifically, TERSE comprises a customized spatial-temporal feature encoder designed to capture the underlying spatial-temporal characteristics, coupled with both temporal restoration and spatial rewiring tasks to reinstate latent representations of the temporally masked time series and the spatially masked correlated structures. During the target adaptation phase, the target encoder is guided to produce spatially and temporally consistent features with the source domain by leveraging the source pre-trained temporal restoration and spatial rewiring networks. Therefore, TERSE can effectively model and transfer spatial-temporal dependencies across domains, facilitating implicit feature alignment. In addition, as the first approach to simultaneously consider spatial-temporal consistency in MTS-SFDA, TERSE can also be integrated as a versatile plug-and-play module into established SFDA methods. Extensive experiments on three real-world time series datasets demonstrate the effectiveness and versatility of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ChemHAS: Hierarchical Agent Stacking for Enhancing Chemistry Tools</title>
<link>https://arxiv.org/abs/2505.21569</link>
<guid>https://arxiv.org/abs/2505.21569</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Chemistry tasks, Agent-stacking, Prediction errors, AI agent applications

Summary: ChemHAS (Chemical Hierarchical Agent Stacking) is a new method proposed in this paper to enhance chemistry tools by optimizing agent-stacking structures using limited data. By leveraging Large Language Model-based agents, ChemHAS aims to reduce prediction errors of chemistry tools, leading to state-of-the-art performance in fundamental chemistry tasks. The method identifies four distinct agent-stacking behaviors, improving interpretability and opening up new possibilities for AI agent applications in scientific research. The code and dataset for ChemHAS are publicly available for further research and development.<br /><br />Summary: <div>
arXiv:2505.21569v1 Announce Type: new 
Abstract: Large Language Model (LLM)-based agents have demonstrated the ability to improve performance in chemistry-related tasks by selecting appropriate tools. However, their effectiveness remains limited by the inherent prediction errors of chemistry tools. In this paper, we take a step further by exploring how LLMbased agents can, in turn, be leveraged to reduce prediction errors of the tools. To this end, we propose ChemHAS (Chemical Hierarchical Agent Stacking), a simple yet effective method that enhances chemistry tools through optimizing agent-stacking structures from limited data. ChemHAS achieves state-of-the-art performance across four fundamental chemistry tasks, demonstrating that our method can effectively compensate for prediction errors of the tools. Furthermore, we identify and characterize four distinct agent-stacking behaviors, potentially improving interpretability and revealing new possibilities for AI agent applications in scientific research. Our code and dataset are publicly available at https: //anonymous.4open.science/r/ChemHAS-01E4/README.md.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FCOS: A Two-Stage Recoverable Model Pruning Framework for Automatic Modulation Recognition</title>
<link>https://arxiv.org/abs/2505.21571</link>
<guid>https://arxiv.org/abs/2505.21571</guid>
<content:encoded><![CDATA[
<div> Keywords: wireless communications, deep learning, Automatic Modulation Recognition, model pruning, channel-level pruning

Summary:<br /><br />
The paper introduces FCOS, a novel Fine-to-Coarse two-Stage pruning framework for Automatic Modulation Recognition (AMR) in wireless communications. FCOS combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression and high performance while maintaining accuracy. In the first stage, hierarchical clustering and parameter fusion are used for channel-level pruning, followed by a Layer Collapse Diagnosis (LaCD) module to identify and remove collapsed layers. Experimental results show that FCOS outperforms existing methods, achieving a 95.51% reduction in FLOPs and 95.31% reduction in parameters while only experiencing a 0.46% drop in accuracy on Sig2019-12 benchmark. The code is available on GitHub for further exploration.  <div>
arXiv:2505.21571v1 Announce Type: new 
Abstract: With the rapid development of wireless communications and the growing complexity of digital modulation schemes, traditional manual modulation recognition methods struggle to extract reliable signal features and meet real-time requirements in modern scenarios. Recently, deep learning based Automatic Modulation Recognition (AMR) approaches have greatly improved classification accuracy. However, their large model sizes and high computational demands hinder deployment on resource-constrained devices. Model pruning provides a general approach to reduce model complexity, but existing weight, channel, and layer pruning techniques each present a trade-off between compression rate, hardware acceleration, and accuracy preservation. To this end, in this paper, we introduce FCOS, a novel Fine-to-COarse two-Stage pruning framework that combines channel-level pruning with layer-level collapse diagnosis to achieve extreme compression, high performance and efficient inference. In the first stage of FCOS, hierarchical clustering and parameter fusion are applied to channel weights to achieve channel-level pruning. Then a Layer Collapse Diagnosis (LaCD) module uses linear probing to identify layer collapse and removes the collapsed layers due to high channel compression ratio. Experiments on multiple AMR benchmarks demonstrate that FCOS outperforms existing channel and layer pruning methods. Specifically, FCOS achieves 95.51% FLOPs reduction and 95.31% parameter reduction while still maintaining performance close to the original ResNet56, with only a 0.46% drop in accuracy on Sig2019-12. Code is available at https://github.com/yaolu-zjut/FCOS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral-inspired Neural Operator for Data-efficient PDE Simulation in Physics-agnostic Regimes</title>
<link>https://arxiv.org/abs/2505.21573</link>
<guid>https://arxiv.org/abs/2505.21573</guid>
<content:encoded><![CDATA[
<div> frequency domain, neural PDE solvers, physics-aware methods, spectral representations, Navier-Stokes equations <br />
Summary: 
The Spectral-inspired Neural Operator (SINO) framework is introduced for learning PDE operators from limited trajectories without known PDE terms. SINO operates in the frequency domain and utilizes a Frequency-to-Vector module for spectral representations. It incorporates a nonlinear operator block with a $\Pi$-Block for modeling nonlinear interactions and introduces operator distillation for efficient inference. SINO outperforms existing methods in PDE benchmarks, showing strong discretization invariance and generalization to out-of-distribution initial conditions. Notably, SINO can accurately simulate globally coupled systems, such as the Navier-Stokes equations, with limited data and no explicit PDE terms present. <div>
arXiv:2505.21573v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) govern the spatiotemporal evolution of various physical systems. Classical numerical solvers, while accurate, require fine discretization and full knowledge of the governing PDEs, limiting their applicability when the physics is unknown or fast inference is required. Data-driven neural PDE solvers alleviate these constraints by learning from data but demand large training datasets and perform poorly in data-scarce regimes. Physics-aware methods mitigate data requirements by incorporating physical knowledge yet rely on known PDE terms or local numerical schemes, restricting their ability to handle unknown or globally coupled systems. In this work, we propose the Spectral-inspired Neural Operator (SINO), a novel framework that learns PDE operators from limited trajectories (as few as 2-5), without any known PDE terms. SINO operates in the frequency domain and introduces a Frequency-to-Vector module to learn spectral representations analogous to derivative multipliers. To model nonlinear physical interactions, we design a nonlinear operator block that includes a $\Pi$-Block with low-pass filtering to prevent aliasing. Finally, we introduce an operator distillation technique to distill the trained model for efficient inference. SINO achieves state-of-the-art results across multiple PDE benchmarks, demonstrating strong discretization invariance and robust generalization to out-of-distribution initial conditions. To our knowledge, SINO is the first physics-aware method capable of accurately simulating globally coupled systems (e.g., the Navier-Stokes equations) from limited data without any explicit PDE terms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Concentration Distribution Learning from Label Distributions</title>
<link>https://arxiv.org/abs/2505.21576</link>
<guid>https://arxiv.org/abs/2505.21576</guid>
<content:encoded><![CDATA[
<div> Keywords: Label distribution learning, background concentration, concentration distribution learning, probabilistic methods, neural networks

Summary:<br /><br />The article introduces a new concept called background concentration to enhance label distribution learning (LDL) by incorporating the absolute intensity of each label. By considering the total description degree of hidden labels not in the label space, the proposed concentration distribution learning improves the representation of instances and minimizes information loss. A novel model combining probabilistic methods and neural networks is developed to learn both label distributions and background concentrations from existing LDL datasets. Experimental results demonstrate that the approach effectively extracts background concentrations and yields more accurate prediction results compared to existing LDL methods. The code for the proposed method is publicly available on GitHub for further research and implementation. <div>
arXiv:2505.21576v1 Announce Type: new 
Abstract: Label distribution learning (LDL) is an effective method to predict the relative label description degree (a.k.a. label distribution) of a sample. However, the label distribution is not a complete representation of an instance because it overlooks the absolute intensity of each label. Specifically, it's impossible to obtain the total description degree of hidden labels that not in the label space, which leads to the loss of information and confusion in instances. To solve the above problem, we come up with a new concept named background concentration to serve as the absolute description degree term of the label distribution and introduce it into the LDL process, forming the improved paradigm of concentration distribution learning. Moreover, we propose a novel model by probabilistic methods and neural networks to learn label distributions and background concentrations from existing LDL datasets. Extensive experiments prove that the proposed approach is able to extract background concentrations from label distributions while producing more accurate prediction results than the state-of-the-art LDL methods. The code is available in https://github.com/seutjw/CDL-LD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fairness in Federated Learning: Fairness for Whom?</title>
<link>https://arxiv.org/abs/2505.21584</link>
<guid>https://arxiv.org/abs/2505.21584</guid>
<content:encoded><![CDATA[
<div> Keywords: fairness, federated learning, sociotechnical contexts, harms, stakeholders 

Summary: 
This paper discusses the limitations of current approaches to fairness in federated learning, highlighting five key pitfalls. Firstly, fairness is often defined solely through the server-client architecture perspective. Secondly, there is a mismatch between simulated scenarios and real-world contexts. Thirdly, definitions often prioritize protecting the system over protecting its users. Fourthly, interventions target specific stages of the lifecycle, neglecting broader impacts. Lastly, there is a lack of alignment among multiple stakeholders with different fairness definitions. The paper proposes a harm-centered framework to address these issues, connecting fairness definitions to concrete risks and stakeholder vulnerabilities. Recommendations are made for more holistic, context-aware, and accountable fairness research in federated learning. 

<br /><br />Summary: <div>
arXiv:2505.21584v1 Announce Type: new 
Abstract: Fairness in federated learning has emerged as a rapidly growing area of research, with numerous works proposing formal definitions and algorithmic interventions. Yet, despite this technical progress, fairness in FL is often defined and evaluated in ways that abstract away from the sociotechnical contexts in which these systems are deployed. In this paper, we argue that existing approaches tend to optimize narrow system level metrics, such as performance parity or contribution-based rewards, while overlooking how harms arise throughout the FL lifecycle and how they impact diverse stakeholders. We support this claim through a critical analysis of the literature, based on a systematic annotation of papers for their fairness definitions, design decisions, evaluation practices, and motivating use cases. Our analysis reveals five recurring pitfalls: 1) fairness framed solely through the lens of server client architecture, 2) a mismatch between simulations and motivating use-cases and contexts, 3) definitions that conflate protecting the system with protecting its users, 4) interventions that target isolated stages of the lifecycle while neglecting upstream and downstream effects, 5) and a lack of multi-stakeholder alignment where multiple fairness definitions can be relevant at once. Building on these insights, we propose a harm centered framework that links fairness definitions to concrete risks and stakeholder vulnerabilities. We conclude with recommendations for more holistic, context-aware, and accountable fairness research in FL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CellCLAT: Preserving Topology and Trimming Redundancy in Self-Supervised Cellular Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.21587</link>
<guid>https://arxiv.org/abs/2505.21587</guid>
<content:encoded><![CDATA[
<div> Cellular Complex Contrastive Learning, Adaptive Trimming, self-supervised, topological deep learning, cellular complexes <br />
<br />
Summary: 
Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT) is a novel framework for self-supervised topological deep learning on cellular complexes. It addresses challenges such as extrinsic structural constraints and intrinsic semantic redundancy in cellular representations. CellCLAT utilizes parameter perturbation-based augmentation to preserve cellular topology during contrastive learning while masking task-irrelevant cells through a cellular trimming scheduler. This approach enhances the representation of unlabeled graphs by capturing higher-order interactions in cellular complexes more effectively. The framework demonstrates significant improvements over existing self-supervised graph learning methods, making a valuable contribution to the field of topological deep learning. <div>
arXiv:2505.21587v1 Announce Type: new 
Abstract: Self-supervised topological deep learning (TDL) represents a nascent but underexplored area with significant potential for modeling higher-order interactions in simplicial complexes and cellular complexes to derive representations of unlabeled graphs. Compared to simplicial complexes, cellular complexes exhibit greater expressive power. However, the advancement in self-supervised learning for cellular TDL is largely hindered by two core challenges: \textit{extrinsic structural constraints} inherent to cellular complexes, and intrinsic semantic redundancy in cellular representations. The first challenge highlights that traditional graph augmentation techniques may compromise the integrity of higher-order cellular interactions, while the second underscores that topological redundancy in cellular complexes potentially diminish task-relevant information. To address these issues, we introduce Cellular Complex Contrastive Learning with Adaptive Trimming (CellCLAT), a twofold framework designed to adhere to the combinatorial constraints of cellular complexes while mitigating informational redundancy. Specifically, we propose a parameter perturbation-based augmentation method that injects controlled noise into cellular interactions without altering the underlying cellular structures, thereby preserving cellular topology during contrastive learning. Additionally, a cellular trimming scheduler is employed to mask gradient contributions from task-irrelevant cells through a bi-level meta-learning approach, effectively removing redundant topological elements while maintaining critical higher-order semantics. We provide theoretical justification and empirical validation to demonstrate that CellCLAT achieves substantial improvements over existing self-supervised graph learning methods, marking a significant attempt in this domain.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pioneering 4-Bit FP Quantization for Diffusion Models: Mixup-Sign Quantization and Timestep-Aware Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.21591</link>
<guid>https://arxiv.org/abs/2505.21591</guid>
<content:encoded><![CDATA[
<div> Keywords: model quantization, diffusion models, floating-point quantization, fine-tuning, denoising

Summary:
Model quantization aims to reduce the bit-width of weights and activations in diffusion models for improved memory efficiency and faster inference speed. However, achieving 4-bit quantization has been challenging. Existing methods based on integer quantization and post-training quantization fine-tuning have shown inconsistent performance. This study explores low-bit floating-point (FP) quantization for diffusion models and identifies key challenges such as handling asymmetric activation distributions and considering temporal complexity during the denoising process. To address these challenges, the mixup-sign floating-point quantization (MSFP) framework is proposed, which introduces unsigned FP quantization and incorporates timestep-aware techniques and denoising-factor loss alignment for precise and stable fine-tuning. Experimental results demonstrate superior performance in 4-bit FP quantization for diffusion models compared to existing methods in 4-bit integer quantization. 

<br /><br />Summary: <div>
arXiv:2505.21591v1 Announce Type: new 
Abstract: Model quantization reduces the bit-width of weights and activations, improving memory efficiency and inference speed in diffusion models. However, achieving 4-bit quantization remains challenging. Existing methods, primarily based on integer quantization and post-training quantization fine-tuning, struggle with inconsistent performance. Inspired by the success of floating-point (FP) quantization in large language models, we explore low-bit FP quantization for diffusion models and identify key challenges: the failure of signed FP quantization to handle asymmetric activation distributions, the insufficient consideration of temporal complexity in the denoising process during fine-tuning, and the misalignment between fine-tuning loss and quantization error. To address these challenges, we propose the mixup-sign floating-point quantization (MSFP) framework, first introducing unsigned FP quantization in model quantization, along with timestep-aware LoRA (TALoRA) and denoising-factor loss alignment (DFA), which ensure precise and stable fine-tuning. Extensive experiments show that we are the first to achieve superior performance in 4-bit FP quantization for diffusion models, outperforming existing PTQ fine-tuning methods in 4-bit INT quantization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Relevance-driven Input Dropout: an Explanation-guided Regularization Technique</title>
<link>https://arxiv.org/abs/2505.21595</link>
<guid>https://arxiv.org/abs/2505.21595</guid>
<content:encoded><![CDATA[
<div> Keywords: overfitting, machine learning, data augmentation, occlusion, generalization 

Summary: 
The article addresses the issue of overfitting in machine learning models, proposing a novel data augmentation technique called Relevance-driven Input Dropout (RelDrop). Unlike existing methods that randomly mask input features, RelDrop selectively occludes the most relevant regions of the input to encourage the model to utilize other important features during training. Through qualitative and quantitative analyses, the study demonstrates that RelDrop improves model robustness towards occlusion, encourages the use of more features within the region of interest, and enhances generalization performance at inference time. Experimental results on benchmark datasets highlight the effectiveness of the proposed approach in mitigating overfitting and reducing the train-test performance gap. The code implementation for RelDrop is available on GitHub for further exploration and application. 

<br /><br />Summary: <div>
arXiv:2505.21595v1 Announce Type: new 
Abstract: Overfitting is a well-known issue extending even to state-of-the-art (SOTA) Machine Learning (ML) models, resulting in reduced generalization, and a significant train-test performance gap. Mitigation measures include a combination of dropout, data augmentation, weight decay, and other regularization techniques. Among the various data augmentation strategies, occlusion is a prominent technique that typically focuses on randomly masking regions of the input during training. Most of the existing literature emphasizes randomness in selecting and modifying the input features instead of regions that strongly influence model decisions. We propose Relevance-driven Input Dropout (RelDrop), a novel data augmentation method which selectively occludes the most relevant regions of the input, nudging the model to use other important features in the prediction process, thus improving model generalization through informed regularization. We further conduct qualitative and quantitative analyses to study how Relevance-driven Input Dropout (RelDrop) affects model decision-making. Through a series of experiments on benchmark datasets, we demonstrate that our approach improves robustness towards occlusion, results in models utilizing more features within the region of interest, and boosts inference time generalization performance. Our code is available at https://github.com/Shreyas-Gururaj/LRP_Relevance_Dropout.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOSBENCH: Benchmarking Safety Alignment on Scientific Knowledge</title>
<link>https://arxiv.org/abs/2505.21605</link>
<guid>https://arxiv.org/abs/2505.21605</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, safety benchmark, hazardous scenarios, SOSBench, model alignment deficiencies

Summary:
The article introduces SOSBench, a new benchmark designed to evaluate the safety and resilience of large language models (LLMs) when handling high-risk scientific scenarios in domains such as chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark consists of 3,000 prompts derived from real-world regulations and laws, including detailed and complex misuse scenarios. The evaluation of state-of-the-art models using SOSBench reveals significant deficiencies in model alignment with safety regulations, with high rates of harmful responses detected across all domains. Advanced models like Deepseek-R1 and GPT-4.1 exhibit alarming levels of policy-violating content, raising urgent concerns about the responsible deployment of powerful LLMs. SOSBench highlights the need for improved safety benchmarks and increased attention to the risks associated with the misuse of advanced language models. 

<br /><br />Summary: <div>
arXiv:2505.21605v1 Announce Type: new 
Abstract: Large language models (LLMs) exhibit advancing capabilities in complex tasks, such as reasoning and graduate-level question answering, yet their resilience against misuse, particularly involving scientifically sophisticated risks, remains underexplored. Existing safety benchmarks typically focus either on instructions requiring minimal knowledge comprehension (e.g., ``tell me how to build a bomb") or utilize prompts that are relatively low-risk (e.g., multiple-choice or classification tasks about hazardous content). Consequently, they fail to adequately assess model safety when handling knowledge-intensive, hazardous scenarios.
  To address this critical gap, we introduce SOSBench, a regulation-grounded, hazard-focused benchmark encompassing six high-risk scientific domains: chemistry, biology, medicine, pharmacology, physics, and psychology. The benchmark comprises 3,000 prompts derived from real-world regulations and laws, systematically expanded via an LLM-assisted evolutionary pipeline that introduces diverse, realistic misuse scenarios (e.g., detailed explosive synthesis instructions involving advanced chemical formulas). We evaluate frontier models within a unified evaluation framework using our SOSBench. Despite their alignment claims, advanced models consistently disclose policy-violating content across all domains, demonstrating alarmingly high rates of harmful responses (e.g., 79.1% for Deepseek-R1 and 47.3% for GPT-4.1). These results highlight significant safety alignment deficiencies and underscore urgent concerns regarding the responsible deployment of powerful LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Where to Learn: Training Distribution Selection for Provable OOD Performance</title>
<link>https://arxiv.org/abs/2505.21626</link>
<guid>https://arxiv.org/abs/2505.21626</guid>
<content:encoded><![CDATA[
<div> distribution-aware training, out-of-distribution generalization, machine learning, optimization, probability measures<br />
Summary:<br />
- The study focuses on addressing the challenge of out-of-distribution (OOD) generalization in machine learning, where models often experience performance degradation on shifted or unseen domains. 
- A theoretical analysis establishes generalization bounds that quantify how the choice of training distribution affects OOD error. 
- Two algorithmic strategies are proposed: formulating OOD risk minimization as a bilevel optimization problem over probability measures and minimizing an upper bound on OOD error. 
- Evaluations across various examples show that the proposed methods significantly enhance OOD accuracy compared to standard empirical risk minimization. 
- Distribution-aware training emerges as a promising framework for robust OOD generalization in machine learning. <br /> 

Summary: <div>
arXiv:2505.21626v1 Announce Type: new 
Abstract: Out-of-distribution (OOD) generalization remains a fundamental challenge in machine learning. Models trained on one data distribution often experience substantial performance degradation when evaluated on shifted or unseen domains. To address this challenge, the present paper studies the design of training data distributions that maximize average-case OOD performance. First, a theoretical analysis establishes a family of generalization bounds that quantify how the choice of training distribution influences OOD error across a predefined family of target distributions. These insights motivate the introduction of two complementary algorithmic strategies: (i) directly formulating OOD risk minimization as a bilevel optimization problem over the space of probability measures and (ii) minimizing a theoretical upper bound on OOD error. Last, the paper evaluates the two approaches across a range of function approximation and operator learning examples. The proposed methods significantly improve OOD accuracy over standard empirical risk minimization with a fixed distribution. These results highlight the potential of distribution-aware training as a principled and practical framework for robust OOD generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Apprenticeship learning with prior beliefs using inverse optimization</title>
<link>https://arxiv.org/abs/2505.21639</link>
<guid>https://arxiv.org/abs/2505.21639</guid>
<content:encoded><![CDATA[
<div> framework, inverse reinforcement learning, inverse optimization, Markov decision processes, apprenticeship learning 
<br />
Summary: 
In this paper, the relationship between inverse reinforcement learning (IRL), inverse optimization (IO) for Markov decision processes (MDPs), and apprenticeship learning (AL) is investigated. The authors incorporate prior beliefs on the cost function structure into IRL and AL, showing that AL can be seen as a relaxation of their framework. By formulating AL as a regularized min-max problem, they address the ill-posedness of IRL using a regularizer to guide the search for plausible cost functions. Stochastic mirror descent (SMD) is employed to solve the resulting regularized-convex-concave-min-max problem, with convergence bounds established. Numerical experiments underscore the importance of regularization in learning cost vectors and apprentice policies. <div>
arXiv:2505.21639v1 Announce Type: new 
Abstract: The relationship between inverse reinforcement learning (IRL) and inverse optimization (IO) for Markov decision processes (MDPs) has been relatively underexplored in the literature, despite addressing the same problem. In this work, we revisit the relationship between the IO framework for MDPs, IRL, and apprenticeship learning (AL). We incorporate prior beliefs on the structure of the cost function into the IRL and AL problems, and demonstrate that the convex-analytic view of the AL formalism (Kamoutsi et al., 2021) emerges as a relaxation of our framework. Notably, the AL formalism is a special case in our framework when the regularization term is absent. Focusing on the suboptimal expert setting, we formulate the AL problem as a regularized min-max problem. The regularizer plays a key role in addressing the ill-posedness of IRL by guiding the search for plausible cost functions. To solve the resulting regularized-convex-concave-min-max problem, we use stochastic mirror descent (SMD) and establish convergence bounds for the proposed method. Numerical experiments highlight the critical role of regularization in learning cost vectors and apprentice policies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Diffusion Models for Symmetric Manifolds</title>
<link>https://arxiv.org/abs/2505.21640</link>
<guid>https://arxiv.org/abs/2505.21640</guid>
<content:encoded><![CDATA[
<div> framework, efficient diffusion models, symmetric-space Riemannian manifolds, spatially-varying covariance, training algorithm 

Summary:<br /><br />Researchers introduce a framework for designing efficient diffusion models on symmetric-space Riemannian manifolds like the torus, sphere, special orthogonal group, and unitary group. They develop a new diffusion model with a spatially-varying covariance, allowing them to bypass heat kernel computations by leveraging a projection of Euclidean Brownian motion. The training algorithm minimizes a novel efficient objective derived through Ito's Lemma, running in O(1) gradient evaluations and nearly-linear-in-d (O(d^1.19)) arithmetic operations per step. The model's manifold symmetries ensure an "average-case" Lipschitz condition, facilitating accurate and efficient sample generation. Empirical results demonstrate that their model outperforms previous methods in training speed and enhances sample quality on synthetic datasets across various symmetric manifolds. <div>
arXiv:2505.21640v1 Announce Type: new 
Abstract: We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PrivATE: Differentially Private Confidence Intervals for Average Treatment Effects</title>
<link>https://arxiv.org/abs/2505.21641</link>
<guid>https://arxiv.org/abs/2505.21641</guid>
<content:encoded><![CDATA[
<div> framework, CIs, ATE, treatment effects, differential privacy  
Summary:  
- The article introduces PrivATE, a machine learning framework for computing Confidence Intervals (CIs) for the Average Treatment Effect (ATE) under differential privacy.  
- Privacy is crucial in medical data analysis, and PrivATE allows valid uncertainty quantification while keeping sensitive data private.  
- PrivATE consists of three steps: estimating a differentially private ATE, estimating a differentially private variance, and constructing CIs that account for uncertainty from both steps.  
- The framework is model agnostic, doubly robust, and ensures valid CIs for the ATE.  
- The authors demonstrate the effectiveness of PrivATE using synthetic and real-world medical datasets, showing it is the first general, doubly robust framework for valid CIs of ATE under differential privacy.<br /><br />Summary: <div>
arXiv:2505.21641v1 Announce Type: new 
Abstract: The average treatment effect (ATE) is widely used to evaluate the effectiveness of drugs and other medical interventions. In safety-critical applications like medicine, reliable inferences about the ATE typically require valid uncertainty quantification, such as through confidence intervals (CIs). However, estimating treatment effects in these settings often involves sensitive data that must be kept private. In this work, we present PrivATE, a novel machine learning framework for computing CIs for the ATE under differential privacy. Specifically, we focus on deriving valid privacy-preserving CIs for the ATE from observational data. Our PrivATE framework consists of three steps: (i) estimating a differentially private ATE through output perturbation; (ii) estimating the differentially private variance through a truncated output perturbation mechanism; and (iii) constructing the CIs while accounting for the uncertainty from both the estimation and privatization steps. Our PrivATE framework is model agnostic, doubly robust, and ensures valid CIs. We demonstrate the effectiveness of our framework using synthetic and real-world medical datasets. To the best of our knowledge, we are the first to derive a general, doubly robust framework for valid CIs of the ATE under ($\varepsilon$, $\delta$)-differential privacy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoSGD: Automatic Learning Rate Selection for Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2505.21651</link>
<guid>https://arxiv.org/abs/2505.21651</guid>
<content:encoded><![CDATA[
<div> Keywords: learning rate, stochastic gradient descent, AutoSGD, convergence, optimization 

Summary: 
AutoSGD is a novel stochastic gradient descent (SGD) method that automatically adjusts the learning rate at each iteration, eliminating the need for manual tuning. It determines whether to increase or decrease the learning rate based on the current iteration's performance. The method is supported by theoretical convergence guarantees and has a deterministic counterpart for standard gradient descent. Empirical results demonstrate the effectiveness of AutoSGD across various optimization problems and machine learning tasks. This approach addresses the challenge of selecting an optimal learning rate schedule and shows promising performance in practice. <div>
arXiv:2505.21651v1 Announce Type: new 
Abstract: The learning rate is an important tuning parameter for stochastic gradient descent (SGD) and can greatly influence its performance. However, appropriate selection of a learning rate schedule across all iterations typically requires a non-trivial amount of user tuning effort. To address this, we introduce AutoSGD: an SGD method that automatically determines whether to increase or decrease the learning rate at a given iteration and then takes appropriate action. We introduce theory supporting the convergence of AutoSGD, along with its deterministic counterpart for standard gradient descent. Empirical results suggest strong performance of the method on a variety of traditional optimization problems and machine learning tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PreGenie: An Agentic Framework for High-quality Visual Presentation Generation</title>
<link>https://arxiv.org/abs/2505.21660</link>
<guid>https://arxiv.org/abs/2505.21660</guid>
<content:encoded><![CDATA[
<div> framework, multimodal input, visual presentations, automated creation, deep learning
Summary:
PreGenie is a new framework that aims to improve the automation of creating visual presentations using multimodal large language models (MLLMs). It addresses issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding that have hindered previous attempts at automation. PreGenie operates in two stages: Analysis and Initial Generation, and Review and Re-generation, leveraging multiple MLLMs to collaborate and share information. Through comprehensive experiments, PreGenie has demonstrated superior performance in multimodal understanding, aesthetics, and content consistency compared to existing models. It also aligns closely with human design preferences, making it suitable for use in formal contexts like business and scientific research. <br /><br />Summary: <div>
arXiv:2505.21660v1 Announce Type: new 
Abstract: Visual presentations are vital for effective communication. Early attempts to automate their creation using deep learning often faced issues such as poorly organized layouts, inaccurate text summarization, and a lack of image understanding, leading to mismatched visuals and text. These limitations restrict their application in formal contexts like business and scientific research. To address these challenges, we propose PreGenie, an agentic and modular framework powered by multimodal large language models (MLLMs) for generating high-quality visual presentations.
  PreGenie is built on the Slidev presentation framework, where slides are rendered from Markdown code. It operates in two stages: (1) Analysis and Initial Generation, which summarizes multimodal input and generates initial code, and (2) Review and Re-generation, which iteratively reviews intermediate code and rendered slides to produce final, high-quality presentations. Each stage leverages multiple MLLMs that collaborate and share information. Comprehensive experiments demonstrate that PreGenie excels in multimodal understanding, outperforming existing models in both aesthetics and content consistency, while aligning more closely with human design preferences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Controllable Diffusion via Optimal Classifier Guidance</title>
<link>https://arxiv.org/abs/2505.21666</link>
<guid>https://arxiv.org/abs/2505.21666</guid>
<content:encoded><![CDATA[
<div> diffusion models, controllable generation, reinforcement learning, supervised learning, classification<br />
<br />
Summary: 
The article introduces a method called Supervised Learning based Controllable Diffusion (SLCD) for the controllable generation of diffusion models. SLCD aims to optimize a KL-regularized objective function by iteratively generating online data and training a small classifier. It does not rely on complex concepts from reinforcement learning or control, making it more resource-efficient. The output from SLCD is proven to converge to the optimal solution of the objective under KL divergence. Empirical results show that SLCD can generate high-quality samples with minimal inference time, comparable to the base model, in tasks such as image generation and biological sequence generation. The code for SLCD is available on GitHub for further exploration and implementation. <br /> <div>
arXiv:2505.21666v1 Announce Type: new 
Abstract: The controllable generation of diffusion models aims to steer the model to generate samples that optimize some given objective functions. It is desirable for a variety of applications including image generation, molecule generation, and DNA/sequence generation. Reinforcement Learning (RL) based fine-tuning of the base model is a popular approach but it can overfit the reward function while requiring significant resources. We frame controllable generation as a problem of finding a distribution that optimizes a KL-regularized objective function. We present SLCD -- Supervised Learning based Controllable Diffusion, which iteratively generates online data and trains a small classifier to guide the generation of the diffusion model. Similar to the standard classifier-guided diffusion, SLCD's key computation primitive is classification and does not involve any complex concepts from RL or control. Via a reduction to no-regret online learning analysis, we show that under KL divergence, the output from SLCD provably converges to the optimal solution of the KL-regularized objective. Further, we empirically demonstrate that SLCD can generate high quality samples with nearly the same inference time as the base model in both image generation with continuous diffusion and biological sequence generation with discrete diffusion. Our code is available at https://github.com/Owen-Oertell/slcd
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What happens when generative AI models train recursively on each others' generated outputs?</title>
<link>https://arxiv.org/abs/2505.21677</link>
<guid>https://arxiv.org/abs/2505.21677</guid>
<content:encoded><![CDATA[
<div> internet, AI-generated content, genAI models, training data, data-mediated interactions 

Summary:
Data-mediated interactions between generative AI (genAI) models and training data from other models on the internet have potential consequences. This study explores the effects of such interactions, showing that models trained on others' outputs can benefit from exposure to novel concepts but may also result in homogenization of performance on shared tasks. The empirical evidence and theoretical model provided shed light on how these interactions might unfold in practice, highlighting the importance of understanding the downstream effects of data-mediated model interactions in an increasingly genAI-dependent society. <div>
arXiv:2505.21677v1 Announce Type: new 
Abstract: The internet is full of AI-generated content while also serving as a common source of training data for generative AI (genAI) models. This duality raises the possibility that future genAI models may be trained on other models' generated outputs. Prior work has studied consequences of models training on their own generated outputs, but limited work has considered what happens if models ingest content produced by other models. Given society's increasing dependence on genAI tools, understanding downstream effects of such data-mediated model interactions is critical. To this end, we provide empirical evidence for how data-mediated interactions might unfold in practice, develop a theoretical model for this interactive training process, and show experimentally possible long-term results of such interactions. We find that data-mediated interactions can benefit models by exposing them to novel concepts perhaps missed in original training data, but also can homogenize their performance on shared tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>multivariateGPT: a decoder-only transformer for multivariate categorical and numeric data</title>
<link>https://arxiv.org/abs/2505.21680</link>
<guid>https://arxiv.org/abs/2505.21680</guid>
<content:encoded><![CDATA[
<div> Keywords: multivariateGPT, mixed data, autoregressive sequence decomposition, likelihood estimation, transformer models <br />
<br />
Summary: <br />
The article introduces multivariateGPT, an architecture designed to model sequences of mixed categorical and numeric data. It addresses limitations in existing approaches by incorporating autoregressive sequence decomposition, embedding schemes, and a loss function that enables likelihood estimation of the joint distribution of next token class and value. By extending the next token prediction task, multivariateGPT efficiently generalizes patterns in simple physical systems and complex time series like electrocardiograms and multivariate electronic health record data. This advancement allows transformer models to effectively handle a wider range of data types, showcasing their versatility and potential for enhancing data analysis in real-world applications. <div>
arXiv:2505.21680v1 Announce Type: new 
Abstract: Real-world processes often generate data that are a mix of categorical and numeric values that are recorded at irregular and informative intervals. Discrete token-based approaches are limited in numeric representation capacity while methods like neural ordinary differential equations are not well suited for categorical data or informative sampling and require augmentation to handle certain classes of trajectories. Here, we present multivariateGPT, a single architecture for modeling sequences of mixed categorical (including tokenized text) and numeric data. This is accomplished with an autoregressive sequence decomposition, embedding scheme, and loss function that extend the next token prediction task to likelihood estimation of the joint distribution of next token class and value. We demonstrate how this approach can efficiently learn to generalize patterns in simple physical systems and model complex time series including electrocardiograms and multivariate electronic health record data. This work extends the utility of transformer based models to additional classes of data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incentivizing Permissionless Distributed Learning of LLMs</title>
<link>https://arxiv.org/abs/2505.21684</link>
<guid>https://arxiv.org/abs/2505.21684</guid>
<content:encoded><![CDATA[
<div> incentive system, distributed deep learning, Gauntlet, bittensor blockchain, pseudo-gradients

Summary:
The article introduces an incentive system called Gauntlet for distributed deep learning, implemented on the bittensor blockchain. It allows for the training of a 1.2B LLM model with permissionless contributions of pseudo-gradients from users with varying hardware. Gauntlet can be applied to synchronous distributed training schemes. The system utilizes a two-stage mechanism to filter peer reliability and synchronization, along with a core component for estimating loss before and after contributions. An OpenSkill rating system tracks the competitiveness of pseudo-gradient scores. A novel mechanism ensures unique computations by peers. The 1.2B run on the system has paid out real-valued tokens to participants based on their contributions, resulting in a competitive model. <div>
arXiv:2505.21684v1 Announce Type: new 
Abstract: We describe an incentive system for distributed deep learning of foundational models where peers are rewarded for contributions. The incentive system, \textit{Gauntlet}, has been deployed on the bittensor blockchain and used to train a 1.2B LLM with completely permissionless contributions of pseudo-gradients: no control over the users that can register or their hardware. \textit{Gauntlet} can be applied to any synchronous distributed training scheme that relies on aggregating updates or pseudo-gradients. We rely on a two-stage mechanism for fast filtering of peer uptime, reliability, and synchronization, combined with the core component that estimates the loss before and after individual pseudo-gradient contributions. We utilized an OpenSkill rating system to track competitiveness of pseudo-gradient scores across time. Finally, we introduce a novel mechanism to ensure peers on the network perform unique computations. Our live 1.2B run, which has paid out real-valued tokens to participants based on the value of their contributions, yielded a competitive (on a per-iteration basis) 1.2B model that demonstrates the utility of our incentive system.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AMSFL: Adaptive Multi-Step Federated Learning via Gradient Difference-Based Error Modeling</title>
<link>https://arxiv.org/abs/2505.21695</link>
<guid>https://arxiv.org/abs/2505.21695</guid>
<content:encoded><![CDATA[
<div> Keywords: Federated learning, communication efficiency, model accuracy, Gradient Difference Approximation, Adaptive Multi-Step Federated Learning framework

Summary: 
The paper introduces a new method called Gradient Difference Approximation (GDA) to address the challenges of balancing communication efficiency and model accuracy in federated learning. GDA utilizes first-order information to estimate local error trends without the need to compute the full Hessian matrix, resulting in a lightweight yet effective solution. This method is a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework, offering a unified error modeling strategy for large-scale multi-step adaptive training environments. By leveraging GDA within the AMSFL framework, the proposed approach aims to improve the efficiency and accuracy of federated learning algorithms. <div>
arXiv:2505.21695v1 Announce Type: new 
Abstract: Federated learning faces critical challenges in balancing communication efficiency and model accuracy. One key issue lies in the approximation of update errors without incurring high computational costs. In this paper, we propose a lightweight yet effective method called Gradient Difference Approximation (GDA), which leverages first-order information to estimate local error trends without computing the full Hessian matrix. The proposed method forms a key component of the Adaptive Multi-Step Federated Learning (AMSFL) framework and provides a unified error modeling strategy for large-scale multi-step adaptive training environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Up Liquid-Resistance Liquid-Capacitance Networks for Efficient Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.21717</link>
<guid>https://arxiv.org/abs/2505.21717</guid>
<content:encoded><![CDATA[
<div> LrcSSM, nonlinear recurrent model, long sequences, fast processing, parallel solution, formal gradient-stability guarantee <br />
<br />
Summary: LrcSSM is a nonlinear recurrent model that efficiently processes long sequences with fast parallel computation. By utilizing a diagonal state-transition matrix learned at each step, it achieves $\mathcal{O}(TD)$ time and memory complexity and low $\mathcal{O}(\log T)$ sequential depth. The model offers a gradient-stability guarantee and outperforms other input-varying systems on long-range forecasting tasks. With a compute-optimal scaling law regime similar to Mamba, LrcSSM demonstrates superior performance compared to quadratic-attention Transformers while avoiding memory overhead. The forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, making it computationally efficient, and its parameter count scales optimally with depth. Overall, LrcSSM showcases impressive capabilities in processing long sequences and outperforming established models on forecasting tasks. <br /><br />Summary: <div>
arXiv:2505.21717v1 Announce Type: new 
Abstract: We present LrcSSM, a \textit{nonlinear} recurrent model that processes long sequences as fast as today's linear state-space layers. By forcing the state-transition matrix to be diagonal and learned at every step, the full sequence can be solved in parallel with a single prefix-scan, giving $\mathcal{O}(TD)$ time and memory and only $\mathcal{O}(\log T)$ sequential depth, for input-sequence length $T$ and a state dimension $D$. Moreover, LrcSSM offers a formal gradient-stability guarantee that other input-varying systems such as Liquid-S4 and Mamba do not provide. Lastly, for network depth $L$, as the forward and backward passes cost $\Theta(T\,D\,L)$ FLOPs, with its low sequential depth and parameter count $\Theta(D\,L)$, the model follows the compute-optimal scaling law regime ($\beta \approx 0.42$) recently observed for Mamba, outperforming quadratic-attention Transformers at equal compute while avoiding the memory overhead of FFT-based long convolutions. We show that on a series of long-range forecasting tasks, LrcSSM outperforms LRU, S5 and Mamba.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Saddle-To-Saddle Dynamics in Deep ReLU Networks: Low-Rank Bias in the First Saddle Escape</title>
<link>https://arxiv.org/abs/2505.21722</link>
<guid>https://arxiv.org/abs/2505.21722</guid>
<content:encoded><![CDATA[
<div> escape directions, ReLU networks, saddle points, low-rank bias, optimization 

Summary: 
The study investigates the behavior of gradient descent (GD) in deep ReLU networks when initialized with small weights. It focuses on the escape directions, analogous to the eigenvectors of the Hessian for strict saddles, and their importance in navigating the parameter space dominated by saddles. The optimal escape direction is characterized by a low-rank bias in deeper layers, with the first singular value of the weight matrix increasing at least as the square root of the layer depth. The research presents results related to these escape directions and suggests that this finding is a crucial step towards understanding Saddle-to-Saddle dynamics in deep ReLU networks, where GD encounters a sequence of saddles with increasing bottleneck rank. <div>
arXiv:2505.21722v1 Announce Type: new 
Abstract: When a deep ReLU network is initialized with small weights, GD is at first dominated by the saddle at the origin in parameter space. We study the so-called escape directions, which play a similar role as the eigenvectors of the Hessian for strict saddles. We show that the optimal escape direction features a low-rank bias in its deeper layers: the first singular value of the $\ell$-th layer weight matrix is at least $\ell^{\frac{1}{4}}$ larger than any other singular value. We also prove a number of related results about these escape directions. We argue that this result is a first step in proving Saddle-to-Saddle dynamics in deep ReLU networks, where GD visits a sequence of saddles with increasing bottleneck rank.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning Agents are not even close to Human Intelligence</title>
<link>https://arxiv.org/abs/2505.21731</link>
<guid>https://arxiv.org/abs/2505.21731</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, HackAtari, task variations, performance drops, generalization testing
<br />
Summary: 
The article introduces HackAtari, a set of task variations in the Arcade Learning Environments, to evaluate the zero-shot adaptation capabilities of deep RL agents. It highlights that RL agents struggle to maintain performance on simpler versions of training tasks, unlike humans. The study reveals a consistent reliance on shortcuts by RL agents, leading to significant performance drops on simplified tasks. The analysis across various algorithms and architectures emphasizes the gap between RL agents and human intelligence. The research underscores the importance of new benchmarks and methodologies for systematic generalization testing beyond standard evaluation protocols. It suggests that training and testing in the same environment are insufficient for developing RL agents with human-like intelligence. 
<br /> <div>
arXiv:2505.21731v1 Announce Type: new 
Abstract: Deep reinforcement learning (RL) agents achieve impressive results in a wide variety of tasks, but they lack zero-shot adaptation capabilities. While most robustness evaluations focus on tasks complexifications, for which human also struggle to maintain performances, no evaluation has been performed on tasks simplifications. To tackle this issue, we introduce HackAtari, a set of task variations of the Arcade Learning Environments. We use it to demonstrate that, contrary to humans, RL agents systematically exhibit huge performance drops on simpler versions of their training tasks, uncovering agents' consistent reliance on shortcuts. Our analysis across multiple algorithms and architectures highlights the persistent gap between RL agents and human behavioral intelligence, underscoring the need for new benchmarks and methodologies that enforce systematic generalization testing beyond static evaluation protocols. Training and testing in the same environment is not enough to obtain agents equipped with human-like intelligence.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaX: Boosting Low-Rank Training of Foundation Models via Latent Crossing</title>
<link>https://arxiv.org/abs/2505.21732</link>
<guid>https://arxiv.org/abs/2505.21732</guid>
<content:encoded><![CDATA[
<div> Latent Crossing, Low-rank Factorization, ViTs, LLMs, Pre-training <br />
Summary: 
The research introduces Latent Crossing (LaX), a module enhancing low-rank models' capacity by enabling information flow across subspaces, overcoming limitations of traditional low-rank factorization methods. LaX significantly improves performance on pre-training tasks with ViT-Base/Large and LLaMA-like models, matching or surpassing full-rank baselines while using 2-3 times fewer parameters. When combined with low-rank adapters (LoRA) for fine-tuning larger models like LLaMA-7/13B, LaX consistently enhances performance on arithmetic and common sense reasoning tasks at minimal cost. The research showcases the effectiveness of LaX as a simple yet powerful plug-and-play solution for boosting the performance of low-rank models in various high-computational-cost training scenarios. <br /> <div>
arXiv:2505.21732v1 Announce Type: new 
Abstract: Training foundation models such as ViTs and LLMs requires tremendous computing cost. Low-rank matrix or tensor factorization offers a parameter-efficient alternative, but often downgrades performance due to the restricted parameter space. In this work, we introduce {\textbf{Latent Crossing (LaX)}} -- a simple yet effective plug-and-play module that enhances the capacity of low-rank models by enabling information flow across low-rank subspaces. We extensively validate the benefits of LaX on pre-training tasks with ViT-Base/Large and LLaMA-like models ranging from 60M to 1B parameters. LaX boosts low-rank model performance to match or exceed the full-rank baselines while using 2-3\(\times\) fewer parameters. When equipped with low-rank adapters (i.e., LoRA) for fine-tuning LLaMA-7/13B, LaX consistently improves performance on arithmetic and common sense reasoning tasks with negligible cost.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simulating the Unseen: Crash Prediction Must Learn from What Did Not Happen</title>
<link>https://arxiv.org/abs/2505.21743</link>
<guid>https://arxiv.org/abs/2505.21743</guid>
<content:encoded><![CDATA[
<div> learning, counterfactual safety, near-miss events, crash prediction, Vision Zero<br />
Summary:<br />
Traffic safety science faces challenges due to limited crash data. To achieve Vision Zero, a paradigm shift towards counterfactual safety learning is needed. This involves considering potential dangerous scenarios that could have occurred. The proposed agenda combines crash-rate priors, generative scene engines, and diverse driver models to synthesize and explain near-miss events. A crash-focused digital twin testbed connects micro scenes to macro patterns, ensuring statistical realism. This transformation of sparse crash data enables proactive prevention of accidents through crash prediction and stress-testing of vehicles, roads, and policies. By learning from almost-crashes, traffic safety can shift from reactive forensics to proactive prevention, ultimately advancing Vision Zero. <br /><br />Summary: <div>
arXiv:2505.21743v1 Announce Type: new 
Abstract: Traffic safety science has long been hindered by a fundamental data paradox: the crashes we most wish to prevent are precisely those events we rarely observe. Existing crash-frequency models and surrogate safety metrics rely heavily on sparse, noisy, and under-reported records, while even sophisticated, high-fidelity simulations undersample the long-tailed situations that trigger catastrophic outcomes such as fatalities. We argue that the path to achieving Vision Zero, i.e., the complete elimination of traffic fatalities and severe injuries, requires a paradigm shift from traditional crash-only learning to a new form of counterfactual safety learning: reasoning not only about what happened, but also about the vast set of plausible yet perilous scenarios that could have happened under slightly different circumstances. To operationalize this shift, our proposed agenda bridges macro to micro. Guided by crash-rate priors, generative scene engines, diverse driver models, and causal learning, near-miss events are synthesized and explained. A crash-focused digital twin testbed links micro scenes to macro patterns, while a multi-objective validator ensures that simulations maintain statistical realism. This pipeline transforms sparse crash data into rich signals for crash prediction, enabling the stress-testing of vehicles, roads, and policies before deployment. By learning from crashes that almost happened, we can shift traffic safety from reactive forensics to proactive prevention, advancing Vision Zero.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bi-Linear State Transitions in Recurrent Neural Networks</title>
<link>https://arxiv.org/abs/2505.21749</link>
<guid>https://arxiv.org/abs/2505.21749</guid>
<content:encoded><![CDATA[
<div> hidden units, recurrent neural networks, bi-linear operations, state tracking tasks, inductive bias<br />
<br />
Summary: In this new research, the role of hidden units in recurrent neural networks is examined from a different perspective. Instead of just being memory stores, hidden units are considered as active participants in network computation. The study focuses on bi-linear operations, which involve interactions between hidden units and input embeddings. The research shows that these operations serve as a natural inductive bias for representing the evolution of hidden states in state tracking tasks. It is demonstrated both theoretically and empirically that bi-linear state updates correspond to a hierarchy of increasing complexity in state tracking tasks, with popular linear recurrent networks like Mamba at the lowest level of complexity. This study sheds new light on the importance of hidden units in network behavior and provides insights into their active role in computations. <br /><br /> <div>
arXiv:2505.21749v1 Announce Type: new 
Abstract: The role of hidden units in recurrent neural networks is typically seen as modeling memory, with research focusing on enhancing information retention through gating mechanisms. A less explored perspective views hidden units as active participants in the computation performed by the network, rather than passive memory stores. In this work, we revisit bi-linear operations, which involve multiplicative interactions between hidden units and input embeddings. We demonstrate theoretically and empirically that they constitute a natural inductive bias for representing the evolution of hidden states in state tracking tasks. These are the simplest type of task that require hidden units to actively contribute to the behavior of the network. We also show that bi-linear state updates form a natural hierarchy corresponding to state tracking tasks of increasing complexity, with popular linear recurrent networks such as Mamba residing at the lowest-complexity center of that hierarchy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hierarchical Reinforcement Learning with Uncertainty-Guided Diffusional Subgoals</title>
<link>https://arxiv.org/abs/2505.21750</link>
<guid>https://arxiv.org/abs/2505.21750</guid>
<content:encoded><![CDATA[
<div> Hierarchical reinforcement learning, Diffusion model, Gaussian Process, subgoals, sample efficiency <br />
Summary: <br />
Hierarchical reinforcement learning (HRL) tackles the challenge of making decisions at multiple levels of temporal abstraction. The dynamic nature of low-level policies poses a difficulty for high-level policies to generate effective subgoals. This study introduces a novel approach that combines a conditional diffusion model with a Gaussian Process (GP) prior to generate a diverse set of subgoals while considering uncertainty. By incorporating GP uncertainty quantification, the proposed method surpasses existing HRL techniques in terms of both sample efficiency and performance on continuous control benchmarks. By selecting subgoals from the diffusion policy and GP's predictive mean, this strategy improves decision-making capabilities in complex environments. <div>
arXiv:2505.21750v1 Announce Type: new 
Abstract: Hierarchical reinforcement learning (HRL) learns to make decisions on multiple levels of temporal abstraction. A key challenge in HRL is that the low-level policy changes over time, making it difficult for the high-level policy to generate effective subgoals. To address this issue, the high-level policy must capture a complex subgoal distribution while also accounting for uncertainty in its estimates. We propose an approach that trains a conditional diffusion model regularized by a Gaussian Process (GP) prior to generate a complex variety of subgoals while leveraging principled GP uncertainty quantification. Building on this framework, we develop a strategy that selects subgoals from both the diffusion policy and GP's predictive mean. Our approach outperforms prior HRL methods in both sample efficiency and performance on challenging continuous control benchmarks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DualSchool: How Reliable are LLMs for Optimization Education?</title>
<link>https://arxiv.org/abs/2505.21775</link>
<guid>https://arxiv.org/abs/2505.21775</guid>
<content:encoded><![CDATA[
<div> conversion, linear program, optimization, LLMs, DualSchool

Summary: 
DualSchool introduces a framework for assessing the performance of large language models (LLMs) in generating duals of linear programs. The framework uses a verification procedure based on Canonical Graph Edit Distance, providing more accurate evaluation compared to existing methods. Experiments show that while LLMs can accurately recite the conversion procedure, they struggle to consistently produce correct duals, even for small instances. This finding has implications for educators, highlighting the limitations of relying solely on LLMs for optimization tasks. It also raises questions about the development of large reasoning systems and the need for further research in the intersection of AI and operations research. <div>
arXiv:2505.21775v1 Announce Type: new 
Abstract: Consider the following task taught in introductory optimization courses which addresses challenges articulated by the community at the intersection of (generative) AI and OR: generate the dual of a linear program. LLMs, being trained at web-scale, have the conversion process and many instances of Primal to Dual Conversion (P2DC) at their disposal. Students may thus reasonably expect that LLMs would perform well on the P2DC task. To assess this expectation, this paper introduces DualSchool, a comprehensive framework for generating and verifying P2DC instances. The verification procedure of DualSchool uses the Canonical Graph Edit Distance, going well beyond existing evaluation methods for optimization models, which exhibit many false positives and negatives when applied to P2DC. Experiments performed by DualSchool reveal interesting findings. Although LLMs can recite the conversion procedure accurately, state-of-the-art open LLMs fail to consistently produce correct duals. This finding holds even for the smallest two-variable instances and for derivative tasks, such as correctness, verification, and error classification. The paper also discusses the implications for educators, students, and the development of large reasoning systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memorization to Generalization: Emergence of Diffusion Models from Associative Memory</title>
<link>https://arxiv.org/abs/2505.21777</link>
<guid>https://arxiv.org/abs/2505.21777</guid>
<content:encoded><![CDATA[
<div> Hopfield networks, associative memory, diffusion models, spurious states, training data<br />
Summary:<br />
This study explores the connection between diffusion models and associative memories (AMs), specifically focusing on the phenomenon of spurious states that occur in Hopfield networks when training data reaches its critical memory load. The training phase of the diffusion model is compared to memory encoding in AMs, while the generation phase represents memory retrieval. In the small data regime, the diffusion model behaves similarly to the Hopfield model below the critical memory load, creating distinct basins of attraction around each sample. In the large data regime, new attractor states are formed corresponding to manifolds of generated samples, with spurious states emerging at the transition boundary. These spurious states exhibit distinct basins of attraction despite not being in the training set. The study offers a new perspective on memorization-generalization in diffusion models, predicts the existence of spurious states, and provides empirical validation of these predictions in commonly-used diffusion models. <br /> <div>
arXiv:2505.21777v1 Announce Type: new 
Abstract: Hopfield networks are associative memory (AM) systems, designed for storing and retrieving patterns as local minima of an energy landscape. In the classical Hopfield model, an interesting phenomenon occurs when the amount of training data reaches its critical memory load $- spurious\,\,states$, or unintended stable points, emerge at the end of the retrieval dynamics, leading to incorrect recall. In this work, we examine diffusion models, commonly used in generative modeling, from the perspective of AMs. The training phase of diffusion model is conceptualized as memory encoding (training data is stored in the memory). The generation phase is viewed as an attempt of memory retrieval. In the small data regime the diffusion model exhibits a strong memorization phase, where the network creates distinct basins of attraction around each sample in the training set, akin to the Hopfield model below the critical memory load. In the large data regime, a different phase appears where an increase in the size of the training set fosters the creation of new attractor states that correspond to manifolds of the generated samples. Spurious states appear at the boundary of this transition and correspond to emergent attractor states, which are absent in the training set, but, at the same time, have distinct basins of attraction around them. Our findings provide: a novel perspective on the memorization-generalization phenomenon in diffusion models via the lens of AMs, theoretical prediction of existence of spurious states, empirical validation of this prediction in commonly-used diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>P-DROP: Poisson-Based Dropout for Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.21783</link>
<guid>https://arxiv.org/abs/2505.21783</guid>
<content:encoded><![CDATA[
<div> Node selection strategy, Poisson processes, Graph Neural Networks, over-smoothing, structure-aware updates

Summary:
The article addresses the issue of over-smoothing in Graph Neural Networks (GNNs) by proposing a novel node selection strategy based on Poisson processes. This strategy introduces stochastic and structure-aware updates for nodes, preventing node representations from converging too quickly and losing discriminative power. By equipping each node with an independent Poisson clock, the approach allows for asynchronous and localized updates that preserve structural diversity. The study explores two applications of the Poisson-based method: as a replacement for traditional dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that the proposed method outperforms traditional approaches like Dropout, DropEdge, and DropNode, especially in later training stages. This showcases the effectiveness and competitiveness of the Poisson-based strategy in improving GNN performance. 

<br /><br />Summary: <div>
arXiv:2505.21783v1 Announce Type: new 
Abstract: Over-smoothing remains a major challenge in Graph Neural Networks (GNNs), where repeated message passing causes node representations to converge and lose discriminative power. To address this, we propose a novel node selection strategy based on Poisson processes, introducing stochastic but structure-aware updates. Specifically, we equip each node with an independent Poisson clock, enabling asynchronous and localized updates that preserve structural diversity. We explore two applications of this strategy: as a replacement for dropout-based regularization and as a dynamic subgraph training scheme. Experimental results on standard benchmarks (Cora, Citeseer, Pubmed) demonstrate that our Poisson-based method yields competitive or improved accuracy compared to traditional Dropout, DropEdge, and DropNode approaches, particularly in later training stages.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Born a Transformer -- Always a Transformer?</title>
<link>https://arxiv.org/abs/2505.21785</link>
<guid>https://arxiv.org/abs/2505.21785</guid>
<content:encoded><![CDATA[
<div> limitations, sequence-to-sequence tasks, Transformers, pretraining data, length generalization

Summary:
Pretraining of large-scale Language Models (LLMs) such as Transformers may not fully overcome theoretical limitations in modeling certain sequence-to-sequence tasks. A study of retrieval and copying tasks revealed an induction-versus-anti-induction asymmetry in pretrained models, with better performance in retrieving tokens to the right rather than the left of a query token. This asymmetry can be eliminated through targeted fine-tuning if length generalization is guaranteed. Analysis showed that the strength of induction versus anti-induction circuits within pretrained Transformers contributes to this asymmetry. Practical experiments on real-world tasks confirmed these findings and highlighted the need to consider reliability risks in utilizing pretrained models. Pretraining selectively enhances specific capabilities of Transformers but does not eliminate fundamental length-generalization limits.<br /><br />Summary: <div>
arXiv:2505.21785v1 Announce Type: new 
Abstract: Transformers have theoretical limitations in modeling certain sequence-to-sequence tasks, yet it remains largely unclear if these limitations play a role in large-scale pretrained LLMs, or whether LLMs might effectively overcome these constraints in practice due to the scale of both the models themselves and their pretraining data. We explore how these architectural constraints manifest after pretraining, by studying a family of $\textit{retrieval}$ and $\textit{copying}$ tasks inspired by Liu et al. [2024]. We use the recently proposed C-RASP framework for studying length generalization [Huang et al., 2025b] to provide guarantees for each of our settings. Empirically, we observe an $\textit{induction-versus-anti-induction}$ asymmetry, where pretrained models are better at retrieving tokens to the right (induction) rather than the left (anti-induction) of a query token. This asymmetry disappears upon targeted fine-tuning if length-generalization is guaranteed by theory. Mechanistic analysis reveals that this asymmetry is connected to the differences in the strength of induction versus anti-induction circuits within pretrained Transformers. We validate our findings through practical experiments on real-world tasks demonstrating reliability risks. Our results highlight that pretraining selectively enhances certain Transformer capabilities, but does not overcome fundamental length-generalization limits.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faster Rates for Private Adversarial Bandits</title>
<link>https://arxiv.org/abs/2505.21790</link>
<guid>https://arxiv.org/abs/2505.21790</guid>
<content:encoded><![CDATA[
<div> bandits, differential privacy, regret upper bound, algorithms, adversarial 

Summary:<br />
- New differentially private algorithms are proposed for adversarial bandits and bandits with expert advice. 
- A simple and efficient conversion method is introduced for transforming non-private bandit algorithms into private ones in the adversarial bandits problem, improving regret upper bound to $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$. 
- The algorithms allow for sublinear expected regret even with small values of epsilon, marking the first separation between central and local differential privacy for adversarial bandits. 
- The first differentially private algorithms for bandits with expert advice are presented, achieving expected regret rates of $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right)$, $O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$. 
- These rates enable sublinear regret for various combinations of small and large numbers of actions, experts, and epsilon values. <div>
arXiv:2505.21790v1 Announce Type: new 
Abstract: We design new differentially private algorithms for the problems of adversarial bandits and bandits with expert advice. For adversarial bandits, we give a simple and efficient conversion of any non-private bandit algorithm to a private bandit algorithm. Instantiating our conversion with existing non-private bandit algorithms gives a regret upper bound of $O\left(\frac{\sqrt{KT}}{\sqrt{\epsilon}}\right)$, improving upon the existing upper bound $O\left(\frac{\sqrt{KT \log(KT)}}{\epsilon}\right)$ for all $\epsilon \leq 1$. In particular, our algorithms allow for sublinear expected regret even when $\epsilon \leq \frac{1}{\sqrt{T}}$, establishing the first known separation between central and local differential privacy for this problem. For bandits with expert advice, we give the first differentially private algorithms, with expected regret $O\left(\frac{\sqrt{NT}}{\sqrt{\epsilon}}\right), O\left(\frac{\sqrt{KT\log(N)}\log(KT)}{\epsilon}\right)$, and $\tilde{O}\left(\frac{N^{1/6}K^{1/2}T^{2/3}\log(NT)}{\epsilon ^{1/3}} + \frac{N^{1/2}\log(NT)}{\epsilon}\right)$, where $K$ and $N$ are the number of actions and experts respectively. These rates allow us to get sublinear regret for different combinations of small and large $K, N$ and $\epsilon.$
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multimodal Federated Learning: A Survey through the Lens of Different FL Paradigms</title>
<link>https://arxiv.org/abs/2505.21792</link>
<guid>https://arxiv.org/abs/2505.21792</guid>
<content:encoded><![CDATA[
<div> paradigms, multimodal data, Federated Learning, challenges, taxonomy
Summary: 
In the paper "Multimodal Federated Learning: A Taxonomy of Challenges and Opportunities," the authors explore the intersection of two key research areas: leveraging multimodal data to enhance inference performance and enabling distributed training for efficiency and privacy preservation. The absence of a comprehensive taxonomy for Multimodal Federated Learning (MFL) is addressed, focusing on Horizontal FL (HFL), Vertical FL (VFL), and Hybrid FL paradigms. The paper examines the problem formulations, algorithms, and challenges associated with each paradigm in the context of multimodal data. Unique challenges such as modality heterogeneity, privacy heterogeneity, and communication inefficiency are discussed. The authors also highlight open challenges and provide insights for future research. This taxonomy sheds light on the novel challenges posed by multimodal data in various FL paradigms, offering a new perspective for advancing MFL research.<br /><br />Summary: <div>
arXiv:2505.21792v1 Announce Type: new 
Abstract: Multimodal Federated Learning (MFL) lies at the intersection of two pivotal research areas: leveraging complementary information from multiple modalities to improve downstream inference performance and enabling distributed training to enhance efficiency and preserve privacy. Despite the growing interest in MFL, there is currently no comprehensive taxonomy that organizes MFL through the lens of different Federated Learning (FL) paradigms. This perspective is important because multimodal data introduces distinct challenges across various FL settings. These challenges, including modality heterogeneity, privacy heterogeneity, and communication inefficiency, are fundamentally different from those encountered in traditional unimodal or non-FL scenarios. In this paper, we systematically examine MFL within the context of three major FL paradigms: horizontal FL (HFL), vertical FL (VFL), and hybrid FL. For each paradigm, we present the problem formulation, review representative training algorithms, and highlight the most prominent challenge introduced by multimodal data in distributed settings. We also discuss open challenges and provide insights for future research. By establishing this taxonomy, we aim to uncover the novel challenges posed by multimodal data from the perspective of different FL paradigms and to offer a new lens through which to understand and advance the development of MFL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Directions to Cones: Exploring Multidimensional Representations of Propositional Facts in LLMs</title>
<link>https://arxiv.org/abs/2505.21800</link>
<guid>https://arxiv.org/abs/2505.21800</guid>
<content:encoded><![CDATA[
<div> conversational abilities, falsehoods, truthfulness, multi-dimensional cones, concept cones<br />
Summary:<br />
This article explores the truthfulness of large language models (LLMs) and their ability to generate factual statements. By extending the concept cone framework to the domain of truth, the researchers identify multidimensional cones that influence how LLMs behave in response to true/false propositions. Through causal interventions, it is shown that these cones play a crucial role in mediating truth-related behavior across different LLM families. The study demonstrates that these cones can flip model responses to factual statements, generalize across model architectures, and preserve unrelated model behavior. These findings suggest a more complex, multidirectional geometry underlying the truthfulness of simple propositions in LLMs and highlight the potential of concept cones as a valuable tool for investigating abstract behaviors. <br /><br /> <div>
arXiv:2505.21800v1 Announce Type: new 
Abstract: Large Language Models (LLMs) exhibit strong conversational abilities but often generate falsehoods. Prior work suggests that the truthfulness of simple propositions can be represented as a single linear direction in a model's internal activations, but this may not fully capture its underlying geometry. In this work, we extend the concept cone framework, recently introduced for modeling refusal, to the domain of truth. We identify multi-dimensional cones that causally mediate truth-related behavior across multiple LLM families. Our results are supported by three lines of evidence: (i) causal interventions reliably flip model responses to factual statements, (ii) learned cones generalize across model architectures, and (iii) cone-based interventions preserve unrelated model behavior. These findings reveal the richer, multidirectional structure governing simple true/false propositions in LLMs and highlight concept cones as a promising tool for probing abstract behaviors.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Operational Automated Greenhouse Gas Plume Detection</title>
<link>https://arxiv.org/abs/2505.21806</link>
<guid>https://arxiv.org/abs/2505.21806</guid>
<content:encoded><![CDATA[
<div> Keywords: greenhouse gas detection, imaging spectroscopy, deep learning, convolutional neural networks, plume detection

Summary:
In the study, the challenges hindering the operational deployment of a fully automated greenhouse gas plume detection system using imaging spectroscopy were reviewed and addressed. Key obstacles such as data and label quality control, spatiotemporal biases, and modeling objectives alignment were tackled. The study demonstrated that convolutional neural networks (CNNs) can achieve operational detection performance when these obstacles are alleviated. A multitask model combining instance detection and pixelwise segmentation was shown to be effective in leading towards operational deployment. The model's ability to detect plumes across emission source types and regions was evaluated, establishing thresholds for operational readiness. The study also provided data, models, and source code for reproducibility, as well as defined best practices and validation standards for future contributions in the field. <div>
arXiv:2505.21806v1 Announce Type: new 
Abstract: Operational deployment of a fully automated greenhouse gas (GHG) plume detection system remains an elusive goal for imaging spectroscopy missions, despite recent advances in deep learning approaches. With the dramatic increase in data availability, however, automation continues to increase in importance for natural and anthropogenic emissions monitoring. This work reviews and addresses several key obstacles in the field: data and label quality control, prevention of spatiotemporal biases, and correctly aligned modeling objectives. We demonstrate through rigorous experiments using multicampaign data from airborne and spaceborne instruments that convolutional neural networks (CNNs) are able to achieve operational detection performance when these obstacles are alleviated. We demonstrate that a multitask model that learns both instance detection and pixelwise segmentation simultaneously can successfully lead towards an operational pathway. We evaluate the model's plume detectability across emission source types and regions, identifying thresholds for operational deployment. Finally, we provide analysis-ready data, models, and source code for reproducibility, and work to define a set of best practices and validation standards to facilitate future contributions to the field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabReason: A Reinforcement Learning-Enhanced Reasoning LLM for Explainable Tabular Data Prediction</title>
<link>https://arxiv.org/abs/2505.21807</link>
<guid>https://arxiv.org/abs/2505.21807</guid>
<content:encoded><![CDATA[
<div> Interpretable modeling, Gradient boosting machines, Deep learning, Tabular data, Large language models (LLMs)<br />
Summary:<br />
The paper introduces a novel approach that utilizes reasoning-based Large Language Models (LLMs) trained with reinforcement learning to enhance prediction accuracy and interpretability on tabular data. Existing models like gradient boosting machines and deep models excel in performance but lack interpretability. By incorporating custom reward functions, the proposed model not only focuses on accurate predictions but also generates human-understandable explanations. Experimental results on financial benchmark datasets showcase superior performance compared to other LLMs. This approach seeks to bridge the gap between accurate predictions and interpretability, offering a promising solution for real-world applications. <br /> <div>
arXiv:2505.21807v1 Announce Type: new 
Abstract: Predictive modeling on tabular data is the cornerstone of many real-world applications. Although gradient boosting machines and some recent deep models achieve strong performance on tabular data, they often lack interpretability. On the other hand, large language models (LLMs) have demonstrated powerful capabilities to generate human-like reasoning and explanations, but remain under-performed for tabular data prediction. In this paper, we propose a new approach that leverages reasoning-based LLMs, trained using reinforcement learning, to perform more accurate and explainable predictions on tabular data. Our method introduces custom reward functions that guide the model not only toward high prediction accuracy but also toward human-understandable reasons for its predictions. Experimental results show that our model achieves promising performance on financial benchmark datasets, outperforming most existing LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Data Augmentation through Bayesian Model Selection</title>
<link>https://arxiv.org/abs/2505.21813</link>
<guid>https://arxiv.org/abs/2505.21813</guid>
<content:encoded><![CDATA[
<div> Probabilistic View, Data Augmentation, Bayesian Optimization, Marginal Likelihood, Bayesian Principles
<br />
Summary: 
The paper introduces a novel framework for optimizing Data Augmentation (DA) strategies in machine learning. By viewing DA from a probabilistic standpoint, the authors treat augmentation parameters as model (hyper)parameters and optimize the marginal likelihood through a Bayesian model selection approach. A tractable Evidence Lower BOund (ELBO) is derived to enable joint optimization of augmentation parameters and model parameters. Theoretical results on variational approximation quality, generalization guarantees, invariance properties, and links to empirical Bayes are provided. Experimental results on computer vision tasks demonstrate improved calibration and robust performance compared to fixed or no augmentation strategies. The work establishes a rigorous foundation for optimizing DA using Bayesian principles, offering significant potential for enhancing robustness in machine learning. 
<br /> <div>
arXiv:2505.21813v1 Announce Type: new 
Abstract: Data Augmentation (DA) has become an essential tool to improve robustness and generalization of modern machine learning. However, when deciding on DA strategies it is critical to choose parameters carefully, and this can be a daunting task which is traditionally left to trial-and-error or expensive optimization based on validation performance. In this paper, we counter these limitations by proposing a novel framework for optimizing DA. In particular, we take a probabilistic view of DA, which leads to the interpretation of augmentation parameters as model (hyper)-parameters, and the optimization of the marginal likelihood with respect to these parameters as a Bayesian model selection problem. Due to its intractability, we derive a tractable Evidence Lower BOund (ELBO), which allows us to optimize augmentation parameters jointly with model parameters. We provide extensive theoretical results on variational approximation quality, generalization guarantees, invariance properties, and connections to empirical Bayes. Through experiments on computer vision tasks, we show that our approach improves calibration and yields robust performance over fixed or no augmentation. Our work provides a rigorous foundation for optimizing DA through Bayesian principles with significant potential for robust machine learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Latent Pattern Analysis for Estimating Type 2 Diabetes Risk in Undiagnosed Populations</title>
<link>https://arxiv.org/abs/2505.21824</link>
<guid>https://arxiv.org/abs/2505.21824</guid>
<content:encoded><![CDATA[
<div> machine learning, diabetes risk prediction, Non-negative Matrix Factorization, multimorbidity, polypharmacy

Summary:
This article introduces a novel unsupervised framework for predicting the risk of developing type 2 diabetes mellitus (T2DM) by integrating Non-negative Matrix Factorization (NMF) with statistical techniques. T2DM poses significant health and economic challenges, making early detection crucial. Traditional supervised learning approaches are limited by the lack of confirmed negative cases, leading to the development of this new method. By identifying latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients, the framework estimates T2DM risk in undiagnosed individuals. This data-driven approach provides interpretable insights for healthcare providers to implement timely interventions, potentially improving patient outcomes and reducing the future health and economic burden of T2DM. <div>
arXiv:2505.21824v1 Announce Type: new 
Abstract: The global prevalence of diabetes, particularly type 2 diabetes mellitus (T2DM), is rapidly increasing, posing significant health and economic challenges. T2DM not only disrupts blood glucose regulation but also damages vital organs such as the heart, kidneys, eyes, nerves, and blood vessels, leading to substantial morbidity and mortality. In the US alone, the economic burden of diagnosed diabetes exceeded \$400 billion in 2022. Early detection of individuals at risk is critical to mitigating these impacts. While machine learning approaches for T2DM prediction are increasingly adopted, many rely on supervised learning, which is often limited by the lack of confirmed negative cases. To address this limitation, we propose a novel unsupervised framework that integrates Non-negative Matrix Factorization (NMF) with statistical techniques to identify individuals at risk of developing T2DM. Our method identifies latent patterns of multimorbidity and polypharmacy among diagnosed T2DM patients and applies these patterns to estimate the T2DM risk in undiagnosed individuals. By leveraging data-driven insights from comorbidity and medication usage, our approach provides an interpretable and scalable solution that can assist healthcare providers in implementing timely interventions, ultimately improving patient outcomes and potentially reducing the future health and economic burden of T2DM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Let Me Think! A Long Chain-of-Thought Can Be Worth Exponentially Many Short Ones</title>
<link>https://arxiv.org/abs/2505.21825</link>
<guid>https://arxiv.org/abs/2505.21825</guid>
<content:encoded><![CDATA[
<div> graph connectivity, inference-time computation, language models, reasoning strategies, parallel vs sequential scaling

Summary:
In the study, the researchers focused on the optimal allocation of inference-time computation in large language models for reasoning tasks. They explored whether sequential scaling or parallel scaling was more effective in improving performance. Through theoretical analysis and experiments on various language models, including those trained specifically for graph connectivity problems, they discovered scenarios where sequential scaling provided an exponential advantage over parallel scaling. These findings were particularly evident in challenging distributions of graphs. The research sheds light on the importance of considering different strategies for reasoning tasks and highlights the potential benefits of sequential scaling in certain scenarios. <div>
arXiv:2505.21825v1 Announce Type: new 
Abstract: Inference-time computation has emerged as a promising scaling axis for improving large language model reasoning. However, despite yielding impressive performance, the optimal allocation of inference-time computation remains poorly understood. A central question is whether to prioritize sequential scaling (e.g., longer chains of thought) or parallel scaling (e.g., majority voting across multiple short chains of thought). In this work, we seek to illuminate the landscape of test-time scaling by demonstrating the existence of reasoning settings where sequential scaling offers an exponential advantage over parallel scaling. These settings are based on graph connectivity problems in challenging distributions of graphs. We validate our theoretical findings with comprehensive experiments across a range of language models, including models trained from scratch for graph connectivity with different chain of thought strategies as well as large reasoning models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Search of Adam's Secret Sauce</title>
<link>https://arxiv.org/abs/2505.21829</link>
<guid>https://arxiv.org/abs/2505.21829</guid>
<content:encoded><![CDATA[
<div> Keywords: Adam, language models, optimization, signed momentum, gradient

Summary:
Adam optimization algorithm has shown remarkable efficacy in training transformer-based language models. Various simplified versions of Adam, such as signed momentum methods, have been proposed but consistently underperform relative to Adam. However, constraining the momentum parameters in Adam to be equal preserves near-optimal performance while offering new insights and formulations. This choice not only ensures robust performance but also allows for a statistical interpretation, showing that Adam can be viewed as an online algorithm for estimating gradient mean and variance from a mean-field Gaussian variational inference perspective. This study conducted over 1,300 language models highlights the significance of Adam in optimization and sheds light on the "secret sauce" behind its success. 

<br /><br />Summary: <div>
arXiv:2505.21829v1 Announce Type: new 
Abstract: Understanding the remarkable efficacy of Adam when training transformer-based language models has become a central research topic within the optimization community. To gain deeper insights, several simplifications of Adam have been proposed, such as the signed gradient and signed momentum methods. In this work, we conduct an extensive empirical study - training over 1,300 language models across different data configurations and scales - comparing Adam to several known simplified variants. We find that signed momentum methods are faster than SGD, but consistently underperform relative to Adam, even after careful tuning of momentum, clipping setting and learning rates. However, our analysis reveals a compelling option that preserves near-optimal performance while allowing for new insightful reformulations: constraining the Adam momentum parameters to be equal. Beyond robust performance, this choice affords new theoretical insights, highlights the "secret sauce" on top of signed momentum, and grants a precise statistical interpretation: we show that Adam in this setting implements a natural online algorithm for estimating the mean and variance of gradients-one that arises from a mean-field Gaussian variational inference perspective.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TuneComp: Joint Fine-tuning and Compression for Large Foundation Models</title>
<link>https://arxiv.org/abs/2505.21835</link>
<guid>https://arxiv.org/abs/2505.21835</guid>
<content:encoded><![CDATA[
<div> compression, knowledge distillation, low-rank approximation, pruning, model size reduction 

Summary: 
Joint fine-tuning and compression methods are proposed to reduce model size after training, aiming to create a smaller model directly guided by the downstream task. This approach avoids sacrificing performance and eliminates the need for a larger intermediate model. By gradually distilling the model to a pruned low-rank structure, the joint fine-tuning and compression method outperforms sequential compression techniques. Experimental results demonstrate the effectiveness of this approach in significantly reducing model size while maintaining performance in downstream tasks. <div>
arXiv:2505.21835v1 Announce Type: new 
Abstract: To reduce model size during post-training, compression methods, including knowledge distillation, low-rank approximation, and pruning, are often applied after fine-tuning the model. However, sequential fine-tuning and compression sacrifices performance, while creating a larger than necessary model as an intermediate step. In this work, we aim to reduce this gap, by directly constructing a smaller model while guided by the downstream task. We propose to jointly fine-tune and compress the model by gradually distilling it to a pruned low-rank structure. Experiments demonstrate that joint fine-tuning and compression significantly outperforms other sequential compression methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimistic Algorithm for online CMDPS with Anytime Adversarial Constraints</title>
<link>https://arxiv.org/abs/2505.21841</link>
<guid>https://arxiv.org/abs/2505.21841</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, safe learning, online learning, adversarial settings, optimal policy <br />
Summary: The article introduces the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, designed for online safe reinforcement learning in dynamic environments with unknown and adversarial constraints. OMDPD aims to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). The algorithm achieves optimal regret and strong constraint violation bounds without relying on strict assumptions about known safe policies. It is the first to address online CMDPs with adversarial constraints and can work effectively even in uncertain environments. The results demonstrate that OMDPD offers practical guarantees for safe decision-making in adversarial settings. Access to accurate estimates of rewards and transitions can further enhance the algorithm's performance, making it a valuable tool for applications in autonomous driving, robotics, and cybersecurity. <br /><br /> <div>
arXiv:2505.21841v1 Announce Type: new 
Abstract: Online safe reinforcement learning (RL) plays a key role in dynamic environments, with applications in autonomous driving, robotics, and cybersecurity. The objective is to learn optimal policies that maximize rewards while satisfying safety constraints modeled by constrained Markov decision processes (CMDPs). Existing methods achieve sublinear regret under stochastic constraints but often fail in adversarial settings, where constraints are unknown, time-varying, and potentially adversarially designed. In this paper, we propose the Optimistic Mirror Descent Primal-Dual (OMDPD) algorithm, the first to address online CMDPs with anytime adversarial constraints. OMDPD achieves optimal regret O(sqrt(K)) and strong constraint violation O(sqrt(K)) without relying on Slater's condition or the existence of a strictly known safe policy. We further show that access to accurate estimates of rewards and transitions can further improve these bounds. Our results offer practical guarantees for safe decision-making in adversarial environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Provable Approach for End-to-End Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21852</link>
<guid>https://arxiv.org/abs/2505.21852</guid>
<content:encoded><![CDATA[
<div> safe reinforcement learning, Provably Lifetime Safe RL (PLS), offline learning, policy deployment, Gaussian processes

Summary:<br />
- This study introduces Provably Lifetime Safe RL (PLS) to address the challenge of ensuring policy safety from learning to operation.
- PLS combines offline safe RL with safe policy deployment and uses return-conditioned supervised learning.
- The method cautiously optimizes target returns using Gaussian processes, ensuring safety with high probability.
- The mathematical relationship between target and actual returns is analyzed and near-optimal target returns are guaranteed.
- Empirical results show that PLS outperforms baselines in safety and reward performance, achieving high rewards while maintaining policy safety throughout its lifetime.
<br /><br /> <div>
arXiv:2505.21852v1 Announce Type: new 
Abstract: A longstanding goal in safe reinforcement learning (RL) is a method to ensure the safety of a policy throughout the entire process, from learning to operation. However, existing safe RL paradigms inherently struggle to achieve this objective. We propose a method, called Provably Lifetime Safe RL (PLS), that integrates offline safe RL with safe policy deployment to address this challenge. Our proposed method learns a policy offline using return-conditioned supervised learning and then deploys the resulting policy while cautiously optimizing a limited set of parameters, known as target returns, using Gaussian processes (GPs). Theoretically, we justify the use of GPs by analyzing the mathematical relationship between target and actual returns. We then prove that PLS finds near-optimal target returns while guaranteeing safety with high probability. Empirically, we demonstrate that PLS outperforms baselines both in safety and reward performance, thereby achieving the longstanding goal to obtain high rewards while ensuring the safety of a policy throughout the lifetime from learning to operation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Bayesian Model Averaging in the Era of Foundation Models</title>
<link>https://arxiv.org/abs/2505.21857</link>
<guid>https://arxiv.org/abs/2505.21857</guid>
<content:encoded><![CDATA[
<div> Keywords: Bayesian model averaging, ensemble learning, pre-trained models, classification, optimization

Summary:
In this study, the authors propose a novel approach to enhance classification performance on image and text data by leveraging Bayesian model averaging (BMA) with pre-trained or lightly-finetuned foundation models. They introduce trainable linear classifiers that take features from these models as inputs, allowing for model ensembling based on model posteriors. Additionally, they present a computationally efficient model averaging scheme (OMA) where ensemble weights are optimized to reduce prediction surprise. These methods provide a principled way to combine multiple models and enhance classification tasks, making it possible to incorporate future, improved foundation models for even better performance. <div>
arXiv:2505.21857v1 Announce Type: new 
Abstract: We revisit the classical, full-fledged Bayesian model averaging (BMA) paradigm to ensemble pre-trained and/or lightly-finetuned foundation models to enhance the classification performance on image and text data. To make BMA tractable under foundation models, we introduce trainable linear classifiers that take frozen features from the pre-trained foundation models as inputs. The model posteriors over the linear classifiers tell us which linear heads and frozen features are better suited for a given dataset, resulting in a principled model ensembling method. Furthermore, we propose a computationally cheaper, optimizable model averaging scheme (OMA). In OMA, we directly optimize the model ensemble weights, just like those weights based on model posterior distributions in BMA, by reducing the amount of surprise (expected entropy of the predictions) we get from predictions of ensembled models. With the rapid development of foundation models, these approaches will enable the incorporation of future, possibly significantly better foundation models to enhance the performance of challenging classification tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Batch Normalisation: Resolving the Dilemma of Batch Normalisation in Federated Learning</title>
<link>https://arxiv.org/abs/2505.21877</link>
<guid>https://arxiv.org/abs/2505.21877</guid>
<content:encoded><![CDATA[
<div> Keywords: Batch Normalisation, Federated Learning, Statistical Parameters, Hybrid Batch Normalisation, Global Statistics

Summary:
Hybrid Batch Normalisation (HBN) is introduced as a solution for the challenge of applying Batch Normalisation (BN) in federated learning, where data among client nodes is not identically distributed. HBN separates the update of statistical parameters from learnable parameters, allowing for unbiased estimation of global statistical parameters in distributed scenarios. By introducing a learnable hybrid distribution factor, HBN enables each computing node to adaptively mix current batch statistics with global statistics, improving federated learning performance. This approach emphasizes the importance of global statistics in shaping the learning process across varying federated learning settings, particularly beneficial for small batch sizes and heterogeneous data.<br /><br />Summary: <div>
arXiv:2505.21877v1 Announce Type: new 
Abstract: Batch Normalisation (BN) is widely used in conventional deep neural network training to harmonise the input-output distributions for each batch of data. However, federated learning, a distributed learning paradigm, faces the challenge of dealing with non-independent and identically distributed data among the client nodes. Due to the lack of a coherent methodology for updating BN statistical parameters, standard BN degrades the federated learning performance. To this end, it is urgent to explore an alternative normalisation solution for federated learning. In this work, we resolve the dilemma of the BN layer in federated learning by developing a customised normalisation approach, Hybrid Batch Normalisation (HBN). HBN separates the update of statistical parameters (i.e. , means and variances used for evaluation) from that of learnable parameters (i.e. , parameters that require gradient updates), obtaining unbiased estimates of global statistical parameters in distributed scenarios. In contrast with the existing solutions, we emphasise the supportive power of global statistics for federated learning. The HBN layer introduces a learnable hybrid distribution factor, allowing each computing node to adaptively mix the statistical parameters of the current batch with the global statistics. Our HBN can serve as a powerful plugin to advance federated learning performance. It reflects promising merits across a wide range of federated learning settings, especially for small batch sizes and heterogeneous data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HydraNet: Momentum-Driven State Space Duality for Multi-Granularity Tennis Tournaments Analysis</title>
<link>https://arxiv.org/abs/2505.21882</link>
<guid>https://arxiv.org/abs/2505.21882</guid>
<content:encoded><![CDATA[
<div> Keywords: tennis tournaments, momentum, HydraNet, multi-granularity analysis, state-space duality<br />
Summary:<br />
This study introduces a novel Momentum Score (MS) metric and HydraNet framework to model momentum in tennis tournaments. The framework integrates thirty-two dimensions of athlete performance and employs a Hydra module based on a state-space duality (SSD) framework to capture explicit and implicit momentum. It also includes a Versus Learning method to enhance the adversarial nature of momentum between athletes and a Collaborative-Adversarial Attention Mechanism (CAAM) for dynamic momentum integration. A million-level tennis cross-tournament dataset is used to validate the HydraNet framework's capabilities in modeling momentum at different granularities. Experimental results demonstrate that the MS metric provides valuable insights into the impact of momentum on match outcomes. This research establishes a new foundation for momentum modeling and sports analysis in tennis tournaments. Source code and datasets are available at https://github.com/ReyJerry/HydraNet. <br /> <div>
arXiv:2505.21882v1 Announce Type: new 
Abstract: In tennis tournaments, momentum, a critical yet elusive phenomenon, reflects the dynamic shifts in performance of athletes that can decisively influence match outcomes. Despite its significance, momentum in terms of effective modeling and multi-granularity analysis across points, games, sets, and matches in tennis tournaments remains underexplored. In this study, we define a novel Momentum Score (MS) metric to quantify a player's momentum level in multi-granularity tennis tournaments, and design HydraNet, a momentum-driven state-space duality-based framework, to model MS by integrating thirty-two heterogeneous dimensions of athletes performance in serve, return, psychology and fatigue. HydraNet integrates a Hydra module, which builds upon a state-space duality (SSD) framework, capturing explicit momentum with a sliding-window mechanism and implicit momentum through cross-game state propagation. It also introduces a novel Versus Learning method to better enhance the adversarial nature of momentum between the two athletes at a macro level, along with a Collaborative-Adversarial Attention Mechanism (CAAM) for capturing and integrating intra-player and inter-player dynamic momentum at a micro level. Additionally, we construct a million-level tennis cross-tournament dataset spanning from 2012-2023 Wimbledon and 2013-2023 US Open, and validate the multi-granularity modeling capability of HydraNet for the MS metric on this dataset. Extensive experimental evaluations demonstrate that the MS metric constructed by the HydraNet framework provides actionable insights into how momentum impacts outcomes at different granularities, establishing a new foundation for momentum modeling and sports analysis. To the best of our knowledge. The source code and datasets are available at https://github.com/ReyJerry/HydraNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SDPO: Importance-Sampled Direct Preference Optimization for Stable Diffusion Training</title>
<link>https://arxiv.org/abs/2505.21893</link>
<guid>https://arxiv.org/abs/2505.21893</guid>
<content:encoded><![CDATA[
<div> Keywords: Preference learning, generative models, diffusion models, optimization, off-policy bias

Summary: 
Preference learning is crucial for aligning generative models with human expectations, with recent advancements extending to diffusion models through methods like Direct Preference Optimization (DPO). However, challenges such as timestep-dependent instability and off-policy bias hinder the performance of existing approaches like Diffusion-DPO. To address these issues, a new practical strategy, DPO-C\&amp;M, is introduced to improve stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Additionally, a principled framework, SDPO, incorporates importance sampling into the objective to fully correct off-policy bias and emphasize informative updates during the diffusion process. Experimental results on various datasets demonstrate that both DPO-C\&amp;M and SDPO outperform standard Diffusion-DPO in terms of VBench scores, human preference alignment, and training robustness. These findings underscore the significance of optimized, distribution-corrected techniques in diffusion-based preference learning.<br /><br />Summary: <div>
arXiv:2505.21893v1 Announce Type: new 
Abstract: Preference learning has become a central technique for aligning generative models with human expectations. Recently, it has been extended to diffusion models through methods like Direct Preference Optimization (DPO). However, existing approaches such as Diffusion-DPO suffer from two key challenges: timestep-dependent instability, caused by a mismatch between the reverse and forward diffusion processes and by high gradient variance in early noisy timesteps, and off-policy bias arising from the mismatch between optimization and data collection policies. We begin by analyzing the reverse diffusion trajectory and observe that instability primarily occurs at early timesteps with low importance weights. To address these issues, we first propose DPO-C\&amp;M, a practical strategy that improves stability by clipping and masking uninformative timesteps while partially mitigating off-policy bias. Building on this, we introduce SDPO (Importance-Sampled Direct Preference Optimization), a principled framework that incorporates importance sampling into the objective to fully correct for off-policy bias and emphasize informative updates during the diffusion process. Experiments on CogVideoX-2B, CogVideoX-5B, and Wan2.1-1.3B demonstrate that both methods outperform standard Diffusion-DPO, with SDPO achieving superior VBench scores, human preference alignment, and training robustness. These results highlight the importance of timestep-aware, distribution-corrected optimization in diffusion-based preference learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Compressing Sine-Activated Low-Rank Adapters through Post-Training Quantization</title>
<link>https://arxiv.org/abs/2505.21895</link>
<guid>https://arxiv.org/abs/2505.21895</guid>
<content:encoded><![CDATA[
arXiv:2505.21895v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has become a standard approach for parameter-efficient fine-tuning, offering substantial reductions in trainable parameters by modeling updates as the product of two low-rank matrices. While effective, the low-rank constraint inherently limits representational capacity, often resulting in reduced performance compared to full-rank fine-tuning. Recent work by Ji et al. (2025) has addressed this limitation by applying a fixed-frequency sinusoidal transformation to low-rank adapters, increasing their stable rank without introducing additional parameters. This raises a crucial question: can the same sine-activated technique be successfully applied within the context of Post-Training Quantization to retain benefits even after model compression? In this paper, we investigate this question by extending the sinusoidal transformation framework to quantized LoRA adapters. We develop a theoretical analysis showing that the stable rank of a quantized adapter is tightly linked to that of its full-precision counterpart, motivating the use of such rank-enhancing functions even under quantization. Our results demonstrate that the expressivity gains from a sinusoidal non-linearity persist after quantization, yielding highly compressed adapters with negligible loss in performance. We validate our approach across a range of fine-tuning tasks for language, vision and text-to-image generation achieving significant memory savings while maintaining competitive accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Out-of-Distribution Reasoning in LLMs: An Empirical Study on Diagnosis-Related Group Coding</title>
<link>https://arxiv.org/abs/2505.21908</link>
<guid>https://arxiv.org/abs/2505.21908</guid>
<content:encoded><![CDATA[
arXiv:2505.21908v1 Announce Type: new 
Abstract: Diagnosis-Related Group (DRG) codes are essential for hospital reimbursement and operations but require labor-intensive assignment. Large Language Models (LLMs) struggle with DRG coding due to the out-of-distribution (OOD) nature of the task: pretraining corpora rarely contain private clinical or billing data. We introduce DRG-Sapphire, which uses large-scale reinforcement learning (RL) for automated DRG coding from clinical notes. Built on Qwen2.5-7B and trained with Group Relative Policy Optimization (GRPO) using rule-based rewards, DRG-Sapphire introduces a series of RL enhancements to address domain-specific challenges not seen in previous mathematical tasks. Our model achieves state-of-the-art accuracy on the MIMIC-IV benchmark and generates physician-validated reasoning for DRG assignments, significantly enhancing explainability. Our study further sheds light on broader challenges of applying RL to knowledge-intensive, OOD tasks. We observe that RL performance scales approximately linearly with the logarithm of the number of supervised fine-tuning (SFT) examples, suggesting that RL effectiveness is fundamentally constrained by the domain knowledge encoded in the base model. For OOD tasks like DRG coding, strong RL performance requires sufficient knowledge infusion prior to RL. Consequently, scaling SFT may be more effective and computationally efficient than scaling RL alone for such tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Transformer Without Using Learning Rate Warmup</title>
<link>https://arxiv.org/abs/2505.21910</link>
<guid>https://arxiv.org/abs/2505.21910</guid>
<content:encoded><![CDATA[
arXiv:2505.21910v1 Announce Type: new 
Abstract: Scaling Transformer to a large scale without using some technical tricks such as learning rate warump and using an obviously lower learning rate is an extremely challenging task, and is increasingly gaining more attention. In this paper, we provide a theoretical analysis for the process of training Transformer and reveal the rationale behind the model crash phenomenon in the training process, termed \textit{spectral energy concentration} of ${\bW_q}^{\top} \bW_k$, which is the reason for a malignant entropy collapse, where ${\bW_q}$ and $\bW_k$ are the projection matrices for the query and the key in Transformer, respectively. To remedy this problem, motivated by \textit{Weyl's Inequality}, we present a novel optimization strategy, \ie, making the weight updating in successive steps smooth -- if the ratio $\frac{\sigma_{1}(\nabla \bW_t)}{\sigma_{1}(\bW_{t-1})}$ is larger than a threshold, we will automatically bound the learning rate to a weighted multiple of $\frac{\sigma_{1}(\bW_{t-1})}{\sigma_{1}(\nabla \bW_t)}$, where $\nabla \bW_t$ is the updating quantity in step $t$. Such an optimization strategy can prevent spectral energy concentration to only a few directions, and thus can avoid malignant entropy collapse which will trigger the model crash. We conduct extensive experiments using ViT, Swin-Transformer and GPT, showing that our optimization strategy can effectively and stably train these Transformers without using learning rate warmup.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-supervised Learning Method Using Transformer for Multi-dimensional Sensor Data Processing</title>
<link>https://arxiv.org/abs/2505.21918</link>
<guid>https://arxiv.org/abs/2505.21918</guid>
<content:encoded><![CDATA[
arXiv:2505.21918v1 Announce Type: new 
Abstract: We developed a deep learning algorithm for human activity recognition using sensor signals as input. In this study, we built a pretrained language model based on the Transformer architecture, which is widely used in natural language processing. By leveraging this pretrained model, we aimed to improve performance on the downstream task of human activity recognition. While this task can be addressed using a vanilla Transformer, we propose an enhanced n-dimensional numerical processing Transformer that incorporates three key features: embedding n-dimensional numerical data through a linear layer, binning-based pre-processing, and a linear transformation in the output layer. We evaluated the effectiveness of our proposed model across five different datasets. Compared to the vanilla Transformer, our model demonstrated 10%-15% improvements in accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FALCON: An ML Framework for Fully Automated Layout-Constrained Analog Circuit Design</title>
<link>https://arxiv.org/abs/2505.21923</link>
<guid>https://arxiv.org/abs/2505.21923</guid>
<content:encoded><![CDATA[
arXiv:2505.21923v1 Announce Type: new 
Abstract: Designing analog circuits from performance specifications is a complex, multi-stage process encompassing topology selection, parameter inference, and layout feasibility. We introduce FALCON, a unified machine learning framework that enables fully automated, specification-driven analog circuit synthesis through topology selection and layout-constrained optimization. Given a target performance, FALCON first selects an appropriate circuit topology using a performance-driven classifier guided by human design heuristics. Next, it employs a custom, edge-centric graph neural network trained to map circuit topology and parameters to performance, enabling gradient-based parameter inference through the learned forward model. This inference is guided by a differentiable layout cost, derived from analytical equations capturing parasitic and frequency-dependent effects, and constrained by design rules. We train and evaluate FALCON on a large-scale custom dataset of 1M analog mm-wave circuits, generated and simulated using Cadence Spectre across 20 expert-designed topologies. Through this evaluation, FALCON demonstrates >99\% accuracy in topology inference, <10\% relative error in performance prediction, and efficient layout-aware design that completes in under 1 second per instance. Together, these results position FALCON as a practical and extensible foundation model for end-to-end analog circuit design automation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Ensemble for Fine-tuning Language Models on Multiple Datasets</title>
<link>https://arxiv.org/abs/2505.21930</link>
<guid>https://arxiv.org/abs/2505.21930</guid>
<content:encoded><![CDATA[
arXiv:2505.21930v1 Announce Type: new 
Abstract: This paper develops an ensemble method for fine-tuning a language model to multiple datasets. Existing methods, such as quantized LoRA (QLoRA), are efficient when adapting to a single dataset. When training on multiple datasets of different tasks, a common setup in practice, it remains unclear how to design an efficient adaptation for fine-tuning language models. We propose to use an ensemble of multiple smaller adapters instead of a single adapter per task. We design an efficient algorithm that partitions $n$ datasets into $m$ groups, where $m$ is typically much smaller than $n$ in practice, and train one adapter for each group before taking a weighted combination to form the ensemble. The algorithm leverages a first-order approximation property of low-rank adaptation to quickly obtain the fine-tuning performances of dataset combinations since methods like LoRA stay close to the base model. Hence, we use the gradients of the base model to estimate its behavior during fine-tuning. Empirically, this approximation holds with less than $1\%$ error on models with up to $34$ billion parameters, leading to an estimation of true fine-tuning performances under $5\%$ error while speeding up computation compared to base fine-tuning by $105$ times. When applied to fine-tune Llama and GPT models on ten text classification tasks, our approach provides up to $10\%$ higher average test accuracy over QLoRA, with only $9\%$ more FLOPs. On a Llama model with $34$ billion parameters, an ensemble of QLoRA increases test accuracy by $3\%$ compared to QLoRA, with only $8\%$ more FLOPs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical Adversarial Attacks on Stochastic Bandits via Fake Data Injection</title>
<link>https://arxiv.org/abs/2505.21938</link>
<guid>https://arxiv.org/abs/2505.21938</guid>
<content:encoded><![CDATA[
arXiv:2505.21938v1 Announce Type: new 
Abstract: Adversarial attacks on stochastic bandits have traditionally relied on some unrealistic assumptions, such as per-round reward manipulation and unbounded perturbations, limiting their relevance to real-world systems. We propose a more practical threat model, Fake Data Injection, which reflects realistic adversarial constraints: the attacker can inject only a limited number of bounded fake feedback samples into the learner's history, simulating legitimate interactions. We design efficient attack strategies under this model, explicitly addressing both magnitude constraints (on reward values) and temporal constraints (on when and how often data can be injected). Our theoretical analysis shows that these attacks can mislead both Upper Confidence Bound (UCB) and Thompson Sampling algorithms into selecting a target arm in nearly all rounds while incurring only sublinear attack cost. Experiments on synthetic and real-world datasets validate the effectiveness of our strategies, revealing significant vulnerabilities in widely used stochastic bandit algorithms under practical adversarial scenarios.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continual Learning Beyond Experience Rehearsal and Full Model Surrogates</title>
<link>https://arxiv.org/abs/2505.21942</link>
<guid>https://arxiv.org/abs/2505.21942</guid>
<content:encoded><![CDATA[
arXiv:2505.21942v1 Announce Type: new 
Abstract: Continual learning (CL) has remained a significant challenge for deep neural networks as learning new tasks erases previously acquired knowledge, either partially or completely. Existing solutions often rely on experience rehearsal or full model surrogates to mitigate CF. While effective, these approaches introduce substantial memory and computational overhead, limiting their scalability and applicability in real-world scenarios. To address this, we propose SPARC, a scalable CL approach that eliminates the need for experience rehearsal and full-model surrogates. By effectively combining task-specific working memories and task-agnostic semantic memory for cross-task knowledge consolidation, SPARC results in a remarkable parameter efficiency, using only 6% of the parameters required by full-model surrogates. Despite its lightweight design, SPARC achieves superior performance on Seq-TinyImageNet and matches rehearsal-based methods on various CL benchmarks. Additionally, weight re-normalization in the classification layer mitigates task-specific biases, establishing SPARC as a practical and scalable solution for CL under stringent efficiency constraints.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Primal-Dual Double Block-Coordinate for Two-way Partial AUC Maximization</title>
<link>https://arxiv.org/abs/2505.21944</link>
<guid>https://arxiv.org/abs/2505.21944</guid>
<content:encoded><![CDATA[
arXiv:2505.21944v1 Announce Type: new 
Abstract: Two-way partial AUC (TPAUC) is a critical performance metric for binary classification with imbalanced data, as it focuses on specific ranges of the true positive rate (TPR) and false positive rate (FPR). However, stochastic algorithms for TPAUC optimization remain under-explored, with existing methods either limited to approximated TPAUC loss functions or burdened by sub-optimal complexities. To overcome these limitations, we introduce two innovative stochastic primal-dual double block-coordinate algorithms for TPAUC maximization. These algorithms utilize stochastic block-coordinate updates for both the primal and dual variables, catering to both convex and non-convex settings. We provide theoretical convergence rate analyses, demonstrating significant improvements over prior approaches. Our experimental results, based on multiple benchmark datasets, validate the superior performance of our algorithms, showcasing faster convergence and better generalization. This work advances the state of the art in TPAUC optimization and offers practical tools for real-world machine learning applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles</title>
<link>https://arxiv.org/abs/2505.21959</link>
<guid>https://arxiv.org/abs/2505.21959</guid>
<content:encoded><![CDATA[
arXiv:2505.21959v1 Announce Type: new 
Abstract: With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called \textbf{EnsemW2S}, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4\%, and 3.2\% improvements on ID datasets and, upto 6\% and 2.28\% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging LLMs on a Simplex</title>
<link>https://arxiv.org/abs/2505.21972</link>
<guid>https://arxiv.org/abs/2505.21972</guid>
<content:encoded><![CDATA[
arXiv:2505.21972v1 Announce Type: new 
Abstract: Automated evaluation of free-form outputs from large language models (LLMs) is challenging because many distinct answers can be equally valid. A common practice is to use LLMs themselves as judges, but the theoretical properties of this approach are not yet well understood. We show that a geometric framework that represents both judges and candidates as points on a probability simplex can provide helpful insight on what is or is not identifiable using LLM judges. Our theoretical analysis uncovers a "phase transition" in ranking identifiability: for binary scoring systems, true rankings are identifiable even with weak judges under mild assumptions, while rankings become non-identifiable for three or more scoring levels even with infinite data, absent additional prior knowledge. This non-identifiability highlights how uncertainty in rankings stems from not only aleatoric uncertainty (i.e., inherent stochasticity in the data) but also epistemic uncertainty regarding which assumptions hold, an aspect that has received limited attention until now. To integrate both types of uncertainty, we use Bayesian inference to encode assumptions as priors and conduct sensitivity analysis of ranking estimates and credible intervals. Empirical evaluations across multiple benchmarks demonstrate that Bayesian inference yields more accurate rankings and substantially improves coverage rates. These results underscore the importance of taking a more holistic approach to uncertainty quantification when using LLMs as judges.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BOFormer: Learning to Solve Multi-Objective Bayesian Optimization via Non-Markovian RL</title>
<link>https://arxiv.org/abs/2505.21974</link>
<guid>https://arxiv.org/abs/2505.21974</guid>
<content:encoded><![CDATA[
arXiv:2505.21974v1 Announce Type: new 
Abstract: Bayesian optimization (BO) offers an efficient pipeline for optimizing black-box functions with the help of a Gaussian process prior and an acquisition function (AF). Recently, in the context of single-objective BO, learning-based AFs witnessed promising empirical results given its favorable non-myopic nature. Despite this, the direct extension of these approaches to multi-objective Bayesian optimization (MOBO) suffer from the \textit{hypervolume identifiability issue}, which results from the non-Markovian nature of MOBO problems. To tackle this, inspired by the non-Markovian RL literature and the success of Transformers in language modeling, we present a generalized deep Q-learning framework and propose \textit{BOFormer}, which substantiates this framework for MOBO via sequence modeling. Through extensive evaluation, we demonstrate that BOFormer constantly outperforms the benchmark rule-based and learning-based algorithms in various synthetic MOBO and real-world multi-objective hyperparameter optimization problems. We have made the source code publicly available to encourage further research in this direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two-Stage Feature Generation with Transformer and Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21978</link>
<guid>https://arxiv.org/abs/2505.21978</guid>
<content:encoded><![CDATA[
arXiv:2505.21978v1 Announce Type: new 
Abstract: Feature generation is a critical step in machine learning, aiming to enhance model performance by capturing complex relationships within the data and generating meaningful new features. Traditional feature generation methods heavily rely on domain expertise and manual intervention, making the process labor-intensive and challenging to adapt to different scenarios. Although automated feature generation techniques address these issues to some extent, they often face challenges such as feature redundancy, inefficiency in feature space exploration, and limited adaptability to diverse datasets and tasks. To address these problems, we propose a Two-Stage Feature Generation (TSFG) framework, which integrates a Transformer-based encoder-decoder architecture with Proximal Policy Optimization (PPO). The encoder-decoder model in TSFG leverages the Transformer's self-attention mechanism to efficiently represent and transform features, capturing complex dependencies within the data. PPO further enhances TSFG by dynamically adjusting the feature generation strategy based on task-specific feedback, optimizing the process for improved performance and adaptability. TSFG dynamically generates high-quality feature sets, significantly improving the predictive performance of machine learning models. Experimental results demonstrate that TSFG outperforms existing state-of-the-art methods in terms of feature quality and adaptability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ACE: Exploring Activation Cosine Similarity and Variance for Accurate and Calibration-Efficient LLM Pruning</title>
<link>https://arxiv.org/abs/2505.21987</link>
<guid>https://arxiv.org/abs/2505.21987</guid>
<content:encoded><![CDATA[
arXiv:2505.21987v1 Announce Type: new 
Abstract: With the rapid expansion of large language models (LLMs), the demand for memory and computational resources has grown significantly. Recent advances in LLM pruning aim to reduce the size and computational cost of these models. However, existing methods often suffer from either suboptimal pruning performance or low time efficiency during the pruning process. In this work, we propose an efficient and effective pruning method that simultaneously achieves high pruning performance and fast pruning speed with improved calibration efficiency. Our approach introduces two key innovations: (1) An activation cosine similarity loss-guided pruning metric, which considers the angular deviation of the output activation between the dense and pruned models. (2) An activation variance-guided pruning metric, which helps preserve semantic distinctions in output activations after pruning, enabling effective pruning with shorter input sequences. These two components can be readily combined to enhance LLM pruning in both accuracy and efficiency. Experimental results show that our method achieves up to an 18% reduction in perplexity and up to 63% decrease in pruning time on prevalent LLMs such as LLaMA, LLaMA-2, and OPT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Compact Spaces with Approximately Normalized Transformers</title>
<link>https://arxiv.org/abs/2505.22014</link>
<guid>https://arxiv.org/abs/2505.22014</guid>
<content:encoded><![CDATA[
arXiv:2505.22014v1 Announce Type: new 
Abstract: In deep learning, regularization and normalization are common solutions for challenges such as overfitting, numerical instabilities, and the increasing variance in the residual stream. An alternative approach is to force all parameters and representations to lie on a hypersphere. This removes the need for regularization and increases convergence speed, but comes with additional costs. In this work, we propose a more holistic but approximate normalization (anTransformer). Our approach constrains the norm of parameters and normalizes all representations via scalar multiplications motivated by the tight concentration of the norms of high-dimensional random vectors. When applied to GPT training, we observe a 40% faster convergence compared to models with QK normalization, with less than 3% additional runtime. Deriving scaling laws for anGPT, we found our method enables training with larger batch sizes and fewer hyperparameters, while matching the favorable scaling characteristics of classic GPT architectures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Weakly-Supervised Contrastive Learning for Imprecise Class Labels</title>
<link>https://arxiv.org/abs/2505.22028</link>
<guid>https://arxiv.org/abs/2505.22028</guid>
<content:encoded><![CDATA[
arXiv:2505.22028v1 Announce Type: new 
Abstract: Contrastive learning has achieved remarkable success in learning effective representations, with supervised contrastive learning often outperforming self-supervised approaches. However, in real-world scenarios, data annotations are often ambiguous or inaccurate, meaning that class labels may not reliably indicate whether two examples belong to the same class. This limitation restricts the applicability of supervised contrastive learning. To address this challenge, we introduce the concept of ``continuous semantic similarity'' to define positive and negative pairs. Instead of directly relying on imprecise class labels, we measure the semantic similarity between example pairs, which quantifies how closely they belong to the same category by iteratively refining weak supervisory signals. Based on this concept, we propose a graph-theoretic framework for weakly-supervised contrastive learning, where semantic similarity serves as the graph weights. Our framework is highly versatile and can be applied to many weakly-supervised learning scenarios. We demonstrate its effectiveness through experiments in two common settings, i.e., noisy label and partial label learning, where existing methods can be easily integrated to significantly improve performance. Theoretically, we establish an error bound for our approach, showing that it can approximate supervised contrastive learning under mild conditions. The implementation code is available at https://github.com/Speechless-10308/WSC.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Undesired Process Behavior by Means of Retrieval Augmented Generation</title>
<link>https://arxiv.org/abs/2505.22041</link>
<guid>https://arxiv.org/abs/2505.22041</guid>
<content:encoded><![CDATA[
arXiv:2505.22041v1 Announce Type: new 
Abstract: Conformance checking techniques detect undesired process behavior by comparing process executions that are recorded in event logs to desired behavior that is captured in a dedicated process model. If such models are not available, conformance checking techniques are not applicable, but organizations might still be interested in detecting undesired behavior in their processes. To enable this, existing approaches use Large Language Models (LLMs), assuming that they can learn to distinguish desired from undesired behavior through fine-tuning. However, fine-tuning is highly resource-intensive and the fine-tuned LLMs often do not generalize well. To address these limitations, we propose an approach that requires neither a dedicated process model nor resource-intensive fine-tuning to detect undesired process behavior. Instead, we use Retrieval Augmented Generation (RAG) to provide an LLM with direct access to a knowledge base that contains both desired and undesired process behavior from other processes, assuming that the LLM can transfer this knowledge to the process at hand. Our evaluation shows that our approach outperforms fine-tuned LLMs in detecting undesired behavior, demonstrating that RAG is a viable alternative to resource-intensive fine-tuning, particularly when enriched with relevant context from the event log, such as frequent traces and activities.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating the Effects of Sample Training Orders for Large Language Models without Retraining</title>
<link>https://arxiv.org/abs/2505.22042</link>
<guid>https://arxiv.org/abs/2505.22042</guid>
<content:encoded><![CDATA[
arXiv:2505.22042v1 Announce Type: new 
Abstract: The order of training samples plays a crucial role in large language models (LLMs), significantly impacting both their external performance and internal learning dynamics. Traditional methods for investigating this effect generally require retraining the model with various sample orders, which is computationally infeasible for LLMs. In this work, we improve traditional methods by designing a retraining-free framework. By approximating Adam optimizer updates with first- and second-order Taylor expansions and utilizing random projection methods to store intermediate checkpoints, our framework can efficiently estimate model parameters for arbitrary training sample orders. Next, we apply our framework to two downstream research problems: (1) Training curriculum design for LLMs -- we base our retraining-free framework to propose a novel curriculum learning strategy that augments curriculum proposals with estimated model performances, enabling more informed sample scheduling. (2) LLMs' memorization and generalization effect analysis -- we use our retraining-free framework to estimate how the positions of training samples influence LLMs' capacity for memorization and generalization. We conduct extensive experiments to validate the effectiveness of our retraining-free framework in reproducing the true model performances, and further demonstrate its potential in optimizing LLM training curricula and analyzing the memorization and generalization effects of LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentiable Generalized Sliced Wasserstein Plans</title>
<link>https://arxiv.org/abs/2505.22049</link>
<guid>https://arxiv.org/abs/2505.22049</guid>
<content:encoded><![CDATA[
arXiv:2505.22049v1 Announce Type: new 
Abstract: Optimal Transport (OT) has attracted significant interest in the machine learning community, not only for its ability to define meaningful distances between probability distributions -- such as the Wasserstein distance -- but also for its formulation of OT plans. Its computational complexity remains a bottleneck, though, and slicing techniques have been developed to scale OT to large datasets. Recently, a novel slicing scheme, dubbed min-SWGG, lifts a single one-dimensional plan back to the original multidimensional space, finally selecting the slice that yields the lowest Wasserstein distance as an approximation of the full OT plan. Despite its computational and theoretical advantages, min-SWGG inherits typical limitations of slicing methods: (i) the number of required slices grows exponentially with the data dimension, and (ii) it is constrained to linear projections. Here, we reformulate min-SWGG as a bilevel optimization problem and propose a differentiable approximation scheme to efficiently identify the optimal slice, even in high-dimensional settings. We furthermore define its generalized extension for accommodating to data living on manifolds. Finally, we demonstrate the practical value of our approach in various applications, including gradient flows on manifolds and high-dimensional spaces, as well as a novel sliced OT-based conditional flow matching for image generation -- where fast computation of transport plans is essential.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Resurrection of the ReLU</title>
<link>https://arxiv.org/abs/2505.22074</link>
<guid>https://arxiv.org/abs/2505.22074</guid>
<content:encoded><![CDATA[
arXiv:2505.22074v1 Announce Type: new 
Abstract: Modeling sophisticated activation functions within deep learning architectures has evolved into a distinct research direction. Functions such as GELU, SELU, and SiLU offer smooth gradients and improved convergence properties, making them popular choices in state-of-the-art models. Despite this trend, the classical ReLU remains appealing due to its simplicity, inherent sparsity, and other advantageous topological characteristics. However, ReLU units are prone to becoming irreversibly inactive - a phenomenon known as the dying ReLU problem - which limits their overall effectiveness. In this work, we introduce surrogate gradient learning for ReLU (SUGAR) as a novel, plug-and-play regularizer for deep architectures. SUGAR preserves the standard ReLU function during the forward pass but replaces its derivative in the backward pass with a smooth surrogate that avoids zeroing out gradients. We demonstrate that SUGAR, when paired with a well-chosen surrogate function, substantially enhances generalization performance over convolutional network architectures such as VGG-16 and ResNet-18, providing sparser activations while effectively resurrecting dead ReLUs. Moreover, we show that even in modern architectures like Conv2NeXt and Swin Transformer - which typically employ GELU - substituting these with SUGAR yields competitive and even slightly superior performance. These findings challenge the prevailing notion that advanced activation functions are necessary for optimal performance. Instead, they suggest that the conventional ReLU, particularly with appropriate gradient handling, can serve as a strong, versatile revived classic across a broad range of deep learning vision models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Test-time Computation Mitigate Memorization Bias in Neural Symbolic Regression?</title>
<link>https://arxiv.org/abs/2505.22081</link>
<guid>https://arxiv.org/abs/2505.22081</guid>
<content:encoded><![CDATA[
arXiv:2505.22081v1 Announce Type: new 
Abstract: Symbolic regression aims to discover mathematical equations that fit given numerical data. It has been applied in various fields of scientific research, such as producing human-readable expressions that explain physical phenomena. Recently, Neural symbolic regression (NSR) methods that involve Transformers pre-trained on large-scale synthetic datasets have gained attention. While these methods offer advantages such as short inference time, they suffer from low performance, particularly when the number of input variables is large. In this study, we hypothesized that this limitation stems from the memorization bias of Transformers in symbolic regression. We conducted a quantitative evaluation of this bias in Transformers using a synthetic dataset and found that Transformers rarely generate expressions not present in the training data. Additional theoretical analysis reveals that this bias arises from the Transformer's inability to construct expressions compositionally while verifying their numerical validity. We finally examined if tailoring test-time strategies can lead to reduced memorization bias and better performance. We empirically demonstrate that providing additional information to the model at test time can significantly mitigate memorization bias. On the other hand, we also find that reducing memorization bias does not necessarily correlate with improved performance. These findings contribute to a deeper understanding of the limitations of NSR approaches and offer a foundation for designing more robust, generalizable symbolic regression methods. Code is available at https://github.com/Shun-0922/Mem-Bias-NSR .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inclusive, Differentially Private Federated Learning for Clinical Data</title>
<link>https://arxiv.org/abs/2505.22108</link>
<guid>https://arxiv.org/abs/2505.22108</guid>
<content:encoded><![CDATA[
arXiv:2505.22108v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a promising approach for training clinical AI models without centralizing sensitive patient data. However, its real-world adoption is hindered by challenges related to privacy, resource constraints, and compliance. Existing Differential Privacy (DP) approaches often apply uniform noise, which disproportionately degrades model performance, even among well-compliant institutions. In this work, we propose a novel compliance-aware FL framework that enhances DP by adaptively adjusting noise based on quantifiable client compliance scores. Additionally, we introduce a compliance scoring tool based on key healthcare and security standards to promote secure, inclusive, and equitable participation across diverse clinical settings. Extensive experiments on public datasets demonstrate that integrating under-resourced, less compliant clinics with highly regulated institutions yields accuracy improvements of up to 15% over traditional FL. This work advances FL by balancing privacy, compliance, and performance, making it a viable solution for real-world clinical workflows in global healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The quest for the GRAph Level autoEncoder (GRALE)</title>
<link>https://arxiv.org/abs/2505.22109</link>
<guid>https://arxiv.org/abs/2505.22109</guid>
<content:encoded><![CDATA[
arXiv:2505.22109v1 Announce Type: new 
Abstract: Although graph-based learning has attracted a lot of attention, graph representation learning is still a challenging task whose resolution may impact key application fields such as chemistry or biology. To this end, we introduce GRALE, a novel graph autoencoder that encodes and decodes graphs of varying sizes into a shared embedding space. GRALE is trained using an Optimal Transport-inspired loss that compares the original and reconstructed graphs and leverages a differentiable node matching module, which is trained jointly with the encoder and decoder. The proposed attention-based architecture relies on Evoformer, the core component of AlphaFold, which we extend to support both graph encoding and decoding. We show, in numerical experiments on simulated and molecular data, that GRALE enables a highly general form of pre-training, applicable to a wide range of downstream tasks, from classification and regression to more complex tasks such as graph interpolation, editing, matching, and prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiMi Sheets: Infosheets for bias mitigation methods</title>
<link>https://arxiv.org/abs/2505.22114</link>
<guid>https://arxiv.org/abs/2505.22114</guid>
<content:encoded><![CDATA[
arXiv:2505.22114v1 Announce Type: new 
Abstract: Over the past 15 years, hundreds of bias mitigation methods have been proposed in the pursuit of fairness in machine learning (ML). However, algorithmic biases are domain-, task-, and model-specific, leading to a `portability trap': bias mitigation solutions in one context may not be appropriate in another. Thus, a myriad of design choices have to be made when creating a bias mitigation method, such as the formalization of fairness it pursues, and where and how it intervenes in the ML pipeline. This creates challenges in benchmarking and comparing the relative merits of different bias mitigation methods, and limits their uptake by practitioners.
  We propose BiMi Sheets as a portable, uniform guide to document the design choices of any bias mitigation method. This enables researchers and practitioners to quickly learn its main characteristics and to compare with their desiderata. Furthermore, the sheets' structure allow for the creation of a structured database of bias mitigation methods. In order to foster the sheets' adoption, we provide a platform for finding and creating BiMi Sheets at bimisheet.com.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Oryx: a Performant and Scalable Algorithm for Many-Agent Coordination in Offline MARL</title>
<link>https://arxiv.org/abs/2505.22151</link>
<guid>https://arxiv.org/abs/2505.22151</guid>
<content:encoded><![CDATA[
arXiv:2505.22151v1 Announce Type: new 
Abstract: A key challenge in offline multi-agent reinforcement learning (MARL) is achieving effective many-agent multi-step coordination in complex environments. In this work, we propose Oryx, a novel algorithm for offline cooperative MARL to directly address this challenge. Oryx adapts the recently proposed retention-based architecture Sable and combines it with a sequential form of implicit constraint Q-learning (ICQ), to develop a novel offline auto-regressive policy update scheme. This allows Oryx to solve complex coordination challenges while maintaining temporal coherence over lengthy trajectories. We evaluate Oryx across a diverse set of benchmarks from prior works (SMAC, RWARE, and Multi-Agent MuJoCo) covering tasks of both discrete and continuous control, varying in scale and difficulty. Oryx achieves state-of-the-art performance on more than 80% of the 65 tested datasets, outperforming prior offline MARL methods and demonstrating robust generalisation across domains with many agents and long horizons. Finally, we introduce new datasets to push the limits of many-agent coordination in offline MARL, and demonstrate Oryx's superior ability to scale effectively in such settings. We will make all of our datasets, experimental data, and code available upon publication.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Estimation for Heterophilic Graphs Through the Lens of Information Theory</title>
<link>https://arxiv.org/abs/2505.22152</link>
<guid>https://arxiv.org/abs/2505.22152</guid>
<content:encoded><![CDATA[
arXiv:2505.22152v1 Announce Type: new 
Abstract: While uncertainty estimation for graphs recently gained traction, most methods rely on homophily and deteriorate in heterophilic settings. We address this by analyzing message passing neural networks from an information-theoretic perspective and developing a suitable analog to data processing inequality to quantify information throughout the model's layers. In contrast to non-graph domains, information about the node-level prediction target can increase with model depth if a node's features are semantically different from its neighbors. Therefore, on heterophilic graphs, the latent embeddings of an MPNN each provide different information about the data distribution - different from homophilic settings. This reveals that considering all node representations simultaneously is a key design principle for epistemic uncertainty estimation on graphs beyond homophily. We empirically confirm this with a simple post-hoc density estimator on the joint node embedding space that provides state-of-the-art uncertainty on heterophilic graphs. At the same time, it matches prior work on homophilic graphs without explicitly exploiting homophily through post-processing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The informativeness of the gradient revisited</title>
<link>https://arxiv.org/abs/2505.22158</link>
<guid>https://arxiv.org/abs/2505.22158</guid>
<content:encoded><![CDATA[
arXiv:2505.22158v1 Announce Type: new 
Abstract: In the past decade gradient-based deep learning has revolutionized several applications. However, this rapid advancement has highlighted the need for a deeper theoretical understanding of its limitations. Research has shown that, in many practical learning tasks, the information contained in the gradient is so minimal that gradient-based methods require an exceedingly large number of iterations to achieve success. The informativeness of the gradient is typically measured by its variance with respect to the random selection of a target function from a hypothesis class.
  We use this framework and give a general bound on the variance in terms of a parameter related to the pairwise independence of the target function class and the collision entropy of the input distribution. Our bound scales as $ \tilde{\mathcal{O}}(\varepsilon+e^{-\frac{1}{2}\mathcal{E}_c}) $, where $ \tilde{\mathcal{O}} $ hides factors related to the regularity of the learning model and the loss function, $ \varepsilon $ measures the pairwise independence of the target function class and $\mathcal{E}_c$ is the collision entropy of the input distribution.
  To demonstrate the practical utility of our bound, we apply it to the class of Learning with Errors (LWE) mappings and high-frequency functions. In addition to the theoretical analysis, we present experiments to understand better the nature of recent deep learning-based attacks on LWE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Augmentation-Aware Theory for Self-Supervised Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.22196</link>
<guid>https://arxiv.org/abs/2505.22196</guid>
<content:encoded><![CDATA[
arXiv:2505.22196v1 Announce Type: new 
Abstract: Self-supervised contrastive learning has emerged as a powerful tool in machine learning and computer vision to learn meaningful representations from unlabeled data. Meanwhile, its empirical success has encouraged many theoretical studies to reveal the learning mechanisms. However, in the existing theoretical research, the role of data augmentation is still under-exploited, especially the effects of specific augmentation types. To fill in the blank, we for the first time propose an augmentation-aware error bound for self-supervised contrastive learning, showing that the supervised risk is bounded not only by the unsupervised risk, but also explicitly by a trade-off induced by data augmentation. Then, under a novel semantic label assumption, we discuss how certain augmentation methods affect the error bound. Lastly, we conduct both pixel- and representation-level experiments to verify our proposed theoretical results.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Uncertainty Estimation and Interpretability via Bayesian Non-negative Decision Layer</title>
<link>https://arxiv.org/abs/2505.22199</link>
<guid>https://arxiv.org/abs/2505.22199</guid>
<content:encoded><![CDATA[
arXiv:2505.22199v1 Announce Type: new 
Abstract: Although deep neural networks have demonstrated significant success due to their powerful expressiveness, most models struggle to meet practical requirements for uncertainty estimation. Concurrently, the entangled nature of deep neural networks leads to a multifaceted problem, where various localized explanation techniques reveal that multiple unrelated features influence the decisions, thereby undermining interpretability. To address these challenges, we develop a Bayesian Non-negative Decision Layer (BNDL), which reformulates deep neural networks as a conditional Bayesian non-negative factor analysis. By leveraging stochastic latent variables, the BNDL can model complex dependencies and provide robust uncertainty estimation. Moreover, the sparsity and non-negativity of the latent variables encourage the model to learn disentangled representations and decision layers, thereby improving interpretability. We also offer theoretical guarantees that BNDL can achieve effective disentangled learning. In addition, we developed a corresponding variational inference method utilizing a Weibull variational inference network to approximate the posterior distribution of the latent variables. Our experimental results demonstrate that with enhanced disentanglement capabilities, BNDL not only improves the model's accuracy but also provides reliable uncertainty estimation and improved interpretability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pitfalls of Rule- and Model-based Verifiers -- A Case Study on Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22203</link>
<guid>https://arxiv.org/abs/2505.22203</guid>
<content:encoded><![CDATA[
arXiv:2505.22203v1 Announce Type: new 
Abstract: Trustworthy verifiers are essential for the success of reinforcement learning with verifiable reward (RLVR), which is the core methodology behind various large reasoning models such as DeepSeek-R1. In complex domains like mathematical reasoning, rule-based verifiers have been widely adopted in previous works to train strong reasoning models. However, the reliability of these verifiers and their impact on the RL training process remain poorly understood. In this work, we take mathematical reasoning as a case study and conduct a comprehensive analysis of various verifiers in both static evaluation and RL training scenarios. First, we find that current open-source rule-based verifiers often fail to recognize equivalent answers presented in different formats across multiple commonly used mathematical datasets, resulting in non-negligible false negative rates. This limitation adversely affects RL training performance and becomes more pronounced as the policy model gets stronger. Subsequently, we investigate model-based verifiers as a potential solution to address these limitations. While the static evaluation shows that model-based verifiers achieve significantly higher verification accuracy, further analysis and RL training results imply that they are highly susceptible to hacking, where they misclassify certain patterns in responses as correct (i.e., false positives). This vulnerability is exploited during policy model optimization, leading to artificially inflated rewards. Our findings underscore the unique risks inherent to both rule-based and model-based verifiers, aiming to offer valuable insights to develop more robust reward systems in reinforcement learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LaMM: Semi-Supervised Pre-Training of Large-Scale Materials Models</title>
<link>https://arxiv.org/abs/2505.22208</link>
<guid>https://arxiv.org/abs/2505.22208</guid>
<content:encoded><![CDATA[
arXiv:2505.22208v1 Announce Type: new 
Abstract: Neural network potentials (NNPs) are crucial for accelerating computational materials science by surrogating density functional theory (DFT) calculations. Improving their accuracy is possible through pre-training and fine-tuning, where an NNP model is first pre-trained on a large-scale dataset and then fine-tuned on a smaller target dataset. However, this approach is computationally expensive, mainly due to the cost of DFT-based dataset labeling and load imbalances during large-scale pre-training. To address this, we propose LaMM, a semi-supervised pre-training method incorporating improved denoising self-supervised learning and a load-balancing algorithm for efficient multi-node training. We demonstrate that our approach effectively leverages a large-scale dataset of $\sim$300 million semi-labeled samples to train a single NNP model, resulting in improved fine-tuning performance in terms of both speed and accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solver-Free Decision-Focused Learning for Linear Optimization Problems</title>
<link>https://arxiv.org/abs/2505.22224</link>
<guid>https://arxiv.org/abs/2505.22224</guid>
<content:encoded><![CDATA[
arXiv:2505.22224v1 Announce Type: new 
Abstract: Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal kernel regression bounds under energy-bounded noise</title>
<link>https://arxiv.org/abs/2505.22235</link>
<guid>https://arxiv.org/abs/2505.22235</guid>
<content:encoded><![CDATA[
arXiv:2505.22235v1 Announce Type: new 
Abstract: Non-conservative uncertainty bounds are key for both assessing an estimation algorithm's accuracy and in view of downstream tasks, such as its deployment in safety-critical contexts. In this paper, we derive a tight, non-asymptotic uncertainty bound for kernel-based estimation, which can also handle correlated noise sequences. Its computation relies on a mild norm-boundedness assumption on the unknown function and the noise, returning the worst-case function realization within the hypothesis class at an arbitrary query input location. The value of this function is shown to be given in terms of the posterior mean and covariance of a Gaussian process for an optimal choice of the measurement noise covariance. By rigorously analyzing the proposed approach and comparing it with other results in the literature, we show its effectiveness in returning tight and easy-to-compute bounds for kernel-based estimates.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-XAIC Dataset: Benchmarking Explainable AI for Graph Neural Networks Using Chemical Data</title>
<link>https://arxiv.org/abs/2505.22252</link>
<guid>https://arxiv.org/abs/2505.22252</guid>
<content:encoded><![CDATA[
arXiv:2505.22252v1 Announce Type: new 
Abstract: Understanding the reasoning behind deep learning model predictions is crucial in cheminformatics and drug discovery, where molecular design determines their properties. However, current evaluation frameworks for Explainable AI (XAI) in this domain often rely on artificial datasets or simplified tasks, employing data-derived metrics that fail to capture the complexity of real-world scenarios and lack a direct link to explanation faithfulness. To address this, we introduce B-XAIC, a novel benchmark constructed from real-world molecular data and diverse tasks with known ground-truth rationales for assigned labels. Through a comprehensive evaluation using B-XAIC, we reveal limitations of existing XAI methods for Graph Neural Networks (GNNs) in the molecular domain. This benchmark provides a valuable resource for gaining deeper insights into the faithfulness of XAI, facilitating the development of more reliable and interpretable models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Unified Online-Offline Framework for Co-Branding Campaign Recommendations</title>
<link>https://arxiv.org/abs/2505.22254</link>
<guid>https://arxiv.org/abs/2505.22254</guid>
<content:encoded><![CDATA[
arXiv:2505.22254v1 Announce Type: new 
Abstract: Co-branding has become a vital strategy for businesses aiming to expand market reach within recommendation systems. However, identifying effective cross-industry partnerships remains challenging due to resource imbalances, uncertain brand willingness, and ever-changing market conditions. In this paper, we provide the first systematic study of this problem and propose a unified online-offline framework to enable co-branding recommendations. Our approach begins by constructing a bipartite graph linking ``initiating'' and ``target'' brands to quantify co-branding probabilities and assess market benefits. During the online learning phase, we dynamically update the graph in response to market feedback, while striking a balance between exploring new collaborations for long-term gains and exploiting established partnerships for immediate benefits. To address the high initial co-branding costs, our framework mitigates redundant exploration, thereby enhancing short-term performance while ensuring sustainable strategic growth. In the offline optimization phase, our framework consolidates the interests of multiple sub-brands under the same parent brand to maximize overall returns, avoid excessive investment in single sub-brands, and reduce unnecessary costs associated with over-prioritizing a single sub-brand. We present a theoretical analysis of our approach, establishing a highly nontrivial sublinear regret bound for online learning in the complex co-branding problem, and enhancing the approximation guarantee for the NP-hard offline budget allocation optimization. Experiments on both synthetic and real-world co-branding datasets demonstrate the practical effectiveness of our framework, with at least 12\% improvement.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train Sparse Autoencoders Efficiently by Utilizing Features Correlation</title>
<link>https://arxiv.org/abs/2505.22255</link>
<guid>https://arxiv.org/abs/2505.22255</guid>
<content:encoded><![CDATA[
arXiv:2505.22255v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have demonstrated significant promise in interpreting the hidden states of language models by decomposing them into interpretable latent directions. However, training SAEs at scale remains challenging, especially when large dictionary sizes are used. While decoders can leverage sparse-aware kernels for efficiency, encoders still require computationally intensive linear operations with large output dimensions. To address this, we propose KronSAE, a novel architecture that factorizes the latent representation via Kronecker product decomposition, drastically reducing memory and computational overhead. Furthermore, we introduce mAND, a differentiable activation function approximating the binary AND operation, which improves interpretability and performance in our factorized framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Group Relative Policy Optimization: Insights into On-Policy and Off-Policy Training</title>
<link>https://arxiv.org/abs/2505.22257</link>
<guid>https://arxiv.org/abs/2505.22257</guid>
<content:encoded><![CDATA[
arXiv:2505.22257v1 Announce Type: new 
Abstract: We revisit Group Relative Policy Optimization (GRPO) in both on-policy and off-policy optimization regimes. Our motivation comes from recent work on off-policy Proximal Policy Optimization (PPO), which improves training stability, sampling efficiency, and memory usage. In addition, a recent analysis of GRPO suggests that estimating the advantage function with off-policy samples could be beneficial. Building on these observations, we adapt GRPO to the off-policy setting. We show that both on-policy and off-policy GRPO objectives yield an improvement in the reward. This result motivates the use of clipped surrogate objectives in the off-policy version of GRPO. We then compare the empirical performance of reinforcement learning with verifiable rewards in post-training using both GRPO variants. Our results show that off-policy GRPO either significantly outperforms or performs on par with its on-policy counterpart.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Full Domain Analysis in Fluid Dynamics</title>
<link>https://arxiv.org/abs/2505.22275</link>
<guid>https://arxiv.org/abs/2505.22275</guid>
<content:encoded><![CDATA[
arXiv:2505.22275v1 Announce Type: new 
Abstract: Novel techniques in evolutionary optimization, simulation and machine learning allow for a broad analysis of domains like fluid dynamics, in which computation is expensive and flow behavior is complex. Under the term of full domain analysis we understand the ability to efficiently determine the full space of solutions in a problem domain, and analyze the behavior of those solutions in an accessible and interactive manner. The goal of full domain analysis is to deepen our understanding of domains by generating many examples of flow, their diversification, optimization and analysis. We define a formal model for full domain analysis, its current state of the art, and requirements of subcomponents. Finally, an example is given to show what we can learn by using full domain analysis. Full domain analysis, rooted in optimization and machine learning, can be a helpful tool in understanding complex systems in computational physics and beyond.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Versatile Cardiovascular Signal Generation with a Unified Diffusion Transformer</title>
<link>https://arxiv.org/abs/2505.22306</link>
<guid>https://arxiv.org/abs/2505.22306</guid>
<content:encoded><![CDATA[
arXiv:2505.22306v1 Announce Type: new 
Abstract: Cardiovascular signals such as photoplethysmography (PPG), electrocardiography (ECG), and blood pressure (BP) are inherently correlated and complementary, together reflecting the health of cardiovascular system. However, their joint utilization in real-time monitoring is severely limited by diverse acquisition challenges from noisy wearable recordings to burdened invasive procedures. Here we propose UniCardio, a multi-modal diffusion transformer that reconstructs low-quality signals and synthesizes unrecorded signals in a unified generative framework. Its key innovations include a specialized model architecture to manage the signal modalities involved in generation tasks and a continual learning paradigm to incorporate varying modality combinations. By exploiting the complementary nature of cardiovascular signals, UniCardio clearly outperforms recent task-specific baselines in signal denoising, imputation, and translation. The generated signals match the performance of ground-truth signals in detecting abnormal health conditions and estimating vital signs, even in unseen domains, while ensuring interpretability for human experts. These advantages position UniCardio as a promising avenue for advancing AI-assisted healthcare.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transformers Pretrained on Procedural Data Contain Modular Structures for Algorithmic Reasoning</title>
<link>https://arxiv.org/abs/2505.22308</link>
<guid>https://arxiv.org/abs/2505.22308</guid>
<content:encoded><![CDATA[
arXiv:2505.22308v1 Announce Type: new 
Abstract: Pretraining on large, semantically rich datasets is key for developing language models. Surprisingly, recent studies have shown that even synthetic data, generated procedurally through simple semantic-free algorithms, can yield some of the same benefits as natural language pretraining. It is unclear what specific capabilities such simple synthetic data instils in a model, where these capabilities reside in the architecture, and how they manifest within its weights. In this short paper, we identify several beneficial forms of procedural data, together with specific algorithmic reasoning skills that improve in small transformers. Our core finding is that different procedural rules instil distinct but complementary inductive structures in the model. With extensive ablations and partial-transfer experiments, we discover that these structures reside in different parts of the model. Attention layers often carry the most transferable information, but some pretraining rules impart useful structure to MLP blocks instead. Most interestingly, the structures induced by multiple rules can be composed to jointly reinforce multiple capabilities. These results suggest an exciting possibility of disentangling the acquisition of knowledge from reasoning in language models, with the goal of improving their robustness and data efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization</title>
<link>https://arxiv.org/abs/2505.22310</link>
<guid>https://arxiv.org/abs/2505.22310</guid>
<content:encoded><![CDATA[
arXiv:2505.22310v1 Announce Type: new 
Abstract: Recent unlearning methods for LLMs are vulnerable to relearning attacks: knowledge believed-to-be-unlearned re-emerges by fine-tuning on a small set of (even seemingly-unrelated) examples. We study this phenomenon in a controlled setting for example-level unlearning in vision classifiers. We make the surprising discovery that forget-set accuracy can recover from around 50% post-unlearning to nearly 100% with fine-tuning on just the retain set -- i.e., zero examples of the forget set. We observe this effect across a wide variety of unlearning methods, whereas for a model retrained from scratch excluding the forget set (gold standard), the accuracy remains at 50%. We observe that resistance to relearning attacks can be predicted by weight-space properties, specifically, $L_2$-distance and linear mode connectivity between the original and the unlearned model. Leveraging this insight, we propose a new class of methods that achieve state-of-the-art resistance to relearning attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Skywork Open Reasoner 1 Technical Report</title>
<link>https://arxiv.org/abs/2505.22312</link>
<guid>https://arxiv.org/abs/2505.22312</guid>
<content:encoded><![CDATA[
arXiv:2505.22312v1 Announce Type: new 
Abstract: The success of DeepSeek-R1 underscores the significant role of reinforcement learning (RL) in enhancing the reasoning capabilities of large language models (LLMs). In this work, we present Skywork-OR1, an effective and scalable RL implementation for long Chain-of-Thought (CoT) models. Building on the DeepSeek-R1-Distill model series, our RL approach achieves notable performance gains, increasing average accuracy across AIME24, AIME25, and LiveCodeBench from 57.8% to 72.8% (+15.0%) for the 32B model and from 43.6% to 57.5% (+13.9%) for the 7B model. Our Skywork-OR1-32B model surpasses both DeepSeek-R1 and Qwen3-32B on the AIME24 and AIME25 benchmarks, while achieving comparable results on LiveCodeBench. The Skywork-OR1-7B and Skywork-OR1-Math-7B models demonstrate competitive reasoning capabilities among models of similar size. We perform comprehensive ablation studies on the core components of our training pipeline to validate their effectiveness. Additionally, we thoroughly investigate the phenomenon of entropy collapse, identify key factors affecting entropy dynamics, and demonstrate that mitigating premature entropy collapse is critical for improved test performance. To support community research, we fully open-source our model weights, training code, and training datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking BPS: A Utility-Based Evaluation Framework</title>
<link>https://arxiv.org/abs/2505.22316</link>
<guid>https://arxiv.org/abs/2505.22316</guid>
<content:encoded><![CDATA[
arXiv:2505.22316v1 Announce Type: new 
Abstract: Business process simulation (BPS) is a key tool for analyzing and optimizing organizational workflows, supporting decision-making by estimating the impact of process changes. The reliability of such estimates depends on the ability of a BPS model to accurately mimic the process under analysis, making rigorous accuracy evaluation essential. However, the state-of-the-art approach to evaluating BPS models has two key limitations. First, it treats simulation as a forecasting problem, testing whether models can predict unseen future events. This fails to assess how well a model captures the as-is process, particularly when process behavior changes from train to test period. Thus, it becomes difficult to determine whether poor results stem from an inaccurate model or the inherent complexity of the data, such as unpredictable drift. Second, the evaluation approach strongly relies on Earth Mover's Distance-based metrics, which can obscure temporal patterns and thus yield misleading conclusions about simulation quality. To address these issues, we propose a novel framework that evaluates simulation quality based on its ability to generate representative process behavior. Instead of comparing simulated logs to future real-world executions, we evaluate whether predictive process monitoring models trained on simulated data perform comparably to those trained on real data for downstream analysis tasks. Empirical results show that our framework not only helps identify sources of discrepancies but also distinguishes between model accuracy and data complexity, offering a more meaningful way to assess BPS quality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look on Memorization in Tabular Diffusion Model: A Data-Centric Perspective</title>
<link>https://arxiv.org/abs/2505.22322</link>
<guid>https://arxiv.org/abs/2505.22322</guid>
<content:encoded><![CDATA[
arXiv:2505.22322v1 Announce Type: new 
Abstract: Diffusion models have shown strong performance in generating high-quality tabular data, but they carry privacy risks by reproducing exact training samples. While prior work focuses on dataset-level augmentation to reduce memorization, little is known about which individual samples contribute most. We present the first data-centric study of memorization dynamics in tabular diffusion models. We quantify memorization for each real sample based on how many generated samples are flagged as replicas, using a relative distance ratio. Our empirical analysis reveals a heavy-tailed distribution of memorization counts: a small subset of samples contributes disproportionately to leakage, confirmed via sample-removal experiments. To understand this, we divide real samples into top- and non-top-memorized groups and analyze their training-time behaviors. We track when each sample is first memorized and monitor per-epoch memorization intensity (AUC). Memorized samples are memorized slightly earlier and show stronger signals in early training. Based on these insights, we propose DynamicCut, a two-stage, model-agnostic mitigation method: (a) rank samples by epoch-wise intensity, (b) prune a tunable top fraction, and (c) retrain on the filtered dataset. Across multiple tabular datasets and models, DynamicCut reduces memorization with minimal impact on data diversity and downstream performance. It also complements augmentation-based defenses. Furthermore, DynamicCut enables cross-model transferability: high-ranked samples identified from one model (e.g., a diffusion model) are also effective for reducing memorization when removed from others, such as GANs and VAEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Look Within or Look Beyond? A Theoretical Comparison Between Parameter-Efficient and Full Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.22355</link>
<guid>https://arxiv.org/abs/2505.22355</guid>
<content:encoded><![CDATA[
arXiv:2505.22355v1 Announce Type: new 
Abstract: Parameter-Efficient Fine-Tuning (PEFT) methods achieve performance comparable to Full Fine-Tuning (FFT) while requiring significantly fewer computing resources, making it the go-to choice for researchers. We find that although PEFT can achieve competitive results on some benchmarks, its performance falls short of FFT in complex tasks, such as reasoning and instruction-based fine-tuning. In this paper, we compare the characteristics of PEFT and FFT in terms of representational capacity and robustness based on optimization theory. We theoretically demonstrate that PEFT is a strict subset of FFT. By providing theoretical upper bounds for PEFT, we show that the limited parameter space constrains the model's representational ability, making it more susceptible to perturbations. Experiments on 15 datasets encompassing classification, generation, reasoning, instruction fine-tuning tasks and 11 adversarial test sets validate our theories. We hope that these results spark further research beyond the realms of well established PEFT. The source code is in the anonymous Github repository\footnote{https://github.com/misonsky/PEFTEval}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Suitability Filter: A Statistical Framework for Classifier Evaluation in Real-World Deployment Settings</title>
<link>https://arxiv.org/abs/2505.22356</link>
<guid>https://arxiv.org/abs/2505.22356</guid>
<content:encoded><![CDATA[
arXiv:2505.22356v1 Announce Type: new 
Abstract: Deploying machine learning models in safety-critical domains poses a key challenge: ensuring reliable model performance on downstream user data without access to ground truth labels for direct validation. We propose the suitability filter, a novel framework designed to detect performance deterioration by utilizing suitability signals -- model output features that are sensitive to covariate shifts and indicative of potential prediction errors. The suitability filter evaluates whether classifier accuracy on unlabeled user data shows significant degradation compared to the accuracy measured on the labeled test dataset. Specifically, it ensures that this degradation does not exceed a pre-specified margin, which represents the maximum acceptable drop in accuracy. To achieve reliable performance evaluation, we aggregate suitability signals for both test and user data and compare these empirical distributions using statistical hypothesis testing, thus providing insights into decision uncertainty. Our modular method adapts to various models and domains. Empirical evaluations across different classification tasks demonstrate that the suitability filter reliably detects performance deviations due to covariate shift. This enables proactive mitigation of potential failures in high-stakes applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs</title>
<link>https://arxiv.org/abs/2505.22358</link>
<guid>https://arxiv.org/abs/2505.22358</guid>
<content:encoded><![CDATA[
arXiv:2505.22358v1 Announce Type: new 
Abstract: Large language models (LLMs) often suffer from catastrophic forgetting in continual learning (CL) scenarios, where performance on previously learned tasks degrades severely while training on sequentially arriving tasks. Although pioneering CL approaches using orthogonal subspaces can mitigate task interference, they typically employ fixed budget allocation, neglecting the varying complexity across tasks and layers. Besides, recent budget-adaptive tuning methods for LLMs often adopt multi-stage paradigms that decouple optimization and budget allocation. Such decoupling results in potential misalignment, which hinders those approaches' practical application in CL scenarios. To address these limitations, we propose OA-Adapter, a novel parameter-efficient approach for continual learning in LLMs that unifies dynamic budget adaptation with orthogonal subspace learning in a single end-to-end training stage. Specifically, OA-Adapter introduces a dynamic bottleneck dimension adaptation mechanism that simultaneously allocates an efficient parameter budget and optimizes task objectives without misalignment. To effectively preserve previously acquired knowledge while coordinating with the dynamic budget allocation, orthogonal constraints are applied specifically between the parameter subspace of the current task and the dynamically allocated parameter subspaces of historical tasks. Experimental results on continual learning benchmarks demonstrate that OA-Adapter outperforms state-of-the-art methods in both accuracy and parameter efficiency, achieving higher average accuracy while using 58.5% fewer parameters on the standard CL benchmark.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiclass Loss Geometry Matters for Generalization of Gradient Descent in Separable Classification</title>
<link>https://arxiv.org/abs/2505.22359</link>
<guid>https://arxiv.org/abs/2505.22359</guid>
<content:encoded><![CDATA[
arXiv:2505.22359v1 Announce Type: new 
Abstract: We study the generalization performance of unregularized gradient methods for separable linear classification. While previous work mostly deal with the binary case, we focus on the multiclass setting with $k$ classes and establish novel population risk bounds for Gradient Descent for loss functions that decay to zero. In this setting, we show risk bounds that reveal that convergence rates are crucially influenced by the geometry of the loss template, as formalized by Wang and Scott (2024), rather than of the loss function itself. Particularly, we establish risk upper bounds that holds for any decay rate of the loss whose template is smooth with respect to the $p$-norm. In the case of exponentially decaying losses, our results indicates a contrast between the $p=\infty$ case, where the risk exhibits a logarithmic dependence on $k$, and $p=2$ where the risk scales linearly with $k$. To establish this separation formally, we also prove a lower bound in the latter scenario, demonstrating that the polynomial dependence on $k$ is unavoidable. Central to our analysis is a novel bound on the Rademacher complexity of low-noise vector-valued linear predictors with a loss template smooth w.r.t.~general $p$-norms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuum-armed Bandit Optimization with Batch Pairwise Comparison Oracles</title>
<link>https://arxiv.org/abs/2505.22361</link>
<guid>https://arxiv.org/abs/2505.22361</guid>
<content:encoded><![CDATA[
arXiv:2505.22361v1 Announce Type: new 
Abstract: This paper studies a bandit optimization problem where the goal is to maximize a function $f(x)$ over $T$ periods for some unknown strongly concave function $f$. We consider a new pairwise comparison oracle, where the decision-maker chooses a pair of actions $(x, x')$ for a consecutive number of periods and then obtains an estimate of $f(x)-f(x')$. We show that such a pairwise comparison oracle finds important applications to joint pricing and inventory replenishment problems and network revenue management. The challenge in this bandit optimization is twofold. First, the decision-maker not only needs to determine a pair of actions $(x, x')$ but also a stopping time $n$ (i.e., the number of queries based on $(x, x')$). Second, motivated by our inventory application, the estimate of the difference $f(x)-f(x')$ is biased, which is different from existing oracles in stochastic optimization literature. To address these challenges, we first introduce a discretization technique and local polynomial approximation to relate this problem to linear bandits. Then we developed a tournament successive elimination technique to localize the discretized cell and run an interactive batched version of LinUCB algorithm on cells. We establish regret bounds that are optimal up to poly-logarithmic factors. Furthermore, we apply our proposed algorithm and analytical framework to the two operations management problems and obtain results that improve state-of-the-art results in the existing literature.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Homophily-Aware Graph Neural Network</title>
<link>https://arxiv.org/abs/2505.22362</link>
<guid>https://arxiv.org/abs/2505.22362</guid>
<content:encoded><![CDATA[
arXiv:2505.22362v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have achieved significant success in various learning tasks on graph-structured data. Nevertheless, most GNNs struggle to generalize to heterophilic neighborhoods. Additionally, many GNNs ignore the directional nature of real-world graphs, resulting in suboptimal performance on directed graphs with asymmetric structures. In this work, we propose Directed Homophily-aware Graph Neural Network (DHGNN), a novel framework that addresses these limitations by incorporating homophily-aware and direction-sensitive components. DHGNN employs a resettable gating mechanism to adaptively modulate message contributions based on homophily levels and informativeness, and a structure-aware noise-tolerant fusion module to effectively integrate node representations from the original and reverse directions. Extensive experiments on both homophilic and heterophilic directed graph datasets demonstrate that DHGNN outperforms state-of-the-art methods in node classification and link prediction. In particular, DHGNN improves over the best baseline by up to 15.07% in link prediction. Our analysis further shows that the gating mechanism captures directional homophily gaps and fluctuating homophily across layers, providing deeper insights into message-passing behavior on complex graph structures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SplitLoRA: Balancing Stability and Plasticity in Continual Learning Through Gradient Space Splitting</title>
<link>https://arxiv.org/abs/2505.22370</link>
<guid>https://arxiv.org/abs/2505.22370</guid>
<content:encoded><![CDATA[
arXiv:2505.22370v1 Announce Type: new 
Abstract: Continual Learning requires a model to learn multiple tasks in sequence while maintaining both stability:preserving knowledge from previously learned tasks, and plasticity:effectively learning new tasks. Gradient projection has emerged as an effective and popular paradigm in CL, where it partitions the gradient space of previously learned tasks into two orthogonal subspaces: a primary subspace and a minor subspace. New tasks are learned effectively within the minor subspace, thereby reducing interference with previously acquired knowledge. However, existing Gradient Projection methods struggle to achieve an optimal balance between plasticity and stability, as it is hard to appropriately partition the gradient space. In this work, we consider a continual learning paradigm based on Low-Rank Adaptation, which has gained considerable attention due to its efficiency and wide applicability, and propose a novel approach for continual learning, called SplitLoRA. We first provide a theoretical analysis of how subspace partitioning affects model stability and plasticity. Informed by this analysis, we then introduce an effective method that derives the optimal partition of the gradient space for previously learned tasks. This approach effectively balances stability and plasticity in continual learning. Experimental results on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Divide-and-Conquer Approach for Modeling Arrival Times in Business Process Simulation</title>
<link>https://arxiv.org/abs/2505.22381</link>
<guid>https://arxiv.org/abs/2505.22381</guid>
<content:encoded><![CDATA[
arXiv:2505.22381v1 Announce Type: new 
Abstract: Business Process Simulation (BPS) is a critical tool for analyzing and improving organizational processes by estimating the impact of process changes. A key component of BPS is the case-arrival model, which determines the pattern of new case entries into a process. Although accurate case-arrival modeling is essential for reliable simulations, as it influences waiting and overall cycle times, existing approaches often rely on oversimplified static distributions of inter-arrival times. These approaches fail to capture the dynamic and temporal complexities inherent in organizational environments, leading to less accurate and reliable outcomes. To address this limitation, we propose Auto Time Kernel Density Estimation (AT-KDE), a divide-and-conquer approach that models arrival times of processes by incorporating global dynamics, day-of-week variations, and intraday distributional changes, ensuring both precision and scalability. Experiments conducted across 20 diverse processes demonstrate that AT-KDE is far more accurate and robust than existing approaches while maintaining sensible execution time efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning</title>
<link>https://arxiv.org/abs/2505.22389</link>
<guid>https://arxiv.org/abs/2505.22389</guid>
<content:encoded><![CDATA[
arXiv:2505.22389v1 Announce Type: new 
Abstract: Continual Learning (CL) aims to enable models to continuously acquire new knowledge from a sequence of tasks with avoiding the forgetting of learned information. However, existing CL methods only rely on the parameters of the most recent task for inference, which makes them susceptible to catastrophic forgetting. Inspired by the recent success of model merging techniques, we propose \textbf{Perturb-and-Merge (P\&amp;M)}, a novel continual learning framework that integrates model merging into the CL paradigm to mitigate forgetting. Specifically, after training on each task, P\&amp;M constructs a new model by forming a convex combination of the previous model and the newly trained task-specific model. Through theoretical analysis, we minimize the total loss increase across all tasks and derive an analytical solution for the optimal merging coefficient. To further improve the performance of the merged model, we observe that the degradation introduced during merging can be alleviated by a regularization term composed of the task vector and the Hessian matrix of the loss function. Interestingly, we show that this term can be efficiently approximated using second-order symmetric finite differences, and a stochastic perturbation strategy along the task vector direction is accordingly devised which incurs no additional forward or backward passes while providing an effective approximation of the regularization term. Finally, we combine P\&amp;M with LoRA, a parameter-efficient fine-tuning method, to reduce memory overhead. Our proposed approach achieves state-of-the-art performance on several continual learning benchmark datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-Informed Distillation of Diffusion Models for PDE-Constrained Generation</title>
<link>https://arxiv.org/abs/2505.22391</link>
<guid>https://arxiv.org/abs/2505.22391</guid>
<content:encoded><![CDATA[
arXiv:2505.22391v1 Announce Type: new 
Abstract: Modeling physical systems in a generative manner offers several advantages, including the ability to handle partial observations, generate diverse solutions, and address both forward and inverse problems. Recently, diffusion models have gained increasing attention in the modeling of physical systems, particularly those governed by partial differential equations (PDEs). However, diffusion models only access noisy data $\boldsymbol{x}_t$ at intermediate steps, making it infeasible to directly enforce constraints on the clean sample $\boldsymbol{x}_0$ at each noisy level. As a workaround, constraints are typically applied to the expectation of clean samples $\mathbb{E}[\boldsymbol{x}_0|\boldsymbol{x}_t]$, which is estimated using the learned score network. However, imposing PDE constraints on the expectation does not strictly represent the one on the true clean data, known as Jensen's Gap. This gap creates a trade-off: enforcing PDE constraints may come at the cost of reduced accuracy in generative modeling. To address this, we propose a simple yet effective post-hoc distillation approach, where PDE constraints are not injected directly into the diffusion process, but instead enforced during a post-hoc distillation stage. We term our method as Physics-Informed Distillation of Diffusion Models (PIDDM). This distillation not only facilitates single-step generation with improved PDE satisfaction, but also support both forward and inverse problem solving and reconstruction from randomly partial observation. Extensive experiments across various PDE benchmarks demonstrate that PIDDM significantly improves PDE satisfaction over several recent and competitive baselines, such as PIDM, DiffusionPDE, and ECI-sampling, with less computation overhead. Our approach can shed light on more efficient and effective strategies for incorporating physical constraints into diffusion models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Overthinking in Large Reasoning Models via Manifold Steering</title>
<link>https://arxiv.org/abs/2505.22411</link>
<guid>https://arxiv.org/abs/2505.22411</guid>
<content:encoded><![CDATA[
arXiv:2505.22411v1 Announce Type: new 
Abstract: Recent advances in Large Reasoning Models (LRMs) have demonstrated remarkable capabilities in solving complex tasks such as mathematics and coding. However, these models frequently exhibit a phenomenon known as overthinking during inference, characterized by excessive validation loops and redundant deliberation, leading to substantial computational overheads. In this paper, we aim to mitigate overthinking by investigating the underlying mechanisms from the perspective of mechanistic interpretability. We first showcase that the tendency of overthinking can be effectively captured by a single direction in the model's activation space and the issue can be eased by intervening the activations along this direction. However, this efficacy soon reaches a plateau and even deteriorates as the intervention strength increases. We therefore systematically explore the activation space and find that the overthinking phenomenon is actually tied to a low-dimensional manifold, which indicates that the limited effect stems from the noises introduced by the high-dimensional steering direction. Based on this insight, we propose Manifold Steering, a novel approach that elegantly projects the steering direction onto the low-dimensional activation manifold given the theoretical approximation of the interference noise. Extensive experiments on DeepSeek-R1 distilled models validate that our method reduces output tokens by up to 71% while maintaining and even improving the accuracy on several mathematical benchmarks. Our method also exhibits robust cross-domain transferability, delivering consistent token reduction performance in code generation and knowledge-based QA tasks. Code is available at: https://github.com/Aries-iai/Manifold_Steering.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STaR-Bets: Sequential Target-Recalculating Bets for Tighter Confidence Intervals</title>
<link>https://arxiv.org/abs/2505.22422</link>
<guid>https://arxiv.org/abs/2505.22422</guid>
<content:encoded><![CDATA[
arXiv:2505.22422v1 Announce Type: new 
Abstract: The construction of confidence intervals for the mean of a bounded random variable is a classical problem in statistics with numerous applications in machine learning and virtually all scientific fields. In particular, obtaining the tightest possible confidence intervals is vital every time the sampling of the random variables is expensive. The current state-of-the-art method to construct confidence intervals is by using betting algorithms. This is a very successful approach for deriving optimal confidence sequences, even matching the rate of law of iterated logarithms. However, in the fixed horizon setting, these approaches are either sub-optimal or based on heuristic solutions with strong empirical performance but without a finite-time guarantee. Hence, no betting-based algorithm guaranteeing the optimal $\mathcal{O}(\sqrt{\frac{\sigma^2\log\frac1\delta}{n}})$ width of the confidence intervals are known. This work bridges this gap. We propose a betting-based algorithm to compute confidence intervals that empirically outperforms the competitors. Our betting strategy uses the optimal strategy in every step (in a certain sense), whereas the standard betting methods choose a constant strategy in advance. Leveraging this fact results in strict improvements even for classical concentration inequalities, such as the ones of Hoeffding or Bernstein. Moreover, we also prove that the width of our confidence intervals is optimal up to an $1+o(1)$ factor diminishing with $n$. The code is available on~https://github.com/vvoracek/STaR-bets-confidence-interval.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Reasoning without Attention</title>
<link>https://arxiv.org/abs/2505.22425</link>
<guid>https://arxiv.org/abs/2505.22425</guid>
<content:encoded><![CDATA[
arXiv:2505.22425v1 Announce Type: new 
Abstract: Large language models (LLMs) have made significant advances in complex reasoning tasks, yet they remain bottlenecked by two core challenges: architectural inefficiency due to reliance on Transformers, and a lack of structured fine-tuning for high-difficulty domains. We introduce \ourmodel, an attention-free language model that addresses both issues through architectural and data-centric innovations. Built on the state space dual (SSD) layers of Mamba-2, our model eliminates the need for self-attention and key-value caching, enabling fixed-memory, constant-time inference. To train it for complex reasoning, we propose a two-phase curriculum fine-tuning strategy based on the \textsc{PromptCoT} synthesis paradigm, which generates pedagogically structured problems via abstract concept selection and rationale-guided generation. On benchmark evaluations, \ourmodel-7B outperforms strong Transformer and hybrid models of comparable scale, and even surpasses the much larger Gemma3-27B by 2.6\% on AIME 24, 0.6\% on AIME 25, and 3.0\% on Livecodebench. These results highlight the potential of state space models as efficient and scalable alternatives to attention-based architectures for high-capacity reasoning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Data-Driven Antenna Miniaturization: A Knowledge-Based System Integrating Quantum PSO and Predictive Machine Learning Models</title>
<link>https://arxiv.org/abs/2505.22440</link>
<guid>https://arxiv.org/abs/2505.22440</guid>
<content:encoded><![CDATA[
arXiv:2505.22440v1 Announce Type: new 
Abstract: The rapid evolution of wireless technologies necessitates automated design frameworks to address antenna miniaturization and performance optimization within constrained development cycles. This study demonstrates a machine learning enhanced workflow integrating Quantum-Behaved Dynamic Particle Swarm Optimization (QDPSO) with ANSYS HFSS simulations to accelerate antenna design. The QDPSO algorithm autonomously optimized loop dimensions in 11.53 seconds, achieving a resonance frequency of 1.4208 GHz a 12.7 percent reduction compared to conventional 1.60 GHz designs. Machine learning models (SVM, Random Forest, XGBoost, and Stacked ensembles) predicted resonance frequencies in 0.75 seconds using 936 simulation datasets, with stacked models showing superior training accuracy (R2=0.9825) and SVM demonstrating optimal validation performance (R2=0.7197). The complete design cycle, encompassing optimization, prediction, and ANSYS validation, required 12.42 minutes on standard desktop hardware (Intel i5-8500, 16GB RAM), contrasting sharply with the 50-hour benchmark of PSADEA-based approaches. This 240 times of acceleration eliminates traditional trial-and-error methods that often extend beyond seven expert-led days. The system enables precise specifications of performance targets with automated generation of fabrication-ready parameters, particularly benefiting compact consumer devices requiring rapid frequency tuning. By bridging AI-driven optimization with CAD validation, this framework reduces engineering workloads while ensuring production-ready designs, establishing a scalable paradigm for next-generation RF systems in 6G and IoT applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SOReL and TOReL: Two Methods for Fully Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22442</link>
<guid>https://arxiv.org/abs/2505.22442</guid>
<content:encoded><![CDATA[
arXiv:2505.22442v1 Announce Type: new 
Abstract: Sample efficiency remains a major obstacle for real world adoption of reinforcement learning (RL): success has been limited to settings where simulators provide access to essentially unlimited environment interactions, which in reality are typically costly or dangerous to obtain. Offline RL in principle offers a solution by exploiting offline data to learn a near-optimal policy before deployment. In practice, however, current offline RL methods rely on extensive online interactions for hyperparameter tuning, and have no reliable bound on their initial online performance. To address these two issues, we introduce two algorithms. Firstly, SOReL: an algorithm for safe offline reinforcement learning. Using only offline data, our Bayesian approach infers a posterior over environment dynamics to obtain a reliable estimate of the online performance via the posterior predictive uncertainty. Crucially, all hyperparameters are also tuned fully offline. Secondly, we introduce TOReL: a tuning for offline reinforcement learning algorithm that extends our information rate based offline hyperparameter tuning methods to general offline RL approaches. Our empirical evaluation confirms SOReL's ability to accurately estimate regret in the Bayesian setting whilst TOReL's offline hyperparameter tuning achieves competitive performance with the best online hyperparameter tuning methods using only offline data. Thus, SOReL and TOReL make a significant step towards safe and reliable offline RL, unlocking the potential for RL in the real world. Our implementations are publicly available: https://github.com/CWibault/sorel\_torel.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: All Current Generative Fidelity and Diversity Metrics are Flawed</title>
<link>https://arxiv.org/abs/2505.22450</link>
<guid>https://arxiv.org/abs/2505.22450</guid>
<content:encoded><![CDATA[
arXiv:2505.22450v1 Announce Type: new 
Abstract: Any method's development and practical application is limited by our ability to measure its reliability. The popularity of generative modeling emphasizes the importance of good synthetic data metrics. Unfortunately, previous works have found many failure cases in current metrics, for example lack of outlier robustness and unclear lower and upper bounds. We propose a list of desiderata for synthetic data metrics, and a suite of sanity checks: carefully chosen simple experiments that aim to detect specific and known generative modeling failure modes. Based on these desiderata and the results of our checks, we arrive at our position: all current generative fidelity and diversity metrics are flawed. This significantly hinders practical use of synthetic data. Our aim is to convince the research community to spend more effort in developing metrics, instead of models. Additionally, through analyzing how current metrics fail, we provide practitioners with guidelines on how these metrics should (not) be used.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pure Exploration with Infinite Answers</title>
<link>https://arxiv.org/abs/2505.22473</link>
<guid>https://arxiv.org/abs/2505.22473</guid>
<content:encoded><![CDATA[
arXiv:2505.22473v1 Announce Type: new 
Abstract: We study pure exploration problems where the set of correct answers is possibly infinite, e.g., the regression of any continuous function of the means of the bandit. We derive an instance-dependent lower bound for these problems. By analyzing it, we discuss why existing methods (i.e., Sticky Track-and-Stop) for finite answer problems fail at being asymptotically optimal in this more general setting. Finally, we present a framework, Sticky-Sequence Track-and-Stop, which generalizes both Track-and-Stop and Sticky Track-and-Stop, and that enjoys asymptotic optimality. Due to its generality, our analysis also highlights special cases where existing methods enjoy optimality.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Forecasting Multivariate Urban Data via Decomposition and Spatio-Temporal Graph Analysis</title>
<link>https://arxiv.org/abs/2505.22474</link>
<guid>https://arxiv.org/abs/2505.22474</guid>
<content:encoded><![CDATA[
arXiv:2505.22474v1 Announce Type: new 
Abstract: The forecasting of multivariate urban data presents a complex challenge due to the intricate dependencies between various urban metrics such as weather, air pollution, carbon intensity, and energy demand. This paper introduces a novel multivariate time-series forecasting model that utilizes advanced Graph Neural Networks (GNNs) to capture spatial dependencies among different time-series variables. The proposed model incorporates a decomposition-based preprocessing step, isolating trend, seasonal, and residual components to enhance the accuracy and interpretability of forecasts. By leveraging the dynamic capabilities of GNNs, the model effectively captures interdependencies and improves the forecasting performance. Extensive experiments on real-world datasets, including electricity usage, weather metrics, carbon intensity, and air pollution data, demonstrate the effectiveness of the proposed approach across various forecasting scenarios. The results highlight the potential of the model to optimize smart infrastructure systems, contributing to energy-efficient urban development and enhanced public well-being.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Asymptotic Analysis of (Sticky) Track-and-Stop</title>
<link>https://arxiv.org/abs/2505.22475</link>
<guid>https://arxiv.org/abs/2505.22475</guid>
<content:encoded><![CDATA[
arXiv:2505.22475v1 Announce Type: new 
Abstract: In pure exploration problems, a statistician sequentially collects information to answer a question about some stochastic and unknown environment. The probability of returning a wrong answer should not exceed a maximum risk parameter $\delta$ and good algorithms make as few queries to the environment as possible. The Track-and-Stop algorithm is a pioneering method to solve these problems. Specifically, it is well-known that it enjoys asymptotic optimality sample complexity guarantees for $\delta\to 0$ whenever the map from the environment to its correct answers is single-valued (e.g., best-arm identification with a unique optimal arm). The Sticky Track-and-Stop algorithm extends these results to settings where, for each environment, there might exist multiple correct answers (e.g., $\epsilon$-optimal arm identification). Although both methods are optimal in the asymptotic regime, their non-asymptotic guarantees remain unknown. In this work, we fill this gap and provide non-asymptotic guarantees for both algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Closer Look at Multimodal Representation Collapse</title>
<link>https://arxiv.org/abs/2505.22483</link>
<guid>https://arxiv.org/abs/2505.22483</guid>
<content:encoded><![CDATA[
arXiv:2505.22483v1 Announce Type: new 
Abstract: We aim to develop a fundamental understanding of modality collapse, a recently observed empirical phenomenon wherein models trained for multimodal fusion tend to rely only on a subset of the modalities, ignoring the rest. We show that modality collapse happens when noisy features from one modality are entangled, via a shared set of neurons in the fusion head, with predictive features from another, effectively masking out positive contributions from the predictive features of the former modality and leading to its collapse. We further prove that cross-modal knowledge distillation implicitly disentangles such representations by freeing up rank bottlenecks in the student encoder, denoising the fusion-head outputs without negatively impacting the predictive features from either modality. Based on the above findings, we propose an algorithm that prevents modality collapse through explicit basis reallocation, with applications in dealing with missing modalities. Extensive experiments on multiple multimodal benchmarks validate our theoretical claims. Project page: https://abhrac.github.io/mmcollapse/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Adversarial Training with Energy-based Models</title>
<link>https://arxiv.org/abs/2505.22486</link>
<guid>https://arxiv.org/abs/2505.22486</guid>
<content:encoded><![CDATA[
arXiv:2505.22486v1 Announce Type: new 
Abstract: We aim at using Energy-based Model (EBM) framework to better understand adversarial training (AT) in classifiers, and additionally to analyze the intrinsic generative capabilities of robust classifiers. By viewing standard classifiers through an energy lens, we begin by analyzing how the energies of adversarial examples, generated by various attacks, differ from those of the natural samples. The central focus of our work is to understand the critical phenomena of Catastrophic Overfitting (CO) and Robust Overfitting (RO) in AT from an energy perspective. We analyze the impact of existing AT approaches on the energy of samples during training and observe that the behavior of the ``delta energy' -- change in energy between original sample and its adversarial counterpart -- diverges significantly when CO or RO occurs. After a thorough analysis of these energy dynamics and their relationship with overfitting, we propose a novel regularizer, the Delta Energy Regularizer (DER), designed to smoothen the energy landscape during training. We demonstrate that DER is effective in mitigating both CO and RO across multiple benchmarks. We further show that robust classifiers, when being used as generative models, have limits in handling trade-off between image quality and variability. We propose an improved technique based on a local class-wise principal component analysis (PCA) and energy-based guidance for better class-specific initialization and adaptive stopping, enhancing sample diversity and generation quality. Considering that we do not explicitly train for generative modeling, we achieve a competitive Inception Score (IS) and Fr\'echet inception distance (FID) compared to hybrid discriminative-generative models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Surprising Effectiveness of Large Learning Rates under Standard Width Scaling</title>
<link>https://arxiv.org/abs/2505.22491</link>
<guid>https://arxiv.org/abs/2505.22491</guid>
<content:encoded><![CDATA[
arXiv:2505.22491v1 Announce Type: new 
Abstract: The dominant paradigm for training large-scale vision and language models is He initialization and a single global learning rate (\textit{standard parameterization}, SP). Despite its practical success, standard parametrization remains poorly understood from a theoretical perspective: Existing infinite-width theory would predict instability under large learning rates and vanishing feature learning under stable learning rates. However, empirically optimal learning rates consistently decay much slower than theoretically predicted. By carefully studying neural network training dynamics, we demonstrate that this discrepancy is not fully explained by finite-width phenomena such as catapult effects or a lack of alignment between weights and incoming activations. We instead show that the apparent contradiction can be fundamentally resolved by taking the loss function into account: In contrast to Mean Squared Error (MSE) loss, we prove that under cross-entropy (CE) loss, an intermediate \textit{controlled divergence} regime emerges, where logits diverge but loss, gradients, and activations remain stable. Stable training under large learning rates enables persistent feature evolution at scale in all hidden layers, which is crucial for the practical success of SP. In experiments across optimizers (SGD, Adam), architectures (MLPs, GPT) and data modalities (vision, language), we validate that neural networks operate in this controlled divergence regime under CE loss but not under MSE loss. Our empirical evidence suggests that width-scaling considerations are surprisingly useful for predicting empirically optimal learning rate exponents. Finally, our analysis clarifies the effectiveness and limitations of recently proposed layerwise learning rate scalings for standard initialization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Demystifying the Paradox of Importance Sampling with an Estimated History-Dependent Behavior Policy in Off-Policy Evaluation</title>
<link>https://arxiv.org/abs/2505.22492</link>
<guid>https://arxiv.org/abs/2505.22492</guid>
<content:encoded><![CDATA[
arXiv:2505.22492v1 Announce Type: new 
Abstract: This paper studies off-policy evaluation (OPE) in reinforcement learning with a focus on behavior policy estimation for importance sampling. Prior work has shown empirically that estimating a history-dependent behavior policy can lead to lower mean squared error (MSE) even when the true behavior policy is Markovian. However, the question of why the use of history should lower MSE remains open. In this paper, we theoretically demystify this paradox by deriving a bias-variance decomposition of the MSE of ordinary importance sampling (IS) estimators, demonstrating that history-dependent behavior policy estimation decreases their asymptotic variances while increasing their finite-sample biases. Additionally, as the estimated behavior policy conditions on a longer history, we show a consistent decrease in variance. We extend these findings to a range of other OPE estimators, including the sequential IS estimator, the doubly robust estimator and the marginalized IS estimator, with the behavior policy estimated either parametrically or non-parametrically.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProSpero: Active Learning for Robust Protein Design Beyond Wild-Type Neighborhoods</title>
<link>https://arxiv.org/abs/2505.22494</link>
<guid>https://arxiv.org/abs/2505.22494</guid>
<content:encoded><![CDATA[
arXiv:2505.22494v1 Announce Type: new 
Abstract: Designing protein sequences of both high fitness and novelty is a challenging task in data-efficient protein engineering. Exploration beyond wild-type neighborhoods often leads to biologically implausible sequences or relies on surrogate models that lose fidelity in novel regions. Here, we propose ProSpero, an active learning framework in which a frozen pre-trained generative model is guided by a surrogate updated from oracle feedback. By integrating fitness-relevant residue selection with biologically-constrained Sequential Monte Carlo sampling, our approach enables exploration beyond wild-type neighborhoods while preserving biological plausibility. We show that our framework remains effective even when the surrogate is misspecified. ProSpero consistently outperforms or matches existing methods across diverse protein engineering tasks, retrieving sequences of both high fitness and novelty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric GNNs for Charged Particle Tracking at GlueX</title>
<link>https://arxiv.org/abs/2505.22504</link>
<guid>https://arxiv.org/abs/2505.22504</guid>
<content:encoded><![CDATA[
arXiv:2505.22504v1 Announce Type: new 
Abstract: Nuclear physics experiments are aimed at uncovering the fundamental building blocks of matter. The experiments involve high-energy collisions that produce complex events with many particle trajectories. Tracking charged particles resulting from collisions in the presence of a strong magnetic field is critical to enable the reconstruction of particle trajectories and precise determination of interactions. It is traditionally achieved through combinatorial approaches that scale worse than linearly as the number of hits grows. Since particle hit data naturally form a 3-dimensional point cloud and can be structured as graphs, Graph Neural Networks (GNNs) emerge as an intuitive and effective choice for this task. In this study, we evaluate the GNN model for track finding on the data from the GlueX experiment at Jefferson Lab. We use simulation data to train the model and test on both simulation and real GlueX measurements. We demonstrate that GNN-based track finding outperforms the currently used traditional method at GlueX in terms of segment-based efficiency at a fixed purity while providing faster inferences. We show that the GNN model can achieve significant speedup by processing multiple events in batches, which exploits the parallel computation capability of Graphical Processing Units (GPUs). Finally, we compare the GNN implementation on GPU and FPGA and describe the trade-off.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsification and Reconstruction from the Perspective of Representation Geometry</title>
<link>https://arxiv.org/abs/2505.22506</link>
<guid>https://arxiv.org/abs/2505.22506</guid>
<content:encoded><![CDATA[
arXiv:2505.22506v1 Announce Type: new 
Abstract: Sparse Autoencoders (SAEs) have emerged as a predominant tool in mechanistic interpretability, aiming to identify interpretable monosemantic features. However, how does sparse encoding organize the representations of activation vector from language models? What is the relationship between this organizational paradigm and feature disentanglement as well as reconstruction performance? To address these questions, we propose the SAEMA, which validates the stratified structure of the representation by observing the variability of the rank of the symmetric semipositive definite (SSPD) matrix corresponding to the modal tensor unfolded along the latent tensor with the level of noise added to the residual stream. To systematically investigate how sparse encoding alters representational structures, we define local and global representations, demonstrating that they amplify inter-feature distinctions by merging similar semantic features and introducing additional dimensionality. Furthermore, we intervene the global representation from an optimization perspective, proving a significant causal relationship between their separability and the reconstruction performance. This study explains the principles of sparsity from the perspective of representational geometry and demonstrates the impact of changes in representational structure on reconstruction performance. Particularly emphasizes the necessity of understanding representations and incorporating representational constraints, providing empirical references for developing new interpretable tools and improving SAEs. The code is available at \hyperlink{https://github.com/wenjie1835/SAERepGeo}{https://github.com/wenjie1835/SAERepGeo}.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating Optimization via Differentiable Stopping Time</title>
<link>https://arxiv.org/abs/2505.22509</link>
<guid>https://arxiv.org/abs/2505.22509</guid>
<content:encoded><![CDATA[
arXiv:2505.22509v1 Announce Type: new 
Abstract: Optimization is an important module of modern machine learning applications. Tremendous efforts have been made to accelerate optimization algorithms. A common formulation is achieving a lower loss at a given time. This enables a differentiable framework with respect to the algorithm hyperparameters. In contrast, its dual, minimizing the time to reach a target loss, is believed to be non-differentiable, as the time is not differentiable. As a result, it usually serves as a conceptual framework or is optimized using zeroth-order methods. To address this limitation, we propose a differentiable stopping time and theoretically justify it based on differential equations. An efficient algorithm is designed to backpropagate through it. As a result, the proposed differentiable stopping time enables a new differentiable formulation for accelerating algorithms. We further discuss its applications, such as online hyperparameter tuning and learning to optimize. Our proposed methods show superior performance in comprehensive experiments across various problems, which confirms their effectiveness.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Supervised Learning Models for Fraud Detection: A Comparative Study of Classical and Deep Architectures on Imbalanced Transaction Data</title>
<link>https://arxiv.org/abs/2505.22521</link>
<guid>https://arxiv.org/abs/2505.22521</guid>
<content:encoded><![CDATA[
arXiv:2505.22521v1 Announce Type: new 
Abstract: Fraud detection remains a critical task in high-stakes domains such as finance and e-commerce, where undetected fraudulent transactions can lead to significant economic losses. In this study, we systematically compare the performance of four supervised learning models - Logistic Regression, Random Forest, Light Gradient Boosting Machine (LightGBM), and a Gated Recurrent Unit (GRU) network - on a large-scale, highly imbalanced online transaction dataset. While ensemble methods such as Random Forest and LightGBM demonstrated superior performance in both overall and class-specific metrics, Logistic Regression offered a reliable and interpretable baseline. The GRU model showed strong recall for the minority fraud class, though at the cost of precision, highlighting a trade-off relevant for real-world deployment. Our evaluation emphasizes not only weighted averages but also per-class precision, recall, and F1-scores, providing a nuanced view of each model's effectiveness in detecting rare but consequential fraudulent activity. The findings underscore the importance of choosing models based on the specific risk tolerance and operational needs of fraud detection systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Alignment of Discrete Diffusion Models with Sequential Monte Carlo</title>
<link>https://arxiv.org/abs/2505.22524</link>
<guid>https://arxiv.org/abs/2505.22524</guid>
<content:encoded><![CDATA[
arXiv:2505.22524v1 Announce Type: new 
Abstract: Discrete diffusion models have become highly effective across various domains. However, real-world applications often require the generative process to adhere to certain constraints but without task-specific fine-tuning. To this end, we propose a training-free method based on Sequential Monte Carlo (SMC) to sample from the reward-aligned target distribution at the test time. Our approach leverages twisted SMC with an approximate locally optimal proposal, obtained via a first-order Taylor expansion of the reward function. To address the challenge of ill-defined gradients in discrete spaces, we incorporate a Gumbel-Softmax relaxation, enabling efficient gradient-based approximation within the discrete generative framework. Empirical results on both synthetic datasets and image modelling validate the effectiveness of our approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training RL Agents for Multi-Objective Network Defense Tasks</title>
<link>https://arxiv.org/abs/2505.22531</link>
<guid>https://arxiv.org/abs/2505.22531</guid>
<content:encoded><![CDATA[
arXiv:2505.22531v1 Announce Type: new 
Abstract: Open-ended learning (OEL) -- which emphasizes training agents that achieve broad capability over narrow competency -- is emerging as a paradigm to develop artificial intelligence (AI) agents to achieve robustness and generalization. However, despite promising results that demonstrate the benefits of OEL, applying OEL to develop autonomous agents for real-world cybersecurity applications remains a challenge.
  We propose a training approach, inspired by OEL, to develop autonomous network defenders. Our results demonstrate that like in other domains, OEL principles can translate into more robust and generalizable agents for cyber defense. To apply OEL to network defense, it is necessary to address several technical challenges. Most importantly, it is critical to provide a task representation approach over a broad universe of tasks that maintains a consistent interface over goals, rewards and action spaces. This way, the learning agent can train with varying network conditions, attacker behaviors, and defender goals while being able to build on previously gained knowledge.
  With our tools and results, we aim to fundamentally impact research that applies AI to solve cybersecurity problems. Specifically, as researchers develop gyms and benchmarks for cyber defense, it is paramount that they consider diverse tasks with consistent representations, such as those we propose in our work.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabularQGAN: A Quantum Generative Model for Tabular Data</title>
<link>https://arxiv.org/abs/2505.22533</link>
<guid>https://arxiv.org/abs/2505.22533</guid>
<content:encoded><![CDATA[
arXiv:2505.22533v1 Announce Type: new 
Abstract: In this paper, we introduce a novel quantum generative model for synthesizing tabular data. Synthetic data is valuable in scenarios where real-world data is scarce or private, it can be used to augment or replace existing datasets. Real-world enterprise data is predominantly tabular and heterogeneous, often comprising a mixture of categorical and numerical features, making it highly relevant across various industries such as healthcare, finance, and software. We propose a quantum generative adversarial network architecture with flexible data encoding and a novel quantum circuit ansatz to effectively model tabular data. The proposed approach is tested on the MIMIC III healthcare and Adult Census datasets, with extensive benchmarking against leading classical models, CTGAN, and CopulaGAN. Experimental results demonstrate that our quantum model outperforms classical models by an average of 8.5% with respect to an overall similarity score from SDMetrics, while using only 0.072% of the parameters of the classical models. Additionally, we evaluate the generalization capabilities of the models using two custom-designed metrics that demonstrate the ability of the proposed quantum model to generate useful and novel samples. To our knowledge, this is one of the first demonstrations of a successful quantum generative model for handling tabular data, indicating that this task could be well-suited to quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncertainty Quantification with Proper Scoring Rules: Adjusting Measures to Prediction Tasks</title>
<link>https://arxiv.org/abs/2505.22538</link>
<guid>https://arxiv.org/abs/2505.22538</guid>
<content:encoded><![CDATA[
arXiv:2505.22538v1 Announce Type: new 
Abstract: We address the problem of uncertainty quantification and propose measures of total, aleatoric, and epistemic uncertainty based on a known decomposition of (strictly) proper scoring rules, a specific type of loss function, into a divergence and an entropy component. This leads to a flexible framework for uncertainty quantification that can be instantiated with different losses (scoring rules), which makes it possible to tailor uncertainty quantification to the use case at hand. We show that this flexibility is indeed advantageous. In particular, we analyze the task of selective prediction and show that the scoring rule should ideally match the task loss. In addition, we perform experiments on two other common tasks. For out-of-distribution detection, our results confirm that a widely used measure of epistemic uncertainty, mutual information, performs best. Moreover, in the setting of active learning, our measure of epistemic uncertainty based on the zero-one-loss consistently outperforms other uncertainty measures.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Human-Centric Approach to Explainable AI for Personalized Education</title>
<link>https://arxiv.org/abs/2505.22541</link>
<guid>https://arxiv.org/abs/2505.22541</guid>
<content:encoded><![CDATA[
arXiv:2505.22541v1 Announce Type: new 
Abstract: Deep neural networks form the backbone of artificial intelligence research, with potential to transform the human experience in areas ranging from autonomous driving to personal assistants, healthcare to education. However, their integration into the daily routines of real-world classrooms remains limited. It is not yet common for a teacher to assign students individualized homework targeting their specific weaknesses, provide students with instant feedback, or simulate student responses to a new exam question. While these models excel in predictive performance, this lack of adoption can be attributed to a significant weakness: the lack of explainability of model decisions, leading to a lack of trust from students, parents, and teachers. This thesis aims to bring human needs to the forefront of eXplainable AI (XAI) research, grounded in the concrete use case of personalized learning and teaching. We frame the contributions along two verticals: technical advances in XAI and their aligned human studies. We investigate explainability in AI for education, revealing systematic disagreements between post-hoc explainers and identifying a need for inherently interpretable model architectures. We propose four novel technical contributions in interpretability with a multimodal modular architecture (MultiModN), an interpretable mixture-of-experts model (InterpretCC), adversarial training for explainer stability, and a theory-driven LLM-XAI framework to present explanations to students (iLLuMinaTE), which we evaluate in diverse settings with professors, teachers, learning scientists, and university students. By combining empirical evaluations of existing explainers with novel architectural designs and human studies, our work lays a foundation for human-centric AI systems that balance state-of-the-art performance with built-in transparency and trust.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DES-LOC: Desynced Low Communication Adaptive Optimizers for Training Foundation Models</title>
<link>https://arxiv.org/abs/2505.22549</link>
<guid>https://arxiv.org/abs/2505.22549</guid>
<content:encoded><![CDATA[
arXiv:2505.22549v1 Announce Type: new 
Abstract: Scaling foundation model training with Distributed Data Parallel (DDP) methods is bandwidth-limited. Existing infrequent communication methods like Local SGD were designed to synchronize only model parameters and cannot be trivially applied to adaptive optimizers due to additional optimizer states. Current approaches extending Local SGD either lack convergence guarantees or require synchronizing all optimizer states, tripling communication costs. We propose Desynced Low Communication Adaptive Optimizers (DES-LOC), a family of optimizers assigning independent synchronization periods to parameters and momenta, enabling lower communication costs while preserving convergence. Through extensive experiments on language models of up to 1.7B, we show that DES-LOC can communicate 170x less than DDP and 2x less than the previous state-of-the-art Local ADAM. Furthermore, unlike previous heuristic approaches, DES-LOC is suited for practical training scenarios prone to system failures. DES-LOC offers a scalable, bandwidth-efficient, and fault-tolerant solution for foundation model training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometric Hyena Networks for Large-scale Equivariant Learning</title>
<link>https://arxiv.org/abs/2505.22560</link>
<guid>https://arxiv.org/abs/2505.22560</guid>
<content:encoded><![CDATA[
arXiv:2505.22560v1 Announce Type: new 
Abstract: Processing global geometric context while preserving equivariance is crucial when modeling biological, chemical, and physical systems. Yet, this is challenging due to the computational demands of equivariance and global context at scale. Standard methods such as equivariant self-attention suffer from quadratic complexity, while local methods such as distance-based message passing sacrifice global information. Inspired by the recent success of state-space and long-convolutional models, we introduce Geometric Hyena, the first equivariant long-convolutional model for geometric systems. Geometric Hyena captures global geometric context at sub-quadratic complexity while maintaining equivariance to rotations and translations. Evaluated on all-atom property prediction of large RNA molecules and full protein molecular dynamics, Geometric Hyena outperforms existing equivariant models while requiring significantly less memory and compute that equivariant self-attention. Notably, our model processes the geometric context of 30k tokens 20x faster than the equivariant transformer and allows 72x longer context within the same budget.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FNOPE: Simulation-based inference on function spaces with Fourier Neural Operators</title>
<link>https://arxiv.org/abs/2505.22573</link>
<guid>https://arxiv.org/abs/2505.22573</guid>
<content:encoded><![CDATA[
arXiv:2505.22573v1 Announce Type: new 
Abstract: Simulation-based inference (SBI) is an established approach for performing Bayesian inference on scientific simulators. SBI so far works best on low-dimensional parametric models. However, it is difficult to infer function-valued parameters, which frequently occur in disciplines that model spatiotemporal processes such as the climate and earth sciences. Here, we introduce an approach for efficient posterior estimation, using a Fourier Neural Operator (FNO) architecture with a flow matching objective. We show that our approach, FNOPE, can perform inference of function-valued parameters at a fraction of the simulation budget of state of the art methods. In addition, FNOPE supports posterior evaluation at arbitrary discretizations of the domain, as well as simultaneous estimation of vector-valued parameters. We demonstrate the effectiveness of our approach on several benchmark tasks and a challenging spatial inference task from glaciology. FNOPE extends the applicability of SBI methods to new scientific domains by enabling the inference of function-valued parameters.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benignity of loss landscape with weight decay requires both large overparametrization and initialization</title>
<link>https://arxiv.org/abs/2505.22578</link>
<guid>https://arxiv.org/abs/2505.22578</guid>
<content:encoded><![CDATA[
arXiv:2505.22578v1 Announce Type: new 
Abstract: The optimization of neural networks under weight decay remains poorly understood from a theoretical standpoint. While weight decay is standard practice in modern training procedures, most theoretical analyses focus on unregularized settings. In this work, we investigate the loss landscape of the $\ell_2$-regularized training loss for two-layer ReLU networks. We show that the landscape becomes benign -- i.e., free of spurious local minima -- under large overparametrization, specifically when the network width $m$ satisfies $m \gtrsim \min(n^d, 2^n)$, where $n$ is the number of data points and $d$ the input dimension. More precisely in this regime, almost all constant activation regions contain a global minimum and no spurious local minima. We further show that this level of overparametrization is not only sufficient but also necessary via the example of orthogonal data. Finally, we demonstrate that such loss landscape results primarily hold relevance in the large initialization regime. In contrast, for small initializations -- corresponding to the feature learning regime -- optimization can still converge to spurious local minima, despite the global benignity of the landscape.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Unlearning under Overparameterization</title>
<link>https://arxiv.org/abs/2505.22601</link>
<guid>https://arxiv.org/abs/2505.22601</guid>
<content:encoded><![CDATA[
arXiv:2505.22601v1 Announce Type: new 
Abstract: Machine unlearning algorithms aim to remove the influence of specific training samples, ideally recovering the model that would have resulted from training on the remaining data alone. We study unlearning in the overparameterized setting, where many models interpolate the data, and defining the unlearning solution as any loss minimizer over the retained set$\unicode{x2013}$as in prior work in the underparameterized setting$\unicode{x2013}$is inadequate, since the original model may already interpolate the retained data and satisfy this condition. In this regime, loss gradients vanish, rendering prior methods based on gradient perturbations ineffective, motivating both new unlearning definitions and algorithms. For this setting, we define the unlearning solution as the minimum-complexity interpolator over the retained data and propose a new algorithmic framework that only requires access to model gradients on the retained set at the original solution. We minimize a regularized objective over perturbations constrained to be orthogonal to these model gradients, a first-order relaxation of the interpolation condition. For different model classes, we provide exact and approximate unlearning guarantees, and we demonstrate that an implementation of our framework outperforms existing baselines across various unlearning experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One Rank at a Time: Cascading Error Dynamics in Sequential Learning</title>
<link>https://arxiv.org/abs/2505.22602</link>
<guid>https://arxiv.org/abs/2505.22602</guid>
<content:encoded><![CDATA[
arXiv:2505.22602v1 Announce Type: new 
Abstract: Sequential learning -- where complex tasks are broken down into simpler, hierarchical components -- has emerged as a paradigm in AI. This paper views sequential learning through the lens of low-rank linear regression, focusing specifically on how errors propagate when learning rank-1 subspaces sequentially. We present an analysis framework that decomposes the learning process into a series of rank-1 estimation problems, where each subsequent estimation depends on the accuracy of previous steps. Our contribution is a characterization of the error propagation in this sequential process, establishing bounds on how errors -- e.g., due to limited computational budgets and finite precision -- affect the overall model accuracy. We prove that these errors compound in predictable ways, with implications for both algorithmic design and stability guarantees.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models</title>
<link>https://arxiv.org/abs/2505.22617</link>
<guid>https://arxiv.org/abs/2505.22617</guid>
<content:encoded><![CDATA[
arXiv:2505.22617v1 Announce Type: new 
Abstract: This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding (Un)Reliability of Steering Vectors in Language Models</title>
<link>https://arxiv.org/abs/2505.22637</link>
<guid>https://arxiv.org/abs/2505.22637</guid>
<content:encoded><![CDATA[
arXiv:2505.22637v1 Announce Type: new 
Abstract: Steering vectors are a lightweight method to control language model behavior by adding a learned bias to the activations at inference time. Although steering demonstrates promising performance, recent work shows that it can be unreliable or even counterproductive in some cases. This paper studies the influence of prompt types and the geometry of activation differences on steering reliability. First, we find that all seven prompt types used in our experiments produce a net positive steering effect, but exhibit high variance across samples, and often give an effect opposite of the desired one. No prompt type clearly outperforms the others, and yet the steering vectors resulting from the different prompt types often differ directionally (as measured by cosine similarity). Second, we show that higher cosine similarity between training set activation differences predicts more effective steering. Finally, we observe that datasets where positive and negative activations are better separated are more steerable. Our results suggest that vector steering is unreliable when the target behavior is not represented by a coherent direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral Survival Analysis</title>
<link>https://arxiv.org/abs/2505.22641</link>
<guid>https://arxiv.org/abs/2505.22641</guid>
<content:encoded><![CDATA[
arXiv:2505.22641v1 Announce Type: new 
Abstract: Survival analysis is widely deployed in a diverse set of fields, including healthcare, business, ecology, etc. The Cox Proportional Hazard (CoxPH) model is a semi-parametric model often encountered in the literature. Despite its popularity, wide deployment, and numerous variants, scaling CoxPH to large datasets and deep architectures poses a challenge, especially in the high-dimensional regime. We identify a fundamental connection between rank regression and the CoxPH model: this allows us to adapt and extend the so-called spectral method for rank regression to survival analysis. Our approach is versatile, naturally generalizing to several CoxPH variants, including deep models. We empirically verify our method's scalability on multiple real-world high-dimensional datasets; our method outperforms legacy methods w.r.t. predictive performance and efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On Learning Verifiers for Chain-of-Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.22650</link>
<guid>https://arxiv.org/abs/2505.22650</guid>
<content:encoded><![CDATA[
arXiv:2505.22650v1 Announce Type: new 
Abstract: Chain-of-Thought reasoning has emerged as a powerful approach for solving complex mathematical and logical problems. However, it can often veer off track through incorrect or unsubstantiated inferences. Formal mathematical reasoning, which can be checked with a formal verifier, is one approach to addressing this issue. However, currently LLMs are simply not good enough to solve complex problems in a formal way, and even just formalizing an informal problem statement can be challenging. Motivated by this fact, in this work we consider the problem of learning reliable verifiers for natural language Chain-of-Thought reasoning. That is, given a problem statement and step-by-step solution in natural language, the aim of the verifier is to output [Yes] if the reasoning steps in the solution are all valid, and [No] otherwise. In this work we give a formal PAC-learning framework for studying this problem. We propose and analyze several natural verification goals, at different levels of strength, in this framework. We provide sample complexity upper-bounds for learning verifiers satisfying these goals, as well as lower-bound and impossibility results for learning other natural verification objectives without additional assumptions.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Uncertainty Quantification Needs Reassessment for Large-language Model Agents</title>
<link>https://arxiv.org/abs/2505.22655</link>
<guid>https://arxiv.org/abs/2505.22655</guid>
<content:encoded><![CDATA[
arXiv:2505.22655v1 Announce Type: new 
Abstract: Large-language models (LLMs) and chatbot agents are known to provide wrong outputs at times, and it was recently found that this can never be fully prevented. Hence, uncertainty quantification plays a crucial role, aiming to quantify the level of ambiguity in either one overall number or two numbers for aleatoric and epistemic uncertainty. This position paper argues that this traditional dichotomy of uncertainties is too limited for the open and interactive setup that LLM agents operate in when communicating with a user, and that we need to research avenues that enrich uncertainties in this novel scenario. We review the literature and find that popular definitions of aleatoric and epistemic uncertainties directly contradict each other and lose their meaning in interactive LLM agent settings. Hence, we propose three novel research directions that focus on uncertainties in such human-computer interactions: Underspecification uncertainties, for when users do not provide all information or define the exact task at the first go, interactive learning, to ask follow-up questions and reduce the uncertainty about the current context, and output uncertainties, to utilize the rich language and speech space to express uncertainties as more than mere numbers. We expect that these new ways of dealing with and communicating uncertainties will lead to LLM agent interactions that are more transparent, trustworthy, and intuitive.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Maximizing Confidence Alone Improves Reasoning</title>
<link>https://arxiv.org/abs/2505.22660</link>
<guid>https://arxiv.org/abs/2505.22660</guid>
<content:encoded><![CDATA[
arXiv:2505.22660v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has enabled machine learning models to achieve significant advances in many fields. Most recently, RL has empowered frontier language models to solve challenging math, science, and coding problems. However, central to any RL algorithm is the reward function, and reward engineering is a notoriously difficult problem in any domain. In this paper, we propose RENT: Reinforcement Learning via Entropy Minimization -- a fully unsupervised RL method that requires no external reward or ground-truth answers, and instead uses the model's entropy of its underlying distribution as an intrinsic reward. We find that by reinforcing the chains of thought that yield high model confidence on its generated answers, the model improves its reasoning ability. In our experiments, we showcase these improvements on an extensive suite of commonly-used reasoning benchmarks, including GSM8K, MATH500, AMC, AIME, and GPQA, and models of varying sizes from the Qwen and Mistral families. The generality of our unsupervised learning method lends itself to applicability in a wide range of domains where external supervision is limited or unavailable.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offset Unlearning for Large Language Models</title>
<link>https://arxiv.org/abs/2404.11045</link>
<guid>https://arxiv.org/abs/2404.11045</guid>
<content:encoded><![CDATA[
arXiv:2404.11045v2 Announce Type: cross 
Abstract: Despite the strong capabilities of Large Language Models (LLMs) to acquire knowledge from their training corpora, the memorization of sensitive information in the corpora such as copyrighted, biased, and private content has led to ethical and legal concerns. In response to these challenges, unlearning has emerged as a potential remedy for LLMs affected by problematic training data. However, previous unlearning techniques are either not applicable to black-box LLMs due to required access to model internal weights, or violate data protection principles by retaining sensitive data for inference-time correction. We propose {\delta}-Unlearning, an offset unlearning framework for black-box LLMs. Instead of tuning the black-box LLM itself, {\delta}-Unlearning learns the logit offset needed for unlearning by contrasting the logits from a pair of smaller models. Experiments demonstrate that {\delta}- Unlearning can effectively unlearn target data while maintaining similar or even stronger performance on general out-of-forget-scope tasks. {\delta}-Unlearning also effectively incorporates different unlearning algorithms, making our approach a versatile solution to adapting various existing unlearning algorithms to black-box LLMs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Provably Robust Training of Quantum Circuit Classifiers Against Parameter Noise</title>
<link>https://arxiv.org/abs/2505.18478</link>
<guid>https://arxiv.org/abs/2505.18478</guid>
<content:encoded><![CDATA[
arXiv:2505.18478v1 Announce Type: cross 
Abstract: Advancements in quantum computing have spurred significant interest in harnessing its potential for speedups over classical systems. However, noise remains a major obstacle to achieving reliable quantum algorithms. In this work, we present a provably noise-resilient training theory and algorithm to enhance the robustness of parameterized quantum circuit classifiers. Our method, with a natural connection to Evolutionary Strategies, guarantees resilience to parameter noise with minimal adjustments to commonly used optimization algorithms. Our approach is function-agnostic and adaptable to various quantum circuits, successfully demonstrated in quantum phase classification tasks. By developing provably guaranteed optimization theory with quantum circuits, our work opens new avenues for practical, robust applications of near-term quantum computers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Capability-Based Scaling Laws for LLM Red-Teaming</title>
<link>https://arxiv.org/abs/2505.20162</link>
<guid>https://arxiv.org/abs/2505.20162</guid>
<content:encoded><![CDATA[
arXiv:2505.20162v1 Announce Type: cross 
Abstract: As large language models grow in capability and agency, identifying vulnerabilities through red-teaming becomes vital for safe deployment. However, traditional prompt-engineering approaches may prove ineffective once red-teaming turns into a weak-to-strong problem, where target models surpass red-teamers in capabilities. To study this shift, we frame red-teaming through the lens of the capability gap between attacker and target. We evaluate more than 500 attacker-target pairs using LLM-based jailbreak attacks that mimic human red-teamers across diverse families, sizes, and capability levels. Three strong trends emerge: (i) more capable models are better attackers, (ii) attack success drops sharply once the target's capability exceeds the attacker's, and (iii) attack success rates correlate with high performance on social science splits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking scaling law that predicts attack success for a fixed target based on attacker-target capability gap. These findings suggest that fixed-capability attackers (e.g., humans) may become ineffective against future models, increasingly capable open-source models amplify risks for existing systems, and model providers must accurately measure and control models' persuasive and manipulative abilities to limit their effectiveness as attackers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Genetic Influences on Brain Aging: Analyzing Sex Differences in the UK Biobank using Structural MRI</title>
<link>https://arxiv.org/abs/2505.20344</link>
<guid>https://arxiv.org/abs/2505.20344</guid>
<content:encoded><![CDATA[
arXiv:2505.20344v1 Announce Type: cross 
Abstract: Brain aging trajectories differ between males and females, yet the genetic factors underlying these differences remain underexplored. Using structural MRI and genotyping data from 40,940 UK Biobank participants (aged 45-83), we computed Brain Age Gap Estimates (BrainAGE) for total brain, hippocampal, and ventricular volumes. We conducted sex-stratified genome-wide association studies (GWAS) and Post-GWAS analyses to identify genetic variants associated with accelerated brain aging. Distinct gene sets emerged by sex: in females, neurotransmitter transport and mitochondrial stress response genes were implicated; in males, immune and inflammation-related genes dominated. Shared genes, including GMNC and OSTN, were consistently linked to brain volumes across sexes, suggesting core roles in neurostructural maintenance. Tissue expression analyses revealed sex-specific enrichment in pathways tied to neurodegeneration. These findings highlight the importance of sex-stratified approaches in aging research and suggest genetic targets for personalized interventions against age-related cognitive decline.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic detection of abnormal clinical EEG: comparison of a finetuned foundation model with two deep learning models</title>
<link>https://arxiv.org/abs/2505.21507</link>
<guid>https://arxiv.org/abs/2505.21507</guid>
<content:encoded><![CDATA[
arXiv:2505.21507v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is commonly used by physicians for the diagnosis of numerous neurological disorders. Due to the large volume of EEGs requiring interpretation and the specific expertise involved, artificial intelligence-based tools are being developed to assist in their visual analysis. In this paper, we compare two deep learning models (CNN-LSTM and Transformer-based) with BioSerenity-E1, a recently proposed foundation model, in the task of classifying entire EEG recordings as normal or abnormal. The three models were trained or finetuned on 2,500 EEG recordings and their performances were evaluated on two private and one public datasets: a large multicenter dataset annotated by a single specialist (dataset A composed of n = 4,480 recordings), a small multicenter dataset annotated by three specialists (dataset B, n = 198), and the Temple University Abnormal (TUAB) EEG corpus evaluation dataset (n = 276). On dataset A, the three models achieved at least 86% balanced accuracy, with BioSerenity-E1 finetuned achieving the highest balanced accuracy (89.19% [88.36-90.41]). BioSerenity-E1 finetuned also achieved the best performance on dataset B, with 94.63% [92.32-98.12] balanced accuracy. The models were then validated on TUAB evaluation dataset, whose corresponding training set was not used during training, where they achieved at least 76% accuracy. Specifically, BioSerenity-E1 finetuned outperformed the other two models, reaching an accuracy of 82.25% [78.27-87.48]. Our results highlight the usefulness of leveraging pre-trained models for automatic EEG classification: enabling robust and efficient interpretation of EEG data with fewer resources and broader applicability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Vision Transformer Explainability Using Artificial Astrocytes</title>
<link>https://arxiv.org/abs/2505.21513</link>
<guid>https://arxiv.org/abs/2505.21513</guid>
<content:encoded><![CDATA[
arXiv:2505.21513v1 Announce Type: cross 
Abstract: Machine learning models achieve high precision, but their decision-making processes often lack explainability. Furthermore, as model complexity increases, explainability typically decreases. Existing efforts to improve explainability primarily involve developing new eXplainable artificial intelligence (XAI) techniques or incorporating explainability constraints during training. While these approaches yield specific improvements, their applicability remains limited. In this work, we propose the Vision Transformer with artificial Astrocytes (ViTA). This training-free approach is inspired by neuroscience and enhances the reasoning of a pretrained deep neural network to generate more human-aligned explanations. We evaluated our approach employing two well-known XAI techniques, Grad-CAM and Grad-CAM++, and compared it to a standard Vision Transformer (ViT). Using the ClickMe dataset, we quantified the similarity between the heatmaps produced by the XAI techniques and a (human-aligned) ground truth. Our results consistently demonstrate that incorporating artificial astrocytes enhances the alignment of model explanations with human perception, leading to statistically significant improvements across all XAI techniques and metrics utilized.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CIM-NET: A Video Denoising Deep Neural Network Model Optimized for Computing-in-Memory Architectures</title>
<link>https://arxiv.org/abs/2505.21522</link>
<guid>https://arxiv.org/abs/2505.21522</guid>
<content:encoded><![CDATA[
arXiv:2505.21522v1 Announce Type: cross 
Abstract: While deep neural network (DNN)-based video denoising has demonstrated significant performance, deploying state-of-the-art models on edge devices remains challenging due to stringent real-time and energy efficiency requirements. Computing-in-Memory (CIM) chips offer a promising solution by integrating computation within memory cells, enabling rapid matrix-vector multiplication (MVM). However, existing DNN models are often designed without considering CIM architectural constraints, thus limiting their acceleration potential during inference. To address this, we propose a hardware-algorithm co-design framework incorporating two innovations: (1) a CIM-Aware Architecture, CIM-NET, optimized for large receptive field operation and CIM's crossbar-based MVM acceleration; and (2) a pseudo-convolutional operator, CIM-CONV, used within CIM-NET to integrate slide-based processing with fully connected transformations for high-quality feature extraction and reconstruction. This framework significantly reduces the number of MVM operations, improving inference speed on CIM chips while maintaining competitive performance. Experimental results indicate that, compared to the conventional lightweight model FastDVDnet, CIM-NET substantially reduces MVM operations with a slight decrease in denoising performance. With a stride value of 8, CIM-NET reduces MVM operations to 1/77th of the original, while maintaining competitive PSNR (35.11 dB vs. 35.56 dB
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Shared Representations from Unpaired Data</title>
<link>https://arxiv.org/abs/2505.21524</link>
<guid>https://arxiv.org/abs/2505.21524</guid>
<content:encoded><![CDATA[
arXiv:2505.21524v1 Announce Type: cross 
Abstract: Learning shared representations is a primary area of multimodal representation learning. The current approaches to achieve a shared embedding space rely heavily on paired samples from each modality, which are significantly harder to obtain than unpaired ones. In this work, we demonstrate that shared representations can be learned almost exclusively from unpaired data. Our arguments are grounded in the spectral embeddings of the random walk matrices constructed independently from each unimodal representation. Empirical results in computer vision and natural language processing domains support its potential, revealing the effectiveness of unpaired data in capturing meaningful cross-modal relations, demonstrating high capabilities in retrieval tasks, generation, arithmetics, zero-shot, and cross-domain classification. This work, to the best of our knowledge, is the first to demonstrate these capabilities almost exclusively from unpaired samples, giving rise to a cross-modal embedding that could be viewed as universal, i.e., independent of the specific modalities of the data. Our code IS publicly available at https://github.com/shaham-lab/SUE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniDB++: Fast Sampling of Unified Diffusion Bridge</title>
<link>https://arxiv.org/abs/2505.21528</link>
<guid>https://arxiv.org/abs/2505.21528</guid>
<content:encoded><![CDATA[
arXiv:2505.21528v1 Announce Type: cross 
Abstract: Diffusion Bridges enable transitions between arbitrary distributions, with the Unified Diffusion Bridge (UniDB) framework achieving high-fidelity image generation via a Stochastic Optimal Control (SOC) formulation. However, UniDB's reliance on iterative Euler sampling methods results in slow, computationally expensive inference, while existing acceleration techniques for diffusion or diffusion bridge models fail to address its unique challenges: missing terminal mean constraints and SOC-specific penalty coefficients in its SDEs. We present UniDB++, a training-free sampling algorithm that significantly improves upon these limitations. The method's key advancement comes from deriving exact closed-form solutions for UniDB's reverse-time SDEs, effectively reducing the error accumulation inherent in Euler approximations and enabling high-quality generation with up to 20$\times$ fewer sampling steps. This method is further complemented by replacing conventional noise prediction with a more stable data prediction model, along with an SDE-Corrector mechanism that maintains perceptual quality for low-step regimes (5-10 steps). Additionally, we demonstrate that UniDB++ aligns with existing diffusion bridge acceleration methods by evaluating their update rules, and UniDB++ can recover DBIMs as special cases under some theoretical conditions. Experiments demonstrate UniDB++'s state-of-the-art performance in image restoration tasks, outperforming Euler-based methods in fidelity and speed while reducing inference time significantly. This work bridges the gap between theoretical generality and practical efficiency in SOC-driven diffusion bridge models. Our code is available at https://github.com/2769433owo/UniDB-plusplus.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EvidenceMoE: A Physics-Guided Mixture-of-Experts with Evidential Critics for Advancing Fluorescence Light Detection and Ranging in Scattering Media</title>
<link>https://arxiv.org/abs/2505.21532</link>
<guid>https://arxiv.org/abs/2505.21532</guid>
<content:encoded><![CDATA[
arXiv:2505.21532v1 Announce Type: cross 
Abstract: Fluorescence LiDAR (FLiDAR), a Light Detection and Ranging (LiDAR) technology employed for distance and depth estimation across medical, automotive, and other fields, encounters significant computational challenges in scattering media. The complex nature of the acquired FLiDAR signal, particularly in such environments, makes isolating photon time-of-flight (related to target depth) and intrinsic fluorescence lifetime exceptionally difficult, thus limiting the effectiveness of current analytical and computational methodologies. To overcome this limitation, we present a Physics-Guided Mixture-of-Experts (MoE) framework tailored for specialized modeling of diverse temporal components. In contrast to the conventional MoE approaches our expert models are informed by underlying physics, such as the radiative transport equation governing photon propagation in scattering media. Central to our approach is EvidenceMoE, which integrates Evidence-Based Dirichlet Critics (EDCs). These critic models assess the reliability of each expert's output by providing per-expert quality scores and corrective feedback. A Decider Network then leverages this information to fuse expert predictions into a robust final estimate adaptively. We validate our method using realistically simulated Fluorescence LiDAR (FLiDAR) data for non-invasive cancer cell depth detection generated from photon transport models in tissue. Our framework demonstrates strong performance, achieving a normalized root mean squared error (NRMSE) of 0.030 for depth estimation and 0.074 for fluorescence lifetime.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Organizing Visual Prototypes for Non-Parametric Representation Learning</title>
<link>https://arxiv.org/abs/2505.21533</link>
<guid>https://arxiv.org/abs/2505.21533</guid>
<content:encoded><![CDATA[
arXiv:2505.21533v1 Announce Type: cross 
Abstract: We present Self-Organizing Visual Prototypes (SOP), a new training technique for unsupervised visual feature learning. Unlike existing prototypical self-supervised learning (SSL) methods that rely on a single prototype to encode all relevant features of a hidden cluster in the data, we propose the SOP strategy. In this strategy, a prototype is represented by many semantically similar representations, or support embeddings (SEs), each containing a complementary set of features that together better characterize their region in space and maximize training performance. We reaffirm the feasibility of non-parametric SSL by introducing novel non-parametric adaptations of two loss functions that implement the SOP strategy. Notably, we introduce the SOP Masked Image Modeling (SOP-MIM) task, where masked representations are reconstructed from the perspective of multiple non-parametric local SEs. We comprehensively evaluate the representations learned using the SOP strategy on a range of benchmarks, including retrieval, linear evaluation, fine-tuning, and object detection. Our pre-trained encoders achieve state-of-the-art performance on many retrieval benchmarks and demonstrate increasing performance gains with more complex encoders.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Attention Required for Transformer Inference? Explore Function-preserving Attention Replacement</title>
<link>https://arxiv.org/abs/2505.21535</link>
<guid>https://arxiv.org/abs/2505.21535</guid>
<content:encoded><![CDATA[
arXiv:2505.21535v1 Announce Type: cross 
Abstract: While transformers excel across vision and language pretraining tasks, their reliance on attention mechanisms poses challenges for inference efficiency, especially on edge and embedded accelerators with limited parallelism and memory bandwidth. Hinted by the observed redundancy of attention at inference time, we hypothesize that though the model learns complicated token dependency through pretraining, the inference-time sequence-to-sequence mapping in each attention layer is actually ''simple'' enough to be represented with a much cheaper function. In this work, we explore FAR, a Function-preserving Attention Replacement framework that replaces all attention blocks in pretrained transformers with learnable sequence-to-sequence modules, exemplified by an LSTM. FAR optimize a multi-head LSTM architecture with a block-wise distillation objective and a global structural pruning framework to achieve a family of efficient LSTM-based models from pretrained transformers. We validate FAR on the DeiT vision transformer family and demonstrate that it matches the accuracy of the original models on ImageNet and multiple downstream tasks with reduced parameters and latency. Further analysis shows that FAR preserves the semantic token relationships and the token-to-token correlation learned in the transformer's attention module.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CiRL: Open-Source Environments for Reinforcement Learning in Circular Economy and Net Zero</title>
<link>https://arxiv.org/abs/2505.21536</link>
<guid>https://arxiv.org/abs/2505.21536</guid>
<content:encoded><![CDATA[
arXiv:2505.21536v1 Announce Type: cross 
Abstract: The demand of finite raw materials will keep increasing as they fuel modern society. Simultaneously, solutions for stopping carbon emissions in the short term are not available, thus making the net zero target extremely challenging to achieve at scale. The circular economy (CE) paradigm is gaining attention as a solution to address climate change and the uncertainties of supplies of critical materials. Hence, in this paper, we introduce CiRL, a deep reinforcement learning (DRL) library of environments focused on the circularity of both solid and fluid materials. The integration of DRL into the design of material circularity is possible thanks to the formalism of thermodynamical material networks, which is underpinned by compartmental dynamical thermodynamics. Along with the focus on circularity, this library has three more features: the new CE-oriented environments are in the state-space form, which is typically used in dynamical systems analysis and control designs; it is based on a state-of-the-art Python library of DRL algorithms, namely, Stable-Baselines3; and it is developed in Google Colaboratory to be accessible to researchers from different disciplines and backgrounds as is often the case for circular economy researchers and engineers. CiRL is publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Corruption-Aware Training of Latent Video Diffusion Models for Robust Text-to-Video Generation</title>
<link>https://arxiv.org/abs/2505.21545</link>
<guid>https://arxiv.org/abs/2505.21545</guid>
<content:encoded><![CDATA[
arXiv:2505.21545v1 Announce Type: cross 
Abstract: Latent Video Diffusion Models (LVDMs) achieve high-quality generation but are sensitive to imperfect conditioning, which causes semantic drift and temporal incoherence on noisy, web-scale video-text datasets. We introduce CAT-LVDM, the first corruption-aware training framework for LVDMs that improves robustness through structured, data-aligned noise injection. Our method includes Batch-Centered Noise Injection (BCNI), which perturbs embeddings along intra-batch semantic directions to preserve temporal consistency. BCNI is especially effective on caption-rich datasets like WebVid-2M, MSR-VTT, and MSVD. We also propose Spectrum-Aware Contextual Noise (SACN), which injects noise along dominant spectral directions to improve low-frequency smoothness, showing strong results on UCF-101. On average, BCNI reduces FVD by 31.9% across WebVid-2M, MSR-VTT, and MSVD, while SACN yields a 12.3% improvement on UCF-101. Ablation studies confirm the benefit of low-rank, data-aligned noise. Our theoretical analysis further explains how such perturbations tighten entropy, Wasserstein, score-drift, mixing-time, and generalization bounds. CAT-LVDM establishes a principled, scalable training approach for robust video diffusion under multimodal noise. Code and models: https://github.com/chikap421/catlvdm
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WhisperD: Dementia Speech Recognition and Filler Word Detection with Whisper</title>
<link>https://arxiv.org/abs/2505.21551</link>
<guid>https://arxiv.org/abs/2505.21551</guid>
<content:encoded><![CDATA[
arXiv:2505.21551v1 Announce Type: cross 
Abstract: Whisper fails to correctly transcribe dementia speech because persons with dementia (PwDs) often exhibit irregular speech patterns and disfluencies such as pauses, repetitions, and fragmented sentences. It was trained on standard speech and may have had little or no exposure to dementia-affected speech. However, correct transcription is vital for dementia speech for cost-effective diagnosis and the development of assistive technology. In this work, we fine-tune Whisper with the open-source dementia speech dataset (DementiaBank) and our in-house dataset to improve its word error rate (WER). The fine-tuning also includes filler words to ascertain the filler inclusion rate (FIR) and F1 score. The fine-tuned models significantly outperformed the off-the-shelf models. The medium-sized model achieved a WER of 0.24, outperforming previous work. Similarly, there was a notable generalisability to unseen data and speech patterns.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the learned look-ahead behavior of chess neural networks</title>
<link>https://arxiv.org/abs/2505.21552</link>
<guid>https://arxiv.org/abs/2505.21552</guid>
<content:encoded><![CDATA[
arXiv:2505.21552v1 Announce Type: cross 
Abstract: We investigate the look-ahead capabilities of chess-playing neural networks, specifically focusing on the Leela Chess Zero policy network. We build on the work of Jenner et al. (2024) by analyzing the model's ability to consider future moves and alternative sequences beyond the immediate next move. Our findings reveal that the network's look-ahead behavior is highly context-dependent, varying significantly based on the specific chess position. We demonstrate that the model can process information about board states up to seven moves ahead, utilizing similar internal mechanisms across different future time steps. Additionally, we provide evidence that the network considers multiple possible move sequences rather than focusing on a single line of play. These results offer new insights into the emergence of sophisticated look-ahead capabilities in neural networks trained on strategic tasks, contributing to our understanding of AI reasoning in complex domains. Our work also showcases the effectiveness of interpretability techniques in uncovering cognitive-like processes in artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSTNet: Multimodal Meta-learning for Cellular Traffic Conformal Prediction</title>
<link>https://arxiv.org/abs/2505.21553</link>
<guid>https://arxiv.org/abs/2505.21553</guid>
<content:encoded><![CDATA[
arXiv:2505.21553v1 Announce Type: cross 
Abstract: Network traffic prediction techniques have attracted much attention since they are valuable for network congestion control and user experience improvement. While existing prediction techniques can achieve favorable performance when there is sufficient training data, it remains a great challenge to make accurate predictions when only a small amount of training data is available. To tackle this problem, we propose a deep learning model, entitled MetaSTNet, based on a multimodal meta-learning framework. It is an end-to-end network architecture that trains the model in a simulator and transfers the meta-knowledge to a real-world environment, which can quickly adapt and obtain accurate predictions on a new task with only a small amount of real-world training data. In addition, we further employ cross conformal prediction to assess the calibrated prediction intervals. Extensive experiments have been conducted on real-world datasets to illustrate the efficiency and effectiveness of MetaSTNet.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Convolutional Neural Network-Based Framework for Complex Multiclass Brassica Seed Classification</title>
<link>https://arxiv.org/abs/2505.21558</link>
<guid>https://arxiv.org/abs/2505.21558</guid>
<content:encoded><![CDATA[
arXiv:2505.21558v1 Announce Type: cross 
Abstract: Agricultural research has accelerated in recent years, yet farmers often lack the time and resources for on-farm research due to the demands of crop production and farm operations. Seed classification offers valuable insights into quality control, production efficiency, and impurity detection. Early identification of seed types is critical to reducing the cost and risk associated with field emergence, which can lead to yield losses or disruptions in downstream processes like harvesting. Seed sampling supports growers in monitoring and managing seed quality, improving precision in determining seed purity levels, guiding management adjustments, and enhancing yield estimations. This study proposes a novel convolutional neural network (CNN)-based framework for the efficient classification of ten common Brassica seed types. The approach addresses the inherent challenge of texture similarity in seed images using a custom-designed CNN architecture. The model's performance was evaluated against several pre-trained state-of-the-art architectures, with adjustments to layer configurations for optimized classification. Experimental results using our collected Brassica seed dataset demonstrate that the proposed model achieved a high accuracy rate of 93 percent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Distillation Approach for SOS Fusion Staging: Towards Fully Automated Skeletal Maturity Assessment</title>
<link>https://arxiv.org/abs/2505.21561</link>
<guid>https://arxiv.org/abs/2505.21561</guid>
<content:encoded><![CDATA[
arXiv:2505.21561v1 Announce Type: cross 
Abstract: We introduce a novel deep learning framework for the automated staging of spheno-occipital synchondrosis (SOS) fusion, a critical diagnostic marker in both orthodontics and forensic anthropology. Our approach leverages a dual-model architecture wherein a teacher model, trained on manually cropped images, transfers its precise spatial understanding to a student model that operates on full, uncropped images. This knowledge distillation is facilitated by a newly formulated loss function that aligns spatial logits as well as incorporates gradient-based attention spatial mapping, ensuring that the student model internalizes the anatomically relevant features without relying on external cropping or YOLO-based segmentation. By leveraging expert-curated data and feedback at each step, our framework attains robust diagnostic accuracy, culminating in a clinically viable end-to-end pipeline. This streamlined approach obviates the need for additional pre-processing tools and accelerates deployment, thereby enhancing both the efficiency and consistency of skeletal maturation assessment in diverse clinical settings.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-instance Learning as Downstream Task of Self-Supervised Learning-based Pre-trained Model</title>
<link>https://arxiv.org/abs/2505.21564</link>
<guid>https://arxiv.org/abs/2505.21564</guid>
<content:encoded><![CDATA[
arXiv:2505.21564v1 Announce Type: cross 
Abstract: In deep multi-instance learning, the number of applicable instances depends on the data set. In histopathology images, deep learning multi-instance learners usually assume there are hundreds to thousands instances in a bag. However, when the number of instances in a bag increases to 256 in brain hematoma CT, learning becomes extremely difficult. In this paper, we address this drawback. To overcome this problem, we propose using a pre-trained model with self-supervised learning for the multi-instance learner as a downstream task. With this method, even when the original target task suffers from the spurious correlation problem, we show improvements of 5% to 13% in accuracy and 40% to 55% in the F1 measure for the hypodensity marker classification of brain hematoma CT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Model-based Activity Completion for AI Motion Capture from Videos</title>
<link>https://arxiv.org/abs/2505.21566</link>
<guid>https://arxiv.org/abs/2505.21566</guid>
<content:encoded><![CDATA[
arXiv:2505.21566v1 Announce Type: cross 
Abstract: AI-based motion capture is an emerging technology that offers a cost-effective alternative to traditional motion capture systems. However, current AI motion capture methods rely entirely on observed video sequences, similar to conventional motion capture. This means that all human actions must be predefined, and movements outside the observed sequences are not possible. To address this limitation, we aim to apply AI motion capture to virtual humans, where flexible actions beyond the observed sequences are required. We assume that while many action fragments exist in the training data, the transitions between them may be missing. To bridge these gaps, we propose a diffusion-model-based action completion technique that generates complementary human motion sequences, ensuring smooth and continuous movements. By introducing a gate module and a position-time embedding module, our approach achieves competitive results on the Human3.6M dataset. Our experimental results show that (1) MDC-Net outperforms existing methods in ADE, FDE, and MMADE but is slightly less accurate in MMFDE, (2) MDC-Net has a smaller model size (16.84M) compared to HumanMAC (28.40M), and (3) MDC-Net generates more natural and coherent motion sequences. Additionally, we propose a method for extracting sensor data, including acceleration and angular velocity, from human motion sequences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EaqVLA: Encoding-aligned Quantization for Vision-Language-Action Models</title>
<link>https://arxiv.org/abs/2505.21567</link>
<guid>https://arxiv.org/abs/2505.21567</guid>
<content:encoded><![CDATA[
arXiv:2505.21567v1 Announce Type: cross 
Abstract: With the development of Embodied Artificial intelligence, the end-to-end control policy such as Vision-Language-Action (VLA) model has become the mainstream. Existing VLA models faces expensive computing/storage cost, which need to be optimized. Quantization is considered as the most effective method which can not only reduce the memory cost but also achieve computation acceleration. However, we find the token alignment of VLA models hinders the application of existing quantization methods. To address this, we proposed an optimized framework called EaqVLA, which apply encoding-aligned quantization to VLA models. Specifically, we propose an complete analysis method to find the misalignment in various granularity. Based on the analysis results, we propose a mixed precision quantization with the awareness of encoding alignment. Experiments shows that the porposed EaqVLA achieves better quantization performance (with the minimal quantization loss for end-to-end action control and xxx times acceleration) than existing quantization methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thickness-aware E(3)-Equivariant 3D Mesh Neural Networks</title>
<link>https://arxiv.org/abs/2505.21572</link>
<guid>https://arxiv.org/abs/2505.21572</guid>
<content:encoded><![CDATA[
arXiv:2505.21572v1 Announce Type: cross 
Abstract: Mesh-based 3D static analysis methods have recently emerged as efficient alternatives to traditional computational numerical solvers, significantly reducing computational costs and runtime for various physics-based analyses. However, these methods primarily focus on surface topology and geometry, often overlooking the inherent thickness of real-world 3D objects, which exhibits high correlations and similar behavior between opposing surfaces. This limitation arises from the disconnected nature of these surfaces and the absence of internal edge connections within the mesh. In this work, we propose a novel framework, the Thickness-aware E(3)-Equivariant 3D Mesh Neural Network (T-EMNN), that effectively integrates the thickness of 3D objects while maintaining the computational efficiency of surface meshes. Additionally, we introduce data-driven coordinates that encode spatial information while preserving E(3)-equivariance or invariance properties, ensuring consistent and robust analysis. Evaluations on a real-world industrial dataset demonstrate the superior performance of T-EMNN in accurately predicting node-level 3D deformations, effectively capturing thickness effects while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do We Need All the Synthetic Data? Towards Targeted Synthetic Image Augmentation via Diffusion Models</title>
<link>https://arxiv.org/abs/2505.21574</link>
<guid>https://arxiv.org/abs/2505.21574</guid>
<content:encoded><![CDATA[
arXiv:2505.21574v1 Announce Type: cross 
Abstract: Synthetically augmenting training datasets with diffusion models has been an effective strategy for improving generalization of image classifiers. However, existing techniques struggle to ensure the diversity of generation and increase the size of the data by up to 10-30x to improve the in-distribution performance. In this work, we show that synthetically augmenting part of the data that is not learned early in training outperforms augmenting the entire dataset. By analyzing a two-layer CNN, we prove that this strategy improves generalization by promoting homogeneity in feature learning speed without amplifying noise. Our extensive experiments show that by augmenting only 30%-40% of the data, our method boosts the performance by up to 2.8% in a variety of scenarios, including training ResNet, ViT and DenseNet on CIFAR-10, CIFAR-100, and TinyImageNet, with a range of optimizers including SGD and SAM. Notably, our method applied with SGD outperforms the SOTA optimizer, SAM, on CIFAR-100 and TinyImageNet. It can also easily stack with existing weak and strong augmentation strategies to further boost the performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Kernelised Stein Discrepancy for Assessing the Fit of Inhomogeneous Random Graph Models</title>
<link>https://arxiv.org/abs/2505.21580</link>
<guid>https://arxiv.org/abs/2505.21580</guid>
<content:encoded><![CDATA[
arXiv:2505.21580v1 Announce Type: cross 
Abstract: Complex data are often represented as a graph, which in turn can often be viewed as a realisation of a random graph, such as of an inhomogeneous random graph model (IRG). For general fast goodness-of-fit tests in high dimensions, kernelised Stein discrepancy (KSD) tests are a powerful tool. Here, we develop, test, and analyse a KSD-type goodness-of-fit test for IRG models that can be carried out with a single observation of the network. The test is applicable to a network of any size and does not depend on the asymptotic distribution of the test statistic. We also provide theoretical guarantees.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Do you see what I see? An Ambiguous Optical Illusion Dataset exposing limitations of Explainable AI</title>
<link>https://arxiv.org/abs/2505.21589</link>
<guid>https://arxiv.org/abs/2505.21589</guid>
<content:encoded><![CDATA[
arXiv:2505.21589v1 Announce Type: cross 
Abstract: From uncertainty quantification to real-world object detection, we recognize the importance of machine learning algorithms, particularly in safety-critical domains such as autonomous driving or medical diagnostics. In machine learning, ambiguous data plays an important role in various machine learning domains. Optical illusions present a compelling area of study in this context, as they offer insight into the limitations of both human and machine perception. Despite this relevance, optical illusion datasets remain scarce. In this work, we introduce a novel dataset of optical illusions featuring intermingled animal pairs designed to evoke perceptual ambiguity. We identify generalizable visual concepts, particularly gaze direction and eye cues, as subtle yet impactful features that significantly influence model accuracy. By confronting models with perceptual ambiguity, our findings underscore the importance of concepts in visual learning and provide a foundation for studying bias and alignment between human and machine vision. To make this dataset useful for general purposes, we generate optical illusions systematically with different concepts discussed in our bias mitigation section. The dataset is accessible in Kaggle via https://kaggle.com/datasets/693bf7c6dd2cb45c8a863f9177350c8f9849a9508e9d50526e2ffcc5559a8333. Our source code can be found at https://github.com/KDD-OpenSource/Ambivision.git.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taylor expansion-based Kolmogorov-Arnold network for blind image quality assessment</title>
<link>https://arxiv.org/abs/2505.21592</link>
<guid>https://arxiv.org/abs/2505.21592</guid>
<content:encoded><![CDATA[
arXiv:2505.21592v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Network (KAN) has attracted growing interest for its strong function approximation capability. In our previous work, KAN and its variants were explored in score regression for blind image quality assessment (BIQA). However, these models encounter challenges when processing high-dimensional features, leading to limited performance gains and increased computational cost. To address these issues, we propose TaylorKAN that leverages the Taylor expansions as learnable activation functions to enhance local approximation capability. To improve the computational efficiency, network depth reduction and feature dimensionality compression are integrated into the TaylorKAN-based score regression pipeline. On five databases (BID, CLIVE, KonIQ, SPAQ, and FLIVE) with authentic distortions, extensive experiments demonstrate that TaylorKAN consistently outperforms the other KAN-related models, indicating that the local approximation via Taylor expansions is more effective than global approximation using orthogonal functions. Its generalization capacity is validated through inter-database experiments. The findings highlight the potential of TaylorKAN as an efficient and robust model for high-dimensional score regression.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning optimal treatment strategies for intraoperative hypotension using deep reinforcement learning</title>
<link>https://arxiv.org/abs/2505.21596</link>
<guid>https://arxiv.org/abs/2505.21596</guid>
<content:encoded><![CDATA[
arXiv:2505.21596v1 Announce Type: cross 
Abstract: Traditional methods of surgical decision making heavily rely on human experience and prompt actions, which are variable. A data-driven system generating treatment recommendations based on patient states can be a substantial asset in perioperative decision-making, as in cases of intraoperative hypotension, for which suboptimal management is associated with acute kidney injury (AKI), a common and morbid postoperative complication. We developed a Reinforcement Learning (RL) model to recommend optimum dose of intravenous (IV) fluid and vasopressors during surgery to avoid intraoperative hypotension and postoperative AKI. We retrospectively analyzed 50,021 surgeries from 42,547 adult patients who underwent major surgery at a quaternary care hospital between June 2014 and September 2020. Of these, 34,186 surgeries were used for model training and 15,835 surgeries were reserved for testing. We developed a Deep Q-Networks based RL model using 16 variables including intraoperative physiologic time series, total dose of IV fluid and vasopressors extracted for every 15-minute epoch. The model replicated 69% of physician's decisions for the dosage of vasopressors and proposed higher or lower dosage of vasopressors than received in 10% and 21% of the treatments, respectively. In terms of IV fluids, the model's recommendations were within 0.05 ml/kg/15 min of the actual dose in 41% of the cases, with higher or lower doses recommended for 27% and 32% of the treatments, respectively. The model resulted in a higher estimated policy value compared to the physicians' actual treatments, as well as random and zero-drug policies. AKI prevalence was the lowest in patients receiving medication dosages that aligned with model's decisions. Our findings suggest that implementation of the model's policy has the potential to reduce postoperative AKI and improve other outcomes driven by intraoperative hypotension.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Deep Learning for Skin Cancer Classification: A Computationally Efficient CNN with Minimal Accuracy Trade-Off</title>
<link>https://arxiv.org/abs/2505.21597</link>
<guid>https://arxiv.org/abs/2505.21597</guid>
<content:encoded><![CDATA[
arXiv:2505.21597v1 Announce Type: cross 
Abstract: The rapid advancement of deep learning in medical image analysis has greatly enhanced the accuracy of skin cancer classification. However, current state-of-the-art models, especially those based on transfer learning like ResNet50, come with significant computational overhead, rendering them impractical for deployment in resource-constrained environments. This study proposes a custom CNN model that achieves a 96.7\% reduction in parameters (from 23.9 million in ResNet50 to 692,000) while maintaining a classification accuracy deviation of less than 0.022\%. Our empirical analysis of the HAM10000 dataset reveals that although transfer learning models provide a marginal accuracy improvement of approximately 0.022\%, they result in a staggering 13,216.76\% increase in FLOPs, considerably raising computational costs and inference latency. In contrast, our lightweight CNN architecture, which encompasses only 30.04 million FLOPs compared to ResNet50's 4.00 billion, significantly reduces energy consumption, memory footprint, and inference time. These findings underscore the trade-off between the complexity of deep models and their real-world feasibility, positioning our optimized CNN as a practical solution for mobile and edge-based skin cancer diagnostics.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing</title>
<link>https://arxiv.org/abs/2505.21600</link>
<guid>https://arxiv.org/abs/2505.21600</guid>
<content:encoded><![CDATA[
arXiv:2505.21600v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging XP and CRISP-DM for Agile Data Science Projects</title>
<link>https://arxiv.org/abs/2505.21603</link>
<guid>https://arxiv.org/abs/2505.21603</guid>
<content:encoded><![CDATA[
arXiv:2505.21603v1 Announce Type: cross 
Abstract: This study explores the integration of eXtreme Programming (XP) and the Cross-Industry Standard Process for Data Mining (CRISP-DM) in agile Data Science projects. We conducted a case study at the e-commerce company Elo7 to answer the research question: How can the agility of the XP method be integrated with CRISP-DM in Data Science projects? Data was collected through interviews and questionnaires with a Data Science team consisting of data scientists, ML engineers, and data product managers. The results show that 86% of the team frequently or always applies CRISP-DM, while 71% adopt XP practices in their projects. Furthermore, the study demonstrates that it is possible to combine CRISP-DM with XP in Data Science projects, providing a structured and collaborative approach. Finally, the study generated improvement recommendations for the company.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VideoMarkBench: Benchmarking Robustness of Video Watermarking</title>
<link>https://arxiv.org/abs/2505.21620</link>
<guid>https://arxiv.org/abs/2505.21620</guid>
<content:encoded><![CDATA[
arXiv:2505.21620v1 Announce Type: cross 
Abstract: The rapid development of video generative models has led to a surge in highly realistic synthetic videos, raising ethical concerns related to disinformation and copyright infringement. Recently, video watermarking has been proposed as a mitigation strategy by embedding invisible marks into AI-generated videos to enable subsequent detection. However, the robustness of existing video watermarking methods against both common and adversarial perturbations remains underexplored. In this work, we introduce VideoMarkBench, the first systematic benchmark designed to evaluate the robustness of video watermarks under watermark removal and watermark forgery attacks. Our study encompasses a unified dataset generated by three state-of-the-art video generative models, across three video styles, incorporating four watermarking methods and seven aggregation strategies used during detection. We comprehensively evaluate 12 types of perturbations under white-box, black-box, and no-box threat models. Our findings reveal significant vulnerabilities in current watermarking approaches and highlight the urgent need for more robust solutions. Our code is available at https://github.com/zhengyuan-jiang/VideoMarkBench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Your LLM Overcharging You? Tokenization, Transparency, and Incentives</title>
<link>https://arxiv.org/abs/2505.21627</link>
<guid>https://arxiv.org/abs/2505.21627</guid>
<content:encoded><![CDATA[
arXiv:2505.21627v1 Announce Type: cross 
Abstract: State-of-the-art large language models require specialized hardware and substantial energy to operate. As a consequence, cloud-based services that provide access to large language models have become very popular. In these services, the price users pay for an output provided by a model depends on the number of tokens the model uses to generate it -- they pay a fixed price per token. In this work, we show that this pricing mechanism creates a financial incentive for providers to strategize and misreport the (number of) tokens a model used to generate an output, and users cannot prove, or even know, whether a provider is overcharging them. However, we also show that, if an unfaithful provider is obliged to be transparent about the generative process used by the model, misreporting optimally without raising suspicion is hard. Nevertheless, as a proof-of-concept, we introduce an efficient heuristic algorithm that allows providers to significantly overcharge users without raising suspicion, highlighting the vulnerability of users under the current pay-per-token pricing mechanism. Further, to completely eliminate the financial incentive to strategize, we introduce a simple incentive-compatible token pricing mechanism. Under this mechanism, the price users pay for an output provided by a model depends on the number of characters of the output -- they pay a fixed price per character. Along the way, to illustrate and complement our theoretical results, we conduct experiments with several large language models from the $\texttt{Llama}$, $\texttt{Gemma}$ and $\texttt{Ministral}$ families, and input prompts from the LMSYS Chatbot Arena platform.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuARI: Query Adaptive Retrieval Improvement</title>
<link>https://arxiv.org/abs/2505.21647</link>
<guid>https://arxiv.org/abs/2505.21647</guid>
<content:encoded><![CDATA[
arXiv:2505.21647v1 Announce Type: cross 
Abstract: Massive-scale pretraining has made vision-language models increasingly popular for image-to-image and text-to-image retrieval across a broad collection of domains. However, these models do not perform well when used for challenging retrieval tasks, such as instance retrieval in very large-scale image collections. Recent work has shown that linear transformations of VLM features trained for instance retrieval can improve performance by emphasizing subspaces that relate to the domain of interest. In this paper, we explore a more extreme version of this specialization by learning to map a given query to a query-specific feature space transformation. Because this transformation is linear, it can be applied with minimal computational cost to millions of image embeddings, making it effective for large-scale retrieval or re-ranking. Results show that this method consistently outperforms state-of-the-art alternatives, including those that require many orders of magnitude more computation at query time.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainability of Large Language Models using SMILE: Statistical Model-agnostic Interpretability with Local Explanations</title>
<link>https://arxiv.org/abs/2505.21657</link>
<guid>https://arxiv.org/abs/2505.21657</guid>
<content:encoded><![CDATA[
arXiv:2505.21657v1 Announce Type: cross 
Abstract: Large language models like GPT, LLAMA, and Claude have become incredibly powerful at generating text, but they are still black boxes, so it is hard to understand how they decide what to say. That lack of transparency can be problematic, especially in fields where trust and accountability matter. To help with this, we introduce SMILE, a new method that explains how these models respond to different parts of a prompt. SMILE is model-agnostic and works by slightly changing the input, measuring how the output changes, and then highlighting which words had the most impact. Create simple visual heat maps showing which parts of a prompt matter the most. We tested SMILE on several leading LLMs and used metrics such as accuracy, consistency, stability, and fidelity to show that it gives clear and reliable explanations. By making these models easier to understand, SMILE brings us one step closer to making AI more transparent and trustworthy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STACI: Spatio-Temporal Aleatoric Conformal Inference</title>
<link>https://arxiv.org/abs/2505.21658</link>
<guid>https://arxiv.org/abs/2505.21658</guid>
<content:encoded><![CDATA[
arXiv:2505.21658v1 Announce Type: cross 
Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying correlation structure. However, spatio-temporal GPs suffer from issues of scalability and various forms of approximation bias resulting from restrictive assumptions of the covariance kernel function. We propose STACI, a novel framework consisting of a variational Bayesian neural network approximation of non-stationary spatio-temporal GP along with a novel spatio-temporal conformal inference algorithm. STACI is highly scalable, taking advantage of GPU training capabilities for neural network models, and provides statistically valid prediction intervals for uncertainty quantification. STACI outperforms competing GPs and deep methods in accurately approximating spatio-temporal processes and we show it easily scales to datasets with millions of observations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
<link>https://arxiv.org/abs/2505.21671</link>
<guid>https://arxiv.org/abs/2505.21671</guid>
<content:encoded><![CDATA[
arXiv:2505.21671v1 Announce Type: cross 
Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from a joint distribution $P$ that is Markov with respect to $G$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$ time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and $O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>tenSVD algorithm for compression</title>
<link>https://arxiv.org/abs/2505.21686</link>
<guid>https://arxiv.org/abs/2505.21686</guid>
<content:encoded><![CDATA[
arXiv:2505.21686v1 Announce Type: cross 
Abstract: Tensors provide a robust framework for managing high-dimensional data. Consequently, tensor analysis has emerged as an active research area in various domains, including machine learning, signal processing, computer vision, graph analysis, and data mining. This study introduces an efficient image storage approach utilizing tensors, aiming to minimize memory to store, bandwidth to transmit and energy to processing. The proposed method organizes original data into a higher-order tensor and applies the Tucker model for compression. Implemented in R, this method is compared to a baseline algorithm. The evaluation focuses on efficient of algorithm measured in term of computational time and the quality of information preserved, using both simulated and real datasets. A detailed analysis of the results is conducted, employing established quantitative metrics, with significant attention paid to sustainability in terms of energy consumption across algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLMPR: A Novel LLM-Driven Transfer Learning based Petition Ranking Model</title>
<link>https://arxiv.org/abs/2505.21689</link>
<guid>https://arxiv.org/abs/2505.21689</guid>
<content:encoded><![CDATA[
arXiv:2505.21689v1 Announce Type: cross 
Abstract: The persistent accumulation of unresolved legal cases, especially within the Indian judiciary, significantly hampers the timely delivery of justice. Manual methods of prioritizing petitions are often prone to inefficiencies and subjective biases further exacerbating delays. To address this issue, we propose LLMPR (Large Language Model-based Petition Ranking), an automated framework that utilizes transfer learning and machine learning to assign priority rankings to legal petitions based on their contextual urgency. Leveraging the ILDC dataset comprising 7,593 annotated petitions, we process unstructured legal text and extract features through various embedding techniques, including DistilBERT, LegalBERT, and MiniLM. These textual embeddings are combined with quantitative indicators such as gap days, rank scores, and word counts to train multiple machine learning models, including Random Forest, Decision Tree, XGBoost, LightGBM, and CatBoost. Our experiments demonstrate that Random Forest and Decision Tree models yield superior performance, with accuracy exceeding 99% and a Spearman rank correlation of 0.99. Notably, models using only numerical features achieve nearly optimal ranking results (R2 = 0.988, \r{ho} = 0.998), while LLM-based embeddings offer only marginal gains. These findings suggest that automated petition ranking can effectively streamline judicial workflows, reduce case backlog, and improve fairness in legal prioritization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What Data Enables Optimal Decisions? An Exact Characterization for Linear Optimization</title>
<link>https://arxiv.org/abs/2505.21692</link>
<guid>https://arxiv.org/abs/2505.21692</guid>
<content:encoded><![CDATA[
arXiv:2505.21692v1 Announce Type: cross 
Abstract: We study the fundamental question of how informative a dataset is for solving a given decision-making task. In our setting, the dataset provides partial information about unknown parameters that influence task outcomes. Focusing on linear programs, we characterize when a dataset is sufficient to recover an optimal decision, given an uncertainty set on the cost vector. Our main contribution is a sharp geometric characterization that identifies the directions of the cost vector that matter for optimality, relative to the task constraints and uncertainty set. We further develop a practical algorithm that, for a given task, constructs a minimal or least-costly sufficient dataset. Our results reveal that small, well-chosen datasets can often fully determine optimal decisions -- offering a principled foundation for task-aware data selection.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Network classification through random walks</title>
<link>https://arxiv.org/abs/2505.21706</link>
<guid>https://arxiv.org/abs/2505.21706</guid>
<content:encoded><![CDATA[
arXiv:2505.21706v1 Announce Type: cross 
Abstract: Network models have been widely used to study diverse systems and analyze their dynamic behaviors. Given the structural variability of networks, an intriguing question arises: Can we infer the type of system represented by a network based on its structure? This classification problem involves extracting relevant features from the network. Existing literature has proposed various methods that combine structural measurements and dynamical processes for feature extraction. In this study, we introduce a novel approach to characterize networks using statistics from random walks, which can be particularly informative about network properties. We present the employed statistical metrics and compare their performance on multiple datasets with other state-of-the-art feature extraction methods. Our results demonstrate that the proposed method is effective in many cases, often outperforming existing approaches, although some limitations are observed across certain datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Nearly Dimension-Independent Convergence of Mean-Field Black-Box Variational Inference</title>
<link>https://arxiv.org/abs/2505.21721</link>
<guid>https://arxiv.org/abs/2505.21721</guid>
<content:encoded><![CDATA[
arXiv:2505.21721v1 Announce Type: cross 
Abstract: We prove that, given a mean-field location-scale variational family, black-box variational inference (BBVI) with the reparametrization gradient converges at an almost dimension-independent rate. Specifically, for strongly log-concave and log-smooth targets, the number of iterations for BBVI with a sub-Gaussian family to achieve an objective $\epsilon$-close to the global optimum is $\mathrm{O}(\log d)$, which improves over the $\mathrm{O}(d)$ dependence of full-rank location-scale families. For heavy-tailed families, we provide a weaker $\mathrm{O}(d^{2/k})$ dimension dependence, where $k$ is the number of finite moments. Additionally, if the Hessian of the target log-density is constant, the complexity is free of any explicit dimension dependence. We also prove that our bound on the gradient variance, which is key to our result, cannot be improved using only spectral bounds on the Hessian of the target log-density.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Statistical Methods Obsolete in the Era of Deep Learning?</title>
<link>https://arxiv.org/abs/2505.21723</link>
<guid>https://arxiv.org/abs/2505.21723</guid>
<content:encoded><![CDATA[
arXiv:2505.21723v1 Announce Type: cross 
Abstract: In the era of AI, neural networks have become increasingly popular for modeling, inference, and prediction, largely due to their potential for universal approximation. With the proliferation of such deep learning models, a question arises: are leaner statistical methods still relevant? To shed insight on this question, we employ the mechanistic nonlinear ordinary differential equation (ODE) inverse problem as a testbed, using physics-informed neural network (PINN) as a representative of the deep learning paradigm and manifold-constrained Gaussian process inference (MAGI) as a representative of statistically principled methods. Through case studies involving the SEIR model from epidemiology and the Lorenz model from chaotic dynamics, we demonstrate that statistical methods are far from obsolete, especially when working with sparse and noisy observations. On tasks such as parameter inference and trajectory reconstruction, statistically principled methods consistently achieve lower bias and variance, while using far fewer parameters and requiring less hyperparameter tuning. Statistical methods can also decisively outperform deep learning models on out-of-sample future prediction, where the absence of relevant data often leads overparameterized models astray. Additionally, we find that statistically principled approaches are more robust to accumulation of numerical imprecision and can represent the underlying system more faithful to the true governing ODEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MIND-Stack: Modular, Interpretable, End-to-End Differentiability for Autonomous Navigation</title>
<link>https://arxiv.org/abs/2505.21734</link>
<guid>https://arxiv.org/abs/2505.21734</guid>
<content:encoded><![CDATA[
arXiv:2505.21734v1 Announce Type: cross 
Abstract: Developing robust, efficient navigation algorithms is challenging. Rule-based methods offer interpretability and modularity but struggle with learning from large datasets, while end-to-end neural networks excel in learning but lack transparency and modularity. In this paper, we present MIND-Stack, a modular software stack consisting of a localization network and a Stanley Controller with intermediate human interpretable state representations and end-to-end differentiability. Our approach enables the upstream localization module to reduce the downstream control error, extending its role beyond state estimation. Unlike existing research on differentiable algorithms that either lack modules of the autonomous stack to span from sensor input to actuator output or real-world implementation, MIND-Stack offers both capabilities. We conduct experiments that demonstrate the ability of the localization module to reduce the downstream control loss through its end-to-end differentiability while offering better performance than state-of-the-art algorithms. We showcase sim-to-real capabilities by deploying the algorithm on a real-world embedded autonomous platform with limited computation power and demonstrate simultaneous training of both the localization and controller towards one goal. While MIND-Stack shows good results, we discuss the incorporation of additional modules from the autonomous navigation pipeline in the future, promising even greater stability and performance in the next iterations of the framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment kernels: a simple and scalable approach for equivariance to rotations and reflections in deep convolutional networks</title>
<link>https://arxiv.org/abs/2505.21736</link>
<guid>https://arxiv.org/abs/2505.21736</guid>
<content:encoded><![CDATA[
arXiv:2505.21736v1 Announce Type: cross 
Abstract: The principle of translation equivariance (if an input image is translated an output image should be translated by the same amount), led to the development of convolutional neural networks that revolutionized machine vision. Other symmetries, like rotations and reflections, play a similarly critical role, especially in biomedical image analysis, but exploiting these symmetries has not seen wide adoption. We hypothesize that this is partially due to the mathematical complexity of methods used to exploit these symmetries, which often rely on representation theory, a bespoke concept in differential geometry and group theory. In this work, we show that the same equivariance can be achieved using a simple form of convolution kernels that we call ``moment kernels,'' and prove that all equivariant kernels must take this form. These are a set of radially symmetric functions of a spatial position $x$, multiplied by powers of the components of $x$ or the identity matrix. We implement equivariant neural networks using standard convolution modules, and provide architectures to execute several biomedical image analysis tasks that depend on equivariance principles: classification (outputs are invariant under orthogonal transforms), 3D image registration (outputs transform like a vector), and cell segmentation (quadratic forms defining ellipses transform like a matrix).
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Adversarial Training for Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.21742</link>
<guid>https://arxiv.org/abs/2505.21742</guid>
<content:encoded><![CDATA[
arXiv:2505.21742v1 Announce Type: cross 
Abstract: We answer the question in the title, showing that adversarial training (AT) for diffusion models (DMs) fundamentally differs from classifiers: while AT in classifiers enforces output invariance, AT in DMs requires equivariance to keep the diffusion process aligned with the data distribution. AT is a way to enforce smoothness in the diffusion flow, improving robustness to outliers and corrupted data. Unlike prior art, our method makes no assumptions about the noise model and integrates seamlessly into diffusion training by adding random noise, similar to randomized smoothing, or adversarial noise, akin to AT. This enables intrinsic capabilities such as handling noisy data, dealing with extreme variability such as outliers, preventing memorization, and improving robustness. We rigorously evaluate our approach with proof-of-concept datasets with known distributions in low- and high-dimensional space, thereby taking a perfect measure of errors; we further evaluate on standard benchmarks such as CIFAR-10, CelebA and LSUN Bedroom, showing strong performance under severe noise, data corruption, and iterative adversarial attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to See More: UAS-Guided Super-Resolution of Satellite Imagery for Precision Agriculture</title>
<link>https://arxiv.org/abs/2505.21746</link>
<guid>https://arxiv.org/abs/2505.21746</guid>
<content:encoded><![CDATA[
arXiv:2505.21746v1 Announce Type: cross 
Abstract: Unmanned Aircraft Systems (UAS) and satellites are key data sources for precision agriculture, yet each presents trade-offs. Satellite data offer broad spatial, temporal, and spectral coverage but lack the resolution needed for many precision farming applications, while UAS provide high spatial detail but are limited by coverage and cost, especially for hyperspectral data. This study presents a novel framework that fuses satellite and UAS imagery using super-resolution methods. By integrating data across spatial, spectral, and temporal domains, we leverage the strengths of both platforms cost-effectively. We use estimation of cover crop biomass and nitrogen (N) as a case study to evaluate our approach. By spectrally extending UAS RGB data to the vegetation red edge and near-infrared regions, we generate high-resolution Sentinel-2 imagery and improve biomass and N estimation accuracy by 18% and 31%, respectively. Our results show that UAS data need only be collected from a subset of fields and time points. Farmers can then 1) enhance the spectral detail of UAS RGB imagery; 2) increase the spatial resolution by using satellite data; and 3) extend these enhancements spatially and across the growing season at the frequency of the satellite flights. Our SRCNN-based spectral extension model shows considerable promise for model transferability over other cropping systems in the Upper and Lower Chesapeake Bay regions. Additionally, it remains effective even when cloud-free satellite data are unavailable, relying solely on the UAS RGB input. The spatial extension model produces better biomass and N predictions than models built on raw UAS RGB images. Once trained with targeted UAS RGB data, the spatial extension model allows farmers to stop repeated UAS flights. While we introduce super-resolution advances, the core contribution is a lightweight and scalable system for affordable on-farm use.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRAMES-VQA: Benchmarking Fine-Tuning Robustness across Multi-Modal Shifts in Visual Question Answering</title>
<link>https://arxiv.org/abs/2505.21755</link>
<guid>https://arxiv.org/abs/2505.21755</guid>
<content:encoded><![CDATA[
arXiv:2505.21755v1 Announce Type: cross 
Abstract: Visual question answering (VQA) systems face significant challenges when adapting to real-world data shifts, especially in multi-modal contexts. While robust fine-tuning strategies are essential for maintaining performance across in-distribution (ID) and out-of-distribution (OOD) scenarios, current evaluation settings are primarily unimodal or particular to some types of OOD, offering limited insight into the complexities of multi-modal contexts. In this work, we propose a new benchmark FRAMES-VQA (Fine-Tuning Robustness across Multi-Modal Shifts in VQA) for evaluating robust fine-tuning for VQA tasks. We utilize ten existing VQA benchmarks, including VQAv2, IV-VQA, VQA-CP, OK-VQA and others, and categorize them into ID, near and far OOD datasets covering uni-modal, multi-modal and adversarial distribution shifts. We first conduct a comprehensive comparison of existing robust fine-tuning methods. We then quantify the distribution shifts by calculating the Mahalanobis distance using uni-modal and multi-modal embeddings extracted from various models. Further, we perform an extensive analysis to explore the interactions between uni- and multi-modal shifts as well as modality importance for ID and OOD samples. These analyses offer valuable guidance on developing more robust fine-tuning methods to handle multi-modal distribution shifts. The code is available at https://github.com/chengyuehuang511/FRAMES-VQA .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond 1D: Vision Transformers and Multichannel Signal Images for PPG-to-ECG Reconstruction</title>
<link>https://arxiv.org/abs/2505.21767</link>
<guid>https://arxiv.org/abs/2505.21767</guid>
<content:encoded><![CDATA[
arXiv:2505.21767v1 Announce Type: cross 
Abstract: Reconstructing ECG from PPG is a promising yet challenging task. While recent advancements in generative models have significantly improved ECG reconstruction, accurately capturing fine-grained waveform features remains a key challenge. To address this, we propose a novel PPG-to-ECG reconstruction method that leverages a Vision Transformer (ViT) as the core network. Unlike conventional approaches that rely on single-channel PPG, our method employs a four-channel signal image representation, incorporating the original PPG, its first-order difference, second-order difference, and area under the curve. This multi-channel design enriches feature extraction by preserving both temporal and physiological variations within the PPG. By leveraging the self-attention mechanism in ViT, our approach effectively captures both inter-beat and intra-beat dependencies, leading to more robust and accurate ECG reconstruction. Experimental results demonstrate that our method consistently outperforms existing 1D convolution-based approaches, achieving up to 29% reduction in PRD and 15% reduction in RMSE. The proposed approach also produces improvements in other evaluation metrics, highlighting its robustness and effectiveness in reconstructing ECG signals. Furthermore, to ensure a clinically relevant evaluation, we introduce new performance metrics, including QRS area error, PR interval error, RT interval error, and RT amplitude difference error. Our findings suggest that integrating a four-channel signal image representation with the self-attention mechanism of ViT enables more effective extraction of informative PPG features and improved modeling of beat-to-beat variations for PPG-to-ECG mapping. Beyond demonstrating the potential of PPG as a viable alternative for heart activity monitoring, our approach opens new avenues for cyclic signal analysis and prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Global Minimizers of $\ell^p$-Regularized Objectives Yield the Sparsest ReLU Neural Networks</title>
<link>https://arxiv.org/abs/2505.21791</link>
<guid>https://arxiv.org/abs/2505.21791</guid>
<content:encoded><![CDATA[
arXiv:2505.21791v1 Announce Type: cross 
Abstract: Overparameterized neural networks can interpolate a given dataset in many different ways, prompting the fundamental question: which among these solutions should we prefer, and what explicit regularization strategies will provably yield these solutions? This paper addresses the challenge of finding the sparsest interpolating ReLU network -- i.e., the network with the fewest nonzero parameters or neurons -- a goal with wide-ranging implications for efficiency, generalization, interpretability, theory, and model compression. Unlike post hoc pruning approaches, we propose a continuous, almost-everywhere differentiable training objective whose global minima are guaranteed to correspond to the sparsest single-hidden-layer ReLU networks that fit the data. This result marks a conceptual advance: it recasts the combinatorial problem of sparse interpolation as a smooth optimization task, potentially enabling the use of gradient-based training methods. Our objective is based on minimizing $\ell^p$ quasinorms of the weights for $0 < p < 1$, a classical sparsity-promoting strategy in finite-dimensional settings. However, applying these ideas to neural networks presents new challenges: the function class is infinite-dimensional, and the weights are learned using a highly nonconvex objective. We prove that, under our formulation, global minimizers correspond exactly to sparsest solutions. Our work lays a foundation for understanding when and how continuous sparsity-inducing objectives can be leveraged to recover sparse networks through training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A General-Purpose Theorem for High-Probability Bounds of Stochastic Approximation with Polyak Averaging</title>
<link>https://arxiv.org/abs/2505.21796</link>
<guid>https://arxiv.org/abs/2505.21796</guid>
<content:encoded><![CDATA[
arXiv:2505.21796v1 Announce Type: cross 
Abstract: Polyak-Ruppert averaging is a widely used technique to achieve the optimal asymptotic variance of stochastic approximation (SA) algorithms, yet its high-probability performance guarantees remain underexplored in general settings. In this paper, we present a general framework for establishing non-asymptotic concentration bounds for the error of averaged SA iterates. Our approach assumes access to individual concentration bounds for the unaveraged iterates and yields a sharp bound on the averaged iterates. We also construct an example, showing the tightness of our result up to constant multiplicative factors. As direct applications, we derive tight concentration bounds for contractive SA algorithms and for algorithms such as temporal difference learning and Q-learning with averaging, obtaining new bounds in settings where traditional analysis is challenging.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PolarGrad: A Class of Matrix-Gradient Optimizers from a Unifying Preconditioning Perspective</title>
<link>https://arxiv.org/abs/2505.21799</link>
<guid>https://arxiv.org/abs/2505.21799</guid>
<content:encoded><![CDATA[
arXiv:2505.21799v1 Announce Type: cross 
Abstract: The ever-growing scale of deep learning models and datasets underscores the critical importance of efficient optimization methods. While preconditioned gradient methods such as Adam and AdamW are the de facto optimizers for training neural networks and large language models, structure-aware preconditioned optimizers like Shampoo and Muon, which utilize the matrix structure of gradients, have demonstrated promising evidence of faster convergence. In this paper, we introduce a unifying framework for analyzing "matrix-aware" preconditioned methods, which not only sheds light on the effectiveness of Muon and related optimizers but also leads to a class of new structure-aware preconditioned methods. A key contribution of this framework is its precise distinction between preconditioning strategies that treat neural network weights as vectors (addressing curvature anisotropy) versus those that consider their matrix structure (addressing gradient anisotropy). This perspective provides new insights into several empirical phenomena in language model pre-training, including Adam's training instabilities, Muon's accelerated convergence, and the necessity of learning rate warmup for Adam. Building upon this framework, we introduce PolarGrad, a new class of preconditioned optimization methods based on the polar decomposition of matrix-valued gradients. As a special instance, PolarGrad includes Muon with updates scaled by the nuclear norm of the gradients. We provide numerical implementations of these methods, leveraging efficient numerical polar decomposition algorithms for enhanced convergence. Our extensive evaluations across diverse matrix optimization problems and language model pre-training tasks demonstrate that PolarGrad outperforms both Adam and Muon.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voice Quality Dimensions as Interpretable Primitives for Speaking Style for Atypical Speech and Affect</title>
<link>https://arxiv.org/abs/2505.21809</link>
<guid>https://arxiv.org/abs/2505.21809</guid>
<content:encoded><![CDATA[
arXiv:2505.21809v1 Announce Type: cross 
Abstract: Perceptual voice quality dimensions describe key characteristics of atypical speech and other speech modulations. Here we develop and evaluate voice quality models for seven voice and speech dimensions (intelligibility, imprecise consonants, harsh voice, naturalness, monoloudness, monopitch, and breathiness). Probes were trained on the public Speech Accessibility (SAP) project dataset with 11,184 samples from 434 speakers, using embeddings from frozen pre-trained models as features. We found that our probes had both strong performance and strong generalization across speech elicitation categories in the SAP dataset. We further validated zero-shot performance on additional datasets, encompassing unseen languages and tasks: Italian atypical speech, English atypical speech, and affective speech. The strong zero-shot performance and the interpretability of results across an array of evaluations suggests the utility of using voice quality dimensions in speaking style-related tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Language Generation</title>
<link>https://arxiv.org/abs/2505.21819</link>
<guid>https://arxiv.org/abs/2505.21819</guid>
<content:encoded><![CDATA[
arXiv:2505.21819v1 Announce Type: cross 
Abstract: We introduce "representative generation," extending the theoretical framework for generation proposed by Kleinberg et al. (2024) and formalized by Li et al. (2024), to additionally address diversity and bias concerns in generative models. Our notion requires outputs of a generative model to proportionally represent groups of interest from the training data. We characterize representative uniform and non-uniform generation, introducing the "group closure dimension" as a key combinatorial quantity. For representative generation in the limit, we analyze both information-theoretic and computational aspects, demonstrating feasibility for countably infinite hypothesis classes and collections of groups under certain conditions, but proving a negative result for computability using only membership queries. This contrasts with Kleinberg et al.'s (2024) positive results for standard generation in the limit. Our findings provide a rigorous foundation for developing more diverse and representative generative models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music Source Restoration</title>
<link>https://arxiv.org/abs/2505.21827</link>
<guid>https://arxiv.org/abs/2505.21827</guid>
<content:encoded><![CDATA[
arXiv:2505.21827v1 Announce Type: cross 
Abstract: We introduce Music Source Restoration (MSR), a novel task addressing the gap between idealized source separation and real-world music production. Current Music Source Separation (MSS) approaches assume mixtures are simple sums of sources, ignoring signal degradations employed during music production like equalization, compression, and reverb. MSR models mixtures as degraded sums of individually degraded sources, with the goal of recovering original, undegraded signals. Due to the lack of data for MSR, we present RawStems, a dataset annotation of 578 songs with unprocessed source signals organized into 8 primary and 17 secondary instrument groups, totaling 354.13 hours. To the best of our knowledge, RawStems is the first dataset that contains unprocessed music stems with hierarchical categories. We consider spectral filtering, dynamic range compression, harmonic distortion, reverb and lossy codec as possible degradations, and establish U-Former as a baseline method, demonstrating the feasibility of MSR on our dataset. We release the RawStems dataset annotations, degradation simulation pipeline, training code and pre-trained models to be publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UniMoGen: Universal Motion Generation</title>
<link>https://arxiv.org/abs/2505.21837</link>
<guid>https://arxiv.org/abs/2505.21837</guid>
<content:encoded><![CDATA[
arXiv:2505.21837v1 Announce Type: cross 
Abstract: Motion generation is a cornerstone of computer graphics, animation, gaming, and robotics, enabling the creation of realistic and varied character movements. A significant limitation of existing methods is their reliance on specific skeletal structures, which restricts their versatility across different characters. To overcome this, we introduce UniMoGen, a novel UNet-based diffusion model designed for skeleton-agnostic motion generation. UniMoGen can be trained on motion data from diverse characters, such as humans and animals, without the need for a predefined maximum number of joints. By dynamically processing only the necessary joints for each character, our model achieves both skeleton agnosticism and computational efficiency. Key features of UniMoGen include controllability via style and trajectory inputs, and the ability to continue motions from past frames. We demonstrate UniMoGen's effectiveness on the 100style dataset, where it outperforms state-of-the-art methods in diverse character motion generation. Furthermore, when trained on both the 100style and LAFAN1 datasets, which use different skeletons, UniMoGen achieves high performance and improved efficiency across both skeletons. These results highlight UniMoGen's potential to advance motion generation by providing a flexible, efficient, and controllable solution for a wide range of character animations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Informed Learning Framework to Solve the Infinite-Horizon Optimal Control Problem</title>
<link>https://arxiv.org/abs/2505.21842</link>
<guid>https://arxiv.org/abs/2505.21842</guid>
<content:encoded><![CDATA[
arXiv:2505.21842v1 Announce Type: cross 
Abstract: We propose a physics-informed neural networks (PINNs) framework to solve the infinite-horizon optimal control problem of nonlinear systems. In particular, since PINNs are generally able to solve a class of partial differential equations (PDEs), they can be employed to learn the value function of the infinite-horizon optimal control problem via solving the associated steady-state Hamilton-Jacobi-Bellman (HJB) equation. However, an issue here is that the steady-state HJB equation generally yields multiple solutions; hence if PINNs are directly employed to it, they may end up approximating a solution that is different from the optimal value function of the problem. We tackle this by instead applying PINNs to a finite-horizon variant of the steady-state HJB that has a unique solution, and which uniformly approximates the optimal value function as the horizon increases. An algorithm to verify if the chosen horizon is large enough is also given, as well as a method to extend it -- with reduced computations and robustness to approximation errors -- in case it is not. Unlike many existing methods, the proposed technique works well with non-polynomial basis functions, does not require prior knowledge of a stabilizing controller, and does not perform iterative policy evaluations. Simulations are performed, which verify and clarify theoretical findings.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spectral clustering for dependent community Hawkes process models of temporal networks</title>
<link>https://arxiv.org/abs/2505.21845</link>
<guid>https://arxiv.org/abs/2505.21845</guid>
<content:encoded><![CDATA[
arXiv:2505.21845v1 Announce Type: cross 
Abstract: Temporal networks observed continuously over time through timestamped relational events data are commonly encountered in application settings including online social media communications, financial transactions, and international relations. Temporal networks often exhibit community structure and strong dependence patterns among node pairs. This dependence can be modeled through mutual excitations, where an interaction event from a sender to a receiver node increases the possibility of future events among other node pairs.
  We provide statistical results for a class of models that we call dependent community Hawkes (DCH) models, which combine the stochastic block model with mutually exciting Hawkes processes for modeling both community structure and dependence among node pairs, respectively. We derive a non-asymptotic upper bound on the misclustering error of spectral clustering on the event count matrix as a function of the number of nodes and communities, time duration, and the amount of dependence in the model. Our result leverages recent results on bounding an appropriate distance between a multivariate Hawkes process count vector and a Gaussian vector, along with results from random matrix theory. We also propose a DCH model that incorporates only self and reciprocal excitation along with highly scalable parameter estimation using a Generalized Method of Moments (GMM) estimator that we demonstrate to be consistent for growing network size and time duration.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Streaming Flow Policy: Simplifying diffusion$/$flow-matching policies by treating action trajectories as flow trajectories</title>
<link>https://arxiv.org/abs/2505.21851</link>
<guid>https://arxiv.org/abs/2505.21851</guid>
<content:encoded><![CDATA[
arXiv:2505.21851v1 Announce Type: cross 
Abstract: Recent advances in diffusion$/$flow-matching policies have enabled imitation learning of complex, multi-modal action trajectories. However, they are computationally expensive because they sample a trajectory of trajectories: a diffusion$/$flow trajectory of action trajectories. They discard intermediate action trajectories, and must wait for the sampling process to complete before any actions can be executed on the robot. We simplify diffusion$/$flow policies by treating action trajectories as flow trajectories. Instead of starting from pure noise, our algorithm samples from a narrow Gaussian around the last action. Then, it incrementally integrates a velocity field learned via flow matching to produce a sequence of actions that constitute a single trajectory. This enables actions to be streamed to the robot on-the-fly during the flow sampling process, and is well-suited for receding horizon policy execution. Despite streaming, our method retains the ability to model multi-modal behavior. We train flows that stabilize around demonstration trajectories to reduce distribution shift and improve imitation learning performance. Streaming flow policy outperforms prior methods while enabling faster policy execution and tighter sensorimotor loops for learning-based robot control. Project website: https://streaming-flow-policy.github.io/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images</title>
<link>https://arxiv.org/abs/2505.21872</link>
<guid>https://arxiv.org/abs/2505.21872</guid>
<content:encoded><![CDATA[
arXiv:2505.21872v1 Announce Type: cross 
Abstract: Machine unlearning aims to remove the influence of specific training samples from a trained model without full retraining. While prior work has largely focused on privacy-motivated settings, we recast unlearning as a general-purpose tool for post-deployment model revision. Specifically, we focus on utilizing unlearning in clinical contexts where data shifts, device deprecation, and policy changes are common. To this end, we propose a bilevel optimization formulation of boundary-based unlearning that can be solved using iterative algorithms. We provide convergence guarantees when first-order algorithms are used to unlearn. Our method introduces tunable loss design for controlling the forgetting-retention tradeoff and supports novel model composition strategies that merge the strengths of distinct unlearning runs. Across benchmark and real-world clinical imaging datasets, our approach outperforms baselines on both forgetting and retention metrics, including scenarios involving imaging devices and anatomical outliers. This work establishes machine unlearning as a modular, practical alternative to retraining for real-world model maintenance in clinical applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symbolic Foundation Regressor on Complex Networks</title>
<link>https://arxiv.org/abs/2505.21879</link>
<guid>https://arxiv.org/abs/2505.21879</guid>
<content:encoded><![CDATA[
arXiv:2505.21879v1 Announce Type: cross 
Abstract: In science, we are interested not only in forecasting but also in understanding how predictions are made, specifically what the interpretable underlying model looks like. Data-driven machine learning technology can significantly streamline the complex and time-consuming traditional manual process of discovering scientific laws, helping us gain insights into fundamental issues in modern science. In this work, we introduce a pre-trained symbolic foundation regressor that can effectively compress complex data with numerous interacting variables while producing interpretable physical representations. Our model has been rigorously tested on non-network symbolic regression, symbolic regression on complex networks, and the inference of network dynamics across various domains, including physics, biochemistry, ecology, and epidemiology. The results indicate a remarkable improvement in equation inference efficiency, being three times more effective than baseline approaches while maintaining accurate predictions. Furthermore, we apply our model to uncover more intuitive laws of interaction transmission from global epidemic outbreak data, achieving optimal data fitting. This model extends the application boundary of pre-trained symbolic regression models to complex networks, and we believe it provides a foundational solution for revealing the hidden mechanisms behind changes in complex phenomena, enhancing interpretability, and inspiring further scientific discoveries.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem</title>
<link>https://arxiv.org/abs/2505.21887</link>
<guid>https://arxiv.org/abs/2505.21887</guid>
<content:encoded><![CDATA[
arXiv:2505.21887v1 Announce Type: cross 
Abstract: Robust routing under uncertainty is central to real-world logistics, yet most benchmarks assume static, idealized settings. We present SVRPBench, the first open benchmark to capture high-fidelity stochastic dynamics in vehicle routing at urban scale. Spanning more than 500 instances with up to 1000 customers, it simulates realistic delivery conditions: time-dependent congestion, log-normal delays, probabilistic accidents, and empirically grounded time windows for residential and commercial clients. Our pipeline generates diverse, constraint-rich scenarios, including multi-depot and multi-vehicle setups. Benchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade by over 20% under distributional shift, while classical and metaheuristic methods remain robust. To enable reproducible research, we release the dataset and evaluation suite. SVRPBench challenges the community to design solvers that generalize beyond synthetic assumptions and adapt to real-world uncertainty.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Almost Linear Convergence under Minimal Score Assumptions: Quantized Transition Diffusion</title>
<link>https://arxiv.org/abs/2505.21892</link>
<guid>https://arxiv.org/abs/2505.21892</guid>
<content:encoded><![CDATA[
arXiv:2505.21892v1 Announce Type: cross 
Abstract: Continuous diffusion models have demonstrated remarkable performance in data generation across various domains, yet their efficiency remains constrained by two critical limitations: (1) the local adjacency structure of the forward Markov process, which restricts long-range transitions in the data space, and (2) inherent biases introduced during the simulation of time-inhomogeneous reverse denoising processes. To address these challenges, we propose Quantized Transition Diffusion (QTD), a novel approach that integrates data quantization with discrete diffusion dynamics. Our method first transforms the continuous data distribution $p_*$ into a discrete one $q_*$ via histogram approximation and binary encoding, enabling efficient representation in a structured discrete latent space. We then design a continuous-time Markov chain (CTMC) with Hamming distance-based transitions as the forward process, which inherently supports long-range movements in the original data space. For reverse-time sampling, we introduce a \textit{truncated uniformization} technique to simulate the reverse CTMC, which can provably provide unbiased generation from $q_*$ under minimal score assumptions. Through a novel KL dynamic analysis of the reverse CTMC, we prove that QTD can generate samples with $O(d\ln^2(d/\epsilon))$ score evaluations in expectation to approximate the $d$--dimensional target distribution $p_*$ within an $\epsilon$ error tolerance. Our method not only establishes state-of-the-art inference efficiency but also advances the theoretical foundations of diffusion-based generative modeling by unifying discrete and continuous diffusion paradigms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RenderFormer: Transformer-based Neural Rendering of Triangle Meshes with Global Illumination</title>
<link>https://arxiv.org/abs/2505.21925</link>
<guid>https://arxiv.org/abs/2505.21925</guid>
<content:encoded><![CDATA[
arXiv:2505.21925v1 Announce Type: cross 
Abstract: We present RenderFormer, a neural rendering pipeline that directly renders an image from a triangle-based representation of a scene with full global illumination effects and that does not require per-scene training or fine-tuning. Instead of taking a physics-centric approach to rendering, we formulate rendering as a sequence-to-sequence transformation where a sequence of tokens representing triangles with reflectance properties is converted to a sequence of output tokens representing small patches of pixels. RenderFormer follows a two stage pipeline: a view-independent stage that models triangle-to-triangle light transport, and a view-dependent stage that transforms a token representing a bundle of rays to the corresponding pixel values guided by the triangle-sequence from the view-independent stage. Both stages are based on the transformer architecture and are learned with minimal prior constraints. We demonstrate and evaluate RenderFormer on scenes with varying complexity in shape and light transport.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subspecialty-Specific Foundation Model for Intelligent Gastrointestinal Pathology</title>
<link>https://arxiv.org/abs/2505.21928</link>
<guid>https://arxiv.org/abs/2505.21928</guid>
<content:encoded><![CDATA[
arXiv:2505.21928v1 Announce Type: cross 
Abstract: Gastrointestinal (GI) diseases represent a clinically significant burden, necessitating precise diagnostic approaches to optimize patient outcomes. Conventional histopathological diagnosis, heavily reliant on the subjective interpretation of pathologists, suffers from limited reproducibility and diagnostic variability. To overcome these limitations and address the lack of pathology-specific foundation models for GI diseases, we develop Digepath, a specialized foundation model for GI pathology. Our framework introduces a dual-phase iterative optimization strategy combining pretraining with fine-screening, specifically designed to address the detection of sparsely distributed lesion areas in whole-slide images. Digepath is pretrained on more than 353 million image patches from over 200,000 hematoxylin and eosin-stained slides of GI diseases. It attains state-of-the-art performance on 33 out of 34 tasks related to GI pathology, including pathological diagnosis, molecular prediction, gene mutation prediction, and prognosis evaluation, particularly in diagnostically ambiguous cases and resolution-agnostic tissue classification.We further translate the intelligent screening module for early GI cancer and achieve near-perfect 99.6% sensitivity across 9 independent medical institutions nationwide. The outstanding performance of Digepath highlights its potential to bridge critical gaps in histopathological practice. This work not only advances AI-driven precision pathology for GI diseases but also establishes a transferable paradigm for other pathology subspecialties.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Higher-Order Group Synchronization</title>
<link>https://arxiv.org/abs/2505.21932</link>
<guid>https://arxiv.org/abs/2505.21932</guid>
<content:encoded><![CDATA[
arXiv:2505.21932v1 Announce Type: cross 
Abstract: Group synchronization is the problem of determining reliable global estimates from noisy local measurements on networks. The typical task for group synchronization is to assign elements of a group to the nodes of a graph in a way that respects group elements given on the edges which encode information about local pairwise relationships between the nodes. In this paper, we introduce a novel higher-order group synchronization problem which operates on a hypergraph and seeks to synchronize higher-order local measurements on the hyperedges to obtain global estimates on the nodes. Higher-order group synchronization is motivated by applications to computer vision and image processing, among other computational problems. First, we define the problem of higher-order group synchronization and discuss its mathematical foundations. Specifically, we give necessary and sufficient synchronizability conditions which establish the importance of cycle consistency in higher-order group synchronization. Then, we propose the first computational framework for general higher-order group synchronization; it acts globally and directly on higher-order measurements using a message passing algorithm. We discuss theoretical guarantees for our framework, including convergence analyses under outliers and noise. Finally, we show potential advantages of our method through numerical experiments. In particular, we show that in certain cases our higher-order method applied to rotational and angular synchronization outperforms standard pairwise synchronization methods and is more robust to outliers. We also show that our method has comparable performance on simulated cryo-electron microscopy (cryo-EM) data compared to a standard cryo-EM reconstruction package.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cross-modal RAG: Sub-dimensional Retrieval-Augmented Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2505.21956</link>
<guid>https://arxiv.org/abs/2505.21956</guid>
<content:encoded><![CDATA[
arXiv:2505.21956v1 Announce Type: cross 
Abstract: Text-to-image generation increasingly demands access to domain-specific, fine-grained, and rapidly evolving knowledge that pretrained models cannot fully capture. Existing Retrieval-Augmented Generation (RAG) methods attempt to address this by retrieving globally relevant images, but they fail when no single image contains all desired elements from a complex user query. We propose Cross-modal RAG, a novel framework that decomposes both queries and images into sub-dimensional components, enabling subquery-aware retrieval and generation. Our method introduces a hybrid retrieval strategy - combining a sub-dimensional sparse retriever with a dense retriever - to identify a Pareto-optimal set of images, each contributing complementary aspects of the query. During generation, a multimodal large language model is guided to selectively condition on relevant visual features aligned to specific subqueries, ensuring subquery-aware image synthesis. Extensive experiments on MS-COCO, Flickr30K, WikiArt, CUB, and ImageNet-LT demonstrate that Cross-modal RAG significantly outperforms existing baselines in both retrieval and generation quality, while maintaining high efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Independent Messaging for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21985</link>
<guid>https://arxiv.org/abs/2505.21985</guid>
<content:encoded><![CDATA[
arXiv:2505.21985v1 Announce Type: cross 
Abstract: In multi-agent reinforcement learning (MARL), effective communication improves agent performance, particularly under partial observability. We propose MARL-CPC, a framework that enables communication among fully decentralized, independent agents without parameter sharing. MARL-CPC incorporates a message learning model based on collective predictive coding (CPC) from emergent communication research. Unlike conventional methods that treat messages as part of the action space and assume cooperation, MARL-CPC links messages to state inference, supporting communication in non-cooperative, reward-independent settings. We introduce two algorithms -Bandit-CPC and IPPO-CPC- and evaluate them in non-cooperative MARL tasks. Benchmarks show that both outperform standard message-as-action approaches, establishing effective communication even when messages offer no direct benefit to the sender. These results highlight MARL-CPC's potential for enabling coordination in complex, decentralized environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Locking-Free Training of Physics-Informed Neural Network for Solving Nearly Incompressible Elasticity Equations</title>
<link>https://arxiv.org/abs/2505.21994</link>
<guid>https://arxiv.org/abs/2505.21994</guid>
<content:encoded><![CDATA[
arXiv:2505.21994v1 Announce Type: cross 
Abstract: Due to divergence instability, the accuracy of low-order conforming finite element methods for nearly incompressible homogeneous elasticity equations deteriorates as the Lam\'e coefficient $\lambda\to\infty$, or equivalently as the Poisson ratio $\nu\to1/2$. This phenomenon, known as locking or non-robustness, remains not fully understood despite extensive investigation. In this paper, we propose a robust method based on a fundamentally different, machine-learning-driven approach. Leveraging recently developed Physics-Informed Neural Networks (PINNs), we address the numerical solution of linear elasticity equations governing nearly incompressible materials. The core idea of our method is to appropriately decompose the given equations to alleviate the extreme imbalance in the coefficients, while simultaneously solving both the forward and inverse problems to recover the solutions of the decomposed systems as well as the associated external conditions. Through various numerical experiments, including constant, variable and parametric Lam\'e coefficients, we illustrate the efficiency of the proposed methodology.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Align-DA: Align Score-based Atmospheric Data Assimilation with Multiple Preferences</title>
<link>https://arxiv.org/abs/2505.22008</link>
<guid>https://arxiv.org/abs/2505.22008</guid>
<content:encoded><![CDATA[
arXiv:2505.22008v1 Announce Type: cross 
Abstract: Data assimilation (DA) aims to estimate the full state of a dynamical system by combining partial and noisy observations with a prior model forecast, commonly referred to as the background. In atmospheric applications, this problem is fundamentally ill-posed due to the sparsity of observations relative to the high-dimensional state space. Traditional methods address this challenge by simplifying background priors to regularize the solution, which are empirical and require continual tuning for application. Inspired by alignment techniques in text-to-image diffusion models, we propose Align-DA, which formulates DA as a generative process and uses reward signals to guide background priors, replacing manual tuning with data-driven alignment. Specifically, we train a score-based model in the latent space to approximate the background-conditioned prior, and align it using three complementary reward signals for DA: (1) assimilation accuracy, (2) forecast skill initialized from the assimilated state, and (3) physical adherence of the analysis fields. Experiments with multiple reward signals demonstrate consistent improvements in analysis quality across different evaluation metrics and observation-guidance strategies. These results show that preference alignment, implemented as a soft constraint, can automatically adapt complex background priors tailored to DA, offering a promising new direction for advancing the field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Curves of Stochastic Gradient Descent in Kernel Regression</title>
<link>https://arxiv.org/abs/2505.22048</link>
<guid>https://arxiv.org/abs/2505.22048</guid>
<content:encoded><![CDATA[
arXiv:2505.22048v1 Announce Type: cross 
Abstract: This paper considers a canonical problem in kernel regression: how good are the model performances when it is trained by the popular online first-order algorithms, compared to the offline ones, such as ridge and ridgeless regression? In this paper, we analyze the foundational single-pass Stochastic Gradient Descent (SGD) in kernel regression under source condition where the optimal predictor can even not belong to the RKHS, i.e. the model is misspecified. Specifically, we focus on the inner product kernel over the sphere and characterize the exact orders of the excess risk curves under different scales of sample sizes $n$ concerning the input dimension $d$. Surprisingly, we show that SGD achieves min-max optimal rates up to constants among all the scales, without suffering the saturation, a prevalent phenomenon observed in (ridge) regression, except when the model is highly misspecified and the learning is in a final stage where $n\gg d^{\gamma}$ with any constant $\gamma >0$. The main reason for SGD to overcome the curse of saturation is the exponentially decaying step size schedule, a common practice in deep neural network training. As a byproduct, we provide the \emph{first} provable advantage of the scheme over the iterative averaging method in the common setting.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforced Reasoning for Embodied Planning</title>
<link>https://arxiv.org/abs/2505.22050</link>
<guid>https://arxiv.org/abs/2505.22050</guid>
<content:encoded><![CDATA[
arXiv:2505.22050v1 Announce Type: cross 
Abstract: Embodied planning requires agents to make coherent multi-step decisions based on dynamic visual observations and natural language goals. While recent vision-language models (VLMs) excel at static perception tasks, they struggle with the temporal reasoning, spatial understanding, and commonsense grounding needed for planning in interactive environments. In this work, we introduce a reinforcement fine-tuning framework that brings R1-style reasoning enhancement into embodied planning. We first distill a high-quality dataset from a powerful closed-source model and perform supervised fine-tuning (SFT) to equip the model with structured decision-making priors. We then design a rule-based reward function tailored to multi-step action quality and optimize the policy via Generalized Reinforced Preference Optimization (GRPO). Our approach is evaluated on Embench, a recent benchmark for interactive embodied tasks, covering both in-domain and out-of-domain scenarios. Experimental results show that our method significantly outperforms models of similar or larger scale, including GPT-4o-mini and 70B+ open-source baselines, and exhibits strong generalization to unseen environments. This work highlights the potential of reinforcement-driven reasoning to advance long-horizon planning in embodied AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hyperbolic recurrent neural network as the first type of non-Euclidean neural quantum state ansatz</title>
<link>https://arxiv.org/abs/2505.22083</link>
<guid>https://arxiv.org/abs/2505.22083</guid>
<content:encoded><![CDATA[
arXiv:2505.22083v1 Announce Type: cross 
Abstract: In this work, we introduce the first type of non-Euclidean neural quantum state (NQS) ansatz, in the form of the hyperbolic GRU (a variant of recurrent neural networks (RNNs)), to be used in the Variational Monte Carlo method of approximating the ground state wavefunction for quantum many-body systems. In particular, we examine the performances of NQS ansatzes constructed from both conventional or Euclidean RNN/GRU and from hyperbolic GRU in the prototypical settings of the one- and two-dimensional transverse field Ising models (TFIM) of up to 100 spins and the one-dimensional Heisenberg $J_1J_2$ and $J_1J_2J_3$ systems of up 50 spins. By virtue of the fact that, for all of the experiments performed in this work, hyperbolic GRU can yield performances comparable to or better than Euclidean RNNs, which have been extensively studied in these settings in the literature, our work is a proof-of-concept for the viability of hyperbolic GRU as the first type of non-Euclidean NQS ansatz for quantum many-body systems. Furthermore, in settings where the Hamiltonian displays a clear hierarchical interaction structure, such as the 1D Heisenberg $J_1J_2$ & $J_1J_2J_3$ systems with the 1st, 2nd and even 3rd nearest neighbor interactions, our results show that hyperbolic GRU definitively outperforms its Euclidean version in all instances. The fact that these results are reminiscent of the established ones from natural language processing where hyperbolic GRU almost always outperforms Euclidean RNNs when the training data exhibit a tree-like or hierarchical structure leads us to hypothesize that hyperbolic GRU NQS ansatz would likely outperform Euclidean RNN/GRU NQS ansatz in quantum spin systems that involve different degrees of nearest neighbor interactions. Finally, with this work, we hope to initiate future studies of other types of non-Euclidean NQS beyond hyperbolic GRU.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PADAM: Parallel averaged Adam reduces the error for stochastic optimization in scientific machine learning</title>
<link>https://arxiv.org/abs/2505.22085</link>
<guid>https://arxiv.org/abs/2505.22085</guid>
<content:encoded><![CDATA[
arXiv:2505.22085v1 Announce Type: cross 
Abstract: Averaging techniques such as Ruppert--Polyak averaging and exponential movering averaging (EMA) are powerful approaches to accelerate optimization procedures of stochastic gradient descent (SGD) optimization methods such as the popular ADAM optimizer. However, depending on the specific optimization problem under consideration, the type and the parameters for the averaging need to be adjusted to achieve the smallest optimization error. In this work we propose an averaging approach, which we refer to as parallel averaged ADAM (PADAM), in which we compute parallely different averaged variants of ADAM and during the training process dynamically select the variant with the smallest optimization error. A central feature of this approach is that this procedure requires no more gradient evaluations than the usual ADAM optimizer as each of the averaged trajectories relies on the same underlying ADAM trajectory and thus on the same underlying gradients. We test the proposed PADAM optimizer in 13 stochastic optimization and deep neural network (DNN) learning problems and compare its performance with known optimizers from the literature such as standard SGD, momentum SGD, Adam with and without EMA, and ADAMW. In particular, we apply the compared optimizers to physics-informed neural network, deep Galerkin, deep backward stochastic differential equation and deep Kolmogorov approximations for boundary value partial differential equation problems from scientific machine learning, as well as to DNN approximations for optimal control and optimal stopping problems. In nearly all of the considered examples PADAM achieves, sometimes among others and sometimes exclusively, essentially the smallest optimization error. This work thus strongly suggest to consider PADAM for scientific machine learning problems and also motivates further research for adaptive averaging procedures within the training of DNNs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High Volume Rate 3D Ultrasound Reconstruction with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.22090</link>
<guid>https://arxiv.org/abs/2505.22090</guid>
<content:encoded><![CDATA[
arXiv:2505.22090v1 Announce Type: cross 
Abstract: Three-dimensional ultrasound enables real-time volumetric visualization of anatomical structures. Unlike traditional 2D ultrasound, 3D imaging reduces the reliance on precise probe orientation, potentially making ultrasound more accessible to clinicians with varying levels of experience and improving automated measurements and post-exam analysis. However, achieving both high volume rates and high image quality remains a significant challenge. While 3D diverging waves can provide high volume rates, they suffer from limited tissue harmonic generation and increased multipath effects, which degrade image quality. One compromise is to retain the focusing in elevation while leveraging unfocused diverging waves in the lateral direction to reduce the number of transmissions per elevation plane. Reaching the volume rates achieved by full 3D diverging waves, however, requires dramatically undersampling the number of elevation planes. Subsequently, to render the full volume, simple interpolation techniques are applied. This paper introduces a novel approach to 3D ultrasound reconstruction from a reduced set of elevation planes by employing diffusion models (DMs) to achieve increased spatial and temporal resolution. We compare both traditional and supervised deep learning-based interpolation methods on a 3D cardiac ultrasound dataset. Our results show that DM-based reconstruction consistently outperforms the baselines in image quality and downstream task performance. Additionally, we accelerate inference by leveraging the temporal consistency inherent to ultrasound sequences. Finally, we explore the robustness of the proposed method by exploiting the probabilistic nature of diffusion posterior sampling to quantify reconstruction uncertainty and demonstrate improved recall on out-of-distribution data with synthetic anomalies under strong subsampling.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReinFlow: Fine-tuning Flow Matching Policy with Online Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.22094</link>
<guid>https://arxiv.org/abs/2505.22094</guid>
<content:encoded><![CDATA[
arXiv:2505.22094v1 Announce Type: cross 
Abstract: We propose ReinFlow, a simple yet effective online reinforcement learning (RL) framework that fine-tunes a family of flow matching policies for continuous robotic control. Derived from rigorous RL theory, ReinFlow injects learnable noise into a flow policy's deterministic path, converting the flow into a discrete-time Markov Process for exact and straightforward likelihood computation. This conversion facilitates exploration and ensures training stability, enabling ReinFlow to fine-tune diverse flow model variants, including Rectified Flow [35] and Shortcut Models [19], particularly at very few or even one denoising step. We benchmark ReinFlow in representative locomotion and manipulation tasks, including long-horizon planning with visual input and sparse reward. The episode reward of Rectified Flow policies obtained an average net growth of 135.36% after fine-tuning in challenging legged locomotion tasks while saving denoising steps and 82.63% of wall time compared to state-of-the-art diffusion RL fine-tuning method DPPO [43]. The success rate of the Shortcut Model policies in state and visual manipulation tasks achieved an average net increase of 40.34% after fine-tuning with ReinFlow at four or even one denoising step, whose performance is comparable to fine-tuned DDIM policies while saving computation time for an average of 23.20%. Project Webpage: https://reinflow.github.io/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Knowledge Base Construction for Knowledge-Augmented Text-to-SQL</title>
<link>https://arxiv.org/abs/2505.22096</link>
<guid>https://arxiv.org/abs/2505.22096</guid>
<content:encoded><![CDATA[
arXiv:2505.22096v1 Announce Type: cross 
Abstract: Text-to-SQL aims to translate natural language queries into SQL statements, which is practical as it enables anyone to easily retrieve the desired information from databases. Recently, many existing approaches tackle this problem with Large Language Models (LLMs), leveraging their strong capability in understanding user queries and generating corresponding SQL code. Yet, the parametric knowledge in LLMs might be limited to covering all the diverse and domain-specific queries that require grounding in various database schemas, which makes generated SQLs less accurate oftentimes. To tackle this, we propose constructing the knowledge base for text-to-SQL, a foundational source of knowledge, from which we retrieve and generate the necessary knowledge for given queries. In particular, unlike existing approaches that either manually annotate knowledge or generate only a few pieces of knowledge for each query, our knowledge base is comprehensive, which is constructed based on a combination of all the available questions and their associated database schemas along with their relevant knowledge, and can be reused for unseen databases from different datasets and domains. We validate our approach on multiple text-to-SQL datasets, considering both the overlapping and non-overlapping database scenarios, where it outperforms relevant baselines substantially.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Transferability and Discriminability of Repersentation Learning in Unsupervised Domain Adaptation</title>
<link>https://arxiv.org/abs/2505.22099</link>
<guid>https://arxiv.org/abs/2505.22099</guid>
<content:encoded><![CDATA[
arXiv:2505.22099v1 Announce Type: cross 
Abstract: In this paper, we addressed the limitation of relying solely on distribution alignment and source-domain empirical risk minimization in Unsupervised Domain Adaptation (UDA). Our information-theoretic analysis showed that this standard adversarial-based framework neglects the discriminability of target-domain features, leading to suboptimal performance. To bridge this theoretical-practical gap, we defined "good representation learning" as guaranteeing both transferability and discriminability, and proved that an additional loss term targeting target-domain discriminability is necessary. Building on these insights, we proposed a novel adversarial-based UDA framework that explicitly integrates a domain alignment objective with a discriminability-enhancing constraint. Instantiated as Domain-Invariant Representation Learning with Global and Local Consistency (RLGLC), our method leverages Asymmetrically-Relaxed Wasserstein of Wasserstein Distance (AR-WWD) to address class imbalance and semantic dimension weighting, and employs a local consistency mechanism to preserve fine-grained target-domain discriminative information. Extensive experiments across multiple benchmark datasets demonstrate that RLGLC consistently surpasses state-of-the-art methods, confirming the value of our theoretical perspective and underscoring the necessity of enforcing both transferability and discriminability in adversarial-based UDA.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Dynamic Shielding for Parametric Safety Specifications</title>
<link>https://arxiv.org/abs/2505.22104</link>
<guid>https://arxiv.org/abs/2505.22104</guid>
<content:encoded><![CDATA[
arXiv:2505.22104v1 Announce Type: cross 
Abstract: Shielding has emerged as a promising approach for ensuring safety of AI-controlled autonomous systems. The algorithmic goal is to compute a shield, which is a runtime safety enforcement tool that needs to monitor and intervene the AI controller's actions if safety could be compromised otherwise. Traditional shields are designed statically for a specific safety requirement. Therefore, if the safety requirement changes at runtime due to changing operating conditions, the shield needs to be recomputed from scratch, causing delays that could be fatal. We introduce dynamic shields for parametric safety specifications, which are succinctly represented sets of all possible safety specifications that may be encountered at runtime. Our dynamic shields are statically designed for a given safety parameter set, and are able to dynamically adapt as the true safety specification (permissible by the parameters) is revealed at runtime. The main algorithmic novelty lies in the dynamic adaptation procedure, which is a simple and fast algorithm that utilizes known features of standard safety shields, like maximal permissiveness. We report experimental results for a robot navigation problem in unknown territories, where the safety specification evolves as new obstacles are discovered at runtime. In our experiments, the dynamic shields took a few minutes for their offline design, and took between a fraction of a second and a few seconds for online adaptation at each step, whereas the brute-force online recomputation approach was up to 5 times slower.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curse of High Dimensionality Issue in Transformer for Long-context Modeling</title>
<link>https://arxiv.org/abs/2505.22107</link>
<guid>https://arxiv.org/abs/2505.22107</guid>
<content:encoded><![CDATA[
arXiv:2505.22107v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) excel in natural language processing tasks by capturing long-range dependencies through self-attention mechanisms. However, long-context modeling faces significant computational inefficiencies due to \textit{redundant} attention computations: while attention weights are often \textit{sparse}, all tokens consume \textit{equal} computational resources. In this paper, we reformulate traditional probabilistic sequence modeling as a \textit{supervised learning task}, enabling the separation of relevant and irrelevant tokens and providing a clearer understanding of redundancy. Based on this reformulation, we theoretically analyze attention sparsity, revealing that only a few tokens significantly contribute to predictions. Building on this, we formulate attention optimization as a linear coding problem and propose a \textit{group coding strategy}, theoretically showing its ability to improve robustness against random noise and enhance learning efficiency. Motivated by this, we propose \textit{Dynamic Group Attention} (DGA), which leverages the group coding to explicitly reduce redundancy by aggregating less important tokens during attention computation. Empirical results show that our DGA significantly reduces computational costs while maintaining competitive performance.Code is available at https://github.com/bolixinyu/DynamicGroupAttention.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RAD: Redundancy-Aware Distillation for Hybrid Models via Self-Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.22135</link>
<guid>https://arxiv.org/abs/2505.22135</guid>
<content:encoded><![CDATA[
arXiv:2505.22135v1 Announce Type: cross 
Abstract: Hybrid models combining Transformers and State Space Models (SSMs) are promising for balancing performance and efficiency. However, optimizing these hybrid models, particularly by addressing the potential redundancy inherent within the Transformer components, remains a significant challenge. In this paper, we propose RAD (Redundancy-Aware Distillation), a novel framework that uses self-speculative decoding as a diagnostic tool to identify redundant attention layers within the model. These identified layers are then selectively replaced with SSM components, followed by targeted (self-)distillation. Specifically, RAD focuses knowledge transfer on the components identified as redundant, considering architectural changes and specific weight initialization strategies. We experimentally demonstrate that self-distillation using RAD significantly surpasses the performance of the original base model on mathematical and coding tasks. Furthermore, RAD is also effective in standard knowledge distillation settings, achieving up to approximately 2x faster convergence compared to baseline methods. Notably, while a baseline model distilled from a Llama-3.1 70B teacher achieves scores of 46.17 on GSM8K and 22.75 on CRUX, RAD achieves significantly higher scores of 71.27 on GSM8K and 28.25 on CRUX, even when using a much smaller Llama-3.1 8B teacher. RAD offers a new pathway for efficient optimization and performance enhancement in the distillation of hybrid models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Limited Generalizability in Argument Mining: State-Of-The-Art Models Learn Datasets, Not Arguments</title>
<link>https://arxiv.org/abs/2505.22137</link>
<guid>https://arxiv.org/abs/2505.22137</guid>
<content:encoded><![CDATA[
arXiv:2505.22137v1 Announce Type: cross 
Abstract: Identifying arguments is a necessary prerequisite for various tasks in automated discourse analysis, particularly within contexts such as political debates, online discussions, and scientific reasoning. In addition to theoretical advances in understanding the constitution of arguments, a significant body of research has emerged around practical argument mining, supported by a growing number of publicly available datasets. On these benchmarks, BERT-like transformers have consistently performed best, reinforcing the belief that such models are broadly applicable across diverse contexts of debate. This study offers the first large-scale re-evaluation of such state-of-the-art models, with a specific focus on their ability to generalize in identifying arguments. We evaluate four transformers, three standard and one enhanced with contrastive pre-training for better generalization, on 17 English sentence-level datasets as most relevant to the task. Our findings show that, to varying degrees, these models tend to rely on lexical shortcuts tied to content words, suggesting that apparent progress may often be driven by dataset-specific cues rather than true task alignment. While the models achieve strong results on familiar benchmarks, their performance drops markedly when applied to unseen datasets. Nonetheless, incorporating both task-specific pre-training and joint benchmark training proves effective in enhancing both robustness and generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unifying Continuous and Discrete Text Diffusion with Non-simultaneous Diffusion Processes</title>
<link>https://arxiv.org/abs/2505.22165</link>
<guid>https://arxiv.org/abs/2505.22165</guid>
<content:encoded><![CDATA[
arXiv:2505.22165v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a promising approach for text generation, with recent works falling into two main categories: discrete and continuous diffusion models. Discrete diffusion models apply token corruption independently using categorical distributions, allowing for different diffusion progress across tokens but lacking fine-grained control. Continuous diffusion models map tokens to continuous spaces and apply fine-grained noise, but the diffusion progress is uniform across tokens, limiting their ability to capture semantic nuances. To address these limitations, we propose \textbf{\underline{N}}on-simultan\textbf{\underline{e}}ous C\textbf{\underline{o}}ntinuous \textbf{\underline{Diff}}usion Models (NeoDiff), a novel diffusion model that integrates the strengths of both discrete and continuous approaches. NeoDiff introduces a Poisson diffusion process for the forward process, enabling a flexible and fine-grained noising paradigm, and employs a time predictor for the reverse process to adaptively modulate the denoising progress based on token semantics. Furthermore, NeoDiff utilizes an optimized schedule for inference to ensure more precise noise control and improved performance. Our approach unifies the theories of discrete and continuous diffusion models, offering a more principled and effective framework for text generation. Experimental results on several text generation tasks demonstrate NeoDiff's superior performance compared to baselines of non-autoregressive continuous and discrete diffusion models, iterative-based methods and autoregressive diffusion-based methods. These results highlight NeoDiff's potential as a powerful tool for generating high-quality text and advancing the field of diffusion-based text generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Speculative Decoding Meets Quantization: Compatibility Evaluation and Hierarchical Framework Design</title>
<link>https://arxiv.org/abs/2505.22179</link>
<guid>https://arxiv.org/abs/2505.22179</guid>
<content:encoded><![CDATA[
arXiv:2505.22179v1 Announce Type: cross 
Abstract: Speculative decoding and quantization effectively accelerate memory-bound inference of large language models. Speculative decoding mitigates the memory bandwidth bottleneck by verifying multiple tokens within a single forward pass, which increases computational effort. Quantization achieves this optimization by compressing weights and activations into lower bit-widths and also reduces computations via low-bit matrix multiplications. To further leverage their strengths, we investigate the integration of these two techniques. Surprisingly, experiments applying the advanced speculative decoding method EAGLE-2 to various quantized models reveal that the memory benefits from 4-bit weight quantization are diminished by the computational load from speculative decoding. Specifically, verifying a tree-style draft incurs significantly more time overhead than a single-token forward pass on 4-bit weight quantized models. This finding led to our new speculative decoding design: a hierarchical framework that employs a small model as an intermediate stage to turn tree-style drafts into sequence drafts, leveraging the memory access benefits of the target quantized model. Experimental results show that our hierarchical approach achieves a 2.78$\times$ speedup across various tasks for the 4-bit weight Llama-3-70B model on an A100 GPU, outperforming EAGLE-2 by 1.31$\times$. Code available at https://github.com/AI9Stars/SpecMQuant.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Physics-inspired Generative AI models via real hardware-based noisy quantum diffusion</title>
<link>https://arxiv.org/abs/2505.22193</link>
<guid>https://arxiv.org/abs/2505.22193</guid>
<content:encoded><![CDATA[
arXiv:2505.22193v1 Announce Type: cross 
Abstract: Quantum Diffusion Models (QDMs) are an emerging paradigm in Generative AI that aims to use quantum properties to improve the performances of their classical counterparts. However, existing algorithms are not easily scalable due to the limitations of near-term quantum devices. Following our previous work on QDMs, here we propose and implement two physics-inspired protocols. In the first, we use the formalism of quantum stochastic walks, showing that a specific interplay of quantum and classical dynamics in the forward process produces statistically more robust models generating sets of MNIST images with lower Fr\'echet Inception Distance (FID) than using totally classical dynamics. In the second approach, we realize an algorithm to generate images by exploiting the intrinsic noise of real IBM quantum hardware with only four qubits. Our work could be a starting point to pave the way for new scenarios for large-scale algorithms in quantum Generative AI, where quantum noise is neither mitigated nor corrected, but instead exploited as a useful resource.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Judging Quality Across Languages: A Multilingual Approach to Pretraining Data Filtering with Language Models</title>
<link>https://arxiv.org/abs/2505.22232</link>
<guid>https://arxiv.org/abs/2505.22232</guid>
<content:encoded><![CDATA[
arXiv:2505.22232v1 Announce Type: cross 
Abstract: High-quality multilingual training data is essential for effectively pretraining large language models (LLMs). Yet, the availability of suitable open-source multilingual datasets remains limited. Existing state-of-the-art datasets mostly rely on heuristic filtering methods, restricting both their cross-lingual transferability and scalability. Here, we introduce JQL, a systematic approach that efficiently curates diverse and high-quality multilingual data at scale while significantly reducing computational demands. JQL distills LLMs' annotation capabilities into lightweight annotators based on pretrained multilingual embeddings. These models exhibit robust multilingual and cross-lingual performance, even for languages and scripts unseen during training. Evaluated empirically across 35 languages, the resulting annotation pipeline substantially outperforms current heuristic filtering methods like Fineweb2. JQL notably enhances downstream model training quality and increases data retention rates. Our research provides practical insights and valuable resources for multilingual data curation, raising the standards of multilingual dataset development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Yambda-5B -- A Large-Scale Multi-modal Dataset for Ranking And Retrieval</title>
<link>https://arxiv.org/abs/2505.22238</link>
<guid>https://arxiv.org/abs/2505.22238</guid>
<content:encoded><![CDATA[
arXiv:2505.22238v1 Announce Type: cross 
Abstract: We present Yambda-5B, a large-scale open dataset sourced from the Yandex.Music streaming platform. Yambda-5B contains 4.79 billion user-item interactions from 1 million users across 9.39 million tracks. The dataset includes two primary types of interactions: implicit feedback (listening events) and explicit feedback (likes, dislikes, unlikes and undislikes). In addition, we provide audio embeddings for most tracks, generated by a convolutional neural network trained on audio spectrograms. A key distinguishing feature of Yambda-5B is the inclusion of the is_organic flag, which separates organic user actions from recommendation-driven events. This distinction is critical for developing and evaluating machine learning algorithms, as Yandex.Music relies on recommender systems to personalize track selection for users. To support rigorous benchmarking, we introduce an evaluation protocol based on a Global Temporal Split, allowing recommendation algorithms to be assessed in conditions that closely mirror real-world use. We report benchmark results for standard baselines (ItemKNN, iALS) and advanced models (SANSA, SASRec) using a variety of evaluation metrics. By releasing Yambda-5B to the community, we aim to provide a readily accessible, industrial-scale resource to advance research, foster innovation, and promote reproducible results in recommender systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UDuo: Universal Dual Optimization Framework for Online Matching</title>
<link>https://arxiv.org/abs/2505.22243</link>
<guid>https://arxiv.org/abs/2505.22243</guid>
<content:encoded><![CDATA[
arXiv:2505.22243v1 Announce Type: cross 
Abstract: Online resource allocation under budget constraints critically depends on proper modeling of user arrival dynamics. Classical approaches employ stochastic user arrival models to derive near-optimal solutions through fractional matching formulations of exposed users for downstream allocation tasks. However, this is no longer a reasonable assumption when the environment changes dynamically. In this work, We propose the Universal Dual optimization framework UDuo, a novel paradigm that fundamentally rethinks online allocation through three key innovations: (i) a temporal user arrival representation vector that explicitly captures distribution shifts in user arrival patterns and resource consumption dynamics, (ii) a resource pacing learner with adaptive allocation policies that generalize to heterogeneous constraint scenarios, and (iii) an online time-series forecasting approach for future user arrival distributions that achieves asymptotically optimal solutions with constraint feasibility guarantees in dynamic environments. Experimental results show that UDuo achieves higher efficiency and faster convergence than the traditional stochastic arrival model in real-world pricing while maintaining rigorous theoretical validity for general online allocation problems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LiDAR Based Semantic Perception for Forklifts in Outdoor Environments</title>
<link>https://arxiv.org/abs/2505.22258</link>
<guid>https://arxiv.org/abs/2505.22258</guid>
<content:encoded><![CDATA[
arXiv:2505.22258v1 Announce Type: cross 
Abstract: In this study, we present a novel LiDAR-based semantic segmentation framework tailored for autonomous forklifts operating in complex outdoor environments. Central to our approach is the integration of a dual LiDAR system, which combines forward-facing and downward-angled LiDAR sensors to enable comprehensive scene understanding, specifically tailored for industrial material handling tasks. The dual configuration improves the detection and segmentation of dynamic and static obstacles with high spatial precision. Using high-resolution 3D point clouds captured from two sensors, our method employs a lightweight yet robust approach that segments the point clouds into safety-critical instance classes such as pedestrians, vehicles, and forklifts, as well as environmental classes such as driveable ground, lanes, and buildings. Experimental validation demonstrates that our approach achieves high segmentation accuracy while satisfying strict runtime requirements, establishing its viability for safety-aware, fully autonomous forklift navigation in dynamic warehouse and yard environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking the Unsolvable: When In-Context Search Meets Test-Time Scaling</title>
<link>https://arxiv.org/abs/2505.22290</link>
<guid>https://arxiv.org/abs/2505.22290</guid>
<content:encoded><![CDATA[
arXiv:2505.22290v1 Announce Type: cross 
Abstract: Recent research has highlighted that Large Language Models (LLMs), even when trained to generate extended long reasoning steps, still face significant challenges on hard reasoning problems. However, much of the existing literature relies on direct prompting with simple in-context learning examples for evaluation, which largely overlooks advanced techniques to elicit LLMs' deliberate reasoning before drawing conclusions that LLMs hit a performance ceiling. In this paper, we systematically explore the combined potential of in-context search and test-time scaling on super hard reasoning tasks. We find that by employing advanced in-context search prompting to LLMs augmented with internal scaling, one can achieve transformative performance breakthroughs on tasks previously deemed "unsolvable" (e.g., reported success rates below 5%). We provide both empirical results and theoretical analysis of how this combination can unleash LLM reasoning capabilities: i) Empirically, on controlled NP-hard tasks and complex real-world planning benchmarks, our approach achieves up to a 30x improvement in success rates compared to previously reported results without any external mechanisms; ii) Theoretically, we show that in-context search prompting, when combined with internal scaling, significantly extends the complexity class of solvable reasoning problems. These findings challenge prevailing assumptions about the limitations of LLMs on complex tasks, indicating that current evaluation paradigms systematically underestimate their true potential. Our work calls for a critical reassessment of how LLM reasoning is benchmarked and a more robust evaluation strategy that fully captures the true capabilities of contemporary LLMs, which can lead to a better understanding of their operational reasoning boundaries in real-world deployments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>360-LLaMA-Factory: Plug &amp; Play Sequence Parallelism for Long Post-Training</title>
<link>https://arxiv.org/abs/2505.22296</link>
<guid>https://arxiv.org/abs/2505.22296</guid>
<content:encoded><![CDATA[
arXiv:2505.22296v1 Announce Type: cross 
Abstract: Adding sequence parallelism into LLaMA-Factory, we open-sourced 360-LLaMA-Factory at https://github.com/Qihoo360/360-LLaMA-Factory. 360-LLaMA-Factory has received wide recognition and used in models such as Light-R1 arXiv:2503.10460, TinyR1 arXiv:2503.04872, Kaggle AIMO math models and also in large companies' training frameworks. This technical report delves deeper into the different sequence parallel modes behind 360-LLaMA-Factory and discusses our implementation insights.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>If Pigs Could Fly... Can LLMs Logically Reason Through Counterfactuals?</title>
<link>https://arxiv.org/abs/2505.22318</link>
<guid>https://arxiv.org/abs/2505.22318</guid>
<content:encoded><![CDATA[
arXiv:2505.22318v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) demonstrate impressive reasoning capabilities in familiar contexts, but struggle when the context conflicts with their parametric knowledge. To investigate this phenomenon, we introduce CounterLogic, a dataset containing 1,800 examples across 9 logical schemas, explicitly designed to evaluate logical reasoning through counterfactual (hypothetical knowledge-conflicting) scenarios. Our systematic evaluation of 11 LLMs across 6 different datasets reveals a consistent performance degradation, with accuracies dropping by 27% on average when reasoning through counterfactual information. We propose Self-Segregate, a prompting method enabling metacognitive awareness (explicitly identifying knowledge conflicts) before reasoning. Our method dramatically narrows the average performance gaps from 27% to just 11%, while significantly increasing the overall accuracy (+7.5%). We discuss the implications of these findings and draw parallels to human cognitive processes, particularly on how humans disambiguate conflicting information during reasoning tasks. Our findings offer practical insights for understanding and enhancing LLMs reasoning capabilities in real-world applications, especially where models must logically reason independently of their factual knowledge.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Individualised Counterfactual Examples Using Conformal Prediction Intervals</title>
<link>https://arxiv.org/abs/2505.22326</link>
<guid>https://arxiv.org/abs/2505.22326</guid>
<content:encoded><![CDATA[
arXiv:2505.22326v1 Announce Type: cross 
Abstract: Counterfactual explanations for black-box models aim to pr ovide insight into an algorithmic decision to its recipient. For a binary classification problem an individual counterfactual details which features might be changed for the model to infer the opposite class. High-dimensional feature spaces that are typical of machine learning classification models admit many possible counterfactual examples to a decision, and so it is important to identify additional criteria to select the most useful counterfactuals. In this paper, we explore the idea that the counterfactuals should be maximally informative when considering the knowledge of a specific individual about the underlying classifier. To quantify this information gain we explicitly model the knowledge of the individual, and assess the uncertainty of predictions which the individual makes by the width of a conformal prediction interval. Regions of feature space where the prediction interval is wide correspond to areas where the confidence in decision making is low, and an additional counterfactual example might be more informative to an individual. To explore and evaluate our individualised conformal prediction interval counterfactuals (CPICFs), first we present a synthetic data set on a hypercube which allows us to fully visualise the decision boundary, conformal intervals via three different methods, and resultant CPICFs. Second, in this synthetic data set we explore the impact of a single CPICF on the knowledge of an individual locally around the original query. Finally, in both our synthetic data set and a complex real world dataset with a combination of continuous and discrete variables, we measure the utility of these counterfactuals via data augmentation, testing the performance on a held out set.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Credal Prediction based on Relative Likelihood</title>
<link>https://arxiv.org/abs/2505.22332</link>
<guid>https://arxiv.org/abs/2505.22332</guid>
<content:encoded><![CDATA[
arXiv:2505.22332v1 Announce Type: cross 
Abstract: Predictions in the form of sets of probability distributions, so-called credal sets, provide a suitable means to represent a learner's epistemic uncertainty. In this paper, we propose a theoretically grounded approach to credal prediction based on the statistical notion of relative likelihood: The target of prediction is the set of all (conditional) probability distributions produced by the collection of plausible models, namely those models whose relative likelihood exceeds a specified threshold. This threshold has an intuitive interpretation and allows for controlling the trade-off between correctness and precision of credal predictions. We tackle the problem of approximating credal sets defined in this way by means of suitably modified ensemble learning techniques. To validate our approach, we illustrate its effectiveness by experiments on benchmark datasets demonstrating superior uncertainty representation without compromising predictive performance. We also compare our method against several state-of-the-art baselines in credal prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Multimodal Reasoning via Reinforcement Learning with Cold Start</title>
<link>https://arxiv.org/abs/2505.22334</link>
<guid>https://arxiv.org/abs/2505.22334</guid>
<content:encoded><![CDATA[
arXiv:2505.22334v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have demonstrated impressive chain-of-thought reasoning capabilities, with reinforcement learning (RL) playing a crucial role in this progress. While "aha moment" patterns--where models exhibit self-correction through reflection--are often attributed to emergent properties from RL, we first demonstrate that these patterns exist in multimodal LLMs (MLLMs) prior to RL training but may not necessarily correlate with improved reasoning performance. Building on these insights, we present a comprehensive study on enhancing multimodal reasoning through a two-stage approach: (1) supervised fine-tuning (SFT) as a cold start with structured chain-of-thought reasoning patterns, followed by (2) reinforcement learning via GRPO to further refine these capabilities. Our extensive experiments show that this combined approach consistently outperforms both SFT-only and RL-only methods across challenging multimodal reasoning benchmarks. The resulting models achieve state-of-the-art performance among open-source MLLMs at both 3B and 7B scales, with our 7B model showing substantial improvements over base models (e.g., 66.3 %$\rightarrow$73.4 % on MathVista, 62.9 %$\rightarrow$70.4 % on We-Math) and our 3B model achieving performance competitive with several 7B models. Overall, this work provides practical guidance for building advanced multimodal reasoning models. Our code is available at https://github.com/waltonfuture/RL-with-Cold-Start.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Progressive Data Dropout: An Embarrassingly Simple Approach to Faster Training</title>
<link>https://arxiv.org/abs/2505.22342</link>
<guid>https://arxiv.org/abs/2505.22342</guid>
<content:encoded><![CDATA[
arXiv:2505.22342v1 Announce Type: cross 
Abstract: The success of the machine learning field has reliably depended on training on large datasets. While effective, this trend comes at an extraordinary cost. This is due to two deeply intertwined factors: the size of models and the size of datasets. While promising research efforts focus on reducing the size of models, the other half of the equation remains fairly mysterious. Indeed, it is surprising that the standard approach to training remains to iterate over and over, uniformly sampling the training dataset. In this paper we explore a series of alternative training paradigms that leverage insights from hard-data-mining and dropout, simple enough to implement and use that can become the new training standard. The proposed Progressive Data Dropout reduces the number of effective epochs to as little as 12.4% of the baseline. This savings actually do not come at any cost for accuracy. Surprisingly, the proposed method improves accuracy by up to 4.82%. Our approach requires no changes to model architecture or optimizer, and can be applied across standard training pipelines, thus posing an excellent opportunity for wide adoption. Code can be found here: https://github.com/bazyagami/LearningWithRevision
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computing Optimal Transport Maps and Wasserstein Barycenters Using Conditional Normalizing Flows</title>
<link>https://arxiv.org/abs/2505.22364</link>
<guid>https://arxiv.org/abs/2505.22364</guid>
<content:encoded><![CDATA[
arXiv:2505.22364v1 Announce Type: cross 
Abstract: We present a novel method for efficiently computing optimal transport maps and Wasserstein barycenters in high-dimensional spaces. Our approach uses conditional normalizing flows to approximate the input distributions as invertible pushforward transformations from a common latent space. This makes it possible to directly solve the primal problem using gradient-based minimization of the transport cost, unlike previous methods that rely on dual formulations and complex adversarial optimization. We show how this approach can be extended to compute Wasserstein barycenters by solving a conditional variance minimization problem. A key advantage of our conditional architecture is that it enables the computation of barycenters for hundreds of input distributions, which was computationally infeasible with previous methods. Our numerical experiments illustrate that our approach yields accurate results across various high-dimensional tasks and compares favorably with previous state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DAM: Domain-Aware Module for Multi-Domain Dataset Condensation</title>
<link>https://arxiv.org/abs/2505.22387</link>
<guid>https://arxiv.org/abs/2505.22387</guid>
<content:encoded><![CDATA[
arXiv:2505.22387v1 Announce Type: cross 
Abstract: Dataset Condensation (DC) has emerged as a promising solution to mitigate the computational and storage burdens associated with training deep learning models. However, existing DC methods largely overlook the multi-domain nature of modern datasets, which are increasingly composed of heterogeneous images spanning multiple domains. In this paper, we extend DC and introduce Multi-Domain Dataset Condensation (MDDC), which aims to condense data that generalizes across both single-domain and multi-domain settings. To this end, we propose the Domain-Aware Module (DAM), a training-time module that embeds domain-related features into each synthetic image via learnable spatial masks. As explicit domain labels are mostly unavailable in real-world datasets, we employ frequency-based pseudo-domain labeling, which leverages low-frequency amplitude statistics. DAM is only active during the condensation process, thus preserving the same images per class (IPC) with prior methods. Experiments show that DAM consistently improves in-domain, out-of-domain, and cross-architecture performance over baseline dataset condensation methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synonymous Variational Inference for Perceptual Image Compression</title>
<link>https://arxiv.org/abs/2505.22438</link>
<guid>https://arxiv.org/abs/2505.22438</guid>
<content:encoded><![CDATA[
arXiv:2505.22438v1 Announce Type: cross 
Abstract: Recent contributions of semantic information theory reveal the set-element relationship between semantic and syntactic information, represented as synonymous relationships. In this paper, we propose a synonymous variational inference (SVI) method based on this synonymity viewpoint to re-analyze the perceptual image compression problem. It takes perceptual similarity as a typical synonymous criterion to build an ideal synonymous set (Synset), and approximate the posterior of its latent synonymous representation with a parametric density by minimizing a partial semantic KL divergence. This analysis theoretically proves that the optimization direction of perception image compression follows a triple tradeoff that can cover the existing rate-distortion-perception schemes. Additionally, we introduce synonymous image compression (SIC), a new image compression scheme that corresponds to the analytical process of SVI, and implement a progressive SIC codec to fully leverage the model's capabilities. Experimental results demonstrate comparable rate-distortion-perception performance using a single progressive SIC codec, thus verifying the effectiveness of our proposed analysis method.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unsupervised Post-Training for Multi-Modal LLM Reasoning via GRPO</title>
<link>https://arxiv.org/abs/2505.22453</link>
<guid>https://arxiv.org/abs/2505.22453</guid>
<content:encoded><![CDATA[
arXiv:2505.22453v1 Announce Type: cross 
Abstract: Improving Multi-modal Large Language Models (MLLMs) in the post-training stage typically relies on supervised fine-tuning (SFT) or reinforcement learning (RL). However, these supervised methods require expensive and manually annotated multi-modal data--an ultimately unsustainable resource. While recent efforts have explored unsupervised post-training, their methods are complex and difficult to iterate. In this work, we are the first to investigate the use of GRPO, a stable and scalable online RL algorithm, for enabling continual self-improvement without any external supervision. We propose MM-UPT, a simple yet effective framework for unsupervised post-training of MLLMs. MM-UPT builds upon GRPO, replacing traditional reward signals with a self-rewarding mechanism based on majority voting over multiple sampled responses. Our experiments demonstrate that MM-UPT significantly improves the reasoning ability of Qwen2.5-VL-7B (e.g., 66.3 %$\rightarrow$72.9 % on MathVista, 62.9 %$\rightarrow$68.7 % on We-Math), using standard dataset without ground truth labels. MM-UPT also outperforms prior unsupervised baselines and even approaches the results of supervised GRPO. Furthermore, we show that incorporating synthetic questions, generated solely by MLLM itself, can boost performance as well, highlighting a promising approach for scalable self-improvement. Overall, MM-UPT offers a new paradigm for continual, autonomous enhancement of MLLMs in the absence of external supervision. Our code is available at https://github.com/waltonfuture/MM-UPT.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Depth-Based Matrix Classification for the HHL Quantum Algorithm</title>
<link>https://arxiv.org/abs/2505.22454</link>
<guid>https://arxiv.org/abs/2505.22454</guid>
<content:encoded><![CDATA[
arXiv:2505.22454v1 Announce Type: cross 
Abstract: Under the nearing error-corrected era of quantum computing, it is necessary to understand the suitability of certain post-NISQ algorithms for practical problems. One of the most promising, applicable and yet difficult to implement in practical terms is the Harrow, Hassidim and Lloyd (HHL) algorithm for linear systems of equations. An enormous number of problems can be expressed as linear systems of equations, from Machine Learning to fluid dynamics. However, in most cases, HHL will not be able to provide a practical, reasonable solution to these problems. This paper's goal inquires about whether problems can be labeled using Machine Learning classifiers as suitable or unsuitable for HHL implementation when some numerical information about the problem is known beforehand. This work demonstrates that training on significantly representative data distributions is critical to achieve good classifications of the problems based on the numerical properties of the matrix representing the system of equations. Accurate classification is possible through Multi-Layer Perceptrons, although with careful design of the training data distribution and classifier parameters.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Structure Learning Should Be A Research Priority for LLM-Based Multi-Agent Systems</title>
<link>https://arxiv.org/abs/2505.22467</link>
<guid>https://arxiv.org/abs/2505.22467</guid>
<content:encoded><![CDATA[
arXiv:2505.22467v1 Announce Type: cross 
Abstract: Large Language Model-based Multi-Agent Systems (MASs) have emerged as a powerful paradigm for tackling complex tasks through collaborative intelligence. Nevertheless, the question of how agents should be structurally organized for optimal cooperation remains largely unexplored. In this position paper, we aim to gently redirect the focus of the MAS research community toward this critical dimension: develop topology-aware MASs for specific tasks. Specifically, the system consists of three core components - agents, communication links, and communication patterns - that collectively shape its coordination performance and efficiency. To this end, we introduce a systematic, three-stage framework: agent selection, structure profiling, and topology synthesis. Each stage would trigger new research opportunities in areas such as language models, reinforcement learning, graph learning, and generative modeling; together, they could unleash the full potential of MASs in complicated real-world applications. Then, we discuss the potential challenges and opportunities in the evaluation of multiple systems. We hope our perspective and framework can offer critical new insights in the era of agentic AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CPINN-ABPI: Physics-Informed Neural Networks for Accurate Power Estimation in MPSoCs</title>
<link>https://arxiv.org/abs/2505.22469</link>
<guid>https://arxiv.org/abs/2505.22469</guid>
<content:encoded><![CDATA[
arXiv:2505.22469v1 Announce Type: cross 
Abstract: Efficient thermal and power management in modern multiprocessor systems-on-chip (MPSoCs) demands accurate power consumption estimation. One of the state-of-the-art approaches, Alternative Blind Power Identification (ABPI), theoretically eliminates the dependence on steady-state temperatures, addressing a major shortcoming of previous approaches. However, ABPI performance has remained unverified in actual hardware implementations. In this study, we conduct the first empirical validation of ABPI on commercial hardware using the NVIDIA Jetson Xavier AGX platform. Our findings reveal that, while ABPI provides computational efficiency and independence from steady-state temperature, it exhibits considerable accuracy deficiencies in real-world scenarios. To overcome these limitations, we introduce a novel approach that integrates Custom Physics-Informed Neural Networks (CPINNs) with the underlying thermal model of ABPI. Our approach employs a specialized loss function that harmonizes physical principles with data-driven learning, complemented by multi-objective genetic algorithm optimization to balance estimation accuracy and computational cost. In experimental validation, CPINN-ABPI achieves a reduction of 84.7\% CPU and 73.9\% GPU in the mean absolute error (MAE) relative to ABPI, with the weighted mean absolute percentage error (WMAPE) improving from 47\%--81\% to $\sim$12\%. The method maintains real-time performance with 195.3~$\mu$s of inference time, with similar 85\%--99\% accuracy gains across heterogeneous SoCs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hypothesis Testing in Imaging Inverse Problems</title>
<link>https://arxiv.org/abs/2505.22481</link>
<guid>https://arxiv.org/abs/2505.22481</guid>
<content:encoded><![CDATA[
arXiv:2505.22481v1 Announce Type: cross 
Abstract: This paper proposes a framework for semantic hypothesis testing tailored to imaging inverse problems. Modern imaging methods struggle to support hypothesis testing, a core component of the scientific method that is essential for the rigorous interpretation of experiments and robust interfacing with decision-making processes. There are three main reasons why image-based hypothesis testing is challenging. First, the difficulty of using a single observation to simultaneously reconstruct an image, formulate hypotheses, and quantify their statistical significance. Second, the hypotheses encountered in imaging are mostly of semantic nature, rather than quantitative statements about pixel values. Third, it is challenging to control test error probabilities because the null and alternative distributions are often unknown. Our proposed approach addresses these difficulties by leveraging concepts from self-supervised computational imaging, vision-language models, and non-parametric hypothesis testing with e-values. We demonstrate our proposed framework through numerical experiments related to image-based phenotyping, where we achieve excellent power while robustly controlling Type I errors.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Assessing Quantum Advantage for Gaussian Process Regression</title>
<link>https://arxiv.org/abs/2505.22502</link>
<guid>https://arxiv.org/abs/2505.22502</guid>
<content:encoded><![CDATA[
arXiv:2505.22502v1 Announce Type: cross 
Abstract: Gaussian Process Regression is a well-known machine learning technique for which several quantum algorithms have been proposed. We show here that in a wide range of scenarios these algorithms show no exponential speedup. We achieve this by rigorously proving that the condition number of a kernel matrix scales at least linearly with the matrix size under general assumptions on the data and kernel. We additionally prove that the sparsity and Frobenius norm of a kernel matrix scale linearly under similar assumptions. The implications for the quantum algorithms runtime are independent of the complexity of loading classical data on a quantum computer and also apply to dequantised algorithms. We supplement our theoretical analysis with numerical verification for popular kernels in machine learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>IGNIS: A Neural Network Framework for Robust Parameter Estimation in Archimedean Copulas</title>
<link>https://arxiv.org/abs/2505.22518</link>
<guid>https://arxiv.org/abs/2505.22518</guid>
<content:encoded><![CDATA[
arXiv:2505.22518v1 Announce Type: cross 
Abstract: Parameter estimation for Archimedean copulas remains a challenging problem, particularly for the recently developed A1 and A2 families that exhibit complex dependency structures. Traditional methods, such as the Method of Moments (MoM), Maximum Likelihood Estimation (MLE), and Maximum Pseudo-Likelihood (MPL), often struggle due to issues of non-monotonic relationship with dependency measures such as Kendall's tau (as in the case of A1) and numerical instability. In this paper, we present the IGNIS Network, a novel, unified neural framework that learns a direct mapping from observable dependency measures to copula parameters, thereby overcoming the limitations of classical approaches. Our approach is trained on simulated data spanning five Archimedean copula families including Clayton, Gumbel, Frank, A1, and A2, ensuring its general applicability across the entire family. Extensive simulation studies demonstrate that the IGNIS Network reduces estimation errors compared to MoM, while inherently enforcing parameter constraints through theory-guided post-processing. We further validate the practical utility of our method on diverse real-world datasets, including financial returns (AAPL-MSFT), healthcare metrics (CDC Diabetes indicators), and environmental measurements (PM2.5 air quality). Our results underscore the transformative potential of neural methods for robust and accurate dependence modeling in modern applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symplectic Generative Networks (SGNs): A Hamiltonian Framework for Invertible Deep Generative Modeling</title>
<link>https://arxiv.org/abs/2505.22527</link>
<guid>https://arxiv.org/abs/2505.22527</guid>
<content:encoded><![CDATA[
arXiv:2505.22527v1 Announce Type: cross 
Abstract: We introduce the Symplectic Generative Network (SGN), a deep generative model that leverages Hamiltonian mechanics to construct an invertible, volume-preserving mapping between a latent space and the data space. By endowing the latent space with a symplectic structure and modeling data generation as the time evolution of a Hamiltonian system, SGN achieves exact likelihood evaluation without incurring the computational overhead of Jacobian determinant calculations. In this work, we provide a rigorous mathematical foundation for SGNs through a comprehensive theoretical framework that includes: (i) complete proofs of invertibility and volume preservation, (ii) a formal complexity analysis with theoretical comparisons to Variational Autoencoders and Normalizing Flows, (iii) strengthened universal approximation results with quantitative error bounds, (iv) an information-theoretic analysis based on the geometry of statistical manifolds, and (v) an extensive stability analysis with adaptive integration guarantees. These contributions highlight the fundamental advantages of SGNs and establish a solid foundation for future empirical investigations and applications to complex, high-dimensional data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RiverMamba: A State Space Model for Global River Discharge and Flood Forecasting</title>
<link>https://arxiv.org/abs/2505.22535</link>
<guid>https://arxiv.org/abs/2505.22535</guid>
<content:encoded><![CDATA[
arXiv:2505.22535v1 Announce Type: cross 
Abstract: Recent deep learning approaches for river discharge forecasting have improved the accuracy and efficiency in flood forecasting, enabling more reliable early warning systems for risk management. Nevertheless, existing deep learning approaches in hydrology remain largely confined to local-scale applications and do not leverage the inherent spatial connections of bodies of water. Thus, there is a strong need for new deep learning methodologies that are capable of modeling spatio-temporal relations to improve river discharge and flood forecasting for scientific and operational applications. To address this, we present RiverMamba, a novel deep learning model that is pretrained with long-term reanalysis data and that can forecast global river discharge and floods on a $0.05^\circ$ grid up to 7 days lead time, which is of high relevance in early warning. To achieve this, RiverMamba leverages efficient Mamba blocks that enable the model to capture global-scale channel network routing and enhance its forecast capability for longer lead times. The forecast blocks integrate ECMWF HRES meteorological forecasts, while accounting for their inaccuracies through spatio-temporal modeling. Our analysis demonstrates that RiverMamba delivers reliable predictions of river discharge, including extreme floods across return periods and lead times, surpassing both operational AI- and physics-based models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Copulas Be Used for Feature Selection? A Machine Learning Study on Diabetes Risk Prediction</title>
<link>https://arxiv.org/abs/2505.22554</link>
<guid>https://arxiv.org/abs/2505.22554</guid>
<content:encoded><![CDATA[
arXiv:2505.22554v1 Announce Type: cross 
Abstract: Accurate diabetes risk prediction relies on identifying key features from complex health datasets, but conventional methods like mutual information (MI) filters and genetic algorithms (GAs) often overlook extreme dependencies critical for high-risk subpopulations. In this study we introduce a feature-selection framework using the upper-tail dependence coefficient ({\lambda}U) of the novel A2 copula, which quantifies how often extreme higher values of a predictor co-occur with diabetes diagnoses (target variable). Applied to the CDC Diabetes Health Indicators dataset (n=253,680), our method prioritizes five predictors (self-reported general health, high blood pressure, body mass index, mobility limitations, and high cholesterol levels) based on upper tail dependencies. These features match or outperform MI and GA selected subsets across four classifiers (Random Forest, XGBoost, Logistic Regression, Gradient Boosting), achieving accuracy up to 86.5% (XGBoost) and AUC up to 0.806 (Gradient Boosting), rivaling the full 21-feature model. Permutation importance confirms clinical relevance, with BMI and general health driving accuracy. To our knowledge, this is the first work to apply a copula's upper-tail dependence for supervised feature selection, bridging extreme-value theory and machine learning to deliver a practical toolkit for diabetes prevention.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRISM: Video Dataset Condensation with Progressive Refinement and Insertion for Sparse Motion</title>
<link>https://arxiv.org/abs/2505.22564</link>
<guid>https://arxiv.org/abs/2505.22564</guid>
<content:encoded><![CDATA[
arXiv:2505.22564v1 Announce Type: cross 
Abstract: Video dataset condensation has emerged as a critical technique for addressing the computational challenges associated with large-scale video data processing in deep learning applications. While significant progress has been made in image dataset condensation, the video domain presents unique challenges due to the complex interplay between spatial content and temporal dynamics. This paper introduces PRISM, Progressive Refinement and Insertion for Sparse Motion, for video dataset condensation, a novel approach that fundamentally reconsiders how video data should be condensed. Unlike the previous method that separates static content from dynamic motion, our method preserves the essential interdependence between these elements. Our approach progressively refines and inserts frames to fully accommodate the motion in an action while achieving better performance but less storage, considering the relation of gradients for each frame. Extensive experiments across standard video action recognition benchmarks demonstrate that PRISM outperforms existing disentangled approaches while maintaining compact representations suitable for resource-constrained environments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Error-Instruct: Generalizing from Errors for LLMs Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.22591</link>
<guid>https://arxiv.org/abs/2505.22591</guid>
<content:encoded><![CDATA[
arXiv:2505.22591v1 Announce Type: cross 
Abstract: Although large language models demonstrate strong performance across various domains, they still struggle with numerous bad cases in mathematical reasoning. Previous approaches to learning from errors synthesize training data by solely extrapolating from isolated bad cases, thereby failing to generalize the extensive patterns inherent within these cases. This paper presents Self-Error-Instruct (SEI), a framework that addresses these model weaknesses and synthesizes more generalized targeted training data. Specifically, we explore a target model on two mathematical datasets, GSM8K and MATH, to pinpoint bad cases. Then, we generate error keyphrases for these cases based on the instructor model's (GPT-4o) analysis and identify error types by clustering these keyphrases. Next, we sample a few bad cases during each generation for each identified error type and input them into the instructor model, which synthesizes additional training data using a self-instruct approach. This new data is refined through a one-shot learning process to ensure that only the most effective examples are kept. Finally, we use these curated data to fine-tune the target model, iteratively repeating the process to enhance performance. We apply our framework to various models and observe improvements in their reasoning abilities across both in-domain and out-of-domain mathematics datasets. These results demonstrate the effectiveness of self-error instruction in improving LLMs' mathematical reasoning through error generalization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HDDLGym: A Tool for Studying Multi-Agent Hierarchical Problems Defined in HDDL with OpenAI Gym</title>
<link>https://arxiv.org/abs/2505.22597</link>
<guid>https://arxiv.org/abs/2505.22597</guid>
<content:encoded><![CDATA[
arXiv:2505.22597v1 Announce Type: cross 
Abstract: In recent years, reinforcement learning (RL) methods have been widely tested using tools like OpenAI Gym, though many tasks in these environments could also benefit from hierarchical planning. However, there is a lack of a tool that enables seamless integration of hierarchical planning with RL. Hierarchical Domain Definition Language (HDDL), used in classical planning, introduces a structured approach well-suited for model-based RL to address this gap. To bridge this integration, we introduce HDDLGym, a Python-based tool that automatically generates OpenAI Gym environments from HDDL domains and problems. HDDLGym serves as a link between RL and hierarchical planning, supporting multi-agent scenarios and enabling collaborative planning among agents. This paper provides an overview of HDDLGym's design and implementation, highlighting the challenges and design choices involved in integrating HDDL with the Gym interface, and applying RL policies to support hierarchical planning. We also provide detailed instructions and demonstrations for using the HDDLGym framework, including how to work with existing HDDL domains and problems from International Planning Competitions, exemplified by the Transport domain. Additionally, we offer guidance on creating new HDDL domains for multi-agent scenarios and demonstrate the practical use of HDDLGym in the Overcooked domain. By leveraging the advantages of HDDL and Gym, HDDLGym aims to be a valuable tool for studying RL in hierarchical planning, particularly in multi-agent contexts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the performance of machine-learning assisted Monte Carlo in sampling from simple statistical physics models</title>
<link>https://arxiv.org/abs/2505.22598</link>
<guid>https://arxiv.org/abs/2505.22598</guid>
<content:encoded><![CDATA[
arXiv:2505.22598v1 Announce Type: cross 
Abstract: Recent years have seen a rise in the application of machine learning techniques to aid the simulation of hard-to-sample systems that cannot be studied using traditional methods. Despite the introduction of many different architectures and procedures, a wide theoretical understanding is still lacking, with the risk of suboptimal implementations. As a first step to address this gap, we provide here a complete analytic study of the widely-used Sequential Tempering procedure applied to a shallow MADE architecture for the Curie-Weiss model. The contribution of this work is twofold: firstly, we give a description of the optimal weights and of the training under Gradient Descent optimization. Secondly, we compare what happens in Sequential Tempering with and without the addition of local Metropolis Monte Carlo steps. We are thus able to give theoretical predictions on the best procedure to apply in this case. This work establishes a clear theoretical basis for the integration of machine learning techniques into Monte Carlo sampling and optimization.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chest Disease Detection In X-Ray Images Using Deep Learning Classification Method</title>
<link>https://arxiv.org/abs/2505.22609</link>
<guid>https://arxiv.org/abs/2505.22609</guid>
<content:encoded><![CDATA[
arXiv:2505.22609v1 Announce Type: cross 
Abstract: In this work, we investigate the performance across multiple classification models to classify chest X-ray images into four categories of COVID-19, pneumonia, tuberculosis (TB), and normal cases. We leveraged transfer learning techniques with state-of-the-art pre-trained Convolutional Neural Networks (CNNs) models. We fine-tuned these pre-trained architectures on a labeled medical x-ray images. The initial results are promising with high accuracy and strong performance in key classification metrics such as precision, recall, and F1 score. We applied Gradient-weighted Class Activation Mapping (Grad-CAM) for model interpretability to provide visual explanations for classification decisions, improving trust and transparency in clinical applications.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Principled Out-of-Distribution Generalization via Simplicity</title>
<link>https://arxiv.org/abs/2505.22622</link>
<guid>https://arxiv.org/abs/2505.22622</guid>
<content:encoded><![CDATA[
arXiv:2505.22622v1 Announce Type: cross 
Abstract: Modern foundation models exhibit remarkable out-of-distribution (OOD) generalization, solving tasks far beyond the support of their training data. However, the theoretical principles underpinning this phenomenon remain elusive. This paper investigates this problem by examining the compositional generalization abilities of diffusion models in image generation. Our analysis reveals that while neural network architectures are expressive enough to represent a wide range of models -- including many with undesirable behavior on OOD inputs -- the true, generalizable model that aligns with human expectations typically corresponds to the simplest among those consistent with the training data.
  Motivated by this observation, we develop a theoretical framework for OOD generalization via simplicity, quantified using a predefined simplicity metric. We analyze two key regimes: (1) the constant-gap setting, where the true model is strictly simpler than all spurious alternatives by a fixed gap, and (2) the vanishing-gap setting, where the fixed gap is replaced by a smoothness condition ensuring that models close in simplicity to the true model yield similar predictions. For both regimes, we study the regularized maximum likelihood estimator and establish the first sharp sample complexity guarantees for learning the true, generalizable, simple model.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SCIZOR: A Self-Supervised Approach to Data Curation for Large-Scale Imitation Learning</title>
<link>https://arxiv.org/abs/2505.22626</link>
<guid>https://arxiv.org/abs/2505.22626</guid>
<content:encoded><![CDATA[
arXiv:2505.22626v1 Announce Type: cross 
Abstract: Imitation learning advances robot capabilities by enabling the acquisition of diverse behaviors from human demonstrations. However, large-scale datasets used for policy training often introduce substantial variability in quality, which can negatively impact performance. As a result, automatically curating datasets by filtering low-quality samples to improve quality becomes essential. Existing robotic curation approaches rely on costly manual annotations and perform curation at a coarse granularity, such as the dataset or trajectory level, failing to account for the quality of individual state-action pairs. To address this, we introduce SCIZOR, a self-supervised data curation framework that filters out low-quality state-action pairs to improve the performance of imitation learning policies. SCIZOR targets two complementary sources of low-quality data: suboptimal data, which hinders learning with undesirable actions, and redundant data, which dilutes training with repetitive patterns. SCIZOR leverages a self-supervised task progress predictor for suboptimal data to remove samples lacking task progression, and a deduplication module operating on joint state-action representation for samples with redundant patterns. Empirically, we show that SCIZOR enables imitation learning policies to achieve higher performance with less data, yielding an average improvement of 15.4% across multiple benchmarks. More information is available at: https://ut-austin-rpl.github.io/SCIZOR/
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spatial Knowledge Graph-Guided Multimodal Synthesis</title>
<link>https://arxiv.org/abs/2505.22633</link>
<guid>https://arxiv.org/abs/2505.22633</guid>
<content:encoded><![CDATA[
arXiv:2505.22633v1 Announce Type: cross 
Abstract: Recent advances in multimodal large language models (MLLMs) have significantly enhanced their capabilities; however, their spatial perception abilities remain a notable limitation. To address this challenge, multimodal data synthesis offers a promising solution. Yet, ensuring that synthesized data adhere to spatial common sense is a non-trivial task. In this work, we introduce SKG2Data, a novel multimodal synthesis approach guided by spatial knowledge graphs, grounded in the concept of knowledge-to-data generation. SKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate human-like perception of spatial directions and distances, which is subsequently utilized to guide multimodal data synthesis. Extensive experiments demonstrate that data synthesized from diverse types of spatial knowledge, including direction and distance, not only enhance the spatial perception and reasoning abilities of MLLMs but also exhibit strong generalization capabilities. We hope that the idea of knowledge-based data synthesis can advance the development of spatial intelligence.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SimProcess: High Fidelity Simulation of Noisy ICS Physical Processes</title>
<link>https://arxiv.org/abs/2505.22638</link>
<guid>https://arxiv.org/abs/2505.22638</guid>
<content:encoded><![CDATA[
arXiv:2505.22638v1 Announce Type: cross 
Abstract: Industrial Control Systems (ICS) manage critical infrastructures like power grids and water treatment plants. Cyberattacks on ICSs can disrupt operations, causing severe economic, environmental, and safety issues. For example, undetected pollution in a water plant can put the lives of thousands at stake. ICS researchers have increasingly turned to honeypots -- decoy systems designed to attract attackers, study their behaviors, and eventually improve defensive mechanisms. However, existing ICS honeypots struggle to replicate the ICS physical process, making them susceptible to detection. Accurately simulating the noise in ICS physical processes is challenging because different factors produce it, including sensor imperfections and external interferences.
  In this paper, we propose SimProcess, a novel framework to rank the fidelity of ICS simulations by evaluating how closely they resemble real-world and noisy physical processes. It measures the simulation distance from a target system by estimating the noise distribution with machine learning models like Random Forest. Unlike existing solutions that require detailed mathematical models or are limited to simple systems, SimProcess operates with only a timeseries of measurements from the real system, making it applicable to a broader range of complex dynamic systems. We demonstrate the framework's effectiveness through a case study using real-world power grid data from the EPIC testbed. We compare the performance of various simulation methods, including static and generative noise techniques. Our model correctly classifies real samples with a recall of up to 1.0. It also identifies Gaussian and Gaussian Mixture as the best distribution to simulate our power systems, together with a generative solution provided by an autoencoder, thereby helping developers to improve honeypot fidelity. Additionally, we make our code publicly available.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid Control</title>
<link>https://arxiv.org/abs/2505.22642</link>
<guid>https://arxiv.org/abs/2505.22642</guid>
<content:encoded><![CDATA[
arXiv:2505.22642v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has driven significant progress in robotics, but its complexity and long training times remain major bottlenecks. In this report, we introduce FastTD3, a simple, fast, and capable RL algorithm that significantly speeds up training for humanoid robots in popular suites such as HumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably simple: we train an off-policy TD3 agent with several modifications -- parallel simulation, large-batch updates, a distributional critic, and carefully tuned hyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours on a single A100 GPU, while remaining stable during training. We also provide a lightweight and easy-to-use implementation of FastTD3 to accelerate RL research in robotics.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pre-training for Recommendation Unlearning</title>
<link>https://arxiv.org/abs/2505.22649</link>
<guid>https://arxiv.org/abs/2505.22649</guid>
<content:encoded><![CDATA[
arXiv:2505.22649v1 Announce Type: cross 
Abstract: Modern recommender systems powered by Graph Neural Networks (GNNs) excel at modeling complex user-item interactions, yet increasingly face scenarios requiring selective forgetting of training data. Beyond user requests to remove specific interactions due to privacy concerns or preference changes, regulatory frameworks mandate recommender systems' ability to eliminate the influence of certain user data from models. This recommendation unlearning challenge presents unique difficulties as removing connections within interaction graphs creates ripple effects throughout the model, potentially impacting recommendations for numerous users. Traditional approaches suffer from significant drawbacks: fragmentation methods damage graph structure and diminish performance, while influence function techniques make assumptions that may not hold in complex GNNs, particularly with self-supervised or random architectures. To address these limitations, we propose a novel model-agnostic pre-training paradigm UnlearnRec that prepares systems for efficient unlearning operations. Our Influence Encoder takes unlearning requests together with existing model parameters and directly produces updated parameters of unlearned model with little fine-tuning, avoiding complete retraining while preserving model performance characteristics. Extensive evaluation on public benchmarks demonstrates that our method delivers exceptional unlearning effectiveness while providing more than 10x speedup compared to retraining approaches. We release our method implementation at: https://github.com/HKUDS/UnlearnRec.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sherlock: Self-Correcting Reasoning in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.22651</link>
<guid>https://arxiv.org/abs/2505.22651</guid>
<content:encoded><![CDATA[
arXiv:2505.22651v1 Announce Type: cross 
Abstract: Reasoning Vision-Language Models (VLMs) have shown promising performance on complex multimodal tasks. However, they still face significant challenges: they are highly sensitive to reasoning errors, require large volumes of annotated data or accurate verifiers, and struggle to generalize beyond specific domains. To address these limitations, we explore self-correction as a strategy to enhance reasoning VLMs. We first conduct an in-depth analysis of reasoning VLMs' self-correction abilities and identify key gaps. Based on our findings, we introduce Sherlock, a self-correction and self-improvement training framework. Sherlock introduces a trajectory-level self-correction objective, a preference data construction method based on visual perturbation, and a dynamic $\beta$ for preference tuning. Once the model acquires self-correction capabilities using only 20k randomly sampled annotated data, it continues to self-improve without external supervision. Built on the Llama3.2-Vision-11B model, Sherlock achieves remarkable results across eight benchmarks, reaching an average accuracy of 64.1 with direct generation and 65.4 after self-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and LlamaV-o1 (63.4) while using less than 20% of the annotated data.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</title>
<link>https://arxiv.org/abs/2505.22657</link>
<guid>https://arxiv.org/abs/2505.22657</guid>
<content:encoded><![CDATA[
arXiv:2505.22657v1 Announce Type: cross 
Abstract: Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent's ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench's most challenging in-the-wild embodied tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoL2S: Auto Long-Short Reasoning for Efficient Large Language Models</title>
<link>https://arxiv.org/abs/2505.22662</link>
<guid>https://arxiv.org/abs/2505.22662</guid>
<content:encoded><![CDATA[
arXiv:2505.22662v1 Announce Type: cross 
Abstract: The reasoning-capable large language models (LLMs) demonstrate strong performance on complex reasoning tasks but often suffer from overthinking, generating unnecessarily long chain-of-thought (CoT) reasoning paths for easy reasoning questions, thereby increasing inference cost and latency. Recent approaches attempt to address this challenge by manually deciding when to apply long or short reasoning. However, they lack the flexibility to adapt CoT length dynamically based on question complexity. In this paper, we propose Auto Long-Short Reasoning (AutoL2S), a dynamic and model-agnostic framework that enables LLMs to dynamically compress their generated reasoning path based on the complexity of the reasoning question. AutoL2S enables a learned paradigm, in which LLMs themselves can decide when longer reasoning is necessary and when shorter reasoning suffices, by training on data annotated with our proposed method, which includes both long and short CoT paths and a special  token. We then use  token to indicate when the model can skip generating lengthy CoT reasoning. This proposed annotation strategy can enhance the LLMs' ability to generate shorter CoT reasoning paths with improved quality after training. Extensive evaluation results show that AutoL2S reduces the length of reasoning generation by up to 57% without compromising performance, demonstrating the effectiveness of AutoL2S for scalable and efficient LLM reasoning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures</title>
<link>https://arxiv.org/abs/2303.09981</link>
<guid>https://arxiv.org/abs/2303.09981</guid>
<content:encoded><![CDATA[
arXiv:2303.09981v3 Announce Type: replace 
Abstract: Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior differs among flight stages. In this paper, we propose a simple probabilistic model that can learn this variability from procedural data and flight tracks collected from radar surveillance data. For each segment, we use a Gaussian mixture model to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we generate synthetic trajectories by sampling a series of deviations from the Gaussian mixture model and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitrary number of aircraft. We demonstrate the proposed models on the arrival tracks and procedures of the John F. Kennedy International Airport. Distributional similarity between the original and the synthetic trajectory dataset was evaluated using the Jensen-Shannon divergence between the empirical distributions of different variables and we provide qualitative analyses of the synthetic trajectories generated.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Empirical Evaluation of Rewiring Approaches in Graph Neural Networks</title>
<link>https://arxiv.org/abs/2305.19717</link>
<guid>https://arxiv.org/abs/2305.19717</guid>
<content:encoded><![CDATA[
arXiv:2305.19717v2 Announce Type: replace 
Abstract: Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification tasks, showing that rewiring the underlying graph rarely does confer a practical benefit for message-passing.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Variational Positive-incentive Noise: How Noise Benefits Models</title>
<link>https://arxiv.org/abs/2306.07651</link>
<guid>https://arxiv.org/abs/2306.07651</guid>
<content:encoded><![CDATA[
arXiv:2306.07651v2 Announce Type: replace 
Abstract: A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers to blur the irrelevant ingredients in complicated images, which meets our expectations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Watermarks in the Sand: Impossibility of Strong Watermarking for Generative Models</title>
<link>https://arxiv.org/abs/2311.04378</link>
<guid>https://arxiv.org/abs/2311.04378</guid>
<content:encoded><![CDATA[
arXiv:2311.04378v5 Announce Type: replace 
Abstract: Watermarking generative models consists of planting a statistical signal (watermark) in a model's output so that it can be later verified that the output was generated by the given model. A strong watermarking scheme satisfies the property that a computationally bounded attacker cannot erase the watermark without causing significant quality degradation. In this paper, we study the (im)possibility of strong watermarking schemes. We prove that, under well-specified and natural assumptions, strong watermarking is impossible to achieve. This holds even in the private detection algorithm setting, where the watermark insertion and detection algorithms share a secret key, unknown to the attacker. To prove this result, we introduce a generic efficient watermark attack; the attacker is not required to know the private key of the scheme or even which scheme is used. Our attack is based on two assumptions: (1) The attacker has access to a "quality oracle" that can evaluate whether a candidate output is a high-quality response to a prompt, and (2) The attacker has access to a "perturbation oracle" which can modify an output with a nontrivial probability of maintaining quality, and which induces an efficiently mixing random walk on high-quality outputs. We argue that both assumptions can be satisfied in practice by an attacker with weaker computational capabilities than the watermarked model itself, to which the attacker has only black-box access. Furthermore, our assumptions will likely only be easier to satisfy over time as models grow in capabilities and modalities. We demonstrate the feasibility of our attack by instantiating it to attack three existing watermarking schemes for large language models: Kirchenbauer et al. (2023), Kuditipudi et al. (2023), and Zhao et al. (2023). The same attack successfully removes the watermarks planted by all three schemes, with only minor quality degradation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models</title>
<link>https://arxiv.org/abs/2401.10690</link>
<guid>https://arxiv.org/abs/2401.10690</guid>
<content:encoded><![CDATA[
arXiv:2401.10690v5 Announce Type: replace 
Abstract: Dyadic regression models, which output real-valued predictions for pairs of entities, are fundamental in many domains (e.g. obtaining user-product ratings in Recommender Systems) and promising and under exploration in others (e.g. tuning patient-drug dosages in precision pharmacology). In this work, we prove that non-uniform observed value distributions of individual entities lead to severe biases in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet crucial cases; we name this phenomenon eccentricity bias. We show that global error metrics like Root Mean Squared Error (RMSE) are insufficient to capture this bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a novel metric that can quantify it in all studied domains and models. We prove the intuitive interpretation of EAUC by experimenting with naive post-training bias corrections, and theorize other options to use EAUC to guide the construction of fair models. This work contributes a bias-aware evaluation of dyadic regression to prevent unfairness in critical real-world applications of such systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intrinsic User-Centric Interpretability through Global Mixture of Experts</title>
<link>https://arxiv.org/abs/2402.02933</link>
<guid>https://arxiv.org/abs/2402.02933</guid>
<content:encoded><![CDATA[
arXiv:2402.02933v4 Announce Type: replace 
Abstract: In human-centric settings like education or healthcare, model accuracy and model explainability are key factors for user adoption. Towards these two goals, intrinsically interpretable deep learning models have gained popularity, focusing on accurate predictions alongside faithful explanations. However, there exists a gap in the human-centeredness of these approaches, which often produce nuanced and complex explanations that are not easily actionable for downstream users. We present InterpretCC (interpretable conditional computation), a family of intrinsically interpretable neural networks at a unique point in the design space that optimizes for ease of human understanding and explanation faithfulness, while maintaining comparable performance to state-of-the-art models. InterpretCC achieves this through adaptive sparse activation of features before prediction, allowing the model to use a different, minimal set of features for each instance. We extend this idea into an interpretable, global mixture-of-experts (MoE) model that allows users to specify topics of interest, discretely separates the feature space for each data point into topical subnetworks, and adaptively and sparsely activates these topical subnetworks for prediction. We apply InterpretCC for text, time series and tabular data across several real-world datasets, demonstrating comparable performance with non-interpretable baselines and outperforming intrinsically interpretable baselines. Through a user study involving 56 teachers, InterpretCC explanations are found to have higher actionability and usefulness over other intrinsically interpretable approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decoupled Subgraph Federated Learning</title>
<link>https://arxiv.org/abs/2402.19163</link>
<guid>https://arxiv.org/abs/2402.19163</guid>
<content:encoded><![CDATA[
arXiv:2402.19163v3 Announce Type: replace 
Abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where interconnections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of clients.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LC-Tsallis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits</title>
<link>https://arxiv.org/abs/2403.03219</link>
<guid>https://arxiv.org/abs/2403.03219</guid>
<content:encoded><![CDATA[
arXiv:2403.03219v3 Announce Type: replace 
Abstract: We investigate the \emph{linear contextual bandit problem} with independent and identically distributed (i.i.d.) contexts. In this problem, we aim to develop a \emph{Best-of-Both-Worlds} (BoBW) algorithm with regret upper bounds in both stochastic and adversarial regimes. We develop an algorithm based on \emph{Follow-The-Regularized-Leader} (FTRL) with Tsallis entropy, referred to as the $\alpha$-\emph{Linear-Contextual (LC)-Tsallis-INF}. We show that its regret is at most $O(\log(T))$ in the stochastic regime under the assumption that the suboptimality gap is uniformly bounded from below, and at most $O(\sqrt{T})$ in the adversarial regime. Furthermore, our regret analysis is extended to more general regimes characterized by the \emph{margin condition} with a parameter $\beta \in (1, \infty]$, which imposes a milder assumption on the suboptimality gap. We show that the proposed algorithm achieves $O\left(\log(T)^{\frac{1+\beta}{2+\beta}}T^{\frac{1}{2+\beta}}\right)$ regret under the margin condition.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Polynomial Chaos Expanded Gaussian Process</title>
<link>https://arxiv.org/abs/2405.01052</link>
<guid>https://arxiv.org/abs/2405.01052</guid>
<content:encoded><![CDATA[
arXiv:2405.01052v2 Announce Type: replace 
Abstract: In complex and unknown processes, global models are initially generated over the entire experimental space but often fail to provide accurate predictions in local areas. A common approach is to use local models, which requires partitioning the experimental space and training multiple models, adding significant complexity. Recognizing this limitation, this study addresses the need for models that effectively represent both global and local experimental spaces. It introduces a novel machine learning (ML) approach: Polynomial Chaos Expanded Gaussian Process (PCEGP), leveraging polynomial chaos expansion (PCE) to calculate input-dependent hyperparameters of the Gaussian process (GP). This provides a mathematically interpretable approach that incorporates non-stationary covariance functions and heteroscedastic noise estimation to generate locally adapted models. The model performance is compared to different algorithms in benchmark tests for regression tasks. The results demonstrate low prediction errors of the PCEGP, highlighting model performance that is often competitive with or better than previous methods. A key advantage of the presented model is its interpretable hyperparameters along with training and prediction runtimes comparable to those of a GP.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast meta-solvers for 3D complex-shape scatterers using neural operators trained on a non-scattering problem</title>
<link>https://arxiv.org/abs/2405.12380</link>
<guid>https://arxiv.org/abs/2405.12380</guid>
<content:encoded><![CDATA[
arXiv:2405.12380v2 Announce Type: replace 
Abstract: Three-dimensional target identification using scattering techniques requires high accuracy solutions and very fast computations for real-time predictions in some critical applications. We first train a deep neural operator~(DeepONet) to solve wave propagation problems described by the Helmholtz equation in a domain \textit{without scatterers} but at different wavenumbers and with a complex absorbing boundary condition. We then design two classes of fast meta-solvers by combining DeepONet with either relaxation methods, such as Jacobi and Gauss-Seidel, or with Krylov methods, such as GMRES and BiCGStab, using the trunk basis of DeepONet as a coarse-scale preconditioner. We leverage the spectral bias of neural networks to account for the lower part of the spectrum in the error distribution while the upper part is handled inexpensively using relaxation methods or fine-scale preconditioners. The meta-solvers are then applied to solve scattering problems with different shape of scatterers, at no extra training cost. We first demonstrate that the resulting meta-solvers are shape-agnostic, fast, and robust, whereas the standard standalone solvers may even fail to converge without the DeepONet. We then apply both classes of meta-solvers to scattering from a submarine, a complex three-dimensional problem. We achieve very fast solutions, especially with the DeepONet-Krylov methods, which require orders of magnitude fewer iterations than any of the standalone solvers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Latent Graph Structures and their Uncertainty</title>
<link>https://arxiv.org/abs/2405.19933</link>
<guid>https://arxiv.org/abs/2405.19933</guid>
<content:encoded><![CDATA[
arXiv:2405.19933v2 Announce Type: replace 
Abstract: Graph neural networks use relational information as an inductive bias to enhance prediction performance. Not rarely, task-relevant relations are unknown and graph structure learning approaches have been proposed to learn them from data. Given their latent nature, no graph observations are available to provide a direct training signal to the learnable relations. Therefore, graph topologies are typically learned on the prediction task alongside the other graph neural network parameters. In this paper, we demonstrate that minimizing point-prediction losses does not guarantee proper learning of the latent relational information and its associated uncertainty. Conversely, we prove that suitable loss functions on the stochastic model outputs simultaneously grant solving two tasks: (i) learning the unknown distribution of the latent graph and (ii) achieving optimal predictions of the target variable. Finally, we propose a sampling-based method that solves this joint learning task. Empirical results validate our theoretical claims and demonstrate the effectiveness of the proposed approach.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CTBENCH: A Library and Benchmark for Certified Training</title>
<link>https://arxiv.org/abs/2406.04848</link>
<guid>https://arxiv.org/abs/2406.04848</guid>
<content:encoded><![CDATA[
arXiv:2406.04848v4 Announce Type: replace 
Abstract: Training certifiably robust neural networks is an important but challenging task. While many algorithms for (deterministic) certified training have been proposed, they are often evaluated on different training schedules, certification methods, and systematically under-tuned hyperparameters, making it difficult to compare their performance. To address this challenge, we introduce CTBench, a unified library and a high-quality benchmark for certified training that evaluates all algorithms under fair settings and systematically tuned hyperparameters. We show that (1) almost all algorithms in CTBench surpass the corresponding reported performance in literature in the magnitude of algorithmic improvements, thus establishing new state-of-the-art, and (2) the claimed advantage of recent algorithms drops significantly when we enhance the outdated baselines with a fair training schedule, a fair certification method and well-tuned hyperparameters. Based on CTBench, we provide new insights into the current state of certified training, including (1) certified models have less fragmented loss surface, (2) certified models share many mistakes, (3) certified models have more sparse activations, (4) reducing regularization cleverly is crucial for certified training especially for large radii and (5) certified training has the potential to improve out-of-distribution generalization. We are confident that CTBench will serve as a benchmark and testbed for future research in certified training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fully Heteroscedastic Count Regression with Deep Double Poisson Networks</title>
<link>https://arxiv.org/abs/2406.09262</link>
<guid>https://arxiv.org/abs/2406.09262</guid>
<content:encoded><![CDATA[
arXiv:2406.09262v4 Announce Type: replace 
Abstract: Neural networks capable of accurate, input-conditional uncertainty representation are essential for real-world AI systems. Deep ensembles of Gaussian networks have proven highly effective for continuous regression due to their ability to flexibly represent aleatoric uncertainty via unrestricted heteroscedastic variance, which in turn enables accurate epistemic uncertainty estimation. However, no analogous approach exists for count regression, despite many important applications. To address this gap, we propose the Deep Double Poisson Network (DDPN), a novel neural discrete count regression model that outputs the parameters of the Double Poisson distribution, enabling arbitrarily high or low predictive aleatoric uncertainty for count data and improving epistemic uncertainty estimation when ensembled. We formalize and prove that DDPN exhibits robust regression properties similar to heteroscedastic Gaussian models via learnable loss attenuation, and introduce a simple loss modification to control this behavior. Experiments on diverse datasets demonstrate that DDPN outperforms current baselines in accuracy, calibration, and out-of-distribution detection, establishing a new state-of-the-art in deep count regression.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Gradient Boosting Reinforcement Learning</title>
<link>https://arxiv.org/abs/2407.08250</link>
<guid>https://arxiv.org/abs/2407.08250</guid>
<content:encoded><![CDATA[
arXiv:2407.08250v2 Announce Type: replace 
Abstract: We present Gradient Boosting Reinforcement Learning (GBRL), a framework that adapts the strengths of gradient boosting trees (GBT) to reinforcement learning (RL) tasks. While neural networks (NNs) have become the de facto choice for RL, they face significant challenges with structured and categorical features and tend to generalize poorly to out-of-distribution samples. These are challenges for which GBTs have traditionally excelled in supervised learning. However, GBT's application in RL has been limited. The design of traditional GBT libraries is optimized for static datasets with fixed labels, making them incompatible with RL's dynamic nature, where both state distributions and reward signals evolve during training. GBRL overcomes this limitation by continuously interleaving tree construction with environment interaction. Through extensive experiments, we demonstrate that GBRL outperforms NNs in domains with structured observations and categorical features while maintaining competitive performance on standard continuous control benchmarks. Like its supervised learning counterpart, GBRL demonstrates superior robustness to out-of-distribution samples and better handles irregular state-action relationships.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mini-batch Coresets for Memory-efficient Language Model Training on Data Mixtures</title>
<link>https://arxiv.org/abs/2407.19580</link>
<guid>https://arxiv.org/abs/2407.19580</guid>
<content:encoded><![CDATA[
arXiv:2407.19580v4 Announce Type: replace 
Abstract: Training with larger mini-batches improves the convergence rate and can yield superior performance. However, training with large mini-batches becomes prohibitive for Large Language Models (LLMs), due to the large GPU memory requirement. To address this problem, an effective approach is finding small mini-batch coresets that closely match the gradient of larger mini-batches. However, this approach becomes infeasible and ineffective for LLMs, due to the highly imbalanced mixture of sources in language data, use of the Adam optimizer, and the very large gradient dimensionality of LLMs. In this work, we address the above challenges by proposing Coresets for Training LLMs (CoLM). First, we show that mini-batch coresets found by gradient matching do not contain representative examples of the small sources w.h.p., and thus including all examples of the small sources in the mini-batch coresets is crucial for optimal performance. Second, we normalize the gradients by their historical exponential to find mini-batch coresets for training with Adam. Finally, we leverage zeroth-order methods to find smooth gradient of the last V-projection matrix and sparsify it to keep the dimensions with the largest normalized gradient magnitude. We apply CoLM to fine-tuning Phi-2, Phi-3, Zephyr, and Llama-3 models with LoRA on MathInstruct and SuperGLUE benchmark. Remarkably, CoLM reduces the memory requirement of fine-tuning by 2x and even outperforms training with 4x larger mini-batches. Moreover, CoLM seamlessly integrates with existing memory-efficient training methods like LoRA, further reducing the memory requirements of training LLMs. Our code is available at https://github.com/BigML-CS-UCLA/CoLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Kernel Learning for Small Dataset Modeling in Semiconductor Fabrication: Application to Ohmic Contact</title>
<link>https://arxiv.org/abs/2409.10803</link>
<guid>https://arxiv.org/abs/2409.10803</guid>
<content:encoded><![CDATA[
arXiv:2409.10803v3 Announce Type: replace 
Abstract: Modeling complex semiconductor fabrication processes such as Ohmic contact formation remains challenging due to high-dimensional parameter spaces and limited experimental data. While classical machine learning (CML) approaches have been successful in many domains, their performance degrades in small-sample, nonlinear scenarios. In this work, we investigate quantum machine learning (QML) as an alternative, exploiting quantum kernels to capture intricate correlations from compact datasets. Using only 159 experimental GaN HEMT samples, we develop a quantum kernel-aligned regressor (QKAR) combining a shallow Pauli-Z feature map with a trainable quantum kernel alignment (QKA) layer. All models, including seven baseline CML regressors, are evaluated under a unified PCA-based preprocessing pipeline to ensure a fair comparison. QKAR consistently outperforms classical baselines across multiple metrics (MAE, MSE, RMSE), achieving a mean absolute error of 0.338 Omega mm when validated on experimental data. We further assess noise robustness and generalization through cross-validation and new device fabrication. These findings suggest that carefully constructed QML models could provide predictive advantages in data-constrained semiconductor modeling, offering a foundation for practical deployment on near-term quantum hardware. While challenges remain for both QML and CML, this study demonstrates QML's potential as a complementary approach in complex process modeling tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Hybrid Multi-Factor Network with Dynamic Sequence Modeling for Early Warning of Intraoperative Hypotension</title>
<link>https://arxiv.org/abs/2409.11064</link>
<guid>https://arxiv.org/abs/2409.11064</guid>
<content:encoded><![CDATA[
arXiv:2409.11064v3 Announce Type: replace 
Abstract: Intraoperative hypotension (IOH) prediction using past physiological signals is crucial, as IOH may lead to inadequate organ perfusion and significantly elevate the risk of severe complications and mortality. However, current methods often rely on static modeling, overlooking the complex temporal dependencies and the inherently non-stationary nature of physiological signals. We propose a Hybrid Multi-Factor (HMF) network that formulates IOH prediction as a dynamic sequence forecasting task, explicitly capturing both temporal dependencies and physiological non-stationarity. We represent signal dynamics as multivariate time series and decompose them into trend and seasonal components, enabling separate modeling of long-term and periodic variations. Each component is encoded with a patch-based Transformer to balance computational efficiency and feature representation. To address distributional drift from evolving signals, we introduce a symmetric normalization mechanism. Experiments on both public and real-world clinical datasets show that HMF significantly outperforms competitive baselines. We hope HMF offers new insights into IOH prediction and ultimately promotes safer surgical care. Our code is available at https://github.com/Mingyue-Cheng/HMF.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Criticality and Safety Margins for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2409.18289</link>
<guid>https://arxiv.org/abs/2409.18289</guid>
<content:encoded><![CDATA[
arXiv:2409.18289v2 Announce Type: replace 
Abstract: State of the art reinforcement learning methods sometimes encounter unsafe situations. Identifying when these situations occur is of interest both for post-hoc analysis and during deployment, where it might be advantageous to call out to a human overseer for help. Efforts to gauge the criticality of different points in time have been developed, but their accuracy is not well established due to a lack of ground truth, and they are not designed to be easily interpretable by end users. Therefore, we seek to define a criticality framework with both a quantifiable ground truth and a clear significance to users. We introduce true criticality as the expected drop in reward when an agent deviates from its policy for n consecutive random actions. We also introduce the concept of proxy criticality, a low-overhead metric that has a statistically monotonic relationship to true criticality. Safety margins make these interpretable, when defined as the number of random actions for which performance loss will not exceed some tolerance with high confidence. We demonstrate this approach in several environment-agent combinations; for an A3C agent in an Atari Beamrider environment, the lowest 5% of safety margins contain 47% of agent losses; i.e., supervising only 5% of decisions could potentially prevent roughly half of an agent's errors. This criticality framework measures the potential impacts of bad decisions, even before those decisions are made, allowing for more effective debugging and oversight of autonomous agents.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking GNN Expressive Power from a Distributed Computational Model Perspective</title>
<link>https://arxiv.org/abs/2410.01308</link>
<guid>https://arxiv.org/abs/2410.01308</guid>
<content:encoded><![CDATA[
arXiv:2410.01308v3 Announce Type: replace 
Abstract: The success of graph neural networks (GNNs) has motivated theoretical studies on their expressive power, often through alignments with the Weisfeiler-Lehman (WL) tests. However, such analyses typically focus on the ability of GNNs to distinguish between graph structures, rather than to compute or approximate specific function classes. The latter is more commonly studied in machine learning theory, including results such as the Turing completeness of recurrent networks and the universal approximation property of feedforward networks. We argue that using well-defined computational models, such as a modified CONGEST model with clearly specified preprocessing and postprocessing, offers a more sound framework for analyzing GNN expressiveness. Within this framework, we show that allowing unrestricted preprocessing or incorporating externally computed features, while claiming that these precomputations enhance the expressiveness, can sometimes lead to problems. We also show that the lower bound on a GNN's capacity (depth multiplied by width) to simulate one iteration of the WL test actually grows nearly linearly with graph size, indicating that the WL test is not locally computable and is misaligned with message-passing GNNs. Despite these negative results, we also present positive results that characterize the effects of virtual nodes and edges from a computational model perspective. Finally, we highlight several open problems regarding GNN expressiveness for further exploration.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diss-l-ECT: Dissecting Graph Data with Local Euler Characteristic Transforms</title>
<link>https://arxiv.org/abs/2410.02622</link>
<guid>https://arxiv.org/abs/2410.02622</guid>
<content:encoded><![CDATA[
arXiv:2410.02622v2 Announce Type: replace 
Abstract: The Euler Characteristic Transform (ECT) is an efficiently-computable geometrical-topological invariant that characterizes the global shape of data. In this paper, we introduce the Local Euler Characteristic Transform ($\ell$-ECT), a novel extension of the ECT particularly designed to enhance expressivity and interpretability in graph representation learning. Unlike traditional Graph Neural Networks (GNNs), which may lose critical local details through aggregation, the $\ell$-ECT provides a lossless representation of local neighborhoods. This approach addresses key limitations in GNNs by preserving nuanced local structures while maintaining global interpretability. Moreover, we construct a rotation-invariant metric based on $\ell$-ECTs for spatial alignment of data spaces. Our method exhibits superior performance compared to standard GNNs on a variety of node-classification tasks, while also offering theoretical guarantees that demonstrate its effectiveness.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Limitations of Mamba in COPY and CoT Reasoning</title>
<link>https://arxiv.org/abs/2410.03810</link>
<guid>https://arxiv.org/abs/2410.03810</guid>
<content:encoded><![CDATA[
arXiv:2410.03810v2 Announce Type: replace 
Abstract: Transformers have become the backbone of modern Large Language Models (LLMs); however, their inference overhead grows linearly with the sequence length, posing challenges for modeling long sequences. In light of this, Mamba has attracted attention for maintaining a constant inference size, with empirical evidence demonstrating that it can match Transformer performance in sequence modeling while significantly reducing computational costs. However, an open question remains: can Mamba always bring savings while achieving performance comparable to Transformers? In this paper, we focus on analyzing the expressive ability of Mamba to perform our defined COPY operation and Chain of Thought (CoT) reasoning. First, inspired by the connection between Mamba and linear attention, we show that constant-sized Mamba may struggle to perform COPY operations while Transformers can handle them more easily. However, when the size of Mamba grows linearly with the input sequence length, it can accurately perform COPY, but in this case, Mamba no longer provides overhead savings. Based on this observation, we further analyze Mamba's ability to tackle CoT tasks, which can be described by the Dynamic Programming (DP) problems. Our findings suggest that to solve arbitrary DP problems, the total cost of Mamba is still comparable to standard Transformers. However, similar to efficient Transformers, when facing DP problems with favorable properties such as locality, Mamba can provide savings in overhead. Our experiments on the copy and CoT tasks further demonstrate Mamba's limitations compared to Transformers in learning these tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding Model Ensemble in Transferable Adversarial Attack</title>
<link>https://arxiv.org/abs/2410.06851</link>
<guid>https://arxiv.org/abs/2410.06851</guid>
<content:encoded><![CDATA[
arXiv:2410.06851v3 Announce Type: replace 
Abstract: Model ensemble adversarial attack has become a powerful method for generating transferable adversarial examples that can target even unknown models, but its theoretical foundation remains underexplored. To address this gap, we provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack. We first define transferability error to measure the error in adversarial transferability, alongside concepts of diversity and empirical model ensemble Rademacher complexity. We then decompose the transferability error into vulnerability, diversity, and a constant, which rigidly explains the origin of transferability error in model ensemble attack: the vulnerability of an adversarial example to ensemble components, and the diversity of ensemble components. Furthermore, we apply the latest mathematical tools in information theory to bound the transferability error using complexity and generalization terms, contributing to three practical guidelines for reducing transferability error: (1) incorporating more surrogate models, (2) increasing their diversity, and (3) reducing their complexity in cases of overfitting. Finally, extensive experiments with 54 models validate our theoretical framework, representing a significant step forward in understanding transferable model ensemble adversarial attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EventFlow: Forecasting Temporal Point Processes with Flow Matching</title>
<link>https://arxiv.org/abs/2410.07430</link>
<guid>https://arxiv.org/abs/2410.07430</guid>
<content:encoded><![CDATA[
arXiv:2410.07430v2 Announce Type: replace 
Abstract: Continuous-time event sequences, in which events occur at irregular intervals, are ubiquitous across a wide range of industrial and scientific domains. The contemporary modeling paradigm is to treat such data as realizations of a temporal point process, and in machine learning it is common to model temporal point processes in an autoregressive fashion using a neural network. While autoregressive models are successful in predicting the time of a single subsequent event, their performance can degrade when forecasting longer horizons due to cascading errors and myopic predictions. We propose EventFlow, a non-autoregressive generative model for temporal point processes. The model builds on the flow matching framework in order to directly learn joint distributions over event times, side-stepping the autoregressive process. EventFlow is simple to implement and achieves a 20%-53% lower error than the nearest baseline on standard TPP benchmarks while simultaneously using fewer model calls at sampling time.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NRFormer: Nationwide Nuclear Radiation Forecasting with Spatio-Temporal Transformer</title>
<link>https://arxiv.org/abs/2410.11924</link>
<guid>https://arxiv.org/abs/2410.11924</guid>
<content:encoded><![CDATA[
arXiv:2410.11924v3 Announce Type: replace 
Abstract: Nuclear radiation, which refers to the energy emitted from atomic nuclei during decay, poses significant risks to human health and environmental safety. Recently, advancements in monitoring technology have facilitated the effective recording of nuclear radiation levels and related factors, such as weather conditions. The abundance of monitoring data enables the development of accurate and reliable nuclear radiation forecasting models, which play a crucial role in informing decision-making for individuals and governments. However, this task is challenging due to the imbalanced distribution of monitoring stations over a wide spatial range and the non-stationary radiation variation patterns. In this study, we introduce NRFormer, a novel framework tailored for the nationwide prediction of nuclear radiation variations. By integrating a non-stationary temporal attention module, an imbalance-aware spatial attention module, and a radiation propagation prompting module, NRFormer collectively captures complex spatio-temporal dynamics of nuclear radiation. Extensive experiments on two real-world datasets demonstrate the superiority of our proposed framework against 11 baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Weight Diffusion: Generating reactive policies instead of trajectories</title>
<link>https://arxiv.org/abs/2410.14040</link>
<guid>https://arxiv.org/abs/2410.14040</guid>
<content:encoded><![CDATA[
arXiv:2410.14040v2 Announce Type: replace 
Abstract: With the increasing availability of open-source robotic data, imitation learning has emerged as a viable approach for both robot manipulation and locomotion. Currently, large generalized policies are trained to predict controls or trajectories using diffusion models, which have the desirable property of learning multimodal action distributions. However, generalizability comes with a cost, namely, larger model size and slower inference. This is especially an issue for robotic tasks that require high control frequency. Further, there is a known trade-off between performance and action horizon for Diffusion Policy (DP), a popular model for generating trajectories: fewer diffusion queries accumulate greater trajectory tracking errors. For these reasons, it is common practice to run these models at high inference frequency, subject to robot computational constraints. To address these limitations, we propose Latent Weight Diffusion (LWD), a method that uses diffusion to generate closed-loop policies (weights for neural policies) for robotic tasks, rather than generating trajectories. Learning the behavior distribution through parameter space over trajectory space offers two key advantages: longer action horizons (fewer diffusion queries) & robustness to perturbations while retaining high performance; and a lower inference compute cost. To this end, we show that LWD has higher success rates than DP when the action horizon is longer and when stochastic perturbations exist in the environment. Furthermore, LWD achieves multitask performance comparable to DP while requiring just ~1/45th of the inference-time FLOPS
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embedding Safety into RL: A New Take on Trust Region Methods</title>
<link>https://arxiv.org/abs/2411.02957</link>
<guid>https://arxiv.org/abs/2411.02957</guid>
<content:encoded><![CDATA[
arXiv:2411.02957v3 Announce Type: replace 
Abstract: Reinforcement Learning (RL) agents can solve diverse tasks but often exhibit unsafe behavior. Constrained Markov Decision Processes (CMDPs) address this by enforcing safety constraints, yet existing methods either sacrifice reward maximization or allow unsafe training. We introduce Constrained Trust Region Policy Optimization (C-TRPO), which reshapes the policy space geometry to ensure trust regions contain only safe policies, guaranteeing constraint satisfaction throughout training. We analyze its theoretical properties and connections to TRPO, Natural Policy Gradient (NPG), and Constrained Policy Optimization (CPO). Experiments show that C-TRPO reduces constraint violations while maintaining competitive returns.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Coarse-to-fine Q-Network with Action Sequence for Data-Efficient Robot Learning</title>
<link>https://arxiv.org/abs/2411.12155</link>
<guid>https://arxiv.org/abs/2411.12155</guid>
<content:encoded><![CDATA[
arXiv:2411.12155v4 Announce Type: replace 
Abstract: Predicting a sequence of actions has been crucial in the success of recent behavior cloning algorithms in robotics. Can similar ideas improve reinforcement learning (RL)? We answer affirmatively by observing that incorporating action sequences when predicting ground-truth return-to-go leads to lower validation loss. Motivated by this, we introduce Coarse-to-fine Q-Network with Action Sequence (CQN-AS), a novel value-based RL algorithm that learns a critic network that outputs Q-values over a sequence of actions, i.e., explicitly training the value function to learn the consequence of executing action sequences. Our experiments show that CQN-AS outperforms several baselines on a variety of sparse-reward humanoid control and tabletop manipulation tasks from BiGym and RLBench.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Natural Language Reinforcement Learning</title>
<link>https://arxiv.org/abs/2411.14251</link>
<guid>https://arxiv.org/abs/2411.14251</guid>
<content:encoded><![CDATA[
arXiv:2411.14251v3 Announce Type: replace 
Abstract: Artificial intelligence progresses towards the "Era of Experience," where agents are expected to learn from continuous, grounded interaction. We argue that traditional Reinforcement Learning (RL), which typically represents value as a scalar, can restrict agent's deep understanding of environments and hinders the active, deliberative learning crucial for navigating this new paradigm. To address the issue, we introduce Natural Language Reinforcement Learning (NLRL), a framework that extends RL principles into natural language counterparts. Central to NLRL is the Language Value Function (LVF), which redefines value as an interpretable linguistic narrative articulating the rationale behind an evaluation. NLRL further extends this concept to core RL components, including policy, the Bellman equation, and policy iteration. Leveraging recent advancements in Large Language Models (LLMs), NLRL can be practically implemented to achieve RL-like policy and value training through unsupervised environment interactions. Experiments over 4 multi-step agentic tasks demonstrate NLRL's effectiveness, efficiency, and its potential to foster deeper understanding and more active learning strategies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AutoElicit: Using Large Language Models for Expert Prior Elicitation in Predictive Modelling</title>
<link>https://arxiv.org/abs/2411.17284</link>
<guid>https://arxiv.org/abs/2411.17284</guid>
<content:encoded><![CDATA[
arXiv:2411.17284v5 Announce Type: replace 
Abstract: Large language models (LLMs) acquire a breadth of information across various domains. However, their computational complexity, cost, and lack of transparency often hinder their direct application for predictive tasks where privacy and interpretability are paramount. In fields such as healthcare, biology, and finance, specialised and interpretable linear models still hold considerable value. In such domains, labelled data may be scarce or expensive to obtain. Well-specified prior distributions over model parameters can reduce the sample complexity of learning through Bayesian inference; however, eliciting expert priors can be time-consuming. We therefore introduce AutoElicit to extract knowledge from LLMs and construct priors for predictive models. We show these priors are informative and can be refined using natural language. We perform a careful study contrasting AutoElicit with in-context learning and demonstrate how to perform model selection between the two methods. We find that AutoElicit yields priors that can substantially reduce error over uninformative priors, using fewer labels, and consistently outperform in-context learning. We show that AutoElicit saves over 6 months of labelling effort when building a new predictive model for urinary tract infections from sensor recordings of people living with dementia.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Label Bayesian Active Learning with Inter-Label Relationships</title>
<link>https://arxiv.org/abs/2411.17941</link>
<guid>https://arxiv.org/abs/2411.17941</guid>
<content:encoded><![CDATA[
arXiv:2411.17941v2 Announce Type: replace 
Abstract: The primary challenge of multi-label active learning, differing it from multi-class active learning, lies in assessing the informativeness of an indefinite number of labels while also accounting for the inherited label correlation. Existing studies either require substantial computational resources to leverage correlations or fail to fully explore label dependencies. Additionally, real-world scenarios often require addressing intrinsic biases stemming from imbalanced data distributions. In this paper, we propose a new multi-label active learning strategy to address both challenges. Our method incorporates progressively updated positive and negative correlation matrices to capture co-occurrence and disjoint relationships within the label space of annotated samples, enabling a holistic assessment of uncertainty rather than treating labels as isolated elements. Furthermore, alongside diversity, our model employs ensemble pseudo labeling and beta scoring rules to address data imbalances. Extensive experiments on four realistic datasets demonstrate that our strategy consistently achieves more reliable and superior performance, compared to several established methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Continual Graph Learning</title>
<link>https://arxiv.org/abs/2411.18919</link>
<guid>https://arxiv.org/abs/2411.18919</guid>
<content:encoded><![CDATA[
arXiv:2411.18919v3 Announce Type: replace 
Abstract: Managing evolving graph data presents substantial challenges in storage and privacy, and training graph neural networks (GNNs) on such data often leads to catastrophic forgetting, impairing performance on earlier tasks. Despite existing continual graph learning (CGL) methods mitigating this to some extent, they rely on centralized architectures and ignore the potential of distributed graph databases to leverage collective intelligence. To this end, we propose Federated Continual Graph Learning (FCGL) to adapt GNNs across multiple evolving graphs under storage and privacy constraints. Our empirical study highlights two core challenges: local graph forgetting (LGF), where clients lose prior knowledge when adapting to new tasks, and global expertise conflict (GEC), where the global GNN exhibits sub-optimal performance in both adapting to new tasks and retaining old ones, arising from inconsistent client expertise during server-side parameter aggregation. To address these, we introduce POWER, a framework that preserves experience nodes with maximum local-global coverage locally to mitigate LGF, and leverages pseudo-prototype reconstruction with trajectory-aware knowledge transfer to resolve GEC. Experiments on various graph datasets demonstrate POWER's superiority over federated adaptations of CGL baselines and vision-centric federated continual learning approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Go With the Flow: Fast Diffusion for Gaussian Mixture Models</title>
<link>https://arxiv.org/abs/2412.09059</link>
<guid>https://arxiv.org/abs/2412.09059</guid>
<content:encoded><![CDATA[
arXiv:2412.09059v4 Announce Type: replace 
Abstract: Schrodinger Bridges (SBs) are diffusion processes that steer, in finite time, a given initial distribution to another final one while minimizing a suitable cost functional. Although various methods for computing SBs have recently been proposed in the literature, most of these approaches require computationally expensive training schemes, even for solving low-dimensional problems. In this work, we propose an analytic parametrization of a set of feasible policies for steering the distribution of a dynamical system from one Gaussian Mixture Model (GMM) to another. Instead of relying on standard non-convex optimization techniques, the optimal policy within the set can be approximated as the solution of a low-dimensional linear program whose dimension scales linearly with the number of components in each mixture. The proposed method generalizes naturally to more general classes of dynamical systems, such as controllable linear time-varying systems, enabling efficient solutions to multi-marginal momentum SB between GMMs, a challenging distribution interpolation problem. We showcase the potential of this approach in low-to-moderate dimensional problems such as image-to-image translation in the latent space of an autoencoder, learning of cellular dynamics using multi-marginal momentum SB problems, and various other examples. We also test our approach on an Entropic Optimal Transport (EOT) benchmark problem and show that it outperforms state-of-the-art methods in cases where the boundary distributions are mixture models while requiring virtually no training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Guidance Mechanisms for Discrete Diffusion Models</title>
<link>https://arxiv.org/abs/2412.10193</link>
<guid>https://arxiv.org/abs/2412.10193</guid>
<content:encoded><![CDATA[
arXiv:2412.10193v3 Announce Type: replace 
Abstract: Diffusion models for continuous data gained widespread adoption owing to their high quality generation and control mechanisms. However, controllable diffusion on discrete data faces challenges given that continuous guidance methods do not directly apply to discrete diffusion. Here, we provide a straightforward derivation of classifier-free and classifier-based guidance for discrete diffusion, as well as a new class of diffusion models that leverage uniform noise and that are more guidable because they can continuously edit their outputs. We improve the quality of these models with a novel continuous-time variational lower bound that yields state-of-the-art performance, especially in settings involving guidance or fast generation. Empirically, we demonstrate that our guidance mechanisms combined with uniform noise diffusion improve controllable generation relative to autoregressive and diffusion baselines on several discrete data domains, including genomic sequences, small molecule design, and discretized image generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GNNs-to-MLPs by Teacher Injection and Dirichlet Energy Distillation</title>
<link>https://arxiv.org/abs/2412.11180</link>
<guid>https://arxiv.org/abs/2412.11180</guid>
<content:encoded><![CDATA[
arXiv:2412.11180v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) are pivotal in graph-based learning, particularly excelling in node classification. However, their scalability is hindered by the need for multi-hop data during inference, limiting their application in latency-sensitive scenarios. Recent efforts to distill GNNs into multi-layer perceptrons (MLPs) for faster inference often underutilize the layer-level insights of GNNs. In this paper, we present TINED, a novel approach that distills GNNs to MLPs on a layer-by-layer basis using Teacher Injection and Dirichlet Energy Distillation techniques. We focus on two key operations in GNN layers: feature transformation (FT) and graph propagation (GP). We recognize that FT is computationally equivalent to a fully-connected (FC) layer in MLPs. Thus, we propose directly transferring teacher parameters from an FT in a GNN to an FC layer in the student MLP, enhanced by fine-tuning. In TINED, the FC layers in an MLP replicate the sequence of FTs and GPs in the GNN. We also establish a theoretical bound for GP approximation. Furthermore, we note that FT and GP operations in GNN layers often exhibit opposing smoothing effects: GP is aggressive, while FT is conservative. Using Dirichlet energy, we develop a DE ratio to measure these effects and propose Dirichlet Energy Distillation to convey these characteristics from GNN layers to MLP layers. Extensive experiments show that TINED outperforms GNNs and leading distillation methods across various settings and seven datasets. Source code are available at https://github.com/scottjiao/TINED_ICML25/.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constraint-Adaptive Policy Switching for Offline Safe Reinforcement Learning</title>
<link>https://arxiv.org/abs/2412.18946</link>
<guid>https://arxiv.org/abs/2412.18946</guid>
<content:encoded><![CDATA[
arXiv:2412.18946v2 Announce Type: replace 
Abstract: Offline safe reinforcement learning (OSRL) involves learning a decision-making policy to maximize rewards from a fixed batch of training data to satisfy pre-defined safety constraints. However, adapting to varying safety constraints during deployment without retraining remains an under-explored challenge. To address this challenge, we introduce constraint-adaptive policy switching (CAPS), a wrapper framework around existing offline RL algorithms. During training, CAPS uses offline data to learn multiple policies with a shared representation that optimize different reward and cost trade-offs. During testing, CAPS switches between those policies by selecting at each state the policy that maximizes future rewards among those that satisfy the current cost constraint. Our experiments on 38 tasks from the DSRL benchmark demonstrate that CAPS consistently outperforms existing methods, establishing a strong wrapper-based baseline for OSRL. The code is publicly available at https://github.com/yassineCh/CAPS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Innovative Data-Driven and Adaptive Reinforcement Learning Approach for Context-Aware Prescriptive Process Monitoring</title>
<link>https://arxiv.org/abs/2501.10543</link>
<guid>https://arxiv.org/abs/2501.10543</guid>
<content:encoded><![CDATA[
arXiv:2501.10543v2 Announce Type: replace 
Abstract: The application of artificial intelligence and machine learning in business process management has advanced significantly, however, the full potential of these technologies remains largely unexplored, primarily due to challenges related to data quality and availability. We present a novel framework called Fine-Tuned Offline Reinforcement Learning Augmented Process Sequence Optimization (FORLAPS), which aims to identify optimal execution paths in business processes by leveraging reinforcement learning enhanced with a state-dependent reward shaping mechanism, thereby enabling context-sensitive prescriptions. Additionally, to compare FORLAPS with the existing models (Permutation Feature Importance and multi-task Long Short Term Memory model), we experimented to evaluate its effectiveness in terms of resource savings and process time reduction. The experimental results on real-life event logs validate that FORLAPS achieves 31% savings in resource time spent and a 23% reduction in process time span. To further enhance learning, we introduce an innovative process-aware data augmentation technique that selectively increases the average estimated Q-values in sampled batches, enabling automatic fine-tuning of the reinforcement learning model. Robustness was assessed through both prefix-level and trace-level evaluations, using the Damerau-Levenshtein distance as the primary metric. Finally, the model's adaptability across industries was further validated through diverse case studies, including healthcare treatment pathways, financial services workflows, permit applications from regulatory bodies, and operations management. In each domain, the proposed model demonstrated exceptional performance, outperforming existing state-of-the-art approaches in prescriptive decision-making, demonstrating its capability to prescribe optimal next steps and predict the best next activities within a process trace.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Logit-based Knowledge Distillation of Deep Spiking Neural Networks for Full-Range Timestep Deployment</title>
<link>https://arxiv.org/abs/2501.15925</link>
<guid>https://arxiv.org/abs/2501.15925</guid>
<content:encoded><![CDATA[
arXiv:2501.15925v2 Announce Type: replace 
Abstract: Spiking Neural Networks (SNNs) are emerging as a brain-inspired alternative to traditional Artificial Neural Networks (ANNs), prized for their potential energy efficiency on neuromorphic hardware. Despite this, SNNs often suffer from accuracy degradation compared to ANNs and face deployment challenges due to fixed inference timesteps, which require retraining for adjustments, limiting operational flexibility. To address these issues, our work considers the spatio-temporal property inherent in SNNs, and proposes a novel distillation framework for deep SNNs that optimizes performance across full-range timesteps without specific retraining, enhancing both efficacy and deployment adaptability. We provide both theoretical analysis and empirical validations to illustrate that training guarantees the convergence of all implicit models across full-range timesteps. Experimental results on CIFAR-10, CIFAR-100, CIFAR10-DVS, and ImageNet demonstrate state-of-the-art performance among distillation-based SNNs training methods. Our code is available at https://github.com/Intelli-Chip-Lab/snn\_temporal\_decoupling\_distillation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Dynamics of In-Context Learning in Linear Attention</title>
<link>https://arxiv.org/abs/2501.16265</link>
<guid>https://arxiv.org/abs/2501.16265</guid>
<content:encoded><![CDATA[
arXiv:2501.16265v2 Announce Type: replace 
Abstract: While attention-based models have demonstrated the remarkable ability of in-context learning (ICL), the theoretical understanding of how these models acquired this ability through gradient descent training is still preliminary. Towards answering this question, we study the gradient descent dynamics of multi-head linear self-attention trained for in-context linear regression. We examine two parametrizations of linear self-attention: one with the key and query weights merged as a single matrix (common in theoretical studies), and one with separate key and query matrices (closer to practical settings). For the merged parametrization, we show that the training dynamics has two fixed points and the loss trajectory exhibits a single, abrupt drop. We derive an analytical time-course solution for a certain class of datasets and initialization. For the separate parametrization, we show that the training dynamics has exponentially many fixed points and the loss exhibits saddle-to-saddle dynamics, which we reduce to scalar ordinary differential equations. During training, the model implements principal component regression in context with the number of principal components increasing over training time. Overall, we provide a theoretical description of how ICL abilities evolve during gradient descent training of linear attention, revealing abrupt acquisition or progressive improvements depending on how the key and query are parametrized.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Informed Diffusion Transformer for Long-Tail Trajectory Prediction in the Crash Scenario</title>
<link>https://arxiv.org/abs/2501.16349</link>
<guid>https://arxiv.org/abs/2501.16349</guid>
<content:encoded><![CDATA[
arXiv:2501.16349v2 Announce Type: replace 
Abstract: Trajectory prediction methods have been widely applied in autonomous driving technologies. Although the overall performance accuracy of trajectory prediction is relatively high, the lack of trajectory data in critical scenarios in the training data leads to the long-tail phenomenon. Normally, the trajectories of the tail data are more critical and more difficult to predict and may include rare scenarios such as crashes. To solve this problem, we extracted the trajectory data from real-world crash scenarios, which contain more long-tail data. Meanwhile, based on the trajectory data in this scenario, we integrated graph-based risk information and diffusion with transformer and proposed the Risk-Informed Diffusion Transformer (RI-DiT) trajectory prediction method. Extensive experiments were conducted on trajectory data in the real-world crash scenario, and the results show that the algorithm we proposed has good performance. When predicting the data of the tail 10\% (Top 10\%), the minADE and minFDE indicators are 0.016/2.667 m. At the same time, we showed the trajectory conditions of different long-tail distributions. The distribution of trajectory data is closer to the tail, the less smooth the trajectory is. Through the trajectory data in real-world crash scenarios, Our work expands the methods to overcome the long-tail challenges in trajectory prediction. Our method, RI-DiT, integrates inverse time to collision (ITTC) and the feature of traffic flow, which can predict long-tail trajectories more accurately and improve the safety of autonomous driving systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Variational Perspective on Generative Protein Fitness Optimization</title>
<link>https://arxiv.org/abs/2501.19200</link>
<guid>https://arxiv.org/abs/2501.19200</guid>
<content:encoded><![CDATA[
arXiv:2501.19200v2 Announce Type: replace 
Abstract: The goal of protein fitness optimization is to discover new protein variants with enhanced fitness for a given use. The vast search space and the sparsely populated fitness landscape, along with the discrete nature of protein sequences, pose significant challenges when trying to determine the gradient towards configurations with higher fitness. We introduce Variational Latent Generative Protein Optimization (VLGPO), a variational perspective on fitness optimization. Our method embeds protein sequences in a continuous latent space to enable efficient sampling from the fitness distribution and combines a (learned) flow matching prior over sequence mutations with a fitness predictor to guide optimization towards sequences with high fitness. VLGPO achieves state-of-the-art results on two different protein benchmarks of varying complexity. Moreover, the variational design with explicit prior and likelihood functions offers a flexible plug-and-play framework that can be easily customized to suit various protein design tasks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PUATE: Efficient Average Treatment Effect Estimation from Treated (Positive) and Unlabeled Units</title>
<link>https://arxiv.org/abs/2501.19345</link>
<guid>https://arxiv.org/abs/2501.19345</guid>
<content:encoded><![CDATA[
arXiv:2501.19345v2 Announce Type: replace 
Abstract: The estimation of average treatment effects (ATEs), defined as the difference in expected outcomes between treatment and control groups, is a central topic in causal inference. This study develops semiparametric efficient estimators for ATE in a setting where only a treatment group and an unlabeled group, consisting of units whose treatment status is unknown, are observed. This scenario constitutes a variant of learning from positive and unlabeled data (PU learning) and can be viewed as a special case of ATE estimation with missing data. For this setting, we derive the semiparametric efficiency bounds, which characterize the lowest achievable asymptotic variance for regular estimators. We then construct semiparametric efficient ATE estimators that attain these bounds. Our results contribute to the literature on causal inference with missing data and weakly supervised learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Online Reinforcement Learning for Diffusion Policy</title>
<link>https://arxiv.org/abs/2502.00361</link>
<guid>https://arxiv.org/abs/2502.00361</guid>
<content:encoded><![CDATA[
arXiv:2502.00361v3 Announce Type: replace 
Abstract: Diffusion policies have achieved superior performance in imitation learning and offline reinforcement learning (RL) due to their rich expressiveness. However, the conventional diffusion training procedure requires samples from target distribution, which is impossible in online RL since we cannot sample from the optimal policy. Backpropagating policy gradient through the diffusion process incurs huge computational costs and instability, thus being expensive and not scalable. To enable efficient training of diffusion policies in online RL, we generalize the conventional denoising score matching by reweighting the loss function. The resulting Reweighted Score Matching (RSM) preserves the optimal solution and low computational cost of denoising score matching, while eliminating the need to sample from the target distribution and allowing learning to optimize value functions. We introduce two tractable reweighted loss functions to solve two commonly used policy optimization problems, policy mirror descent and max-entropy policy, resulting in two practical algorithms named Diffusion Policy Mirror Descent (DPMD) and Soft Diffusion Actor-Critic (SDAC). We conducted comprehensive comparisons on MuJoCo benchmarks. The empirical results show that the proposed algorithms outperform recent diffusion-policy online RLs on most tasks, and the DPMD improves more than 120% over soft actor-critic on Humanoid and Ant.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message-Passing GNNs Fail to Approximate Sparse Triangular Factorizations</title>
<link>https://arxiv.org/abs/2502.01397</link>
<guid>https://arxiv.org/abs/2502.01397</guid>
<content:encoded><![CDATA[
arXiv:2502.01397v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have been proposed as a tool for learning sparse matrix preconditioners, which are key components in accelerating linear solvers. This position paper argues that message-passing GNNs are fundamentally incapable of approximating sparse triangular factorizations. We demonstrate that message-passing GNNs fundamentally fail to approximate sparse triangular factorizations for classes of matrices for which high-quality preconditioners exist but require non-local dependencies. To illustrate this, we construct a set of baselines using both synthetic matrices and real-world examples from the SuiteSparse collection. Across a range of GNN architectures, including Graph Attention Networks and Graph Transformers, we observe severe performance degradation compared to exact or K-optimal factorizations, with cosine similarity dropping below $0.6$ in key cases. Our theoretical and empirical results suggest that architectural innovations beyond message-passing are necessary for applying GNNs to scientific computing tasks such as matrix factorization. Experiments demonstrate that overcoming non-locality alone is insufficient. Tailored architectures are necessary to capture the required dependencies since even a completely non-local Graph Transformer fails to match the proposed baselines.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Memento No More: Coaching AI Agents to Master Multiple Tasks via Hints Internalization</title>
<link>https://arxiv.org/abs/2502.01562</link>
<guid>https://arxiv.org/abs/2502.01562</guid>
<content:encoded><![CDATA[
arXiv:2502.01562v2 Announce Type: replace 
Abstract: As the general capabilities of artificial intelligence (AI) agents continue to evolve, their ability to learn to master multiple complex tasks through experience remains a key challenge. Current LLM agents, particularly those based on proprietary language models, typically rely on prompts to incorporate knowledge about the target tasks. This approach does not allow the agent to internalize this information and instead relies on ever-expanding prompts to sustain its functionality in diverse scenarios. This resembles a system of notes used by a person affected by anterograde amnesia, the inability to form new memories. In this paper, we propose a novel method to train AI agents to incorporate knowledge and skills for multiple tasks without the need for either cumbersome note systems or prior high-quality demonstration data. Our approach employs an iterative process where the agent collects new experiences, receives corrective feedback from humans in the form of hints, and integrates this feedback into its weights via a context distillation training procedure. We demonstrate the efficacy of our approach by implementing it in a Llama-3-based agent that, after only a few rounds of feedback, outperforms advanced models GPT-4o and DeepSeek-V3 in tasksets requiring correct sequencing of information retrieval, tool use, and question answering.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving Rule-based Reasoning in LLMs using Neurosymbolic Representations</title>
<link>https://arxiv.org/abs/2502.01657</link>
<guid>https://arxiv.org/abs/2502.01657</guid>
<content:encoded><![CDATA[
arXiv:2502.01657v3 Announce Type: replace 
Abstract: Large language models (LLMs) continue to face challenges in reliably solving reasoning tasks, particularly those that require precise rule following, as often found in mathematical reasoning. This paper introduces a novel neurosymbolic method that improves LLM reasoning by encoding hidden states into neurosymbolic vectors, enabling problem-solving within a neurosymbolic vector space. The results are decoded and merged with the original hidden state, significantly boosting the model's performance on numerical reasoning tasks. By offloading computation through neurosymbolic representations, this method enhances efficiency, reliability, and interpretability. Experimental results demonstrate an average of 88.6% lower cross-entropy loss and 15.4 times more problems correctly solved on a suite of mathematical reasoning tasks compared to chain-of-thought prompting and supervised fine-tuning (LoRA), without degrading performance on other tasks. We make our code available at: https://github.com/vdhanraj/Neurosymbolic-LLM.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust LLM Alignment via Distributionally Robust Direct Preference Optimization</title>
<link>https://arxiv.org/abs/2502.01930</link>
<guid>https://arxiv.org/abs/2502.01930</guid>
<content:encoded><![CDATA[
arXiv:2502.01930v2 Announce Type: replace 
Abstract: A major challenge in aligning large language models (LLMs) with human preferences is the issue of distribution shift. LLM alignment algorithms rely on static preference datasets, assuming that they accurately represent real-world user preferences. However, user preferences vary significantly across geographical regions, demographics, linguistic patterns, and evolving cultural trends. This preference distribution shift leads to catastrophic alignment failures in many real-world applications. We address this problem using the principled framework of distributionally robust optimization, and develop two novel distributionally robust direct preference optimization (DPO) algorithms, namely, Wasserstein DPO (WDPO) and Kullback-Leibler DPO (KLDPO). We characterize the sample complexity of learning the optimal policy parameters for WDPO and KLDPO. Moreover, we propose scalable gradient descent-style learning algorithms by developing suitable approximations for the challenging minimax loss functions of WDPO and KLDPO. Our empirical experiments using benchmark data sets and LLMs demonstrate the superior performance of WDPO and KLDPO in substantially improving the alignment when there is a preference distribution shift.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BILBO: BILevel Bayesian Optimization</title>
<link>https://arxiv.org/abs/2502.02121</link>
<guid>https://arxiv.org/abs/2502.02121</guid>
<content:encoded><![CDATA[
arXiv:2502.02121v2 Announce Type: replace 
Abstract: Bilevel optimization is characterized by a two-level optimization structure, where the upper-level problem is constrained by optimal lower-level solutions, and such structures are prevalent in real-world problems. The constraint by optimal lower-level solutions poses significant challenges, especially in noisy, constrained, and derivative-free settings, as repeating lower-level optimizations is sample inefficient and predicted lower-level solutions may be suboptimal. We present BILevel Bayesian Optimization (BILBO), a novel Bayesian optimization algorithm for general bilevel problems with blackbox functions, which optimizes both upper- and lower-level problems simultaneously, without the repeated lower-level optimization required by existing methods. BILBO samples from confidence-bounds based trusted sets, which bounds the suboptimality on the lower level. Moreover, BILBO selects only one function query per iteration, where the function query selection strategy incorporates the uncertainty of estimated lower-level solutions and includes a conditional reassignment of the query to encourage exploration of the lower-level objective. The performance of BILBO is theoretically guaranteed with a sublinear regret bound for commonly used kernels and is empirically evaluated on several synthetic and real-world problems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGNet: Reciprocal Space-Aware Long-Range Modeling for Crystalline Property Prediction</title>
<link>https://arxiv.org/abs/2502.02748</link>
<guid>https://arxiv.org/abs/2502.02748</guid>
<content:encoded><![CDATA[
arXiv:2502.02748v2 Announce Type: replace 
Abstract: Predicting properties of crystals from their structures is a fundamental yet challenging task in materials science. Unlike molecules, crystal structures exhibit infinite periodic arrangements of atoms, requiring methods capable of capturing both local and global information effectively. However, most current works fall short of capturing long-range interactions within periodic structures. To address this limitation, we leverage \emph{reciprocal space} to efficiently encode long-range interactions with learnable filters within Fourier transforms. We introduce Reciprocal Geometry Network (ReGNet), a novel architecture that integrates geometric GNNs and reciprocal blocks to model short-range and long-range interactions, respectively. Experimental results on JARVIS, Materials Project, and MatBench demonstrate that ReGNet achieves state-of-the-art predictive accuracy across a range of crystal property prediction tasks. Additionally, we explore a model extension that employs the mixture-of-experts for multi-property prediction with promising results and high computational efficiency. These findings highlight the potential of our model as a scalable and accurate solution for crystal property prediction. The code will be released upon paper acceptance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Reward Alignment via Hypothesis Space Batch Cutting</title>
<link>https://arxiv.org/abs/2502.02921</link>
<guid>https://arxiv.org/abs/2502.02921</guid>
<content:encoded><![CDATA[
arXiv:2502.02921v3 Announce Type: replace 
Abstract: Reward design in reinforcement learning and optimal control is challenging. Preference-based alignment addresses this by enabling agents to learn rewards from ranked trajectory pairs provided by humans. However, existing methods often struggle from poor robustness to unknown false human preferences. In this work, we propose a robust and efficient reward alignment method based on a novel and geometrically interpretable perspective: hypothesis space batched cutting. Our method iteratively refines the reward hypothesis space through "cuts" based on batches of human preferences. Within each batch, human preferences, queried based on disagreement, are grouped using a voting function to determine the appropriate cut, ensuring a bounded human query complexity. To handle unknown erroneous preferences, we introduce a conservative cutting method within each batch, preventing erroneous human preferences from making overly aggressive cuts to the hypothesis space. This guarantees provable robustness against false preferences, while eliminating the need to explicitly identify them. We evaluate our method in a model predictive control setting across diverse tasks. The results demonstrate that our framework achieves comparable or superior performance to state-of-the-art methods in error-free settings while significantly outperforming existing methods when handling a high percentage of erroneous human preferences.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction of the Most Fire-Sensitive Point in Building Structures with Differentiable Agents for Thermal Simulators</title>
<link>https://arxiv.org/abs/2502.03424</link>
<guid>https://arxiv.org/abs/2502.03424</guid>
<content:encoded><![CDATA[
arXiv:2502.03424v4 Announce Type: replace 
Abstract: Fire safety is crucial for ensuring the stability of building structures, yet evaluating whether a structure meets fire safety requirement is challenging. Fires can originate at any point within a structure, and simulating every potential fire scenario is both expensive and time-consuming. To address this challenge, we propose the concept of the Most Fire-Sensitive Point (MFSP) and an efficient machine learning framework for its identification. The MFSP is defined as the location at which a fire, if initiated, would cause the most severe detrimental impact on the building's stability, effectively representing the worst-case fire scenario. In our framework, a Graph Neural Network (GNN) serves as an efficient and differentiable agent for conventional Finite Element Analysis (FEA) simulators by predicting the Maximum Interstory Drift Ratio (MIDR) under fire, which then guides the training and evaluation of the MFSP predictor. Additionally, we enhance our framework with a novel edge update mechanism and a transfer learning-based training scheme. Evaluations on a large-scale simulation dataset demonstrate the good performance of the proposed framework in identifying the MFSP, offering a transformative tool for optimizing fire safety assessments in structural design. All developed datasets and codes are open-sourced online.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Path Planning for Masked Diffusion Model Sampling</title>
<link>https://arxiv.org/abs/2502.03540</link>
<guid>https://arxiv.org/abs/2502.03540</guid>
<content:encoded><![CDATA[
arXiv:2502.03540v4 Announce Type: replace 
Abstract: Any order generation of discrete data using masked diffusion models (MDMs) offers a compelling alternative to traditional autoregressive models, especially in domains that lack a natural causal ordering of data. However, current popular MDMs depart from their successful continuous diffusion model counterparts with simplified masked inference wherein unmasked tokens cannot be iteratively refined -- even if there is a mistake. In this paper, we extract the full power of MDMs by introducing a novel inference sampling strategy termed Path Planning (P2) that decomposes each generation step into two sub-stages: planning and denoising. Under P2, the planner at every step selects appropriate tokens that are marked to be updated, which can then be sampled using the denoiser. We demonstrate that P2 generalizes all existing sampling strategies for MDMs and critically enhances generative quality through the new capability of refining and updating existing unmasked tokens. We theoretically prove that P2 establishes a (new) expanded evidence lower bound (ELBO) on the log marginal likelihood of data. We instantiate P2 with a family of planners including: 1.) Self-Planning, 2.) BERT-Planning, and 3.) Trained-Planning with a learned planner leading to SOTA generative performance for MDMs on a suite of domains. Specifically, solely using P2 inference, we observe relative improvements of 22% in protein sequence foldability, 8% in RNA sequence pLDDT, 4% in math reasoning, 68% in story generation (ROUGE score), and 33% in code generation for the challenging pass@1 metric.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Devil is in the Details: Density Guidance for Detail-Aware Generation with Flow Models</title>
<link>https://arxiv.org/abs/2502.05807</link>
<guid>https://arxiv.org/abs/2502.05807</guid>
<content:encoded><![CDATA[
arXiv:2502.05807v2 Announce Type: replace 
Abstract: Diffusion models have emerged as a powerful class of generative models, capable of producing high-quality images by mapping noise to a data distribution. However, recent findings suggest that image likelihood does not align with perceptual quality: high-likelihood samples tend to be smooth, while lower-likelihood ones are more detailed. Controlling sample density is thus crucial for balancing realism and detail. In this paper, we analyze an existing technique, Prior Guidance, which scales the latent code to influence image detail. We introduce score alignment, a condition that explains why this method works and show that it can be tractably checked for any continuous normalizing flow model. We then propose Density Guidance, a principled modification of the generative ODE that enables exact log-density control during sampling. Finally, we extend Density Guidance to stochastic sampling, ensuring precise log-density control while allowing controlled variation in structure or fine details. Our experiments demonstrate that these techniques provide fine-grained control over image detail without compromising sample quality. Code is available at https://github.com/Aalto-QuML/density-guidance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model Diffusion for Certifiable Few-shot Transfer Learning</title>
<link>https://arxiv.org/abs/2502.06970</link>
<guid>https://arxiv.org/abs/2502.06970</guid>
<content:encoded><![CDATA[
arXiv:2502.06970v2 Announce Type: replace 
Abstract: In contemporary deep learning, a prevalent and effective workflow for solving low-data problems is adapting powerful pre-trained foundation models (FMs) to new tasks via parameter-efficient fine-tuning (PEFT). However, while empirically effective, the resulting solutions lack generalisation guarantees to certify their accuracy - which may be required for ethical or legal reasons prior to deployment in high-importance applications. In this paper we develop a novel transfer learning approach that is designed to facilitate non-vacuous learning theoretic generalisation guarantees for downstream tasks, even in the low-shot regime. Specifically, we first use upstream tasks to train a distribution over PEFT parameters. We then learn the downstream task by a sample-and-evaluate procedure -- sampling plausible PEFTs from the trained diffusion model and selecting the one with the highest likelihood on the downstream data. Crucially, this confines our model hypothesis to a finite set of PEFT samples. In contrast to the typical continuous hypothesis spaces of neural network weights, this facilitates tighter risk certificates. We instantiate our bound and show non-trivial generalization guarantees compared to existing learning approaches which lead to vacuous bounds in the low-shot regime.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outsourced diffusion sampling: Efficient posterior inference in latent spaces of generative models</title>
<link>https://arxiv.org/abs/2502.06999</link>
<guid>https://arxiv.org/abs/2502.06999</guid>
<content:encoded><![CDATA[
arXiv:2502.06999v2 Announce Type: replace 
Abstract: Any well-behaved generative model over a variable $\mathbf{x}$ can be expressed as a deterministic transformation of an exogenous ('outsourced') Gaussian noise variable $\mathbf{z}$: $\mathbf{x}=f_\theta(\mathbf{z})$. In such a model (\eg, a VAE, GAN, or continuous-time flow-based model), sampling of the target variable $\mathbf{x} \sim p_\theta(\mathbf{x})$ is straightforward, but sampling from a posterior distribution of the form $p(\mathbf{x}\mid\mathbf{y}) \propto p_\theta(\mathbf{x})r(\mathbf{x},\mathbf{y})$, where $r$ is a constraint function depending on an auxiliary variable $\mathbf{y}$, is generally intractable. We propose to amortize the cost of sampling from such posterior distributions with diffusion models that sample a distribution in the noise space ($\mathbf{z}$). These diffusion samplers are trained by reinforcement learning algorithms to enforce that the transformed samples $f_\theta(\mathbf{z})$ are distributed according to the posterior in the data space ($\mathbf{x}$). For many models and constraints, the posterior in noise space is smoother than in data space, making it more suitable for amortized inference. Our method enables conditional sampling under unconditional GAN, (H)VAE, and flow-based priors, comparing favorably with other inference methods. We demonstrate the proposed outsourced diffusion sampling in several experiments with large pretrained prior models: conditional image generation, reinforcement learning with human feedback, and protein structure generation.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>You Do Not Fully Utilize Transformer's Representation Capacity</title>
<link>https://arxiv.org/abs/2502.09245</link>
<guid>https://arxiv.org/abs/2502.09245</guid>
<content:encoded><![CDATA[
arXiv:2502.09245v2 Announce Type: replace 
Abstract: In contrast to RNNs, which compress their history into a single hidden state, Transformers can attend to all past tokens directly. However, standard Transformers rely solely on the hidden state from the previous layer to represent the entire context. We show that this design choice induces representation collapse and degrades performance. To address this issue, we introduce Layer-Integrated Memory (LIMe), a lightweight extension that leverages existing key-value buffers and learns per-head, per-layer routing weights to integrate representations from all previous layers with negligible overhead. Through extensive experiments-including language modeling, synthetic reasoning benchmarks, and very deep architectures-LIMe consistently achieves faster convergence, lower perplexity per FLOP, and substantial accuracy improvements on synthetic tasks while preserving higher value-vector entropy and improved token separability. Finally, our analysis of the learned routing weights reveals systematic reuse of both local and long-distance features, demonstrating how LIMe mitigates collapse, unlocks richer representations without increasing hidden-state size, and points to promising directions for future research.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When do neural networks learn world models?</title>
<link>https://arxiv.org/abs/2502.09297</link>
<guid>https://arxiv.org/abs/2502.09297</guid>
<content:encoded><![CDATA[
arXiv:2502.09297v3 Announce Type: replace 
Abstract: Humans develop world models that capture the underlying generation process of data. Whether neural networks can learn similar world models remains an open problem. In this work, we present the first theoretical results for this problem, showing that in a multi-task setting, models with a low-degree bias provably recover latent data-generating variables under mild assumptions -- even if proxy tasks involve complex, non-linear functions of the latents. However, such recovery is sensitive to model architecture. Our analysis leverages Boolean models of task solutions via the Fourier-Walsh transform and introduces new techniques for analyzing invertible Boolean transforms, which may be of independent interest. We illustrate the algorithmic implications of our results and connect them to related research areas, including self-supervised learning, out-of-distribution generalization, and the linear representation hypothesis in large language models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffMS: Diffusion Generation of Molecules Conditioned on Mass Spectra</title>
<link>https://arxiv.org/abs/2502.09571</link>
<guid>https://arxiv.org/abs/2502.09571</guid>
<content:encoded><![CDATA[
arXiv:2502.09571v2 Announce Type: replace 
Abstract: Mass spectrometry plays a fundamental role in elucidating the structures of unknown molecules and subsequent scientific discoveries. One formulation of the structure elucidation task is the conditional de novo generation of molecular structure given a mass spectrum. Toward a more accurate and efficient scientific discovery pipeline for small molecules, we present DiffMS, a formula-restricted encoder-decoder generative network that achieves state-of-the-art performance on this task. The encoder utilizes a transformer architecture and models mass spectra domain knowledge such as peak formulae and neutral losses, and the decoder is a discrete graph diffusion model restricted by the heavy-atom composition of a known chemical formula. To develop a robust decoder that bridges latent embeddings and molecular structures, we pretrain the diffusion decoder with fingerprint-structure pairs, which are available in virtually infinite quantities, compared to structure-spectrum pairs that number in the tens of thousands. Extensive experiments on established benchmarks show that DiffMS outperforms existing models on de novo molecule generation. We provide several ablations to demonstrate the effectiveness of our diffusion and pretraining approaches and show consistent performance scaling with increasing pretraining dataset size. DiffMS code is publicly available at https://github.com/coleygroup/DiffMS.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-Markovian Discrete Diffusion with Causal Language Models</title>
<link>https://arxiv.org/abs/2502.09767</link>
<guid>https://arxiv.org/abs/2502.09767</guid>
<content:encoded><![CDATA[
arXiv:2502.09767v2 Announce Type: replace 
Abstract: Discrete diffusion models offer a flexible, controllable approach to structured sequence generation, yet they still lag behind causal language models in expressive power. A key limitation lies in their reliance on the Markovian assumption, which restricts each step to condition only on the current state, leading to potential uncorrectable error accumulation. In this paper, we introduce CaDDi, a discrete diffusion model that conditions on the entire generative trajectory, thereby lifting the Markov constraint and allowing the model to revisit and improve past states. By unifying sequential (causal) and temporal (diffusion) reasoning in a single non-Markovian transformer, CaDDi also treats standard causal language models as a special case and permits the direct reuse of pretrained LLM weights with no architectural changes. Empirically, CaDDi outperforms state-of-the-art discrete diffusion baselines on natural-language benchmarks, substantially narrowing the remaining gap to large autoregressive transformers.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Empirical Bayes via Transformers</title>
<link>https://arxiv.org/abs/2502.09844</link>
<guid>https://arxiv.org/abs/2502.09844</guid>
<content:encoded><![CDATA[
arXiv:2502.09844v2 Announce Type: replace 
Abstract: This work applies modern AI tools (transformers) to solving one of the oldest statistical problems: Poisson means under empirical Bayes (Poisson-EB) setting. In Poisson-EB a high-dimensional mean vector $\theta$ (with iid coordinates sampled from an unknown prior $\pi$) is estimated on the basis of $X=\mathrm{Poisson}(\theta)$. A transformer model is pre-trained on a set of synthetically generated pairs $(X,\theta)$ and learns to do in-context learning (ICL) by adapting to unknown $\pi$. Theoretically, we show that a sufficiently wide transformer can achieve vanishing regret with respect to an oracle estimator who knows $\pi$ as dimension grows to infinity. Practically, we discover that already very small models (100k parameters) are able to outperform the best classical algorithm (non-parametric maximum likelihood, or NPMLE) both in runtime and validation loss, which we compute on out-of-distribution synthetic data as well as real-world datasets (NHL hockey, MLB baseball, BookCorpusOpen). Finally, by using linear probes, we confirm that the transformer's EB estimator appears to internally work differently from either NPMLE or Robbins' estimators.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Closed-Form Training Dynamics Reveal Learned Features and Linear Structure in Word2Vec-like Models</title>
<link>https://arxiv.org/abs/2502.09863</link>
<guid>https://arxiv.org/abs/2502.09863</guid>
<content:encoded><![CDATA[
arXiv:2502.09863v2 Announce Type: replace 
Abstract: Self-supervised word embedding algorithms such as word2vec provide a minimal setting for studying representation learning in language modeling. We examine the quartic Taylor approximation of the word2vec loss around the origin, and we show that both the resulting training dynamics and the final performance on downstream tasks are empirically very similar to those of word2vec. Our main contribution is to analytically solve for both the gradient flow training dynamics and the final word embeddings in terms of only the corpus statistics and training hyperparameters. The solutions reveal that these models learn orthogonal linear subspaces one at a time, each one incrementing the effective rank of the embeddings until model capacity is saturated. Training on Wikipedia, we find that each of the top linear subspaces represents an interpretable topic-level concept. Finally, we apply our theory to describe how linear representations of more abstract semantic concepts emerge during training; these can be used to complete analogies via vector addition.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL</title>
<link>https://arxiv.org/abs/2502.11107</link>
<guid>https://arxiv.org/abs/2502.11107</guid>
<content:encoded><![CDATA[
arXiv:2502.11107v3 Announce Type: replace 
Abstract: As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last linear layer, reverse KL guarantees that it outperforms its weak supervisor by the magnitude of their disagreement. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to successfully outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MUDDFormer: Breaking Residual Bottlenecks in Transformers via Multiway Dynamic Dense Connections</title>
<link>https://arxiv.org/abs/2502.12170</link>
<guid>https://arxiv.org/abs/2502.12170</guid>
<content:encoded><![CDATA[
arXiv:2502.12170v2 Announce Type: replace 
Abstract: We propose MUltiway Dynamic Dense (MUDD) connections, a simple yet effective method to address the limitations of residual connections and enhance cross-layer information flow in Transformers. Unlike existing dense connection approaches with static and shared connection weights, MUDD generates connection weights dynamically depending on hidden states at each sequence position and for each decoupled input stream (the query, key, value or residual) of a Transformer block. MUDD connections can be seamlessly integrated into any Transformer architecture to create MUDDFormer. Extensive experiments show that MUDDFormer significantly outperforms Transformers across various model architectures and scales in language modeling, achieving the performance of Transformers trained with 1.8X-2.4X compute. Notably, MUDDPythia-2.8B matches Pythia-6.9B in pretraining ppl and downstream tasks and even rivals Pythia-12B in five-shot settings, while adding only 0.23% parameters and 0.4% computation. Code in JAX and PyTorch and pre-trained models are available at https://github.com/Caiyun-AI/MUDDFormer .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReQFlow: Rectified Quaternion Flow for Efficient and High-Quality Protein Backbone Generation</title>
<link>https://arxiv.org/abs/2502.14637</link>
<guid>https://arxiv.org/abs/2502.14637</guid>
<content:encoded><![CDATA[
arXiv:2502.14637v3 Announce Type: replace 
Abstract: Protein backbone generation plays a central role in de novo protein design and is significant for many biological and medical applications. Although diffusion and flow-based generative models provide potential solutions to this challenging task, they often generate proteins with undesired designability and suffer computational inefficiency. In this study, we propose a novel rectified quaternion flow (ReQFlow) matching method for fast and high-quality protein backbone generation. In particular, our method generates a local translation and a 3D rotation from random noise for each residue in a protein chain, which represents each 3D rotation as a unit quaternion and constructs its flow by spherical linear interpolation (SLERP) in an exponential format. We train the model by quaternion flow (QFlow) matching with guaranteed numerical stability and rectify the QFlow model to accelerate its inference and improve the designability of generated protein backbones, leading to the proposed ReQFlow model. Experiments show that ReQFlow achieves on-par performance in protein backbone generation while requiring much fewer sampling steps and significantly less inference time (e.g., being 37x faster than RFDiffusion and 63x faster than Genie2 when generating a backbone of length 300), demonstrating its effectiveness and efficiency. The code is available at https://github.com/AngxiaoYue/ReQFlow.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Inverse Problems with Deep Linear Neural Networks: Global Convergence Guarantees for Gradient Descent with Weight Decay</title>
<link>https://arxiv.org/abs/2502.15522</link>
<guid>https://arxiv.org/abs/2502.15522</guid>
<content:encoded><![CDATA[
arXiv:2502.15522v2 Announce Type: replace 
Abstract: Machine learning methods are commonly used to solve inverse problems, wherein an unknown signal must be estimated from few measurements generated via a known acquisition procedure. In particular, neural networks perform well empirically but have limited theoretical guarantees. In this work, we study an underdetermined linear inverse problem that admits several possible solution mappings. A standard remedy (e.g., in compressed sensing) establishing uniqueness of the solution mapping is to assume knowledge of latent low-dimensional structure in the source signal. We ask the following question: do deep neural networks adapt to this low-dimensional structure when trained by gradient descent with weight decay regularization? We prove that mildly overparameterized deep linear networks trained in this manner converge to an approximate solution that accurately solves the inverse problem while implicitly encoding latent subspace structure. To our knowledge, this is the first result to rigorously show that deep linear networks trained with weight decay automatically adapt to latent subspace structure in the data under practical stepsize and weight initialization schemes. Our work highlights that regularization and overparameterization improve generalization, while overparameterization also accelerates convergence during training.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
<link>https://arxiv.org/abs/2502.18807</link>
<guid>https://arxiv.org/abs/2502.18807</guid>
<content:encoded><![CDATA[
arXiv:2502.18807v3 Announce Type: replace 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.5 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 59 chemical systems, 9 operating temperatures, and 421 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in various neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explainable Multi-modal Time Series Prediction with LLM-in-the-Loop</title>
<link>https://arxiv.org/abs/2503.01013</link>
<guid>https://arxiv.org/abs/2503.01013</guid>
<content:encoded><![CDATA[
arXiv:2503.01013v2 Announce Type: replace 
Abstract: Time series analysis provides essential insights for real-world system dynamics and informs downstream decision-making, yet most existing methods often overlook the rich contextual signals present in auxiliary modalities. To bridge this gap, we introduce TimeXL, a multi-modal prediction framework that integrates a prototype-based time series encoder with three collaborating Large Language Models (LLMs) to deliver more accurate predictions and interpretable explanations. First, a multi-modal prototype-based encoder processes both time series and textual inputs to generate preliminary forecasts alongside case-based rationales. These outputs then feed into a prediction LLM, which refines the forecasts by reasoning over the encoder's predictions and explanations. Next, a reflection LLM compares the predicted values against the ground truth, identifying textual inconsistencies or noise. Guided by this feedback, a refinement LLM iteratively enhances text quality and triggers encoder retraining. This closed-loop workflow -- prediction, critique (reflect), and refinement -- continuously boosts the framework's performance and interpretability. Empirical evaluations on four real-world datasets demonstrate that TimeXL achieves up to 8.9\% improvement in AUC and produces human-centric, multi-modal explanations, highlighting the power of LLM-driven reasoning for time series prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wanda++: Pruning Large Language Models via Regional Gradients</title>
<link>https://arxiv.org/abs/2503.04992</link>
<guid>https://arxiv.org/abs/2503.04992</guid>
<content:encoded><![CDATA[
arXiv:2503.04992v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal accuracy impact. However, existing methods often suffer from accuracy degradation without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Moreover, despite updating weights with regional optimization, Wanda++ remains orthogonal to sparsity-aware fine-tuning, further reducing perplexity with LoRA in great extend. Our approach is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single H100 GPU.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Visualizations of Data Spaces for Classification Problems</title>
<link>https://arxiv.org/abs/2503.05861</link>
<guid>https://arxiv.org/abs/2503.05861</guid>
<content:encoded><![CDATA[
arXiv:2503.05861v2 Announce Type: replace 
Abstract: How do classification models "see" our data? Based on their success in delineating behaviors, there must be some lens through which it is easy to see the boundary between classes; however, our current set of visualization techniques makes this prospect difficult. In this work, we propose a hybrid supervised-unsupervised technique distinctly suited to visualizing the decision boundaries determined by classification problems. This method provides a human-interpretable map that can be analyzed qualitatively and quantitatively, which we demonstrate through visualizing and interpreting a decision boundary for chemical neurotoxicity. While we discuss this method in the context of chemistry-driven problems, its application can be generalized across subfields for "unboxing" the operations of machine-learning classification models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning with Verifiable Rewards: GRPO's Effective Loss, Dynamics, and Success Amplification</title>
<link>https://arxiv.org/abs/2503.06639</link>
<guid>https://arxiv.org/abs/2503.06639</guid>
<content:encoded><![CDATA[
arXiv:2503.06639v3 Announce Type: replace 
Abstract: Group Relative Policy Optimization (GRPO) was introduced recently and used successfully to train DeepSeek-R1 models for promoting reasoning capabilities of LLMs using verifiable or binary rewards. We show in this paper that GRPO with verifiable rewards can be written as a Kullback--Leibler (KL) regularized contrastive loss, where the contrastive samples are synthetic data sampled from the old policy. The optimal GRPO policy $\pi_{n}$ can be expressed explicitly in terms of the binary reward, as well as the first- and second-order statistics of the old policy ($\pi_{n-1}$) and the reference policy $\pi_{\text{ref}}$. Iterating this scheme, we obtain a sequence of policies $\pi_{n}$ for which we can quantify the probability of success $p_n$. We show that the probability of success of the policy satisfies a recurrence that converges to a fixed point of a function that depends on the initial probability of success $p_{\text{ref}}$ and the regularization parameter $\beta$ of the $KL$ regularizer. We show that the fixed point $p^*$ is guaranteed to be larger than $p_{\text{ref}}$, thereby demonstrating that GRPO effectively amplifies the probability of success of the policy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TS-RAG: Retrieval-Augmented Generation based Time Series Foundation Models are Stronger Zero-Shot Forecaster</title>
<link>https://arxiv.org/abs/2503.07649</link>
<guid>https://arxiv.org/abs/2503.07649</guid>
<content:encoded><![CDATA[
arXiv:2503.07649v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) and Foundation Models (FMs) have recently become prevalent for time series forecasting tasks. While fine-tuning LLMs enables domain adaptation, they often struggle to generalize across diverse and unseen datasets. Moreover, existing Time Series Foundation Models (TSFMs) still face challenges in handling non-stationary dynamics and distribution shifts, largely due to the lack of effective mechanisms for adaptation. To this end, we present TS-RAG, a retrieval-augmented generation framework for time series forecasting that enhances the generalization and interpretability of TSFMs. Specifically, TS-RAG leverages pre-trained time series encoders to retrieve semantically relevant segments from a dedicated knowledge base, enriching the contextual representation of the input query. Furthermore, we propose an Adaptive Retrieval Mixer (ARM) module that dynamically fuses the retrieved patterns with the TSFM's internal representation, improving forecasting accuracy without requiring task-specific fine-tuning. Thorough empirical studies on seven public benchmark datasets demonstrate that TS-RAG achieves state-of-the-art zero-shot forecasting performance, outperforming the existing TSFMs by up to 6.84% across diverse domains while also providing desirable interpretability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Language-Enhanced Representation Learning for Single-Cell Transcriptomics</title>
<link>https://arxiv.org/abs/2503.09427</link>
<guid>https://arxiv.org/abs/2503.09427</guid>
<content:encoded><![CDATA[
arXiv:2503.09427v3 Announce Type: replace 
Abstract: Single-cell RNA sequencing (scRNA-seq) offers detailed insights into cellular heterogeneity. Recent advancements leverage single-cell large language models (scLLMs) for effective representation learning. These models focus exclusively on transcriptomic data, neglecting complementary biological knowledge from textual descriptions. To overcome this limitation, we propose scMMGPT, a novel multimodal framework designed for language-enhanced representation learning in single-cell transcriptomics. Unlike existing methods, scMMGPT employs robust cell representation extraction, preserving quantitative gene expression data, and introduces an innovative two-stage pre-training strategy combining discriminative precision with generative flexibility. Extensive experiments demonstrate that scMMGPT significantly outperforms unimodal and multimodal baselines across key downstream tasks, including cell annotation and clustering, and exhibits superior generalization in out-of-distribution scenarios.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In-Context Linear Regression Demystified: Training Dynamics and Mechanistic Interpretability of Multi-Head Softmax Attention</title>
<link>https://arxiv.org/abs/2503.12734</link>
<guid>https://arxiv.org/abs/2503.12734</guid>
<content:encoded><![CDATA[
arXiv:2503.12734v2 Announce Type: replace 
Abstract: We study how multi-head softmax attention models are trained to perform in-context learning on linear data. Through extensive empirical experiments and rigorous theoretical analysis, we demystify the emergence of elegant attention patterns: a diagonal and homogeneous pattern in the key-query (KQ) weights, and a last-entry-only and zero-sum pattern in the output-value (OV) weights. Remarkably, these patterns consistently appear from gradient-based training starting from random initialization. Our analysis reveals that such emergent structures enable multi-head attention to approximately implement a debiased gradient descent predictor -- one that outperforms single-head attention and nearly achieves Bayesian optimality up to proportional factor. Furthermore, compared to linear transformers, the softmax attention readily generalizes to sequences longer than those seen during training. We also extend our study to scenarios with anisotropic covariates and multi-task linear regression. In the former, multi-head attention learns to implement a form of pre-conditioned gradient descent. In the latter, we uncover an intriguing regime where the interplay between head number and task number triggers a superposition phenomenon that efficiently resolves multi-task in-context learning. Our results reveal that in-context learning ability emerges from the trained transformer as an aggregated effect of its architecture and the underlying data distribution, paving the way for deeper understanding and broader applications of in-context learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MultiScale Contextual Bandits for Long Term Objectives</title>
<link>https://arxiv.org/abs/2503.17674</link>
<guid>https://arxiv.org/abs/2503.17674</guid>
<content:encoded><![CDATA[
arXiv:2503.17674v2 Announce Type: replace 
Abstract: The feedback that AI systems (e.g., recommender systems, chatbots) collect from user interactions is a crucial source of training data. While short-term feedback (e.g., clicks, engagement) is widely used for training, there is ample evidence that optimizing short-term feedback does not necessarily achieve the desired long-term objectives. Unfortunately, directly optimizing for long-term objectives is challenging, and we identify the disconnect in the timescales of short-term interventions (e.g., rankings) and the long-term feedback (e.g., user retention) as one of the key obstacles. To overcome this disconnect, we introduce the framework of MultiScale Policy Learning to contextually reconcile that AI systems need to act and optimize feedback at multiple interdependent timescales. Following a PAC-Bayes motivation, we show how the lower timescales with more plentiful data can provide a data-dependent hierarchical prior for faster learning at higher scales, where data is more scarce. As a result, the policies at all levels effectively optimize for the long-term. We instantiate the framework with MultiScale Off-Policy Bandit Learning (MSBL) and demonstrate its effectiveness on three tasks relating to recommender and conversational systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Verifiable Rewards: Scaling Reinforcement Learning for Language Models to Unverifiable Data</title>
<link>https://arxiv.org/abs/2503.19618</link>
<guid>https://arxiv.org/abs/2503.19618</guid>
<content:encoded><![CDATA[
arXiv:2503.19618v2 Announce Type: replace 
Abstract: We propose to scale RL to unverifiable data with a novel algorithm JEPO (Jensen's Evidence lower bound Policy Optimization). While most prior efforts on scaling RL for LLMs focus on verifiable data where ground truth answers are typically short-form and can be matched easily; we investigate the case where such assumptions are less valid (e.g., when answers are long-form such as mathematical proofs). To scale RL training to unverifiable data with contemporary training constraints, we propose JEPO. JEPO applies Jensen's evidence lower bound, a pragmatic simplification of the evidence lower bound which views chain-of-thought as a latent variable in the generative process. We show that on verifiable data (math), JEPO is as effective as RL with verifiable rewards; on semi-verifiable data (numina), JEPO improves on soft-match based evaluations compared to RL with verifiable rewards which can only leverage a subset of the data source; finally, on unverifiable data (numina-proof), JEPO outperforms SFT and a few ablation baselines on likelihood evaluations.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving User Behavior Prediction: Leveraging Annotator Metadata in Supervised Machine Learning Models</title>
<link>https://arxiv.org/abs/2503.21000</link>
<guid>https://arxiv.org/abs/2503.21000</guid>
<content:encoded><![CDATA[
arXiv:2503.21000v2 Announce Type: replace 
Abstract: Supervised machine-learning models often underperform in predicting user behaviors from conversational text, hindered by poor crowdsourced label quality and low NLP task accuracy. We introduce the Metadata-Sensitive Weighted-Encoding Ensemble Model (MSWEEM), which integrates annotator meta-features like fatigue and speeding. First, our results show MSWEEM outperforms standard ensembles by 14% on held-out data and 12% on an alternative dataset. Second, we find that incorporating signals of annotator behavior, such as speed and fatigue, significantly boosts model performance. Third, we find that annotators with higher qualifications, such as Master's, deliver more consistent and faster annotations. Given the increasing uncertainty over annotation quality, our experiments show that understanding annotator patterns is crucial for enhancing model accuracy in user behavior prediction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluation of the impact of expert knowledge: How decision support scores impact the effectiveness of automatic knowledge-driven feature engineering (aKDFE)</title>
<link>https://arxiv.org/abs/2504.05928</link>
<guid>https://arxiv.org/abs/2504.05928</guid>
<content:encoded><![CDATA[
arXiv:2504.05928v2 Announce Type: replace 
Abstract: Adverse Drug Events (ADEs), harmful medication effects, pose significant healthcare challenges, impacting patient safety and costs. This study evaluates automatic Knowledge-Driven Feature Engineering (aKDFE) for improved ADE prediction from Electronic Health Record (EHR) data, comparing it with automated event-based Knowledge Discovery in Databases (KDD). We investigated how incorporating domain-specific ADE risk scores for prolonged heart QT interval, extracted from the Janusmed Riskprofile (Janusmed) Clinical Decision Support System (CDSS), affects prediction performance using EHR data and medication handling events. Results indicate that, while aKDFE step 1 (event-based feature generation) alone did not significantly improve ADE prediction performance, aKDFE step 2 (patient-centric transformation) enhances the prediction performance. High Area Under the Receiver Operating Characteristic curve (AUROC) values suggest strong feature correlations to the outcome, aligning with the predictive power of patients' prior healthcare history for ADEs. Statistical analysis did not confirm that incorporating the Janusmed information (i) risk scores and (ii) medication route of administration into the model's feature set enhanced predictive performance. However, the patient-centric transformation applied by aKDFE proved to be a highly effective feature engineering approach. Limitations include a single-project focus, potential bias from machine learning pipeline methods, and reliance on AUROC. In conclusion, aKDFE, particularly with patient-centric transformation, improves ADE prediction from EHR data. Future work will explore attention-based models, event feature sequences, and automatic methods for incorporating domain knowledge into the aKDFE framework.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adjoint Sampling: Highly Scalable Diffusion Samplers via Adjoint Matching</title>
<link>https://arxiv.org/abs/2504.11713</link>
<guid>https://arxiv.org/abs/2504.11713</guid>
<content:encoded><![CDATA[
arXiv:2504.11713v3 Announce Type: replace 
Abstract: We introduce Adjoint Sampling, a highly scalable and efficient algorithm for learning diffusion processes that sample from unnormalized densities, or energy functions. It is the first on-policy approach that allows significantly more gradient updates than the number of energy evaluations and model samples, allowing us to scale to much larger problem settings than previously explored by similar methods. Our framework is theoretically grounded in stochastic optimal control and shares the same theoretical guarantees as Adjoint Matching, being able to train without the need for corrective measures that push samples towards the target distribution. We show how to incorporate key symmetries, as well as periodic boundary conditions, for modeling molecules in both cartesian and torsional coordinates. We demonstrate the effectiveness of our approach through extensive experiments on classical energy functions, and further scale up to neural network-based energy models where we perform amortized conformer generation across many molecular systems. To encourage further research in developing highly scalable sampling methods, we plan to open source these challenging benchmarks, where successful methods can directly impact progress in computational chemistry.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraphOmni: A Comprehensive and Extendable Benchmark Framework for Large Language Models on Graph-theoretic Tasks</title>
<link>https://arxiv.org/abs/2504.12764</link>
<guid>https://arxiv.org/abs/2504.12764</guid>
<content:encoded><![CDATA[
arXiv:2504.12764v3 Announce Type: replace 
Abstract: This paper introduces GraphOmni, a comprehensive benchmark designed to evaluate the reasoning capabilities of LLMs on graph-theoretic tasks articulated in natural language. GraphOmni encompasses diverse graph types, serialization formats, and prompting schemes, significantly exceeding prior efforts in both scope and depth. Through extensive systematic evaluation, we identify critical interactions among these dimensions, demonstrating their substantial impact on model performance. Our experiments reveal that state-of-the-art models like Claude-3.5 and o4-mini consistently outperform other models, yet even these leading models exhibit substantial room for improvement. Performance variability is evident depending on the specific combinations of factors we considered, underscoring the necessity of comprehensive evaluations across these interconnected dimensions. Additionally, we observe distinct impacts of serialization and prompting strategies between open-source and closed-source models, encouraging the development of tailored approaches. Motivated by the findings, we also propose a reinforcement learning-inspired framework that adaptively selects the optimal factors influencing LLM reasoning capabilities. This flexible and extendable benchmark not only deepens our understanding of LLM performance on structured tasks but also provides a robust foundation for advancing research in LLM-based graph reasoning. The code and datasets are available at https://github.com/GAI-Community/GraphOmni.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization</title>
<link>https://arxiv.org/abs/2504.18026</link>
<guid>https://arxiv.org/abs/2504.18026</guid>
<content:encoded><![CDATA[
arXiv:2504.18026v2 Announce Type: replace 
Abstract: Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis on some key properties of the CPO objective showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to concept noise. We empirically confirm our analysis finding that CPO consistently outperforms BCE in three real world datasets with and without added label noise.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry-Informed Neural Operator Transformer</title>
<link>https://arxiv.org/abs/2504.19452</link>
<guid>https://arxiv.org/abs/2504.19452</guid>
<content:encoded><![CDATA[
arXiv:2504.19452v3 Announce Type: replace 
Abstract: Machine-learning-based surrogate models offer significant computational efficiency and faster simulations compared to traditional numerical methods, especially for problems requiring repeated evaluations of partial differential equations. This work introduces the Geometry-Informed Neural Operator Transformer (GINOT), which integrates the transformer architecture with the neural operator framework to enable forward predictions for arbitrary geometries. GINOT encodes the surface points cloud of a geometry using a sampling and grouping mechanism combined with an attention mechanism, ensuring invariance to point order and padding while maintaining robustness to variations in point density. The geometry information is seamlessly integrated with query points in the solution decoder through the attention mechanism. The performance of GINOT is validated on multiple challenging datasets, showcasing its high accuracy and strong generalization capabilities for complex and arbitrary 2D and 3D geometries.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model</title>
<link>https://arxiv.org/abs/2505.05082</link>
<guid>https://arxiv.org/abs/2505.05082</guid>
<content:encoded><![CDATA[
arXiv:2505.05082v3 Announce Type: replace 
Abstract: Generative modeling of non-negative, discrete data, such as symbolic music, remains challenging due to two persistent limitations in existing methods. Firstly, many approaches rely on modeling continuous embeddings, which is suboptimal for inherently discrete data distributions. Secondly, most models optimize variational bounds rather than exact data likelihood, resulting in inaccurate likelihood estimates and degraded sampling quality. While recent diffusion-based models have addressed these issues separately, we tackle them jointly. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), inspired by photon arrival process, which combines exact likelihood estimation with fully discrete-state modeling. Central to our approach is an information-theoretic Poisson Reconstruction Loss (PRL) that has a provable exact relationship with the true data likelihood. ItDPDM achieves improved likelihood and sampling performance over prior discrete and continuous diffusion models on a variety of synthetic discrete datasets. Furthermore, on real-world datasets such as symbolic music and images, ItDPDM attains superior likelihood estimates and competitive generation quality-demonstrating a proof of concept for distribution-robust discrete generative modeling.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous Thought Machines</title>
<link>https://arxiv.org/abs/2505.05522</link>
<guid>https://arxiv.org/abs/2505.05522</guid>
<content:encoded><![CDATA[
arXiv:2505.05522v3 Announce Type: replace 
Abstract: Biological brains demonstrate complex neural activity, where the timing and interplay between neurons is critical to how brains process information. Most deep learning architectures simplify neural activity by abstracting away temporal dynamics. In this paper we challenge that paradigm. By incorporating neuron-level processing and synchronization, we can effectively reintroduce neural timing as a foundational element. We present the Continuous Thought Machine (CTM), a model designed to leverage neural dynamics as its core representation. The CTM has two core innovations: (1) neuron-level temporal processing, where each neuron uses unique weight parameters to process a history of incoming signals; and (2) neural synchronization employed as a latent representation. The CTM aims to strike a balance between oversimplified neuron abstractions that improve computational efficiency, and biological realism. It operates at a level of abstraction that effectively captures essential temporal dynamics while remaining computationally tractable for deep learning. We demonstrate the CTM's strong performance and versatility across a range of challenging tasks, including ImageNet-1K classification, solving 2D mazes, sorting, parity computation, question-answering, and RL tasks. Beyond displaying rich internal representations and offering a natural avenue for interpretation owing to its internal process, the CTM is able to perform tasks that require complex sequential reasoning. The CTM can also leverage adaptive compute, where it can stop earlier for simpler tasks, or keep computing when faced with more challenging instances. The goal of this work is to share the CTM and its associated innovations, rather than pushing for new state-of-the-art results. To that end, we believe the CTM represents a significant step toward developing more biologically plausible and powerful artificial intelligence systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Causal Direction via Variational Bayesian Compression</title>
<link>https://arxiv.org/abs/2505.07503</link>
<guid>https://arxiv.org/abs/2505.07503</guid>
<content:encoded><![CDATA[
arXiv:2505.07503v3 Announce Type: replace 
Abstract: Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning</title>
<link>https://arxiv.org/abs/2505.11953</link>
<guid>https://arxiv.org/abs/2505.11953</guid>
<content:encoded><![CDATA[
arXiv:2505.11953v2 Announce Type: replace 
Abstract: Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at https://github.com/tmlr-group/SatImp.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backdoors in DRL: Four Environments Focusing on In-distribution Triggers</title>
<link>https://arxiv.org/abs/2505.17248</link>
<guid>https://arxiv.org/abs/2505.17248</guid>
<content:encoded><![CDATA[
arXiv:2505.17248v2 Announce Type: replace 
Abstract: Backdoor attacks, or trojans, pose a security risk by concealing undesirable behavior in deep neural network models. Open-source neural networks are downloaded from the internet daily, possibly containing backdoors, and third-party model developers are common. To advance research on backdoor attack mitigation, we develop several trojans for deep reinforcement learning (DRL) agents. We focus on in-distribution triggers, which occur within the agent's natural data distribution, since they pose a more significant security threat than out-of-distribution triggers due to their ease of activation by the attacker during model deployment. We implement backdoor attacks in four reinforcement learning (RL) environments: LavaWorld, Randomized LavaWorld, Colorful Memory, and Modified Safety Gymnasium. We train various models, both clean and backdoored, to characterize these attacks. We find that in-distribution triggers can require additional effort to implement and be more challenging for models to learn, but are nevertheless viable threats in DRL even using basic data poisoning attacks.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>C-LoRA: Contextual Low-Rank Adaptation for Uncertainty Estimation in Large Language Models</title>
<link>https://arxiv.org/abs/2505.17773</link>
<guid>https://arxiv.org/abs/2505.17773</guid>
<content:encoded><![CDATA[
arXiv:2505.17773v2 Announce Type: replace 
Abstract: Low-Rank Adaptation (LoRA) offers a cost-effective solution for fine-tuning large language models (LLMs), but it often produces overconfident predictions in data-scarce few-shot settings. To address this issue, several classical statistical learning approaches have been repurposed for scalable uncertainty-aware LoRA fine-tuning. However, these approaches neglect how input characteristics affect the predictive uncertainty estimates. To address this limitation, we propose Contextual Low-Rank Adaptation (\textbf{C-LoRA}) as a novel uncertainty-aware and parameter efficient fine-tuning approach, by developing new lightweight LoRA modules contextualized to each input data sample to dynamically adapt uncertainty estimates. Incorporating data-driven contexts into the parameter posteriors, C-LoRA mitigates overfitting, achieves well-calibrated uncertainties, and yields robust predictions. Extensive experiments demonstrate that C-LoRA consistently outperforms the state-of-the-art uncertainty-aware LoRA methods in both uncertainty quantification and model generalization. Ablation studies further confirm the critical role of our contextual modules in capturing sample-specific uncertainties. C-LoRA sets a new standard for robust, uncertainty-aware LLM fine-tuning in few-shot regimes.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Supervised Learning and Reinforcement Learning in Math Reasoning</title>
<link>https://arxiv.org/abs/2505.18116</link>
<guid>https://arxiv.org/abs/2505.18116</guid>
<content:encoded><![CDATA[
arXiv:2505.18116v2 Announce Type: replace 
Abstract: Reinforcement Learning (RL) has played a central role in the recent surge of LLMs' math abilities by enabling self-improvement through binary verifier signals. In contrast, Supervised Learning (SL) is rarely considered for such verification-driven training, largely due to its heavy reliance on reference answers and inability to reflect on mistakes. In this work, we challenge the prevailing notion that self-improvement is exclusive to RL and propose Negative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to reflect on their failures and improve autonomously with no external teachers. In online training, instead of throwing away self-generated negative answers, NFT constructs an implicit negative policy to model them. This implicit policy is parameterized with the same positive LLM we target to optimize on positive data, enabling direct policy optimization on all LLMs' generations. We conduct experiments on 7B and 32B models in math reasoning tasks. Results consistently show that through the additional leverage of negative feedback, NFT significantly improves over SL baselines like Rejection sampling Fine-Tuning, matching or even surpassing leading RL algorithms like GRPO and DAPO. Furthermore, we demonstrate that NFT and GRPO are actually equivalent in strict-on-policy training, even though they originate from entirely different theoretical foundations. Our experiments and theoretical findings bridge the gap between SL and RL methods in binary-feedback learning systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning in Stackelberg Games with Non-myopic Agents</title>
<link>https://arxiv.org/abs/2208.09407</link>
<guid>https://arxiv.org/abs/2208.09407</guid>
<content:encoded><![CDATA[
arXiv:2208.09407v3 Announce Type: replace-cross 
Abstract: We study Stackelberg games where a principal repeatedly interacts with a non-myopic long-lived agent, without knowing the agent's payoff function. Although learning in Stackelberg games is well-understood when the agent is myopic, dealing with non-myopic agents poses additional complications. In particular, non-myopic agents may strategize and select actions that are inferior in the present in order to mislead the principal's learning algorithm and obtain better outcomes in the future.
  We provide a general framework that reduces learning in presence of non-myopic agents to robust bandit optimization in the presence of myopic agents. Through the design and analysis of minimally reactive bandit algorithms, our reduction trades off the statistical efficiency of the principal's learning algorithm against its effectiveness in inducing near-best-responses. We apply this framework to Stackelberg security games (SSGs), pricing with unknown demand curve, general finite Stackelberg games, and strategic classification. In each setting, we characterize the type and impact of misspecifications present in near-best responses and develop a learning algorithm robust to such misspecifications.
  On the way, we improve the state-of-the-art query complexity of learning in SSGs with $n$ targets from $O(n^3)$ to a near-optimal $\widetilde{O}(n)$ by uncovering a fundamental structural property of these games. The latter result is of independent interest beyond learning with non-myopic agents.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving the Variance of Differentially Private Randomized Experiments through Clustering</title>
<link>https://arxiv.org/abs/2308.00957</link>
<guid>https://arxiv.org/abs/2308.00957</guid>
<content:encoded><![CDATA[
arXiv:2308.00957v3 Announce Type: replace-cross 
Abstract: Estimating causal effects from randomized experiments is only possible if participants are willing to disclose their potentially sensitive responses. Differential privacy, a widely used framework for ensuring an algorithms privacy guarantees, can encourage participants to share their responses without the risk of de-anonymization. However, many mechanisms achieve differential privacy by adding noise to the original dataset, which reduces the precision of causal effect estimation. This introduces a fundamental trade-off between privacy and variance when performing causal analyses on differentially private data. In this work, we propose a new differentially private mechanism, "Cluster-DP", which leverages a given cluster structure in the data to improve the privacy-variance trade-off. While our results apply to any clustering, we demonstrate that selecting higher-quality clusters, according to a quality metric we introduce, can decrease the variance penalty without compromising privacy guarantees. Finally, we evaluate the theoretical and empirical performance of our Cluster-DP algorithm on both real and simulated data, comparing it to common baselines, including two special cases of our algorithm: its unclustered version and a uniform-prior version.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Novelty Detection in Reinforcement Learning with World Models</title>
<link>https://arxiv.org/abs/2310.08731</link>
<guid>https://arxiv.org/abs/2310.08731</guid>
<content:encoded><![CDATA[
arXiv:2310.08731v4 Announce Type: replace-cross 
Abstract: Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as novelties. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional machine learning novelty detection methods as well as currently accepted RL focused novelty detection algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Breast Cancer Radiotherapy Planning via LMMs with Consistency Embedding</title>
<link>https://arxiv.org/abs/2311.15876</link>
<guid>https://arxiv.org/abs/2311.15876</guid>
<content:encoded><![CDATA[
arXiv:2311.15876v4 Announce Type: replace-cross 
Abstract: Recent advances in AI foundation models have significant potential for lightening the clinical workload by mimicking the comprehensive and multi-faceted approaches used by medical professionals. In the field of radiation oncology, the integration of multiple modalities holds great importance, so the opportunity of foundational model is abundant. Inspired by this, here we present RO-LMM, a multi-purpose, comprehensive large multimodal model (LMM) tailored for the field of radiation oncology. This model effectively manages a series of tasks within the clinical workflow, including clinical context summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation by leveraging the capabilities of LMM. In particular, to perform consecutive clinical tasks without error accumulation, we present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the consistency of handling clean inputs. We further extend this concept to LMM-driven segmentation framework, leading to a novel Consistency Embedding Segmentation (CESEG) techniques. Experimental results including multi-centre validation confirm that our RO-LMM with CEFTune and CESEG results in promising performance for multiple clinical tasks with generalization capabilities.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Meta Co-Training: Two Views are Better than One</title>
<link>https://arxiv.org/abs/2311.18083</link>
<guid>https://arxiv.org/abs/2311.18083</guid>
<content:encoded><![CDATA[
arXiv:2311.18083v5 Announce Type: replace-cross 
Abstract: In many critical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One representative class of semi-supervised algorithms are co-training algorithms. Co-training algorithms leverage two different models which have access to different independent and sufficient representations or "views" of the data to jointly make better predictions. Each of these models creates pseudo-labels on unlabeled points which are used to improve the other model. We show that in the common case where independent views are not available, we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning. We present Meta Co-Training, a novel semi-supervised learning algorithm, which has two advantages over co-training: (i) learning is more robust when there is large discrepancy between the information content of the different views, and (ii) does not require retraining from scratch on each iteration. Our method achieves new state-of-the-art performance on ImageNet-10% achieving a ~4.7% reduction in error rate over prior work. Our method also outperforms prior semi-supervised work on several other fine-grained image classification datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks</title>
<link>https://arxiv.org/abs/2404.01932</link>
<guid>https://arxiv.org/abs/2404.01932</guid>
<content:encoded><![CDATA[
arXiv:2404.01932v2 Announce Type: replace-cross 
Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate the challenges raised by the individual tasks such as object or robot position variability, number of distractors or the task length. Our work thus also sheds light on the potential benefits and limitations of using the current multimodal VAEs for unsupervised learning of robotic motion trajectories based on vision and language.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision-Focused Forecasting: A Differentiable Multistage Optimisation Architecture</title>
<link>https://arxiv.org/abs/2405.14719</link>
<guid>https://arxiv.org/abs/2405.14719</guid>
<content:encoded><![CDATA[
arXiv:2405.14719v2 Announce Type: replace-cross 
Abstract: Most decision-focused learning work has focused on single stage problems whereas many real-world decision problems are more appropriately modelled using multistage optimisation. In multistage problems contextual information is revealed over time, decisions have to be taken sequentially, and decisions now have an intertemporal effect on future decisions. Decision-focused forecasting is a recurrent differentiable optimisation architecture that expresses a fully differentiable multistage optimisation approach. This architecture enables us to account for the intertemporal decision effects of forecasts. We show what gradient adjustments are made to account for the state-path caused by forecasting. We apply the model to multistage problems in energy storage arbitrage and portfolio optimisation and report that our model outperforms existing approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Edit Distance Robust Watermarks via Indexing Pseudorandom Codes</title>
<link>https://arxiv.org/abs/2406.02633</link>
<guid>https://arxiv.org/abs/2406.02633</guid>
<content:encoded><![CDATA[
arXiv:2406.02633v2 Announce Type: replace-cross 
Abstract: Motivated by the problem of detecting AI-generated text, we consider the problem of watermarking the output of language models with provable guarantees. We aim for watermarks which satisfy: (a) undetectability, a cryptographic notion introduced by Christ, Gunn & Zamir (2024) which stipulates that it is computationally hard to distinguish watermarked language model outputs from the model's actual output distribution; and (b) robustness to channels which introduce a constant fraction of adversarial insertions, substitutions, and deletions to the watermarked text. Earlier schemes could only handle stochastic substitutions and deletions, and thus we are aiming for a more natural and appealing robustness guarantee that holds with respect to edit distance.
  Our main result is a watermarking scheme which achieves both undetectability and robustness to edits when the alphabet size for the language model is allowed to grow as a polynomial in the security parameter. To derive such a scheme, we follow an approach introduced by Christ & Gunn (2024), which proceeds via first constructing pseudorandom codes satisfying undetectability and robustness properties analogous to those above; our key idea is to handle adversarial insertions and deletions by interpreting the symbols as indices into the codeword, which we call indexing pseudorandom codes. Additionally, our codes rely on weaker computational assumptions than used in previous work. Then we show that there is a generic transformation from such codes over large alphabets to watermarking schemes for arbitrary language models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Empirical analysis of binding precedent efficiency in Brazilian Supreme Court via case classification</title>
<link>https://arxiv.org/abs/2407.07004</link>
<guid>https://arxiv.org/abs/2407.07004</guid>
<content:encoded><![CDATA[
arXiv:2407.07004v3 Announce Type: replace-cross 
Abstract: Binding precedents (s\'umulas vinculantes) constitute a juridical instrument unique to the Brazilian legal system and whose objectives include the protection of the Federal Supreme Court against repetitive demands. Studies of the effectiveness of these instruments in decreasing the Court's exposure to similar cases, however, indicate that they tend to fail in such a direction, with some of the binding precedents seemingly creating new demands. We empirically assess the legal impact of five binding precedents, 11, 14, 17, 26, and 37, at the highest Court level through their effects on the legal subjects they address. This analysis is only possible through the comparison of the Court's ruling about the precedents' themes before they are created, which means that these decisions should be detected through techniques of Similar Case Retrieval, which we tackle from the angle of Case Classification. The contributions of this article are therefore twofold: on the mathematical side, we compare the use of different methods of Natural Language Processing -- TF-IDF, LSTM, Longformer, and regex -- for Case Classification, whereas on the legal side, we contrast the inefficiency of these binding precedents with a set of hypotheses that may justify their repeated usage. We observe that the TF-IDF models performed slightly better than LSTM and Longformer when compared through common metrics; however, the deep learning models were able to detect certain important legal events that TF-IDF missed. On the legal side, we argue that the reasons for binding precedents to fail in responding to repetitive demand are heterogeneous and case-dependent, making it impossible to single out a specific cause. We identify five main hypotheses, which are found in different combinations in each of the precedents studied.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Infinite-dimensional Mahalanobis Distance with Applications to Kernelized Novelty Detection</title>
<link>https://arxiv.org/abs/2407.11873</link>
<guid>https://arxiv.org/abs/2407.11873</guid>
<content:encoded><![CDATA[
arXiv:2407.11873v2 Announce Type: replace-cross 
Abstract: The Mahalanobis distance is a classical tool used to measure the covariance-adjusted distance between points in $\bbR^d$. In this work, we extend the concept of Mahalanobis distance to separable Banach spaces by reinterpreting it as a Cameron-Martin norm associated with a probability measure. This approach leads to a basis-free, data-driven notion of anomaly distance through the so-called variance norm, which can naturally be estimated using empirical measures of a sample. Our framework generalizes the classical $\bbR^d$, functional $(L^2[0,1])^d$, and kernelized settings; importantly, it incorporates non-injective covariance operators. We prove that the variance norm is invariant under invertible bounded linear transformations of the data, extending previous results which are limited to unitary operators. In the Hilbert space setting, we connect the variance norm to the RKHS of the covariance operator and establish consistency and convergence results for estimation using empirical measures. Using the variance norm, we introduce the notion of a kernelized nearest-neighbour Mahalanobis distance. In an empirical study on 12 real-world data sets, we demonstrate that the kernelized nearest-neighbour Mahalanobis distance outperforms the traditional kernelized Mahalanobis distance for multivariate time series novelty detection, using state-of-the-art time series kernels such as the signature, global alignment, and Volterra reservoir kernels.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Base and Exponent Prediction in Mathematical Expressions using Multi-Output CNN</title>
<link>https://arxiv.org/abs/2407.14967</link>
<guid>https://arxiv.org/abs/2407.14967</guid>
<content:encoded><![CDATA[
arXiv:2407.14967v2 Announce Type: replace-cross 
Abstract: The use of neural networks and deep learning techniques in image processing has significantly advanced the field, enabling highly accurate recognition results. However, achieving high recognition rates often necessitates complex network models, which can be challenging to train and require substantial computational resources. This research presents a simplified yet effective approach to predicting both the base and exponent from images of mathematical expressions using a multi-output Convolutional Neural Network (CNN). The model is trained on 10,900 synthetically generated images containing exponent expressions, incorporating random noise, font size variations, and blur intensity to simulate real-world conditions. The proposed CNN model demonstrates robust performance with efficient training time. The experimental results indicate that the model achieves high accuracy in predicting the base and exponent values, proving the efficacy of this approach in handling noisy and varied input images.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LAMBDA: A Large Model Based Data Agent</title>
<link>https://arxiv.org/abs/2407.17535</link>
<guid>https://arxiv.org/abs/2407.17535</guid>
<content:encoded><![CDATA[
arXiv:2407.17535v3 Announce Type: replace-cross 
Abstract: We introduce LArge Model Based Data Agent (LAMBDA), a novel open-source, code-free multi-agent data analysis system that leverages the power of large language models. LAMBDA is designed to address data analysis challenges in data-driven applications through innovatively designed data agents using natural language. At the core of LAMBDA are two key agent roles: the programmer and the inspector, which are engineered to work together seamlessly. Specifically, the programmer generates code based on the user's instructions and domain-specific knowledge, while the inspector debugs the code when necessary. To ensure robustness and handle adverse scenarios, LAMBDA features a user interface that allows direct user intervention. Moreover, LAMBDA can flexibly integrate external models and algorithms through our proposed Knowledge Integration Mechanism, catering to the needs of customized data analysis. LAMBDA has demonstrated strong performance on various data analysis tasks. It has the potential to enhance data analysis paradigms by seamlessly integrating human and artificial intelligence, making it more accessible, effective, and efficient for users from diverse backgrounds. The strong performance of LAMBDA in solving data analysis problems is demonstrated using real-world data examples. The code for LAMBDA is available at https://github.com/AMA-CMFAI/LAMBDA and videos of three case studies can be viewed at https://www.polyu.edu.hk/ama/cmfai/lambda.html.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Eigenvalue Theorems for Tensor Analysis in Multi-Modal Data Fusion</title>
<link>https://arxiv.org/abs/2409.09392</link>
<guid>https://arxiv.org/abs/2409.09392</guid>
<content:encoded><![CDATA[
arXiv:2409.09392v3 Announce Type: replace-cross 
Abstract: This paper presents a novel framework for tensor eigenvalue analysis in the context of multi-modal data fusion, leveraging topological invariants such as Betti numbers. Traditional approaches to tensor eigenvalue analysis often extend matrix theory, whereas this work introduces a topological perspective to enhance the understanding of tensor structures. By establishing new theorems that link eigenvalues to topological features, the proposed framework provides deeper insights into the latent structure of data, improving both interpretability and robustness. Applications in data fusion demonstrate the theoretical and practical significance of this approach, with potential for broad impact in machine learning and data science.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Within-class Variation Issue in Alzheimer's Disease Detection</title>
<link>https://arxiv.org/abs/2409.16322</link>
<guid>https://arxiv.org/abs/2409.16322</guid>
<content:encoded><![CDATA[
arXiv:2409.16322v2 Announce Type: replace-cross 
Abstract: Alzheimer's Disease (AD) detection employs machine learning classification models to distinguish between individuals with AD and those without. Different from conventional classification tasks, we identify within-class variation as a critical challenge in AD detection: individuals with AD exhibit a spectrum of cognitive impairments. Therefore, simplistic binary AD classification may overlook two crucial aspects: within-class heterogeneity and instance-level imbalance. In this work, we found using a sample score estimator can generate sample-specific soft scores aligning with cognitive scores. We subsequently propose two simple yet effective methods: Soft Target Distillation (SoTD) and Instance-level Re-balancing (InRe), targeting two problems respectively. Based on the ADReSS and CU-MARVEL corpora, we demonstrated and analyzed the advantages of the proposed approaches in detection performance. These findings provide insights for developing robust and reliable AD detection models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Shielded Diffusion: Generating Novel and Diverse Images using Sparse Repellency</title>
<link>https://arxiv.org/abs/2410.06025</link>
<guid>https://arxiv.org/abs/2410.06025</guid>
<content:encoded><![CDATA[
arXiv:2410.06025v3 Announce Type: replace-cross 
Abstract: The adoption of text-to-image diffusion models raises concerns over reliability, drawing scrutiny under the lens of various metrics like calibration, fairness, or compute efficiency. We focus in this work on two issues that arise when deploying these models: a lack of diversity when prompting images, and a tendency to recreate images from the training set. To solve both problems, we propose a method that coaxes the sampled trajectories of pretrained diffusion models to land on images that fall outside of a reference set. We achieve this by adding repellency terms to the diffusion SDE throughout the generation trajectory, which are triggered whenever the path is expected to land too closely to an image in the shielded reference set. Our method is sparse in the sense that these repellency terms are zero and inactive most of the time, and even more so towards the end of the generation trajectory. Our method, named SPELL for sparse repellency, can be used either with a static reference set that contains protected images, or dynamically, by updating the set at each timestep with the expected images concurrently generated within a batch, and with the images of previously generated batches. We show that adding SPELL to popular diffusion models improves their diversity while impacting their FID only marginally, and performs comparatively better than other recent training-free diversity methods. We also demonstrate how SPELL can ensure a shielded generation away from a very large set of protected images by considering all 1.2M images from ImageNet as the protected set.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Privacy for Free in the Overparameterized Regime</title>
<link>https://arxiv.org/abs/2410.14787</link>
<guid>https://arxiv.org/abs/2410.14787</guid>
<content:encoded><![CDATA[
arXiv:2410.14787v2 Announce Type: replace-cross 
Abstract: Differentially private gradient descent (DP-GD) is a popular algorithm to train deep learning models with provable guarantees on the privacy of the training data. In the last decade, the problem of understanding its performance cost with respect to standard GD has received remarkable attention from the research community, which formally derived upper bounds on the excess population risk $R_{P}$ in different learning settings. However, existing bounds typically degrade with over-parameterization, i.e., as the number of parameters $p$ gets larger than the number of training samples $n$ -- a regime which is ubiquitous in current deep-learning practice. As a result, the lack of theoretical insights leaves practitioners without clear guidance, leading some to reduce the effective number of trainable parameters to improve performance, while others use larger models to achieve better results through scale. In this work, we show that in the popular random features model with quadratic loss, for any sufficiently large $p$, privacy can be obtained for free, i.e., $\left|R_{P} \right| = o(1)$, not only when the privacy parameter $\varepsilon$ has constant order, but also in the strongly private setting $\varepsilon = o(1)$. This challenges the common wisdom that over-parameterization inherently hinders performance in private learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneously Solving FBSDEs and their Associated Semilinear Elliptic PDEs with Small Neural Operators</title>
<link>https://arxiv.org/abs/2410.14788</link>
<guid>https://arxiv.org/abs/2410.14788</guid>
<content:encoded><![CDATA[
arXiv:2410.14788v2 Announce Type: replace-cross 
Abstract: Forward-backwards stochastic differential equations (FBSDEs) play an important role in optimal control, game theory, economics, mathematical finance, and in reinforcement learning. Unfortunately, the available FBSDE solvers operate on \textit{individual} FBSDEs, meaning that they cannot provide a computationally feasible strategy for solving large families of FBSDEs, as these solvers must be re-run several times. \textit{Neural operators} (NOs) offer an alternative approach for \textit{simultaneously solving} large families of decoupled FBSDEs by directly approximating the solution operator mapping \textit{inputs:} terminal conditions and dynamics of the backwards process to \textit{outputs:} solutions to the associated FBSDE. Though universal approximation theorems (UATs) guarantee the existence of such NOs, these NOs are unrealistically large. Upon making only a few simple theoretically-guided tweaks to the standard convolutional NO build, we confirm that ``small'' NOs can uniformly approximate the solution operator to structured families of FBSDEs with random terminal time, uniformly on suitable compact sets determined by Sobolev norms using a logarithmic depth, a constant width, and a polynomial rank in the reciprocal approximation error.
  This result is rooted in our second result, and main contribution to the NOs for PDE literature, showing that our convolutional NOs of similar depth and width but grow only \textit{quadratically} (at a dimension-free rate) when uniformly approximating the solution operator of the associated class of semilinear Elliptic PDEs to these families of FBSDEs. A key insight into how NOs work we uncover is that the convolutional layers of our NO can approximately implement the fixed point iteration used to prove the existence of a unique solution to these semilinear Elliptic PDEs.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Novel Characterization of the Population Area Under the Risk Coverage Curve (AURC) and Rates of Finite Sample Estimators</title>
<link>https://arxiv.org/abs/2410.15361</link>
<guid>https://arxiv.org/abs/2410.15361</guid>
<content:encoded><![CDATA[
arXiv:2410.15361v3 Announce Type: replace-cross 
Abstract: The selective classifier (SC) has been proposed for rank based uncertainty thresholding, which could have applications in safety critical areas such as medical diagnostics, autonomous driving, and the justice system. The Area Under the Risk-Coverage Curve (AURC) has emerged as the foremost evaluation metric for assessing the performance of SC systems. In this work, we present a formal statistical formulation of population AURC, presenting an equivalent expression that can be interpreted as a reweighted risk function. Through Monte Carlo methods, we derive empirical AURC plug-in estimators for finite sample scenarios. The weight estimators associated with these plug-in estimators are shown to be consistent, with low bias and tightly bounded mean squared error (MSE). The plug-in estimators are proven to converge at a rate of $\mathcal{O}(\sqrt{\ln(n)/n})$ demonstrating statistical consistency. We empirically validate the effectiveness of our estimators through experiments across multiple datasets, model architectures, and confidence score functions (CSFs), demonstrating consistency and effectiveness in fine-tuning AURC performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Statistical Inference for Temporal Difference Learning with Linear Function Approximation</title>
<link>https://arxiv.org/abs/2410.16106</link>
<guid>https://arxiv.org/abs/2410.16106</guid>
<content:encoded><![CDATA[
arXiv:2410.16106v3 Announce Type: replace-cross 
Abstract: We investigate the statistical properties of Temporal Difference (TD) learning with Polyak-Ruppert averaging, arguably one of the most widely used algorithms in reinforcement learning, for the task of estimating the parameters of the optimal linear approximation to the value function. We make three significant contributions that improve the current state-of-the-art results: (i) we derive sharper high probability convergence guarantee that depend explicitly on the asymptotic variance and hold under weaker conditions than those normally assumed; (ii) we establish refined high-dimensional Berry-Esseen bounds over the class of convex sets, achieving faster rates than those previously established in the literature, and (iii) we propose and analyze a novel, computationally efficient online plug-in estimator of the asymptotic covariance matrix.These results enable the construction of confidence regions and simultaneous confidence intervals for the linear parameters of the value function approximation, with guaranteed finite-sample coverage. We demonstrate the applicability of our theoretical findings through numerical experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Approximation of Mean-Field Models via Transformers</title>
<link>https://arxiv.org/abs/2410.16295</link>
<guid>https://arxiv.org/abs/2410.16295</guid>
<content:encoded><![CDATA[
arXiv:2410.16295v2 Announce Type: replace-cross 
Abstract: This paper investigates the use of transformers to approximate the mean-field dynamics of interacting particle systems exhibiting collective behavior. Such systems are fundamental in modeling phenomena across physics, biology, and engineering, including opinion formation, biological networks, and swarm robotics. The key characteristic of these systems is that the particles are indistinguishable, leading to permutation-equivariant dynamics. First, we empirically demonstrate that transformers are well-suited for approximating a variety of mean field models, including the Cucker-Smale model for flocking and milling, and the mean-field system for training two-layer neural networks. We validate our numerical experiments via mathematical theory. Specifically, we prove that if a finite-dimensional transformer effectively approximates the finite-dimensional vector field governing the particle system, then the $L_2$ distance between the \textit{expected transformer} and the infinite-dimensional mean-field vector field can be uniformly bounded by a function of the number of particles observed during training. Leveraging this result, we establish theoretical bounds on the distance between the true mean-field dynamics and those obtained using the transformer.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Stochastic Approximation Approach for Efficient Decentralized Optimization on Random Networks</title>
<link>https://arxiv.org/abs/2410.18774</link>
<guid>https://arxiv.org/abs/2410.18774</guid>
<content:encoded><![CDATA[
arXiv:2410.18774v2 Announce Type: replace-cross 
Abstract: A challenging problem in decentralized optimization is to develop algorithms with fast convergence on random and time varying topologies under unreliable and bandwidth-constrained communication network. This paper studies a stochastic approximation approach with a Fully Stochastic Primal Dual Algorithm (FSPDA) framework. Our framework relies on a novel observation that randomness in time varying topology can be incorporated in a stochastic augmented Lagrangian formulation, whose expected value admits saddle points that coincide with stationary solutions of the decentralized optimization problem. With the FSPDA framework, we develop two new algorithms supporting efficient sparsified communication on random time varying topologies -- FSPDA-SA allows agents to execute multiple local gradient steps depending on the time varying topology to accelerate convergence, and FSPDA-STORM further incorporates a variance reduction step to improve sample complexity. For problems with smooth (possibly non-convex) objective function, within $T$ iterations, we show that FSPDA-SA (resp. FSPDA-STORM) finds an $\mathcal{O}( 1/\sqrt{T} )$-stationary (resp. $\mathcal{O}( 1/T^{2/3} )$) solution. Numerical experiments show the benefits of the FSPDA algorithms.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Models as Cartoonists: The Curious Case of High Density Regions</title>
<link>https://arxiv.org/abs/2411.01293</link>
<guid>https://arxiv.org/abs/2411.01293</guid>
<content:encoded><![CDATA[
arXiv:2411.01293v4 Announce Type: replace-cross 
Abstract: We investigate what kind of images lie in the high-density regions of diffusion models. We introduce a theoretical mode-tracking process capable of pinpointing the exact mode of the denoising distribution, and we propose a practical high-density sampler that consistently generates images of higher likelihood than usual samplers. Our empirical findings reveal the existence of significantly higher likelihood samples that typical samplers do not produce, often manifesting as cartoon-like drawings or blurry images depending on the noise level. Curiously, these patterns emerge in datasets devoid of such examples. We also present a novel approach to track sample likelihoods in diffusion SDEs, which remarkably incurs no additional computational cost. Code is available at https://github.com/Aalto-QuML/high-density-diffusion.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Head Knowledge Distillation: Enhancing Logits Utilization with an Auxiliary Head</title>
<link>https://arxiv.org/abs/2411.08937</link>
<guid>https://arxiv.org/abs/2411.08937</guid>
<content:encoded><![CDATA[
arXiv:2411.08937v2 Announce Type: replace-cross 
Abstract: Traditional knowledge distillation focuses on aligning the student's predicted probabilities with both ground-truth labels and the teacher's predicted probabilities. However, the transition to predicted probabilities from logits would obscure certain indispensable information. To address this issue, it is intuitive to additionally introduce a logit-level loss function as a supplement to the widely used probability-level loss function, for exploiting the latent information of logits. Unfortunately, we empirically find that the amalgamation of the newly introduced logit-level loss and the previous probability-level loss will lead to performance degeneration, even trailing behind the performance of employing either loss in isolation. We attribute this phenomenon to the collapse of the classification head, which is verified by our theoretical analysis based on the neural collapse theory. Specifically, the gradients of the two loss functions exhibit contradictions in the linear classifier yet display no such conflict within the backbone. Drawing from the theoretical analysis, we propose a novel method called dual-head knowledge distillation, which partitions the linear classifier into two classification heads responsible for different losses, thereby preserving the beneficial effects of both losses on the backbone while eliminating adverse influences on the classification head. Extensive experiments validate that our method can effectively exploit the information inside the logits and achieve superior performance against state-of-the-art counterparts. Our code is available at: https://github.com/penghui-yang/DHKD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LL\"aMmlein: Compact and Competitive German-Only Language Models from Scratch</title>
<link>https://arxiv.org/abs/2411.11171</link>
<guid>https://arxiv.org/abs/2411.11171</guid>
<content:encoded><![CDATA[
arXiv:2411.11171v4 Announce Type: replace-cross 
Abstract: We create two German-only decoder models, LL\"aMmlein 120M and 1B, transparently from scratch and publish them, along with the training data, for the German NLP research community to use. The model training involved several key steps, including extensive data preprocessing, the creation of a custom German tokenizer, the training itself, as well as the evaluation of the final models on various benchmarks. Throughout the training process, multiple checkpoints were saved and analyzed using the SuperGLEBer benchmark to monitor the models' learning dynamics. Compared to state-of-the-art models on the SuperGLEBer benchmark, both LL\"aMmlein models performed competitively, consistently matching or surpassing models with similar parameter sizes. The results show that the models' quality scales with size as expected, but performance improvements on some tasks plateaued early, offering valuable insights into resource allocation for future model development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Efficient Robot Learning in Supervised Effect Prediction Tasks</title>
<link>https://arxiv.org/abs/2412.02331</link>
<guid>https://arxiv.org/abs/2412.02331</guid>
<content:encoded><![CDATA[
arXiv:2412.02331v2 Announce Type: replace-cross 
Abstract: In self-supervised robotic learning, agents acquire data through active interaction with their environment, incurring costs such as energy use, human oversight, and experimental time. To mitigate these, sample-efficient exploration is essential. While intrinsic motivation (IM) methods like learning progress (LP) are widely used in robotics, and active learning (AL) is well established for classification in machine learning, few frameworks address continuous, high-dimensional regression tasks typical of world model learning. We propose MUSEL (Model Uncertainty for Sample-Efficient Learning), a novel AL framework tailored for regression tasks in robotics, such as action-effect prediction. MUSEL introduces a model uncertainty metric that combines total predictive uncertainty, learning progress, and input diversity to guide data acquisition. We validate our approach using a Stochastic Variational Deep Kernel Learning (SVDKL) model in two robotic tabletop tasks. Experimental results demonstrate that MUSEL improves both learning accuracy and sample efficiency, validating its effectiveness in learning action effects and selecting informative samples.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Zero-Shot Mono-to-Binaural Speech Synthesis</title>
<link>https://arxiv.org/abs/2412.08356</link>
<guid>https://arxiv.org/abs/2412.08356</guid>
<content:encoded><![CDATA[
arXiv:2412.08356v2 Announce Type: replace-cross 
Abstract: We present ZeroBAS, a neural method to synthesize binaural audio from monaural audio recordings and positional information without training on any binaural data. To our knowledge, this is the first published zero-shot neural approach to mono-to-binaural audio synthesis. Specifically, we show that a parameter-free geometric time warping and amplitude scaling based on source location suffices to get an initial binaural synthesis that can be refined by iteratively applying a pretrained denoising vocoder. Furthermore, we find this leads to generalization across room conditions, which we measure by introducing a new dataset, TUT Mono-to-Binaural, to evaluate state-of-the-art monaural-to-binaural synthesis methods on unseen conditions. Our zero-shot method is perceptually on-par with the performance of supervised methods on the standard mono-to-binaural dataset, and even surpasses them on our out-of-distribution TUT Mono-to-Binaural dataset. Our results highlight the potential of pretrained generative audio models and zero-shot learning to unlock robust binaural audio synthesis.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preference Adaptive and Sequential Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2412.10419</link>
<guid>https://arxiv.org/abs/2412.10419</guid>
<content:encoded><![CDATA[
arXiv:2412.10419v2 Announce Type: replace-cross 
Abstract: We address the problem of interactive text-to-image (T2I) generation, designing a reinforcement learning (RL) agent which iteratively improves a set of generated images for a user through a sequence of prompt expansions. Using human raters, we create a novel dataset of sequential preferences, which we leverage, together with large-scale open-source (non-sequential) datasets. We construct user-preference and user-choice models using an EM strategy and identify varying user preference types. We then leverage a large multimodal language model (LMM) and a value-based RL approach to suggest an adaptive and diverse slate of prompt expansions to the user. Our Preference Adaptive and Sequential Text-to-image Agent (PASTA) extends T2I models with adaptive multi-turn capabilities, fostering collaborative co-creation and addressing uncertainty or underspecification in a user's intent. We evaluate PASTA using human raters, showing significant improvement compared to baseline methods. We also open-source our sequential rater dataset and simulated user-rater interactions to support future research in user-centric multi-turn T2I systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Core Context Aware Transformers for Long Context Language Modeling</title>
<link>https://arxiv.org/abs/2412.12465</link>
<guid>https://arxiv.org/abs/2412.12465</guid>
<content:encoded><![CDATA[
arXiv:2412.12465v2 Announce Type: replace-cross 
Abstract: Transformer-based Large Language Models (LLMs) have exhibited remarkable success in extensive tasks primarily attributed to self-attention mechanism, which requires a token to consider all preceding tokens as its context to compute attention. However, when the context length L becomes very large (e.g., 128K), the amount of potentially redundant information in the context tends to increase. The redundant context not only hampers the modeling representation performance but also incurs unnecessary computational and storage overhead. In this paper, we propose a plug-and-play Core Context Aware (CCA) Attention for efficient long-context modeling, comprising two complementary modules: 1) Globality-aware pooling module groups input tokens and dynamically compresses each group into one core token based on their significance. In this way, our method automatically focuses and strengthens core context while diminishing redundancy during the learning process, leading to effective long-term dependency modeling. 2) Locality-preserving module incorporates neighboring tokens to preserve local context for detailed representation. Notably, our CCA-Attention is able to replace the self-attention module in existing LLMs with minimal fine-tuning cost. Extensive experimental results show the superiority of our method in both long-context modeling and computational efficiency over state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Synthesize Text Data without Model Collapse?</title>
<link>https://arxiv.org/abs/2412.14689</link>
<guid>https://arxiv.org/abs/2412.14689</guid>
<content:encoded><![CDATA[
arXiv:2412.14689v3 Announce Type: replace-cross 
Abstract: Model collapse in synthetic data indicates that iterative training on self-generated data leads to a gradual decline in performance. With the proliferation of AI models, synthetic data will fundamentally reshape the web data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend of synthetic and human-produced data. In this paper, we focus on two questions: what is the impact of synthetic data on language model training, and how to synthesize data without model collapse? We first pre-train language models across different proportions of synthetic data, revealing a negative correlation between the proportion of synthetic data and model performance. We further conduct statistical analysis on synthetic data to uncover distributional shift phenomenon and over-concentration of n-gram features. Inspired by the above findings, we propose token editing on human-produced data to obtain semi-synthetic data. As a proof of concept, we theoretically demonstrate that token-level editing can prevent model collapse, as the test error is constrained by a finite upper bound. We conduct extensive experiments on pre-training from scratch, continual pre-training, and supervised fine-tuning. The results validate our theoretical proof that token-level editing improves model performance.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting In-Context Learning with Long Context Language Models</title>
<link>https://arxiv.org/abs/2412.16926</link>
<guid>https://arxiv.org/abs/2412.16926</guid>
<content:encoded><![CDATA[
arXiv:2412.16926v3 Announce Type: replace-cross 
Abstract: In-Context Learning (ICL) is a technique by which language models make predictions based on examples provided in their input context. Previously, their context window size imposed a limit on the number of examples that can be shown, making example selection techniques crucial for identifying the maximally effective set of examples. However, the recent advent of Long Context Language Models (LCLMs) has significantly increased the number of examples that can be included in context, raising an important question of whether ICL performance in a many-shot regime is still sensitive to the method of sample selection. To answer this, we revisit these approaches in the context of LCLMs through extensive experiments on 18 datasets spanning 4 tasks. Surprisingly, we observe that sophisticated example selection techniques do not yield significant improvements over a simple random sample selection method. Instead, we discover that the advent of LCLMs has fundamentally shifted the challenge of ICL from that of selecting the most effective examples to that of collecting sufficient examples to fill the context window. Specifically, in certain datasets, including all available examples does not fully utilize the context window; however, by augmenting the examples in context with a simple data augmentation approach, we substantially improve ICL performance by 5%.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum framework for Reinforcement Learning: Integrating Markov decision process, quantum arithmetic, and trajectory search</title>
<link>https://arxiv.org/abs/2412.18208</link>
<guid>https://arxiv.org/abs/2412.18208</guid>
<content:encoded><![CDATA[
arXiv:2412.18208v3 Announce Type: replace-cross 
Abstract: This paper introduces a quantum framework for addressing reinforcement learning (RL) tasks, grounded in the quantum principles and leveraging a fully quantum model of the classical Markov decision process (MDP). By employing quantum concepts and a quantum search algorithm, this work presents the implementation and optimization of the agent-environment interactions entirely within the quantum domain, eliminating reliance on classical computations. Key contributions include the quantum-based state transitions, return calculation, and trajectory search mechanism that utilize quantum principles to demonstrate the realization of RL processes through quantum phenomena. The implementation emphasizes the fundamental role of quantum superposition in enhancing computational efficiency for RL tasks. Results demonstrate the capacity of a quantum model to achieve quantum enhancement in RL, highlighting the potential of fully quantum implementations in decision-making tasks. This work not only underscores the applicability of quantum computing in machine learning but also contributes to the field of quantum reinforcement learning (QRL) by offering a robust framework for understanding and exploiting quantum computing in RL systems.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Outlier-Robust Linear System Identification Under Heavy-tailed Noise</title>
<link>https://arxiv.org/abs/2501.00421</link>
<guid>https://arxiv.org/abs/2501.00421</guid>
<content:encoded><![CDATA[
arXiv:2501.00421v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating the state transition matrix of a linear time-invariant (LTI) system, given access to multiple independent trajectories sampled from the system. Several recent papers have conducted a non-asymptotic analysis of this problem, relying crucially on the assumption that the process noise is either Gaussian or sub-Gaussian, i.e., "light-tailed". In sharp contrast, we work under a significantly weaker noise model, assuming nothing more than the existence of the fourth moment of the noise distribution. For this setting, we provide the first set of results demonstrating that one can obtain sample-complexity bounds for linear system identification that are nearly of the same order as under sub-Gaussian noise. To achieve such results, we develop a novel robust system identification algorithm that relies on constructing multiple weakly-concentrated estimators, and then boosting their performance using suitable tools from high-dimensional robust statistics. Interestingly, our analysis reveals how the kurtosis of the noise distribution, a measure of heavy-tailedness, affects the number of trajectories needed to achieve desired estimation error bounds. Finally, we show that our algorithm and analysis technique can be easily extended to account for scenarios where an adversary can arbitrarily corrupt a small fraction of the collected trajectory data. Our work takes the first steps towards building a robust statistical learning theory for control under non-ideal assumptions on the data-generating process.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FitCF: A Framework for Automatic Feature Importance-guided Counterfactual Example Generation</title>
<link>https://arxiv.org/abs/2501.00777</link>
<guid>https://arxiv.org/abs/2501.00777</guid>
<content:encoded><![CDATA[
arXiv:2501.00777v3 Announce Type: replace-cross 
Abstract: Counterfactual examples are widely used in natural language processing (NLP) as valuable data to improve models, and in explainable artificial intelligence (XAI) to understand model behavior. The automated generation of counterfactual examples remains a challenging task even for large language models (LLMs), despite their impressive performance on many tasks. In this paper, we first introduce ZeroCF, a faithful approach for leveraging important words derived from feature attribution methods to generate counterfactual examples in a zero-shot setting. Second, we present a new framework, FitCF, which further verifies aforementioned counterfactuals by label flip verification and then inserts them as demonstrations for few-shot prompting, outperforming two state-of-the-art baselines. Through ablation studies, we identify the importance of each of FitCF's core components in improving the quality of counterfactuals, as assessed through flip rate, perplexity, and similarity measures. Furthermore, we show the effectiveness of LIME and Integrated Gradients as backbone attribution methods for FitCF and find that the number of demonstrations has the largest effect on performance. Finally, we reveal a strong correlation between the faithfulness of feature attribution scores and the quality of generated counterfactuals, which we hope will serve as an important finding for future research in this direction.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models</title>
<link>https://arxiv.org/abs/2501.03124</link>
<guid>https://arxiv.org/abs/2501.03124</guid>
<content:encoded><![CDATA[
arXiv:2501.03124v4 Announce Type: replace-cross 
Abstract: Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Adversarial Post-Training for One-Step Video Generation</title>
<link>https://arxiv.org/abs/2501.08316</link>
<guid>https://arxiv.org/abs/2501.08316</guid>
<content:encoded><![CDATA[
arXiv:2501.08316v2 Announce Type: replace-cross 
Abstract: The diffusion models are widely used for image and video generation, but their iterative generation process is slow and expansive. While existing distillation approaches have demonstrated the potential for one-step generation in the image domain, they still suffer from significant quality degradation. In this work, we propose Adversarial Post-Training (APT) against real data following diffusion pre-training for one-step video generation. To improve the training stability and quality, we introduce several improvements to the model architecture and training procedures, along with an approximated R1 regularization objective. Empirically, our experiments show that our adversarial post-trained model, Seaweed-APT, can generate 2-second, 1280x720, 24fps videos in real time using a single forward evaluation step. Additionally, our model is capable of generating 1024px images in a single step, achieving quality comparable to state-of-the-art methods.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kimi k1.5: Scaling Reinforcement Learning with LLMs</title>
<link>https://arxiv.org/abs/2501.12599</link>
<guid>https://arxiv.org/abs/2501.12599</guid>
<content:encoded><![CDATA[
arXiv:2501.12599v3 Announce Type: replace-cross 
Abstract: Language model pretraining with next token prediction has proved effective for scaling compute but is limited to the amount of available training data. Scaling reinforcement learning (RL) unlocks a new axis for the continued improvement of artificial intelligence, with the promise that large language models (LLMs) can scale their training data by learning to explore with rewards. However, prior published work has not produced competitive results. In light of this, we report on the training practice of Kimi k1.5, our latest multi-modal LLM trained with RL, including its RL training techniques, multi-modal data recipes, and infrastructure optimization. Long context scaling and improved policy optimization methods are key ingredients of our approach, which establishes a simplistic, effective RL framework without relying on more complex techniques such as Monte Carlo tree search, value functions, and process reward models. Notably, our system achieves state-of-the-art reasoning performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME, 96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching OpenAI's o1. Moreover, we present effective long2short methods that use long-CoT techniques to improve short-CoT models, yielding state-of-the-art short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and Claude Sonnet 3.5 by a large margin (up to +550%).
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domaino1s: Guiding LLM Reasoning for Explainable Answers in High-Stakes Domains</title>
<link>https://arxiv.org/abs/2501.14431</link>
<guid>https://arxiv.org/abs/2501.14431</guid>
<content:encoded><![CDATA[
arXiv:2501.14431v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are widely applied to downstream domains. However, current LLMs for high-stakes domain tasks, such as financial investment and legal QA, typically generate brief answers without reasoning processes and explanations. This limits users' confidence in making decisions based on their responses. While original CoT shows promise, it lacks self-correction mechanisms during reasoning. This work introduces Domain$o1$s, which enhances LLMs' reasoning capabilities on domain tasks through supervised fine-tuning and tree search. We construct CoT-stock-2k and CoT-legal-2k datasets for fine-tuning models that activate domain-specific reasoning steps based on their judgment. Additionally, we propose Selective Tree Exploration to spontaneously explore solution spaces and sample optimal reasoning paths to improve performance. We also introduce PROOF-Score, a new metric for evaluating domain models' explainability, complementing traditional accuracy metrics with richer assessment dimensions. Extensive experiments on stock investment recommendation and legal reasoning QA tasks demonstrate Domaino1s's leading performance and explainability. Our code is available at https://github.com/Hyalinesky/Domaino1s.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Feature Representation Boosting</title>
<link>https://arxiv.org/abs/2501.18283</link>
<guid>https://arxiv.org/abs/2501.18283</guid>
<content:encoded><![CDATA[
arXiv:2501.18283v3 Announce Type: replace-cross 
Abstract: We introduce Random Feature Representation Boosting (RFRBoost), a novel method for constructing deep residual random feature neural networks (RFNNs) using boosting theory. RFRBoost uses random features at each layer to learn the functional gradient of the network representation, enhancing performance while preserving the convex optimization benefits of RFNNs. In the case of MSE loss, we obtain closed-form solutions to greedy layer-wise boosting with random features. For general loss functions, we show that fitting random feature residual blocks reduces to solving a quadratically constrained least squares problem. Through extensive numerical experiments on tabular datasets for both regression and classification, we show that RFRBoost significantly outperforms RFNNs and end-to-end trained MLP ResNets in the small- to medium-scale regime where RFNNs are typically applied. Moreover, RFRBoost offers substantial computational benefits, and theoretical guarantees stemming from boosting theory.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Heterogeneous Token Overfitting in LLM Knowledge Editing</title>
<link>https://arxiv.org/abs/2502.00602</link>
<guid>https://arxiv.org/abs/2502.00602</guid>
<content:encoded><![CDATA[
arXiv:2502.00602v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have achieved remarkable performance on various natural language tasks. However, they are trained on static corpora and their knowledge can become outdated quickly in the fast-changing world. This motivates the development of knowledge editing (KE) to update specific knowledge in LLMs without changing unrelated others or compromising their pre-trained capabilities. Previous efforts sought to update a small amount of parameters of a LLM and proved effective for making selective updates. Nonetheless, the edited LLM often exhibits degraded ability to reason about the new knowledge. In this work, we identify a key issue: heterogeneous token overfitting (HTO), where the LLM overfits different tokens in the provided knowledge at varying rates. To tackle this, we propose OVERTONE, a token-level smoothing method that mitigates HTO by adaptively refining the target distribution. Theoretically, OVERTONE offers better parameter updates with negligible computation overhead. It also induces an implicit DPO but does not require preference data pairs. Extensive experiments across four editing methods, two LLMs, and diverse scenarios demonstrate the effectiveness and versatility of our method.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoRA-One: One-Step Full Gradient Could Suffice for Fine-Tuning Large Language Models, Provably and Efficiently</title>
<link>https://arxiv.org/abs/2502.01235</link>
<guid>https://arxiv.org/abs/2502.01235</guid>
<content:encoded><![CDATA[
arXiv:2502.01235v2 Announce Type: replace-cross 
Abstract: This paper explores how theory can guide and enhance practical algorithms, using Low-Rank Adaptation (LoRA, Hu et al. 2022) in large language models as a case study. We rigorously prove that, under gradient descent, LoRA adapters align with specific singular subspaces of the one-step full fine-tuning gradient. This result suggests that, by properly initializing the adapters using the one-step full gradient, subspace alignment can be achieved immediately and applicable to both linear and nonlinear models. Building on our theory, we propose a theory-driven algorithm, LoRA-One, where the linear convergence (as well as generalization) is built and incorporating preconditioners theoretically helps mitigate the effects of ill-conditioning. Besides, our theory reveals connections between LoRA-One and other gradient-alignment-based methods, helping to clarify misconceptions in the design of such algorithms. LoRA-One achieves significant empirical improvements over LoRA and its variants across benchmarks in natural language understanding, mathematical reasoning, and code generation. Code is available at: https://github.com/YuanheZ/LoRA-One.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Spurious Correlations in High Dimensional Regression: The Roles of Regularization, Simplicity Bias and Over-Parameterization</title>
<link>https://arxiv.org/abs/2502.01347</link>
<guid>https://arxiv.org/abs/2502.01347</guid>
<content:encoded><![CDATA[
arXiv:2502.01347v2 Announce Type: replace-cross 
Abstract: Learning models have been shown to rely on spurious correlations between non-predictive features and the associated labels in the training data, with negative implications on robustness, bias and fairness. In this work, we provide a statistical characterization of this phenomenon for high-dimensional regression, when the data contains a predictive core feature $x$ and a spurious feature $y$. Specifically, we quantify the amount of spurious correlations $C$ learned via linear regression, in terms of the data covariance and the strength $\lambda$ of the ridge regularization. As a consequence, we first capture the simplicity of $y$ through the spectrum of its covariance, and its correlation with $x$ through the Schur complement of the full data covariance. Next, we prove a trade-off between $C$ and the in-distribution test loss $L$, by showing that the value of $\lambda$ that minimizes $L$ lies in an interval where $C$ is increasing. Finally, we investigate the effects of over-parameterization via the random features model, by showing its equivalence to regularized linear regression. Our theoretical results are supported by numerical experiments on Gaussian, Color-MNIST, and CIFAR-10 datasets.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LV-XAttn: Distributed Cross-Attention for Long Visual Inputs in Multimodal Large Language Models</title>
<link>https://arxiv.org/abs/2502.02406</link>
<guid>https://arxiv.org/abs/2502.02406</guid>
<content:encoded><![CDATA[
arXiv:2502.02406v3 Announce Type: replace-cross 
Abstract: Cross-attention is commonly adopted in multimodal large language models (MLLMs) for integrating visual information into the language backbone. However, in applications with large visual inputs, such as video understanding, processing a large number of visual tokens in cross-attention layers leads to high memory demands and often necessitates distributed computation across multiple GPUs. Existing distributed attention mechanisms face significant communication overheads, making cross-attention layers a critical bottleneck for efficient training and inference of MLLMs. To address this, we propose LV-XAttn, a distributed, exact cross-attention mechanism with minimal communication overhead. We observe that in applications involving large visual inputs, the size of the query block is typically much smaller than that of the key-value blocks. Thus, in LV-XAttn we keep the large key-value blocks locally on each GPU and exchange smaller query blocks across GPUs. We also introduce an efficient activation recomputation technique to support longer visual context. We theoretically analyze the communication benefits of LV-XAttn and show that it can achieve speedups for a wide range of models. Our evaluations with Llama 3-V, mPLUG-Owl3 and OpenFlamingo models find that LV-XAttn achieves up to 10.62$\times$ end-to-end speedup compared to existing approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Kernels to Features: A Multi-Scale Adaptive Theory of Feature Learning</title>
<link>https://arxiv.org/abs/2502.03210</link>
<guid>https://arxiv.org/abs/2502.03210</guid>
<content:encoded><![CDATA[
arXiv:2502.03210v2 Announce Type: replace-cross 
Abstract: Feature learning in neural networks is crucial for their expressive power and inductive biases, motivating various theoretical approaches. Some approaches describe network behavior after training through a change in kernel scale from initialization, resulting in a generalization power comparable to a Gaussian process. Conversely, in other approaches training results in the adaptation of the kernel to the data, involving directional changes to the kernel. The relationship and respective strengths of these two views have so far remained unresolved. This work presents a theoretical framework of multi-scale adaptive feature learning bridging these two views. Using methods from statistical mechanics, we derive analytical expressions for network output statistics which are valid across scaling regimes and in the continuum between them. A systematic expansion of the network's probability distribution reveals that mean-field scaling requires only a saddle-point approximation, while standard scaling necessitates additional correction terms. Remarkably, we find across regimes that kernel adaptation can be reduced to an effective kernel rescaling when predicting the mean network output in the special case of a linear network. However, for linear and non-linear networks, the multi-scale adaptive approach captures directional feature learning effects, providing richer insights than what could be recovered from a rescaling of the kernel alone.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond External Monitors: Enhancing Transparency of Large Language Models for Easier Monitoring</title>
<link>https://arxiv.org/abs/2502.05242</link>
<guid>https://arxiv.org/abs/2502.05242</guid>
<content:encoded><![CDATA[
arXiv:2502.05242v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are becoming increasingly capable, but the mechanisms of their thinking and decision-making process remain unclear. Chain-of-thoughts (CoTs) have been commonly utilized to monitor LLMs, but this strategy fails to accurately reflect LLMs' thinking process. Techniques based on LLMs' hidden representations provide an inner perspective to monitor their latent thinking. However, previous methods only try to develop external monitors instead of making LLMs themselves easier to monitor. In this paper, we propose a novel method TELLME, improving the transparency of LLMs and helping monitors identify unsuitable and sensitive behaviors. Furthermore, we showcase the applications of TELLME on trustworthiness tasks (\eg, safety risks monitoring tasks and detoxification tasks), where LLMs achieve consistent improvement in transparency and task performance. More crucially, we theoretically analyze the improvement of TELLME on LLMs' generalization ability through optimal transport theory.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial Association</title>
<link>https://arxiv.org/abs/2502.06067</link>
<guid>https://arxiv.org/abs/2502.06067</guid>
<content:encoded><![CDATA[
arXiv:2502.06067v2 Announce Type: replace-cross 
Abstract: Estimating associations between spatial covariates and responses - rather than merely predicting responses - is central to environmental science, epidemiology, and economics. For instance, public health officials might be interested in whether air pollution has a strictly positive association with a health outcome, and the magnitude of any effect. Standard machine learning methods often provide accurate predictions but offer limited insight into covariate-response relationships. And we show that existing methods for constructing confidence (or credible) intervals for associations fail to provide nominal coverage in the face of model misspecification and distribution shift - despite both being essentially always present in spatial problems. We introduce a method that constructs valid frequentist confidence intervals for associations in spatial settings. Our method requires minimal assumptions beyond a form of spatial smoothness. In particular, we do not require model correctness or covariate overlap between training and target locations. Our approach is the first to guarantee nominal coverage in this setting and outperforms existing techniques in both real and simulated experiments.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LongReD: Mitigating Short-Text Degradation of Long-Context Large Language Models via Restoration Distillation</title>
<link>https://arxiv.org/abs/2502.07365</link>
<guid>https://arxiv.org/abs/2502.07365</guid>
<content:encoded><![CDATA[
arXiv:2502.07365v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have gained extended context windows through scaling positional encodings and lightweight continual pre-training. However, this often leads to degraded performance on short-text tasks, while the reasons for this degradation remain insufficiently explored. In this work, we identify two primary factors contributing to this issue: distribution drift in hidden states and attention scores, and catastrophic forgetting during continual pre-training. To address these challenges, we propose Long Context Pre-training with Restoration Distillation (LongReD), a novel approach designed to mitigate short-text performance degradation through minimizing the distribution discrepancy between the extended and original models. Besides training on long texts, LongReD distills the hidden state of selected layers from the original model on short texts. Additionally, LongReD also introduces a short-to-long distillation, aligning the output distribution on short texts with that on long texts by leveraging skipped positional indices. Experiments on common text benchmarks demonstrate that LongReD effectively preserves the model's short-text performance while maintaining comparable or even better capacity to handle long texts than baselines. Our code is available at https://github.com/RUCAIBox/LongReD.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Combinatorial Reinforcement Learning with Preference Feedback</title>
<link>https://arxiv.org/abs/2502.10158</link>
<guid>https://arxiv.org/abs/2502.10158</guid>
<content:encoded><![CDATA[
arXiv:2502.10158v2 Announce Type: replace-cross 
Abstract: In this paper, we consider combinatorial reinforcement learning with preference feedback, where a learning agent sequentially offers an action--an assortment of multiple items to--a user, whose preference feedback follows a multinomial logistic (MNL) model. This framework allows us to model real-world scenarios, particularly those involving long-term user engagement, such as in recommender systems and online advertising. However, this framework faces two main challenges: (1) the unknown value of each item, unlike traditional MNL bandits that only address single-step preference feedback, and (2) the difficulty of ensuring optimism while maintaining tractable assortment selection in the combinatorial action space with unknown values. In this paper, we assume a contextual MNL preference model, where the mean utilities are linear, and the value of each item is approximated by a general function. We propose an algorithm, MNL-VQL, that addresses these challenges, making it both computationally and statistically efficient. As a special case, for linear MDPs (with the MNL preference feedback), we establish the first regret lower bound in this framework and show that MNL-VQL achieves nearly minimax-optimal regret. To the best of our knowledge, this is the first work to provide statistical guarantees in combinatorial RL with preference feedback.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReLearn: Unlearning via Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2502.11190</link>
<guid>https://arxiv.org/abs/2502.11190</guid>
<content:encoded><![CDATA[
arXiv:2502.11190v3 Announce Type: replace-cross 
Abstract: Current unlearning methods for large language models usually rely on reverse optimization to reduce target token probabilities. However, this paradigm disrupts the subsequent tokens prediction, degrading model performance and linguistic coherence. Moreover, existing evaluation metrics overemphasize contextual forgetting while inadequately assessing response fluency and relevance. To address these challenges, we propose ReLearn, a data augmentation and fine-tuning pipeline for effective unlearning, along with a comprehensive evaluation framework. This framework introduces Knowledge Forgetting Rate (KFR) and Knowledge Retention Rate (KRR) to measure knowledge-level preservation, and Linguistic Score (LS) to evaluate generation quality. Our experiments show that ReLearn successfully achieves targeted forgetting while preserving high-quality output. Through mechanistic analysis, we further demonstrate how reverse optimization disrupts coherent text generation, while ReLearn preserves this essential capability. Code is available at https://github.com/zjunlp/unlearn.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Dual Process Theory in Language Agent Framework for Real-time Simultaneous Human-AI Collaboration</title>
<link>https://arxiv.org/abs/2502.11882</link>
<guid>https://arxiv.org/abs/2502.11882</guid>
<content:encoded><![CDATA[
arXiv:2502.11882v5 Announce Type: replace-cross 
Abstract: Agents built on large language models (LLMs) have excelled in turn-by-turn human-AI collaboration but struggle with simultaneous tasks requiring real-time interaction. Latency issues and the challenge of inferring variable human strategies hinder their ability to make autonomous decisions without explicit instructions. Through experiments with current independent System 1 and System 2 methods, we validate the necessity of using Dual Process Theory (DPT) in real-time tasks. We propose DPT-Agent, a novel language agent framework that integrates System 1 and System 2 for efficient real-time simultaneous human-AI collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and code-as-policy for fast, intuitive, and controllable decision-making. DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous reflection to infer human intentions and perform reasoning-based autonomous decisions. We demonstrate the effectiveness of DPT-Agent through further experiments with rule-based agents and human collaborators, showing significant improvements over mainstream LLM-based frameworks. DPT-Agent can effectively help LLMs convert correct slow thinking and reasoning into executable actions, thereby improving performance. To the best of our knowledge, DPT-Agent is the first language agent framework that achieves successful real-time simultaneous human-AI collaboration autonomously. Code of DPT-Agent can be found in https://github.com/sjtu-marl/DPT-Agent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThinkGuard: Deliberative Slow Thinking Leads to Cautious Guardrails</title>
<link>https://arxiv.org/abs/2502.13458</link>
<guid>https://arxiv.org/abs/2502.13458</guid>
<content:encoded><![CDATA[
arXiv:2502.13458v2 Announce Type: replace-cross 
Abstract: Ensuring the safety of large language models (LLMs) is critical as they are deployed in real-world applications. Existing guardrails rely on rule-based filtering or single-pass classification, limiting their ability to handle nuanced safety violations. To address this, we propose ThinkGuard, a critique-augmented guardrail model that distills knowledge from high-capacity LLMs by generating structured critiques alongside safety labels. Fine-tuned on critique-augmented data, the captured deliberative thinking ability drastically enhances the guardrail's cautiousness and interpretability. Evaluated on multiple safety benchmarks, ThinkGuard achieves the highest average F1 and AUPRC, outperforming all baselines. Compared to LLaMA Guard 3, ThinkGuard improves accuracy by 16.1% and macro F1 by 27.0%. Moreover, it surpasses label-only fine-tuned models, confirming that structured critiques enhance both classification precision and nuanced safety reasoning while maintaining computational efficiency.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimize Cardinality Estimation Model Pretraining by Simplifying the Training Datasets</title>
<link>https://arxiv.org/abs/2502.14350</link>
<guid>https://arxiv.org/abs/2502.14350</guid>
<content:encoded><![CDATA[
arXiv:2502.14350v2 Announce Type: replace-cross 
Abstract: The cardinality estimation is a key aspect of query optimization research, and its performance has significantly improved with the integration of machine learning. To overcome the "cold start" problem or the lack of model transferability in learned cardinality estimators, some pre-training cardinality estimation models have been proposed that use learning across multiple datasets and corresponding workloads. These models typically train on a dataset created by uniformly sampling from many datasets, but this approach may not be optimal. By applying the Group Distributionally Robust Optimization (Group DRO) algorithm to training datasets, we find that some specific training datasets contribute more significantly to model performance than others. Based on this observation, we conduct extensive experiments to delve deeper into pre-training cardinality estimators. Our results show how the performance of these models can be influenced by the datasets and corresponding workloads. Finally, we introduce a simplified training dataset, which has been reduced to a fraction of the size of existing pretraining datasets. Sufficient experimental results demonstrate that the pre-trained cardinality estimator based on this simplified dataset can still achieve comparable performance to existing models in zero-shot setups.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robustness and Cybersecurity in the EU Artificial Intelligence Act</title>
<link>https://arxiv.org/abs/2502.16184</link>
<guid>https://arxiv.org/abs/2502.16184</guid>
<content:encoded><![CDATA[
arXiv:2502.16184v2 Announce Type: replace-cross 
Abstract: The EU Artificial Intelligence Act (AIA) establishes different legal principles for different types of AI systems. While prior work has sought to clarify some of these principles, little attention has been paid to robustness and cybersecurity. This paper aims to fill this gap. We identify legal challenges and shortcomings in provisions related to robustness and cybersecurity for high-risk AI systems(Art. 15 AIA) and general-purpose AI models (Art. 55 AIA). We show that robustness and cybersecurity demand resilience against performance disruptions. Furthermore, we assess potential challenges in implementing these provisions in light of recent advancements in the machine learning (ML) literature. Our analysis informs efforts to develop harmonized standards, guidelines by the European Commission, as well as benchmarks and measurement methodologies under Art. 15(2) AIA. With this, we seek to bridge the gap between legal terminology and ML research, fostering a better alignment between research and implementation efforts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpreting CLIP with Hierarchical Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2502.20578</link>
<guid>https://arxiv.org/abs/2502.20578</guid>
<content:encoded><![CDATA[
arXiv:2502.20578v2 Announce Type: replace-cross 
Abstract: Sparse autoencoders (SAEs) are useful for detecting and steering interpretable features in neural networks, with particular potential for understanding complex multimodal representations. Given their ability to uncover interpretable features, SAEs are particularly valuable for analyzing large-scale vision-language models (e.g., CLIP and SigLIP), which are fundamental building blocks in modern systems yet remain challenging to interpret and control. However, current SAE methods are limited by optimizing both reconstruction quality and sparsity simultaneously, as they rely on either activation suppression or rigid sparsity constraints. To this end, we introduce Matryoshka SAE (MSAE), a new architecture that learns hierarchical representations at multiple granularities simultaneously, enabling a direct optimization of both metrics without compromise. MSAE establishes a new state-of-the-art Pareto frontier between reconstruction quality and sparsity for CLIP, achieving 0.99 cosine similarity and less than 0.1 fraction of variance unexplained while maintaining ~80% sparsity. Finally, we demonstrate the utility of MSAE as a tool for interpreting and controlling CLIP by extracting over 120 semantic concepts from its representation to perform concept-based similarity search and bias analysis in downstream tasks like CelebA. We make the codebase available at https://github.com/WolodjaZ/MSAE.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Steer Learners in Games</title>
<link>https://arxiv.org/abs/2502.20770</link>
<guid>https://arxiv.org/abs/2502.20770</guid>
<content:encoded><![CDATA[
arXiv:2502.20770v2 Announce Type: replace-cross 
Abstract: We consider the problem of learning to exploit learning algorithms through repeated interactions in games. Specifically, we focus on the case of repeated two player, finite-action games, in which an optimizer aims to steer a no-regret learner to a Stackelberg equilibrium without knowledge of its payoffs. We first show that this is impossible if the optimizer only knows that the learner is using an algorithm from the general class of no-regret algorithms. This suggests that the optimizer requires more information about the learner's objectives or algorithm to successfully exploit them. Building on this intuition, we reduce the problem for the optimizer to that of recovering the learner's payoff structure. We demonstrate the effectiveness of this approach if the learner's algorithm is drawn from a smaller class by analyzing two examples: one where the learner uses an ascent algorithm, and another where the learner uses stochastic mirror ascent with known regularizer and step sizes.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Don't Use the CLT in LLM Evals With Fewer Than a Few Hundred Datapoints</title>
<link>https://arxiv.org/abs/2503.01747</link>
<guid>https://arxiv.org/abs/2503.01747</guid>
<content:encoded><![CDATA[
arXiv:2503.01747v3 Announce Type: replace-cross 
Abstract: Rigorous statistical evaluations of large language models (LLMs), including valid error bars and significance testing, are essential for meaningful and reliable performance assessment. Currently, when such statistical measures are reported, they typically rely on the Central Limit Theorem (CLT). In this position paper, we argue that while CLT-based methods for uncertainty quantification are appropriate when benchmarks consist of thousands of examples, they fail to provide adequate uncertainty estimates for LLM evaluations that rely on smaller, highly specialized benchmarks. In these small-data settings, we demonstrate that CLT-based methods perform very poorly, usually dramatically underestimating uncertainty (i.e. producing error bars that are too small). We give recommendations for alternative frequentist and Bayesian methods that are both easy to implement and more appropriate in these increasingly common scenarios. We provide a simple Python library for these Bayesian methods at https://github.com/sambowyer/bayes_evals .
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Probabilistic Reasoning with LLMs for k-anonymity Estimation</title>
<link>https://arxiv.org/abs/2503.09674</link>
<guid>https://arxiv.org/abs/2503.09674</guid>
<content:encoded><![CDATA[
arXiv:2503.09674v3 Announce Type: replace-cross 
Abstract: Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Discrete Diffusion</title>
<link>https://arxiv.org/abs/2503.09790</link>
<guid>https://arxiv.org/abs/2503.09790</guid>
<content:encoded><![CDATA[
arXiv:2503.09790v2 Announce Type: replace-cross 
Abstract: Discrete diffusion models are a class of generative models that construct sequences by progressively denoising samples from a categorical noise distribution. Beyond their rapidly growing ability to generate coherent natural language, these models present a new and important opportunity to enforce sequence-level constraints, a capability that current autoregressive models cannot natively provide. This paper capitalizes on this opportunity by introducing Constrained Discrete Diffusion (CDD), a novel integration of differentiable constraint optimization within the diffusion process to ensure adherence to constraints, logic rules, or safety requirements for generated sequences. Unlike conventional text generators that often rely on post-hoc filtering or model retraining for controllable generation, CDD directly imposes constraints into the discrete diffusion sampling process, resulting in a training-free and effective approach. Experiments in toxicity-controlled text generation, property-constrained molecule design, and instruction-constrained text completion demonstrate that CDD achieves zero constraint violations in a diverse array of tasks while preserving fluency, novelty, and coherence while outperforming autoregressive and existing discrete diffusion approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Light-R1: Curriculum SFT, DPO and RL for Long COT from Scratch and Beyond</title>
<link>https://arxiv.org/abs/2503.10460</link>
<guid>https://arxiv.org/abs/2503.10460</guid>
<content:encoded><![CDATA[
arXiv:2503.10460v4 Announce Type: replace-cross 
Abstract: This paper introduces Light-R1, an open-source suite for training long reasoning models using reproducible and cost-effective methodology. Given the proprietary nature of data used in the DeepSeek-R1 series, we develop an alternative approach leveraging exclusively public data and models. Our curriculum training progressively increases data difficulty, combined with multi-staged post-training. Our Light-R1-32B model, trained from Qwen2.5-32B-Instruct, outperforms DeepSeek-R1-Distill-Qwen-32B in math reasoning.
  Experimental results show that this curriculum approach becomes more effective when distinct, diverse datasets are available for different training stages: fine-tuning DeepSeek-R1-Distilled models (pre-tuned by DeepSeek team on proprietary data) with 3,000 challenging examples from our curriculum dataset yielded state-of-the-art 7B and 14B models, while the 32B model, Light-R1-32B-DS performed comparably to QwQ-32B and DeepSeek-R1.
  Furthermore, we extend our work by applying GRPO on long reasoning models. Our final Light-R1-14B-DS achieves SOTA performance among 14B models in math, with AIME24 & 25 scores of 74.0 and 60.2 respectively, surpassing many 32B models and DeepSeek-R1-Distill-Llama-70B. Despite math-focused training, Light-R1-14B-DS demonstrates strong cross-domain generalization.
  Light-R1 represents a significant advancement in making sophisticated reasoning models more accessible and implementable in real-world applications. Our models, training data and code have been made available at https://github.com/Qihoo360/Light-R1.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Resilient and Sustainable Global Industrial Systems: An Evolutionary-Based Approach</title>
<link>https://arxiv.org/abs/2503.11688</link>
<guid>https://arxiv.org/abs/2503.11688</guid>
<content:encoded><![CDATA[
arXiv:2503.11688v2 Announce Type: replace-cross 
Abstract: This paper presents a new complex optimization problem in the field of automatic design of advanced industrial systems and proposes a hybrid optimization approach to solve the problem. The problem is multi-objective as it aims at finding solutions that minimize CO2 emissions, transportation time, and costs. The optimization approach combines an evolutionary algorithm and classical mathematical programming to design resilient and sustainable global manufacturing networks. Further, it makes use of the OWL ontology for data consistency and constraint management. The experimental validation demonstrates the effectiveness of the approach in both single and double sourcing scenarios. The proposed methodology, in general, can be applied to any industry case with complex manufacturing and supply chain challenges.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Inference-Time Scaling for Flow Models via Stochastic Generation and Rollover Budget Forcing</title>
<link>https://arxiv.org/abs/2503.19385</link>
<guid>https://arxiv.org/abs/2503.19385</guid>
<content:encoded><![CDATA[
arXiv:2503.19385v4 Announce Type: replace-cross 
Abstract: We propose an inference-time scaling approach for pretrained flow models. Recently, inference-time scaling has gained significant attention in LLMs and diffusion models, improving sample quality or better aligning outputs with user preferences by leveraging additional computation. For diffusion models, particle sampling has allowed more efficient scaling due to the stochasticity at intermediate denoising steps. On the contrary, while flow models have gained popularity as an alternative to diffusion models--offering faster generation and high-quality outputs in state-of-the-art image and video generative models--efficient inference-time scaling methods used for diffusion models cannot be directly applied due to their deterministic generative process. To enable efficient inference-time scaling for flow models, we propose three key ideas: 1) SDE-based generation, enabling particle sampling in flow models, 2) Interpolant conversion, broadening the search space and enhancing sample diversity, and 3) Rollover Budget Forcing (RBF), an adaptive allocation of computational resources across timesteps to maximize budget utilization. Our experiments show that SDE-based generation, particularly variance-preserving (VP) interpolant-based generation, improves the performance of particle sampling methods for inference-time scaling in flow models. Additionally, we demonstrate that RBF with VP-SDE achieves the best performance, outperforming all previous inference-time scaling approaches.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation</title>
<link>https://arxiv.org/abs/2503.22194</link>
<guid>https://arxiv.org/abs/2503.22194</guid>
<content:encoded><![CDATA[
arXiv:2503.22194v2 Announce Type: replace-cross 
Abstract: We introduce ORIGEN, the first zero-shot method for 3D orientation grounding in text-to-image generation across multiple objects and diverse categories. While previous work on spatial grounding in image generation has mainly focused on 2D positioning, it lacks control over 3D orientation. To address this, we propose a reward-guided sampling approach using a pretrained discriminative model for 3D orientation estimation and a one-step text-to-image generative flow model. While gradient-ascent-based optimization is a natural choice for reward-based guidance, it struggles to maintain image realism. Instead, we adopt a sampling-based approach using Langevin dynamics, which extends gradient ascent by simply injecting random noise--requiring just a single additional line of code. Additionally, we introduce adaptive time rescaling based on the reward function to accelerate convergence. Our experiments show that ORIGEN outperforms both training-based and test-time guidance methods across quantitative metrics and user studies.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Compact LLMs for Zero-Shot Iberian Language Tasks on End-User Devices</title>
<link>https://arxiv.org/abs/2504.03312</link>
<guid>https://arxiv.org/abs/2504.03312</guid>
<content:encoded><![CDATA[
arXiv:2504.03312v2 Announce Type: replace-cross 
Abstract: Large Language Models have significantly advanced natural language processing, achieving remarkable performance in tasks such as language generation, translation, and reasoning. However, their substantial computational requirements restrict deployment to high-end systems, limiting accessibility on consumer-grade devices. This challenge is especially pronounced for under-resourced languages like those spoken in the Iberian Peninsula, where relatively limited linguistic resources and benchmarks hinder effective evaluation. This work presents a comprehensive evaluation of compact state-of-the-art LLMs across several essential NLP tasks tailored for Iberian languages. The results reveal that while some models consistently excel in certain tasks, significant performance gaps remain, particularly for languages such as Basque. These findings highlight the need for further research on balancing model compactness with robust multilingual performance
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SynWorld: Virtual Scenario Synthesis for Agentic Action Knowledge Refinement</title>
<link>https://arxiv.org/abs/2504.03561</link>
<guid>https://arxiv.org/abs/2504.03561</guid>
<content:encoded><![CDATA[
arXiv:2504.03561v2 Announce Type: replace-cross 
Abstract: In the interaction between agents and their environments, agents expand their capabilities by planning and executing actions. However, LLM-based agents face substantial challenges when deployed in novel environments or required to navigate unconventional action spaces. To empower agents to autonomously explore environments, optimize workflows, and enhance their understanding of actions, we propose SynWorld, a framework that allows agents to synthesize possible scenarios with multi-step action invocation within the action space and perform Monte Carlo Tree Search (MCTS) exploration to effectively refine their action knowledge in the current environment. Our experiments demonstrate that SynWorld is an effective and general approach to learning action knowledge in new environments. Code is available at https://github.com/zjunlp/SynWorld.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations</title>
<link>https://arxiv.org/abs/2504.06792</link>
<guid>https://arxiv.org/abs/2504.06792</guid>
<content:encoded><![CDATA[
arXiv:2504.06792v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1(671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few in-domain demonstrations, the model consistently activates a sparse and stable subset of experts on tasks within the same domain. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and L2 norm of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities before and after routed experts. Experiments on DeepSeek-R1 and DeepSeek-V3-0324 show that our method can achieve comparable performances and $2.99\times$ throughput under the same memory budget with full model with only half the experts.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Comprehensive Survey in LLM(-Agent) Full Stack Safety: Data, Training and Deployment</title>
<link>https://arxiv.org/abs/2504.15585</link>
<guid>https://arxiv.org/abs/2504.15585</guid>
<content:encoded><![CDATA[
arXiv:2504.15585v3 Announce Type: replace-cross 
Abstract: The remarkable success of Large Language Models (LLMs) has illuminated a promising pathway toward achieving Artificial General Intelligence for both academic and industrial communities, owing to their unprecedented performance across various applications. As LLMs continue to gain prominence in both research and commercial domains, their security and safety implications have become a growing concern, not only for researchers and corporations but also for every nation. Currently, existing surveys on LLM safety primarily focus on specific stages of the LLM lifecycle, e.g., deployment phase or fine-tuning phase, lacking a comprehensive understanding of the entire "lifechain" of LLMs. To address this gap, this paper introduces, for the first time, the concept of "full-stack" safety to systematically consider safety issues throughout the entire process of LLM training, deployment, and eventual commercialization. Compared to the off-the-shelf LLM safety surveys, our work demonstrates several distinctive advantages: (I) Comprehensive Perspective. We define the complete LLM lifecycle as encompassing data preparation, pre-training, post-training, deployment and final commercialization. To our knowledge, this represents the first safety survey to encompass the entire lifecycle of LLMs. (II) Extensive Literature Support. Our research is grounded in an exhaustive review of over 800+ papers, ensuring comprehensive coverage and systematic organization of security issues within a more holistic understanding. (III) Unique Insights. Through systematic literature analysis, we have developed reliable roadmaps and perspectives for each chapter. Our work identifies promising research directions, including safety in data generation, alignment techniques, model editing, and LLM-based agent systems. These insights provide valuable guidance for researchers pursuing future work in this field.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Enhanced Ensemble Filters</title>
<link>https://arxiv.org/abs/2504.17836</link>
<guid>https://arxiv.org/abs/2504.17836</guid>
<content:encoded><![CDATA[
arXiv:2504.17836v2 Announce Type: replace-cross 
Abstract: The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state-observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed by approximating the mean-field evolution using a novel form of neural operator taking probability distributions as input: a measure neural mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-field limit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. The derivation of methods from a mean-field formulation allows a single parameterization of the algorithm to be deployed at different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root mean-square-error performance relative to leading methods in filtering the Lorenz 96 and Kuramoto-Sivashinsky models.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Dimensional Factorization Limits in Discrete Diffusion Models through Quantum Joint Distribution Learning</title>
<link>https://arxiv.org/abs/2505.05151</link>
<guid>https://arxiv.org/abs/2505.05151</guid>
<content:encoded><![CDATA[
arXiv:2505.05151v2 Announce Type: replace-cross 
Abstract: This study explores quantum-enhanced discrete diffusion models to overcome classical limitations in learning high-dimensional distributions. We rigorously prove that classical discrete diffusion models, which calculate per-dimension transition probabilities to avoid exponential computational cost, exhibit worst-case linear scaling of Kullback-Leibler (KL) divergence with data dimension. To address this, we propose a Quantum Discrete Denoising Diffusion Probabilistic Model (QD3PM), which enables joint probability learning through diffusion and denoising in exponentially large Hilbert spaces. By deriving posterior states through quantum Bayes' theorem, similar to the crucial role of posterior probabilities in classical diffusion models, and by learning the joint probability, we establish a solid theoretical foundation for quantum-enhanced diffusion models. For denoising, we design a quantum circuit using temporal information for parameter sharing and learnable classical-data-controlled rotations for encoding. Exploiting joint distribution learning, our approach enables single-step sampling from pure noise, eliminating iterative requirements of existing models. Simulations demonstrate the proposed model's superior accuracy in modeling complex distributions compared to factorization methods. Hence, this paper establishes a new theoretical paradigm in generative models by leveraging the quantum advantage in joint distribution learning.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Sequential Numerical Prediction in Autoregressive Models</title>
<link>https://arxiv.org/abs/2505.13077</link>
<guid>https://arxiv.org/abs/2505.13077</guid>
<content:encoded><![CDATA[
arXiv:2505.13077v2 Announce Type: replace-cross 
Abstract: Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Taming Recommendation Bias with Causal Intervention on Evolving Personal Popularity</title>
<link>https://arxiv.org/abs/2505.14310</link>
<guid>https://arxiv.org/abs/2505.14310</guid>
<content:encoded><![CDATA[
arXiv:2505.14310v2 Announce Type: replace-cross 
Abstract: Popularity bias occurs when popular items are recommended far more frequently than they should be, negatively impacting both user experience and recommendation accuracy. Existing debiasing methods mitigate popularity bias often uniformly across all users and only partially consider the time evolution of users or items. However, users have different levels of preference for item popularity, and this preference is evolving over time. To address these issues, we propose a novel method called CausalEPP (Causal Intervention on Evolving Personal Popularity) for taming recommendation bias, which accounts for the evolving personal popularity of users. Specifically, we first introduce a metric called {Evolving Personal Popularity} to quantify each user's preference for popular items. Then, we design a causal graph that integrates evolving personal popularity into the conformity effect, and apply deconfounded training to mitigate the popularity bias of the causal graph. During inference, we consider the evolution consistency between users and items to achieve a better recommendation. Empirical studies demonstrate that CausalEPP outperforms baseline methods in reducing popularity bias while improving recommendation accuracy.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BACON: A fully explainable AI model with graded logic for decision making problems</title>
<link>https://arxiv.org/abs/2505.14510</link>
<guid>https://arxiv.org/abs/2505.14510</guid>
<content:encoded><![CDATA[
arXiv:2505.14510v3 Announce Type: replace-cross 
Abstract: As machine learning models and autonomous agents are increasingly deployed in high-stakes, real-world domains such as healthcare, security, finance, and robotics, the need for transparent and trustworthy explanations has become critical. To ensure end-to-end transparency of AI decisions, we need models that are not only accurate but also fully explainable and human-tunable. We introduce BACON, a novel framework for automatically training explainable AI models for decision making problems using graded logic. BACON achieves high predictive accuracy while offering full structural transparency and precise, logic-based symbolic explanations, enabling effective human-AI collaboration and expert-guided refinement. We evaluate BACON with a diverse set of scenarios: classic Boolean approximation, Iris flower classification, house purchasing decisions and breast cancer diagnosis. In each case, BACON provides high-performance models while producing compact, human-verifiable decision logic. These results demonstrate BACON's potential as a practical and principled approach for delivering crisp, trustworthy explainable AI.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AKRMap: Adaptive Kernel Regression for Trustworthy Visualization of Cross-Modal Embeddings</title>
<link>https://arxiv.org/abs/2505.14664</link>
<guid>https://arxiv.org/abs/2505.14664</guid>
<content:encoded><![CDATA[
arXiv:2505.14664v2 Announce Type: replace-cross 
Abstract: Cross-modal embeddings form the foundation for multi-modal models. However, visualization methods for interpreting cross-modal embeddings have been primarily confined to traditional dimensionality reduction (DR) techniques like PCA and t-SNE. These DR methods primarily focus on feature distributions within a single modality, whilst failing to incorporate metrics (e.g., CLIPScore) across multiple modalities. This paper introduces AKRMap, a new DR technique designed to visualize cross-modal embeddings metric with enhanced accuracy by learning kernel regression of the metric landscape in the projection space. Specifically, AKRMap constructs a supervised projection network guided by a post-projection kernel regression loss, and employs adaptive generalized kernels that can be jointly optimized with the projection. This approach enables AKRMap to efficiently generate visualizations that capture complex metric distributions, while also supporting interactive features such as zoom and overlay for deeper exploration. Quantitative experiments demonstrate that AKRMap outperforms existing DR methods in generating more accurate and trustworthy visualizations. We further showcase the effectiveness of AKRMap in visualizing and comparing cross-modal embeddings for text-to-image models. Code and demo are available at https://github.com/yilinye/AKRMap.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From EduVisBench to EduVisAgent: A Benchmark and Multi-Agent Framework for Reasoning-Driven Pedagogical Visualization</title>
<link>https://arxiv.org/abs/2505.16832</link>
<guid>https://arxiv.org/abs/2505.16832</guid>
<content:encoded><![CDATA[
arXiv:2505.16832v2 Announce Type: replace-cross 
Abstract: While foundation models (FMs), such as diffusion models and large vision-language models (LVLMs), have been widely applied in educational contexts, their ability to generate pedagogically effective visual explanations remains limited. Most existing approaches focus primarily on textual reasoning, overlooking the critical role of structured and interpretable visualizations in supporting conceptual understanding. To better assess the visual reasoning capabilities of FMs in educational settings, we introduce EduVisBench, a multi-domain, multi-level benchmark. EduVisBench features diverse STEM problem sets requiring visually grounded solutions, along with a fine-grained evaluation rubric informed by pedagogical theory. Our empirical analysis reveals that existing models frequently struggle with the inherent challenge of decomposing complex reasoning and translating it into visual representations aligned with human cognitive processes. To address these limitations, we propose EduVisAgent, a multi-agent collaborative framework that coordinates specialized agents for instructional planning, reasoning decomposition, metacognitive prompting, and visualization design. Experimental results show that EduVisAgent substantially outperforms all baselines, achieving a 40.2% improvement and delivering more educationally aligned visualizations. EduVisBench and EduVisAgent are available at https://github.com/aiming-lab/EduVisBench and https://github.com/aiming-lab/EduVisAgent.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Practical Defect-Focused Automated Code Review</title>
<link>https://arxiv.org/abs/2505.17928</link>
<guid>https://arxiv.org/abs/2505.17928</guid>
<content:encoded><![CDATA[
arXiv:2505.17928v2 Announce Type: replace-cross 
Abstract: The complexity of code reviews has driven efforts to automate review comments, but prior approaches oversimplify this task by treating it as snippet-level code-to-text generation and relying on text similarity metrics like BLEU for evaluation. These methods overlook repository context, real-world merge request evaluation, and defect detection, limiting their practicality. To address these issues, we explore the full automation pipeline within the online recommendation service of a company with nearly 400 million daily active users, analyzing industry-grade C++ codebases comprising hundreds of thousands of lines of code. We identify four key challenges: 1) capturing relevant context, 2) improving key bug inclusion (KBI), 3) reducing false alarm rates (FAR), and 4) integrating human workflows. To tackle these, we propose 1) code slicing algorithms for context extraction, 2) a multi-role LLM framework for KBI, 3) a filtering mechanism for FAR reduction, and 4) a novel prompt design for better human interaction. Our approach, validated on real-world merge requests from historical fault reports, achieves a 2x improvement over standard LLMs and a 10x gain over previous baselines. While the presented results focus on C++, the underlying framework design leverages language-agnostic principles (e.g., AST-based analysis), suggesting potential for broader applicability.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Feature Interactions from the Perspective of Quadratic Neural Networks for Click-through Rate Prediction</title>
<link>https://arxiv.org/abs/2505.17999</link>
<guid>https://arxiv.org/abs/2505.17999</guid>
<content:encoded><![CDATA[
arXiv:2505.17999v2 Announce Type: replace-cross 
Abstract: Hadamard Product (HP) has long been a cornerstone in click-through rate (CTR) prediction tasks due to its simplicity, effectiveness, and ability to capture feature interactions without additional parameters. However, the underlying reasons for its effectiveness remain unclear. In this paper, we revisit HP from the perspective of Quadratic Neural Networks (QNN), which leverage quadratic interaction terms to model complex feature relationships. We further reveal QNN's ability to expand the feature space and provide smooth nonlinear approximations without relying on activation functions. Meanwhile, we find that traditional post-activation does not further improve the performance of the QNN. Instead, mid-activation is a more suitable alternative. Through theoretical analysis and empirical evaluation of 25 QNN neuron formats, we identify a good-performing variant and make further enhancements on it. Specifically, we propose the Multi-Head Khatri-Rao Product as a superior alternative to HP and a Self-Ensemble Loss with dynamic ensemble capability within the same network to enhance computational efficiency and performance. Ultimately, we propose a novel neuron format, QNN-alpha, which is tailored for CTR prediction tasks. Experimental results show that QNN-alpha achieves new state-of-the-art performance on six public datasets while maintaining low inference latency, good scalability, and excellent compatibility. The code, running logs, and detailed hyperparameter configurations are available at: https://github.com/salmon1802/QNN.
]]></content:encoded>
<pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GMoE: Empowering LLMs Fine-Tuning via MoE Graph Collaboration</title>
<link>https://arxiv.org/abs/2412.16216</link>
<guid>https://arxiv.org/abs/2412.16216</guid>
<content:encoded><![CDATA[
<div> Graph-based MoE, load imbalance, collaboration, fine-tuning, LLMs <br />
Summary: 
The paper introduces a novel Graph-based Mixture-of-Experts (GMoE) framework to address the load imbalance issue in large language models (LLMs). By utilizing a graph router function, GMoE enhances collaboration among experts, allowing them to dynamically allocate information from input data. Two coordination strategies, Poisson distribution-based distinction, and Normal distribution-based balance, further enhance the model stability during fine-tuning of LLMs. The implementation of GMoE using the Low-Rank Adaptation (LoRA) technique shows promising results on real-world datasets, demonstrating the effectiveness of facilitating collaborations among multiple experts in LLM fine-tuning. The experimental code is available at https://github.com/BAI-LAB/GMoE <br /><br /> <div>
arXiv:2412.16216v3 Announce Type: replace 
Abstract: The sparse Mixture-of-Experts (MoE) architecture of large language models (LLMs) confronts an inherent issue of load imbalance arising from the simplistic linear router strategy, which ultimately causes the instability and inefficient learning of LLMs. To address this challenge, we introduce a novel MoE graph-based framework $\textbf{GMoE}$, aimed at enhancing the collaboration among multiple experts. In GMoE, a graph router function is designed to capture the collaboration signals among experts. This enables all experts to dynamically allocate information derived from input data by sharing information with their neighboring experts. Moreover, we put forward two coordination strategies in GMoE: the $\textit{Poisson distribution-based distinction strategy}$ and the $\textit{Normal distribution-based balance strategy}$, to further release the capacity of each expert and increase the model stability in the fine-tuning of LLMs. Specifically, we leverage a parameter-efficient fine-tuning technique, i.e., Low-Rank Adaptation (LoRA), to implement the graph MoE architecture. Extensive experiments on four real-world benchmark datasets demonstrate the effectiveness of GMoE, showing the benefits of facilitating collaborations of multiple experts in LLM fine-tuning. The code of experimental implementation is available at https://github.com/BAI-LAB/GMoE
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Boundary of Diffusion-based Methods for Solving Constrained Optimization</title>
<link>https://arxiv.org/abs/2502.10330</link>
<guid>https://arxiv.org/abs/2502.10330</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Continuous Constrained Optimization, DiOpt, supervised learning, multimodal distributions <br />
Summary: <br />
This paper explores the application of diffusion models to Continuous Constrained Optimization problems, specifically focusing on a two-dimensional constrained quadratic optimization example. The researchers identify challenges and propose the DiOpt framework, which consists of a warm-start phase and a bootstrapping phase to iteratively refine solutions while meeting constraints. The framework samples multiple candidate solutions and selects the optimal one through a screening process. Extensive experiments demonstrate the training dynamics, performance on various optimization problems, and the impact of hyperparameters on DiOpt's effectiveness. <div>
arXiv:2502.10330v3 Announce Type: replace 
Abstract: Diffusion models have achieved remarkable success in generative tasks such as image and video synthesis, and in control domains like robotics, owing to their strong generalization capabilities and proficiency in fitting complex multimodal distributions. However, their full potential in solving Continuous Constrained Optimization problems remains largely underexplored. Our work commences by investigating a two-dimensional constrained quadratic optimization problem as an illustrative example to explore the inherent challenges and issues when applying diffusion models to such optimization tasks and providing theoretical analyses for these observations. To address the identified gaps and harness diffusion models for Continuous Constrained Optimization, we build upon this analysis to propose a novel diffusion-based framework for optimization problems called DiOpt. This framework operates in two distinct phases: an initial warm-start phase, implemented via supervised learning, followed by a bootstrapping phase. This dual-phase architecture is designed to iteratively refine solutions, thereby improving the objective function while rigorously satisfying problem constraints. Finally, multiple candidate solutions are sampled, and the optimal one is selected through a screening process. We present extensive experiments detailing the training dynamics of DiOpt, its performance across a diverse set of Continuous Constrained Optimization problems, and an analysis of the impact of DiOpt's various hyperparameters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRIREN: Beyond Trajectories -- A Spectral Lens on Time</title>
<link>https://arxiv.org/abs/2505.17370</link>
<guid>https://arxiv.org/abs/2505.17370</guid>
<content:encoded><![CDATA[
<div> Forecasting, LTSF, Geometric structure, Wasserstein-2 distance, FRIREN <br />
Summary:
The study challenges the conventional approach of pointwise prediction in long-term time-series forecasting models and emphasizes the importance of capturing geometric changes. The proposed FRIREN model incorporates a normalizing flow block to embed data into a latent representation and generate optimal paths that preserve geometry. By focusing on the Wasserstein-2 distance and spectral analysis, FRIREN achieves high accuracy on chaotic systems like Lorenz-63 and Rossler, outperforming existing models like TimeMixer. The model offers interpretable predictions by identifying local and global dynamics, making it a promising solution for various forecasting tasks. FRIREN sets a new benchmark by combining generative flows with spectral analysis, providing accurate and interpretable long-term forecasting capabilities. <div>
arXiv:2505.17370v2 Announce Type: replace 
Abstract: Long-term time-series forecasting (LTSF) models are often presented as general-purpose solutions that can be applied across domains, implicitly assuming that all data is pointwise predictable. Using chaotic systems such as Lorenz-63 as a case study, we argue that geometric structure - not pointwise prediction - is the right abstraction for a dynamic-agnostic foundational model. Minimizing the Wasserstein-2 distance (W2), which captures geometric changes, and providing a spectral view of dynamics are essential for long-horizon forecasting. Our model, FRIREN (Flow-inspired Representations via Interpretable Eigen-networks), implements an augmented normalizing-flow block that embeds data into a normally distributed latent representation. It then generates a W2-efficient optimal path that can be decomposed into rotation, scaling, inverse rotation, and translation. This architecture yields locally generated, geometry-preserving predictions that are independent of the underlying dynamics, and a global spectral representation that functions as a finite Koopman operator with a small modification. This enables practitioners to identify which modes grow, decay, or oscillate, both locally and system-wide. FRIREN achieves an MSE of 11.4, MAE of 1.6, and SWD of 0.96 on Lorenz-63 in a 336-in, 336-out, dt=0.01 setting, surpassing TimeMixer (MSE 27.3, MAE 2.8, SWD 2.1). The model maintains effective prediction for 274 out of 336 steps, approximately 2.5 Lyapunov times. On Rossler (96-in, 336-out), FRIREN achieves an MSE of 0.0349, MAE of 0.0953, and SWD of 0.0170, outperforming TimeMixer's MSE of 4.3988, MAE of 0.886, and SWD of 3.2065. FRIREN is also competitive on standard LTSF datasets such as ETT and Weather. By connecting modern generative flows with classical spectral analysis, FRIREN makes long-term forecasting both accurate and interpretable, setting a new benchmark for LTSF model design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeUQI: Near-Optimal Uniform Quantization Parameter Initialization</title>
<link>https://arxiv.org/abs/2505.17595</link>
<guid>https://arxiv.org/abs/2505.17595</guid>
<content:encoded><![CDATA[
<div> Keywords: Large language models, Post-training quantization, Uniform quantization, NeUQI, Model performance

Summary:
NeUQI is a method proposed to efficiently determine initial parameters for uniform quantization in Large language models (LLMs). It aims to reduce memory consumption and decoding latency, making deployment on consumer-grade GPUs and personal devices more feasible. NeUQI stands out for its ability to integrate with existing quantization methodologies and consistently outperform them in experiments with the LLaMA and Qwen families across various tasks. The method offers near-optimal parameter initialization for uniform quantization, enhancing the performance of post-quantization models. When combined with a lightweight distillation strategy, NeUQI surpasses the resource-intensive PV-tuning approach, showcasing superior performance in reducing memory footprint and inference costs. <div>
arXiv:2505.17595v2 Announce Type: replace 
Abstract: Large language models (LLMs) achieve impressive performance across domains but face significant challenges when deployed on consumer-grade GPUs or personal devices such as laptops, due to high memory consumption and inference costs. Post-training quantization (PTQ) of LLMs offers a promising solution that reduces their memory footprint and decoding latency. In practice, PTQ with uniform quantization representation is favored for its efficiency and ease of deployment since uniform quantization is widely supported by mainstream hardware and software libraries. Recent studies on $\geq 2$-bit uniform quantization have led to noticeable improvements in post-quantization model performance; however, they primarily focus on quantization methodologies, while the initialization of quantization parameters is underexplored and still relies on the suboptimal Min-Max strategies. In this work, we propose NeUQI, a method devoted to efficiently determining near-optimal initial parameters for uniform quantization. NeUQI is orthogonal to prior quantization methodologies and can seamlessly integrate with them. The experiments with the LLaMA and Qwen families on various tasks demonstrate that our NeUQI consistently outperforms existing methods. Furthermore, when combined with a lightweight distillation strategy, NeUQI can achieve superior performance to PV-tuning, a much more resource-intensive approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Navigate the Unknown: Enhancing LLM Reasoning with Intrinsic Motivation Guided Exploration</title>
<link>https://arxiv.org/abs/2505.17621</link>
<guid>https://arxiv.org/abs/2505.17621</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Large Language Models, i-MENTOR, dense rewards, exploration<br />
Summary:<br />
The article discusses the limitations of existing reinforcement learning approaches for Large Language Models (LLMs) and introduces i-MENTOR as a novel method to address these challenges. i-MENTOR focuses on delivering dense rewards and promoting exploration in RL training by introducing trajectory-aware exploration rewards, dynamic reward scaling, and advantage-preserving reward implementation. These innovations aim to mitigate bias in token-level strategies, stabilize exploration and exploitation in large action spaces, and maintain advantage distribution integrity while providing exploratory guidance. Experimental results on three public datasets demonstrate i-MENTOR's effectiveness, showing a significant 22.39% improvement on the challenging Countdown-4 dataset. <div>
arXiv:2505.17621v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has emerged as a pivotal method for improving the reasoning capabilities of Large Language Models (LLMs). However, prevalent RL approaches such as Proximal Policy Optimization (PPO) and Group-Regularized Policy Optimization (GRPO) face critical limitations due to their reliance on sparse outcome-based rewards and inadequate mechanisms for incentivizing exploration. These limitations result in inefficient guidance for multi-step reasoning processes. Specifically, sparse reward signals fail to deliver effective or sufficient feedback, particularly for challenging problems. Furthermore, such reward structures induce systematic biases that prioritize exploitation of familiar trajectories over novel solution discovery. These shortcomings critically hinder performance in complex reasoning tasks, which inherently demand iterative refinement across ipntermediate steps. To address these challenges, we propose an Intrinsic Motivation guidEd exploratioN meThOd foR LLM Reasoning (i-MENTOR), a novel method designed to both deliver dense rewards and amplify explorations in the RL-based training paradigm. i-MENTOR introduces three key innovations: trajectory-aware exploration rewards that mitigate bias in token-level strategies while maintaining computational efficiency; dynamic reward scaling to stabilize exploration and exploitation in large action spaces; and advantage-preserving reward implementation that maintains advantage distribution integrity while incorporating exploratory guidance. Experiments across three public datasets demonstrate i-MENTOR's effectiveness with a 22.39% improvement on the difficult dataset Countdown-4.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Third Pillar of Causal Analysis? A Measurement Perspective on Causal Representations</title>
<link>https://arxiv.org/abs/2505.17708</link>
<guid>https://arxiv.org/abs/2505.17708</guid>
<content:encoded><![CDATA[
<div> Causal reasoning, discovery, causal analysis, causal representation learning, downstream tasks<br />
<br />
Summary: Causal analysis faces challenges due to data complexity. This paper reinterprets causal representation learning using a measurement model framework where learned representations are proxy measurements of latent causal variables. This approach clarifies conditions for supporting downstream causal reasoning and introduces a Test-based Measurement EXclusivity (T-MEX) score to evaluate representation quality. The T-MEX score is validated through diverse causal inference scenarios, including simulations and ecological video analysis, demonstrating its effectiveness in assessing learned representations for causal tasks. <div>
arXiv:2505.17708v2 Announce Type: replace 
Abstract: Causal reasoning and discovery, two fundamental tasks of causal analysis, often face challenges in applications due to the complexity, noisiness, and high-dimensionality of real-world data. Despite recent progress in identifying latent causal structures using causal representation learning (CRL), what makes learned representations useful for causal downstream tasks and how to evaluate them are still not well understood. In this paper, we reinterpret CRL using a measurement model framework, where the learned representations are viewed as proxy measurements of the latent causal variables. Our approach clarifies the conditions under which learned representations support downstream causal reasoning and provides a principled basis for quantitatively assessing the quality of representations using a new Test-based Measurement EXclusivity (T-MEX) score. We validate T-MEX across diverse causal inference scenarios, including numerical simulations and real-world ecological video analysis, demonstrating that the proposed framework and corresponding score effectively assess the identification of learned representations and their usefulness for causal downstream tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Out of the Shadows: Exploring a Latent Space for Neural Network Verification</title>
<link>https://arxiv.org/abs/2505.17854</link>
<guid>https://arxiv.org/abs/2505.17854</guid>
<content:encoded><![CDATA[
<div> Neural networks; Formal verification; Latent space; Projection-based set representation; Zonotopes <br />
Summary:<br />
This study introduces a novel approach for formal verification of neural networks by designing a latent space that enables iterative input refinement based on output specifications. By leveraging projection-based set representations like zonotopes, the approach allows for efficient transfer of constraints between input and output spaces. The verification tool developed based on this approach significantly reduces the number of subproblems in a branch-and-bound procedure, achieving competitive performance with potential to rank highly in neural network verification competitions. Unlike many existing methods, the tool relies solely on matrix operations, enabling efficient GPU acceleration and enhancing overall speed. This innovative approach addresses the conservatism issues often encountered in neural network verification, providing a promising solution for ensuring the safety of neural networks in critical applications. <br /> <div>
arXiv:2505.17854v2 Announce Type: replace 
Abstract: Neural networks are ubiquitous. However, they are often sensitive to small input changes. Hence, to prevent unexpected behavior in safety-critical applications, their formal verification -- a notoriously hard problem -- is necessary. Many state-of-the-art verification algorithms use reachability analysis or abstract interpretation to enclose the set of possible outputs of a neural network. Often, the verification is inconclusive due to the conservatism of the enclosure. To address this problem, we design a novel latent space for formal verification that enables the transfer of output specifications to the input space for an iterative specification-driven input refinement, i.e., we iteratively reduce the set of possible inputs to only enclose the unsafe ones. The latent space is constructed from a novel view of projection-based set representations, e.g., zonotopes, which are commonly used in reachability analysis of neural networks. A projection-based set representation is a "shadow" of a higher-dimensional set -- a latent space -- that does not change during a set propagation through a neural network. Hence, the input set and the output enclosure are "shadows" of the same latent space that we can use to transfer constraints. We present an efficient verification tool for neural networks that uses our iterative refinement to significantly reduce the number of subproblems in a branch-and-bound procedure. Using zonotopes as a set representation, unlike many other state-of-the-art approaches, our approach can be realized by only using matrix operations, which enables a significant speed-up through efficient GPU acceleration. We demonstrate that our tool achieves competitive performance, which would place it among the top-ranking tools of the last neural network verification competition (VNN-COMP'24).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BLAST: Balanced Sampling Time Series Corpus for Universal Forecasting Models</title>
<link>https://arxiv.org/abs/2505.17871</link>
<guid>https://arxiv.org/abs/2505.17871</guid>
<content:encoded><![CDATA[
<div> Keywords: universal time series forecasting, data diversity, BLAST, pre-training corpus, grid sampling

Summary:
BLAST is a pre-training corpus designed to enhance data diversity for universal time series forecasting models. It incorporates 321 billion observations from various datasets and uses statistical metrics to characterize time series patterns. The data is clustered using grid-based partitioning for pattern-oriented sampling. By integrating grid sampling and mixup techniques, BLAST ensures a balanced coverage of diverse patterns. Models pre-trained on BLAST achieve state-of-the-art performance with fewer computational resources. The study highlights the importance of data diversity in improving training efficiency and model performance for the universal forecasting task. <div>
arXiv:2505.17871v2 Announce Type: replace 
Abstract: The advent of universal time series forecasting models has revolutionized zero-shot forecasting across diverse domains, yet the critical role of data diversity in training these models remains underexplored. Existing large-scale time series datasets often suffer from inherent biases and imbalanced distributions, leading to suboptimal model performance and generalization. To address this gap, we introduce BLAST, a novel pre-training corpus designed to enhance data diversity through a balanced sampling strategy. First, BLAST incorporates 321 billion observations from publicly available datasets and employs a comprehensive suite of statistical metrics to characterize time series patterns. Then, to facilitate pattern-oriented sampling, the data is implicitly clustered using grid-based partitioning. Furthermore, by integrating grid sampling and grid mixup techniques, BLAST ensures a balanced and representative coverage of diverse patterns. Experimental results demonstrate that models pre-trained on BLAST achieve state-of-the-art performance with a fraction of the computational resources and training tokens required by existing methods. Our findings highlight the pivotal role of data diversity in improving both training efficiency and model performance for the universal forecasting task.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mixture of Low Rank Adaptation with Partial Parameter Sharing for Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.17872</link>
<guid>https://arxiv.org/abs/2505.17872</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-task forecasting, Expressiveness Bottleneck, LoRA modules, Mixture-of-LoRA (MoLA) model, Time-series forecasting

Summary:
Multi-task forecasting in time-series forecasting (TSF) faces the Expressiveness Bottleneck, leading to errors even with optimal representations. To overcome this limitation, a two-stage framework is proposed: first pre-training a foundation model for one-step-ahead prediction, then adapting it using step-specific LoRA modules. The Mixture-of-LoRA (MoLA) model uses adaptively weighted LoRA experts to achieve partial parameter sharing across steps, enhancing efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experimental results demonstrate that MoLA significantly improves model expressiveness and outperforms existing time-series forecasting methods. The code for MoLA is available at https://anonymous.4open.science/r/MoLA-BC92. 

<br /><br />Summary: <div>
arXiv:2505.17872v2 Announce Type: replace 
Abstract: Multi-task forecasting has become the standard approach for time-series forecasting (TSF). However, we show that it suffers from an Expressiveness Bottleneck, where predictions at different time steps share the same representation, leading to unavoidable errors even with optimal representations. To address this issue, we propose a two-stage framework: first, pre-train a foundation model for one-step-ahead prediction; then, adapt it using step-specific LoRA modules.This design enables the foundation model to handle any number of forecast steps while avoiding the expressiveness bottleneck. We further introduce the Mixture-of-LoRA (MoLA) model, which employs adaptively weighted LoRA experts to achieve partial parameter sharing across steps. This approach enhances both efficiency and forecasting performance by exploiting interdependencies between forecast steps. Experiments show that MoLA significantly improves model expressiveness and outperforms state-of-the-art time-series forecasting methods. Code is available at https://anonymous.4open.science/r/MoLA-BC92.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Directed Semi-Simplicial Learning with Applications to Brain Activity Decoding</title>
<link>https://arxiv.org/abs/2505.17939</link>
<guid>https://arxiv.org/abs/2505.17939</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Topological Deep Learning, Semi-Simplicial Neural Networks, Routing-SSNs, Brain dynamics representation learning

Summary:
Semi-Simplicial Neural Networks (SSNs) are introduced as a class of Topological Deep Learning (TDL) models that operate on semi-simplicial sets to capture directed higher-order motifs in complex systems such as brain networks. Routing-SSNs dynamically select informative relations in a learnable way, enhancing scalability. SSNs are proven to be more expressive than standard graph and TDL models. A new framework for brain dynamics representation learning based on SSNs' ability to recover topological descriptors is presented, achieving state-of-the-art performance in brain dynamics classification tasks. Empirical results show that SSNs outperform other models by up to 50% in accuracy. Additionally, SSNs demonstrate competitive performance in node classification and edge regression tasks. The code and data will be made publicly available.<br /><br />Summary: <div>
arXiv:2505.17939v2 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) excel at learning from pairwise interactions but often overlook multi-way and hierarchical relationships. Topological Deep Learning (TDL) addresses this limitation by leveraging combinatorial topological spaces. However, existing TDL models are restricted to undirected settings and fail to capture the higher-order directed patterns prevalent in many complex systems, e.g., brain networks, where such interactions are both abundant and functionally significant. To fill this gap, we introduce Semi-Simplicial Neural Networks (SSNs), a principled class of TDL models that operate on semi-simplicial sets -- combinatorial structures that encode directed higher-order motifs and their directional relationships. To enhance scalability, we propose Routing-SSNs, which dynamically select the most informative relations in a learnable manner. We prove that SSNs are strictly more expressive than standard graph and TDL models. We then introduce a new principled framework for brain dynamics representation learning, grounded in the ability of SSNs to provably recover topological descriptors shown to successfully characterize brain activity. Empirically, SSNs achieve state-of-the-art performance on brain dynamics classification tasks, outperforming the second-best model by up to 27%, and message passing GNNs by up to 50% in accuracy. Our results highlight the potential of principled topological models for learning from structured brain data, establishing a unique real-world case study for TDL. We also test SSNs on standard node classification and edge regression tasks, showing competitive performance. We will make the code and data publicly available.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Revealing the Effectiveness of Small-Scale Fine-tuning in R1-style Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.17988</link>
<guid>https://arxiv.org/abs/2505.17988</guid>
<content:encoded><![CDATA[
<div> RL, SFT, efficiency, re-distillation, empirical success  
Summary:  
- Small-scale SFT influences RL but is inefficient.  
- A framework comparing SFT and RL efficiency using sample effect measurement is proposed.  
- SFT efficiency is limited by training data based on hypothetical analysis.  
- Re-distillation technique is introduced to fine-tune pretrain model through distillation from RL-trained policy.  
- Empirical verification on Knight & Knave and MATH datasets shows re-distillation's efficiency, surpassing DeepSeek-V3-0324 with fewer samples and less computation, and matching instruct-tuned variant without RL on MATH with only 500 samples.  
<br /><br />Summary: <div>
arXiv:2505.17988v2 Announce Type: replace 
Abstract: R1-style Reinforcement Learning (RL) significantly enhances Large Language Models' reasoning capabilities, yet the mechanism behind rule-based RL remains unclear. We found that small-scale SFT has significant influence on RL but shows poor efficiency. To explain our observations, we propose an analytical framework and compare the efficiency of SFT and RL by measuring sample effect. Hypothetical analysis show that SFT efficiency is limited by training data. Guided by our analysis, we propose Re-distillation, a technique that fine-tunes pretrain model through small-scale distillation from the RL-trained policy. Experiments on Knight & Knave and MATH datasets demonstrate re-distillation's surprising efficiency: re-distilled models match RL performance with far fewer samples and less computation. Empirical verification shows that sample effect is a good indicator of performance improvements. As a result, on K&amp;K dataset, our re-distilled Qwen2.5-1.5B model surpasses DeepSeek-V3-0324 with only 1K SFT samples. On MATH, Qwen2.5-1.5B fine-tuned with re-distilled 500 samples matches its instruct-tuned variant without RL. Our work explains several interesting phenomena in R1-style RL, shedding light on the mechanisms behind its empirical success. Code is available at: https://github.com/on1262/deep-reasoning
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Analyzing and Understanding the Limitations of VAPO: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2505.17997</link>
<guid>https://arxiv.org/abs/2505.17997</guid>
<content:encoded><![CDATA[
<div> Keywords: VAPO framework, reinforcement learning, long chain-of-thought reasoning tasks, large language models, value function approximation

Summary:<br />
The paper discusses the VAPO framework, which has shown significant empirical success in enhancing the efficiency and reliability of reinforcement learning for tasks requiring long chain-of-thought reasoning with large language models. It addresses challenges such as value model bias, sequence length variations, and sparse reward signals, leading to state-of-the-art performance. Theoretical exploration of VAPO's mechanisms and limitations is crucial for future advancements. Key areas of discussion include value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and challenges related to exploration and generalization. This theoretical perspective aims to guide future research efforts in developing more robust and generalizable reasoning agents.<br /><br />Summary: <div>
arXiv:2505.17997v2 Announce Type: replace 
Abstract: The VAPO framework has demonstrated significant empirical success in enhancing the efficiency and reliability of reinforcement learning for long chain-of-thought (CoT) reasoning tasks with large language models (LLMs). By systematically addressing challenges such as value model bias, heterogeneous sequence lengths, and sparse reward signals, VAPO achieves state-of-the-art performance. While its practical benefits are evident, a deeper theoretical understanding of its underlying mechanisms and potential limitations is crucial for guiding future advancements. This paper aims to initiate such a discussion by exploring VAPO from a theoretical perspective, highlighting areas where its assumptions might be challenged and where further investigation could yield more robust and generalizable reasoning agents. We delve into the intricacies of value function approximation in complex reasoning spaces, the optimality of adaptive advantage estimation, the impact of token-level optimization, and the enduring challenges of exploration and generalization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Many Heads Are Better Than One: Improved Scientific Idea Generation by A LLM-Based Multi-Agent System</title>
<link>https://arxiv.org/abs/2410.09403</link>
<guid>https://arxiv.org/abs/2410.09403</guid>
<content:encoded><![CDATA[
<div> Keywords: AI, large language models, multi-agent system, scientific research, collaboration <br />
Summary: <br />
The article introduces Virtual Scientists (VirSci), a multi-agent system based on large language models (LLMs) designed to simulate the collaborative nature of scientific research. VirSci utilizes a team of agents to collectively generate, evaluate, and refine research ideas, surpassing existing methods in producing novel scientific concepts. Through extensive experimentation, the article demonstrates the effectiveness of this approach in fostering innovation. The study also delves into the collaborative mechanisms that enhance idea novelty, offering valuable insights for future research endeavors. The code for VirSci is openly available for further exploration and development. In summary, VirSci represents a significant leap forward in the quest for autonomous scientific discovery by leveraging AI technologies and collaborative teamwork. <br /> <div>
arXiv:2410.09403v4 Announce Type: replace-cross 
Abstract: The rapid advancement of scientific progress requires innovative tools that can accelerate knowledge discovery. Although recent AI methods, particularly large language models (LLMs), have shown promise in tasks such as hypothesis generation and experimental design, they fall short of replicating the collaborative nature of real-world scientific practices, where diverse experts work together in teams to tackle complex problems. To address the limitations, we propose an LLM-based multi-agent system, i.e., Virtual Scientists (VirSci), designed to mimic the teamwork inherent in scientific research. VirSci organizes a team of agents to collaboratively generate, evaluate, and refine research ideas. Through comprehensive experiments, we demonstrate that this multi-agent approach outperforms the state-of-the-art method in producing novel scientific ideas. We further investigate the collaboration mechanisms that contribute to its tendency to produce ideas with higher novelty, offering valuable insights to guide future research and illuminating pathways toward building a robust system for autonomous scientific discovery. The code is available at https://github.com/open-sciencelab/Virtual-Scientists.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When More is Less: Understanding Chain-of-Thought Length in LLMs</title>
<link>https://arxiv.org/abs/2502.07266</link>
<guid>https://arxiv.org/abs/2502.07266</guid>
<content:encoded><![CDATA[
<div> Optimal CoT Length, Chain-of-Thought Reasoning, Large Language Models, Performance, Simplicity Bias<br />
<br />Summary: This paper challenges the assumption that longer Chain-of-Thought (CoT) reasoning is always better for Large Language Models (LLMs). Through real-world observations, experiments, and analysis, it is shown that task accuracy follows an inverted U-shaped curve with CoT length, with performance initially improving but eventually declining with more CoT steps. The optimal CoT length scales with task difficulty and model capability, revealing a simplicity bias where more capable models prefer shorter CoTs. This bias is also observed in Reinforcement Learning training where models gravitate towards shorter CoTs as accuracy improves. A theoretical model is proposed to explain these phenomena and guide the calibration of CoTs for optimal reasoning performance tailored to task complexity and model capability. Practical benefits are demonstrated by training with optimally-lengthed CoTs and employing length-aware filtering during inference. Overall, this study offers insights into the "overthinking" phenomenon and provides guidelines for adaptive CoT calibration in LLMs. <br /><br /> <div>
arXiv:2502.07266v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) employ Chain-of-Thought (CoT) reasoning to deconstruct complex problems. While longer CoTs are often presumed superior, this paper challenges that notion, arguing that longer is not always better. Drawing on combined evidence from real-world observations, controlled experiments, and theoretical analysis, we demonstrate that task accuracy typically follows an inverted U-shaped curve with CoT length, where performance initially improves but eventually decreases as the number of CoT steps increases. With controlled experiments, we further uncover the scaling behaviors of the optimal CoT length: it increases with task difficulty but decreases with model capability, exposing an inherent simplicity bias where more capable models favor shorter, more efficient CoT reasoning. This bias is also evident in Reinforcement Learning (RL) training, where models gravitate towards shorter CoTs as their accuracy improves. To have a deep understanding of these dynamics, we establish a simple theoretical model that formally proves these phenomena, including the optimal length's scaling laws and the emergence of simplicity bias during RL. Guided by this framework, we demonstrate significant practical benefits from training with optimally-lengthed CoTs and employing length-aware filtering at inference. These findings offer both a principled understanding of the "overthinking" phenomenon and multiple practical guidelines for CoT calibration, enabling LLMs to achieve optimal reasoning performance with adaptive CoTs tailored to task complexity and model capability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to Upscale Neural Networks with Scaling Law? A Survey and Practical Guidelines</title>
<link>https://arxiv.org/abs/2502.12051</link>
<guid>https://arxiv.org/abs/2502.12051</guid>
<content:encoded><![CDATA[
<div> scaling laws, AI models, model performance, computational resources, neural networks

Summary:
Neural scaling laws have transformed the design and optimization of large-scale AI models, revealing predictable relationships between model size, dataset volume, and computational resources. While power-law relationships in model performance have led to compute-optimal scaling strategies, recent studies have shown limitations across different architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Scaling behaviors also differ across domains like vision, reinforcement learning, and fine-tuning. It is crucial to develop more nuanced approaches in scaling strategies to address challenges such as data efficiency, inference scaling, and architecture-specific constraints. Adaptive scaling strategies tailored to real-world applications are recommended, as scaling laws may not always generalize across all architectures and training strategies. <div>
arXiv:2502.12051v3 Announce Type: replace-cross 
Abstract: Neural scaling laws have revolutionized the design and optimization of large-scale AI models by revealing predictable relationships between model size, dataset volume, and computational resources. Early research established power-law relationships in model performance, leading to compute-optimal scaling strategies. However, recent studies highlighted their limitations across architectures, modalities, and deployment contexts. Sparse models, mixture-of-experts, retrieval-augmented learning, and multimodal models often deviate from traditional scaling patterns. Moreover, scaling behaviors vary across domains such as vision, reinforcement learning, and fine-tuning, underscoring the need for more nuanced approaches. In this survey, we synthesize insights from over 50 studies, examining the theoretical foundations, empirical findings, and practical implications of scaling laws. We also explore key challenges, including data efficiency, inference scaling, and architecture-specific constraints, advocating for adaptive scaling strategies tailored to real-world applications. We suggest that while scaling laws provide a useful guide, they do not always generalize across all architectures and training strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FMEnets: Flow, Material, and Energy networks for non-ideal plug flow reactor design</title>
<link>https://arxiv.org/abs/2505.20300</link>
<guid>https://arxiv.org/abs/2505.20300</guid>
<content:encoded><![CDATA[
<div> framework, machine learning, plug flow reactors, FMEnets, physics-informed<br />
<br />
Summary: 
The article introduces FMEnets, a physics-informed machine learning framework for non-ideal plug flow reactors. FMEnets integrates fundamental equations to create a multi-scale network model, enabling both forward and inverse problem-solving. It predicts various profiles using limited input data and can infer unknown parameters and states. FMEnets can be implemented as FME-PINNs or FME-KANs, with the latter being more robust to noise. The framework is tested on different reaction scenarios, showing accuracy comparable to finite element simulations. With relative errors below 2.5% for unknown parameters, FMEnets provides an efficient alternative for reactor design and optimization. It also allows the integration of empirical correlations and limited experimental data, guiding reactor design effectively. <div>
arXiv:2505.20300v1 Announce Type: new 
Abstract: We propose FMEnets, a physics-informed machine learning framework for the design and analysis of non-ideal plug flow reactors. FMEnets integrates the fundamental governing equations (Navier-Stokes for fluid flow, material balance for reactive species transport, and energy balance for temperature distribution) into a unified multi-scale network model. The framework is composed of three interconnected sub-networks with independent optimizers that enable both forward and inverse problem-solving. In the forward mode, FMEnets predicts velocity, pressure, species concentrations, and temperature profiles using only inlet and outlet information. In the inverse mode, FMEnets utilizes sparse multi-residence-time measurements to simultaneously infer unknown kinetic parameters and states. FMEnets can be implemented either as FME-PINNs, which employ conventional multilayer perceptrons, or as FME-KANs, based on Kolmogorov-Arnold Networks. Comprehensive ablation studies highlight the critical role of the FMEnets architecture in achieving accurate predictions. Specifically, FME-KANs are more robust to noise than FME-PINNs, although both representations are comparable in accuracy and speed in noise-free conditions. The proposed framework is applied to three different sets of reaction scenarios and is compared with finite element simulations. FMEnets effectively captures the complex interactions, achieving relative errors less than 2.5% for the unknown kinetic parameters. The new network framework not only provides a computationally efficient alternative for reactor design and optimization, but also opens new avenues for integrating empirical correlations, limited and noisy experimental data, and fundamental physical equations to guide reactor design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint-stochastic-approximation Random Fields with Application to Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2505.20330</link>
<guid>https://arxiv.org/abs/2505.20330</guid>
<content:encoded><![CDATA[
<div> Keywords: deep generative models, semi-supervised learning, mode missing, mode covering, joint-stochastic-approximation random fields 

Summary: 
The study explores issues in deep generative models (DGMs) like GANs and VAEs used in semi-supervised learning (SSL). It identifies problems like mode missing and mode covering in generation and the conflict between classification and generation in SSL with directed generative models. To overcome these challenges, the researchers introduce joint-stochastic-approximation random fields (JRFs) as a new algorithm for building deep undirected generative models. Synthetic experiments show that JRFs effectively balance mode covering and mode missing, accurately reflect empirical data distribution, and achieve competitive classification results on popular datasets like MNIST, SVHN, and CIFAR-10 in SSL. Additionally, JRFs also demonstrate good generation capabilities. Overall, JRFs offer a promising solution for improving the performance of deep generative models in semi-supervised learning tasks. 

<br /><br />Summary: <div>
arXiv:2505.20330v1 Announce Type: new 
Abstract: Our examination of deep generative models (DGMs) developed for semi-supervised learning (SSL), mainly GANs and VAEs, reveals two problems. First, mode missing and mode covering phenomenons are observed in genertion with GANs and VAEs. Second, there exists an awkward conflict between good classification and good generation in SSL by employing directed generative models. To address these problems, we formally present joint-stochastic-approximation random fields (JRFs) -- a new family of algorithms for building deep undirected generative models, with application to SSL. It is found through synthetic experiments that JRFs work well in balancing mode covering and mode missing, and match the empirical data distribution well. Empirically, JRFs achieve good classification results comparable to the state-of-art methods on widely adopted datasets -- MNIST, SVHN, and CIFAR-10 in SSL, and simultaneously perform good generation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PDFBench: A Benchmark for De novo Protein Design from Function</title>
<link>https://arxiv.org/abs/2505.20346</link>
<guid>https://arxiv.org/abs/2505.20346</guid>
<content:encoded><![CDATA[
<div> Keywords: natural language processing, multimodal learning, de novo protein design, PDFBench, evaluation metrics

Summary:
PDFBench introduces a comprehensive benchmark for evaluating de novo protein design, addressing the challenges of proprietary datasets and evaluation rubrics. The benchmark supports description-guided design and keyword-guided design tasks, encompassing 22 metrics covering sequence plausibility, structural fidelity, language-protein alignment, novelty, and diversity. Five state-of-the-art baselines are evaluated, highlighting their strengths and weaknesses across tasks. The analysis of inter-metric correlations provides insights into the relationships between different metric categories and suggests guidelines for metric selection. PDFBench aims to drive future advancements in function-driven de novo protein design by offering a unified evaluation framework. 

<br /><br />Summary: <div>
arXiv:2505.20346v1 Announce Type: new 
Abstract: In recent years, while natural language processing and multimodal learning have seen rapid advancements, the field of de novo protein design has also experienced significant growth. However, most current methods rely on proprietary datasets and evaluation rubrics, making fair comparisons between different approaches challenging. Moreover, these methods often employ evaluation metrics that capture only a subset of the desired properties of designed proteins, lacking a comprehensive assessment framework. To address these, we introduce PDFBench, the first comprehensive benchmark for evaluating de novo protein design from function. PDFBench supports two tasks: description-guided design and keyword-guided design. To ensure fair and multifaceted evaluation, we compile 22 metrics covering sequence plausibility, structural fidelity, and language-protein alignment, along with measures of novelty and diversity. We evaluate five state-of-the-art baselines, revealing their respective strengths and weaknesses across tasks. Finally, we analyze inter-metric correlations, exploring the relationships between four categories of metrics, and offering guidelines for metric selection. PDFBench establishes a unified framework to drive future advances in function-driven de novo protein design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decision Flow Policy Optimization</title>
<link>https://arxiv.org/abs/2505.20350</link>
<guid>https://arxiv.org/abs/2505.20350</guid>
<content:encoded><![CDATA[
<div> Flow-based models, decision-making, reinforcement learning, multi-modal action distributions, robotic control<br />
<br />
Summary: 
Generative models have shown significant progress in various fields. By incorporating flow-based models into reinforcement learning, complex multi-modal action distributions can be effectively modeled, leading to better robotic control in continuous action spaces. Existing methods often treat generative models as behavior models for fitting action distributions from datasets, separate from policy optimization. This separation limits training and performance. The proposed Decision Flow framework integrates multi-modal action distribution modeling and policy optimization. It formulates action generation as a flow decision-making process, enabling seamless optimization of the flow policy while capturing multi-modal action distributions. Rigorous proofs support Decision Flow's effectiveness, validated through extensive experiments in offline RL environments. Results show competitive or superior performance compared to established baselines.<br /><br /> <div>
arXiv:2505.20350v1 Announce Type: new 
Abstract: In recent years, generative models have shown remarkable capabilities across diverse fields, including images, videos, language, and decision-making. By applying powerful generative models such as flow-based models to reinforcement learning, we can effectively model complex multi-modal action distributions and achieve superior robotic control in continuous action spaces, surpassing the limitations of single-modal action distributions with traditional Gaussian-based policies. Previous methods usually adopt the generative models as behavior models to fit state-conditioned action distributions from datasets, with policy optimization conducted separately through additional policies using value-based sample weighting or gradient-based updates. However, this separation prevents the simultaneous optimization of multi-modal distribution fitting and policy improvement, ultimately hindering the training of models and degrading the performance. To address this issue, we propose Decision Flow, a unified framework that integrates multi-modal action distribution modeling and policy optimization. Specifically, our method formulates the action generation procedure of flow-based models as a flow decision-making process, where each action generation step corresponds to one flow decision. Consequently, our method seamlessly optimizes the flow policy while capturing multi-modal action distributions. We provide rigorous proofs of Decision Flow and validate the effectiveness through extensive experiments across dozens of offline RL environments. Compared with established offline RL baselines, the results demonstrate that our method achieves or matches the SOTA performance.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FastCache: Fast Caching for Diffusion Transformer Through Learnable Linear Approximation</title>
<link>https://arxiv.org/abs/2505.20353</link>
<guid>https://arxiv.org/abs/2505.20353</guid>
<content:encoded><![CDATA[
<div> Keywords: Diffusion Transformers, FastCache, generative models, inference acceleration, compression

Summary: 
FastCache is a novel framework designed to accelerate the inference process of Diffusion Transformers (DiT), which are known for their computational intensity. This framework leverages hidden-state-level caching and compression techniques to optimize the generation process while maintaining output quality. It incorporates a spatial-aware token selection mechanism to filter out redundant tokens based on their saliency in the hidden states. Additionally, a transformer-level cache is utilized to reuse latent activations across timesteps when statistically insignificant changes occur. These modules work together to reduce unnecessary computations and memory usage without compromising the fidelity of generation. Theoretical analysis demonstrates that FastCache maintains a bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across different DiT variants confirm significant reductions in latency and memory usage compared to other caching methods, while still producing high-quality generation outputs. The code implementation of FastCache is available on GitHub for further exploration. 

<br /><br />Summary: <div>
arXiv:2505.20353v1 Announce Type: new 
Abstract: Diffusion Transformers (DiT) are powerful generative models but remain computationally intensive due to their iterative structure and deep transformer stacks. To alleviate this inefficiency, we propose FastCache, a hidden-state-level caching and compression framework that accelerates DiT inference by exploiting redundancy within the model's internal representations. FastCache introduces a dual strategy: (1) a spatial-aware token selection mechanism that adaptively filters redundant tokens based on hidden state saliency, and (2) a transformer-level cache that reuses latent activations across timesteps when changes are statistically insignificant. These modules work jointly to reduce unnecessary computation while preserving generation fidelity through learnable linear approximation. Theoretical analysis shows that FastCache maintains bounded approximation error under a hypothesis-testing-based decision rule. Empirical evaluations across multiple DiT variants demonstrate substantial reductions in latency and memory usage, with best generation output quality compared to other cache methods, as measured by FID and t-FID. Code implementation of FastCache is available on GitHub at https://github.com/NoakLiu/FastCache-xDiT.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraLoRA: Granular Low-Rank Adaptation for Parameter-Efficient Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.20355</link>
<guid>https://arxiv.org/abs/2505.20355</guid>
<content:encoded><![CDATA[
<div> adaptation, low-rank, generative models, fine-tuning, GraLoRA

Summary:
Granular Low-Rank Adaptation (GraLoRA) is introduced as a novel method to address the limitations of Low-Rank Adaptation (LoRA) in fine-tuning generative models. The fundamental issue of overfitting when widening the bottleneck in LoRA is identified as stemming from gradient entanglement in unrelated input channels due to its structural bottleneck. GraLoRA partitions weight matrices into sub-blocks, each with its own low-rank adapter, effectively increasing representational capacity and approximating full fine-tuning (FFT) behavior. Experimental results on code generation and commonsense reasoning benchmarks demonstrate GraLoRA's superior performance over LoRA and other baselines, with up to an 8.5% absolute gain in Pass@1 on HumanEval+. This improvement is consistent across different model sizes and rank settings, making GraLoRA a scalable and robust solution for parameter-efficient fine-tuning. <div>
arXiv:2505.20355v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is a popular method for parameter-efficient fine-tuning (PEFT) of generative models, valued for its simplicity and effectiveness. Despite recent enhancements, LoRA still suffers from a fundamental limitation: overfitting when the bottleneck is widened. It performs best at ranks 32-64, yet its accuracy stagnates or declines at higher ranks, still falling short of full fine-tuning (FFT) performance. We identify the root cause as LoRA's structural bottleneck, which introduces gradient entanglement to the unrelated input channels and distorts gradient propagation. To address this, we introduce a novel structure, Granular Low-Rank Adaptation (GraLoRA) that partitions weight matrices into sub-blocks, each with its own low-rank adapter. With negligible computational or storage cost, GraLoRA overcomes LoRA's limitations, effectively increases the representational capacity, and more closely approximates FFT behavior. Experiments on code generation and commonsense reasoning benchmarks show that GraLoRA consistently outperforms LoRA and other baselines, achieving up to +8.5% absolute gain in Pass@1 on HumanEval+. These improvements hold across model sizes and rank settings, making GraLoRA a scalable and robust solution for PEFT. Code, data, and scripts are available at https://github.com/SqueezeBits/GraLoRA.git
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning and Interpreting Gravitational-Wave Features from CNNs with a Random Forest Approach</title>
<link>https://arxiv.org/abs/2505.20357</link>
<guid>https://arxiv.org/abs/2505.20357</guid>
<content:encoded><![CDATA[
<div> Keywords: convolutional neural networks, gravitational wave detection, interpretability, random forest classifier, feature extraction<br />
Summary:<br />
- The article introduces a hybrid architecture for gravitational wave detection, combining a CNN-based feature extractor with a random forest classifier for improved interpretability and performance.
- Physically interpretable metrics such as variance, SNR, waveform overlap, and peak amplitude are computed from the final convolutional layer, enhancing decision boundaries in the RF classifier.
- The hybrid model outperforms a baseline CNN model, achieving a 21% relative improvement in sensitivity and showing better detection of low-SNR signals.
- Feature attribution analysis via the RF model highlights the importance of both CNN-extracted and handcrafted features, with learned variance and CNN outputs being particularly informative.
- This approach demonstrates the value of incorporating physically motivated post-processing of CNN feature maps for interpretable and efficient gravitational wave detection, effectively leveraging deep learning techniques with domain knowledge.<br /> 
Summary: <div>
arXiv:2505.20357v1 Announce Type: new 
Abstract: Convolutional neural networks (CNNs) have become widely adopted in gravitational wave (GW) detection pipelines due to their ability to automatically learn hierarchical features from raw strain data. However, the physical meaning of these learned features remains underexplored, limiting the interpretability of such models. In this work, we propose a hybrid architecture that combines a CNN-based feature extractor with a random forest (RF) classifier to improve both detection performance and interpretability. Unlike prior approaches that directly connect classifiers to CNN outputs, our method introduces four physically interpretable metrics - variance, signal-to-noise ratio (SNR), waveform overlap, and peak amplitude - computed from the final convolutional layer. These are jointly used with the CNN output in the RF classifier to enable more informed decision boundaries. Tested on long-duration strain datasets, our hybrid model outperforms a baseline CNN model, achieving a relative improvement of 21\% in sensitivity at a fixed false alarm rate of 10 events per month. Notably, it also shows improved detection of low-SNR signals (SNR $\le$ 10), which are especially vulnerable to misclassification in noisy environments. Feature attribution via the RF model reveals that both CNN-extracted and handcrafted features contribute significantly to classification decisions, with learned variance and CNN outputs ranked among the most informative. These findings suggest that physically motivated post-processing of CNN feature maps can serve as a valuable tool for interpretable and efficient GW detection, bridging the gap between deep learning and domain knowledge.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-aware Direct Preference Optimization under Nested Risk Measure</title>
<link>https://arxiv.org/abs/2505.20359</link>
<guid>https://arxiv.org/abs/2505.20359</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, fine-tuning, human values, risk-awareness, preferences<br />
Summary:<br />
When fine-tuning Large Language Models to align with human values, maximizing reward can lead to better performance but also introduces risks. Existing methods use KL divergence to constrain deviations, but it may not be sufficient for tight risk control. This paper introduces Ra-DPO, a novel approach that incorporates risk-awareness using nested risk measures. It formulates a constrained risk-aware advantage function maximization problem and converts the Bradley-Terry model into a token-level representation. The objective function maximizes policy likelihood while suppressing deviation between trained and reference models using sequential risk ratio. Experimental results on three datasets show Ra-DPO's superior performance in balancing alignment and model drift. The code is available at https://github.com/zlj123-max/Ra-DPO. <br /> <div>
arXiv:2505.20359v1 Announce Type: new 
Abstract: When fine-tuning pre-trained Large Language Models (LLMs) to align with human values and intentions, maximizing the estimated reward can lead to superior performance, but it also introduces potential risks due to deviations from the reference model's intended behavior. Most existing methods typically introduce KL divergence to constrain deviations between the trained model and the reference model; however, this may not be sufficient in certain applications that require tight risk control. In this paper, we introduce Risk-aware Direct Preference Optimization (Ra-DPO), a novel approach that incorporates risk-awareness by employing a class of nested risk measures. This approach formulates a constrained risk-aware advantage function maximization problem and then converts the Bradley-Terry model into a token-level representation. The objective function maximizes the likelihood of the policy while suppressing the deviation between a trained model and the reference model using a sequential risk ratio, thereby enhancing the model's risk-awareness. Experimental results across three open-source datasets: IMDb Dataset, Anthropic HH Dataset, and AlpacaEval, demonstrate the proposed method's superior performance in balancing alignment performance and model drift. Our code is opensourced at https://github.com/zlj123-max/Ra-DPO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GRAPE: Optimize Data Mixture for Group Robust Multi-target Adaptive Pretraining</title>
<link>https://arxiv.org/abs/2505.20380</link>
<guid>https://arxiv.org/abs/2505.20380</guid>
<content:encoded><![CDATA[
<div> domain reweighting, language models, pretraining, adaptive, multi-target

Summary: 
The paper introduces GRAPE, a multi-source-multi-target domain reweighting framework for optimizing pretraining data mixtures across multiple target tasks simultaneously. GRAPE dynamically adjusts domain and task weights to prioritize tasks based on their learning difficulty during training. This adaptive process is formulated as a minimax optimization problem, utilizing group distributed-robust-optimization to adjust task weights and optimize domain weights for maximum loss reduction on prioritized tasks. Experimental results on ClimbLab and SlimPajama datasets show that GRAPE outperforms baseline methods in reasoning performance across 6 benchmarks. Moreover, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, enhancing language modeling capabilities across 8 low-resource target languages. <br /><br />Summary: <div>
arXiv:2505.20380v1 Announce Type: new 
Abstract: The performance of large language models (LLMs) across diverse downstream applications is fundamentally governed by the quality and composition of their pretraining corpora. Existing domain reweighting algorithms primarily optimize data mixtures for a single target task, thereby resulting in models that overfit to specialized objectives while exhibiting substantial performance degradation on other benchmarks. This paper introduces Group Robust Multi-target Adaptive PrEtraining (GRAPE), a novel multi-source-multi-target domain reweighting framework designed to calibrate pretraining data mixtures for robust performance across multiple target tasks simultaneously. GRAPE dynamically adjusts sampling weights across source domains (domain weights) while concurrently modulating task weights that quantify the relative importance of each individual target task. This adaptive process prioritizes tasks based on their learning difficulty throughout training. We formulate this interleaved reweighting mechanism as a minimax optimization problem: The inner maximization adjusts task weights leveraging group distributed-robust-optimization (DRO), where those tasks demonstrating the least improvement under the current data mixture are prioritized with higher weights; The outer minimization then optimizes domain weights to maximize loss reduction on the prioritized tasks. Experiments on ClimbLab and SlimPajama datasets demonstrate that GRAPE consistently outperforms baseline methods in terms of reasoning performance across 6 benchmarks. Furthermore, when applied to multilingual targets, GRAPE effectively identifies optimal training mixtures from mainstream languages, achieving superior language modeling capabilities across 8 low-resource target languages.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Holes in Latent Space: Topological Signatures Under Adversarial Influence</title>
<link>https://arxiv.org/abs/2505.20435</link>
<guid>https://arxiv.org/abs/2505.20435</guid>
<content:encoded><![CDATA[
<div> topological data analysis, language models, adversarial conditions, latent space dynamics, neuron-level framework
Summary: 
This study introduces the use of persistent homology (PH), a tool from topological data analysis, to examine the effects of adversarial conditions on language models (LLMs). The research focuses on two attack modes  backdoor fine-tuning and indirect prompt injection  and analyzes six state-of-the-art LLMs. The findings reveal that adversarial conditions lead to a compression of latent topologies, reducing structural diversity at smaller scales while emphasizing dominant features at larger scales. These topological changes are consistent across different layers, architectures, and model sizes, indicating a deep-seated impact of adversarial effects within the network. Additionally, a neuron-level PH framework is introduced to better understand information flow and transformation within and between layers, providing insights into the underlying mechanisms of shifts in LLM representations under distributional shift. This study highlights how PH can offer a systematic and comprehensive approach to interpreting representational dynamics in LLMs under adversarial conditions. 
<br /><br />Summary: <div>
arXiv:2505.20435v1 Announce Type: new 
Abstract: Understanding how adversarial conditions affect language models requires techniques that capture both global structure and local detail within high-dimensional activation spaces. We propose persistent homology (PH), a tool from topological data analysis, to systematically characterize multiscale latent space dynamics in LLMs under two distinct attack modes -- backdoor fine-tuning and indirect prompt injection. By analyzing six state-of-the-art LLMs, we show that adversarial conditions consistently compress latent topologies, reducing structural diversity at smaller scales while amplifying dominant features at coarser ones. These topological signatures are statistically robust across layers, architectures, model sizes, and align with the emergence of adversarial effects deeper in the network. To capture finer-grained mechanisms underlying these shifts, we introduce a neuron-level PH framework that quantifies how information flows and transforms within and across layers. Together, our findings demonstrate that PH offers a principled and unifying approach to interpreting representational dynamics in LLMs, particularly under distributional shift.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HoPE: Hybrid of Position Embedding for Length Generalization in Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20444</link>
<guid>https://arxiv.org/abs/2505.20444</guid>
<content:encoded><![CDATA[
<div> RoPE, VLMs, long-context capabilities, HoPE, video benchmarks <br />
Summary: <br />
Vision-Language Models (VLMs) have shown progress in multimodal tasks but struggle with long-context scenarios, especially in videos. Current methods for extending RoPE to capture spatial-temporal dependencies lack theoretical depth. The proposed Hybrid of Position Embedding (HoPE) addresses this issue by introducing a hybrid frequency allocation strategy and dynamic temporal scaling mechanism. It enhances semantic modeling over long contexts, leading to improved performance in long video understanding and retrieval tasks. Experiments on four video benchmarks demonstrate the effectiveness of HoPE, outperforming existing methods consistently. The code for HoPE is publicly available. <br /> <div>
arXiv:2505.20444v1 Announce Type: new 
Abstract: Vision-Language Models (VLMs) have made significant progress in multimodal tasks. However, their performance often deteriorates in long-context scenarios, particularly long videos. While Rotary Position Embedding (RoPE) has been widely adopted for length generalization in Large Language Models (LLMs), extending vanilla RoPE to capture the intricate spatial-temporal dependencies in videos remains an unsolved challenge. Existing methods typically allocate different frequencies within RoPE to encode 3D positional information. However, these allocation strategies mainly rely on heuristics, lacking in-depth theoretical analysis. In this paper, we first study how different allocation strategies impact the long-context capabilities of VLMs. Our analysis reveals that current multimodal RoPEs fail to reliably capture semantic similarities over extended contexts. To address this issue, we propose HoPE, a Hybrid of Position Embedding designed to improve the long-context capabilities of VLMs. HoPE introduces a hybrid frequency allocation strategy for reliable semantic modeling over arbitrarily long context, and a dynamic temporal scaling mechanism to facilitate robust learning and flexible inference across diverse context lengths. Extensive experiments across four video benchmarks on long video understanding and retrieval tasks demonstrate that HoPE consistently outperforms existing methods, confirming its effectiveness. Code is available at https://github.com/hrlics/HoPE.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time Series Generation Under Data Scarcity: A Unified Generative Modeling Approach</title>
<link>https://arxiv.org/abs/2505.20446</link>
<guid>https://arxiv.org/abs/2505.20446</guid>
<content:encoded><![CDATA[
<div> Generative modeling, time series, data-scarce conditions, performance evaluation, diffusion-based framework<br />
Summary:<br />
A study evaluated generative models in data-scarce settings, revealing a performance gap. A unified diffusion-based framework was proposed, pre-trained on diverse time series datasets for generalizable representations. The model includes dynamic convolutional layers and dataset token conditioning for domain-specific generation. It achieves state-of-the-art performance in few-shot settings without requiring abundant supervision, outperforming domain-specific baselines and even full dataset benchmarks. The model's strength lies in pre-training and cross-domain generalization. The work emphasizes few-shot generative modeling as a pivotal issue in time series research and advocates for unified solutions that efficiently scale across domains.  <br />Summary: <div>
arXiv:2505.20446v1 Announce Type: new 
Abstract: Generative modeling of time series is a central challenge in time series analysis, particularly under data-scarce conditions. Despite recent advances in generative modeling, a comprehensive understanding of how state-of-the-art generative models perform under limited supervision remains lacking. In this work, we conduct the first large-scale study evaluating leading generative models in data-scarce settings, revealing a substantial performance gap between full-data and data-scarce regimes. To close this gap, we propose a unified diffusion-based generative framework that can synthesize high-fidelity time series across diverse domains using just a few examples. Our model is pre-trained on a large, heterogeneous collection of time series datasets, enabling it to learn generalizable temporal representations. It further incorporates architectural innovations such as dynamic convolutional layers for flexible channel adaptation and dataset token conditioning for domain-aware generation. Without requiring abundant supervision, our unified model achieves state-of-the-art performance in few-shot settings-outperforming domain-specific baselines across a wide range of subset sizes. Remarkably, it also surpasses all baselines even when tested on full datasets benchmarks, highlighting the strength of pre-training and cross-domain generalization. We hope this work encourages the community to revisit few-shot generative modeling as a key problem in time series research and pursue unified solutions that scale efficiently across domains. Code is available at https://github.com/azencot-group/ImagenFew.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Active Learning for Multiple Change Point Detection in Non-stationary Time Series with Deep Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.20452</link>
<guid>https://arxiv.org/abs/2505.20452</guid>
<content:encoded><![CDATA[
<div> Active Learning, Deep Gaussian Processes, Multiple change point detection, Non-stationary time series, Spectral analysis

Summary:
The study introduces a novel algorithm that combines Active Learning (AL) with Deep Gaussian Processes (DGPs) to improve multiple change point (MCP) detection in non-stationary time series. By utilizing spectral analysis to identify potential changes and employing AL to strategically select new sampling points, the algorithm enhances efficiency. The integration of DGPs and spectral methods allows for adaptability to various spectral change behaviors and accurate localization of multiple change points. Experimental results on simulated and real-world data demonstrate the algorithm's superiority in detection accuracy and sampling efficiency for non-stationary time series.<br /><br />Summary: <div>
arXiv:2505.20452v1 Announce Type: new 
Abstract: Multiple change point (MCP) detection in non-stationary time series is challenging due to the variety of underlying patterns. To address these challenges, we propose a novel algorithm that integrates Active Learning (AL) with Deep Gaussian Processes (DGPs) for robust MCP detection. Our method leverages spectral analysis to identify potential changes and employs AL to strategically select new sampling points for improved efficiency. By incorporating the modeling flexibility of DGPs with the change-identification capabilities of spectral methods, our approach adapts to diverse spectral change behaviors and effectively localizes multiple change points. Experiments on both simulated and real-world data demonstrate that our method outperforms existing techniques in terms of detection accuracy and sampling efficiency for non-stationary time series.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BlastOFormer: Attention and Neural Operator Deep Learning Methods for Explosive Blast Prediction</title>
<link>https://arxiv.org/abs/2505.20454</link>
<guid>https://arxiv.org/abs/2505.20454</guid>
<content:encoded><![CDATA[
<div> Transformer, blast pressure prediction, surrogate model, CFD simulations, real-time alternative<br />
<br />
Summary: 
The article introduces BlastOFormer, a Transformer-based surrogate model for accurately predicting blast pressure fields. Traditional methods like empirical models and CFD simulations have limitations in capturing complex interactions and are computationally expensive. BlastOFormer, trained on a dataset from blastFoam CFD solver, outperforms CNNs and FNOs, achieving the highest R2 score (0.9516) and low error metrics. It requires only 6.4 milliseconds for inference, significantly faster than CFD simulations. The model shows superior spatial coherence and generalization capabilities, making it a real-time alternative for blast pressure estimation in complex environments. <div>
arXiv:2505.20454v1 Announce Type: new 
Abstract: Accurate prediction of blast pressure fields is essential for applications in structural safety, defense planning, and hazard mitigation. Traditional methods such as empirical models and computational fluid dynamics (CFD) simulations offer limited trade offs between speed and accuracy; empirical models fail to capture complex interactions in cluttered environments, while CFD simulations are computationally expensive and time consuming. In this work, we introduce BlastOFormer, a novel Transformer based surrogate model for full field maximum pressure prediction from arbitrary obstacle and charge configurations. BlastOFormer leverages a signed distance function (SDF) encoding and a grid to grid attention based architecture inspired by OFormer and Vision Transformer (ViT) frameworks. Trained on a dataset generated using the open source blastFoam CFD solver, our model outperforms convolutional neural networks (CNNs) and Fourier Neural Operators (FNOs) across both log transformed and unscaled domains. Quantitatively, BlastOFormer achieves the highest R2 score (0.9516) and lowest error metrics, while requiring only 6.4 milliseconds for inference, more than 600,000 times faster than CFD simulations. Qualitative visualizations and error analyses further confirm BlastOFormer's superior spatial coherence and generalization capabilities. These results highlight its potential as a real time alternative to conventional CFD approaches for blast pressure estimation in complex environments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Avoid Forgetting by Preserving Global Knowledge Gradients in Federated Learning with Non-IID Data</title>
<link>https://arxiv.org/abs/2505.20485</link>
<guid>https://arxiv.org/abs/2505.20485</guid>
<content:encoded><![CDATA[
<div> Analysis, Federated learning, Data heterogeneity, Global decision boundary, Forgetting

Summary:
In the study, the challenges of data heterogeneity in federated learning are addressed. Existing methods suffer from forgetting, where clients forget the global decision boundary and only learn the perfect local one. This forgetting occurs irrespective of the initial weights, leading to suboptimal performance. A new federated learning framework, FedProj, is introduced to overcome this issue. FedProj robustly learns the global decision boundary and prevents forgetting during local training. The framework incorporates a server-side ensemble knowledge transfer loss for better ensemble knowledge fusion and utilizes an episodic memory technique to regulate gradient updates and maintain the learned global decision boundary. Experimental results demonstrate that FedProj outperforms current state-of-the-art methods significantly, showcasing its effectiveness in handling data heterogeneity and improving model performance in federated learning.<br /><br />Summary: <div>
arXiv:2505.20485v1 Announce Type: new 
Abstract: The inevitable presence of data heterogeneity has made federated learning very challenging. There are numerous methods to deal with this issue, such as local regularization, better model fusion techniques, and data sharing. Though effective, they lack a deep understanding of how data heterogeneity can affect the global decision boundary. In this paper, we bridge this gap by performing an experimental analysis of the learned decision boundary using a toy example. Our observations are surprising: (1) we find that the existing methods suffer from forgetting and clients forget the global decision boundary and only learn the perfect local one, and (2) this happens regardless of the initial weights, and clients forget the global decision boundary even starting from pre-trained optimal weights. In this paper, we present FedProj, a federated learning framework that robustly learns the global decision boundary and avoids its forgetting during local training. To achieve better ensemble knowledge fusion, we design a novel server-side ensemble knowledge transfer loss to further calibrate the learned global decision boundary. To alleviate the issue of learned global decision boundary forgetting, we further propose leveraging an episodic memory of average ensemble logits on a public unlabeled dataset to regulate the gradient updates at each step of local training. Experimental results demonstrate that FedProj outperforms state-of-the-art methods by a large margin.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Explicit Neural DAEs: Learning Long-Horizon Dynamical Systems with Algebraic Constraints</title>
<link>https://arxiv.org/abs/2505.20515</link>
<guid>https://arxiv.org/abs/2505.20515</guid>
<content:encoded><![CDATA[
<div> Keywords: scientific machine learning, neural differential equations, conservation laws, manifold projection, constraint violation error<br />
Summary:<br />
- The article introduces Manifold-Projected Neural ODEs (PNODEs) as a method to incorporate hard constraints in neural differential equations for modeling physical systems with conservation laws.
- PNODEs enforce algebraic constraints by projecting each ODE step onto the constraint manifold, showing superior performance in constraint violation error compared to baseline methods.
- The framework is derived from semi-explicit differential-algebraic equations (DAEs) and includes robust iterative and fast approximation variants.
- PNODEs outperform previous relaxation methods and achieve physically consistent long-horizon dynamics with low error tolerance and runtime.
- The results demonstrate that constraint projection provides a simple strategy for combining data-driven techniques with mechanistic modeling in scientific machine learning.<br /><br />Summary: <div>
arXiv:2505.20515v1 Announce Type: new 
Abstract: Despite the promise of scientific machine learning (SciML) in combining data-driven techniques with mechanistic modeling, existing approaches for incorporating hard constraints in neural differential equations (NDEs) face significant limitations. Scalability issues and poor numerical properties prevent these neural models from being used for modeling physical systems with complicated conservation laws. We propose Manifold-Projected Neural ODEs (PNODEs), a method that explicitly enforces algebraic constraints by projecting each ODE step onto the constraint manifold. This framework arises naturally from semi-explicit differential-algebraic equations (DAEs), and includes both a robust iterative variant and a fast approximation requiring a single Jacobian factorization. We further demonstrate that prior works on relaxation methods are special cases of our approach. PNODEs consistently outperform baselines across six benchmark problems achieving a mean constraint violation error below $10^{-10}$. Additionally, PNODEs consistently achieve lower runtime compared to other methods for a given level of error tolerance. These results show that constraint projection offers a simple strategy for learning physically consistent long-horizon dynamics.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Fully FP8 GEMM LLM Training at Scale</title>
<link>https://arxiv.org/abs/2505.20524</link>
<guid>https://arxiv.org/abs/2505.20524</guid>
<content:encoded><![CDATA[
<div> FP8, large language model, LLM architectures, matrix multiplications, low-precision training<br />
<br />
Summary: 
This study introduces a new class of large language model (LLM) architectures that support FP8 computation for all matrix multiplications (GEMMs) within transformer blocks, enabling significant throughput gains without compromising downstream performance. By reducing large outlier activations, stable long-term FP8 training is promoted. Key metrics are identified to monitor low-precision training and predict potential divergences in the future. Through these advancements, the adoption of FP8 data formats for LLM pre-training is expected to increase, offering improved efficiency and scalability in large-scale language modeling tasks. <div>
arXiv:2505.20524v1 Announce Type: new 
Abstract: Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-shot Robust Federated Learning of Independent Component Analysis</title>
<link>https://arxiv.org/abs/2505.20532</link>
<guid>https://arxiv.org/abs/2505.20532</guid>
<content:encoded><![CDATA[
<div> aggregation algorithm, independent component analysis, federated learning, geometric median, k-means clustering

Summary: 
This paper presents a novel robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problems. The proposed algorithm utilizes a geometric median-based approach combined with k-means clustering to address the issue of permutation ambiguity in local client estimations. By partitioning client-provided estimators into clusters using k-means and then aggregating them within each cluster using the geometric median, the method proves to be effective even in highly heterogeneous scenarios where some clients have minimal sample sizes. The theoretical analysis incorporates error bounds of the geometric median with sample quantiles and misclustering rates of k-means. Simulation studies demonstrate the effectiveness of the approach in various heterogeneous settings. <div>
arXiv:2505.20532v1 Announce Type: new 
Abstract: This paper investigates a general robust one-shot aggregation framework for distributed and federated Independent Component Analysis (ICA) problem. We propose a geometric median-based aggregation algorithm that leverages $k$-means clustering to resolve the permutation ambiguity in local client estimations. Our method first performs k-means to partition client-provided estimators into clusters and then aggregates estimators within each cluster using the geometric median. This approach provably remains effective even in highly heterogeneous scenarios where at most half of the clients can observe only a minimal number of samples. The key theoretical contribution lies in the combined analysis of the geometric median's error bound-aided by sample quantiles-and the maximum misclustering rates of the aforementioned solution of $k$-means. The effectiveness of the proposed approach is further supported by simulation studies conducted under various heterogeneous settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rotary Masked Autoencoders are Versatile Learners</title>
<link>https://arxiv.org/abs/2505.20535</link>
<guid>https://arxiv.org/abs/2505.20535</guid>
<content:encoded><![CDATA[
<div> Keywords: Transformers, time-series, RoMAE, positional embeddings, representation learning

Summary: 
The article introduces the Rotary Masked Autoencoder (RoMAE) as a method for applying Transformers to irregular time-series without the need for specialized architectural modifications. RoMAE leverages Rotary Positional Embeddings (RoPE) to incorporate continuous positional information in multidimensional form, enhancing representation learning across various modalities like images, audio, and multivariate time-series. By outperforming specialized time-series architectures on challenging datasets such as the DESC ELAsTiCC Challenge, RoMAE showcases its versatility and efficiency. The study also delves into the reconstruction of embedded continuous positions, revealing that introducing learned embeddings in the input sequence disrupts RoPE's relative position property. RoMAE successfully achieves superior performance while maintaining the simplicity and effectiveness of Masked Autoencoders (MAE) in non-time-series contexts.<br /><br />Summary: <div>
arXiv:2505.20535v1 Announce Type: new 
Abstract: Applying Transformers to irregular time-series typically requires specializations to their baseline architecture, which can result in additional computational overhead and increased method complexity. We present the Rotary Masked Autoencoder (RoMAE), which utilizes the popular Rotary Positional Embedding (RoPE) method for continuous positions. RoMAE is an extension to the Masked Autoencoder (MAE) that enables representation learning with multidimensional continuous positional information while avoiding any time-series-specific architectural specializations. We showcase RoMAE's performance on a variety of modalities including irregular and multivariate time-series, images, and audio, demonstrating that RoMAE surpasses specialized time-series architectures on difficult datasets such as the DESC ELAsTiCC Challenge while maintaining MAE's usual performance across other modalities. In addition, we investigate RoMAE's ability to reconstruct the embedded continuous positions, demonstrating that including learned embeddings in the input sequence breaks RoPE's relative position property.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A ZeNN architecture to avoid the Gaussian trap</title>
<link>https://arxiv.org/abs/2505.20553</link>
<guid>https://arxiv.org/abs/2505.20553</guid>
<content:encoded><![CDATA[
<div> architecture, Zeta Neural Networks, ZeNNs, multi-layer perceptrons, MLPs, harmonic analysis <br />
<br />
Summary: The article introduces a new architecture called Zeta Neural Networks (ZeNNs) that addresses shortcomings of standard multi-layer perceptrons (MLPs). Inspired by principles from harmonic analysis, ZeNNs enumerate perceptrons with non-learnable weights, introduce scaling factors, and use activation functions that lead to near orthogonal systems. In the infinite width limit, ZeNNs converge pointwise, exhibit a rich asymptotic structure beyond Gaussianity, and enable feature learning. Additionally, finite width ZeNNs excel at learning high-frequency features with low-dimensional domains when suitable activation functions are selected. These innovations in architecture allow ZeNNs to overcome limitations of MLPs and enhance their performance in handling various types of data. <br /><br /> <div>
arXiv:2505.20553v1 Announce Type: new 
Abstract: We propose a new simple architecture, Zeta Neural Networks (ZeNNs), in order to overcome several shortcomings of standard multi-layer perceptrons (MLPs). Namely, in the large width limit, MLPs are non-parametric, they do not have a well-defined pointwise limit, they lose non-Gaussian attributes and become unable to perform feature learning; moreover, finite width MLPs perform poorly in learning high frequencies. The new ZeNN architecture is inspired by three simple principles from harmonic analysis:
  i) Enumerate the perceptons and introduce a non-learnable weight to enforce convergence;
  ii) Introduce a scaling (or frequency) factor;
  iii) Choose activation functions that lead to near orthogonal systems.
  We will show that these ideas allow us to fix the referred shortcomings of MLPs. In fact, in the infinite width limit, ZeNNs converge pointwise, they exhibit a rich asymptotic structure beyond Gaussianity, and perform feature learning. Moreover, when appropriate activation functions are chosen, (finite width) ZeNNs excel at learning high-frequency features of functions with low dimensional domains.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning a Pessimistic Reward Model in RLHF</title>
<link>https://arxiv.org/abs/2505.20556</link>
<guid>https://arxiv.org/abs/2505.20556</guid>
<content:encoded><![CDATA[
<div> method, reward hacking, offline reinforcement learning, human feedback, pessimistic reward<br />
Summary:<br />
This work introduces PET, a new method for fine-tuning pessimistic rewards in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF are prone to reward hacking, but PET aims to prevent this issue without using regularization. By optimizing policies on a pessimistic reward model, reward hacking can be mitigated effectively. The study conducted experiments on a summarization dataset and showed that high-quality policies can be learned without the need for regularization. These policies exhibit high performance despite having a high KL divergence from the dataset distribution. This research demonstrates the feasibility of using a pessimistic reward model to guide policy optimization and prevent reward hacking in RLHF settings. <br />Summary: <div>
arXiv:2505.20556v1 Announce Type: new 
Abstract: This work proposes `PET', a novel pessimistic reward fine-tuning method, to learn a pessimistic reward model robust against reward hacking in offline reinforcement learning from human feedback (RLHF). Traditional reward modeling techniques in RLHF train an imperfect reward model, on which a KL regularization plays a pivotal role in mitigating reward hacking when optimizing a policy. Such an intuition-based method still suffers from reward hacking, and the policies with large KL divergence from the dataset distribution are excluded during learning. In contrast, we show that when optimizing a policy on a pessimistic reward model fine-tuned through PET, reward hacking can be prevented without relying on any regularization. We test our methods on the standard TL;DR summarization dataset. We find that one can learn a high-quality policy on our pessimistic reward without using any regularization. Such a policy has a high KL divergence from the dataset distribution while having high performance in practice. In summary, our work shows the feasibility of learning a pessimistic reward model against reward hacking. The agent can greedily search for the policy with a high pessimistic reward without suffering from reward hacking.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM Reasoning</title>
<link>https://arxiv.org/abs/2505.20561</link>
<guid>https://arxiv.org/abs/2505.20561</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Models, Reinforcement Learning, Reflective Reasoning, Bayes-Adaptive RL, BARL

Summary:
Large Language Models (LLMs) trained using Reinforcement Learning (RL) have shown strong reasoning abilities and emergent reflective behaviors like backtracking and error correction. However, traditional Markovian RL limits exploration to the training phase, leading to uncertainty about the emergence of reflective reasoning. To address this, reflective exploration is reframed in the Bayes-Adaptive RL framework, optimizing expected return under a posterior distribution over Markov decision processes. The resulting algorithm, BARL, guides the LLM to adapt strategies based on observed outcomes, balancing reward-maximizing exploitation and information-gathering exploration. BARL outperforms standard Markovian RL at test time, offering superior token efficiency and improved exploration effectiveness. Empirical results on synthetic and mathematical reasoning tasks demonstrate the effectiveness of BARL. The code for BARL is available on GitHub. 

<br /><br />Summary: <div>
arXiv:2505.20561v1 Announce Type: new 
Abstract: Large Language Models (LLMs) trained via Reinforcement Learning (RL) have exhibited strong reasoning capabilities and emergent reflective behaviors, such as backtracking and error correction. However, conventional Markovian RL confines exploration to the training phase to learn an optimal deterministic policy and depends on the history contexts only through the current state. Therefore, it remains unclear whether reflective reasoning will emerge during Markovian RL training, or why they are beneficial at test time. To remedy this, we recast reflective exploration within the Bayes-Adaptive RL framework, which explicitly optimizes the expected return under a posterior distribution over Markov decision processes. This Bayesian formulation inherently incentivizes both reward-maximizing exploitation and information-gathering exploration via belief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and switch strategies based on the observed outcomes, offering principled guidance on when and how the model should reflectively explore. Empirical results on both synthetic and mathematical reasoning tasks demonstrate that BARL outperforms standard Markovian RL approaches at test time, achieving superior token efficiency with improved exploration effectiveness. Our code is available at https://github.com/shenao-zhang/BARL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bi-Level Unsupervised Feature Selection</title>
<link>https://arxiv.org/abs/2505.20563</link>
<guid>https://arxiv.org/abs/2505.20563</guid>
<content:encoded><![CDATA[
<div> feature selection, unsupervised, bi-level, clustering, projection matrix

Summary:
The article introduces a novel bi-level unsupervised feature selection (BLUFS) method that addresses the limitations of existing UFS methods by combining a clustering level and a feature level. Spectral clustering generates pseudo-labels to represent the data structure, while a linear regression model learns the projection matrix. The $\ell_{2,0}$-norm constraint is applied to the projection matrix for improved feature selection. A proximal alternating minimization (PAM) algorithm is designed to solve the bi-level model efficiently, with established convergence and computational complexity. Experimental results on synthetic and real datasets demonstrate BLUFS' superiority in clustering and classification tasks. <div>
arXiv:2505.20563v1 Announce Type: new 
Abstract: Unsupervised feature selection (UFS) is an important task in data engineering. However, most UFS methods construct models from a single perspective and often fail to simultaneously evaluate feature importance and preserve their inherent data structure, thus limiting their performance. To address this challenge, we propose a novel bi-level unsupervised feature selection (BLUFS) method, including a clustering level and a feature level. Specifically, at the clustering level, spectral clustering is used to generate pseudo-labels for representing the data structure, while a continuous linear regression model is developed to learn the projection matrix. At the feature level, the $\ell_{2,0}$-norm constraint is imposed on the projection matrix for more effectively selecting features. To the best of our knowledge, this is the first work to combine a bi-level framework with the $\ell_{2,0}$-norm. To solve the proposed bi-level model, we design an efficient proximal alternating minimization (PAM) algorithm, whose subproblems either have explicit solutions or can be computed by fast solvers. Furthermore, we establish the convergence result and computational complexity. Finally, extensive experiments on two synthetic datasets and eight real datasets demonstrate the superiority of BLUFS in clustering and classification tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Ctrl-DNA: Controllable Cell-Type-Specific Regulatory DNA Design via Constrained RL</title>
<link>https://arxiv.org/abs/2505.20578</link>
<guid>https://arxiv.org/abs/2505.20578</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, Cell-type specificity, Regulatory DNA sequences, Transformer-based language models, Synthetic biology<br />
Summary:<br />
Designing precise cell-type-specific gene expression regulatory DNA sequences is vital for various fields like synthetic biology. Existing generative models struggle with reliably producing novel sequences. This study introduces Ctrl-DNA, a constrained reinforcement learning framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating the problem as a constrained optimization one and applying reinforcement learning to genomic language models, Ctrl-DNA outperforms existing approaches in generating high-fitness regulatory sequences while achieving state-of-the-art cell-type specificity. The generated sequences also contain key cell-type-specific transcription factor binding sites, demonstrating biological plausibility. <div>
arXiv:2505.20578v1 Announce Type: new 
Abstract: Designing regulatory DNA sequences that achieve precise cell-type-specific gene expression is crucial for advancements in synthetic biology, gene therapy and precision medicine. Although transformer-based language models (LMs) can effectively capture patterns in regulatory DNA, their generative approaches often struggle to produce novel sequences with reliable cell-specific activity. Here, we introduce Ctrl-DNA, a novel constrained reinforcement learning (RL) framework tailored for designing regulatory DNA sequences with controllable cell-type specificity. By formulating regulatory sequence design as a biologically informed constrained optimization problem, we apply RL to autoregressive genomic LMs, enabling the models to iteratively refine sequences that maximize regulatory activity in targeted cell types while constraining off-target effects. Our evaluation on human promoters and enhancers demonstrates that Ctrl-DNA consistently outperforms existing generative and RL-based approaches, generating high-fitness regulatory sequences and achieving state-of-the-art cell-type specificity. Moreover, Ctrl-DNA-generated sequences capture key cell-type-specific transcription factor binding sites (TFBS), short DNA motifs recognized by regulatory proteins that control gene expression, demonstrating the biological plausibility of the generated sequences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The challenge of hidden gifts in multi-agent reinforcement learning</title>
<link>https://arxiv.org/abs/2505.20579</link>
<guid>https://arxiv.org/abs/2505.20579</guid>
<content:encoded><![CDATA[
<div> Keywords: hidden gifts, multi-agent reinforcement learning, grid-world environment, credit assignment, independent agents

Summary:
In the study of hidden gifts in multi-agent reinforcement learning (MARL), researchers explore the challenge of assigning credit when beneficial actions of others are concealed. A simple MARL task in a grid-world environment is used to illustrate this concept, where agents must collectively unlock doors using a shared key to receive rewards. Despite individual success, achieving the collective reward requires agents to drop the key for others, a hidden gift. Existing state-of-the-art RL algorithms struggle to learn this task, but independent model-free policy gradient agents succeed when provided with their action history. Introducing a correction term inspired by learning aware approaches enhances the performance of independent agents, reducing learning variance and promoting collective success. This study highlights the difficulty of credit assignment in multi-agent scenarios with hidden gifts and suggests that learning awareness in independent agents can improve performance. 

<br /><br />Summary: <div>
arXiv:2505.20579v1 Announce Type: new 
Abstract: Sometimes we benefit from actions that others have taken even when we are unaware that they took those actions. For example, if your neighbor chooses not to take a parking spot in front of your house when you are not there, you can benefit, even without being aware that they took this action. These "hidden gifts" represent an interesting challenge for multi-agent reinforcement learning (MARL), since assigning credit when the beneficial actions of others are hidden is non-trivial. Here, we study the impact of hidden gifts with a very simple MARL task. In this task, agents in a grid-world environment have individual doors to unlock in order to obtain individual rewards. As well, if all the agents unlock their door the group receives a larger collective reward. However, there is only one key for all of the doors, such that the collective reward can only be obtained when the agents drop the key for others after they use it. Notably, there is nothing to indicate to an agent that the other agents have dropped the key, thus the act of dropping the key for others is a "hidden gift". We show that several different state-of-the-art RL algorithms, including MARL algorithms, fail to learn how to obtain the collective reward in this simple task. Interestingly, we find that independent model-free policy gradient agents can solve the task when we provide them with information about their own action history, but MARL agents still cannot solve the task with action history. Finally, we derive a correction term for these independent agents, inspired by learning aware approaches, which reduces the variance in learning and helps them to converge to collective success more reliably. These results show that credit assignment in multi-agent settings can be particularly challenging in the presence of "hidden gifts", and demonstrate that learning awareness in independent agents can benefit these settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prot2Token: A Unified Framework for Protein Modeling via Next-Token Prediction</title>
<link>https://arxiv.org/abs/2505.20589</link>
<guid>https://arxiv.org/abs/2505.20589</guid>
<content:encoded><![CDATA[
<div> Framework, Protein Language Models, Multi-task learning, Autoregressive decoder, Benchmark

Summary:<br />
- Prot2Token is introduced as a unified framework for protein prediction tasks, converting various predictions into a standardized next-token format.
- The model utilizes an autoregressive decoder and pre-trained protein encoders for multi-task learning, showcasing strong predictive power across different benchmarks.
- It offers significant speedups and performance comparable to or exceeding specialized models.
- An auxiliary self-supervised decoder pre-training approach is introduced to enhance spatially sensitive task performance.
- Prot2Token represents a versatile and high-throughput paradigm for protein modeling, promising to accelerate biological discovery and therapeutics development.<br /> <div>
arXiv:2505.20589v1 Announce Type: new 
Abstract: The diverse nature of protein prediction tasks has traditionally necessitated specialized models, hindering the development of broadly applicable and computationally efficient Protein Language Models (PLMs). In this work, we introduce Prot2Token, a unified framework that overcomes these challenges by converting a wide spectrum of protein-related predictions, from sequence-level properties and residue-specific attributes to complex inter-protein interactions, into a standardized next-token prediction format. At its core, Prot2Token employs an autoregressive decoder, conditioned on embeddings from pre-trained protein encoders and guided by learnable task tokens, to perform diverse predictions. This architecture uniquely facilitates multi-task learning, enabling a single model to master numerous tasks with improved efficiency. We present extensive experimental validation across a variety of benchmarks, demonstrating Prot2Tokens strong predictive power in different types of protein-prediction tasks. Key results include significant speedups (e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or exceeding specialized approaches. Beyond that, we introduce an auxiliary self-supervised decoder pre-training approach to improve spatially sensitive task performance. Prot2Token thus offers a significant step towards a versatile, high-throughput paradigm for protein modeling, promising to accelerate biological discovery and the development of novel therapeutics. The code is available at https://github.com/mahdip72/prot2token .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-level Certified Defense Against Poisoning Attacks in Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.20621</link>
<guid>https://arxiv.org/abs/2505.20621</guid>
<content:encoded><![CDATA[
<div> machine learning, offline reinforcement learning, poisoning attacks, differential privacy, certified defenses 
<br /> 
Summary:
This study introduces a novel approach to enhance the robustness of Offline Reinforcement Learning (RL) against poisoning attacks. By extending certified defenses, the framework provides larger guarantees for per-state actions and overall expected cumulative rewards in both continuous and discrete spaces, and in stochastic and deterministic environments. Leveraging properties of Differential Privacy, the approach significantly expands the scope and applicability of achievable guarantees. Empirical evaluations show that the performance drop is limited to 50% with up to 7% of the training data poisoned, a notable improvement over previous work. Additionally, the certified radii produced by the framework are 5 times larger. These results signify the potential of this framework to improve safety and reliability in offline RL. 
<br /> 
Summary: <div>
arXiv:2505.20621v1 Announce Type: new 
Abstract: Similar to other machine learning frameworks, Offline Reinforcement Learning (RL) is shown to be vulnerable to poisoning attacks, due to its reliance on externally sourced datasets, a vulnerability that is exacerbated by its sequential nature. To mitigate the risks posed by RL poisoning, we extend certified defenses to provide larger guarantees against adversarial manipulation, ensuring robustness for both per-state actions, and the overall expected cumulative reward. Our approach leverages properties of Differential Privacy, in a manner that allows this work to span both continuous and discrete spaces, as well as stochastic and deterministic environments -- significantly expanding the scope and applicability of achievable guarantees. Empirical evaluations demonstrate that our approach ensures the performance drops to no more than $50\%$ with up to $7\%$ of the training data poisoned, significantly improving over the $0.008\%$ in prior work~\citep{wu_copa_2022}, while producing certified radii that is $5$ times larger as well. This highlights the potential of our framework to enhance safety and reliability in offline RL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Position: Adopt Constraints Over Penalties in Deep Learning</title>
<link>https://arxiv.org/abs/2505.20628</link>
<guid>https://arxiv.org/abs/2505.20628</guid>
<content:encoded><![CDATA[
<div> Machine learning, Trustworthy AI, Constrained optimization, Penalization, Lagrangian approach

Summary:
Tailored constrained optimization methods are proposed for developing trustworthy AI systems with accountability guarantees, integrating external requirements or constraints seamlessly into deep learning pipelines. The traditional approach of penalization through fixed-weight terms is deemed ineffective, as it often requires costly tuning of penalty coefficients and may not yield optimal solutions. Alternatively, tailored constrained methods, like the Lagrangian approach, optimize the penalization coefficients (Lagrange multipliers) alongside the model, ensuring solutions that satisfy constraints and achieve good performance. These methods not only solve the constrained problem but also add accountability without the need for extensive penalty tuning. By adopting tailored constrained optimization methods, researchers can enhance the robustness and reliability of AI systems while streamlining the integration of external requirements. <br /><br />Summary: <div>
arXiv:2505.20628v1 Announce Type: new 
Abstract: Recent efforts toward developing trustworthy AI systems with accountability guarantees have led to a growing reliance on machine learning formulations that incorporate external requirements, or constraints. These requirements are often enforced through penalization--adding fixed-weight terms to the task loss. We argue that this approach is ill-suited, and that tailored constrained optimization methods should be adopted instead. In particular, no penalty coefficient may yield a solution that both satisfies the constraints and achieves good performance--i.e., one solving the constrained problem. Moreover, tuning these coefficients is costly, incurring significant time and computational overhead. In contrast, tailored constrained methods--such as the Lagrangian approach, which optimizes the penalization "coefficients" (the Lagrange multipliers) alongside the model--(i) truly solve the constrained problem and add accountability, (ii) eliminate the need for extensive penalty tuning, and (iii) integrate seamlessly with modern deep learning pipelines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Explaining Concept Shift with Interpretable Feature Attribution</title>
<link>https://arxiv.org/abs/2505.20634</link>
<guid>https://arxiv.org/abs/2505.20634</guid>
<content:encoded><![CDATA[
<div> shift detection; concept shift; tabular data; machine learning; feature selection

Summary:<br /><br />Concept shift occurs when the distribution of labels conditioned on the features changes, leading to reduced model performance even for well-trained machine learning models. In this paper, SGShift, a model for detecting concept shift in tabular data, is proposed. SGShift utilizes a Generalized Additive Model (GAM) to model concept shift, followed by feature selection to identify shifted features. Extensions of SGShift include incorporating knockoffs to control false discoveries and an absorption term for models with poor data fit. Extensive experiments on synthetic and real data show that SGShift can identify shifted features with high AUC and recall values, outperforming baseline methods. The proposed approach provides valuable insight into understanding dataset differences and helps in attributing reduced model performance to specific shifted features. <div>
arXiv:2505.20634v1 Announce Type: new 
Abstract: Regardless the amount of data a machine learning (ML) model is trained on, there will inevitably be data that differs from their training set, lowering model performance. Concept shift occurs when the distribution of labels conditioned on the features changes, making even a well-tuned ML model to have learned a fundamentally incorrect representation. Identifying these shifted features provides unique insight into how one dataset differs from another, considering the difference may be across a scientifically relevant dimension, such as time, disease status, population, etc. In this paper, we propose SGShift, a model for detecting concept shift in tabular data and attributing reduced model performance to a sparse set of shifted features. SGShift models concept shift with a Generalized Additive Model (GAM) and performs subsequent feature selection to identify shifted features. We propose further extensions of SGShift by incorporating knockoffs to control false discoveries and an absorption term to account for models with poor fit to the data. We conduct extensive experiments in synthetic and real data across various ML models and find SGShift can identify shifted features with AUC $>0.9$ and recall $>90\%$, often 2 or 3 times as high as baseline methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Past Experience Accelerate LLM Reasoning?</title>
<link>https://arxiv.org/abs/2505.20643</link>
<guid>https://arxiv.org/abs/2505.20643</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, reasoning speedup, compute allocation, memory mechanisms, benchmarking

Summary: 
The paper explores the concept of making large language models (LLMs) faster at reasoning through recurrent exposure to relevant tasks. It formalizes the problem of LLM reasoning speedup and proposes the SpeedupLLM framework to implement and benchmark this behavior. The framework is based on adaptive compute allocation and memory mechanisms. Comprehensive experiments are conducted to assess the reasoning speedup across various question similarity levels, memory methods, and reasoning techniques. Results indicate that LLMs can indeed reason faster with past experience, leading to up to a 56% reduction in compute cost when equipped with suitable memory and reasoning methods. The study sheds light on the potential for LLMs to improve their efficiency through repeated exposure to relevant tasks, showcasing the importance of memory retention and adaptive compute allocation in enhancing reasoning abilities. 

<br /><br />Summary: <div>
arXiv:2505.20643v1 Announce Type: new 
Abstract: Allocating more compute to large language models (LLMs) reasoning has generally been demonstrated to improve their effectiveness, but also results in increased inference time. In contrast, humans can perform tasks faster and better with increased experience and exposure. Hence, this paper aims to investigate the question: Can LLMs also become faster at reasoning through recurrent exposure on relevant tasks, and if so, how can it be achieved? To address these questions, we first formalize the problem setting of LLM reasoning speedup systematically in the dimensions of task relevancy and compute budget calculation. We then propose SpeedupLLM, a theoretically guaranteed framework to implement and benchmark such reasoning speedup behaviour based on adaptive compute allocation and memory mechanisms. We further conduct comprehensive experiments to benchmark such behaviour across different question similarity levels, memory methods, and reasoning methods. Results show that LLMs can generally reason faster with past experience, achieving up to a 56% reduction in compute cost when equipped with appropriate memory and reasoning methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evaluating Training in Binarized Neural Networks Through the Lens of Algorithmic Information Theory</title>
<link>https://arxiv.org/abs/2505.20646</link>
<guid>https://arxiv.org/abs/2505.20646</guid>
<content:encoded><![CDATA[
<div> algorithmic information theory, Binarized Neural Networks, Block Decomposition Method, learning dynamics, algorithmic compression<br />
<br />Summary:
This study explores the use of algorithmic information theory to understand and control the informational complexity of neural networks. By focusing on Binarized Neural Networks (BNNs) and using the Block Decomposition Method (BDM) to approximate algorithmic complexity, the researchers show that this approach better captures structural changes during training compared to traditional entropy-based methods. The results suggest that training can be viewed as a process of algorithmic compression, where learning involves internalizing structured regularities. This perspective offers a principled estimate of learning progression and provides a framework for complexity-aware learning and regularization based on fundamental principles from information theory, complexity, and computability. <div>
arXiv:2505.20646v1 Announce Type: new 
Abstract: Understanding and controlling the informational complexity of neural networks is a central challenge in machine learning, with implications for generalization, optimization, and model capacity. While most approaches rely on entropy-based loss functions and statistical metrics, these measures often fail to capture deeper, causally relevant algorithmic regularities embedded in network structure. We propose a shift toward algorithmic information theory, using Binarized Neural Networks (BNNs) as a first proxy. Grounded in algorithmic probability (AP) and the universal distribution it defines, our approach characterizes learning dynamics through a formal, causally grounded lens. We apply the Block Decomposition Method (BDM) -- a scalable approximation of algorithmic complexity based on AP -- and demonstrate that it more closely tracks structural changes during training than entropy, consistently exhibiting stronger correlations with training loss across varying model sizes and randomized training runs. These results support the view of training as a process of algorithmic compression, where learning corresponds to the progressive internalization of structured regularities. In doing so, our work offers a principled estimate of learning progression and suggests a framework for complexity-aware learning and regularization, grounded in first principles from information theory, complexity, and computability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Voronoi-grid-based Pareto Front Learning and Its Application to Collaborative Federated Learning</title>
<link>https://arxiv.org/abs/2505.20648</link>
<guid>https://arxiv.org/abs/2505.20648</guid>
<content:encoded><![CDATA[
<div> Keywords: Multi-objective optimization, Pareto front, Hypernetworks, Genetic algorithm, Federated learning

Summary:
Multi-objective optimization is crucial in machine learning, particularly in areas like federated learning. Pareto-Front Learning using Hypernetworks (PHN) is a powerful method to approximate the Pareto front. However, existing approaches face challenges in sampling rays in high-dimensional spaces and covering the entire Pareto front. To address these issues, a novel framework called PHN-HVVS is introduced, which uses Voronoi grids and genetic algorithm for partitioning the design space. A new loss function is proposed to enhance coverage of the Pareto front and maximize the HV Indicator. Experimental results show that PHN-HVVS outperforms baselines in generating Pareto front and advances methodologies in Federated Learning. The code for PHN-HVVS is available on GitHub, enabling further research and application in the field. 

<br /><br />Summary: <div>
arXiv:2505.20648v1 Announce Type: new 
Abstract: Multi-objective optimization (MOO) exists extensively in machine learning, and aims to find a set of Pareto-optimal solutions, called the Pareto front, e.g., it is fundamental for multiple avenues of research in federated learning (FL). Pareto-Front Learning (PFL) is a powerful method implemented using Hypernetworks (PHNs) to approximate the Pareto front. This method enables the acquisition of a mapping function from a given preference vector to the solutions on the Pareto front. However, most existing PFL approaches still face two challenges: (a) sampling rays in high-dimensional spaces; (b) failing to cover the entire Pareto Front which has a convex shape. Here, we introduce a novel PFL framework, called as PHN-HVVS, which decomposes the design space into Voronoi grids and deploys a genetic algorithm (GA) for Voronoi grid partitioning within high-dimensional space. We put forward a new loss function, which effectively contributes to more extensive coverage of the resultant Pareto front and maximizes the HV Indicator. Experimental results on multiple MOO machine learning tasks demonstrate that PHN-HVVS outperforms the baselines significantly in generating Pareto front. Also, we illustrate that PHN-HVVS advances the methodologies of several recent problems in the FL field. The code is available at https://github.com/buptcmm/phnhvvs}{https://github.com/buptcmm/phnhvvs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Optimisation Framework for Unsupervised Environment Design</title>
<link>https://arxiv.org/abs/2505.20659</link>
<guid>https://arxiv.org/abs/2505.20659</guid>
<content:encoded><![CDATA[
<div> optimisation, reinforcement learning, robustness, unsupervised environment design, generalisability

Summary:
The paper introduces an approach called unsupervised environment design (UED), which aims to enhance the robustness of reinforcement learning agents in high-risk settings. It focuses on optimizing a nonconvex-strongly-concave objective to improve generalisability across different environment configurations. The method provides theoretical guarantees for practical applications, unlike previous techniques that rely on convergence. An algorithm is proposed for the zero-sum setting, ensuring provable convergence. Empirical results demonstrate the effectiveness of the approach, showcasing superior performance compared to existing methods in various challenging environments. This research contributes to advancing the field of reinforcement learning by enhancing agent robustness through innovative optimization strategies. <div>
arXiv:2505.20659v1 Announce Type: new 
Abstract: For reinforcement learning agents to be deployed in high-risk settings, they must achieve a high level of robustness to unfamiliar scenarios. One method for improving robustness is unsupervised environment design (UED), a suite of methods aiming to maximise an agent's generalisability across configurations of an environment. In this work, we study UED from an optimisation perspective, providing stronger theoretical guarantees for practical settings than prior work. Whereas previous methods relied on guarantees if they reach convergence, our framework employs a nonconvex-strongly-concave objective for which we provide a provably convergent algorithm in the zero-sum setting. We empirically verify the efficacy of our method, outperforming prior methods in a number of environments with varying difficulties.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Continuous-Time Attention: PDE-Guided Mechanisms for Long-Sequence Transformers</title>
<link>https://arxiv.org/abs/2505.20666</link>
<guid>https://arxiv.org/abs/2505.20666</guid>
<content:encoded><![CDATA[
<div> Keywords: Continuous_Time Attention, partial differential equations, Transformer, long input sequences, optimization landscapes

Summary:
Continuous_Time Attention is a novel framework that incorporates partial differential equations (PDEs) into the Transformer's attention mechanism to handle long input sequences. By allowing attention weights to evolve over a pseudo_time dimension using diffusion, wave, or reaction_diffusion dynamics, this mechanism smooths local noise, enhances long-range dependencies, and stabilizes gradient flow. The PDE_based attention leads to improved optimization landscapes and polynomial decay of distant interactions. Empirical benchmarks show consistent improvements over standard Transformer models on various experiments. This approach showcases the potential of PDE_based formulations in enhancing attention mechanisms with continuous_time dynamics and global coherence.<br /><br />Summary: <div>
arXiv:2505.20666v1 Announce Type: new 
Abstract: We propose a novel framework, Continuous_Time Attention, which infuses partial differential equations (PDEs) into the Transformer's attention mechanism to address the challenges of extremely long input sequences. Instead of relying solely on a static attention matrix, we allow attention weights to evolve over a pseudo_time dimension via diffusion, wave, or reaction_diffusion dynamics. This mechanism systematically smooths local noise, enhances long_range dependencies, and stabilizes gradient flow. Theoretically, our analysis shows that PDE_based attention leads to better optimization landscapes and polynomial rather than exponential decay of distant interactions. Empirically, we benchmark our method on diverse experiments_demonstrating consistent gains over both standard and specialized long sequence Transformer variants. Our findings highlight the potential of PDE_based formulations to enrich attention mechanisms with continuous_time dynamics and global coherence.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerating RL for LLM Reasoning with Optimal Advantage Regression</title>
<link>https://arxiv.org/abs/2505.20686</link>
<guid>https://arxiv.org/abs/2505.20686</guid>
<content:encoded><![CDATA[
<div> Keywords: Reinforcement learning, language models, policy optimization, reasoning tasks, A*-PO

Summary:
Reinforcement learning (RL) is utilized to enhance large language models (LLMs) for complex reasoning tasks. The A*-PO framework proposed in this paper streamlines policy optimization for LLM training by approximating the optimal advantage function. It involves a two-stage process where offline sampling estimates the optimal value function $V$* in the first stage, eliminating the need for expensive online value estimation. The second stage uses a simple least-squares regression loss with only one generation per prompt for on-policy updates. The framework provides performance guarantees and simplifies the optimization of the KL-regularized RL objective without complex exploration strategies. Empirically, A*-PO outperforms existing methods such as PPO, GRPO, and REBEL in mathematical reasoning benchmarks, while reducing training time by up to 2 times and peak memory usage by over 30%. The implementation of A*-PO is available at https://github.com/ZhaolinGao/A-PO.

<br /><br />Summary: A*-PO is a novel policy optimization framework that enhances LLM training efficiency by approximating the optimal advantage function through a two-stage process. It eliminates costly online value estimation, simplifies optimization with a simple loss function, and achieves competitive performance with reduced training time and memory usage. <div>
arXiv:2505.20686v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has emerged as a powerful tool for fine-tuning large language models (LLMs) to improve complex reasoning abilities. However, state-of-the-art policy optimization methods often suffer from high computational overhead and memory consumption, primarily due to the need for multiple generations per prompt and the reliance on critic networks or advantage estimates of the current policy. In this paper, we propose $A$*-PO, a novel two-stage policy optimization framework that directly approximates the optimal advantage function and enables efficient training of LLMs for reasoning tasks. In the first stage, we leverage offline sampling from a reference policy to estimate the optimal value function $V$*, eliminating the need for costly online value estimation. In the second stage, we perform on-policy updates using a simple least-squares regression loss with only a single generation per prompt. Theoretically, we establish performance guarantees and prove that the KL-regularized RL objective can be optimized without requiring complex exploration strategies. Empirically, $A$*-PO achieves competitive performance across a wide range of mathematical reasoning benchmarks, while reducing training time by up to 2$\times$ and peak memory usage by over 30% compared to PPO, GRPO, and REBEL. Implementation of $A$*-PO can be found at https://github.com/ZhaolinGao/A-PO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidential Deep Active Learning for Semi-Supervised Classification</title>
<link>https://arxiv.org/abs/2505.20691</link>
<guid>https://arxiv.org/abs/2505.20691</guid>
<content:encoded><![CDATA[
<div> active learning, semi-supervised classification, uncertainty estimation, evidential deep learning, sample selection strategy

Summary:
In this paper, the authors propose an evidential deep active learning approach for semi-supervised classification (EDALSSC) that takes into account uncertainty estimation during the learning process. EDALSSC integrates evidential deep learning to quantify uncertainty in labeled data and combines ignorance and conflict information for unlabeled data. A heuristic method balances evidence and number of classes to avoid counter-intuitive results. The sample selection strategy prioritizes samples with high uncertainty estimation as training loss increases. Experimental results show that EDALSSC surpasses existing active learning and supervised approaches on image classification tasks. The proposed method addresses the limitations of existing methods by incorporating uncertainty estimation effectively throughout the learning process. <br /><br />Summary: <div>
arXiv:2505.20691v1 Announce Type: new 
Abstract: Semi-supervised classification based on active learning has made significant progress, but the existing methods often ignore the uncertainty estimation (or reliability) of the prediction results during the learning process, which makes it questionable whether the selected samples can effectively update the model. Hence, this paper proposes an evidential deep active learning approach for semi-supervised classification (EDALSSC). EDALSSC builds a semi-supervised learning framework to simultaneously quantify the uncertainty estimation of labeled and unlabeled data during the learning process. The uncertainty estimation of the former is associated with evidential deep learning, while that of the latter is modeled by combining ignorance information and conflict information of the evidence from the perspective of the T-conorm operator. Furthermore, this article constructs a heuristic method to dynamically balance the influence of evidence and the number of classes on uncertainty estimation to ensure that it does not produce counter-intuitive results in EDALSSC. For the sample selection strategy, EDALSSC selects the sample with the greatest uncertainty estimation that is calculated in the form of a sum when the training loss increases in the latter half of the learning process. Experimental results demonstrate that EDALSSC outperforms existing semi-supervised and supervised active learning approaches on image classification datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generating Hypotheses of Dynamic Causal Graphs in Neuroscience: Leveraging Generative Factor Models of Observed Time Series</title>
<link>https://arxiv.org/abs/2505.20697</link>
<guid>https://arxiv.org/abs/2505.20697</guid>
<content:encoded><![CDATA[
<div> Keywords: hypothesis generation, neuroscience, machine learning, dynamic graphs, causal relationships 

Summary:
Our novel method in hypothesis generation in neuroscience leverages machine learning techniques to model dynamic graphs as a combination of static graphs. This allows us to capture complex, time-varying relationships beyond linear patterns in systems with dynamic, state-dependent behavior like the brain. Our approach improves the prediction of dynamic causal patterns significantly, with f1-scores increasing by 22-28% on average over baselines and up to over 60% in some cases. A case study on real brain data showcases the method's effectiveness in uncovering relationships associated with specific behavioral states, providing valuable insights into neural dynamics. This innovative approach offers a promising way to reduce costs in neuroscience research by narrowing down the range of interventional studies needed and enhancing our understanding of complex brain dynamics. 

Summary: <div>
arXiv:2505.20697v1 Announce Type: new 
Abstract: The field of hypothesis generation promises to reduce costs in neuroscience by narrowing the range of interventional studies needed to study various phenomena. Existing machine learning methods can generate scientific hypotheses from complex datasets, but many approaches assume causal relationships are static over time, limiting their applicability to systems with dynamic, state-dependent behavior, such as the brain. While some techniques attempt dynamic causal discovery through factor models, they often restrict relationships to linear patterns or impose other simplifying assumptions. We propose a novel method that models dynamic graphs as a conditionally weighted superposition of static graphs, where each static graph can capture nonlinear relationships. This approach enables the detection of complex, time-varying interactions between variables beyond linear limitations. Our method improves f1-scores of predicted dynamic causal patterns by roughly 22-28% on average over baselines in some of our experiments, with some improvements reaching well over 60%. A case study on real brain data demonstrates our method's ability to uncover relationships linked to specific behavioral states, offering valuable insights into neural dynamics.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sparsified State-Space Models are Efficient Highway Networks</title>
<link>https://arxiv.org/abs/2505.20698</link>
<guid>https://arxiv.org/abs/2505.20698</guid>
<content:encoded><![CDATA[
<div> Keywords: State-space models, sequence modeling, sparsification, token pruning, natural language tasks

Summary: 
State-space models (SSMs) offer an efficient alternative to Transformers for sequence modeling by utilizing linear recurrences instead of self-attention. This paper introduces Simba, a hierarchical sparsification method for SSMs that enhances their performance within given computational constraints. By sparsifying tokens based on a novel pruning criterion that measures global impact through accumulated local recurrences, Simba optimizes information flow in SSMs. The approach focuses on reducing redundancy in upper layers while retaining important global context, resembling the behavior of highways. Experimental results demonstrate that Simba outperforms the baseline model, Mamba, in various natural language tasks while maintaining the same computational complexity. The code for implementing Simba is publicly available, allowing for easy adoption and further research in the field. 

<br /><br />Summary: <div>
arXiv:2505.20698v1 Announce Type: new 
Abstract: State-space models (SSMs) offer a promising architecture for sequence modeling, providing an alternative to Transformers by replacing expensive self-attention with linear recurrences. In this paper, we propose a simple yet effective trick to enhance SSMs within given computational budgets by sparsifying them. Our intuition is that tokens in SSMs are highly redundant due to gradual recurrent updates, and dense recurrence operations block the delivery of past information. In particular, we observe that upper layers of SSMs tend to be more redundant as they encode global information, while lower layers encode local information. Motivated by this, we introduce Simba, a hierarchical sparsification method for SSMs based on token pruning. Simba sparsifies upper layers more than lower layers, encouraging the upper layers to behave like highways. To achieve this, we propose a novel token pruning criterion for SSMs, measuring the global impact of tokens on the final output by accumulating local recurrences. We demonstrate that Simba outperforms the baseline model, Mamba, with the same FLOPS in various natural language tasks. Moreover, we illustrate the effect of highways, showing that Simba not only enhances efficiency but also improves the information flow across long sequences. Code is available at https://github.com/woominsong/Simba.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Are Data Embeddings effective in time series forecasting?</title>
<link>https://arxiv.org/abs/2505.20716</link>
<guid>https://arxiv.org/abs/2505.20716</guid>
<content:encoded><![CDATA[
<div> Keywords: time series forecasting, data embedding, state-of-the-art models, ablation studies, computational efficiency <br />
Summary: 
Data embedding techniques in time series forecasting models are not as effective as previously believed. Extensive ablation studies on fifteen state-of-the-art models and four benchmark datasets show that removing data embedding layers does not decrease performance; in fact, it can improve both accuracy and computational efficiency. The gains from removing these layers often surpass the differences in performance typically seen between competing models. This research challenges the notion that complex data embedding layers are necessary for enhancing forecasting accuracy, suggesting a potential simplification of model architectures for improved efficiency. The study's findings open up new avenues for developing more streamlined and effective forecasting models. The code for replicating the study is available on GitHub for further exploration and validation. <br /><br />Summary: <div>
arXiv:2505.20716v1 Announce Type: new 
Abstract: Time series forecasting plays a crucial role in many real-world applications, and numerous complex forecasting models have been proposed in recent years. Despite their architectural innovations, most state-of-the-art models report only marginal improvements -- typically just a few thousandths in standard error metrics. These models often incorporate complex data embedding layers to transform raw inputs into higher-dimensional representations to enhance accuracy. But are data embedding techniques actually effective in time series forecasting? Through extensive ablation studies across fifteen state-of-the-art models and four benchmark datasets, we find that removing data embedding layers from many state-of-the-art models does not degrade forecasting performance. In many cases, it improves both accuracy and computational efficiency. The gains from removing embedding layers often exceed the performance differences typically reported between competing models. Code available at: https://github.com/neuripsdataembedidng/DataEmbedding
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Neural Operators: Stable Long-Term PDE Prediction</title>
<link>https://arxiv.org/abs/2505.20721</link>
<guid>https://arxiv.org/abs/2505.20721</guid>
<content:encoded><![CDATA[
<div> Neural operators, recurrent training, partial differential equations, temporal dynamics, long-term predictions <br />
<br />
Summary: <br />
Neural operators have shown promise in solving partial differential equations, but face challenges with long-term predictions due to a mismatch between training and inference methods. Recurrent Neural Operators (RNOs) address this by incorporating recurrent training into neural operator architectures, simulating inference-time dynamics during training. This alignment reduces error accumulation and improves accuracy and stability in long-term predictions. Theoretical analysis suggests that recurrent training can mitigate exponential error growth seen in standard training strategies. Empirical results demonstrate that Multigrid Neural Operators trained with recurrence significantly outperform teacher-forced counterparts on standard benchmarks. This research highlights the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning. <div>
arXiv:2505.20721v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful tools for learning solution operators of partial differential equations. However, in time-dependent problems, standard training strategies such as teacher forcing introduce a mismatch between training and inference, leading to compounding errors in long-term autoregressive predictions. To address this issue, we propose Recurrent Neural Operators (RNOs)-a novel framework that integrates recurrent training into neural operator architectures. Instead of conditioning each training step on ground-truth inputs, RNOs recursively apply the operator to their own predictions over a temporal window, effectively simulating inference-time dynamics during training. This alignment mitigates exposure bias and enhances robustness to error accumulation. Theoretically, we show that recurrent training can reduce the worst-case exponential error growth typical of teacher forcing to linear growth. Empirically, we demonstrate that recurrently trained Multigrid Neural Operators significantly outperform their teacher-forced counterparts in long-term accuracy and stability on standard benchmarks. Our results underscore the importance of aligning training with inference dynamics for robust temporal generalization in neural operator learning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A reinforcement learning agent for maintenance of deteriorating systems with increasingly imperfect repairs</title>
<link>https://arxiv.org/abs/2505.20725</link>
<guid>https://arxiv.org/abs/2505.20725</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, maintenance optimization, Industry 4.0, degradation process, Double Deep Q-Network

Summary:
- Efficient maintenance is crucial for engineering systems, particularly in the context of Industry 4.0.
- Machine learning techniques, such as reinforcement learning, are being increasingly utilized in maintenance practices.
- A novel maintenance model is proposed, incorporating a gamma degradation process and imperfect repairs to reflect real-world system behavior.
- A reinforcement-learning-based agent, utilizing a Double Deep Q-Network architecture, is developed to generate maintenance policies without a predefined preventive threshold.
- The agent demonstrates flexibility in adapting to different scenarios and significantly improves long-run cost compared to common maintenance strategies. 

<br /><br />Summary: <div>
arXiv:2505.20725v1 Announce Type: new 
Abstract: Efficient maintenance has always been essential for the successful application of engineering systems. However, the challenges to be overcome in the implementation of Industry 4.0 necessitate new paradigms of maintenance optimization. Machine learning techniques are becoming increasingly used in engineering and maintenance, with reinforcement learning being one of the most promising. In this paper, we propose a gamma degradation process together with a novel maintenance model in which repairs are increasingly imperfect, i.e., the beneficial effect of system repairs decreases as more repairs are performed, reflecting the degradational behavior of real-world systems. To generate maintenance policies for this system, we developed a reinforcement-learning-based agent using a Double Deep Q-Network architecture. This agent presents two important advantages: it works without a predefined preventive threshold, and it can operate in a continuous degradation state space. Our agent learns to behave in different scenarios, showing great flexibility. In addition, we performed an analysis of how changes in the main parameters of the environment affect the maintenance policy proposed by the agent. The proposed approach is demonstrated to be appropriate and to significatively improve long-run cost as compared with other common maintenance strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial bandit optimization for approximately linear functions</title>
<link>https://arxiv.org/abs/2505.20734</link>
<guid>https://arxiv.org/abs/2505.20734</guid>
<content:encoded><![CDATA[
<div> bandit optimization, nonconvex functions, non-smooth functions, regret bounds, linear optimization <br />
<br />
Summary: 
The article addresses a bandit optimization problem focusing on nonconvex and non-smooth functions. It examines scenarios where the loss function is a combination of a linear function and a small arbitrary perturbation chosen after observing the players choice. The study presents expected and high probability regret bounds for this problem, which also leads to an enhanced high-probability regret bound for bandit linear optimization  a special case lacking perturbations. Additionally, a lower bound on the expected regret is provided. The findings offer valuable insights into optimizing bandit problems with complex function forms and uncertainties, enhancing the understanding of efficient decision-making strategies in dynamic and turbulent environments. <div>
arXiv:2505.20734v1 Announce Type: new 
Abstract: We consider a bandit optimization problem for nonconvex and non-smooth functions, where in each trial the loss function is the sum of a linear function and a small but arbitrary perturbation chosen after observing the player's choice. We give both expected and high probability regret bounds for the problem. Our result also implies an improved high-probability regret bound for the bandit linear optimization, a special case with no perturbation. We also give a lower bound on the expected regret.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Detecting Informative Channels: ActionFormer</title>
<link>https://arxiv.org/abs/2505.20739</link>
<guid>https://arxiv.org/abs/2505.20739</guid>
<content:encoded><![CDATA[
<div> Transformer-based models have advanced Human Activity Recognition (HAR), with ActionFormer offering new insights by detecting activity borders and labels. While originally designed for image/video inputs, it has been adapted for sensor signals. However, challenges include capturing subtle changes effectively due to high temporal dynamics and managing spatial-temporal feature interdependencies. To address these issues, a modified ActionFormer approach is proposed for sensor signals. The method utilizes the Sequence-and-Excitation strategy to minimize additional parameters and incorporates the swish activation function for directional information retention. Experiments on the WEAR dataset demonstrate a significant 16.01% average mean Average Precision (mAP) improvement for inertial data.<br /><br />Keywords: Human Activity Recognition, Transformer-based models, ActionFormer, sensor signals, Sequence-and-Excitation strategy<br /><br />Summary: Transformer-based models like ActionFormer have enhanced HAR by detecting activity borders and labels. Adapting ActionFormer for sensor signals addresses challenges of capturing subtle changes due to high temporal dynamics and managing spatial-temporal feature interdependencies. The proposed modified approach shows a substantial 16.01% improvement in average mAP for inertial data on the WEAR dataset. <div>
arXiv:2505.20739v1 Announce Type: new 
Abstract: Human Activity Recognition (HAR) has recently witnessed advancements with Transformer-based models. Especially, ActionFormer shows us a new perspectives for HAR in the sense that this approach gives us additional outputs which detect the border of the activities as well as the activity labels. ActionFormer was originally proposed with its input as image/video. However, this was converted to with its input as sensor signals as well. We analyze this extensively in terms of deep learning architectures. Based on the report of high temporal dynamics which limits the model's ability to capture subtle changes effectively and of the interdependencies between the spatial and temporal features. We propose the modified ActionFormer which will decrease these defects for sensor signals. The key to our approach lies in accordance with the Sequence-and-Excitation strategy to minimize the increase in additional parameters and opt for the swish activation function to retain the information about direction in the negative range. Experiments on the WEAR dataset show that our method achieves substantial improvement of a 16.01\% in terms of average mAP for inertial data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>'Hello, World!': Making GNNs Talk with LLMs</title>
<link>https://arxiv.org/abs/2505.20742</link>
<guid>https://arxiv.org/abs/2505.20742</guid>
<content:encoded><![CDATA[
<div> Graph Lingual Network, GNN, hidden representations, large language models, node classification<br />
<br />
Summary:<br />
Graph Lingual Network (GLN) is a graph neural network (GNN) that utilizes large language models (LLMs) to create human-readable text representations for improved interpretability. By incorporating GNN techniques like graph attention and initial residual connection, GLN allows for intuitive analysis of how node representations change across layers and under different GNN techniques. The comprehensibility of GLN's hidden representations offers insights into the inner workings of GNNs. Additionally, GLN demonstrates impressive zero-shot performance on tasks like node classification and link prediction, surpassing existing LLM-based methods. <div>
arXiv:2505.20742v1 Announce Type: new 
Abstract: While graph neural networks (GNNs) have shown remarkable performance across diverse graph-related tasks, their high-dimensional hidden representations render them black boxes. In this work, we propose Graph Lingual Network (GLN), a GNN built on large language models (LLMs), with hidden representations in the form of human-readable text. Through careful prompt design, GLN incorporates not only the message passing module of GNNs but also advanced GNN techniques, including graph attention and initial residual connection. The comprehensibility of GLN's hidden representations enables an intuitive analysis of how node representations change (1) across layers and (2) under advanced GNN techniques, shedding light on the inner workings of GNNs. Furthermore, we demonstrate that GLN achieves strong zero-shot performance on node classification and link prediction, outperforming existing LLM-based baseline methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uni-Instruct: One-step Diffusion Model through Unified Diffusion Divergence Instruction</title>
<link>https://arxiv.org/abs/2505.20755</link>
<guid>https://arxiv.org/abs/2505.20755</guid>
<content:encoded><![CDATA[
<div> Keywords: Uni-Instruct, one-step diffusion distillation, $f$-divergence family, generation benchmarks, text-to-3D generation <br />
<br />
Summary: 
The paper introduces Uni-Instruct, a framework that unifies multiple existing one-step diffusion distillation approaches within the $f$-divergence family theory. By overcoming the intractability issue of the original expanded $f$-divergence, Uni-Instruct offers a tractable loss for training one-step diffusion models effectively. This unification not only provides new theoretical insights into existing approaches but also achieves state-of-the-art performance on generation benchmarks. Uni-Instruct sets record-breaking Frechet Inception Distance values for both unconditional and conditional generation on CIFAR10 and achieves a new state-of-the-art FID on the ImageNet-$64\times 64$ benchmark. Additionally, Uni-Instruct is applied successfully to text-to-3D generation tasks, outperforming previous methods in terms of quality and diversity. The theoretical and empirical contributions of Uni-Instruct are poised to advance research in one-step diffusion distillation and knowledge transfer for diffusion models. <br /> <div>
arXiv:2505.20755v1 Announce Type: new 
Abstract: In this paper, we unify more than 10 existing one-step diffusion distillation approaches, such as Diff-Instruct, DMD, SIM, SiD, $f$-distill, etc, inside a theory-driven framework which we name the \textbf{\emph{Uni-Instruct}}. Uni-Instruct is motivated by our proposed diffusion expansion theory of the $f$-divergence family. Then we introduce key theories that overcome the intractability issue of the original expanded $f$-divergence, resulting in an equivalent yet tractable loss that effectively trains one-step diffusion models by minimizing the expanded $f$-divergence family. The novel unification introduced by Uni-Instruct not only offers new theoretical contributions that help understand existing approaches from a high-level perspective but also leads to state-of-the-art one-step diffusion generation performances. On the CIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet Inception Distance (FID) values of \textbf{\emph{1.46}} for unconditional generation and \textbf{\emph{1.38}} for conditional generation. On the ImageNet-$64\times 64$ generation benchmark, Uni-Instruct achieves a new SoTA one-step generation FID of \textbf{\emph{1.02}}, which outperforms its 79-step teacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35). We also apply Uni-Instruct on broader tasks like text-to-3D generation. For text-to-3D generation, Uni-Instruct gives decent results, which slightly outperforms previous methods, such as SDS and VSD, in terms of both generation quality and diversity. Both the solid theoretical and empirical contributions of Uni-Instruct will potentially help future studies on one-step diffusion distillation and knowledge transferring of diffusion models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Practical estimation of the optimal classification error with soft labels and calibration</title>
<link>https://arxiv.org/abs/2505.20761</link>
<guid>https://arxiv.org/abs/2505.20761</guid>
<content:encoded><![CDATA[
<div> Keywords: machine learning, binary classification, Bayes error estimation, soft labels, isotonic calibration <br />
Summary: 
This paper addresses the question of improving machine learning models in the context of binary classification. The authors extend previous work on estimating the Bayes error using soft labels in two key ways. They investigate the bias of the estimator based on hard labels and show that it decays faster as the number of hard labels per instance increases. Additionally, they tackle the issue of estimating with corrupted soft labels and demonstrate that isotonic calibration can provide a statistically consistent estimator. Importantly, their method is instance-free, making it suitable for scenarios where input instances are not available. Experimental results on synthetic and real-world datasets confirm the effectiveness of their approach and theoretical findings. <br /><br />Summary: <div>
arXiv:2505.20761v1 Announce Type: new 
Abstract: While the performance of machine learning systems has experienced significant improvement in recent years, relatively little attention has been paid to the fundamental question: to what extent can we improve our models? This paper provides a means of answering this question in the setting of binary classification, which is practical and theoretically supported. We extend a previous work that utilizes soft labels for estimating the Bayes error, the optimal error rate, in two important ways. First, we theoretically investigate the properties of the bias of the hard-label-based estimator discussed in the original work. We reveal that the decay rate of the bias is adaptive to how well the two class-conditional distributions are separated, and it can decay significantly faster than the previous result suggested as the number of hard labels per instance grows. Second, we tackle a more challenging problem setting: estimation with corrupted soft labels. One might be tempted to use calibrated soft labels instead of clean ones. However, we reveal that calibration guarantee is not enough, that is, even perfectly calibrated soft labels can result in a substantially inaccurate estimate. Then, we show that isotonic calibration can provide a statistically consistent estimator under an assumption weaker than that of the previous work. Our method is instance-free, i.e., we do not assume access to any input instances. This feature allows it to be adopted in practical scenarios where the instances are not available due to privacy issues. Experiments with synthetic and real-world datasets show the validity of our methods and theory.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Explainable Detector of Time Series Anomaly via Augmenting Multiclass Pseudo-Anomalies</title>
<link>https://arxiv.org/abs/2505.20765</link>
<guid>https://arxiv.org/abs/2505.20765</guid>
<content:encoded><![CDATA[
<div> unsupervised anomaly detection, time series, data augmentation, multiclass classification, robustness

Summary: 
RedLamp proposes a novel approach to unsupervised anomaly detection in time series data. Unlike existing methods that focus on learning normality, RedLamp utilizes diverse data augmentations to generate multiclass pseudo-anomalies, covering a wide range of time series anomalies. By employing soft labels in multiclass classification, the model prevents overconfidence and ensures robustness against contaminated or false anomalies. The learned latent space is inherently explainable as it separates pseudo-anomalies into multiclasses. Experimental results demonstrate the effectiveness of RedLamp in anomaly detection and its ability to handle anomaly contamination, making it a promising approach in the field of unsupervised anomaly detection in time series data.  <div>
arXiv:2505.20765v1 Announce Type: new 
Abstract: Unsupervised anomaly detection in time series has been a pivotal research area for decades. Current mainstream approaches focus on learning normality, on the assumption that all or most of the samples in the training set are normal. However, anomalies in the training set (i.e., anomaly contamination) can be misleading. Recent studies employ data augmentation to generate pseudo-anomalies and learn the boundary separating the training samples from the augmented samples. Although this approach mitigates anomaly contamination if augmented samples mimic unseen real anomalies, it suffers from several limitations. (1) Covering a wide range of time series anomalies is challenging. (2) It disregards augmented samples that resemble normal samples (i.e., false anomalies). (3) It places too much trust in the labels of training and augmented samples. In response, we propose RedLamp, which employs diverse data augmentations to generate multiclass pseudo-anomalies and learns the multiclass boundary. Such multiclass pseudo-anomalies cover a wide variety of time series anomalies. We conduct multiclass classification using soft labels, which prevents the model from being overconfident and ensures its robustness against contaminated/false anomalies. The learned latent space is inherently explainable as it is trained to separate pseudo-anomalies into multiclasses. Extensive experiments demonstrate the effectiveness of RedLamp in anomaly detection and its robustness against anomaly contamination.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TimePro: Efficient Multivariate Long-term Time Series Forecasting with Variable- and Time-Aware Hyper-state</title>
<link>https://arxiv.org/abs/2505.20774</link>
<guid>https://arxiv.org/abs/2505.20774</guid>
<content:encoded><![CDATA[
<div> Keywords: long-term time series forecasting, multi-delay issue, TimePro, variate tokens, real-world benchmarks

Summary:<br />
- The article introduces TimePro, a novel Mamba-based model designed to address the multi-delay issue in long-term time series forecasting by considering the variable relationships and temporal information.
- Traditional models often struggle with capturing distinct variable influences over different time intervals, limiting their forecasting capabilities.
- TimePro constructs variate- and time-aware hyper-states, allowing for the preservation of fine-grained temporal features of variate tokens and adaptive selection of focused time points.
- By reconstructing hyper-states that capture variable relationships and important temporal information, TimePro achieves accurate forecasting results on eight real-world benchmarks.
- The model demonstrates competitive performance with linear complexity and the code is available for deployment and further research on the GitHub repository. 

<br /><br />Summary: TimePro is an innovative Mamba-based model that addresses the multi-delay issue in long-term time series forecasting by creating variate- and time-aware hyper-states. By preserving fine-grained temporal features of variate tokens and selecting focused time points, TimePro improves forecasting accuracy on real-world benchmarks. The model's competitive performance with linear complexity and availability of code on GitHub make it a promising tool for accurate long-term forecasting. <div>
arXiv:2505.20774v1 Announce Type: new 
Abstract: In long-term time series forecasting, different variables often influence the target variable over distinct time intervals, a challenge known as the multi-delay issue. Traditional models typically process all variables or time points uniformly, which limits their ability to capture complex variable relationships and obtain non-trivial time representations. To address this issue, we propose TimePro, an innovative Mamba-based model that constructs variate- and time-aware hyper-states. Unlike conventional approaches that merely transfer plain states across variable or time dimensions, TimePro preserves the fine-grained temporal features of each variate token and adaptively selects the focused time points to tune the plain state. The reconstructed hyper-state can perceive both variable relationships and salient temporal information, which helps the model make accurate forecasting. In experiments, TimePro performs competitively on eight real-world long-term forecasting benchmarks with satisfactory linear complexity. Code is available at https://github.com/xwmaxwma/TimePro.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-invasive maturity assessment of iPSC-CMs based on optical maturity characteristics using interpretable AI</title>
<link>https://arxiv.org/abs/2505.20775</link>
<guid>https://arxiv.org/abs/2505.20775</guid>
<content:encoded><![CDATA[
<div> Keywords: iPSC-CMs, maturation, artificial intelligence, motion analysis, therapeutic targets <br />
<br />
Summary: 
The study focused on developing a non-invasive method for automatically assessing the maturity level of induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) using artificial intelligence (AI) and motion analysis. The researchers analyzed video recordings of immature iPSC-CMs and more mature ones cultured in maturation medium, extracting 10 features from each recording. An optimized support vector machine (SVM) model achieved a high accuracy of 99.5% on a test set. Important features for maturity assessment were identified as displacement, relaxation-rise time, and beating duration using Shapley Additive Explanations (SHAP). This non-invasive approach could potentially improve the reproducibility of experimental studies by providing a quick and reliable method to assess iPSC-CMs maturity before conducting functional tests or drug screenings. <div>
arXiv:2505.20775v1 Announce Type: new 
Abstract: Human induced pluripotent stem cell-derived cardiomyocytes (iPSC-CMs) are an important resource for the identification of new therapeutic targets and cardioprotective drugs. After differentiation iPSC-CMs show an immature, fetal-like phenotype. Cultivation of iPSC-CMs in lipid-supplemented maturation medium (MM) strongly enhances their structural, metabolic and functional phenotype. Nevertheless, assessing iPSC-CM maturation state remains challenging as most methods are time consuming and go in line with cell damage or loss of the sample. To address this issue, we developed a non-invasive approach for automated classification of iPSC-CM maturity through interpretable artificial intelligence (AI)-based analysis of beat characteristics derived from video-based motion analysis. In a prospective study, we evaluated 230 video recordings of early-state, immature iPSC-CMs on day 21 after differentiation (d21) and more mature iPSC-CMs cultured in MM (d42, MM). For each recording, 10 features were extracted using Maia motion analysis software and entered into a support vector machine (SVM). The hyperparameters of the SVM were optimized in a grid search on 80 % of the data using 5-fold cross-validation. The optimized model achieved an accuracy of 99.5 $\pm$ 1.1 % on a hold-out test set. Shapley Additive Explanations (SHAP) identified displacement, relaxation-rise time and beating duration as the most relevant features for assessing maturity level. Our results suggest the use of non-invasive, optical motion analysis combined with AI-based methods as a tool to assess iPSC-CMs maturity and could be applied before performing functional readouts or drug testing. This may potentially reduce the variability and improve the reproducibility of experimental studies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-VQC: A Novel QML Approach for Enhancing Healthcare Classification</title>
<link>https://arxiv.org/abs/2505.20797</link>
<guid>https://arxiv.org/abs/2505.20797</guid>
<content:encoded><![CDATA[
<div> Keywords: diagnosis, diseases, Machine Learning, Quantum models, class imbalances 

Summary: 
Machine Learning has revolutionized disease diagnosis by creating accurate classification models. However, class imbalances in these problems can hinder their effectiveness. Quantum models have emerged as a promising solution to overcome these limitations, as they can map data in a higher-dimensional computational space and express complex patterns. Accurate and reliable diagnosis of diseases is crucial for timely medical treatment and improving patient survival rates. The interest in Quantum models has been driven by their potential to address the challenges faced by traditional classification models due to class imbalances. Quantum models offer a new approach to disease diagnosis, leveraging their ability to handle complex patterns and increase the accuracy of results. <div>
arXiv:2505.20797v1 Announce Type: new 
Abstract: Accurate and reliable diagnosis of diseases is crucial in enabling timely medical treatment and enhancing patient survival rates. In recent years, Machine Learning has revolutionized diagnostic practices by creating classification models capable of identifying diseases. However, these classification problems often suffer from significant class imbalances, which can inhibit the effectiveness of traditional models. Therefore, the interest in Quantum models has arisen, driven by the captivating promise of overcoming the limitations of the classical counterpart thanks to their ability to express complex patterns by mapping data in a higher-dimensional computational space.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leaner Transformers: More Heads, Less Depth</title>
<link>https://arxiv.org/abs/2505.20802</link>
<guid>https://arxiv.org/abs/2505.20802</guid>
<content:encoded><![CDATA[
arXiv:2505.20802v1 Announce Type: new 
Abstract: Transformers have reshaped machine learning by utilizing attention mechanisms to capture complex patterns in large datasets, leading to significant improvements in performance. This success has contributed to the belief that "bigger means better", leading to ever-increasing model sizes. This paper challenge this ideology by showing that many existing transformers might be unnecessarily oversized. We discover a theoretical principle that redefines the role of multi-head attention. An important benefit of the multiple heads is in improving the conditioning of the attention block. We exploit this theoretical insight and redesign popular architectures with an increased number of heads. The improvement in the conditioning proves so significant in practice that model depth can be decreased, reducing the parameter count by up to 30-50% while maintaining accuracy. We obtain consistent benefits across a variety of transformer-based architectures of various scales, on tasks in computer vision (ImageNet-1k) as well as language and sequence modeling (GLUE benchmark, TinyStories, and the Long-Range Arena benchmark).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Machine Learning in Healthcare: Evaluating QNN and QSVM Models</title>
<link>https://arxiv.org/abs/2505.20804</link>
<guid>https://arxiv.org/abs/2505.20804</guid>
<content:encoded><![CDATA[
arXiv:2505.20804v1 Announce Type: new 
Abstract: Effective and accurate diagnosis of diseases such as cancer, diabetes, and heart failure is crucial for timely medical intervention and improving patient survival rates. Machine learning has revolutionized diagnostic methods in recent years by developing classification models that detect diseases based on selected features. However, these classification tasks are often highly imbalanced, limiting the performance of classical models. Quantum models offer a promising alternative, exploiting their ability to express complex patterns by operating in a higher-dimensional computational space through superposition and entanglement. These unique properties make quantum models potentially more effective in addressing the challenges of imbalanced datasets. This work evaluates the potential of quantum classifiers in healthcare, focusing on Quantum Neural Networks (QNNs) and Quantum Support Vector Machines (QSVMs), comparing them with popular classical models. The study is based on three well-known healthcare datasets -- Prostate Cancer, Heart Failure, and Diabetes. The results indicate that QSVMs outperform QNNs across all datasets due to their susceptibility to overfitting. Furthermore, quantum models prove the ability to overcome classical models in scenarios with high dataset imbalance. Although preliminary, these findings highlight the potential of quantum models in healthcare classification tasks and lead the way for further research in this domain.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple yet Effective Graph Distillation via Clustering</title>
<link>https://arxiv.org/abs/2505.20807</link>
<guid>https://arxiv.org/abs/2505.20807</guid>
<content:encoded><![CDATA[
arXiv:2505.20807v1 Announce Type: new 
Abstract: Despite plentiful successes achieved by graph representation learning in various domains, the training of graph neural networks (GNNs) still remains tenaciously challenging due to the tremendous computational overhead needed for sizable graphs in practice. Recently, graph data distillation (GDD), which seeks to distill large graphs into compact and informative ones, has emerged as a promising technique to enable efficient GNN training. However, most existing GDD works rely on heuristics that align model gradients or representation distributions on condensed and original graphs, leading to compromised result quality, expensive training for distilling large graphs, or both. Motivated by this, this paper presents an efficient and effective GDD approach, ClustGDD. Under the hood, ClustGDD resorts to synthesizing the condensed graph and node attributes through fast and theoretically-grounded clustering that minimizes the within-cluster sum of squares and maximizes the homophily on the original graph. The fundamental idea is inspired by our empirical and theoretical findings unveiling the connection between clustering and empirical condensation quality using Fr\'echet Inception Distance, a well-known quality metric for synthetic images. Furthermore, to mitigate the adverse effects caused by the homophily-based clustering, ClustGDD refines the nodal attributes of the condensed graph with a small augmentation learned via class-aware graph sampling and consistency loss. Our extensive experiments exhibit that GNNs trained over condensed graphs output by ClustGDD consistently achieve superior or comparable performance to state-of-the-art GDD methods in terms of node classification on five benchmark datasets, while being orders of magnitude faster.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Credit Default Prediction with Ensemble Learning and SHAP</title>
<link>https://arxiv.org/abs/2505.20815</link>
<guid>https://arxiv.org/abs/2505.20815</guid>
<content:encoded><![CDATA[
arXiv:2505.20815v1 Announce Type: new 
Abstract: This study focuses on the problem of credit default prediction, builds a modeling framework based on machine learning, and conducts comparative experiments on a variety of mainstream classification algorithms. Through preprocessing, feature engineering, and model training of the Home Credit dataset, the performance of multiple models including logistic regression, random forest, XGBoost, LightGBM, etc. in terms of accuracy, precision, and recall is evaluated. The results show that the ensemble learning method has obvious advantages in predictive performance, especially in dealing with complex nonlinear relationships between features and data imbalance problems. It shows strong robustness. At the same time, the SHAP method is used to analyze the importance and dependency of features, and it is found that the external credit score variable plays a dominant role in model decision making, which helps to improve the model's interpretability and practical application value. The research results provide effective reference and technical support for the intelligent development of credit risk control systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HAD: Hybrid Architecture Distillation Outperforms Teacher in Genomic Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.20836</link>
<guid>https://arxiv.org/abs/2505.20836</guid>
<content:encoded><![CDATA[
arXiv:2505.20836v1 Announce Type: new 
Abstract: Inspired by the great success of Masked Language Modeling (MLM) in the natural language domain, the paradigm of self-supervised pre-training and fine-tuning has also achieved remarkable progress in the field of DNA sequence modeling. However, previous methods often relied on massive pre-training data or large-scale base models with huge parameters, imposing a significant computational burden. To address this, many works attempted to use more compact models to achieve similar outcomes but still fell short by a considerable margin. In this work, we propose a Hybrid Architecture Distillation (HAD) approach, leveraging both distillation and reconstruction tasks for more efficient and effective pre-training. Specifically, we employ the NTv2-500M as the teacher model and devise a grouping masking strategy to align the feature embeddings of visible tokens while concurrently reconstructing the invisible tokens during MLM pre-training. To validate the effectiveness of our proposed method, we conducted comprehensive experiments on the Nucleotide Transformer Benchmark and Genomic Benchmark. Compared to models with similar parameters, our model achieved excellent performance. More surprisingly, it even surpassed the distillation ceiling-teacher model on some sub-tasks, which is more than 500 $\times$ larger. Lastly, we utilize t-SNE for more intuitive visualization, which shows that our model can gain a sophisticated understanding of the intrinsic representation pattern in genomic sequences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FireQ: Fast INT4-FP8 Kernel and RoPE-aware Quantization for LLM Inference Acceleration</title>
<link>https://arxiv.org/abs/2505.20839</link>
<guid>https://arxiv.org/abs/2505.20839</guid>
<content:encoded><![CDATA[
arXiv:2505.20839v1 Announce Type: new 
Abstract: As large language models become increasingly prevalent, memory bandwidth constraints significantly limit inference throughput, motivating post-training quantization (PTQ). In this paper, we propose FireQ, a co-designed PTQ framework and an INT4-FP8 matrix multiplication kernel that accelerates LLM inference across all linear layers. Specifically, FireQ quantizes linear layer weights and key-values to INT4, and activations and queries to FP8, significantly enhancing throughput. Additionally, we introduce a three-stage pipelining for the prefill phase, which modifies the FlashAttention-3 kernel, effectively reducing time-to-first-token in the prefill phase. To minimize accuracy loss from quantization, we develop novel outlier smoothing techniques tailored separately for linear and attention layers. In linear layers, we explicitly use per-tensor scaling to prevent underflow caused by the FP8 quantization scaling factor of INT4 quantization, and channel-wise scaling to compensate for coarse granularity of INT4. In attention layers, we address quantization challenges posed by rotary positional embeddings (RoPE) by combining pre-RoPE and post-RoPE scaling strategies. FireQ significantly outperforms state-of-the-art methods, achieving 1.68x faster inference in feed-forward network layers on Llama2-7B and 1.26x faster prefill phase performance on Llama3-8B compared to QServe, with negligible accuracy loss.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Aggregation Buffer: Revisiting DropEdge with a New Parameter Block</title>
<link>https://arxiv.org/abs/2505.20840</link>
<guid>https://arxiv.org/abs/2505.20840</guid>
<content:encoded><![CDATA[
arXiv:2505.20840v1 Announce Type: new 
Abstract: We revisit DropEdge, a data augmentation technique for GNNs which randomly removes edges to expose diverse graph structures during training. While being a promising approach to effectively reduce overfitting on specific connections in the graph, we observe that its potential performance gain in supervised learning tasks is significantly limited. To understand why, we provide a theoretical analysis showing that the limited performance of DropEdge comes from the fundamental limitation that exists in many GNN architectures. Based on this analysis, we propose Aggregation Buffer, a parameter block specifically designed to improve the robustness of GNNs by addressing the limitation of DropEdge. Our method is compatible with any GNN model, and shows consistent performance improvements on multiple datasets. Moreover, our method effectively addresses well-known problems such as degree bias or structural disparity as a unifying solution. Code and datasets are available at https://github.com/dooho00/agg-buffer.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cooperation of Experts: Fusing Heterogeneous Information with Large Margin</title>
<link>https://arxiv.org/abs/2505.20853</link>
<guid>https://arxiv.org/abs/2505.20853</guid>
<content:encoded><![CDATA[
arXiv:2505.20853v1 Announce Type: new 
Abstract: Fusing heterogeneous information remains a persistent challenge in modern data analysis. While significant progress has been made, existing approaches often fail to account for the inherent heterogeneity of object patterns across different semantic spaces. To address this limitation, we propose the Cooperation of Experts (CoE) framework, which encodes multi-typed information into unified heterogeneous multiplex networks. By overcoming modality and connection differences, CoE provides a powerful and flexible model for capturing the intricate structures of real-world complex data. In our framework, dedicated encoders act as domain-specific experts, each specializing in learning distinct relational patterns in specific semantic spaces. To enhance robustness and extract complementary knowledge, these experts collaborate through a novel large margin mechanism supported by a tailored optimization strategy. Rigorous theoretical analyses guarantee the framework's feasibility and stability, while extensive experiments across diverse benchmarks demonstrate its superior performance and broad applicability. Our code is available at https://github.com/strangeAlan/CoE.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable Heuristic Generation Through Large Language Models with Meta-Optimization</title>
<link>https://arxiv.org/abs/2505.20881</link>
<guid>https://arxiv.org/abs/2505.20881</guid>
<content:encoded><![CDATA[
arXiv:2505.20881v1 Announce Type: new 
Abstract: Heuristic design with large language models (LLMs) has emerged as a promising approach for tackling combinatorial optimization problems (COPs). However, existing approaches often rely on manually predefined evolutionary computation (EC) optimizers and single-task training schemes, which may constrain the exploration of diverse heuristic algorithms and hinder the generalization of the resulting heuristics. To address these issues, we propose Meta-Optimization of Heuristics (MoH), a novel framework that operates at the optimizer level, discovering effective optimizers through the principle of meta-learning. Specifically, MoH leverages LLMs to iteratively refine a meta-optimizer that autonomously constructs diverse optimizers through (self-)invocation, thereby eliminating the reliance on a predefined EC optimizer. These constructed optimizers subsequently evolve heuristics for downstream tasks, enabling broader heuristic exploration. Moreover, MoH employs a multi-task training scheme to promote its generalization capability. Experiments on classic COPs demonstrate that MoH constructs an effective and interpretable meta-optimizer, achieving state-of-the-art performance across various downstream tasks, particularly in cross-size settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fedivertex: a Graph Dataset based on Decentralized Social Networks for Trustworthy Machine Learning</title>
<link>https://arxiv.org/abs/2505.20882</link>
<guid>https://arxiv.org/abs/2505.20882</guid>
<content:encoded><![CDATA[
arXiv:2505.20882v1 Announce Type: new 
Abstract: Decentralized machine learning - where each client keeps its own data locally and uses its own computational resources to collaboratively train a model by exchanging peer-to-peer messages - is increasingly popular, as it enables better scalability and control over the data. A major challenge in this setting is that learning dynamics depend on the topology of the communication graph, which motivates the use of real graph datasets for benchmarking decentralized algorithms. Unfortunately, existing graph datasets are largely limited to for-profit social networks crawled at a fixed point in time and often collected at the user scale, where links are heavily influenced by the platform and its recommendation algorithms. The Fediverse, which includes several free and open-source decentralized social media platforms such as Mastodon, Misskey, and Lemmy, offers an interesting real-world alternative. We introduce Fedivertex, a new dataset of 182 graphs, covering seven social networks from the Fediverse, crawled weekly over 14 weeks. We release the dataset along with a Python package to facilitate its use, and illustrate its utility on several tasks, including a new defederation task, which captures a process of link deletion observed on these networks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Bounds for Swap Multicalibration and Swap Omniprediction</title>
<link>https://arxiv.org/abs/2505.20885</link>
<guid>https://arxiv.org/abs/2505.20885</guid>
<content:encoded><![CDATA[
arXiv:2505.20885v1 Announce Type: new 
Abstract: In this paper, we consider the related problems of multicalibration -- a multigroup fairness notion and omniprediction -- a simultaneous loss minimization paradigm, both in the distributional and online settings. The recent work of Garg et al. (2024) raised the open problem of whether it is possible to efficiently achieve $O(\sqrt{T})$ $\ell_{2}$-multicalibration error against bounded linear functions. In this paper, we answer this question in a strongly affirmative sense. We propose an efficient algorithm that achieves $O(T^{\frac{1}{3}})$ $\ell_{2}$-swap multicalibration error (both in high probability and expectation). On propagating this bound onward, we obtain significantly improved rates for $\ell_{1}$-swap multicalibration and swap omniprediction for a loss class of convex Lipschitz functions. In particular, we show that our algorithm achieves $O(T^{\frac{2}{3}})$ $\ell_{1}$-swap multicalibration and swap omniprediction errors, thereby improving upon the previous best-known bound of $O(T^{\frac{7}{8}})$. As a consequence of our improved online results, we further obtain several improved sample complexity rates in the distributional setting. In particular, we establish a $O(\varepsilon ^ {-3})$ sample complexity of efficiently learning an $\varepsilon$-swap omnipredictor for the class of convex and Lipschitz functions, $O(\varepsilon ^{-2.5})$ sample complexity of efficiently learning an $\varepsilon$-swap agnostic learner for the squared loss, and $O(\varepsilon ^ {-5}), O(\varepsilon ^ {-2.5})$ sample complexities of learning $\ell_{1}, \ell_{2}$-swap multicalibrated predictors against linear functions, all of which significantly improve on the previous best-known bounds.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>One-Time Soft Alignment Enables Resilient Learning without Weight Transport</title>
<link>https://arxiv.org/abs/2505.20892</link>
<guid>https://arxiv.org/abs/2505.20892</guid>
<content:encoded><![CDATA[
arXiv:2505.20892v1 Announce Type: new 
Abstract: Backpropagation is the cornerstone of deep learning, but its reliance on symmetric weight transport and global synchronization makes it computationally expensive and biologically implausible. Feedback alignment offers a promising alternative by approximating error gradients through fixed random feedback, thereby avoiding symmetric weight transport. However, this approach often struggles with poor learning performance and instability, especially in deep networks. Here, we show that a one-time soft alignment between forward and feedback weights at initialization enables deep networks to achieve performance comparable to backpropagation, without requiring weight transport during learning. This simple initialization condition guides stable error minimization in the loss landscape, improving network trainability. Spectral analyses further reveal that initial alignment promotes smoother gradient flow and convergence to flatter minima, resulting in better generalization and robustness. Notably, we also find that allowing moderate deviations from exact weight symmetry can improve adversarial robustness compared to standard backpropagation. These findings demonstrate that a simple initialization strategy can enable effective learning in deep networks in a biologically plausible and resource-efficient manner.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeepConvContext: A Multi-Scale Approach to Timeseries Classification in Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.20894</link>
<guid>https://arxiv.org/abs/2505.20894</guid>
<content:encoded><![CDATA[
arXiv:2505.20894v1 Announce Type: new 
Abstract: Despite recognized limitations in modeling long-range temporal dependencies, Human Activity Recognition (HAR) has traditionally relied on a sliding window approach to segment labeled datasets. Deep learning models like the DeepConvLSTM typically classify each window independently, thereby restricting learnable temporal context to within-window information. To address this constraint, we propose DeepConvContext, a multi-scale time series classification framework for HAR. Drawing inspiration from the vision-based Temporal Action Localization community, DeepConvContext models both intra- and inter-window temporal patterns by processing sequences of time-ordered windows. Unlike recent HAR models that incorporate attention mechanisms, DeepConvContext relies solely on LSTMs -- with ablation studies demonstrating the superior performance of LSTMs over attention-based variants for modeling inertial sensor data. Across six widely-used HAR benchmarks, DeepConvContext achieves an average 10% improvement in F1-score over the classic DeepConvLSTM, with gains of up to 21%. Code to reproduce our experiments is publicly available via github.com/mariusbock/context_har.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Do Transformers Learn Variable Binding in Symbolic Programs?</title>
<link>https://arxiv.org/abs/2505.20896</link>
<guid>https://arxiv.org/abs/2505.20896</guid>
<content:encoded><![CDATA[
arXiv:2505.20896v1 Announce Type: new 
Abstract: Variable binding -- the ability to associate variables with values -- is fundamental to symbolic computation and cognition. Although classical architectures typically implement variable binding via addressable memory, it is not well understood how modern neural networks lacking built-in binding operations may acquire this capacity. We investigate this by training a Transformer to dereference queried variables in symbolic programs where variables are assigned either numerical constants or other variables. Each program requires following chains of variable assignments up to four steps deep to find the queried value, and also contains irrelevant chains of assignments acting as distractors. Our analysis reveals a developmental trajectory with three distinct phases during training: (1) random prediction of numerical constants, (2) a shallow heuristic prioritizing early variable assignments, and (3) the emergence of a systematic mechanism for dereferencing assignment chains. Using causal interventions, we find that the model learns to exploit the residual stream as an addressable memory space, with specialized attention heads routing information across token positions. This mechanism allows the model to dynamically track variable bindings across layers, resulting in accurate dereferencing. Our results show how Transformer models can learn to implement systematic variable binding without explicit architectural support, bridging connectionist and symbolic approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Humble AI in the real-world: the case of algorithmic hiring</title>
<link>https://arxiv.org/abs/2505.20918</link>
<guid>https://arxiv.org/abs/2505.20918</guid>
<content:encoded><![CDATA[
arXiv:2505.20918v1 Announce Type: new 
Abstract: Humble AI (Knowles et al., 2023) argues for cautiousness in AI development and deployments through scepticism (accounting for limitations of statistical learning), curiosity (accounting for unexpected outcomes), and commitment (accounting for multifaceted values beyond performance). We present a real-world case study for humble AI in the domain of algorithmic hiring. Specifically, we evaluate virtual screening algorithms in a widely used hiring platform that matches candidates to job openings. There are several challenges in misrecognition and stereotyping in such contexts that are difficult to assess through standard fairness and trust frameworks; e.g., someone with a non-traditional background is less likely to rank highly. We demonstrate technical feasibility of how humble AI principles can be translated to practice through uncertainty quantification of ranks, entropy estimates, and a user experience that highlights algorithmic unknowns. We describe preliminary discussions with focus groups made up of recruiters. Future user studies seek to evaluate whether the higher cognitive load of a humble AI system fosters a climate of trust in its outcomes.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Label Leakage in Federated Inertial-based Human Activity Recognition</title>
<link>https://arxiv.org/abs/2505.20924</link>
<guid>https://arxiv.org/abs/2505.20924</guid>
<content:encoded><![CDATA[
arXiv:2505.20924v1 Announce Type: new 
Abstract: While prior work has shown that Federated Learning updates can leak sensitive information, label reconstruction attacks, which aim to recover input labels from shared gradients, have not yet been examined in the context of Human Activity Recognition (HAR). Given the sensitive nature of activity labels, this study evaluates the effectiveness of state-of-the-art gradient-based label leakage attacks on HAR benchmark datasets. Our findings show that the number of activity classes, sampling strategy, and class imbalance are critical factors influencing the extent of label leakage, with reconstruction accuracies reaching up to 90% on two benchmark datasets, even for trained models. Moreover, we find that Local Differential Privacy techniques such as gradient noise and clipping offer only limited protection, as certain attacks still reliably infer both majority and minority class labels. We conclude by offering practical recommendations for the privacy-aware deployment of federated HAR systems and identify open challenges for future research. Code to reproduce our experiments is publicly available via github.com/mariusbock/leakage_har.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MLMC-based Resource Adequacy Assessment with Active Learning Trained Surrogate Models</title>
<link>https://arxiv.org/abs/2505.20930</link>
<guid>https://arxiv.org/abs/2505.20930</guid>
<content:encoded><![CDATA[
arXiv:2505.20930v1 Announce Type: new 
Abstract: Multilevel Monte Carlo (MLMC) is a flexible and effective variance reduction technique for accelerating reliability assessments of complex power system. Recently, data-driven surrogate models have been proposed as lower-level models in the MLMC framework due to their high correlation and negligible execution time once trained. However, in resource adequacy assessments, pre-labeled datasets are typically unavailable. For large-scale systems, the efficiency gains from surrogate models are often offset by the substantial time required for labeling training data. Therefore, this paper introduces a speed metric that accounts for training time in evaluating MLMC efficiency. Considering the total time budget is limited, a vote-by-committee active learning approach is proposed to reduce the required labeling calls. A case study demonstrates that, within practical variance thresholds, active learning enables significantly improved MLMC efficiency with reduced training effort, compared to regular surrogate modelling approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NatADiff: Adversarial Boundary Guidance for Natural Adversarial Diffusion</title>
<link>https://arxiv.org/abs/2505.20934</link>
<guid>https://arxiv.org/abs/2505.20934</guid>
<content:encoded><![CDATA[
arXiv:2505.20934v1 Announce Type: new 
Abstract: Adversarial samples exploit irregularities in the manifold ``learned'' by deep learning models to cause misclassifications. The study of these adversarial samples provides insight into the features a model uses to classify inputs, which can be leveraged to improve robustness against future attacks. However, much of the existing literature focuses on constrained adversarial samples, which do not accurately reflect test-time errors encountered in real-world settings. To address this, we propose `NatADiff', an adversarial sampling scheme that leverages denoising diffusion to generate natural adversarial samples. Our approach is based on the observation that natural adversarial samples frequently contain structural elements from the adversarial class. Deep learning models can exploit these structural elements to shortcut the classification process, rather than learning to genuinely distinguish between classes. To leverage this behavior, we guide the diffusion trajectory towards the intersection of the true and adversarial classes, combining time-travel sampling with augmented classifier guidance to enhance attack transferability while preserving image fidelity. Our method achieves comparable attack success rates to current state-of-the-art techniques, while exhibiting significantly higher transferability across model architectures and better alignment with natural test-time errors as measured by FID. These results demonstrate that NatADiff produces adversarial samples that not only transfer more effectively across models, but more faithfully resemble naturally occurring test-time errors.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Sparsity Constraint Under High-Rank Property in Partial Multi-Label Learning</title>
<link>https://arxiv.org/abs/2505.20938</link>
<guid>https://arxiv.org/abs/2505.20938</guid>
<content:encoded><![CDATA[
arXiv:2505.20938v1 Announce Type: new 
Abstract: Partial Multi-Label Learning (PML) extends the multi-label learning paradigm to scenarios where each sample is associated with a candidate label set containing both ground-truth labels and noisy labels. Existing PML methods commonly rely on two assumptions: sparsity of the noise label matrix and low-rankness of the ground-truth label matrix. However, these assumptions are inherently conflicting and impractical for real-world scenarios, where the true label matrix is typically full-rank or close to full-rank. To address these limitations, we demonstrate that the sparsity constraint contributes to the high-rank property of the predicted label matrix. Based on this, we propose a novel method Schirn, which introduces a sparsity constraint on the noise label matrix while enforcing a high-rank property on the predicted label matrix. Extensive experiments demonstrate the superior performance of Schirn compared to state-of-the-art methods, validating its effectiveness in tackling real-world PML challenges.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Spectral Control of Partially Observed Linear Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.20943</link>
<guid>https://arxiv.org/abs/2505.20943</guid>
<content:encoded><![CDATA[
arXiv:2505.20943v1 Announce Type: new 
Abstract: We propose a new method for the problem of controlling linear dynamical systems under partial observation and adversarial disturbances. Our new algorithm, Double Spectral Control (DSC), matches the best known regret guarantees while exponentially improving runtime complexity over previous approaches in its dependence on the system's stability margin. Our key innovation is a two-level spectral approximation strategy, leveraging double convolution with a universal basis of spectral filters, enabling efficient and accurate learning of the best linear dynamical controllers.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semantic Communication meets System 2 ML: How Abstraction, Compositionality and Emergent Languages Shape Intelligence</title>
<link>https://arxiv.org/abs/2505.20964</link>
<guid>https://arxiv.org/abs/2505.20964</guid>
<content:encoded><![CDATA[
arXiv:2505.20964v1 Announce Type: new 
Abstract: The trajectories of 6G and AI are set for a creative collision. However, current visions for 6G remain largely incremental evolutions of 5G, while progress in AI is hampered by brittle, data-hungry models that lack robust reasoning capabilities. This paper argues for a foundational paradigm shift, moving beyond the purely technical level of communication toward systems capable of semantic understanding and effective, goal-oriented interaction. We propose a unified research vision rooted in the principles of System-2 cognition, built upon three pillars: Abstraction, enabling agents to learn meaningful world models from raw sensorimotor data; Compositionality, providing the algebraic tools to combine learned concepts and subsystems; and Emergent Communication, allowing intelligent agents to create their own adaptive and grounded languages. By integrating these principles, we lay the groundwork for truly intelligent systems that can reason, adapt, and collaborate, unifying advances in wireless communications, machine learning, and robotics under a single coherent framework.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Understanding the behavior of representation forgetting in continual learning</title>
<link>https://arxiv.org/abs/2505.20970</link>
<guid>https://arxiv.org/abs/2505.20970</guid>
<content:encoded><![CDATA[
arXiv:2505.20970v1 Announce Type: new 
Abstract: In continual learning scenarios, catastrophic forgetting of previously learned tasks is a critical issue, making it essential to effectively measure such forgetting. Recently, there has been growing interest in focusing on representation forgetting, the forgetting measured at the hidden layer. In this paper, we provide the first theoretical analysis of representation forgetting and use this analysis to better understand the behavior of continual learning. First, we introduce a new metric called representation discrepancy, which measures the difference between representation spaces constructed by two snapshots of a model trained through continual learning. We demonstrate that our proposed metric serves as an effective surrogate for the representation forgetting while remaining analytically tractable. Second, through mathematical analysis of our metric, we derive several key findings about the dynamics of representation forgetting: the forgetting occurs more rapidly to a higher degree as the layer index increases, while increasing the width of the network slows down the forgetting process. Third, we support our theoretical findings through experiments on real image datasets, including Split-CIFAR100 and ImageNet1K.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Deep k-grouping: An Unsupervised Learning Framework for Combinatorial Optimization on Graphs and Hypergraphs</title>
<link>https://arxiv.org/abs/2505.20972</link>
<guid>https://arxiv.org/abs/2505.20972</guid>
<content:encoded><![CDATA[
arXiv:2505.20972v1 Announce Type: new 
Abstract: Along with AI computing shining in scientific discovery, its potential in the combinatorial optimization (CO) domain has also emerged in recent years. Yet, existing unsupervised neural network solvers struggle to solve $k$-grouping problems (e.g., coloring, partitioning) on large-scale graphs and hypergraphs, due to limited computational frameworks. In this work, we propose Deep $k$-grouping, an unsupervised learning-based CO framework. Specifically, we contribute: Novel one-hot encoded polynomial unconstrained binary optimization (OH-PUBO), a formulation for modeling k-grouping problems on graphs and hypergraphs (e.g., graph/hypergraph coloring and partitioning); GPU-accelerated algorithms for large-scale k-grouping CO problems. Deep $k$-grouping employs the relaxation of large-scale OH-PUBO objectives as differentiable loss functions and trains to optimize them in an unsupervised manner. To ensure scalability, it leverages GPU-accelerated algorithms to unify the training pipeline; A Gini coefficient-based continuous relaxation annealing strategy to enforce discreteness of solutions while preventing convergence to local optima. Experimental results demonstrate that Deep $k$-grouping outperforms existing neural network solvers and classical heuristics such as SCIP and Tabu.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Identity and Position Graph Embedding via Spectral-Based Random Feature Aggregation</title>
<link>https://arxiv.org/abs/2505.20992</link>
<guid>https://arxiv.org/abs/2505.20992</guid>
<content:encoded><![CDATA[
arXiv:2505.20992v1 Announce Type: new 
Abstract: Graph neural networks (GNNs), which capture graph structures via a feature aggregation mechanism following the graph embedding framework, have demonstrated a powerful ability to support various tasks. According to the topology properties (e.g., structural roles or community memberships of nodes) to be preserved, graph embedding can be categorized into identity and position embedding. However, it is unclear for most GNN-based methods which property they can capture. Some of them may also suffer from low efficiency and scalability caused by several time- and space-consuming procedures (e.g., feature extraction and training). From a perspective of graph signal processing, we find that high- and low-frequency information in the graph spectral domain may characterize node identities and positions, respectively. Based on this investigation, we propose random feature aggregation (RFA) for efficient identity and position embedding, serving as an extreme ablation study regarding GNN feature aggregation. RFA (i) adopts a spectral-based GNN without learnable parameters as its backbone, (ii) only uses random noises as inputs, and (iii) derives embeddings via just one feed-forward propagation (FFP). Inspired by degree-corrected spectral clustering, we further introduce a degree correction mechanism to the GNN backbone. Surprisingly, our experiments demonstrate that two variants of RFA with high- and low-pass filters can respectively derive informative identity and position embeddings via just one FFP (i.e., without any training). As a result, RFA can achieve a better trade-off between quality and efficiency for both identity and position embedding over various baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BIPNN: Learning to Solve Binary Integer Programming via Hypergraph Neural Networks</title>
<link>https://arxiv.org/abs/2505.20997</link>
<guid>https://arxiv.org/abs/2505.20997</guid>
<content:encoded><![CDATA[
arXiv:2505.20997v1 Announce Type: new 
Abstract: Binary (0-1) integer programming (BIP) is pivotal in scientific domains requiring discrete decision-making. As the advance of AI computing, recent works explore neural network-based solvers for integer linear programming (ILP) problems. Yet, they lack scalability for tackling nonlinear challenges. To handle nonlinearities, state-of-the-art Branch-and-Cut solvers employ linear relaxations, leading to exponential growth in auxiliary variables and severe computation limitations. To overcome these limitations, we propose BIPNN (Binary Integer Programming Neural Network), an unsupervised learning framework to solve nonlinear BIP problems via hypergraph neural networks (HyperGNN). Specifically, BIPNN reformulates BIPs-constrained, discrete, and nonlinear (sin, log, exp) optimization problems-into unconstrained, differentiable, and polynomial loss functions. The reformulation stems from the observation of a precise one-to-one mapping between polynomial BIP objectives and hypergraph structures, enabling the unsupervised training of HyperGNN to optimize BIP problems in an end-to-end manner. On this basis, we propose a GPU-accelerated and continuous-annealing-enhanced training pipeline for BIPNN. The pipeline enables BIPNN to optimize large-scale nonlinear terms in BIPs fully in parallel via straightforward gradient descent, thus significantly reducing the training cost while ensuring the generation of discrete, high-quality solutions. Extensive experiments on synthetic and real-world datasets highlight the superiority of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Unbiased Sampling from Boltzmann Distributions via Variance-Tuned Diffusion Models</title>
<link>https://arxiv.org/abs/2505.21005</link>
<guid>https://arxiv.org/abs/2505.21005</guid>
<content:encoded><![CDATA[
arXiv:2505.21005v1 Announce Type: new 
Abstract: Score-based diffusion models (SBDMs) are powerful amortized samplers for Boltzmann distributions; however, imperfect score estimates bias downstream Monte Carlo estimates. Classical importance sampling (IS) can correct this bias, but computing exact likelihoods requires solving the probability-flow ordinary differential equation (PF-ODE), a procedure that is prohibitively costly and scales poorly with dimensionality. We introduce Variance-Tuned Diffusion Importance Sampling (VT-DIS), a lightweight post-training method that adapts the per-step noise covariance of a pretrained SBDM by minimizing the $\alpha$-divergence ($\alpha=2$) between its forward diffusion and reverse denoising trajectories. VT-DIS assigns a single trajectory-wise importance weight to the joint forward-reverse process, yielding unbiased expectation estimates at test time with negligible overhead compared to standard sampling. On the DW-4, LJ-13, and alanine-dipeptide benchmarks, VT-DIS achieves effective sample sizes of approximately 80 %, 35 %, and 3.5 %, respectively, while using only a fraction of the computational budget required by vanilla diffusion + IS or PF-ODE-based IS.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Instrumental Variable Analysis via Federated Generalized Method of Moments</title>
<link>https://arxiv.org/abs/2505.21012</link>
<guid>https://arxiv.org/abs/2505.21012</guid>
<content:encoded><![CDATA[
arXiv:2505.21012v1 Announce Type: new 
Abstract: Instrumental variables (IV) analysis is an important applied tool for areas such as healthcare and consumer economics. For IV analysis in high-dimensional settings, the Generalized Method of Moments (GMM) using deep neural networks offers an efficient approach. With non-i.i.d. data sourced from scattered decentralized clients, federated learning is a popular paradigm for training the models while promising data privacy. However, to our knowledge, no federated algorithm for either GMM or IV analysis exists to date. In this work, we introduce federated instrumental variables analysis (FedIV) via federated generalized method of moments (FedGMM). We formulate FedGMM as a federated zero-sum game defined by a federated non-convex non-concave minimax optimization problem, which is solved using federated gradient descent ascent (FedGDA) algorithm. One key challenge arises in theoretically characterizing the federated local optimality. To address this, we present properties and existence results of clients' local equilibria via FedGDA limit points. Thereby, we show that the federated solution consistently estimates the local moment conditions of every participating client. The proposed algorithm is backed by extensive experiments to demonstrate the efficacy of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NeuralOM: Neural Ocean Model for Subseasonal-to-Seasonal Simulation</title>
<link>https://arxiv.org/abs/2505.21020</link>
<guid>https://arxiv.org/abs/2505.21020</guid>
<content:encoded><![CDATA[
arXiv:2505.21020v1 Announce Type: new 
Abstract: Accurate Subseasonal-to-Seasonal (S2S) ocean simulation is critically important for marine research, yet remains challenging due to its substantial thermal inertia and extended time delay. Machine learning (ML)-based models have demonstrated significant advancements in simulation accuracy and computational efficiency compared to traditional numerical methods. Nevertheless, a significant limitation of current ML models for S2S ocean simulation is their inadequate incorporation of physical consistency and the slow-changing properties of the ocean system. In this work, we propose a neural ocean model (NeuralOM) for S2S ocean simulation with a multi-scale interactive graph neural network to emulate diverse physical phenomena associated with ocean systems effectively. Specifically, we propose a multi-stage framework tailored to model the ocean's slowly changing nature. Additionally, we introduce a multi-scale interactive messaging module to capture complex dynamical behaviors, such as gradient changes and multiplicative coupling relationships inherent in ocean dynamics. Extensive experimental evaluations confirm that our proposed NeuralOM outperforms state-of-the-art models in S2S and extreme event simulation. The codes are available at https://github.com/YuanGao-YG/NeuralOM.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pause Tokens Strictly Increase the Expressivity of Constant-Depth Transformers</title>
<link>https://arxiv.org/abs/2505.21024</link>
<guid>https://arxiv.org/abs/2505.21024</guid>
<content:encoded><![CDATA[
arXiv:2505.21024v1 Announce Type: new 
Abstract: Pause tokens, simple filler symbols such as "...", consistently improve Transformer performance on both language and mathematical tasks, yet their theoretical effect remains unexplained. We provide the first formal separation result, proving that adding pause tokens to constant-depth, logarithmic-width Transformers strictly increases their computational expressivity. With bounded-precision activations, Transformers without pause tokens compute only a strict subset of $\mathsf{AC}^0$ functions, while adding a polynomial number of pause tokens allows them to express the entire class. For logarithmic-precision Transformers, we show that adding pause tokens achieves expressivity equivalent to $\mathsf{TC}^0$, matching known upper bounds. Empirically, we demonstrate that two-layer causally masked Transformers can learn parity when supplied with pause tokens, a function that they appear unable to learn without them. Our results provide a rigorous theoretical explanation for prior empirical findings, clarify how pause tokens interact with width, depth, and numeric precision, and position them as a distinct mechanism, complementary to chain-of-thought prompting, for enhancing Transformer reasoning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TabAttackBench: A Benchmark for Adversarial Attacks on Tabular Data</title>
<link>https://arxiv.org/abs/2505.21027</link>
<guid>https://arxiv.org/abs/2505.21027</guid>
<content:encoded><![CDATA[
arXiv:2505.21027v1 Announce Type: new 
Abstract: Adversarial attacks pose a significant threat to machine learning models by inducing incorrect predictions through imperceptible perturbations to input data. While these attacks have been extensively studied in unstructured data like images, their application to tabular data presents new challenges. These challenges arise from the inherent heterogeneity and complex feature interdependencies in tabular data, which differ significantly from those in image data. To address these differences, it is crucial to consider imperceptibility as a key criterion specific to tabular data. Most current research focuses primarily on achieving effective adversarial attacks, often overlooking the importance of maintaining imperceptibility. To address this gap, we propose a new benchmark for adversarial attacks on tabular data that evaluates both effectiveness and imperceptibility. In this study, we assess the effectiveness and imperceptibility of five adversarial attacks across four models using eleven tabular datasets, including both mixed and numerical-only datasets. Our analysis explores how these factors interact and influence the overall performance of the attacks. We also compare the results across different dataset types to understand the broader implications of these findings. The findings from this benchmark provide valuable insights for improving the design of adversarial attack algorithms, thereby advancing the field of adversarial machine learning on tabular data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaMEA-BO: A Large Language Model Evolutionary Algorithm for Automatically Generating Bayesian Optimization Algorithms</title>
<link>https://arxiv.org/abs/2505.21034</link>
<guid>https://arxiv.org/abs/2505.21034</guid>
<content:encoded><![CDATA[
arXiv:2505.21034v1 Announce Type: new 
Abstract: Bayesian optimization (BO) is a powerful class of algorithms for optimizing expensive black-box functions, but designing effective BO algorithms remains a manual, expertise-driven task. Recent advancements in Large Language Models (LLMs) have opened new avenues for automating scientific discovery, including the automatic design of optimization algorithms. While prior work has used LLMs within optimization loops or to generate non-BO algorithms, we tackle a new challenge: Using LLMs to automatically generate full BO algorithm code. Our framework uses an evolution strategy to guide an LLM in generating Python code that preserves the key components of BO algorithms: An initial design, a surrogate model, and an acquisition function. The LLM is prompted to produce multiple candidate algorithms, which are evaluated on the established Black-Box Optimization Benchmarking (BBOB) test suite from the COmparing Continuous Optimizers (COCO) platform. Based on their performance, top candidates are selected, combined, and mutated via controlled prompt variations, enabling iterative refinement. Despite no additional fine-tuning, the LLM-generated algorithms outperform state-of-the-art BO baselines in 19 (out of 24) BBOB functions in dimension 5 and generalize well to higher dimensions, and different tasks (from the Bayesmark framework). This work demonstrates that LLMs can serve as algorithmic co-designers, offering a new paradigm for automating BO development and accelerating the discovery of novel algorithmic combinations. The source code is provided at https://github.com/Ewendawi/LLaMEA-BO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable and adaptive prediction bands with kernel sum-of-squares</title>
<link>https://arxiv.org/abs/2505.21039</link>
<guid>https://arxiv.org/abs/2505.21039</guid>
<content:encoded><![CDATA[
arXiv:2505.21039v1 Announce Type: new 
Abstract: Conformal Prediction (CP) is a popular framework for constructing prediction bands with valid coverage in finite samples, while being free of any distributional assumption. A well-known limitation of conformal prediction is the lack of adaptivity, although several works introduced practically efficient alternate procedures. In this work, we build upon recent ideas that rely on recasting the CP problem as a statistical learning problem, directly targeting coverage and adaptivity. This statistical learning problem is based on reproducible kernel Hilbert spaces (RKHS) and kernel sum-of-squares (SoS) methods. First, we extend previous results with a general representer theorem and exhibit the dual formulation of the learning problem. Crucially, such dual formulation can be solved efficiently by accelerated gradient methods with several hundreds or thousands of samples, unlike previous strategies based on off-the-shelf semidefinite programming algorithms. Second, we introduce a new hyperparameter tuning strategy tailored specifically to target adaptivity through bounds on test-conditional coverage. This strategy, based on the Hilbert-Schmidt Independence Criterion (HSIC), is introduced here to tune kernel lengthscales in our framework, but has broader applicability since it could be used in any CP algorithm where the score function is learned. Finally, extensive experiments are conducted to show how our method compares to related work. All figures can be reproduced with the accompanying code.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A domain adaptation neural network for digital twin-supported fault diagnosis</title>
<link>https://arxiv.org/abs/2505.21046</link>
<guid>https://arxiv.org/abs/2505.21046</guid>
<content:encoded><![CDATA[
arXiv:2505.21046v1 Announce Type: new 
Abstract: Digital twins offer a promising solution to the lack of sufficient labeled data in deep learning-based fault diagnosis by generating simulated data for model training. However, discrepancies between simulation and real-world systems can lead to a significant drop in performance when models are applied in real scenarios. To address this issue, we propose a fault diagnosis framework based on Domain-Adversarial Neural Networks (DANN), which enables knowledge transfer from simulated (source domain) to real-world (target domain) data. We evaluate the proposed framework using a publicly available robotics fault diagnosis dataset, which includes 3,600 sequences generated by a digital twin model and 90 real sequences collected from physical systems. The DANN method is compared with commonly used lightweight deep learning models such as CNN, TCN, Transformer, and LSTM. Experimental results show that incorporating domain adaptation significantly improves the diagnostic performance. For example, applying DANN to a baseline CNN model improves its accuracy from 70.00% to 80.22% on real-world test data, demonstrating the effectiveness of domain adaptation in bridging the sim-to-real gap.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bridging Arbitrary and Tree Metrics via Differentiable Gromov Hyperbolicity</title>
<link>https://arxiv.org/abs/2505.21073</link>
<guid>https://arxiv.org/abs/2505.21073</guid>
<content:encoded><![CDATA[
arXiv:2505.21073v1 Announce Type: new 
Abstract: Trees and the associated shortest-path tree metrics provide a powerful framework for representing hierarchical and combinatorial structures in data. Given an arbitrary metric space, its deviation from a tree metric can be quantified by Gromov's $\delta$-hyperbolicity. Nonetheless, designing algorithms that bridge an arbitrary metric to its closest tree metric is still a vivid subject of interest, as most common approaches are either heuristical and lack guarantees, or perform moderately well. In this work, we introduce a novel differentiable optimization framework, coined DeltaZero, that solves this problem. Our method leverages a smooth surrogate for Gromov's $\delta$-hyperbolicity which enables a gradient-based optimization, with a tractable complexity. The corresponding optimization procedure is derived from a problem with better worst case guarantees than existing bounds, and is justified statistically. Experiments on synthetic and real-world datasets demonstrate that our method consistently achieves state-of-the-art distortion.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Red-Teaming Text-to-Image Systems by Rule-based Preference Modeling</title>
<link>https://arxiv.org/abs/2505.21074</link>
<guid>https://arxiv.org/abs/2505.21074</guid>
<content:encoded><![CDATA[
arXiv:2505.21074v1 Announce Type: new 
Abstract: Text-to-image (T2I) models raise ethical and safety concerns due to their potential to generate inappropriate or harmful images. Evaluating these models' security through red-teaming is vital, yet white-box approaches are limited by their need for internal access, complicating their use with closed-source models. Moreover, existing black-box methods often assume knowledge about the model's specific defense mechanisms, limiting their utility in real-world commercial API scenarios. A significant challenge is how to evade unknown and diverse defense mechanisms. To overcome this difficulty, we propose a novel Rule-based Preference modeling Guided Red-Teaming (RPG-RT), which iteratively employs LLM to modify prompts to query and leverages feedback from T2I systems for fine-tuning the LLM. RPG-RT treats the feedback from each iteration as a prior, enabling the LLM to dynamically adapt to unknown defense mechanisms. Given that the feedback is often labeled and coarse-grained, making it difficult to utilize directly, we further propose rule-based preference modeling, which employs a set of rules to evaluate desired or undesired feedback, facilitating finer-grained control over the LLM's dynamic adaptation process. Extensive experiments on nineteen T2I systems with varied safety mechanisms, three online commercial API services, and T2V models verify the superiority and practicality of our approach.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient Large Language Model Inference with Neural Block Linearization</title>
<link>https://arxiv.org/abs/2505.21077</link>
<guid>https://arxiv.org/abs/2505.21077</guid>
<content:encoded><![CDATA[
arXiv:2505.21077v1 Announce Type: new 
Abstract: The high inference demands of transformer-based Large Language Models (LLMs) pose substantial challenges in their deployment. To this end, we introduce Neural Block Linearization (NBL), a novel framework for accelerating transformer model inference by replacing self-attention layers with linear approximations derived from Linear Minimum Mean Squared Error estimators. NBL leverages Canonical Correlation Analysis to compute a theoretical upper bound on the approximation error. Then, we use this bound as a criterion for substitution, selecting the LLM layers with the lowest linearization error. NBL can be efficiently applied to pre-trained LLMs without the need for fine-tuning. In experiments, NBL achieves notable computational speed-ups while preserving competitive accuracy on multiple reasoning benchmarks. For instance, applying NBL to 12 self-attention layers in DeepSeek-R1-Distill-Llama-8B increases the inference speed by 32% with less than 1% accuracy trade-off, making it a flexible and promising solution to improve the inference efficiency of LLMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Impossible Tuning and Lipschitz-Adaptive Universal Online Learning with Gradient Variations</title>
<link>https://arxiv.org/abs/2505.21095</link>
<guid>https://arxiv.org/abs/2505.21095</guid>
<content:encoded><![CDATA[
arXiv:2505.21095v1 Announce Type: new 
Abstract: A central goal in online learning is to achieve adaptivity to unknown problem characteristics, such as environmental changes captured by gradient variation (GV), function curvature (universal online learning, UOL), and gradient scales (Lipschitz adaptivity, LA). Simultaneously achieving these with optimal performance is a major challenge, partly due to limitations in algorithms for prediction with expert advice. These algorithms often serve as meta-algorithms in online ensemble frameworks, and their sub-optimality hinders overall UOL performance. Specifically, existing algorithms addressing the ``impossible tuning'' issue incur an excess $\sqrt{\log T}$ factor in their regret bound compared to the lower bound. To solve this problem, we propose a novel optimistic online mirror descent algorithm with an auxiliary initial round using large learning rates. This design enables a refined analysis where a generated negative term cancels the gap-related factor, resolving the impossible tuning issue up to $\log\log T$ factors. Leveraging our improved algorithm as a meta-algorithm, we develop the first UOL algorithm that simultaneously achieves state-of-the-art GV bounds and LA under standard assumptions. Our UOL result overcomes key limitations of prior works, notably resolving the conflict between LA mechanisms and regret analysis for GV bounds -- an open problem highlighted by Xie et al.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Diffusion Models with Classifier-Free Gibbs-like Guidance</title>
<link>https://arxiv.org/abs/2505.21101</link>
<guid>https://arxiv.org/abs/2505.21101</guid>
<content:encoded><![CDATA[
arXiv:2505.21101v1 Announce Type: new 
Abstract: Classifier-Free Guidance (CFG) is a widely used technique for improving conditional diffusion models by linearly combining the outputs of conditional and unconditional denoisers. While CFG enhances visual quality and improves alignment with prompts, it often reduces sample diversity, leading to a challenging trade-off between quality and diversity. To address this issue, we make two key contributions. First, CFG generally does not correspond to a well-defined denoising diffusion model (DDM). In particular, contrary to common intuition, CFG does not yield samples from the target distribution associated with the limiting CFG score as the noise level approaches zero -- where the data distribution is tilted by a power $w \gt 1$ of the conditional distribution. We identify the missing component: a R\'enyi divergence term that acts as a repulsive force and is required to correct CFG and render it consistent with a proper DDM. Our analysis shows that this correction term vanishes in the low-noise limit. Second, motivated by this insight, we propose a Gibbs-like sampling procedure to draw samples from the desired tilted distribution. This method starts with an initial sample from the conditional diffusion model without CFG and iteratively refines it, preserving diversity while progressively enhancing sample quality. We evaluate our approach on both image and text-to-audio generation tasks, demonstrating substantial improvements over CFG across all considered metrics. The code is available at https://github.com/yazidjanati/cfgig
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Universal Value-Function Uncertainties</title>
<link>https://arxiv.org/abs/2505.21119</link>
<guid>https://arxiv.org/abs/2505.21119</guid>
<content:encoded><![CDATA[
arXiv:2505.21119v1 Announce Type: new 
Abstract: Estimating epistemic uncertainty in value functions is a crucial challenge for many aspects of reinforcement learning (RL), including efficient exploration, safe decision-making, and offline RL. While deep ensembles provide a robust method for quantifying value uncertainty, they come with significant computational overhead. Single-model methods, while computationally favorable, often rely on heuristics and typically require additional propagation mechanisms for myopic uncertainty estimates. In this work we introduce universal value-function uncertainties (UVU), which, similar in spirit to random network distillation (RND), quantify uncertainty as squared prediction errors between an online learner and a fixed, randomly initialized target network. Unlike RND, UVU errors reflect policy-conditional value uncertainty, incorporating the future uncertainties any given policy may encounter. This is due to the training procedure employed in UVU: the online network is trained using temporal difference learning with a synthetic reward derived from the fixed, randomly initialized target network. We provide an extensive theoretical analysis of our approach using neural tangent kernel (NTK) theory and show that in the limit of infinite network width, UVU errors are exactly equivalent to the variance of an ensemble of independent universal value functions. Empirically, we show that UVU achieves equal performance to large ensembles on challenging multi-task offline RL settings, while offering simplicity and substantial computational savings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust and Computation-Aware Gaussian Processes</title>
<link>https://arxiv.org/abs/2505.21133</link>
<guid>https://arxiv.org/abs/2505.21133</guid>
<content:encoded><![CDATA[
arXiv:2505.21133v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are widely used for regression and optimization tasks such as Bayesian optimization (BO) due to their expressiveness and principled uncertainty estimates. However, in settings with large datasets corrupted by outliers, standard GPs and their sparse approximations struggle with computational tractability and robustness. We introduce Robust Computation-aware Gaussian Process (RCaGP), a novel GP model that jointly addresses these challenges by combining a principled treatment of approximation-induced uncertainty with robust generalized Bayesian updating. The key insight is that robustness and approximation-awareness are not orthogonal but intertwined: approximations can exacerbate the impact of outliers, and mitigating one without the other is insufficient. Unlike previous work that focuses narrowly on either robustness or approximation quality, RCaGP combines both in a principled and scalable framework, thus effectively managing both outliers and computational uncertainties introduced by approximations such as low-rank matrix multiplications. Our model ensures more conservative and reliable uncertainty estimates, a property we rigorously demonstrate. Additionally, we establish a robustness property and show that the mean function is key to preserving it, motivating a tailored model selection scheme for robust mean functions. Empirical results confirm that solving these challenges jointly leads to superior performance across both clean and outlier-contaminated settings, both on regression and high-throughput Bayesian optimization benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Single Index Models with Diffusion Priors</title>
<link>https://arxiv.org/abs/2505.21135</link>
<guid>https://arxiv.org/abs/2505.21135</guid>
<content:encoded><![CDATA[
arXiv:2505.21135v1 Announce Type: new 
Abstract: Diffusion models (DMs) have demonstrated remarkable ability to generate diverse and high-quality images by efficiently modeling complex data distributions. They have also been explored as powerful generative priors for signal recovery, resulting in a substantial improvement in the quality of reconstructed signals. However, existing research on signal recovery with diffusion models either focuses on specific reconstruction problems or is unable to handle nonlinear measurement models with discontinuous or unknown link functions. In this work, we focus on using DMs to achieve accurate recovery from semi-parametric single index models, which encompass a variety of popular nonlinear models that may have {\em discontinuous} and {\em unknown} link functions. We propose an efficient reconstruction method that only requires one round of unconditional sampling and (partial) inversion of DMs. Theoretical analysis on the effectiveness of the proposed methods has been established under appropriate conditions. We perform numerical experiments on image datasets for different nonlinear measurement models. We observe that compared to competing methods, our approach can yield more accurate reconstructions while utilizing significantly fewer neural function evaluations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SageAttention2++: A More Efficient Implementation of SageAttention2</title>
<link>https://arxiv.org/abs/2505.21136</link>
<guid>https://arxiv.org/abs/2505.21136</guid>
<content:encoded><![CDATA[
arXiv:2505.21136v1 Announce Type: new 
Abstract: The efficiency of attention is critical because its time complexity grows quadratically with sequence length. SageAttention2 addresses this by utilizing quantization to accelerate matrix multiplications (Matmul) in attention. To further accelerate SageAttention2, we propose to utilize the faster instruction of FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8 Matmul used in SageAttention2. Our experiments show that SageAttention2++ achieves a 3.9x speedup over FlashAttention while maintaining the same attention accuracy as SageAttention2. This means SageAttention2++ effectively accelerates various models, including those for language, image, and video generation, with negligible end-to-end metrics loss. The code will be available at https://github.com/thu-ml/SageAttention.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HeteroBA: A Structure-Manipulating Backdoor Attack on Heterogeneous Graphs</title>
<link>https://arxiv.org/abs/2505.21140</link>
<guid>https://arxiv.org/abs/2505.21140</guid>
<content:encoded><![CDATA[
arXiv:2505.21140v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) have recently drawn increasing attention for modeling complex multi-relational data in domains such as recommendation, finance, and social networks. While existing research has been largely focused on enhancing HGNNs' predictive performance, their robustness and security, especially under backdoor attacks, remain underexplored. In this paper, we propose a novel Heterogeneous Backdoor Attack (HeteroBA) framework for node classification tasks on heterogeneous graphs. HeteroBA inserts carefully crafted trigger nodes with realistic features and targeted structural connections, leveraging attention-based and clustering-based strategies to select influential auxiliary nodes for effective trigger propagation, thereby causing the model to misclassify specific nodes into a target label while maintaining accuracy on clean data. Experimental results on three datasets and various HGNN architectures demonstrate that HeteroBA achieves high attack success rates with minimal impact on the clean accuracy. Our method sheds light on potential vulnerabilities in HGNNs and calls for more robust defenses against backdoor threats in multi-relational graph scenarios.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Predicting Phishing Websites Using Support Vector Machine and MultiClass Classification Based on Association Rule Techniques</title>
<link>https://arxiv.org/abs/2505.21141</link>
<guid>https://arxiv.org/abs/2505.21141</guid>
<content:encoded><![CDATA[
arXiv:2505.21141v1 Announce Type: new 
Abstract: Phishing is a semantic attack which targets the user rather than the computer. It is a new Internet crime in comparison with other forms such as virus and hacking. Considering the damage phishing websites has caused to various economies by collapsing organizations, stealing information and financial diversion, various researchers have embarked on different ways of detecting phishing websites but there has been no agreement about the best algorithm to be used for prediction. This study is interested in integrating the strengths of two algorithms, Support Vector Machines (SVM) and Multi-Class Classification Rules based on Association Rules (MCAR) to establish a strong and better means of predicting phishing websites. A total of 11,056 websites were used from both PhishTank and yahoo directory to verify the effectiveness of this approach. Feature extraction and rules generation were done by the MCAR technique; classification and prediction were done by SVM technique. The result showed that the technique achieved 98.30% classification accuracy with a computation time of 2205.33s with minimum error rate. It showed a total of 98% Area under the Curve (AUC) which showed the proportion of accuracy in classifying phishing websites. The model showed 82.84% variance in the prediction of phishing websites based on the coefficient of determination. The use of two techniques together in detecting phishing websites produced a more accurate result as it combined the strength of both techniques respectively. This research work centralized on this advantage by building a hybrid of two techniques to help produce a more accurate result.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Conformal Prediction With Unlabeled Nonconformity Score</title>
<link>https://arxiv.org/abs/2505.21147</link>
<guid>https://arxiv.org/abs/2505.21147</guid>
<content:encoded><![CDATA[
arXiv:2505.21147v1 Announce Type: new 
Abstract: Conformal prediction (CP) is a powerful framework for uncertainty quantification, providing prediction sets with coverage guarantees when calibrated on sufficient labeled data. However, in real-world applications where labeled data is often limited, standard CP can lead to coverage deviation and output overly large prediction sets. In this paper, we extend CP to the semi-supervised setting and propose SemiCP, leveraging both labeled data and unlabeled data for calibration. Specifically, we introduce a novel nonconformity score function, NNM, designed for unlabeled data. This function selects labeled data with similar pseudo-label scores to estimate nonconformity scores, integrating them into the calibration process to overcome sample size limitations. We theoretically demonstrate that, under mild assumptions, SemiCP provide asymptotically coverage guarantee for prediction sets. Extensive experiments further validate that our approach effectively reduces instability and inefficiency under limited calibration data, can be adapted to conditional coverage settings, and integrates seamlessly with existing CP methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STEB: In Search of the Best Evaluation Approach for Synthetic Time Series</title>
<link>https://arxiv.org/abs/2505.21160</link>
<guid>https://arxiv.org/abs/2505.21160</guid>
<content:encoded><![CDATA[
arXiv:2505.21160v1 Announce Type: new 
Abstract: The growing need for synthetic time series, due to data augmentation or privacy regulations, has led to numerous generative models, frameworks, and evaluation measures alike. Objectively comparing these measures on a large scale remains an open challenge. We propose the Synthetic Time series Evaluation Benchmark (STEB) -- the first benchmark framework that enables comprehensive and interpretable automated comparisons of synthetic time series evaluation measures. Using 10 diverse datasets, randomness injection, and 13 configurable data transformations, STEB computes indicators for measure reliability and score consistency. It tracks running time, test errors, and features sequential and parallel modes of operation. In our experiments, we determine a ranking of 41 measures from literature and confirm that the choice of upstream time series embedding heavily impacts the final score.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Topological Deep Learning for Speech Data</title>
<link>https://arxiv.org/abs/2505.21173</link>
<guid>https://arxiv.org/abs/2505.21173</guid>
<content:encoded><![CDATA[
arXiv:2505.21173v1 Announce Type: new 
Abstract: Topological data analysis (TDA) offers novel mathematical tools for deep learning. Inspired by Carlsson et al., this study designs topology-aware convolutional kernels that significantly improve speech recognition networks. Theoretically, by investigating orthogonal group actions on kernels, we establish a fiber-bundle decomposition of matrix spaces, enabling new filter generation methods. Practically, our proposed Orthogonal Feature (OF) layer achieves superior performance in phoneme recognition, particularly in low-noise scenarios, while demonstrating cross-domain adaptability. This work reveals TDA's potential in neural network optimization, opening new avenues for mathematics-deep learning interdisciplinary studies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent label distribution grid representation for modeling uncertainty</title>
<link>https://arxiv.org/abs/2505.21180</link>
<guid>https://arxiv.org/abs/2505.21180</guid>
<content:encoded><![CDATA[
arXiv:2505.21180v1 Announce Type: new 
Abstract: Although \textbf{L}abel \textbf{D}istribution \textbf{L}earning (LDL) has promising representation capabilities for characterizing the polysemy of an instance, the complexity and high cost of the label distribution annotation lead to inexact in the construction of the label space. The existence of a large number of inexact labels generates a label space with uncertainty, which misleads the LDL algorithm to yield incorrect decisions. To alleviate this problem, we model the uncertainty of label distributions by constructing a \textbf{L}atent \textbf{L}abel \textbf{D}istribution \textbf{G}rid (LLDG) to form a low-noise representation space. Specifically, we first construct a label correlation matrix based on the differences between labels, and then expand each value of the matrix into a vector that obeys a Gaussian distribution, thus building a LLDG to model the uncertainty of the label space. Finally, the LLDG is reconstructed by the LLDG-Mixer to generate an accurate label distribution. Note that we enforce a customized low-rank scheme on this grid, which assumes that the label relations may be noisy and it needs to perform noise-reduction with the help of a Tucker reconstruction technique. Furthermore, we attempt to evaluate the effectiveness of the LLDG by considering its generation as an upstream task to achieve the classification of the objects. Extensive experimental results show that our approach performs competitively on several benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning What to Do and What Not To Do: Offline Imitation from Expert and Undesirable Demonstrations</title>
<link>https://arxiv.org/abs/2505.21182</link>
<guid>https://arxiv.org/abs/2505.21182</guid>
<content:encoded><![CDATA[
arXiv:2505.21182v1 Announce Type: new 
Abstract: Offline imitation learning typically learns from expert and unlabeled demonstrations, yet often overlooks the valuable signal in explicitly undesirable behaviors. In this work, we study offline imitation learning from contrasting behaviors, where the dataset contains both expert and undesirable demonstrations. We propose a novel formulation that optimizes a difference of KL divergences over the state-action visitation distributions of expert and undesirable (or bad) data. Although the resulting objective is a DC (Difference-of-Convex) program, we prove that it becomes convex when expert demonstrations outweigh undesirable demonstrations, enabling a practical and stable non-adversarial training objective. Our method avoids adversarial training and handles both positive and negative demonstrations in a unified framework. Extensive experiments on standard offline imitation learning benchmarks demonstrate that our approach consistently outperforms state-of-the-art baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PoisonSwarm: Universal Harmful Information Synthesis via Model Crowdsourcing</title>
<link>https://arxiv.org/abs/2505.21184</link>
<guid>https://arxiv.org/abs/2505.21184</guid>
<content:encoded><![CDATA[
arXiv:2505.21184v1 Announce Type: new 
Abstract: To construct responsible and secure AI applications, harmful information data is widely utilized for adversarial testing and the development of safeguards. Existing studies mainly leverage Large Language Models (LLMs) to synthesize data to obtain high-quality task datasets at scale, thereby avoiding costly human annotation. However, limited by the safety alignment mechanisms of LLMs, the synthesis of harmful data still faces challenges in generation reliability and content diversity. In this study, we propose a novel harmful information synthesis framework, PoisonSwarm, which applies the model crowdsourcing strategy to generate diverse harmful data while maintaining a high success rate. Specifically, we generate abundant benign data as the based templates in a counterfactual manner. Subsequently, we decompose each based template into multiple semantic units and perform unit-by-unit toxification and final refinement through dynamic model switching, thus ensuring the success of synthesis. Experimental results demonstrate that PoisonSwarm achieves state-of-the-art performance in synthesizing different categories of harmful data with high scalability and diversity.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Crop recommendation with machine learning: leveraging environmental and economic factors for optimal crop selection</title>
<link>https://arxiv.org/abs/2505.21201</link>
<guid>https://arxiv.org/abs/2505.21201</guid>
<content:encoded><![CDATA[
arXiv:2505.21201v1 Announce Type: new 
Abstract: Agriculture constitutes a primary source of food production, economic growth and employment in India, but the sector is confronted with low farm productivity and yields aggravated by increased pressure on natural resources and adverse climate change variability. Efforts involving green revolution, land irrigations, improved seeds and organic farming have yielded suboptimal outcomes. The adoption of computational tools like crop recommendation systems offers a new way to provide insights and help farmers tackle low productivity. However, most agricultural recommendation systems in India focus narrowly on environmental factors and regions, limiting accurate predictions of high-yield, profitable crops. This study uses environmental and economic factors with 19 crops across 15 states to develop and evaluate Random Forest and SVM models using 10-fold Cross Validation, Time-series Split, and Lag Variables. The 10-fold cross validation showed high accuracy (RF: 99.96%, SVM: 94.71%) but raised overfitting concerns. Introducing temporal order, better reflecting real-world conditions, reduced performance (RF: 78.55%, SVM: 71.18%) in the Time-series Split.To further increase the model accuracy while maintaining the temporal order, the Lag Variables approach was employed, which resulted in improved performance (RF: 83.62%, SVM: 74.38%) compared to the 10-fold cross validation approach. Overall, the models in the Time-series Split and Lag Variable Approaches offer practical insights by handling temporal dependencies and enhancing its adaptability to changing agricultural conditions over time. Consequently, the study shows the Random Forest model developed based on the Lag Variables as the most preferred algorithm for optimal crop recommendation in the Indian context.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Developing hybrid mechanistic and data-driven personalized prediction models for platelet dynamics</title>
<link>https://arxiv.org/abs/2505.21204</link>
<guid>https://arxiv.org/abs/2505.21204</guid>
<content:encoded><![CDATA[
arXiv:2505.21204v1 Announce Type: new 
Abstract: Hematotoxicity, drug-induced damage to the blood-forming system, is a frequent side effect of cytotoxic chemotherapy and poses a significant challenge in clinical practice due to its high inter-patient variability and limited predictability. Current mechanistic models often struggle to accurately forecast outcomes for patients with irregular or atypical trajectories. In this study, we develop and compare hybrid mechanistic and data-driven approaches for individualized time series modeling of platelet counts during chemotherapy. We consider hybrid models that combine mechanistic models with neural networks, known as universal differential equations. As a purely data-driven alternative, we utilize a nonlinear autoregressive exogenous model using gated recurrent units as the underlying architecture. These models are evaluated across a range of real patient scenarios, varying in data availability and sparsity, to assess predictive performance. Our findings demonstrate that data-driven methods, when provided with sufficient data, significantly improve prediction accuracy, particularly for high-risk patients with irregular platelet dynamics. This highlights the potential of data-driven approaches in enhancing clinical decision-making. In contrast, hybrid and mechanistic models are superior in scenarios with limited or sparse data. The proposed modeling and comparison framework is generalizable and could be extended to predict other treatment-related toxicities, offering broad applicability in personalized medicine.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Addressing Data Quality Decompensation in Federated Learning via Dynamic Client Selection</title>
<link>https://arxiv.org/abs/2505.21219</link>
<guid>https://arxiv.org/abs/2505.21219</guid>
<content:encoded><![CDATA[
arXiv:2505.21219v1 Announce Type: new 
Abstract: In cross-silo Federated Learning (FL), client selection is critical to ensure high model performance, yet it remains challenging due to data quality decompensation, budget constraints, and incentive compatibility. As training progresses, these factors exacerbate client heterogeneity and degrade global performance. Most existing approaches treat these challenges in isolation, making jointly optimizing multiple factors difficult. To address this, we propose Shapley-Bid Reputation Optimized Federated Learning (SBRO-FL), a unified framework integrating dynamic bidding, reputation modeling, and cost-aware selection. Clients submit bids based on their perceived data quality, and their contributions are evaluated using Shapley values to quantify their marginal impact on the global model. A reputation system, inspired by prospect theory, captures historical performance while penalizing inconsistency. The client selection problem is formulated as a 0-1 integer program that maximizes reputation-weighted utility under budget constraints. Experiments on FashionMNIST, EMNIST, CIFAR-10, and SVHN datasets show that SBRO-FL improves accuracy, convergence speed, and robustness, even in adversarial and low-bid interference scenarios. Our results highlight the importance of balancing data reliability, incentive compatibility, and cost efficiency to enable scalable and trustworthy FL deployments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Why Do More Experts Fail? A Theoretical Analysis of Model Merging</title>
<link>https://arxiv.org/abs/2505.21226</link>
<guid>https://arxiv.org/abs/2505.21226</guid>
<content:encoded><![CDATA[
arXiv:2505.21226v1 Announce Type: new 
Abstract: Model merging dramatically reduces storage and computational resources by combining multiple expert models into a single multi-task model. Although recent model merging methods have shown promising results, they struggle to maintain performance gains as the number of merged models increases. In this paper, we investigate the key obstacles that limit the scalability of model merging when integrating a large number of expert models. First, we prove that there is an upper bound on model merging. Further theoretical analysis reveals that the limited effective parameter space imposes a strict constraint on the number of models that can be successfully merged. Gaussian Width shows that the marginal benefit of merging additional models diminishes according to a strictly concave function. This implies that the effective parameter space becomes rapidly saturated as the number of merged models increases. Furthermore, using Approximate Kinematics Theory, we prove the existence of a unique optimal threshold beyond which adding more models does not yield significant performance improvements. At the same time, we introduce a straightforward Reparameterized Heavy-Tailed method (RHT) to extend the coverage of the merged model, thereby enhancing its performance. Empirical results on 12 benchmarks, including both knowledge-intensive and general-purpose tasks, validate our theoretical analysis. We believe that these results spark further research beyond the current scope of model merging. The source code is in the anonymous Github repository https://github.com/wzj1718/ModelMergingAnalysis.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking the Performance Ceiling in Complex Reinforcement Learning requires Inference Strategies</title>
<link>https://arxiv.org/abs/2505.21236</link>
<guid>https://arxiv.org/abs/2505.21236</guid>
<content:encoded><![CDATA[
arXiv:2505.21236v1 Announce Type: new 
Abstract: Reinforcement learning (RL) systems have countless applications, from energy-grid management to protein design. However, such real-world scenarios are often extremely difficult, combinatorial in nature, and require complex coordination between multiple agents. This level of complexity can cause even state-of-the-art RL systems, trained until convergence, to hit a performance ceiling which they are unable to break out of with zero-shot inference. Meanwhile, many digital or simulation-based applications allow for an inference phase that utilises a specific time and compute budget to explore multiple attempts before outputting a final solution. In this work, we show that such an inference phase employed at execution time, and the choice of a corresponding inference strategy, are key to breaking the performance ceiling observed in complex multi-agent RL problems. Our main result is striking: we can obtain up to a 126% and, on average, a 45% improvement over the previous state-of-the-art across 17 tasks, using only a couple seconds of extra wall-clock time during execution. We also demonstrate promising compute scaling properties, supported by over 60k experiments, making it the largest study on inference strategies for complex RL to date. Our experimental data and code are available at https://sites.google.com/view/inf-marl.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BindEnergyCraft: Casting Protein Structure Predictors as Energy-Based Models for Binder Design</title>
<link>https://arxiv.org/abs/2505.21241</link>
<guid>https://arxiv.org/abs/2505.21241</guid>
<content:encoded><![CDATA[
arXiv:2505.21241v1 Announce Type: new 
Abstract: Protein binder design has been transformed by hallucination-based methods that optimize structure prediction confidence metrics, such as the interface predicted TM-score (ipTM), via backpropagation. However, these metrics do not reflect the statistical likelihood of a binder-target complex under the learned distribution and yield sparse gradients for optimization. In this work, we propose a method to extract such likelihoods from structure predictors by reinterpreting their confidence outputs as an energy-based model (EBM). By leveraging the Joint Energy-based Modeling (JEM) framework, we introduce pTMEnergy, a statistical energy function derived from predicted inter-residue error distributions. We incorporate pTMEnergy into BindEnergyCraft (BECraft), a design pipeline that maintains the same optimization framework as BindCraft but replaces ipTM with our energy-based objective. BECraft outperforms BindCraft, RFDiffusion, and ESM3 across multiple challenging targets, achieving higher in silico binder success rates while reducing structural clashes. Furthermore, pTMEnergy establishes a new state-of-the-art in structure-based virtual screening tasks for miniprotein and RNA aptamer binders.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Copresheaf Topological Neural Networks: A Generalized Deep Learning Framework</title>
<link>https://arxiv.org/abs/2505.21251</link>
<guid>https://arxiv.org/abs/2505.21251</guid>
<content:encoded><![CDATA[
arXiv:2505.21251v1 Announce Type: new 
Abstract: We introduce copresheaf topological neural networks (CTNNs), a powerful and unifying framework that encapsulates a wide spectrum of deep learning architectures, designed to operate on structured data: including images, point clouds, graphs, meshes, and topological manifolds. While deep learning has profoundly impacted domains ranging from digital assistants to autonomous systems, the principled design of neural architectures tailored to specific tasks and data types remains one of the field's most persistent open challenges. CTNNs address this gap by grounding model design in the language of copresheaves, a concept from algebraic topology that generalizes and subsumes most practical deep learning models in use today. This abstract yet constructive formulation yields a rich design space from which theoretically sound and practically effective solutions can be derived to tackle core challenges in representation learning: long-range dependencies, oversmoothing, heterophily, and non-Euclidean domains. Our empirical results on structured data benchmarks demonstrate that CTNNs consistently outperform conventional baselines, particularly in tasks requiring hierarchical or localized sensitivity. These results underscore CTNNs as a principled, multi-scale foundation for the next generation of deep learning architectures.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learnable Kernel Density Estimation for Graphs</title>
<link>https://arxiv.org/abs/2505.21285</link>
<guid>https://arxiv.org/abs/2505.21285</guid>
<content:encoded><![CDATA[
arXiv:2505.21285v1 Announce Type: new 
Abstract: This work proposes a framework LGKDE that learns kernel density estimation for graphs. The key challenge in graph density estimation lies in effectively capturing both structural patterns and semantic variations while maintaining theoretical guarantees. Combining graph kernels and kernel density estimation (KDE) is a standard approach to graph density estimation, but has unsatisfactory performance due to the handcrafted and fixed features of kernels. Our method LGKDE leverages graph neural networks to represent each graph as a discrete distribution and utilizes maximum mean discrepancy to learn the graph metric for multi-scale KDE, where all parameters are learned by maximizing the density of graphs relative to the density of their well-designed perturbed counterparts. The perturbations are conducted on both node features and graph spectra, which helps better characterize the boundary of normal density regions. Theoretically, we establish consistency and convergence guarantees for LGKDE, including bounds on the mean integrated squared error, robustness, and complexity. We validate LGKDE by demonstrating its effectiveness in recovering the underlying density of synthetic graph distributions and applying it to graph anomaly detection across diverse benchmark datasets. Extensive empirical evaluation shows that LGKDE demonstrates superior performance compared to state-of-the-art baselines on most benchmark datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GSAT: Graph Structure Attention Networks</title>
<link>https://arxiv.org/abs/2505.21288</link>
<guid>https://arxiv.org/abs/2505.21288</guid>
<content:encoded><![CDATA[
arXiv:2505.21288v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have emerged as a powerful tool for processing data represented in graph structures, achieving remarkable success across a wide range of applications. However, to further improve the performance on graph classification benchmarks, structural representation of each node that encodes rich local topological information in the neighbourhood of nodes is an important type of feature that is often overlooked in the modeling. The consequence of neglecting the structural information has resulted high number of layers to connect messages from distant nodes which by itself produces other problems such as oversmoothing. In the present paper, we leverage these structural information that are modeled by anonymous random walks (ARWs) and introduce graph structure attention network (GSAT) which is a generalization of graph attention network(GAT) to integrate the original attribute and the structural representation to enforce the model to automatically find patterns for attending to different edges in the node neighbourhood to enrich graph representation. Our experiments show GSAT slightly improves SOTA on some graph classification benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoFT: Low-Rank Adaptation That Behaves Like Full Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.21289</link>
<guid>https://arxiv.org/abs/2505.21289</guid>
<content:encoded><![CDATA[
arXiv:2505.21289v1 Announce Type: new 
Abstract: Large pre-trained models are commonly adapted to downstream tasks using parameter-efficient fine-tuning methods such as Low-Rank Adaptation (LoRA), which injects small trainable low-rank matrices instead of updating all weights. While LoRA dramatically reduces trainable parameters with little overhead, it can still underperform full fine-tuning in accuracy and often converges more slowly. We introduce LoFT, a novel low-rank adaptation method that behaves like full fine-tuning by aligning the optimizer's internal dynamics with those of updating all model weights. LoFT not only learns weight updates in a low-rank subspace (like LoRA) but also properly projects the optimizer's first and second moments (Adam's momentum and variance) into the same subspace, mirroring full-model updates. By aligning the low-rank update itself with the full update, LoFT eliminates the need for tuning extra hyperparameters, e.g., LoRA scaling factor $\alpha$. Empirically, this approach substantially narrows the performance gap between adapter-based tuning and full fine-tuning and consistently outperforms standard LoRA-style methods, all without increasing inference cost.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Cross Modal Knowledge Distillation &amp; Data Augmentation Recipe for Improving Transcriptomics Representations through Morphological Features</title>
<link>https://arxiv.org/abs/2505.21317</link>
<guid>https://arxiv.org/abs/2505.21317</guid>
<content:encoded><![CDATA[
arXiv:2505.21317v1 Announce Type: new 
Abstract: Understanding cellular responses to stimuli is crucial for biological discovery and drug development. Transcriptomics provides interpretable, gene-level insights, while microscopy imaging offers rich predictive features but is harder to interpret. Weakly paired datasets, where samples share biological states, enable multimodal learning but are scarce, limiting their utility for training and multimodal inference. We propose a framework to enhance transcriptomics by distilling knowledge from microscopy images. Using weakly paired data, our method aligns and binds modalities, enriching gene expression representations with morphological information. To address data scarcity, we introduce (1) Semi-Clipped, an adaptation of CLIP for cross-modal distillation using pretrained foundation models, achieving state-of-the-art results, and (2) PEA (Perturbation Embedding Augmentation), a novel augmentation technique that enhances transcriptomics data while preserving inherent biological information. These strategies improve the predictive power and retain the interpretability of transcriptomics, enabling rich unimodal representations for complex biological tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bencher: Simple and Reproducible Benchmarking for Black-Box Optimization</title>
<link>https://arxiv.org/abs/2505.21321</link>
<guid>https://arxiv.org/abs/2505.21321</guid>
<content:encoded><![CDATA[
arXiv:2505.21321v1 Announce Type: new 
Abstract: We present Bencher, a modular benchmarking framework for black-box optimization that fundamentally decouples benchmark execution from optimization logic. Unlike prior suites that focus on combining many benchmarks in a single project, Bencher introduces a clean abstraction boundary: each benchmark is isolated in its own virtual Python environment and accessed via a unified, version-agnostic remote procedure call (RPC) interface. This design eliminates dependency conflicts and simplifies the integration of diverse, real-world benchmarks, which often have complex and conflicting software requirements. Bencher can be deployed locally or remotely via Docker or on high-performance computing (HPC) clusters via Singularity, providing a containerized, reproducible runtime for any benchmark. Its lightweight client requires minimal setup and supports drop-in evaluation of 80 benchmarks across continuous, categorical, and binary domains.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UGCE: User-Guided Incremental Counterfactual Exploration</title>
<link>https://arxiv.org/abs/2505.21330</link>
<guid>https://arxiv.org/abs/2505.21330</guid>
<content:encoded><![CDATA[
arXiv:2505.21330v1 Announce Type: new 
Abstract: Counterfactual explanations (CFEs) are a popular approach for interpreting machine learning predictions by identifying minimal feature changes that alter model outputs. However, in real-world settings, users often refine feasibility constraints over time, requiring counterfactual generation to adapt dynamically. Existing methods fail to support such iterative updates, instead recomputing explanations from scratch with each change, an inefficient and rigid approach. We propose User-Guided Incremental Counterfactual Exploration (UGCE), a genetic algorithm-based framework that incrementally updates counterfactuals in response to evolving user constraints. Experimental results across five benchmark datasets demonstrate that UGCE significantly improves computational efficiency while maintaining high-quality solutions compared to a static, non-incremental approach. Our evaluation further shows that UGCE supports stable performance under varying constraint sequences, benefits from an efficient warm-start strategy, and reveals how different constraint types may affect search behavior.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint Learning in the Gaussian Single Index Model</title>
<link>https://arxiv.org/abs/2505.21336</link>
<guid>https://arxiv.org/abs/2505.21336</guid>
<content:encoded><![CDATA[
arXiv:2505.21336v1 Announce Type: new 
Abstract: We consider the problem of jointly learning a one-dimensional projection and a univariate function in high-dimensional Gaussian models. Specifically, we study predictors of the form $f(x)=\varphi^\star(\langle w^\star, x \rangle)$, where both the direction $w^\star \in \mathcal{S}_{d-1}$, the sphere of $\mathbb{R}^d$, and the function $\varphi^\star: \mathbb{R} \to \mathbb{R}$ are learned from Gaussian data. This setting captures a fundamental non-convex problem at the intersection of representation learning and nonlinear regression. We analyze the gradient flow dynamics of a natural alternating scheme and prove convergence, with a rate controlled by the information exponent reflecting the \textit{Gaussian regularity} of the function $\varphi^\star$. Strikingly, our analysis shows that convergence still occurs even when the initial direction is negatively correlated with the target. On the practical side, we demonstrate that such joint learning can be effectively implemented using a Reproducing Kernel Hilbert Space (RKHS) adapted to the structure of the problem, enabling efficient and flexible estimation of the univariate function. Our results offer both theoretical insight and practical methodology for learning low-dimensional structure in high-dimensional settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Uncertainty-Aware ED-LSTM for Probabilistic Suffix Prediction</title>
<link>https://arxiv.org/abs/2505.21339</link>
<guid>https://arxiv.org/abs/2505.21339</guid>
<content:encoded><![CDATA[
arXiv:2505.21339v1 Announce Type: new 
Abstract: Suffix prediction of business processes forecasts the remaining sequence of events until process completion. Current approaches focus on predicting a single, most likely suffix. However, if the future course of a process is exposed to uncertainty or has high variability, the expressiveness of a single suffix prediction can be limited. To address this limitation, we propose probabilistic suffix prediction, a novel approach that approximates a probability distribution of suffixes. The proposed approach is based on an Uncertainty-Aware Encoder-Decoder LSTM (U-ED-LSTM) and a Monte Carlo (MC) suffix sampling algorithm. We capture epistemic uncertainties via MC dropout and aleatoric uncertainties as learned loss attenuation. This technical report provides a detailed evaluation of the U-ED-LSTM's predictive performance and assesses its calibration on four real-life event logs with three different hyperparameter settings. The results show that i) the U-ED-LSTM has reasonable predictive performance across various datasets, ii) aggregating probabilistic suffix predictions into mean values can outperform most likely predictions, particularly for rare prefixes or longer suffixes, and iii) the approach effectively captures uncertainties present in event logs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OVERT: A Benchmark for Over-Refusal Evaluation on Text-to-Image Models</title>
<link>https://arxiv.org/abs/2505.21347</link>
<guid>https://arxiv.org/abs/2505.21347</guid>
<content:encoded><![CDATA[
arXiv:2505.21347v1 Announce Type: new 
Abstract: Text-to-Image (T2I) models have achieved remarkable success in generating visual content from text inputs. Although multiple safety alignment strategies have been proposed to prevent harmful outputs, they often lead to overly cautious behavior -- rejecting even benign prompts -- a phenomenon known as $\textit{over-refusal}$ that reduces the practical utility of T2I models. Despite over-refusal having been observed in practice, there is no large-scale benchmark that systematically evaluates this phenomenon for T2I models. In this paper, we present an automatic workflow to construct synthetic evaluation data, resulting in OVERT ($\textbf{OVE}$r-$\textbf{R}$efusal evaluation on $\textbf{T}$ext-to-image models), the first large-scale benchmark for assessing over-refusal behaviors in T2I models. OVERT includes 4,600 seemingly harmful but benign prompts across nine safety-related categories, along with 1,785 genuinely harmful prompts (OVERT-unsafe) to evaluate the safety-utility trade-off. Using OVERT, we evaluate several leading T2I models and find that over-refusal is a widespread issue across various categories (Figure 1), underscoring the need for further research to enhance the safety alignment of T2I models without compromising their functionality.As a preliminary attempt to reduce over-refusal, we explore prompt rewriting; however, we find it often compromises faithfulness to the meaning of the original prompts. Finally, we demonstrate the flexibility of our generation framework in accommodating diverse safety requirements by generating customized evaluation data adapting to user-defined policies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRISP-NAM: Competing Risks Interpretable Survival Prediction with Neural Additive Models</title>
<link>https://arxiv.org/abs/2505.21360</link>
<guid>https://arxiv.org/abs/2505.21360</guid>
<content:encoded><![CDATA[
arXiv:2505.21360v1 Announce Type: new 
Abstract: Competing risks are crucial considerations in survival modelling, particularly in healthcare domains where patients may experience multiple distinct event types. We propose CRISP-NAM (Competing Risks Interpretable Survival Prediction with Neural Additive Models), an interpretable neural additive model for competing risks survival analysis which extends the neural additive architecture to model cause-specific hazards while preserving feature-level interpretability. Each feature contributes independently to risk estimation through dedicated neural networks, allowing for visualization of complex non-linear relationships between covariates and each competing risk. We demonstrate competitive performance on multiple datasets compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Subgroups Matter for Robust Bias Mitigation</title>
<link>https://arxiv.org/abs/2505.21363</link>
<guid>https://arxiv.org/abs/2505.21363</guid>
<content:encoded><![CDATA[
arXiv:2505.21363v1 Announce Type: new 
Abstract: Despite the constant development of new bias mitigation methods for machine learning, no method consistently succeeds, and a fundamental question remains unanswered: when and why do bias mitigation techniques fail? In this paper, we hypothesise that a key factor may be the often-overlooked but crucial step shared by many bias mitigation methods: the definition of subgroups. To investigate this, we conduct a comprehensive evaluation of state-of-the-art bias mitigation methods across multiple vision and language classification tasks, systematically varying subgroup definitions, including coarse, fine-grained, intersectional, and noisy subgroups. Our results reveal that subgroup choice significantly impacts performance, with certain groupings paradoxically leading to worse outcomes than no mitigation at all. Our findings suggest that observing a disparity between a set of subgroups is not a sufficient reason to use those subgroups for mitigation. Through theoretical analysis, we explain these phenomena and uncover a counter-intuitive insight that, in some cases, improving fairness with respect to a particular set of subgroups is best achieved by using a different set of subgroups for mitigation. Our work highlights the importance of careful subgroup definition in bias mitigation and suggest it as a alternative lever for improving the robustness and fairness of machine learning models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders</title>
<link>https://arxiv.org/abs/2505.21364</link>
<guid>https://arxiv.org/abs/2505.21364</guid>
<content:encoded><![CDATA[
arXiv:2505.21364v1 Announce Type: new 
Abstract: Multilayer perceptrons (MLPs) are an integral part of large language models, yet their dense representations render them difficult to understand, edit, and steer. Recent methods learn interpretable approximations via neuron-level sparsity, yet fail to faithfully reconstruct the original mapping--significantly increasing model's next-token cross-entropy loss. In this paper, we advocate for moving to layer-level sparsity to overcome the accuracy trade-off in sparse layer approximation. Under this paradigm, we introduce Mixture of Decoders (MxDs). MxDs generalize MLPs and Gated Linear Units, expanding pre-trained dense layers into tens of thousands of specialized sublayers. Through a flexible form of tensor factorization, each sparsely activating MxD sublayer implements a linear transformation with full-rank weights--preserving the original decoders' expressive capacity even under heavy sparsity. Experimentally, we show that MxDs significantly outperform state-of-the-art methods (e.g., Transcoders) on the sparsity-accuracy frontier in language models with up to 3B parameters. Further evaluations on sparse probing and feature steering demonstrate that MxDs learn similarly specialized features of natural language--opening up a promising new avenue for designing interpretable yet faithful decompositions. Our code is included at: https://github.com/james-oldfield/MxD/.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLANETALIGN: A Comprehensive Python Library for Benchmarking Network Alignment</title>
<link>https://arxiv.org/abs/2505.21366</link>
<guid>https://arxiv.org/abs/2505.21366</guid>
<content:encoded><![CDATA[
arXiv:2505.21366v1 Announce Type: new 
Abstract: Network alignment (NA) aims to identify node correspondence across different networks and serves as a critical cornerstone behind various downstream multi-network learning tasks. Despite growing research in NA, there lacks a comprehensive library that facilitates the systematic development and benchmarking of NA methods. In this work, we introduce PLANETALIGN, a comprehensive Python library for network alignment that features a rich collection of built-in datasets, methods, and evaluation pipelines with easy-to-use APIs. Specifically, PLANETALIGN integrates 18 datasets and 14 NA methods with extensible APIs for easy use and development of NA methods. Our standardized evaluation pipeline encompasses a wide range of metrics, enabling a systematic assessment of the effectiveness, scalability, and robustness of NA methods. Through extensive comparative studies, we reveal practical insights into the strengths and limitations of existing NA methods. We hope that PLANETALIGN can foster a deeper understanding of the NA problem and facilitate the development and benchmarking of more effective, scalable, and robust methods in the future. The source code of PLANETALIGN is available at https://github.com/yq-leo/PlanetAlign.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improving LLM-based Global Optimization with Search Space Partitioning</title>
<link>https://arxiv.org/abs/2505.21372</link>
<guid>https://arxiv.org/abs/2505.21372</guid>
<content:encoded><![CDATA[
arXiv:2505.21372v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have recently emerged as effective surrogate models and candidate generators within global optimization frameworks for expensive blackbox functions. Despite promising results, LLM-based methods often struggle in high-dimensional search spaces or when lacking domain-specific priors, leading to sparse or uninformative suggestions. To overcome these limitations, we propose HOLLM, a novel global optimization algorithm that enhances LLM-driven sampling by partitioning the search space into promising subregions. Each subregion acts as a ``meta-arm'' selected via a bandit-inspired scoring mechanism that effectively balances exploration and exploitation. Within each selected subregion, an LLM then proposes high-quality candidate points, without any explicit domain knowledge. Empirical evaluation on standard optimization benchmarks shows that HOLLM consistently matches or surpasses leading Bayesian optimization and trust-region methods, while substantially outperforming global LLM-based sampling strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeCAF: Decentralized Consensus-And-Factorization for Low-Rank Adaptation of Foundation Models</title>
<link>https://arxiv.org/abs/2505.21382</link>
<guid>https://arxiv.org/abs/2505.21382</guid>
<content:encoded><![CDATA[
arXiv:2505.21382v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) has emerged as one of the most effective, computationally tractable fine-tuning approaches for training Vision-Language Models (VLMs) and Large Language Models (LLMs). LoRA accomplishes this by freezing the pre-trained model weights and injecting trainable low-rank matrices, allowing for efficient learning of these foundation models even on edge devices. However, LoRA in decentralized settings still remains under explored, particularly for the theoretical underpinnings due to the lack of smoothness guarantee and model consensus interference (defined formally below). This work improves the convergence rate of decentralized LoRA (DLoRA) to match the rate of decentralized SGD by ensuring gradient smoothness. We also introduce DeCAF, a novel algorithm integrating DLoRA with truncated singular value decomposition (TSVD)-based matrix factorization to resolve consensus interference. Theoretical analysis shows TSVD's approximation error is bounded and consensus differences between DLoRA and DeCAF vanish as rank increases, yielding DeCAF's matching convergence rate. Extensive experiments across vision/language tasks demonstrate our algorithms outperform local training and rivals federated learning under both IID and non-IID data distributions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite Sample Analysis of Linear Temporal Difference Learning with Arbitrary Features</title>
<link>https://arxiv.org/abs/2505.21391</link>
<guid>https://arxiv.org/abs/2505.21391</guid>
<content:encoded><![CDATA[
arXiv:2505.21391v1 Announce Type: new 
Abstract: Linear TD($\lambda$) is one of the most fundamental reinforcement learning algorithms for policy evaluation. Previously, convergence rates are typically established under the assumption of linearly independent features, which does not hold in many practical scenarios. This paper instead establishes the first $L^2$ convergence rates for linear TD($\lambda$) operating under arbitrary features, without making any algorithmic modification or additional assumptions. Our results apply to both the discounted and average-reward settings. To address the potential non-uniqueness of solutions resulting from arbitrary features, we develop a novel stochastic approximation result featuring convergence rates to the solution set instead of a single point.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging the Power of Conversations: Optimal Key Term Selection in Conversational Contextual Bandits</title>
<link>https://arxiv.org/abs/2505.21393</link>
<guid>https://arxiv.org/abs/2505.21393</guid>
<content:encoded><![CDATA[
arXiv:2505.21393v1 Announce Type: new 
Abstract: Conversational recommender systems proactively query users with relevant "key terms" and leverage the feedback to elicit users' preferences for personalized recommendations. Conversational contextual bandits, a prevalent approach in this domain, aim to optimize preference learning by balancing exploitation and exploration. However, several limitations hinder their effectiveness in real-world scenarios. First, existing algorithms employ key term selection strategies with insufficient exploration, often failing to thoroughly probe users' preferences and resulting in suboptimal preference estimation. Second, current algorithms typically rely on deterministic rules to initiate conversations, causing unnecessary interactions when preferences are well-understood and missed opportunities when preferences are uncertain. To address these limitations, we propose three novel algorithms: CLiSK, CLiME, and CLiSK-ME. CLiSK introduces smoothed key term contexts to enhance exploration in preference learning, CLiME adaptively initiates conversations based on preference uncertainty, and CLiSK-ME integrates both techniques. We theoretically prove that all three algorithms achieve a tighter regret upper bound of $O(\sqrt{dT\log{T}})$ with respect to the time horizon $T$, improving upon existing methods. Additionally, we provide a matching lower bound $\Omega(\sqrt{dT})$ for conversational bandits, demonstrating that our algorithms are nearly minimax optimal. Extensive evaluations on both synthetic and real-world datasets show that our approaches achieve at least a 14.6% improvement in cumulative regret.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Square$\chi$PO: Differentially Private and Robust $\chi^2$-Preference Optimization in Offline Direct Alignment</title>
<link>https://arxiv.org/abs/2505.21395</link>
<guid>https://arxiv.org/abs/2505.21395</guid>
<content:encoded><![CDATA[
arXiv:2505.21395v1 Announce Type: new 
Abstract: In this paper, we theoretically study the offline alignment of language models with human preference feedback, under both preference label corruption and privacy protections. To this end, we propose Square$\chi$PO, a simple one-line change to $\chi$PO where the standard log-loss is replaced by a new square loss over probability. Thanks to the inherent properties of this new loss, we have advanced the state-of-the-art of differentially private and robust offline direct alignment. Specifically, for the local model of label privacy, Square$\chi$PO is the first algorithm that attains an optimal rate based on single-policy concentrability even with general function approximations. It also gives the first result under the central model of privacy protection over both prompts (responses) and labels. On the robustness side against Huber label corruption, Square$\chi$PO is the first alignment method that has a meaningful theoretical guarantee under general function approximations. More importantly, Square$\chi$PO can address privacy protection and corruption simultaneously, where an interesting separation is observed, implying that the order of privacy and corruption matters. Furthermore, we show that Square$\chi$PO can also be easily extended to handle the scenario of the general preference model with state-of-the-art guarantees under corruption and privacy. Last but not least, all of our theoretical guarantees enjoy a unified analysis, building upon a new result on the generalization error bounds of least-square regression under corruption and privacy constraints, which we believe is of independent interest to the community.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Convergence Theory for Diffusion Language Models: An Information-Theoretic Perspective</title>
<link>https://arxiv.org/abs/2505.21400</link>
<guid>https://arxiv.org/abs/2505.21400</guid>
<content:encoded><![CDATA[
arXiv:2505.21400v1 Announce Type: new 
Abstract: Diffusion models have emerged as a powerful paradigm for modern generative modeling, demonstrating strong potential for large language models (LLMs). Unlike conventional autoregressive (AR) models that generate tokens sequentially, diffusion models enable parallel token sampling, leading to faster generation and eliminating left-to-right generation constraints. Despite their empirical success, the theoretical understanding of diffusion model approaches remains underdeveloped. In this work, we develop convergence guarantees for diffusion language models from an information-theoretic perspective. Our analysis demonstrates that the sampling error, measured by the Kullback-Leibler (KL) divergence, decays inversely with the number of iterations $T$ and scales linearly with the mutual information between tokens in the target text sequence. In particular, we establish matching upper and lower bounds, up to some constant factor, to demonstrate the tightness of our convergence analysis. These results offer novel theoretical insights into the practical effectiveness of diffusion language models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual Natural Gradient Descent for Scalable Training of Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.21404</link>
<guid>https://arxiv.org/abs/2505.21404</guid>
<content:encoded><![CDATA[
arXiv:2505.21404v1 Announce Type: new 
Abstract: Natural-gradient methods markedly accelerate the training of Physics-Informed Neural Networks (PINNs), yet their Gauss--Newton update must be solved in the parameter space, incurring a prohibitive $O(n^3)$ time complexity, where $n$ is the number of network trainable weights. We show that exactly the same step can instead be formulated in a generally smaller residual space of size $m = \sum_{\gamma} N_{\gamma} d_{\gamma}$, where each residual class $\gamma$ (e.g. PDE interior, boundary, initial data) contributes $N_{\gamma}$ collocation points of output dimension $d_{\gamma}$.
  Building on this insight, we introduce \textit{Dual Natural Gradient Descent} (D-NGD). D-NGD computes the Gauss--Newton step in residual space, augments it with a geodesic-acceleration correction at negligible extra cost, and provides both a dense direct solver for modest $m$ and a Nystrom-preconditioned conjugate-gradient solver for larger $m$.
  Experimentally, D-NGD scales second-order PINN optimization to networks with up to 12.8 million parameters, delivers one- to three-order-of-magnitude lower final error $L^2$ than first-order methods (Adam, SGD) and quasi-Newton methods, and -- crucially -- enables natural-gradient training of PINNs at this scale on a single GPU.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Framework for Adversarial Analysis of Decision Support Systems Prior to Deployment</title>
<link>https://arxiv.org/abs/2505.21414</link>
<guid>https://arxiv.org/abs/2505.21414</guid>
<content:encoded><![CDATA[
arXiv:2505.21414v1 Announce Type: new 
Abstract: This paper introduces a comprehensive framework designed to analyze and secure decision-support systems trained with Deep Reinforcement Learning (DRL), prior to deployment, by providing insights into learned behavior patterns and vulnerabilities discovered through simulation. The introduced framework aids in the development of precisely timed and targeted observation perturbations, enabling researchers to assess adversarial attack outcomes within a strategic decision-making context. We validate our framework, visualize agent behavior, and evaluate adversarial outcomes within the context of a custom-built strategic game, CyberStrike. Utilizing the proposed framework, we introduce a method for systematically discovering and ranking the impact of attacks on various observation indices and time-steps, and we conduct experiments to evaluate the transferability of adversarial attacks across agent architectures and DRL training algorithms. The findings underscore the critical need for robust adversarial defense mechanisms to protect decision-making policies in high-stakes environments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Shift Happens - Confounding Is to Blame</title>
<link>https://arxiv.org/abs/2505.21422</link>
<guid>https://arxiv.org/abs/2505.21422</guid>
<content:encoded><![CDATA[
arXiv:2505.21422v1 Announce Type: new 
Abstract: Distribution shifts introduce uncertainty that undermines the robustness and generalization capabilities of machine learning models. While conventional wisdom suggests that learning causal-invariant representations enhances robustness to such shifts, recent empirical studies present a counterintuitive finding: (i) empirical risk minimization (ERM) can rival or even outperform state-of-the-art out-of-distribution (OOD) generalization methods, and (ii) its OOD generalization performance improves when all available covariates, not just causal ones, are utilized. Drawing on both empirical and theoretical evidence, we attribute this phenomenon to hidden confounding. Shifts in hidden confounding induce changes in data distributions that violate assumptions commonly made by existing OOD generalization approaches. Under such conditions, we prove that effective generalization requires learning environment-specific relationships, rather than relying solely on invariant ones. Furthermore, we show that models augmented with proxies for hidden confounders can mitigate the challenges posed by hidden confounding shifts. These findings offer new theoretical insights and practical guidance for designing robust OOD generalization algorithms and principled covariate selection strategies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conflicting Biases at the Edge of Stability: Norm versus Sharpness Regularization</title>
<link>https://arxiv.org/abs/2505.21423</link>
<guid>https://arxiv.org/abs/2505.21423</guid>
<content:encoded><![CDATA[
arXiv:2505.21423v1 Announce Type: new 
Abstract: A widely believed explanation for the remarkable generalization capacities of overparameterized neural networks is that the optimization algorithms used for training induce an implicit bias towards benign solutions. To grasp this theoretically, recent works examine gradient descent and its variants in simplified training settings, often assuming vanishing learning rates. These studies reveal various forms of implicit regularization, such as $\ell_1$-norm minimizing parameters in regression and max-margin solutions in classification. Concurrently, empirical findings show that moderate to large learning rates exceeding standard stability thresholds lead to faster, albeit oscillatory, convergence in the so-called Edge-of-Stability regime, and induce an implicit bias towards minima of low sharpness (norm of training loss Hessian). In this work, we argue that a comprehensive understanding of the generalization performance of gradient descent requires analyzing the interaction between these various forms of implicit regularization. We empirically demonstrate that the learning rate balances between low parameter norm and low sharpness of the trained model. We furthermore prove for diagonal linear networks trained on a simple regression task that neither implicit bias alone minimizes the generalization error. These findings demonstrate that focusing on a single implicit bias is insufficient to explain good generalization, and they motivate a broader view of implicit regularization that captures the dynamic trade-off between norm and sharpness induced by non-negligible learning rates.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Attribute-Efficient PAC Learning of Sparse Halfspaces with Constant Malicious Noise Rate</title>
<link>https://arxiv.org/abs/2505.21430</link>
<guid>https://arxiv.org/abs/2505.21430</guid>
<content:encoded><![CDATA[
arXiv:2505.21430v1 Announce Type: new 
Abstract: Attribute-efficient learning of sparse halfspaces has been a fundamental problem in machine learning theory. In recent years, machine learning algorithms are faced with prevalent data corruptions or even adversarial attacks. It is of central interest to design efficient algorithms that are robust to noise corruptions. In this paper, we consider that there exists a constant amount of malicious noise in the data and the goal is to learn an underlying $s$-sparse halfspace $w^* \in \mathbb{R}^d$ with $\text{poly}(s,\log d)$ samples. Specifically, we follow a recent line of works and assume that the underlying distribution satisfies a certain concentration condition and a margin condition at the same time. Under such conditions, we show that attribute-efficiency can be achieved by simple variants to existing hinge loss minimization programs. Our key contribution includes: 1) an attribute-efficient PAC learning algorithm that works under constant malicious noise rate; 2) a new gradient analysis that carefully handles the sparsity constraint in hinge loss minimization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Measuring Fine-Grained Relatedness in Multitask Learning via Data Attribution</title>
<link>https://arxiv.org/abs/2505.21438</link>
<guid>https://arxiv.org/abs/2505.21438</guid>
<content:encoded><![CDATA[
arXiv:2505.21438v1 Announce Type: new 
Abstract: Measuring task relatedness and mitigating negative transfer remain a critical open challenge in Multitask Learning (MTL). This work extends data attribution -- which quantifies the influence of individual training data points on model predictions -- to MTL setting for measuring task relatedness. We propose the MultiTask Influence Function (MTIF), a method that adapts influence functions to MTL models with hard or soft parameter sharing. Compared to conventional task relatedness measurements, MTIF provides a fine-grained, instance-level relatedness measure beyond the entire-task level. This fine-grained relatedness measure enables a data selection strategy to effectively mitigate negative transfer in MTL. Through extensive experiments, we demonstrate that the proposed MTIF efficiently and accurately approximates the performance of models trained on data subsets. Moreover, the data selection strategy enabled by MTIF consistently improves model performance in MTL. Our work establishes a novel connection between data attribution and MTL, offering an efficient and fine-grained solution for measuring task relatedness and enhancing MTL models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Reasoning Models Self-Train?</title>
<link>https://arxiv.org/abs/2505.21444</link>
<guid>https://arxiv.org/abs/2505.21444</guid>
<content:encoded><![CDATA[
arXiv:2505.21444v1 Announce Type: new 
Abstract: Scaling the performance of large language models (LLMs) increasingly depends on methods that reduce reliance on human supervision. Reinforcement learning from automated verification offers an alternative, but it incurs scalability limitations due to dependency upon human-designed verifiers. Self-training, where the model's own judgment provides the supervisory signal, presents a compelling direction. We propose an online self-training reinforcement learning algorithm that leverages the model's self-consistency to infer correctness signals and train without any ground-truth supervision. We apply the algorithm to challenging mathematical reasoning tasks and show that it quickly reaches performance levels rivaling reinforcement-learning methods trained explicitly on gold-standard answers. Additionally, we analyze inherent limitations of the algorithm, highlighting how the self-generated proxy reward initially correlated with correctness can incentivize reward hacking, where confidently incorrect outputs are favored. Our results illustrate how self-supervised improvement can achieve significant performance gains without external labels, while also revealing its fundamental challenges.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Designing Cyclic Peptides via Harmonic SDE with Atom-Bond Modeling</title>
<link>https://arxiv.org/abs/2505.21452</link>
<guid>https://arxiv.org/abs/2505.21452</guid>
<content:encoded><![CDATA[
arXiv:2505.21452v1 Announce Type: new 
Abstract: Cyclic peptides offer inherent advantages in pharmaceuticals. For example, cyclic peptides are more resistant to enzymatic hydrolysis compared to linear peptides and usually exhibit excellent stability and affinity. Although deep generative models have achieved great success in linear peptide design, several challenges prevent the development of computational methods for designing diverse types of cyclic peptides. These challenges include the scarcity of 3D structural data on target proteins and associated cyclic peptide ligands, the geometric constraints that cyclization imposes, and the involvement of non-canonical amino acids in cyclization. To address the above challenges, we introduce CpSDE, which consists of two key components: AtomSDE, a generative structure prediction model based on harmonic SDE, and ResRouter, a residue type predictor. Utilizing a routed sampling algorithm that alternates between these two models to iteratively update sequences and structures, CpSDE facilitates the generation of cyclic peptides. By employing explicit all-atom and bond modeling, CpSDE overcomes existing data limitations and is proficient in designing a wide variety of cyclic peptides. Our experimental results demonstrate that the cyclic peptides designed by our method exhibit reliable stability and affinity.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>High-Dimensional Calibration from Swap Regret</title>
<link>https://arxiv.org/abs/2505.21460</link>
<guid>https://arxiv.org/abs/2505.21460</guid>
<content:encoded><![CDATA[
arXiv:2505.21460v1 Announce Type: new 
Abstract: We study the online calibration of multi-dimensional forecasts over an arbitrary convex set $\mathcal{P} \subset \mathbb{R}^d$ relative to an arbitrary norm $\Vert\cdot\Vert$. We connect this with the problem of external regret minimization for online linear optimization, showing that if it is possible to guarantee $O(\sqrt{\rho T})$ worst-case regret after $T$ rounds when actions are drawn from $\mathcal{P}$ and losses are drawn from the dual $\Vert \cdot \Vert_*$ unit norm ball, then it is also possible to obtain $\epsilon$-calibrated forecasts after $T = \exp(O(\rho /\epsilon^2))$ rounds. When $\mathcal{P}$ is the $d$-dimensional simplex and $\Vert \cdot \Vert$ is the $\ell_1$-norm, the existence of $O(\sqrt{T\log d})$-regret algorithms for learning with experts implies that it is possible to obtain $\epsilon$-calibrated forecasts after $T = \exp(O(\log{d}/\epsilon^2)) = d^{O(1/\epsilon^2)}$ rounds, recovering a recent result of Peng (2025).
  Interestingly, our algorithm obtains this guarantee without requiring access to any online linear optimization subroutine or knowledge of the optimal rate $\rho$ -- in fact, our algorithm is identical for every setting of $\mathcal{P}$ and $\Vert \cdot \Vert$. Instead, we show that the optimal regularizer for the above OLO problem can be used to upper bound the above calibration error by a swap regret, which we then minimize by running the recent TreeSwap algorithm with Follow-The-Leader as a subroutine.
  Finally, we prove that any online calibration algorithm that guarantees $\epsilon T$ $\ell_1$-calibration error over the $d$-dimensional simplex requires $T \geq \exp(\mathrm{poly}(1/\epsilon))$ (assuming $d \geq \mathrm{poly}(1/\epsilon)$). This strengthens the corresponding $d^{\Omega(\log{1/\epsilon})}$ lower bound of Peng, and shows that an exponential dependence on $1/\epsilon$ is necessary.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Posterior Estimation</title>
<link>https://arxiv.org/abs/2505.21468</link>
<guid>https://arxiv.org/abs/2505.21468</guid>
<content:encoded><![CDATA[
arXiv:2505.21468v1 Announce Type: new 
Abstract: We present Causal Posterior Estimation (CPE), a novel method for Bayesian inference in simulator models, i.e., models where the evaluation of the likelihood function is intractable or too computationally expensive, but where one can simulate model outputs given parameter values. CPE utilizes a normalizing flow-based (NF) approximation to the posterior distribution which carefully incorporates the conditional dependence structure induced by the graphical representation of the model into the neural network. Thereby it is possible to improve the accuracy of the approximation. We introduce both discrete and continuous NF architectures for CPE and propose a constant-time sampling procedure for the continuous case which reduces the computational complexity of drawing samples to O(1) as for discrete NFs. We show, through an extensive experimental evaluation, that by incorporating the conditional dependencies induced by the graphical model directly into the neural network, rather than learning them from data, CPE is able to conduct highly accurate posterior inference either outperforming or matching the state of the art in the field.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithms and SQ Lower Bounds for Robustly Learning Real-valued Multi-index Models</title>
<link>https://arxiv.org/abs/2505.21475</link>
<guid>https://arxiv.org/abs/2505.21475</guid>
<content:encoded><![CDATA[
arXiv:2505.21475v1 Announce Type: new 
Abstract: We study the complexity of learning real-valued Multi-Index Models (MIMs) under the Gaussian distribution. A $K$-MIM is a function $f:\mathbb{R}^d\to \mathbb{R}$ that depends only on the projection of its input onto a $K$-dimensional subspace. We give a general algorithm for PAC learning a broad class of MIMs with respect to the square loss, even in the presence of adversarial label noise. Moreover, we establish a nearly matching Statistical Query (SQ) lower bound, providing evidence that the complexity of our algorithm is qualitatively optimal as a function of the dimension. Specifically, we consider the class of bounded variation MIMs with the property that degree at most $m$ distinguishing moments exist with respect to projections onto any subspace. In the presence of adversarial label noise, the complexity of our learning algorithm is $d^{O(m)}2^{\mathrm{poly}(K/\epsilon)}$. For the realizable and independent noise settings, our algorithm incurs complexity $d^{O(m)}2^{\mathrm{poly}(K)}(1/\epsilon)^{O(K)}$. To complement our upper bound, we show that if for some subspace degree-$m$ distinguishing moments do not exist, then any SQ learner for the corresponding class of MIMs requires complexity $d^{\Omega(m)}$. As an application, we give the first efficient learner for the class of positive-homogeneous $L$-Lipschitz $K$-MIMs. The resulting algorithm has complexity $\mathrm{poly}(d) 2^{\mathrm{poly}(KL/\epsilon)}$. This gives a new PAC learning algorithm for Lipschitz homogeneous ReLU networks with complexity independent of the network size, removing the exponential dependence incurred in prior work.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hardware-Efficient Attention for Fast Decoding</title>
<link>https://arxiv.org/abs/2505.21487</link>
<guid>https://arxiv.org/abs/2505.21487</guid>
<content:encoded><![CDATA[
arXiv:2505.21487v1 Announce Type: new 
Abstract: LLM decoding is bottlenecked for large batches and long contexts by loading the key-value (KV) cache from high-bandwidth memory, which inflates per-token latency, while the sequential nature of decoding limits parallelism. We analyze the interplay among arithmetic intensity, parallelization, and model quality and question whether current architectures fully exploit modern hardware. This work redesigns attention to perform more computation per byte loaded from memory to maximize hardware efficiency without trading off parallel scalability. We first propose Grouped-Tied Attention (GTA), a simple variant that combines and reuses key and value states, reducing memory transfers without compromising model quality. We then introduce Grouped Latent Attention (GLA), a parallel-friendly latent attention paired with low-level optimizations for fast decoding while maintaining high model quality. Experiments show that GTA matches Grouped-Query Attention (GQA) quality while using roughly half the KV cache and that GLA matches Multi-head Latent Attention (MLA) and is easier to shard. Our optimized GLA kernel is up to 2$\times$ faster than FlashMLA, for example, in a speculative decoding setting when the query length exceeds one. Furthermore, by fetching a smaller KV cache per device, GLA reduces end-to-end latency and increases throughput in online serving benchmarks by up to 2$\times$.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcing General Reasoning without Verifiers</title>
<link>https://arxiv.org/abs/2505.21493</link>
<guid>https://arxiv.org/abs/2505.21493</guid>
<content:encoded><![CDATA[
arXiv:2505.21493v1 Announce Type: new 
Abstract: The recent paradigm shift towards training large language models (LLMs) using DeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has led to impressive advancements in code and mathematical reasoning. However, this methodology is limited to tasks where rule-based answer verification is possible and does not naturally extend to real-world domains such as chemistry, healthcare, engineering, law, biology, business, and economics. Current practical workarounds use an additional LLM as a model-based verifier; however, this introduces issues such as reliance on a strong verifier LLM, susceptibility to reward hacking, and the practical burden of maintaining the verifier model in memory during training. To address this and extend DeepSeek-R1-Zero-style training to general reasoning domains, we propose a verifier-free method (VeriFree) that bypasses answer verification and instead uses RL to directly maximize the probability of generating the reference answer. We compare VeriFree with verifier-based methods and demonstrate that, in addition to its significant practical benefits and reduced compute requirements, VeriFree matches and even surpasses verifier-based methods on extensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related benchmarks. Moreover, we provide insights into this method from multiple perspectives: as an elegant integration of training both the policy and implicit verifier in a unified model, and as a variational optimization approach. Code is available at https://github.com/sail-sg/VeriFree.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CRUST-Bench: A Comprehensive Benchmark for C-to-safe-Rust Transpilation</title>
<link>https://arxiv.org/abs/2504.15254</link>
<guid>https://arxiv.org/abs/2504.15254</guid>
<content:encoded><![CDATA[
arXiv:2504.15254v1 Announce Type: cross 
Abstract: C-to-Rust transpilation is essential for modernizing legacy C code while enhancing safety and interoperability with modern Rust ecosystems. However, no dataset currently exists for evaluating whether a system can transpile C into safe Rust that passes a set of test cases. We introduce CRUST-Bench, a dataset of 100 C repositories, each paired with manually-written interfaces in safe Rust as well as test cases that can be used to validate correctness of the transpilation. By considering entire repositories rather than isolated functions, CRUST-Bench captures the challenges of translating complex projects with dependencies across multiple files. The provided Rust interfaces provide explicit specifications that ensure adherence to idiomatic, memory-safe Rust patterns, while the accompanying test cases enforce functional correctness. We evaluate state-of-the-art large language models (LLMs) on this task and find that safe and idiomatic Rust generation is still a challenging problem for various state-of-the-art methods and techniques. We also provide insights into the errors LLMs usually make in transpiling code from C to safe Rust. The best performing model, OpenAI o1, is able to solve only 15 tasks in a single-shot setting. Improvements on CRUST-Bench would lead to improved transpilation systems that can reason about complex scenarios and help in migrating legacy codebases from C into languages like Rust that ensure memory safety. You can find the dataset and code at https://github.com/anirudhkhatry/CRUST-bench.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequence-Only Prediction of Binding Affinity Changes: A Robust and Interpretable Model for Antibody Engineering</title>
<link>https://arxiv.org/abs/2505.20301</link>
<guid>https://arxiv.org/abs/2505.20301</guid>
<content:encoded><![CDATA[
arXiv:2505.20301v1 Announce Type: cross 
Abstract: A pivotal area of research in antibody engineering is to find effective modifications that enhance antibody-antigen binding affinity. Traditional wet-lab experiments assess mutants in a costly and time-consuming manner. Emerging deep learning solutions offer an alternative by modeling antibody structures to predict binding affinity changes. However, they heavily depend on high-quality complex structures, which are frequently unavailable in practice. Therefore, we propose ProtAttBA, a deep learning model that predicts binding affinity changes based solely on the sequence information of antibody-antigen complexes. ProtAttBA employs a pre-training phase to learn protein sequence patterns, following a supervised training phase using labeled antibody-antigen complex data to train a cross-attention-based regressor for predicting binding affinity changes. We evaluated ProtAttBA on three open benchmarks under different conditions. Compared to both sequence- and structure-based prediction methods, our approach achieves competitive performance, demonstrating notable robustness, especially with uncertain complex structures. Notably, our method possesses interpretability from the attention mechanism. We show that the learned attention scores can identify critical residues with impacts on binding affinity. This work introduces a rapid and cost-effective computational tool for antibody engineering, with the potential to accelerate the development of novel therapeutic antibodies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guiding Giants: Lightweight Controllers for Weighted Activation Steering in LLMs</title>
<link>https://arxiv.org/abs/2505.20309</link>
<guid>https://arxiv.org/abs/2505.20309</guid>
<content:encoded><![CDATA[
arXiv:2505.20309v1 Announce Type: cross 
Abstract: Controlling undesirable Large Language Model (LLM) behaviors, such as the generation of unsafe content or failing to adhere to safety guidelines, often relies on costly fine-tuning. Activation steering provides an alternative for inference-time control, but existing methods typically lack fine-grained, adaptive mechanisms. We introduce a novel approach using a lightweight, trainable controller network integrated during inference. This controller network observes specific intermediate LLM activations and predicts both a global scaling factor and layer-specific weights. The predicted global scaling factor and layer-specific weights then dynamically modulate the intensity of a steering patch, derived from a pre-computed "refusal direction" vector, applied across the LLM's layers during generation. Trained on activations from both harmful and benign prompts, our controller learns to discriminatively apply nuanced, layer-aware interventions, activating steering primarily for harmful inputs. Experiments using safety benchmarks like ToxicChat & In-The-Wild Jailbreak Prompts demonstrate that our weighted steering controller significantly increases refusal rates compared to the base LLM, achieving targeted behavioral modification without altering the original model parameters. Our experiments with Llama-3.1-8B, Llama-3.2-1B & Mistral-7B show our approach outperforms existing methods, presenting an efficient and adaptive method for fine-grained control over LLM behavior at inference time.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge Bases</title>
<link>https://arxiv.org/abs/2505.20321</link>
<guid>https://arxiv.org/abs/2505.20321</guid>
<content:encoded><![CDATA[
arXiv:2505.20321v1 Announce Type: cross 
Abstract: Biomedical researchers increasingly rely on large-scale structured databases for complex analytical tasks. However, current text-to-SQL systems often struggle to map qualitative scientific questions into executable SQL, particularly when implicit domain reasoning is required. We introduce BiomedSQL, the first benchmark explicitly designed to evaluate scientific reasoning in text-to-SQL generation over a real-world biomedical knowledge base. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in a harmonized BigQuery knowledge base that integrates gene-disease associations, causal inference from omics data, and drug approval records. Each question requires models to infer domain-specific criteria, such as genome-wide significance thresholds, effect directionality, or trial phase filtering, rather than rely on syntactic translation alone. We evaluate a range of open- and closed-source LLMs across prompting strategies and interaction paradigms. Our results reveal a substantial performance gap: GPT-o3-mini achieves 59.0% execution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%, both well below the expert baseline of 90.0%. BiomedSQL provides a new foundation for advancing text-to-SQL systems capable of supporting scientific discovery through robust reasoning over structured biomedical knowledge bases. Our dataset is publicly available at https://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source at https://github.com/NIH-CARD/biomedsql.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Prompt Engineering: Robust Behavior Control in LLMs via Steering Target Atoms</title>
<link>https://arxiv.org/abs/2505.20322</link>
<guid>https://arxiv.org/abs/2505.20322</guid>
<content:encoded><![CDATA[
arXiv:2505.20322v1 Announce Type: cross 
Abstract: Precise control over language model generation is vital for ensuring both safety and reliability. Although prompt engineering and steering are commonly used to intervene in model behaviors, the vast number of parameters in models often results in highly intertwined internal representations. This interdependency can limit control precision and sometimes lead to unintended side effects. Recent research has explored the use of sparse autoencoders (SAE) to disentangle knowledge in high-dimensional spaces for steering. However, these applications have been limited to toy tasks owing to the nontrivial issue of locating atomic knowledge components. In this paper, we propose Steering Target Atoms (STA), a novel method that isolates and manipulates disentangled knowledge components to enhance safety. Comprehensive experiments demonstrate the effectiveness of our approach. Further analysis reveals that steering exhibits superior robustness and flexibility, particularly in adversarial scenarios. We also apply the steering strategy to the large reasoning model, confirming its effectiveness in precise reasoning control.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</title>
<link>https://arxiv.org/abs/2505.20323</link>
<guid>https://arxiv.org/abs/2505.20323</guid>
<content:encoded><![CDATA[
arXiv:2505.20323v1 Announce Type: cross 
Abstract: Understanding temporal dynamics in clinical narratives is essential for modeling patient trajectories, yet large-scale temporally annotated resources remain limited. We present PMOA-TTS, the first openly available dataset of 124,699 PubMed Open Access (PMOA) case reports, each converted into structured (event, time) timelines via a scalable LLM-based pipeline. Our approach combines heuristic filtering with Llama 3.3 to identify single-patient case reports, followed by prompt-driven extraction using Llama 3.3 and DeepSeek R1, resulting in over 5.6 million timestamped clinical events. To assess timeline quality, we evaluate against a clinician-curated reference set using three metrics: (i) event-level matching (80% match at a cosine similarity threshold of 0.1), (ii) temporal concordance (c-index > 0.90), and (iii) Area Under the Log-Time CDF (AULTC) for timestamp alignment. Corpus-level analysis shows wide diagnostic and demographic coverage. In a downstream survival prediction task, embeddings from extracted timelines achieve time-dependent concordance indices up to 0.82 $\pm$ 0.01, demonstrating the predictive value of temporally structured narratives. PMOA-TTS provides a scalable foundation for timeline extraction, temporal reasoning, and longitudinal modeling in biomedical NLP. The dataset is available at: https://huggingface.co/datasets/snoroozi/pmoa-tts .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Artificial Intelligence Model for Early Stage Breast Cancer Detection from Biopsy Images</title>
<link>https://arxiv.org/abs/2505.20332</link>
<guid>https://arxiv.org/abs/2505.20332</guid>
<content:encoded><![CDATA[
arXiv:2505.20332v1 Announce Type: cross 
Abstract: Accurate identification of breast cancer types plays a critical role in guiding treatment decisions and improving patient outcomes. This paper presents an artificial intelligence enabled tool designed to aid in the identification of breast cancer types using histopathological biopsy images. Traditionally additional tests have to be done on women who are detected with breast cancer to find out the types of cancer it is to give the necessary cure. Those tests are not only invasive but also delay the initiation of treatment and increase patient burden. The proposed model utilizes a convolutional neural network (CNN) architecture to distinguish between benign and malignant tissues as well as accurate subclassification of breast cancer types. By preprocessing the images to reduce noise and enhance features, the model achieves reliable levels of classification performance. Experimental results on such datasets demonstrate the model's effectiveness, outperforming several existing solutions in terms of accuracy, precision, recall, and F1-score. The study emphasizes the potential of deep learning techniques in clinical diagnostics and offers a promising tool to assist pathologists in breast cancer classification.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predictive Performance of Deep Quantum Data Re-uploading Models</title>
<link>https://arxiv.org/abs/2505.20337</link>
<guid>https://arxiv.org/abs/2505.20337</guid>
<content:encoded><![CDATA[
arXiv:2505.20337v1 Announce Type: cross 
Abstract: Quantum machine learning models incorporating data re-uploading circuits have garnered significant attention due to their exceptional expressivity and trainability. However, their ability to generate accurate predictions on unseen data, referred to as the predictive performance, remains insufficiently investigated. This study reveals a fundamental limitation in predictive performance when deep encoding layers are employed within the data re-uploading model. Concretely, we theoretically demonstrate that when processing high-dimensional data with limited-qubit data re-uploading models, their predictive performance progressively degenerates to near random-guessing levels as the number of encoding layers increases. In this context, the repeated data uploading cannot mitigate the performance degradation. These findings are validated through experiments on both synthetic linearly separable datasets and real-world datasets. Our results demonstrate that when processing high-dimensional data, the quantum data re-uploading models should be designed with wider circuit architectures rather than deeper and narrower ones.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FD-Bench: A Modular and Fair Benchmark for Data-driven Fluid Simulation</title>
<link>https://arxiv.org/abs/2505.20349</link>
<guid>https://arxiv.org/abs/2505.20349</guid>
<content:encoded><![CDATA[
arXiv:2505.20349v1 Announce Type: cross 
Abstract: Data-driven modeling of fluid dynamics has advanced rapidly with neural PDE solvers, yet a fair and strong benchmark remains fragmented due to the absence of unified PDE datasets and standardized evaluation protocols. Although architectural innovations are abundant, fair assessment is further impeded by the lack of clear disentanglement between spatial, temporal and loss modules. In this paper, we introduce FD-Bench, the first fair, modular, comprehensive and reproducible benchmark for data-driven fluid simulation. FD-Bench systematically evaluates 85 baseline models across 10 representative flow scenarios under a unified experimental setup. It provides four key contributions: (1) a modular design enabling fair comparisons across spatial, temporal, and loss function modules; (2) the first systematic framework for direct comparison with traditional numerical solvers; (3) fine-grained generalization analysis across resolutions, initial conditions, and temporal windows; and (4) a user-friendly, extensible codebase to support future research. Through rigorous empirical studies, FD-Bench establishes the most comprehensive leaderboard to date, resolving long-standing issues in reproducibility and comparability, and laying a foundation for robust evaluation of future data-driven fluid models. The code is open-sourced at https://anonymous.4open.science/r/FD-Bench-15BC.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Differentially private ratio statistics</title>
<link>https://arxiv.org/abs/2505.20351</link>
<guid>https://arxiv.org/abs/2505.20351</guid>
<content:encoded><![CDATA[
arXiv:2505.20351v1 Announce Type: cross 
Abstract: Ratio statistics--such as relative risk and odds ratios--play a central role in hypothesis testing, model evaluation, and decision-making across many areas of machine learning, including causal inference and fairness analysis. However, despite privacy concerns surrounding many datasets and despite increasing adoption of differential privacy, differentially private ratio statistics have largely been neglected by the literature and have only recently received an initial treatment by Lin et al. [1]. This paper attempts to fill this lacuna, giving results that can guide practice in evaluating ratios when the results must be protected by differential privacy. In particular, we show that even a simple algorithm can provide excellent properties concerning privacy, sample accuracy, and bias, not just asymptotically but also at quite small sample sizes. Additionally, we analyze a differentially private estimator for relative risk, prove its consistency, and develop a method for constructing valid confidence intervals. Our approach bridges a gap in the differential privacy literature and provides a practical solution for ratio estimation in private machine learning pipelines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Solving Euler equations with Multiple Discontinuities via Separation-Transfer Physics-Informed Neural Networks</title>
<link>https://arxiv.org/abs/2505.20361</link>
<guid>https://arxiv.org/abs/2505.20361</guid>
<content:encoded><![CDATA[
arXiv:2505.20361v1 Announce Type: cross 
Abstract: Despite the remarkable progress of physics-informed neural networks (PINNs) in scientific computing, they continue to face challenges when solving hydrodynamic problems with multiple discontinuities. In this work, we propose Separation-Transfer Physics Informed Neural Networks (ST-PINNs) to address such problems. By sequentially resolving discontinuities from strong to weak and leveraging transfer learning during training, ST-PINNs significantly reduce the problem complexity and enhance solution accuracy. To the best of our knowledge, this is the first study to apply a PINNs-based approach to the two-dimensional unsteady planar shock refraction problem, offering new insights into the application of PINNs to complex shock-interface interactions. Numerical experiments demonstrate that ST-PINNs more accurately capture sharp discontinuities and substantially reduce solution errors in hydrodynamic problems involving multiple discontinuities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DiffNMR: Advancing Inpainting of Randomly Sampled Nuclear Magnetic Resonance Signals</title>
<link>https://arxiv.org/abs/2505.20367</link>
<guid>https://arxiv.org/abs/2505.20367</guid>
<content:encoded><![CDATA[
arXiv:2505.20367v1 Announce Type: cross 
Abstract: Nuclear Magnetic Resonance (NMR) spectroscopy leverages nuclear magnetization to probe molecules' chemical environment, structure, and dynamics, with applications spanning from pharmaceuticals to the petroleum industry. Despite its utility, the high cost of NMR instrumentation, operation and the lengthy duration of experiments necessitate the development of computational techniques to optimize acquisition times. Non-Uniform sampling (NUS) is widely employed as a sub-sampling method to address these challenges, but it often introduces artifacts and degrades spectral quality, offsetting the benefits of reduced acquisition times. In this work, we propose the use of deep learning techniques to enhance the reconstruction quality of NUS spectra. Specifically, we explore the application of diffusion models, a relatively untapped approach in this domain. Our methodology involves applying diffusion models to both time-time and time-frequency NUS data, yielding satisfactory reconstructions of challenging spectra from the benchmark Artina dataset. This approach demonstrates the potential of diffusion models to improve the efficiency and accuracy of NMR spectroscopy as well as the superiority of using a time-frequency domain data over the time-time one, opening new landscapes for future studies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning mechanical systems from real-world data using discrete forced Lagrangian dynamics</title>
<link>https://arxiv.org/abs/2505.20370</link>
<guid>https://arxiv.org/abs/2505.20370</guid>
<content:encoded><![CDATA[
arXiv:2505.20370v1 Announce Type: cross 
Abstract: We introduce a data-driven method for learning the equations of motion of mechanical systems directly from position measurements, without requiring access to velocity data. This is particularly relevant in system identification tasks where only positional information is available, such as motion capture, pixel data or low-resolution tracking. Our approach takes advantage of the discrete Lagrange-d'Alembert principle and the forced discrete Euler-Lagrange equations to construct a physically grounded model of the system's dynamics. We decompose the dynamics into conservative and non-conservative components, which are learned separately using feed-forward neural networks. In the absence of external forces, our method reduces to a variational discretization of the action principle naturally preserving the symplectic structure of the underlying Hamiltonian system. We validate our approach on a variety of synthetic and real-world datasets, demonstrating its effectiveness compared to baseline methods. In particular, we apply our model to (1) measured human motion data and (2) latent embeddings obtained via an autoencoder trained on image sequences. We demonstrate that we can faithfully reconstruct and separate both the conservative and forced dynamics, yielding interpretable and physically consistent predictions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Algorithmic Control Improves Residential Building Energy and EV Management when PV Capacity is High but Battery Capacity is Low</title>
<link>https://arxiv.org/abs/2505.20377</link>
<guid>https://arxiv.org/abs/2505.20377</guid>
<content:encoded><![CDATA[
arXiv:2505.20377v1 Announce Type: cross 
Abstract: Efficient energy management in prosumer households is key to alleviating grid stress in an energy transition marked by electric vehicles (EV), renewable energies and battery storage. However, it is unclear how households optimize prosumer EV charging. Here we study real-world data from 90 households on fixed-rate electricity tariffs in German-speaking countries to investigate the potential of Deep Reinforcement Learning (DRL) and other control approaches (Rule-Based, Model Predictive Control) to manage the dynamic and uncertain environment of Home Energy Management (HEM) and optimize household charging patterns. The DRL agent efficiently aligns charging of EV and battery storage with photovoltaic (PV) surplus. We find that frequent EV charging transactions, early EV connections and PV surplus increase optimization potential. A detailed analysis of nine households (1 hour resolution, 1 year) demonstrates that high battery capacity facilitates self optimization; in this case further algorithmic control shows little value. In cases with relatively low battery capacity, algorithmic control with DRL improves energy management and cost savings by a relevant margin. This result is further corroborated by our simulation of a synthetic household. We conclude that prosumer households with optimization potential would profit from DRL, thus benefiting also the full electricity system and its decarbonization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Kernel Quantile Embeddings and Associated Probability Metrics</title>
<link>https://arxiv.org/abs/2505.20433</link>
<guid>https://arxiv.org/abs/2505.20433</guid>
<content:encoded><![CDATA[
arXiv:2505.20433v1 Announce Type: cross 
Abstract: Embedding probability distributions into reproducing kernel Hilbert spaces (RKHS) has enabled powerful nonparametric methods such as the maximum mean discrepancy (MMD), a statistical distance with strong theoretical and computational properties. At its core, the MMD relies on kernel mean embeddings to represent distributions as mean functions in RKHS. However, it remains unclear if the mean function is the only meaningful RKHS representation. Inspired by generalised quantiles, we introduce the notion of kernel quantile embeddings (KQEs). We then use KQEs to construct a family of distances that: (i) are probability metrics under weaker kernel conditions than MMD; (ii) recover a kernelised form of the sliced Wasserstein distance; and (iii) can be efficiently estimated with near-linear cost. Through hypothesis testing, we show that these distances offer a competitive alternative to MMD and its fast approximations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning-Distillation Alternation for Resource-Constrained IoT</title>
<link>https://arxiv.org/abs/2505.20456</link>
<guid>https://arxiv.org/abs/2505.20456</guid>
<content:encoded><![CDATA[
arXiv:2505.20456v1 Announce Type: cross 
Abstract: Federated learning (FL) faces significant challenges in Internet of Things (IoT) networks due to device limitations in energy and communication resources, especially when considering the large size of FL models. From an energy perspective, the challenge is aggravated if devices rely on energy harvesting (EH), as energy availability can vary significantly over time, influencing the average number of participating users in each iteration. Additionally, the transmission of large model updates is more susceptible to interference from uncorrelated background traffic in shared wireless environments. As an alternative, federated distillation (FD) reduces communication overhead and energy consumption by transmitting local model outputs, which are typically much smaller than the entire model used in FL. However, this comes at the cost of reduced model accuracy. Therefore, in this paper, we propose FL-distillation alternation (FLDA). In FLDA, devices alternate between FD and FL phases, balancing model information with lower communication overhead and energy consumption per iteration. We consider a multichannel slotted-ALOHA EH-IoT network subject to background traffic/interference. In such a scenario, FLDA demonstrates higher model accuracy than both FL and FD, and achieves faster convergence than FL. Moreover, FLDA achieves target accuracies saving up to 98% in energy consumption, while also being less sensitive to interference, both relative to FL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Expected Signatures: Theory and Applications</title>
<link>https://arxiv.org/abs/2505.20465</link>
<guid>https://arxiv.org/abs/2505.20465</guid>
<content:encoded><![CDATA[
arXiv:2505.20465v1 Announce Type: cross 
Abstract: The expected signature maps a collection of data streams to a lower dimensional representation, with a remarkable property: the resulting feature tensor can fully characterize the data generating distribution. This "model-free" embedding has been successfully leveraged to build multiple domain-agnostic machine learning (ML) algorithms for time series and sequential data. The convergence results proved in this paper bridge the gap between the expected signature's empirical discrete-time estimator and its theoretical continuous-time value, allowing for a more complete probabilistic interpretation of expected signature-based ML methods. Moreover, when the data generating process is a martingale, we suggest a simple modification of the expected signature estimator with significantly lower mean squared error and empirically demonstrate how it can be effectively applied to improve predictive performance.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>WeatherEdit: Controllable Weather Editing with 4D Gaussian Field</title>
<link>https://arxiv.org/abs/2505.20471</link>
<guid>https://arxiv.org/abs/2505.20471</guid>
<content:encoded><![CDATA[
arXiv:2505.20471v1 Announce Type: cross 
Abstract: In this work, we present WeatherEdit, a novel weather editing pipeline for generating realistic weather effects with controllable types and severity in 3D scenes. Our approach is structured into two key components: weather background editing and weather particle construction. For weather background editing, we introduce an all-in-one adapter that integrates multiple weather styles into a single pretrained diffusion model, enabling the generation of diverse weather effects in 2D image backgrounds. During inference, we design a Temporal-View (TV-) attention mechanism that follows a specific order to aggregate temporal and spatial information, ensuring consistent editing across multi-frame and multi-view images. To construct the weather particles, we first reconstruct a 3D scene using the edited images and then introduce a dynamic 4D Gaussian field to generate snowflakes, raindrops and fog in the scene. The attributes and dynamics of these particles are precisely controlled through physical-based modelling and simulation, ensuring realistic weather representation and flexible severity adjustments. Finally, we integrate the 4D Gaussian field with the 3D scene to render consistent and highly realistic weather effects. Experiments on multiple driving datasets demonstrate that WeatherEdit can generate diverse weather effects with controllable condition severity, highlighting its potential for autonomous driving simulation in adverse weather. See project page: https://jumponthemoon.github.io/w-edit
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Preconditioning for Neural Field Optimization</title>
<link>https://arxiv.org/abs/2505.20473</link>
<guid>https://arxiv.org/abs/2505.20473</guid>
<content:encoded><![CDATA[
arXiv:2505.20473v1 Announce Type: cross 
Abstract: Neural fields are a highly effective representation across visual computing. This work observes that fitting these fields is greatly improved by incorporating spatial stochasticity during training, and that this simple technique can replace or even outperform custom-designed hierarchies and frequency space constructions. The approach is formalized as implicitly operating on a blurred version of the field, evaluated in-expectation by sampling with Gaussian-distributed offsets. Querying the blurred field during optimization greatly improves convergence and robustness, akin to the role of preconditioners in numerical linear algebra. This implicit, sampling-based perspective fits naturally into the neural field paradigm, comes at no additional cost, and is extremely simple to implement. We describe the basic theory of this technique, including details such as handling boundary conditions, and extending to a spatially-varying blur. Experiments demonstrate this approach on representations including coordinate MLPs, neural hashgrids, triplanes, and more, across tasks including surface reconstruction and radiance fields. In settings where custom-designed hierarchies have already been developed, stochastic preconditioning nearly matches or improves their performance with a simple and unified approach; in settings without existing hierarchies it provides an immediate boost to quality and robustness.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CardioPatternFormer: Pattern-Guided Attention for Interpretable ECG Classification with Transformer Architecture</title>
<link>https://arxiv.org/abs/2505.20481</link>
<guid>https://arxiv.org/abs/2505.20481</guid>
<content:encoded><![CDATA[
arXiv:2505.20481v1 Announce Type: cross 
Abstract: Accurate ECG interpretation is vital, yet complex cardiac data and "black-box" AI models limit clinical utility. Inspired by Transformer architectures' success in NLP for understanding sequential data, we frame ECG as the heart's unique "language" of temporal patterns. We present CardioPatternFormer, a novel Transformer-based model for interpretable ECG classification. It employs a sophisticated attention mechanism to precisely identify and classify diverse cardiac patterns, excelling at discerning subtle anomalies and distinguishing multiple co-occurring conditions. This pattern-guided attention provides clear insights by highlighting influential signal regions, effectively allowing the "heart to talk" through transparent interpretations. CardioPatternFormer demonstrates robust performance on challenging ECGs, including complex multi-pathology cases. Its interpretability via attention maps enables clinicians to understand the model's rationale, fostering trust and aiding informed diagnostic decisions. This work offers a powerful, transparent solution for advanced ECG analysis, paving the way for more reliable and clinically actionable AI in cardiology.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ControlTac: Force- and Position-Controlled Tactile Data Augmentation with a Single Reference Image</title>
<link>https://arxiv.org/abs/2505.20498</link>
<guid>https://arxiv.org/abs/2505.20498</guid>
<content:encoded><![CDATA[
arXiv:2505.20498v1 Announce Type: cross 
Abstract: Vision-based tactile sensing has been widely used in perception, reconstruction, and robotic manipulation. However, collecting large-scale tactile data remains costly due to the localized nature of sensor-object interactions and inconsistencies across sensor instances. Existing approaches to scaling tactile data, such as simulation and free-form tactile generation, often suffer from unrealistic output and poor transferability to downstream tasks.To address this, we propose ControlTac, a two-stage controllable framework that generates realistic tactile images conditioned on a single reference tactile image, contact force, and contact position. With those physical priors as control input, ControlTac generates physically plausible and varied tactile images that can be used for effective data augmentation. Through experiments on three downstream tasks, we demonstrate that ControlTac can effectively augment tactile datasets and lead to consistent gains. Our three real-world experiments further validate the practical utility of our approach. Project page: https://dongyuluo.github.io/controltac.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Embodied AI with Foundation Models for Mobile Service Robots: A Systematic Review</title>
<link>https://arxiv.org/abs/2505.20503</link>
<guid>https://arxiv.org/abs/2505.20503</guid>
<content:encoded><![CDATA[
arXiv:2505.20503v1 Announce Type: cross 
Abstract: Rapid advancements in foundation models, including Large Language Models, Vision-Language Models, Multimodal Large Language Models, and Vision-Language-Action Models have opened new avenues for embodied AI in mobile service robotics. By combining foundation models with the principles of embodied AI, where intelligent systems perceive, reason, and act through physical interactions, robots can improve understanding, adapt to, and execute complex tasks in dynamic real-world environments. However, embodied AI in mobile service robots continues to face key challenges, including multimodal sensor fusion, real-time decision-making under uncertainty, task generalization, and effective human-robot interactions (HRI). In this paper, we present the first systematic review of the integration of foundation models in mobile service robotics, identifying key open challenges in embodied AI and examining how foundation models can address them. Namely, we explore the role of such models in enabling real-time sensor fusion, language-conditioned control, and adaptive task execution. Furthermore, we discuss real-world applications in the domestic assistance, healthcare, and service automation sectors, demonstrating the transformative impact of foundation models on service robotics. We also include potential future research directions, emphasizing the need for predictive scaling laws, autonomous long-term adaptation, and cross-embodiment generalization to enable scalable, efficient, and robust deployment of foundation models in human-centric robotic systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling over Scaling: Exploring Test-Time Scaling Pareto in Large Reasoning Models</title>
<link>https://arxiv.org/abs/2505.20522</link>
<guid>https://arxiv.org/abs/2505.20522</guid>
<content:encoded><![CDATA[
arXiv:2505.20522v1 Announce Type: cross 
Abstract: Large reasoning models (LRMs) have exhibited the capacity of enhancing reasoning performance via internal test-time scaling. Building upon this, a promising direction is to further scale test-time compute to unlock even greater reasoning capabilities. However, as we push these scaling boundaries, systematically understanding the practical limits and achieving optimal resource allocation becomes a critical challenge. In this paper, we investigate the scaling Pareto of test-time scaling and introduce the Test-Time Scaling Performance Model (TTSPM). We theoretically analyze two fundamental paradigms for such extended scaling, parallel scaling and sequential scaling, from a probabilistic modeling perspective. Our primary contribution is the derivation of the saturation point on the scaling budget for both strategies, identifying thresholds beyond which additional computation yields diminishing returns. Remarkably, despite their distinct mechanisms, both paradigms converge to a unified mathematical structure in their upper bounds. We empirically validate our theoretical findings on challenging reasoning benchmarks, including AIME, MATH-500, and GPQA, demonstrating the practical utility of these bounds for test-time resource allocation. We hope that this work provides insights into the cost-benefit trade-offs of test-time scaling, guiding the development of more resource-efficient inference strategies for large reasoning models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training Articulatory Inversion Models for Inter-Speaker Consistency</title>
<link>https://arxiv.org/abs/2505.20529</link>
<guid>https://arxiv.org/abs/2505.20529</guid>
<content:encoded><![CDATA[
arXiv:2505.20529v1 Announce Type: cross 
Abstract: Acoustic-to-Articulatory Inversion (AAI) attempts to model the inverse mapping from speech to articulation. Exact articulatory prediction from speech alone may be impossible, as speakers can choose different forms of articulation seemingly without reference to their vocal tract structure. However, once a speaker has selected an articulatory form, their productions vary minimally. Recent works in AAI have proposed adapting Self-Supervised Learning (SSL) models to single-speaker datasets, claiming that these single-speaker models provide a universal articulatory template. In this paper, we investigate whether SSL-adapted models trained on single and multi-speaker data produce articulatory targets which are consistent across speaker identities for English and Russian. We do this through the use of a novel evaluation method which extracts articulatory targets using minimal pair sets. We also present a training method which can improve inter-speaker consistency using only speech data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Covariate-Adjusted Deep Causal Learning for Heterogeneous Panel Data Models</title>
<link>https://arxiv.org/abs/2505.20536</link>
<guid>https://arxiv.org/abs/2505.20536</guid>
<content:encoded><![CDATA[
arXiv:2505.20536v1 Announce Type: cross 
Abstract: This paper studies the task of estimating heterogeneous treatment effects in causal panel data models, in the presence of covariate effects. We propose a novel Covariate-Adjusted Deep Causal Learning (CoDEAL) for panel data models, that employs flexible model structures and powerful neural network architectures to cohesively deal with the underlying heterogeneity and nonlinearity of both panel units and covariate effects. The proposed CoDEAL integrates nonlinear covariate effect components (parameterized by a feed-forward neural network) with nonlinear factor structures (modeled by a multi-output autoencoder) to form a heterogeneous causal panel model. The nonlinear covariate component offers a flexible framework for capturing the complex influences of covariates on outcomes. The nonlinear factor analysis enables CoDEAL to effectively capture both cross-sectional and temporal dependencies inherent in the data panel. This latent structural information is subsequently integrated into a customized matrix completion algorithm, thereby facilitating more accurate imputation of missing counterfactual outcomes. Moreover, the use of a multi-output autoencoder explicitly accounts for heterogeneity across units and enhances the model interpretability of the latent factors. We establish theoretical guarantees on the convergence of the estimated counterfactuals, and demonstrate the compelling performance of the proposed method using extensive simulation studies and a real data application.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AstroVisBench: A Code Benchmark for Scientific Computing and Visualization in Astronomy</title>
<link>https://arxiv.org/abs/2505.20538</link>
<guid>https://arxiv.org/abs/2505.20538</guid>
<content:encoded><![CDATA[
arXiv:2505.20538v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are being explored for applications in scientific research, including their capabilities to synthesize literature, answer research questions, generate research ideas, and even conduct computational experiments. Ultimately, our goal is for these to help scientists derive novel scientific insights. In many areas of science, such insights often arise from processing and visualizing data to understand its patterns. However, evaluating whether an LLM-mediated scientific workflow produces outputs conveying the correct scientific insights is challenging to evaluate and has not been addressed in past work. We introduce AstroVisBench, the first benchmark for both scientific computing and visualization in the astronomy domain. AstroVisBench judges a language model's ability to both (1) create astronomy-specific workflows to process and analyze data and (2) visualize the results of these workflows through complex plots. Our evaluation of visualizations uses a novel LLM-as-a-judge workflow, which is validated against annotation by five professional astronomers. Using AstroVisBench we present an evaluation of state-of-the-art language models, showing a significant gap in their ability to engage in astronomy research as useful assistants. This evaluation provides a strong end-to-end evaluation for AI scientists that offers a path forward for the development of visualization-based workflows, which are central to a broad range of domains from physics to biology.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieval Visual Contrastive Decoding to Mitigate Object Hallucinations in Large Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20569</link>
<guid>https://arxiv.org/abs/2505.20569</guid>
<content:encoded><![CDATA[
arXiv:2505.20569v1 Announce Type: cross 
Abstract: Despite significant advancements in Large Vision-Language Models, Object Hallucination (OH) remains a persistent challenge. Building upon prior studies on contrastive decoding that address this issue without requiring additional model training, we introduce RVCD (Retrieval Visual Contrastive Decoding), an advanced method to suppress OH. RVCD leverages both negative and positive images at the logit level, explicitly referencing AI-generated images designed to represent a single concept. Our approach demonstrates substantial improvements over existing decoding-based methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Classification In-Context in Spanish</title>
<link>https://arxiv.org/abs/2505.20571</link>
<guid>https://arxiv.org/abs/2505.20571</guid>
<content:encoded><![CDATA[
arXiv:2505.20571v1 Announce Type: cross 
Abstract: Classifying customer feedback into distinct emotion categories is essential for understanding sentiment and improving customer experience. In this paper, we classify customer feedback in Spanish into three emotion categories--positive, neutral, and negative--using advanced NLP and ML techniques. Traditional methods translate feedback from widely spoken languages to less common ones, resulting in a loss of semantic integrity and contextual nuances inherent to the original language. To address this limitation, we propose a hybrid approach that combines TF-IDF with BERT embeddings, effectively transforming Spanish text into rich numerical representations that preserve the semantic depth of the original language by using a Custom Stacking Ensemble (CSE) approach. To evaluate emotion classification, we utilize a range of models, including Logistic Regression, KNN, Bagging classifier with LGBM, and AdaBoost. The CSE model combines these classifiers as base models and uses a one-vs-all Logistic Regression as the meta-model. Our experimental results demonstrate that CSE significantly outperforms the individual and BERT model, achieving a test accuracy of 93.3% on the native Spanish dataset--higher than the accuracy obtained from the translated version. These findings underscore the challenges of emotion classification in Spanish and highlight the advantages of combining vectorization techniques like TF-IDF with BERT for improved accuracy. Our results provide valuable insights for businesses seeking to leverage emotion classification to enhance customer feedback analysis and service improvements.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Balancing Performance and Costs in Best Arm Identification</title>
<link>https://arxiv.org/abs/2505.20583</link>
<guid>https://arxiv.org/abs/2505.20583</guid>
<content:encoded><![CDATA[
arXiv:2505.20583v1 Announce Type: cross 
Abstract: We consider the problem of identifying the best arm in a multi-armed bandit model. Despite a wealth of literature in the traditional fixed budget and fixed confidence regimes of the best arm identification problem, it still remains a mystery to most practitioners as to how to choose an approach and corresponding budget or confidence parameter. We propose a new formalism to avoid this dilemma altogether by minimizing a risk functional which explicitly balances the performance of the recommended arm and the cost incurred by learning this arm. In this framework, a cost is incurred for each observation during the sampling phase, and upon recommending an arm, a performance penalty is incurred for identifying a suboptimal arm. The learner's goal is to minimize the sum of the penalty and cost. This new regime mirrors the priorities of many practitioners, e.g. maximizing profit in an A/B testing framework, better than classical fixed budget or confidence settings. We derive theoretical lower bounds for the risk of each of two choices for the performance penalty, the probability of misidentification and the simple regret, and propose an algorithm called DBCARE to match these lower bounds up to polylog factors on nearly all problem instances. We then demonstrate the performance of DBCARE on a number of simulated models, comparing to fixed budget and confidence algorithms to show the shortfalls of existing BAI paradigms on this problem.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>InstGenIE: Generative Image Editing Made Efficient with Mask-aware Caching and Scheduling</title>
<link>https://arxiv.org/abs/2505.20600</link>
<guid>https://arxiv.org/abs/2505.20600</guid>
<content:encoded><![CDATA[
arXiv:2505.20600v1 Announce Type: cross 
Abstract: Generative image editing using diffusion models has become a prevalent application in today's AI cloud services. In production environments, image editing typically involves a mask that specifies the regions of an image template to be edited. The use of masks provides direct control over the editing process and introduces sparsity in the model inference. In this paper, we present InstGenIE, a system that efficiently serves image editing requests. The key insight behind InstGenIE is that image editing only modifies the masked regions of image templates while preserving the original content in the unmasked areas. Driven by this insight, InstGenIE judiciously skips redundant computations associated with the unmasked areas by reusing cached intermediate activations from previous inferences. To mitigate the high cache loading overhead, InstGenIE employs a bubble-free pipeline scheme that overlaps computation with cache loading. Additionally, to reduce queuing latency in online serving while improving the GPU utilization, InstGenIE proposes a novel continuous batching strategy for diffusion model serving, allowing newly arrived requests to join the running batch in just one step of denoising computation, without waiting for the entire batch to complete. As heterogeneous masks induce imbalanced loads, InstGenIE also develops a load balancing strategy that takes into account the loads of both computation and cache loading. Collectively, InstGenIE outperforms state-of-the-art diffusion serving systems for image editing, achieving up to 3x higher throughput and reducing average request latency by up to 14.7x while ensuring image quality.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Roboflow100-VL: A Multi-Domain Object Detection Benchmark for Vision-Language Models</title>
<link>https://arxiv.org/abs/2505.20612</link>
<guid>https://arxiv.org/abs/2505.20612</guid>
<content:encoded><![CDATA[
arXiv:2505.20612v1 Announce Type: cross 
Abstract: Vision-language models (VLMs) trained on internet-scale data achieve remarkable zero-shot detection performance on common objects like car, truck, and pedestrian. However, state-of-the-art models still struggle to generalize to out-of-distribution classes, tasks and imaging modalities not typically found in their pre-training. Rather than simply re-training VLMs on more visual data, we argue that one should align VLMs to new concepts with annotation instructions containing a few visual examples and rich textual descriptions. To this end, we introduce Roboflow100-VL, a large-scale collection of 100 multi-modal object detection datasets with diverse concepts not commonly found in VLM pre-training. We evaluate state-of-the-art models on our benchmark in zero-shot, few-shot, semi-supervised, and fully-supervised settings, allowing for comparison across data regimes. Notably, we find that VLMs like GroundingDINO and Qwen2.5-VL achieve less than 2% zero-shot accuracy on challenging medical imaging datasets within Roboflow100-VL, demonstrating the need for few-shot concept alignment. Our code and dataset are available at https://github.com/roboflow/rf100-vl/ and https://universe.roboflow.com/rf100-vl/
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>REAL-Prover: Retrieval Augmented Lean Prover for Mathematical Reasoning</title>
<link>https://arxiv.org/abs/2505.20613</link>
<guid>https://arxiv.org/abs/2505.20613</guid>
<content:encoded><![CDATA[
arXiv:2505.20613v1 Announce Type: cross 
Abstract: Nowadays, formal theorem provers have made monumental progress on high-school and competition-level mathematics, but few of them generalize to more advanced mathematics. In this paper, we present REAL-Prover, a new open-source stepwise theorem prover for Lean 4 to push this boundary. This prover, based on our fine-tuned large language model (REAL-Prover-v1) and integrated with a retrieval system (Leansearch-PS), notably boosts performance on solving college-level mathematics problems. To train REAL-Prover-v1, we developed HERALD-AF, a data extraction pipeline that converts natural language math problems into formal statements, and a new open-source Lean 4 interactive environment (Jixia-interactive) to facilitate synthesis data collection. In our experiments, our prover using only supervised fine-tune achieves competitive results with a 23.7% success rate (Pass@64) on the ProofNet dataset-comparable to state-of-the-art (SOTA) models. To further evaluate our approach, we introduce FATE-M, a new benchmark focused on algebraic problems, where our prover achieves a SOTA success rate of 56.7% (Pass@64).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Intelligent Incident Hypertension Prediction in Obstructive Sleep Apnea</title>
<link>https://arxiv.org/abs/2505.20615</link>
<guid>https://arxiv.org/abs/2505.20615</guid>
<content:encoded><![CDATA[
arXiv:2505.20615v1 Announce Type: cross 
Abstract: Obstructive sleep apnea (OSA) is a significant risk factor for hypertension, primarily due to intermittent hypoxia and sleep fragmentation. Predicting whether individuals with OSA will develop hypertension within five years remains a complex challenge. This study introduces a novel deep learning approach that integrates Discrete Cosine Transform (DCT)-based transfer learning to enhance prediction accuracy. We are the first to incorporate all polysomnography signals together for hypertension prediction, leveraging their collective information to improve model performance. Features were extracted from these signals and transformed into a 2D representation to utilize pre-trained 2D neural networks such as MobileNet, EfficientNet, and ResNet variants. To further improve feature learning, we introduced a DCT layer, which transforms input features into a frequency-based representation, preserving essential spectral information, decorrelating features, and enhancing robustness to noise. This frequency-domain approach, coupled with transfer learning, is especially beneficial for limited medical datasets, as it leverages rich representations from pre-trained networks to improve generalization. By strategically placing the DCT layer at deeper truncation depths within EfficientNet, our model achieved a best area under the curve (AUC) of 72.88%, demonstrating the effectiveness of frequency-domain feature extraction and transfer learning in predicting hypertension risk in OSA patients over a five-year period.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SeqPO-SiMT: Sequential Policy Optimization for Simultaneous Machine Translation</title>
<link>https://arxiv.org/abs/2505.20622</link>
<guid>https://arxiv.org/abs/2505.20622</guid>
<content:encoded><![CDATA[
arXiv:2505.20622v1 Announce Type: cross 
Abstract: We present Sequential Policy Optimization for Simultaneous Machine Translation (SeqPO-SiMT), a new policy optimization framework that defines the simultaneous machine translation (SiMT) task as a sequential decision making problem, incorporating a tailored reward to enhance translation quality while reducing latency. In contrast to popular Reinforcement Learning from Human Feedback (RLHF) methods, such as PPO and DPO, which are typically applied in single-step tasks, SeqPO-SiMT effectively tackles the multi-step SiMT task. This intuitive framework allows the SiMT LLMs to simulate and refine the SiMT process using a tailored reward. We conduct experiments on six datasets from diverse domains for En to Zh and Zh to En SiMT tasks, demonstrating that SeqPO-SiMT consistently achieves significantly higher translation quality with lower latency. In particular, SeqPO-SiMT outperforms the supervised fine-tuning (SFT) model by 1.13 points in COMET, while reducing the Average Lagging by 6.17 in the NEWSTEST2021 En to Zh dataset. While SiMT operates with far less context than offline translation, the SiMT results of SeqPO-SiMT on 7B LLM surprisingly rival the offline translation of high-performing LLMs, including Qwen-2.5-7B-Instruct and LLaMA-3-8B-Instruct.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Incorporating Flexible Image Conditioning into Text-to-Video Diffusion Models without Training</title>
<link>https://arxiv.org/abs/2505.20629</link>
<guid>https://arxiv.org/abs/2505.20629</guid>
<content:encoded><![CDATA[
arXiv:2505.20629v1 Announce Type: cross 
Abstract: Text-image-to-video (TI2V) generation is a critical problem for controllable video generation using both semantic and visual conditions. Most existing methods typically add visual conditions to text-to-video (T2V) foundation models by finetuning, which is costly in resources and only limited to a few predefined conditioning settings. To tackle this issue, we introduce a unified formulation for TI2V generation with flexible visual conditioning. Furthermore, we propose an innovative training-free approach, dubbed FlexTI2V, that can condition T2V foundation models on an arbitrary amount of images at arbitrary positions. Specifically, we firstly invert the condition images to noisy representation in a latent space. Then, in the denoising process of T2V models, our method uses a novel random patch swapping strategy to incorporate visual features into video representations through local image patches. To balance creativity and fidelity, we use a dynamic control mechanism to adjust the strength of visual conditioning to each video frame. Extensive experiments validate that our method surpasses previous training-free image conditioning methods by a notable margin. We also show more insights of our method by detailed ablation study and analysis.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Learning for Large Language Models</title>
<link>https://arxiv.org/abs/2505.20633</link>
<guid>https://arxiv.org/abs/2505.20633</guid>
<content:encoded><![CDATA[
arXiv:2505.20633v1 Announce Type: cross 
Abstract: While Large Language Models (LLMs) have exhibited remarkable emergent capabilities through extensive pre-training, they still face critical limitations in generalizing to specialized domains and handling diverse linguistic variations, known as distribution shifts. In this paper, we propose a Test-Time Learning (TTL) paradigm for LLMs, namely TLM, which dynamically adapts LLMs to target domains using only unlabeled test data during testing. Specifically, we first provide empirical evidence and theoretical insights to reveal that more accurate predictions from LLMs can be achieved by minimizing the input perplexity of the unlabeled test data. Based on this insight, we formulate the Test-Time Learning process of LLMs as input perplexity minimization, enabling self-supervised enhancement of LLM performance. Furthermore, we observe that high-perplexity samples tend to be more informative for model optimization. Accordingly, we introduce a Sample Efficient Learning Strategy that actively selects and emphasizes these high-perplexity samples for test-time updates. Lastly, to mitigate catastrophic forgetting and ensure adaptation stability, we adopt Low-Rank Adaptation (LoRA) instead of full-parameter optimization, which allows lightweight model updates while preserving more original knowledge from the model. We introduce the AdaptEval benchmark for TTL and demonstrate through experiments that TLM improves performance by at least 20% compared to original LLMs on domain knowledge adaptation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Moment Expansions of the Energy Distance</title>
<link>https://arxiv.org/abs/2505.20647</link>
<guid>https://arxiv.org/abs/2505.20647</guid>
<content:encoded><![CDATA[
arXiv:2505.20647v1 Announce Type: cross 
Abstract: The energy distance is used to test distributional equality, and as a loss function in machine learning. While $D^2(X, Y)=0$ only when $X\sim Y$, the sensitivity to different moments is of practical importance. This work considers $D^2(X, Y)$ in the case where the distributions are close. In this regime, $D^2(X, Y)$ is more sensitive to differences in the means $\bar{X}-\bar{Y}$, than differences in the covariances $\Delta$. This is due to the structure of the energy distance and is independent of dimension. The sensitivity to on versus off diagonal components of $\Delta$ is examined when $X$ and $Y$ are close to isotropic. Here a dimension dependent averaging occurs and, in many cases, off diagonal correlations contribute significantly less. Numerical results verify these relationships hold even when distributional assumptions are not strictly met.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Guided Reinforcement Learning: Addressing Training Bottlenecks through Policy Modulation</title>
<link>https://arxiv.org/abs/2505.20671</link>
<guid>https://arxiv.org/abs/2505.20671</guid>
<content:encoded><![CDATA[
arXiv:2505.20671v1 Announce Type: cross 
Abstract: While reinforcement learning (RL) has achieved notable success in various domains, training effective policies for complex tasks remains challenging. Agents often converge to local optima and fail to maximize long-term rewards. Existing approaches to mitigate training bottlenecks typically fall into two categories: (i) Automated policy refinement, which identifies critical states from past trajectories to guide policy updates, but suffers from costly and uncertain model training; and (ii) Human-in-the-loop refinement, where human feedback is used to correct agent behavior, but this does not scale well to environments with large or continuous action spaces. In this work, we design a large language model-guided policy modulation framework that leverages LLMs to improve RL training without additional model training or human intervention. We first prompt an LLM to identify critical states from a sub-optimal agent's trajectories. Based on these states, the LLM then provides action suggestions and assigns implicit rewards to guide policy refinement. Experiments across standard RL benchmarks demonstrate that our method outperforms state-of-the-art baselines, highlighting the effectiveness of LLM-based explanations in addressing RL training bottlenecks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SELF-PERCEPT: Introspection Improves Large Language Models' Detection of Multi-Person Mental Manipulation in Conversations</title>
<link>https://arxiv.org/abs/2505.20679</link>
<guid>https://arxiv.org/abs/2505.20679</guid>
<content:encoded><![CDATA[
arXiv:2505.20679v1 Announce Type: cross 
Abstract: Mental manipulation is a subtle yet pervasive form of abuse in interpersonal communication, making its detection critical for safeguarding potential victims. However, due to manipulation's nuanced and context-specific nature, identifying manipulative language in complex, multi-turn, and multi-person conversations remains a significant challenge for large language models (LLMs). To address this gap, we introduce the MultiManip dataset, comprising 220 multi-turn, multi-person dialogues balanced between manipulative and non-manipulative interactions, all drawn from reality shows that mimic real-world scenarios. For manipulative interactions, it includes 11 distinct manipulations depicting real-life scenarios. We conduct extensive evaluations of state-of-the-art LLMs, such as GPT-4o and Llama-3.1-8B, employing various prompting strategies. Despite their capabilities, these models often struggle to detect manipulation effectively. To overcome this limitation, we propose SELF-PERCEPT, a novel, two-stage prompting framework inspired by Self-Perception Theory, demonstrating strong performance in detecting multi-person, multi-turn mental manipulation. Our code and data are publicly available at https://github.com/danushkhanna/self-percept .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A False Discovery Rate Control Method Using a Fully Connected Hidden Markov Random Field for Neuroimaging Data</title>
<link>https://arxiv.org/abs/2505.20688</link>
<guid>https://arxiv.org/abs/2505.20688</guid>
<content:encoded><![CDATA[
arXiv:2505.20688v1 Announce Type: cross 
Abstract: False discovery rate (FDR) control methods are essential for voxel-wise multiple testing in neuroimaging data analysis, where hundreds of thousands or even millions of tests are conducted to detect brain regions associated with disease-related changes. Classical FDR control methods (e.g., BH, q-value, and LocalFDR) assume independence among tests and often lead to high false non-discovery rates (FNR). Although various spatial FDR control methods have been developed to improve power, they still fall short in jointly addressing three major challenges in neuroimaging applications: capturing complex spatial dependencies, maintaining low variability in both false discovery proportion (FDP) and false non-discovery proportion (FNP) across replications, and achieving computational scalability for high-resolution data. To address these challenges, we propose fcHMRF-LIS, a powerful, stable, and scalable spatial FDR control method for voxel-wise multiple testing. It integrates the local index of significance (LIS)-based testing procedure with a novel fully connected hidden Markov random field (fcHMRF) designed to model complex spatial structures using a parsimonious parameterization. We develop an efficient expectation-maximization algorithm incorporating mean-field approximation, the Conditional Random Fields as Recurrent Neural Networks (CRF-RNN) technique, and permutohedral lattice filtering, reducing the computational complexity from quadratic to linear in the number of tests. Extensive simulations demonstrate that fcHMRF-LIS achieves accurate FDR control, lower FNR, reduced variability in FDP and FNP, and a higher number of true positives compared to existing methods. Applied to an FDG-PET dataset from the Alzheimer's Disease Neuroimaging Initiative, fcHMRF-LIS identifies neurobiologically relevant brain regions and offers notable advantages in computational efficiency.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Phir Hera Fairy: An English Fairytaler is a Strong Faker of Fluent Speech in Low-Resource Indian Languages</title>
<link>https://arxiv.org/abs/2505.20693</link>
<guid>https://arxiv.org/abs/2505.20693</guid>
<content:encoded><![CDATA[
arXiv:2505.20693v1 Announce Type: cross 
Abstract: What happens when an English Fairytaler is fine-tuned on Indian languages? We evaluate how the English F5-TTS model adapts to 11 Indian languages, measuring polyglot fluency, voice-cloning, style-cloning, and code-mixing. We compare: (i) training from scratch, (ii) fine-tuning English F5 on Indian data, and (iii) fine-tuning on both Indian and English data to prevent forgetting. Fine-tuning with only Indian data proves most effective and the resultant IN-F5 is a near-human polyglot; that enables speakers of one language (e.g., Odia) to fluently speak in another (e.g., Hindi). Our results show English pretraining aids low-resource TTS in reaching human parity. To aid progress in other low-resource languages, we study data-constrained setups and arrive at a compute optimal strategy. Finally, we show IN-F5 can synthesize unseen languages like Bhojpuri and Tulu using a human-in-the-loop approach for zero-resource TTS via synthetic data generation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temporal Saliency-Guided Distillation: A Scalable Framework for Distilling Video Datasets</title>
<link>https://arxiv.org/abs/2505.20694</link>
<guid>https://arxiv.org/abs/2505.20694</guid>
<content:encoded><![CDATA[
arXiv:2505.20694v1 Announce Type: cross 
Abstract: Dataset distillation (DD) has emerged as a powerful paradigm for dataset compression, enabling the synthesis of compact surrogate datasets that approximate the training utility of large-scale ones. While significant progress has been achieved in distilling image datasets, extending DD to the video domain remains challenging due to the high dimensionality and temporal complexity inherent in video data. Existing video distillation (VD) methods often suffer from excessive computational costs and struggle to preserve temporal dynamics, as na\"ive extensions of image-based approaches typically lead to degraded performance. In this paper, we propose a novel uni-level video dataset distillation framework that directly optimizes synthetic videos with respect to a pre-trained model. To address temporal redundancy and enhance motion preservation, we introduce a temporal saliency-guided filtering mechanism that leverages inter-frame differences to guide the distillation process, encouraging the retention of informative temporal cues while suppressing frame-level redundancy. Extensive experiments on standard video benchmarks demonstrate that our method achieves state-of-the-art performance, bridging the gap between real and distilled video data and offering a scalable solution for video dataset compression.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Time-Series Learning for Proactive Fault Prediction in Distributed Systems with Deep Neural Structures</title>
<link>https://arxiv.org/abs/2505.20705</link>
<guid>https://arxiv.org/abs/2505.20705</guid>
<content:encoded><![CDATA[
arXiv:2505.20705v1 Announce Type: cross 
Abstract: This paper addresses the challenges of fault prediction and delayed response in distributed systems by proposing an intelligent prediction method based on temporal feature learning. The method takes multi-dimensional performance metric sequences as input. We use a Gated Recurrent Unit (GRU) to model the evolution of system states over time. An attention mechanism is then applied to enhance key temporal segments, improving the model's ability to identify potential faults. On this basis, a feedforward neural network is designed to perform the final classification, enabling early warning of system failures. To validate the effectiveness of the proposed approach, comparative experiments and ablation analyses were conducted using data from a large-scale real-world cloud system. The experimental results show that the model outperforms various mainstream time-series models in terms of Accuracy, F1-Score, and AUC. This demonstrates strong prediction capability and stability. Furthermore, the loss function curve confirms the convergence and reliability of the training process. It indicates that the proposed method effectively learns system behavior patterns and achieves efficient fault detection.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wideband RF Radiance Field Modeling Using Frequency-embedded 3D Gaussian Splatting</title>
<link>https://arxiv.org/abs/2505.20714</link>
<guid>https://arxiv.org/abs/2505.20714</guid>
<content:encoded><![CDATA[
arXiv:2505.20714v1 Announce Type: cross 
Abstract: This paper presents an innovative frequency-embedded 3D Gaussian splatting (3DGS) algorithm for wideband radio-frequency (RF) radiance field modeling, offering an advancement over the existing works limited to single-frequency modeling. Grounded in fundamental physics, we uncover the complex relationship between EM wave propagation behaviors and RF frequencies. Inspired by this, we design an EM feature network with attenuation and radiance modules to learn the complex relationships between RF frequencies and the key properties of each 3D Gaussian, specifically the attenuation factor and RF signal intensity. By training the frequency-embedded 3DGS model, we can efficiently reconstruct RF radiance fields at arbitrary unknown frequencies within a given 3D environment. Finally, we propose a large-scale power angular spectrum (PAS) dataset containing 50000 samples ranging from 1 to 100 GHz in 6 indoor environments, and conduct extensive experiments to verify the effectiveness of our method. Our approach achieves an average Structural Similarity Index Measure (SSIM) up to 0.72, and a significant improvement up to 17.8% compared to the current state-of-the-art (SOTA) methods trained on individual test frequencies. Additionally, our method achieves an SSIM of 0.70 without prior training on these frequencies, which represents only a 2.8% performance drop compared to models trained with full PAS data. This demonstrates our model's capability to estimate PAS at unknown frequencies. For related code and datasets, please refer to https://github.com/sim-2-real/Wideband3DGS.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LeDiFlow: Learned Distribution-guided Flow Matching to Accelerate Image Generation</title>
<link>https://arxiv.org/abs/2505.20723</link>
<guid>https://arxiv.org/abs/2505.20723</guid>
<content:encoded><![CDATA[
arXiv:2505.20723v1 Announce Type: cross 
Abstract: Enhancing the efficiency of high-quality image generation using Diffusion Models (DMs) is a significant challenge due to the iterative nature of the process. Flow Matching (FM) is emerging as a powerful generative modeling paradigm based on a simulation-free training objective instead of a score-based one used in DMs. Typical FM approaches rely on a Gaussian distribution prior, which induces curved, conditional probability paths between the prior and target data distribution. These curved paths pose a challenge for the Ordinary Differential Equation (ODE) solver, requiring a large number of inference calls to the flow prediction network. To address this issue, we present Learned Distribution-guided Flow Matching (LeDiFlow), a novel scalable method for training FM-based image generation models using a better-suited prior distribution learned via a regression-based auxiliary model. By initializing the ODE solver with a prior closer to the target data distribution, LeDiFlow enables the learning of more computationally tractable probability paths. These paths directly translate to fewer solver steps needed for high-quality image generation at inference time. Our method utilizes a State-Of-The-Art (SOTA) transformer architecture combined with latent space sampling and can be trained on a consumer workstation. We empirically demonstrate that LeDiFlow remarkably outperforms the respective FM baselines. For instance, when operating directly on pixels, our model accelerates inference by up to 3.75x compared to the corresponding pixel-space baseline. Simultaneously, our latent FM model enhances image quality on average by 1.32x in CLIP Maximum Mean Discrepancy (CMMD) metric against its respective baseline.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What LLMs Miss in Recommendations: Bridging the Gap with Retrieval-Augmented Collaborative Signals</title>
<link>https://arxiv.org/abs/2505.20730</link>
<guid>https://arxiv.org/abs/2505.20730</guid>
<content:encoded><![CDATA[
arXiv:2505.20730v1 Announce Type: cross 
Abstract: User-item interactions contain rich collaborative signals that form the backbone of many successful recommender systems. While recent work has explored the use of large language models (LLMs) for recommendation, it remains unclear whether LLMs can effectively reason over this type of collaborative information. In this paper, we conduct a systematic comparison between LLMs and classical matrix factorization (MF) models to assess LLMs' ability to leverage user-item interaction data. We further introduce a simple retrieval-augmented generation (RAG) method that enhances LLMs by grounding their predictions in structured interaction data. Our experiments reveal that current LLMs often fall short in capturing collaborative patterns inherent to MF models, but that our RAG-based approach substantially improves recommendation quality-highlighting a promising direction for future LLM-based recommenders.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-supervised Clustering Through Representation Learning of Large-scale EHR Data</title>
<link>https://arxiv.org/abs/2505.20731</link>
<guid>https://arxiv.org/abs/2505.20731</guid>
<content:encoded><![CDATA[
arXiv:2505.20731v1 Announce Type: cross 
Abstract: Electronic Health Records (EHR) offer rich real-world data for personalized medicine, providing insights into disease progression, treatment responses, and patient outcomes. However, their sparsity, heterogeneity, and high dimensionality make them difficult to model, while the lack of standardized ground truth further complicates predictive modeling. To address these challenges, we propose SCORE, a semi-supervised representation learning framework that captures multi-domain disease profiles through patient embeddings. SCORE employs a Poisson-Adapted Latent factor Mixture (PALM) Model with pre-trained code embeddings to characterize codified features and extract meaningful patient phenotypes and embeddings. To handle the computational challenges of large-scale data, it introduces a hybrid Expectation-Maximization (EM) and Gaussian Variational Approximation (GVA) algorithm, leveraging limited labeled data to refine estimates on a vast pool of unlabeled samples. We theoretically establish the convergence of this hybrid approach, quantify GVA errors, and derive SCORE's error rate under diverging embedding dimensions. Our analysis shows that incorporating unlabeled data enhances accuracy and reduces sensitivity to label scarcity. Extensive simulations confirm SCORE's superior finite-sample performance over existing methods. Finally, we apply SCORE to predict disability status for patients with multiple sclerosis (MS) using partially labeled EHR data, demonstrating that it produces more informative and predictive patient embeddings for multiple MS-related conditions compared to existing approaches.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPA-RL: Reinforcing LLM Agents via Stepwise Progress Attribution</title>
<link>https://arxiv.org/abs/2505.20732</link>
<guid>https://arxiv.org/abs/2505.20732</guid>
<content:encoded><![CDATA[
arXiv:2505.20732v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) holds significant promise for training LLM agents to handle complex, goal-oriented tasks that require multi-step interactions with external environments. However, a critical challenge when applying RL to these agentic tasks arises from delayed rewards: feedback signals are typically available only after the entire task is completed. This makes it non-trivial to assign delayed rewards to earlier actions, providing insufficient guidance regarding environmental constraints and hindering agent training. In this work, we draw on the insight that the ultimate completion of a task emerges from the cumulative progress an agent makes across individual steps. We propose Stepwise Progress Attribution (SPA), a general reward redistribution framework that decomposes the final reward into stepwise contributions, each reflecting its incremental progress toward overall task completion. To achieve this, we train a progress estimator that accumulates stepwise contributions over a trajectory to match the task completion. During policy optimization, we combine the estimated per-step contribution with a grounding signal for actions executed in the environment as the fine-grained, intermediate reward for effective agent training. Extensive experiments on common agent benchmarks (including Webshop, ALFWorld, and VirtualHome) demonstrate that SPA consistently outperforms the state-of-the-art method in both success rate (+2.5\% on average) and grounding accuracy (+1.9\% on average). Further analyses demonstrate that our method remarkably provides more effective intermediate rewards for RL training. Our code is available at https://github.com/WangHanLinHenry/SPA-RL-Agent.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Model Hidden Representations for Heart Rate Estimation from Auscultation</title>
<link>https://arxiv.org/abs/2505.20745</link>
<guid>https://arxiv.org/abs/2505.20745</guid>
<content:encoded><![CDATA[
arXiv:2505.20745v1 Announce Type: cross 
Abstract: Auscultation, particularly heart sound, is a non-invasive technique that provides essential vital sign information. Recently, self-supervised acoustic representation foundation models (FMs) have been proposed to offer insights into acoustics-based vital signs. However, there has been little exploration of the extent to which auscultation is encoded in these pre-trained FM representations. In this work, using a publicly available phonocardiogram (PCG) dataset and a heart rate (HR) estimation model, we conduct a layer-wise investigation of six acoustic representation FMs: HuBERT, wav2vec2, wavLM, Whisper, Contrastive Language-Audio Pretraining (CLAP), and an in-house CLAP model. Additionally, we implement the baseline method from Nie et al., 2024 (which relies on acoustic features) and show that overall, representation vectors from pre-trained foundation models (FMs) offer comparable performance to the baseline. Notably, HR estimation using the representations from the audio encoder of the in-house CLAP model outperforms the results obtained from the baseline, achieving a lower mean absolute error (MAE) across various train/validation/test splits despite the domain mismatch.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stationary MMD Points for Cubature</title>
<link>https://arxiv.org/abs/2505.20754</link>
<guid>https://arxiv.org/abs/2505.20754</guid>
<content:encoded><![CDATA[
arXiv:2505.20754v1 Announce Type: cross 
Abstract: Approximation of a target probability distribution using a finite set of points is a problem of fundamental importance, arising in cubature, data compression, and optimisation. Several authors have proposed to select points by minimising a maximum mean discrepancy (MMD), but the non-convexity of this objective precludes global minimisation in general. Instead, we consider \emph{stationary} points of the MMD which, in contrast to points globally minimising the MMD, can be accurately computed. Our main theoretical contribution is the (perhaps surprising) result that, for integrands in the associated reproducing kernel Hilbert space, the cubature error of stationary MMD points vanishes \emph{faster} than the MMD. Motivated by this \emph{super-convergence} property, we consider discretised gradient flows as a practical strategy for computing stationary points of the MMD, presenting a refined convergence analysis that establishes a novel non-asymptotic finite-particle error bound, which may be of independent interest.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ConText-CIR: Learning from Concepts in Text for Composed Image Retrieval</title>
<link>https://arxiv.org/abs/2505.20764</link>
<guid>https://arxiv.org/abs/2505.20764</guid>
<content:encoded><![CDATA[
arXiv:2505.20764v1 Announce Type: cross 
Abstract: Composed image retrieval (CIR) is the task of retrieving a target image specified by a query image and a relative text that describes a semantic modification to the query image. Existing methods in CIR struggle to accurately represent the image and the text modification, resulting in subpar performance. To address this limitation, we introduce a CIR framework, ConText-CIR, trained with a Text Concept-Consistency loss that encourages the representations of noun phrases in the text modification to better attend to the relevant parts of the query image. To support training with this loss function, we also propose a synthetic data generation pipeline that creates training data from existing CIR datasets or unlabeled images. We show that these components together enable stronger performance on CIR tasks, setting a new state-of-the-art in composed image retrieval in both the supervised and zero-shot settings on multiple benchmark datasets, including CIRR and CIRCO. Source code, model checkpoints, and our new datasets are available at https://github.com/mvrl/ConText-CIR.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaSlot: Break Through the Fixed Number of Slots in Object-Centric Learning</title>
<link>https://arxiv.org/abs/2505.20772</link>
<guid>https://arxiv.org/abs/2505.20772</guid>
<content:encoded><![CDATA[
arXiv:2505.20772v1 Announce Type: cross 
Abstract: Learning object-level, structured representations is widely regarded as a key to better generalization in vision and underpins the design of next-generation Pre-trained Vision Models (PVMs). Mainstream Object-Centric Learning (OCL) methods adopt Slot Attention or its variants to iteratively aggregate objects' super-pixels into a fixed set of query feature vectors, termed slots. However, their reliance on a static slot count leads to an object being represented as multiple parts when the number of objects varies. We introduce MetaSlot, a plug-and-play Slot Attention variant that adapts to variable object counts. MetaSlot (i) maintains a codebook that holds prototypes of objects in a dataset by vector-quantizing the resulting slot representations; (ii) removes duplicate slots from the traditionally aggregated slots by quantizing them with the codebook; and (iii) injects progressively weaker noise into the Slot Attention iterations to accelerate and stabilize the aggregation. MetaSlot is a general Slot Attention variant that can be seamlessly integrated into existing OCL architectures. Across multiple public datasets and tasks--including object discovery and recognition--models equipped with MetaSlot achieve significant performance gains and markedly interpretable slot representations, compared with existing Slot Attention variants.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SpecExtend: A Drop-in Enhancement for Speculative Decoding of Long Sequences</title>
<link>https://arxiv.org/abs/2505.20776</link>
<guid>https://arxiv.org/abs/2505.20776</guid>
<content:encoded><![CDATA[
arXiv:2505.20776v1 Announce Type: cross 
Abstract: Speculative decoding is a widely adopted technique for accelerating inference in large language models (LLMs), but its performance degrades on long inputs due to increased attention cost and reduced draft accuracy. We introduce SpecExtend, a drop-in enhancement that improves the performance of speculative decoding on long sequences without any additional training. SpecExtend integrates efficient attention mechanisms such as FlashAttention and Hybrid Tree Attention into both the draft and target models, reducing latency across all stages. To improve draft accuracy and speed, we propose Cross-model Retrieval, a novel KV cache update strategy that uses the target model's attention scores to dynamically select relevant context for the draft model. Extensive evaluations on three long-context understanding datasets show that SpecExtend accelerates standard tree-based speculative decoding by up to 2.22x for inputs up to 16K tokens, providing an effective solution for speculative decoding of long sequences. The code is available at https://github.com/jycha98/SpecExtend .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STITCH-OPE: Trajectory Stitching with Guided Diffusion for Off-Policy Evaluation</title>
<link>https://arxiv.org/abs/2505.20781</link>
<guid>https://arxiv.org/abs/2505.20781</guid>
<content:encoded><![CDATA[
arXiv:2505.20781v1 Announce Type: cross 
Abstract: Off-policy evaluation (OPE) estimates the performance of a target policy using offline data collected from a behavior policy, and is crucial in domains such as robotics or healthcare where direct interaction with the environment is costly or unsafe. Existing OPE methods are ineffective for high-dimensional, long-horizon problems, due to exponential blow-ups in variance from importance weighting or compounding errors from learned dynamics models. To address these challenges, we propose STITCH-OPE, a model-based generative framework that leverages denoising diffusion for long-horizon OPE in high-dimensional state and action spaces. Starting with a diffusion model pre-trained on the behavior data, STITCH-OPE generates synthetic trajectories from the target policy by guiding the denoising process using the score function of the target policy. STITCH-OPE proposes two technical innovations that make it advantageous for OPE: (1) prevents over-regularization by subtracting the score of the behavior policy during guidance, and (2) generates long-horizon trajectories by stitching partial trajectories together end-to-end. We provide a theoretical guarantee that under mild assumptions, these modifications result in an exponential reduction in variance versus long-horizon trajectory diffusion. Experiments on the D4RL and OpenAI Gym benchmarks show substantial improvement in mean squared error, correlation, and regret metrics compared to state-of-the-art OPE methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Wearable Tap Water Audio Detection through Subclass Annotation in the HD-Epic Dataset</title>
<link>https://arxiv.org/abs/2505.20788</link>
<guid>https://arxiv.org/abs/2505.20788</guid>
<content:encoded><![CDATA[
arXiv:2505.20788v1 Announce Type: cross 
Abstract: Wearable human activity recognition has been shown to benefit from the inclusion of acoustic data, as the sounds around a person often contain valuable context. However, due to privacy concerns, it is usually not ethically feasible to record and save microphone data from the device, since the audio could, for instance, also contain private conversations. Rather, the data should be processed locally, which in turn requires processing power and consumes energy on the wearable device. One special use case of contextual information that can be utilized to augment special tasks in human activity recognition is water flow detection, which can, e.g., be used to aid wearable hand washing detection. We created a new label called tap water for the recently released HD-Epic data set, creating 717 hand-labeled annotations of tap water flow, based on existing annotations of the water class. We analyzed the relation of tap water and water in the dataset and additionally trained and evaluated two lightweight classifiers to evaluate the newly added label class, showing that the new class can be learned more easily.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integrating Intermediate Layer Optimization and Projected Gradient Descent for Solving Inverse Problems with Diffusion Models</title>
<link>https://arxiv.org/abs/2505.20789</link>
<guid>https://arxiv.org/abs/2505.20789</guid>
<content:encoded><![CDATA[
arXiv:2505.20789v1 Announce Type: cross 
Abstract: Inverse problems (IPs) involve reconstructing signals from noisy observations. Traditional approaches often rely on handcrafted priors, which can fail to capture the complexity of real-world data. The advent of pre-trained generative models has introduced new paradigms, offering improved reconstructions by learning rich priors from data. Among these, diffusion models (DMs) have emerged as a powerful framework, achieving remarkable reconstruction performance across numerous IPs. However, existing DM-based methods frequently encounter issues such as heavy computational demands and suboptimal convergence. In this work, building upon the idea of the recent work DMPlug~\cite{wang2024dmplug}, we propose two novel methods, DMILO and DMILO-PGD, to address these challenges. Our first method, DMILO, employs intermediate layer optimization (ILO) to alleviate the memory burden inherent in DMPlug. Additionally, by introducing sparse deviations, we expand the range of DMs, enabling the exploration of underlying signals that may lie outside the range of the diffusion model. We further propose DMILO-PGD, which integrates ILO with projected gradient descent (PGD), thereby reducing the risk of suboptimal convergence. We provide an intuitive theoretical analysis of our approach under appropriate conditions and validate its superiority through extensive experiments on diverse image datasets, encompassing both linear and nonlinear IPs. Our results demonstrate significant performance gains over state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD in addressing common challenges in DM-based IP solvers.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence of Clipped-SGD for Convex $(L_0,L_1)$-Smooth Optimization with Heavy-Tailed Noise</title>
<link>https://arxiv.org/abs/2505.20817</link>
<guid>https://arxiv.org/abs/2505.20817</guid>
<content:encoded><![CDATA[
arXiv:2505.20817v1 Announce Type: cross 
Abstract: Gradient clipping is a widely used technique in Machine Learning and Deep Learning (DL), known for its effectiveness in mitigating the impact of heavy-tailed noise, which frequently arises in the training of large language models. Additionally, first-order methods with clipping, such as Clip-SGD, exhibit stronger convergence guarantees than SGD under the $(L_0,L_1)$-smoothness assumption, a property observed in many DL tasks. However, the high-probability convergence of Clip-SGD under both assumptions -- heavy-tailed noise and $(L_0,L_1)$-smoothness -- has not been fully addressed in the literature. In this paper, we bridge this critical gap by establishing the first high-probability convergence bounds for Clip-SGD applied to convex $(L_0,L_1)$-smooth optimization with heavy-tailed noise. Our analysis extends prior results by recovering known bounds for the deterministic case and the stochastic setting with $L_1 = 0$ as special cases. Notably, our rates avoid exponentially large factors and do not rely on restrictive sub-Gaussian noise assumptions, significantly broadening the applicability of gradient clipping.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Diffusion Models for Parameterized Quantum Circuit Generation</title>
<link>https://arxiv.org/abs/2505.20863</link>
<guid>https://arxiv.org/abs/2505.20863</guid>
<content:encoded><![CDATA[
arXiv:2505.20863v1 Announce Type: cross 
Abstract: Quantum computing holds immense potential, yet its practical success depends on multiple factors, including advances in quantum circuit design. In this paper, we introduce a generative approach based on denoising diffusion models (DMs) to synthesize parameterized quantum circuits (PQCs). Extending the recent diffusion model pipeline of F\"urrutter et al. [1], our model effectively conditions the synthesis process, enabling the simultaneous generation of circuit architectures and their continuous gate parameters. We demonstrate our approach in synthesizing PQCs optimized for generating high-fidelity Greenberger-Horne-Zeilinger (GHZ) states and achieving high accuracy in quantum machine learning (QML) classification tasks. Our results indicate a strong generalization across varying gate sets and scaling qubit counts, highlighting the versatility and computational efficiency of diffusion-based methods. This work illustrates the potential of generative models as a powerful tool for accelerating and optimizing the design of PQCs, supporting the development of more practical and scalable quantum applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>In Context Learning with Vision Transformers: Case Study</title>
<link>https://arxiv.org/abs/2505.20872</link>
<guid>https://arxiv.org/abs/2505.20872</guid>
<content:encoded><![CDATA[
arXiv:2505.20872v1 Announce Type: cross 
Abstract: Large transformer models have been shown to be capable of performing in-context learning. By using examples in a prompt as well as a query, they are capable of performing tasks such as few-shot, one-shot, or zero-shot learning to output the corresponding answer to this query. One area of interest to us is that these transformer models have been shown to be capable of learning the general class of certain functions, such as linear functions and small 2-layer neural networks, on random data (Garg et al, 2023). We aim to extend this to the image space to analyze their capability to in-context learn more complex functions on the image space, such as convolutional neural networks and other methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Revisiting Multi-Agent World Modeling from a Diffusion-Inspired Perspective</title>
<link>https://arxiv.org/abs/2505.20922</link>
<guid>https://arxiv.org/abs/2505.20922</guid>
<content:encoded><![CDATA[
arXiv:2505.20922v1 Announce Type: cross 
Abstract: World models have recently attracted growing interest in Multi-Agent Reinforcement Learning (MARL) due to their ability to improve sample efficiency for policy learning. However, accurately modeling environments in MARL is challenging due to the exponentially large joint action space and highly uncertain dynamics inherent in multi-agent systems. To address this, we reduce modeling complexity by shifting from jointly modeling the entire state-action transition dynamics to focusing on the state space alone at each timestep through sequential agent modeling. Specifically, our approach enables the model to progressively resolve uncertainty while capturing the structured dependencies among agents, providing a more accurate representation of how agents influence the state. Interestingly, this sequential revelation of agents' actions in a multi-agent system aligns with the reverse process in diffusion models--a class of powerful generative models known for their expressiveness and training stability compared to autoregressive or latent variable models. Leveraging this insight, we develop a flexible and robust world model for MARL using diffusion models. Our method, Diffusion-Inspired Multi-Agent world model (DIMA), achieves state-of-the-art performance across multiple multi-agent control benchmarks, significantly outperforming prior world models in terms of final return and sample efficiency, including MAMuJoCo and Bi-DexHands. DIMA establishes a new paradigm for constructing multi-agent world models, advancing the frontier of MARL research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scattering Networks on Noncommutative Finite Groups</title>
<link>https://arxiv.org/abs/2505.20950</link>
<guid>https://arxiv.org/abs/2505.20950</guid>
<content:encoded><![CDATA[
arXiv:2505.20950v1 Announce Type: cross 
Abstract: Scattering Networks were initially designed to elucidate the behavior of early layers in Convolutional Neural Networks (CNNs) over Euclidean spaces and are grounded in wavelets. In this work, we introduce a scattering transform on an arbitrary finite group (not necessarily abelian) within the context of group-equivariant convolutional neural networks (G-CNNs). We present wavelets on finite groups and analyze their similarity to classical wavelets. We demonstrate that, under certain conditions in the wavelet coefficients, the scattering transform is non-expansive, stable under deformations, preserves energy, equivariant with respect to left and right group translations, and, as depth increases, the scattering coefficients are less sensitive to group translations of the signal, all desirable properties of convolutional neural networks. Furthermore, we provide examples illustrating the application of the scattering transform to classify data with domains involving abelian and nonabelian groups.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Impact of Frequency Components on Membership Inference Attacks for Diffusion Models</title>
<link>https://arxiv.org/abs/2505.20955</link>
<guid>https://arxiv.org/abs/2505.20955</guid>
<content:encoded><![CDATA[
arXiv:2505.20955v1 Announce Type: cross 
Abstract: Diffusion models have achieved tremendous success in image generation, but they also raise significant concerns regarding privacy and copyright issues. Membership Inference Attacks (MIAs) are designed to ascertain whether specific data were utilized during a model's training phase. As current MIAs for diffusion models typically exploit the model's image prediction ability, we formalize them into a unified general paradigm which computes the membership score for membership identification. Under this paradigm, we empirically find that existing attacks overlook the inherent deficiency in how diffusion models process high-frequency information. Consequently, this deficiency leads to member data with more high-frequency content being misclassified as hold-out data, and hold-out data with less high-frequency content tend to be misclassified as member data. Moreover, we theoretically demonstrate that this deficiency reduces the membership advantage of attacks, thereby interfering with the effective discrimination of member data and hold-out data. Based on this insight, we propose a plug-and-play high-frequency filter module to mitigate the adverse effects of the deficiency, which can be seamlessly integrated into any attacks within this general paradigm without additional time costs. Extensive experiments corroborate that this module significantly improves the performance of baseline attacks across different datasets and models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Disagreement-Diversity Active Learning for Bioacoustic Sound Event Detection</title>
<link>https://arxiv.org/abs/2505.20956</link>
<guid>https://arxiv.org/abs/2505.20956</guid>
<content:encoded><![CDATA[
arXiv:2505.20956v1 Announce Type: cross 
Abstract: Bioacoustic sound event detection (BioSED) is crucial for biodiversity conservation but faces practical challenges during model development and training: limited amounts of annotated data, sparse events, species diversity, and class imbalance. To address these challenges efficiently with a limited labeling budget, we apply the mismatch-first farthest-traversal (MFFT), an active learning method integrating committee voting disagreement and diversity analysis. We also refine an existing BioSED dataset specifically for evaluating active learning algorithms. Experimental results demonstrate that MFFT achieves a mAP of 68% when cold-starting and 71% when warm-starting (which is close to the fully-supervised mAP of 75%) while using only 2.3% of the annotations. Notably, MFFT excels in cold-start scenarios and with rare species, which are critical for monitoring endangered species, demonstrating its practical value.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficient and Microphone-Fault-Tolerant 3D Sound Source Localization</title>
<link>https://arxiv.org/abs/2505.20961</link>
<guid>https://arxiv.org/abs/2505.20961</guid>
<content:encoded><![CDATA[
arXiv:2505.20961v1 Announce Type: cross 
Abstract: Sound source localization (SSL) is a critical technology for determining the position of sound sources in complex environments. However, existing methods face challenges such as high computational costs and precise calibration requirements, limiting their deployment in dynamic or resource-constrained environments. This paper introduces a novel 3D SSL framework, which uses sparse cross-attention, pretraining, and adaptive signal coherence metrics, to achieve accurate and computationally efficient localization with fewer input microphones. The framework is also fault-tolerant to unreliable or even unknown microphone position inputs, ensuring its applicability in real-world scenarios. Preliminary experiments demonstrate its scalability for multi-source localization without requiring additional hardware. This work advances SSL by balancing the model's performance and efficiency and improving its robustness for real-world scenarios.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Super Spreaders in Multilayer Networks</title>
<link>https://arxiv.org/abs/2505.20980</link>
<guid>https://arxiv.org/abs/2505.20980</guid>
<content:encoded><![CDATA[
arXiv:2505.20980v1 Announce Type: cross 
Abstract: Identifying super-spreaders can be framed as a subtask of the influence maximisation problem. It seeks to pinpoint agents within a network that, if selected as single diffusion seeds, disseminate information most effectively. Multilayer networks, a specific class of heterogeneous graphs, can capture diverse types of interactions (e.g., physical-virtual or professional-social), and thus offer a more accurate representation of complex relational structures. In this work, we introduce a novel approach to identifying super-spreaders in such networks by leveraging graph neural networks. To this end, we construct a dataset by simulating information diffusion across hundreds of networks - to the best of our knowledge, the first of its kind tailored specifically to multilayer networks. We further formulate the task as a variation of the ranking prediction problem based on a four-dimensional vector that quantifies each agent's spreading potential: (i) the number of activations; (ii) the duration of the diffusion process; (iii) the peak number of activations; and (iv) the simulation step at which this peak occurs. Our model, TopSpreadersNetwork, comprises a relationship-agnostic encoder and a custom aggregation layer. This design enables generalisation to previously unseen data and adapts to varying graph sizes. In an extensive evaluation, we compare our model against classic centrality-based heuristics and competitive deep learning methods. The results, obtained across a broad spectrum of real-world and synthetic multilayer networks, demonstrate that TopSpreadersNetwork achieves superior performance in identifying high-impact nodes, while also offering improved interpretability through its structured output.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unified Alignment Protocol: Making Sense of the Unlabeled Data in New Domains</title>
<link>https://arxiv.org/abs/2505.21010</link>
<guid>https://arxiv.org/abs/2505.21010</guid>
<content:encoded><![CDATA[
arXiv:2505.21010v1 Announce Type: cross 
Abstract: Semi-Supervised Federated Learning (SSFL) is gaining popularity over conventional Federated Learning in many real-world applications. Due to the practical limitation of limited labeled data on the client side, SSFL considers that participating clients train with unlabeled data, and only the central server has the necessary resources to access limited labeled data, making it an ideal fit for real-world applications (e.g., healthcare). However, traditional SSFL assumes that the data distributions in the training phase and testing phase are the same. In practice, however, domain shifts frequently occur, making it essential for SSFL to incorporate generalization capabilities and enhance their practicality. The core challenge is improving model generalization to new, unseen domains while the client participate in SSFL. However, the decentralized setup of SSFL and unsupervised client training necessitates innovation to achieve improved generalization across domains. To achieve this, we propose a novel framework called the Unified Alignment Protocol (UAP), which consists of an alternating two-stage training process. The first stage involves training the server model to learn and align the features with a parametric distribution, which is subsequently communicated to clients without additional communication overhead. The second stage proposes a novel training algorithm that utilizes the server feature distribution to align client features accordingly. Our extensive experiments on standard domain generalization benchmark datasets across multiple model architectures reveal that proposed UAP successfully achieves SOTA generalization performance in SSFL setting.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Cardiac Digital Twins at Scale from MRI: Open Tools and Representative Models from ~55000 UK Biobank Participants</title>
<link>https://arxiv.org/abs/2505.21019</link>
<guid>https://arxiv.org/abs/2505.21019</guid>
<content:encoded><![CDATA[
arXiv:2505.21019v1 Announce Type: cross 
Abstract: A cardiac digital twin is a virtual replica of a patient's heart for screening, diagnosis, prognosis, risk assessment, and treatment planning of cardiovascular diseases. This requires an anatomically accurate patient-specific 3D structural representation of the heart, suitable for electro-mechanical simulations or study of disease mechanisms. However, generation of cardiac digital twins at scale is demanding and there are no public repositories of models across demographic groups. We describe an automatic open-source pipeline for creating patient-specific left and right ventricular meshes from cardiovascular magnetic resonance images, its application to a large cohort of ~55000 participants from UK Biobank, and the construction of the most comprehensive cohort of adult heart models to date, comprising 1423 representative meshes across sex (male, female), body mass index (range: 16 - 42 kg/m$^2$) and age (range: 49 - 80 years). Our code is available at https://github.com/cdttk/biv-volumetric-meshing/tree/plos2025 , and pre-trained networks, representative volumetric meshes with fibers and UVCs will be made available soon.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Text-Queried Audio Source Separation via Hierarchical Modeling</title>
<link>https://arxiv.org/abs/2505.21025</link>
<guid>https://arxiv.org/abs/2505.21025</guid>
<content:encoded><![CDATA[
arXiv:2505.21025v1 Announce Type: cross 
Abstract: Target audio source separation with natural language queries presents a promising paradigm for extracting arbitrary audio events through arbitrary text descriptions. Existing methods mainly face two challenges, the difficulty in jointly modeling acoustic-textual alignment and semantic-aware separation within a blindly-learned single-stage architecture, and the reliance on large-scale accurately-labeled training data to compensate for inefficient cross-modal learning and separation. To address these challenges, we propose a hierarchical decomposition framework, HSM-TSS, that decouples the task into global-local semantic-guided feature separation and structure-preserving acoustic reconstruction. Our approach introduces a dual-stage mechanism for semantic separation, operating on distinct global and local semantic feature spaces. We first perform global-semantic separation through a global semantic feature space aligned with text queries. A Q-Audio architecture is employed to align audio and text modalities, serving as pretrained global-semantic encoders. Conditioned on the predicted global feature, we then perform the second-stage local-semantic separation on AudioMAE features that preserve time-frequency structures, followed by acoustic reconstruction. We also propose an instruction processing pipeline to parse arbitrary text queries into structured operations, extraction or removal, coupled with audio descriptions, enabling flexible sound manipulation. Our method achieves state-of-the-art separation performance with data-efficient training while maintaining superior semantic consistency with queries in complex auditory scenes.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Mode Process Control Using Multi-Task Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.21026</link>
<guid>https://arxiv.org/abs/2505.21026</guid>
<content:encoded><![CDATA[
arXiv:2505.21026v1 Announce Type: cross 
Abstract: In the era of Industry 4.0 and smart manufacturing, process systems engineering must adapt to digital transformation. While reinforcement learning offers a model-free approach to process control, its applications are limited by the dependence on accurate digital twins and well-designed reward functions. To address these limitations, this paper introduces a novel framework that integrates inverse reinforcement learning (IRL) with multi-task learning for data-driven, multi-mode control design. Using historical closed-loop data as expert demonstrations, IRL extracts optimal reward functions and control policies. A latent-context variable is incorporated to distinguish modes, enabling the training of mode-specific controllers. Case studies on a continuous stirred tank reactor and a fed-batch bioreactor validate the effectiveness of this framework in handling multi-mode data and training adaptable controllers.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FeatInv: Spatially resolved mapping from feature space to input space using conditional diffusion models</title>
<link>https://arxiv.org/abs/2505.21032</link>
<guid>https://arxiv.org/abs/2505.21032</guid>
<content:encoded><![CDATA[
arXiv:2505.21032v1 Announce Type: cross 
Abstract: Internal representations are crucial for understanding deep neural networks, such as their properties and reasoning patterns, but remain difficult to interpret. While mapping from feature space to input space aids in interpreting the former, existing approaches often rely on crude approximations. We propose using a conditional diffusion model - a pretrained high-fidelity diffusion model conditioned on spatially resolved feature maps - to learn such a mapping in a probabilistic manner. We demonstrate the feasibility of this approach across various pretrained image classifiers from CNNs to ViTs, showing excellent reconstruction capabilities. Through qualitative comparisons and robustness analysis, we validate our method and showcase possible applications, such as the visualization of concept steering in input space or investigations of the composite nature of the feature space. This approach has broad potential for improving feature space understanding in computer vision models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thinker: Learning to Think Fast and Slow</title>
<link>https://arxiv.org/abs/2505.21097</link>
<guid>https://arxiv.org/abs/2505.21097</guid>
<content:encoded><![CDATA[
arXiv:2505.21097v1 Announce Type: cross 
Abstract: Recent studies show that the reasoning capabilities of Large Language Models (LLMs) can be improved by applying Reinforcement Learning (RL) to question-answering (QA) tasks in areas such as math and coding. With a long context length, LLMs may learn to perform search, as indicated by the self-correction behavior observed in DeepSeek R1. However, this search behavior is often imprecise and lacks confidence, resulting in long, redundant responses and highlighting deficiencies in intuition and verification. Inspired by the Dual Process Theory in psychology, we introduce a simple modification to the QA task that includes four stages: Fast Thinking, where the LLM must answer within a strict token budget; Verification, where the model evaluates its initial response; Slow Thinking, where it refines the initial response with more deliberation; and Summarization, where it distills the refinement from the previous stage into precise steps. Our proposed task improves average accuracy from 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for DeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone achieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial inference efficiency gains. These findings suggest that intuition and deliberative reasoning are distinct, complementary systems benefiting from targeted training.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Multi-Expert Generative Language Model System for Engineering Information and Knowledge Extraction</title>
<link>https://arxiv.org/abs/2505.21109</link>
<guid>https://arxiv.org/abs/2505.21109</guid>
<content:encoded><![CDATA[
arXiv:2505.21109v1 Announce Type: cross 
Abstract: Despite recent advancements in domain adaptation techniques for large language models, these methods remain computationally intensive, and the resulting models can still exhibit hallucination issues. Most existing adaptation methods do not prioritize reducing the computational resources required for fine-tuning and inference of language models. Hallucination issues have gradually decreased with each new model release. However, they remain prevalent in engineering contexts, where generating well-structured text with minimal errors and inconsistencies is critical. This work introduces a novel approach called the Small Language Graph (SLG), which is a lightweight adaptation solution designed to address the two key challenges outlined above. The system is structured in the form of a graph, where each node represents a lightweight expert - a small language model fine-tuned on specific and concise texts. The results of this study have shown that SLG was able to surpass conventional fine-tuning methods on the Exact Match metric by 3 times. Additionally, the fine-tuning process was 1.7 times faster compared to that of a larger stand-alone language model. These findings introduce a potential for small to medium-sized engineering companies to confidently use generative AI technologies, such as LLMs, without the necessity to invest in expensive computational resources. Also, the graph architecture and the small size of expert nodes offer a possible opportunity for distributed AI systems, thus potentially diverting the global need for expensive centralized compute clusters.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Identifying Heart Attack Risk in Vulnerable Population: A Machine Learning Approach</title>
<link>https://arxiv.org/abs/2505.21139</link>
<guid>https://arxiv.org/abs/2505.21139</guid>
<content:encoded><![CDATA[
arXiv:2505.21139v1 Announce Type: cross 
Abstract: The COVID-19 pandemic has significantly increased the incidence of post-infection cardiovascular events, particularly myocardial infarction, in individuals over 40. While the underlying mechanisms remain elusive, this study employs a hybrid machine learning approach to analyze epidemiological data in assessing 13 key heart attack risk factors and their susceptibility. Based on a unique dataset that combines demographic, biochemical, ECG, and thallium stress-tests, this study categorizes distinct subpopulations against varying risk profiles and then divides the population into 'at-risk' (AR) and 'not-at-risk' (NAR) groups using clustering algorithms. The study reveals strong association between the likelihood of experiencing a heart attack on the 13 risk factors studied. The aggravated risk for postmenopausal patients indicates compromised individual risk factors due to estrogen depletion that may be, further compromised by extraneous stress impacts, like anxiety and fear, aspects that have traditionally eluded data modeling predictions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model as Loss: A Self-Consistent Training Paradigm</title>
<link>https://arxiv.org/abs/2505.21156</link>
<guid>https://arxiv.org/abs/2505.21156</guid>
<content:encoded><![CDATA[
arXiv:2505.21156v1 Announce Type: cross 
Abstract: Conventional methods for speech enhancement rely on handcrafted loss functions (e.g., time or frequency domain losses) or deep feature losses (e.g., using WavLM or wav2vec), which often fail to capture subtle signal properties essential for optimal performance. To address this, we propose Model as Loss, a novel training paradigm that utilizes the encoder from the same model as a loss function to guide the training.
  The Model as Loss paradigm leverages the encoder's task-specific feature space, optimizing the decoder to produce output consistent with perceptual and task-relevant characteristics of the clean signal. By using the encoder's learned features as a loss function, this framework enforces self-consistency between the clean reference speech and the enhanced model output. Our approach outperforms pre-trained deep feature losses on standard speech enhancement benchmarks, offering better perceptual quality and robust generalization to both in-domain and out-of-domain datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exploring the Latent Capacity of LLMs for One-Step Text Generation</title>
<link>https://arxiv.org/abs/2505.21189</link>
<guid>https://arxiv.org/abs/2505.21189</guid>
<content:encoded><![CDATA[
arXiv:2505.21189v1 Announce Type: cross 
Abstract: A recent study showed that large language models (LLMs) can reconstruct surprisingly long texts - up to thousands of tokens - via autoregressive generation from just one specially trained input embedding. In this work, we explore whether such reconstruction is possible without autoregression. We show that frozen LLMs can generate hundreds of accurate tokens in just one forward pass, when provided with only two learned embeddings. This reveals a surprising and underexplored capability of LLMs - multi-token generation without iterative decoding. We investigate the behaviour of these embeddings and provide insight into the type of information they encode. We also empirically show that although these representations are not unique for a given text, they form connected and local regions in embedding space - a property that suggests the potential of learning a dedicated encoder into that space.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveiling Instruction-Specific Neurons &amp; Experts: An Analytical Framework for LLM's Instruction-Following Capabilities</title>
<link>https://arxiv.org/abs/2505.21191</link>
<guid>https://arxiv.org/abs/2505.21191</guid>
<content:encoded><![CDATA[
arXiv:2505.21191v1 Announce Type: cross 
Abstract: The finetuning of Large Language Models (LLMs) has significantly advanced their instruction-following capabilities, yet the underlying computational mechanisms driving these improvements remain poorly understood. This study systematically examines how fine-tuning reconfigures LLM computations by isolating and analyzing instruction-specific sparse components, i.e., neurons in dense models and both neurons and experts in Mixture-of-Experts (MoE) architectures. In particular, we introduce HexaInst, a carefully curated and balanced instructional dataset spanning six distinct categories, and propose SPARCOM, a novel analytical framework comprising three key contributions: (1) a method for identifying these sparse components, (2) an evaluation of their functional generality and uniqueness, and (3) a systematic comparison of their alterations. Through experiments, we demonstrate functional generality, uniqueness, and the critical role of these components in instruction execution. By elucidating the relationship between fine-tuning-induced adaptations and sparse computational substrates, this work provides deeper insights into how LLMs internalize instruction-following behavior for the trustworthy LLM community.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Input Convex Kolmogorov Arnold Networks</title>
<link>https://arxiv.org/abs/2505.21208</link>
<guid>https://arxiv.org/abs/2505.21208</guid>
<content:encoded><![CDATA[
arXiv:2505.21208v1 Announce Type: cross 
Abstract: This article presents an input convex neural network architecture using Kolmogorov-Arnold networks (ICKAN). Two specific networks are presented: the first is based on a low-order, linear-by-part, representation of functions, and a universal approximation theorem is provided. The second is based on cubic splines, for which only numerical results support convergence. We demonstrate on simple tests that these networks perform competitively with classical input convex neural networks (ICNNs). In a second part, we use the networks to solve some optimal transport problems needing a convex approximation of functions and demonstrate their effectiveness. Comparisons with ICNNs show that cubic ICKANs produce results similar to those of classical ICNNs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Transfer learning for multifidelity simulation-based inference in cosmology</title>
<link>https://arxiv.org/abs/2505.21215</link>
<guid>https://arxiv.org/abs/2505.21215</guid>
<content:encoded><![CDATA[
arXiv:2505.21215v1 Announce Type: cross 
Abstract: Simulation-based inference (SBI) enables cosmological parameter estimation when closed-form likelihoods or models are unavailable. However, SBI relies on machine learning for neural compression and density estimation. This requires large training datasets which are prohibitively expensive for high-quality simulations. We overcome this limitation with multifidelity transfer learning, combining less expensive, lower-fidelity simulations with a limited number of high-fidelity simulations. We demonstrate our methodology on dark matter density maps from two separate simulation suites in the hydrodynamical CAMELS Multifield Dataset. Pre-training on dark-matter-only $N$-body simulations reduces the required number of high-fidelity hydrodynamical simulations by a factor between $8$ and $15$, depending on the model complexity, posterior dimensionality, and performance metrics used. By leveraging cheaper simulations, our approach enables performant and accurate inference on high-fidelity models while substantially reducing computational costs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wavelet Flow For Extragalactic Foreground Simulations</title>
<link>https://arxiv.org/abs/2505.21220</link>
<guid>https://arxiv.org/abs/2505.21220</guid>
<content:encoded><![CDATA[
arXiv:2505.21220v1 Announce Type: cross 
Abstract: Extragalactic foregrounds in cosmic microwave background (CMB) observations are both a source of cosmological and astrophysical information and a nuisance to the CMB. Effective field-level modeling that captures their non-Gaussian statistical distributions is increasingly important for optimal information extraction, particularly given the precise and low-noise observations from current and upcoming experiments. We explore the use of Wavelet Flow (WF) models to tackle the novel task of modeling the field-level probability distributions of multi-component CMB secondaries. Specifically, we jointly train correlated CMB lensing convergence ($\kappa$) and cosmic infrared background (CIB) maps with a WF model and obtain a network that statistically recovers the input to high accuracy -- the trained network generates samples of $\kappa$ and CIB fields whose average power spectra are within a few percent of the inputs across all scales, and whose Minkowski functionals are similarly accurate compared to the inputs. Leveraging the multiscale architecture of these models, we fine-tune both the model parameters and the priors at each scale independently, optimizing performance across different resolutions. These results demonstrate that WF models can accurately simulate correlated components of CMB secondaries, supporting improved analysis of cosmological data. Our code and trained models can be found here (https://github.com/matiwosm/HybridPriorWavletFlow.git).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Is Hyperbolic Space All You Need for Medical Anomaly Detection?</title>
<link>https://arxiv.org/abs/2505.21228</link>
<guid>https://arxiv.org/abs/2505.21228</guid>
<content:encoded><![CDATA[
arXiv:2505.21228v1 Announce Type: cross 
Abstract: Medical anomaly detection has emerged as a promising solution to challenges in data availability and labeling constraints. Traditional methods extract features from different layers of pre-trained networks in Euclidean space; however, Euclidean representations fail to effectively capture the hierarchical relationships within these features, leading to suboptimal anomaly detection performance. We propose a novel yet simple approach that projects feature representations into hyperbolic space, aggregates them based on confidence levels, and classifies samples as healthy or anomalous. Our experiments demonstrate that hyperbolic space consistently outperforms Euclidean-based frameworks, achieving higher AUROC scores at both image and pixel levels across multiple medical benchmark datasets. Additionally, we show that hyperbolic space exhibits resilience to parameter variations and excels in few-shot scenarios, where healthy images are scarce. These findings underscore the potential of hyperbolic space as a powerful alternative for medical anomaly detection. The project website can be found at https://hyperbolic-anomalies.github.io
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Large Language Models Miss the Multi-Agent Mark</title>
<link>https://arxiv.org/abs/2505.21298</link>
<guid>https://arxiv.org/abs/2505.21298</guid>
<content:encoded><![CDATA[
arXiv:2505.21298v1 Announce Type: cross 
Abstract: Recent interest in Multi-Agent Systems of Large Language Models (MAS LLMs) has led to an increase in frameworks leveraging multiple LLMs to tackle complex tasks. However, much of this literature appropriates the terminology of MAS without engaging with its foundational principles. In this position paper, we highlight critical discrepancies between MAS theory and current MAS LLMs implementations, focusing on four key areas: the social aspect of agency, environment design, coordination and communication protocols, and measuring emergent behaviours. Our position is that many MAS LLMs lack multi-agent characteristics such as autonomy, social interaction, and structured environments, and often rely on oversimplified, LLM-centric architectures. The field may slow down and lose traction by revisiting problems the MAS literature has already addressed. Therefore, we systematically analyse this issue and outline associated research opportunities; we advocate for better integrating established MAS concepts and more precise terminology to avoid mischaracterisation and missed opportunities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing fMRI Data Acquisition for Decoding Natural Speech with Limited Participants</title>
<link>https://arxiv.org/abs/2505.21304</link>
<guid>https://arxiv.org/abs/2505.21304</guid>
<content:encoded><![CDATA[
arXiv:2505.21304v1 Announce Type: cross 
Abstract: We investigate optimal strategies for decoding perceived natural speech from fMRI data acquired from a limited number of participants. Leveraging Lebel et al. (2023)'s dataset of 8 participants, we first demonstrate the effectiveness of training deep neural networks to predict LLM-derived text representations from fMRI activity. Then, in this data regime, we observe that multi-subject training does not improve decoding accuracy compared to single-subject approach. Furthermore, training on similar or different stimuli across subjects has a negligible effect on decoding accuracy. Finally, we find that our decoders better model syntactic than semantic features, and that stories containing sentences with complex syntax or rich semantic content are more challenging to decode. While our results demonstrate the benefits of having extensive data per participant (deep phenotyping), they suggest that leveraging multi-subject for natural speech decoding likely requires deeper phenotyping or a substantially larger cohort.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Something's Fishy In The Data Lake: A Critical Re-evaluation of Table Union Search Benchmarks</title>
<link>https://arxiv.org/abs/2505.21329</link>
<guid>https://arxiv.org/abs/2505.21329</guid>
<content:encoded><![CDATA[
arXiv:2505.21329v1 Announce Type: cross 
Abstract: Recent table representation learning and data discovery methods tackle table union search (TUS) within data lakes, which involves identifying tables that can be unioned with a given query table to enrich its content. These methods are commonly evaluated using benchmarks that aim to assess semantic understanding in real-world TUS tasks. However, our analysis of prominent TUS benchmarks reveals several limitations that allow simple baselines to perform surprisingly well, often outperforming more sophisticated approaches. This suggests that current benchmark scores are heavily influenced by dataset-specific characteristics and fail to effectively isolate the gains from semantic understanding. To address this, we propose essential criteria for future benchmarks to enable a more realistic and reliable evaluation of progress in semantic table union search.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scheduling with Uncertain Holding Costs and its Application to Content Moderation</title>
<link>https://arxiv.org/abs/2505.21331</link>
<guid>https://arxiv.org/abs/2505.21331</guid>
<content:encoded><![CDATA[
arXiv:2505.21331v1 Announce Type: cross 
Abstract: In content moderation for social media platforms, the cost of delaying the review of a content is proportional to its view trajectory, which fluctuates and is apriori unknown. Motivated by such uncertain holding costs, we consider a queueing model where job states evolve based on a Markov chain with state-dependent instantaneous holding costs. We demonstrate that in the presence of such uncertain holding costs, the two canonical algorithmic principles, instantaneous-cost ($c\mu$-rule) and expected-remaining-cost ($c\mu/\theta$-rule), are suboptimal. By viewing each job as a Markovian ski-rental problem, we develop a new index-based algorithm, Opportunity-adjusted Remaining Cost (OaRC), that adjusts to the opportunity of serving jobs in the future when uncertainty partly resolves. We show that the regret of OaRC scales as $\tilde{O}(L^{1.5}\sqrt{N})$, where $L$ is the maximum length of a job's holding cost trajectory and $N$ is the system size. This regret bound shows that OaRC achieves asymptotic optimality when the system size $N$ scales to infinity. Moreover, its regret is independent of the state-space size, which is a desirable property when job states contain contextual information. We corroborate our results with an extensive simulation study based on two holding cost patterns (online ads and user-generated content) that arise in content moderation for social media platforms. Our simulations based on synthetic and real datasets demonstrate that OaRC consistently outperforms existing practice, which is based on the two canonical algorithmic principles.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structure from Collision</title>
<link>https://arxiv.org/abs/2505.21335</link>
<guid>https://arxiv.org/abs/2505.21335</guid>
<content:encoded><![CDATA[
arXiv:2505.21335v1 Announce Type: cross 
Abstract: Recent advancements in neural 3D representations, such as neural radiance fields (NeRF) and 3D Gaussian splatting (3DGS), have enabled the accurate estimation of 3D structures from multiview images. However, this capability is limited to estimating the visible external structure, and identifying the invisible internal structure hidden behind the surface is difficult. To overcome this limitation, we address a new task called Structure from Collision (SfC), which aims to estimate the structure (including the invisible internal structure) of an object from appearance changes during collision. To solve this problem, we propose a novel model called SfC-NeRF that optimizes the invisible internal structure of an object through a video sequence under physical, appearance (i.e., visible external structure)-preserving, and keyframe constraints. In particular, to avoid falling into undesirable local optima owing to its ill-posed nature, we propose volume annealing; that is, searching for global optima by repeatedly reducing and expanding the volume. Extensive experiments on 115 objects involving diverse structures (i.e., various cavity shapes, locations, and sizes) and material properties revealed the properties of SfC and demonstrated the effectiveness of the proposed SfC-NeRF.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Large Language Models for Bengali Math Word Problem Solving with Chain of Thought Reasoning</title>
<link>https://arxiv.org/abs/2505.21354</link>
<guid>https://arxiv.org/abs/2505.21354</guid>
<content:encoded><![CDATA[
arXiv:2505.21354v1 Announce Type: cross 
Abstract: Solving Bengali Math Word Problems (MWPs) remains a major challenge in natural language processing (NLP) due to the language's low-resource status and the multi-step reasoning required. Existing models struggle with complex Bengali MWPs, largely because no human-annotated Bengali dataset has previously addressed this task. This gap has limited progress in Bengali mathematical reasoning. To address this, we created SOMADHAN, a dataset of 8792 complex Bengali MWPs with manually written, step-by-step solutions. We designed this dataset to support reasoning-focused evaluation and model development in a linguistically underrepresented context. Using SOMADHAN, we evaluated a range of large language models (LLMs) - including GPT-4o, GPT-3.5 Turbo, LLaMA series models, Deepseek, and Qwen - through both zero-shot and few-shot prompting with and without Chain of Thought (CoT) reasoning. CoT prompting consistently improved performance over standard prompting, especially in tasks requiring multi-step logic. LLaMA-3.3 70B achieved the highest accuracy of 88% with few-shot CoT prompting. We also applied Low-Rank Adaptation (LoRA) to fine-tune models efficiently, enabling them to adapt to Bengali MWPs with minimal computational cost. Our work fills a critical gap in Bengali NLP by providing a high-quality reasoning dataset and a scalable framework for solving complex MWPs. We aim to advance equitable research in low-resource languages and enhance reasoning capabilities in educational and language technologies.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Automated Perceptual Voice Quality Assessment with Deep Learning</title>
<link>https://arxiv.org/abs/2505.21356</link>
<guid>https://arxiv.org/abs/2505.21356</guid>
<content:encoded><![CDATA[
arXiv:2505.21356v1 Announce Type: cross 
Abstract: Objective: Perceptual voice quality assessment plays a critical role in diagnosing and monitoring voice disorders by providing standardized evaluation of vocal function. Traditionally, this process relies on expert raters utilizing standard scales, such as the Consensus Auditory-Perceptual Evaluation of Voice (CAPE-V) and Grade, Roughness, Breathiness, Asthenia, and Strain (GRBAS). However, these metrics are inherently subjective and susceptible to inter-rater variability, motivating the need for automated and objective assessment methods. Methods: We propose Voice Quality Assessment Network (VOQANet), a deep learning-based framework with an attention mechanism that leverages a Speech Foundation Model (SFM) to capture high-level acoustic and prosodic information from raw speech. To enhance robustness and interpretability, we present VOQANet+, which integrates handcrafted acoustic features such as jitter, shimmer, and harmonics-to-noise ratio (HNR) with SFM embeddings. Results: Sentence-based input yields stronger performance than vowel-based input, especially at the patient level. VOQANet consistently outperforms baseline methods in RMSE and PCC, while VOQANet+ performs even better and maintains robustness under noisy conditions. Conclusion: Combining SFM embeddings with domain-informed acoustic features improves interpretability and resilience. Significance: VOQANet+ shows strong potential for deployment in real-world and telehealth settings, addressing the limitations of subjective perceptual assessments with an interpretable and noise-resilient solution.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AgriFM: A Multi-source Temporal Remote Sensing Foundation Model for Crop Mapping</title>
<link>https://arxiv.org/abs/2505.21357</link>
<guid>https://arxiv.org/abs/2505.21357</guid>
<content:encoded><![CDATA[
arXiv:2505.21357v1 Announce Type: cross 
Abstract: Accurate crop mapping fundamentally relies on modeling multi-scale spatiotemporal patterns, where spatial scales range from individual field textures to landscape-level context, and temporal scales capture both short-term phenological transitions and full growing-season dynamics. Transformer-based remote sensing foundation models (RSFMs) offer promising potential for crop mapping due to their innate ability for unified spatiotemporal processing. However, current RSFMs remain suboptimal for crop mapping: they either employ fixed spatiotemporal windows that ignore the multi-scale nature of crop systems or completely disregard temporal information by focusing solely on spatial patterns. To bridge these gaps, we present AgriFM, a multi-source remote sensing foundation model specifically designed for agricultural crop mapping. Our approach begins by establishing the necessity of simultaneous hierarchical spatiotemporal feature extraction, leading to the development of a modified Video Swin Transformer architecture where temporal down-sampling is synchronized with spatial scaling operations. This modified backbone enables efficient unified processing of long time-series satellite inputs. AgriFM leverages temporally rich data streams from three satellite sources including MODIS, Landsat-8/9 and Sentinel-2, and is pre-trained on a global representative dataset comprising over 25 million image samples supervised by land cover products. The resulting framework incorporates a versatile decoder architecture that dynamically fuses these learned spatiotemporal representations, supporting diverse downstream tasks. Comprehensive evaluations demonstrate AgriFM's superior performance over conventional deep learning approaches and state-of-the-art general-purpose RSFMs across all downstream tasks. Codes will be available at urlhttps://github.com/flyakon/AgriFM.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DeSocial: Blockchain-based Decentralized Social Networks</title>
<link>https://arxiv.org/abs/2505.21388</link>
<guid>https://arxiv.org/abs/2505.21388</guid>
<content:encoded><![CDATA[
arXiv:2505.21388v1 Announce Type: cross 
Abstract: Web 2.0 social platforms are inherently centralized, with user data and algorithmic decisions controlled by the platform. However, users can only passively receive social predictions without being able to choose the underlying algorithm, which limits personalization. Fortunately, with the emergence of blockchain, users are allowed to choose algorithms that are tailored to their local situation, improving prediction results in a personalized way. In a blockchain environment, each user possesses its own model to perform the social prediction, capturing different perspectives on social interactions. In our work, we propose DeSocial, a decentralized social network learning framework deployed on an Ethereum (ETH) local development chain that integrates distributed data storage, node-level consensus, and user-driven model selection through Ganache. In the first stage, each user leverages DeSocial to evaluate multiple backbone models on their local subgraph. DeSocial coordinates the execution and returns model-wise prediction results, enabling the user to select the most suitable backbone for personalized social prediction. Then, DeSocial uniformly selects several validation nodes that possess the algorithm specified by each user, and aggregates the prediction results by majority voting, to prevent errors caused by any single model's misjudgment. Extensive experiments show that DeSocial has an evident improvement compared to the five classical centralized social network learning models, promoting user empowerment in blockchain-based decentralized social networks, showing the importance of multi-node validation and personalized algorithm selection based on blockchain. Our implementation is available at: https://github.com/agiresearch/DeSocial.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Factual Self-Awareness in Language Models: Representation, Robustness, and Scaling</title>
<link>https://arxiv.org/abs/2505.21399</link>
<guid>https://arxiv.org/abs/2505.21399</guid>
<content:encoded><![CDATA[
arXiv:2505.21399v1 Announce Type: cross 
Abstract: Factual incorrectness in generated content is one of the primary concerns in ubiquitous deployment of large language models (LLMs). Prior findings suggest LLMs can (sometimes) detect factual incorrectness in their generated content (i.e., fact-checking post-generation). In this work, we provide evidence supporting the presence of LLMs' internal compass that dictate the correctness of factual recall at the time of generation. We demonstrate that for a given subject entity and a relation, LLMs internally encode linear features in the Transformer's residual stream that dictate whether it will be able to recall the correct attribute (that forms a valid entity-relation-attribute triplet). This self-awareness signal is robust to minor formatting variations. We investigate the effects of context perturbation via different example selection strategies. Scaling experiments across model sizes and training dynamics highlight that self-awareness emerges rapidly during training and peaks in intermediate layers. These findings uncover intrinsic self-monitoring capabilities within LLMs, contributing to their interpretability and reliability.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MRSD: Multi-Resolution Skill Discovery for HRL Agents</title>
<link>https://arxiv.org/abs/2505.21410</link>
<guid>https://arxiv.org/abs/2505.21410</guid>
<content:encoded><![CDATA[
arXiv:2505.21410v1 Announce Type: cross 
Abstract: Hierarchical reinforcement learning (HRL) relies on abstract skills to solve long-horizon tasks efficiently. While existing skill discovery methods learns these skills automatically, they are limited to a single skill per task. In contrast, humans learn and use both fine-grained and coarse motor skills simultaneously. Inspired by human motor control, we propose Multi-Resolution Skill Discovery (MRSD), an HRL framework that learns multiple skill encoders at different temporal resolutions in parallel. A high-level manager dynamically selects among these skills, enabling adaptive control strategies over time. We evaluate MRSD on tasks from the DeepMind Control Suite and show that it outperforms prior state-of-the-art skill discovery and HRL methods, achieving faster convergence and higher final performance. Our findings highlight the benefits of integrating multi-resolution skills in HRL, paving the way for more versatile and efficient agents.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Physics-Augmented GraphGPS Framework for the Reconstruction of 3D Riemann Problems from Sparse Data</title>
<link>https://arxiv.org/abs/2505.21421</link>
<guid>https://arxiv.org/abs/2505.21421</guid>
<content:encoded><![CDATA[
arXiv:2505.21421v1 Announce Type: cross 
Abstract: In compressible fluid flow, reconstructing shocks, discontinuities, rarefactions, and their interactions from sparse measurements is an important inverse problem with practical applications. Moreover, physics-informed machine learning has recently become an increasingly popular approach for performing reconstructions tasks. In this work we explore a machine learning recipe, known as GraphGPS, for reconstructing canonical compressible flows known as 3D Riemann problems from sparse observations, in a physics-informed manner. The GraphGPS framework combines the benefits of positional encodings, local message-passing of graphs, and global contextual awareness, and we explore the latter two components through an ablation study. Furthermore, we modify the aggregation step of message-passing such that it is aware of shocks and discontinuities, resulting in sharper reconstructions of these features. Additionally, we modify message-passing such that information flows strictly from known nodes only, which results in computational savings, better training convergence, and no degradation of reconstruction accuracy. We also show that the GraphGPS framework outperforms numerous machine learning benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Individual Behavior in Agent-Based Models with Graph Diffusion Networks</title>
<link>https://arxiv.org/abs/2505.21426</link>
<guid>https://arxiv.org/abs/2505.21426</guid>
<content:encoded><![CDATA[
arXiv:2505.21426v1 Announce Type: cross 
Abstract: Agent-Based Models (ABMs) are powerful tools for studying emergent properties in complex systems. In ABMs, agent behaviors are governed by local interactions and stochastic rules. However, these rules are, in general, non-differentiable, limiting the use of gradient-based methods for optimization, and thus integration with real-world data. We propose a novel framework to learn a differentiable surrogate of any ABM by observing its generated data. Our method combines diffusion models to capture behavioral stochasticity and graph neural networks to model agent interactions. Distinct from prior surrogate approaches, our method introduces a fundamental shift: rather than approximating system-level outputs, it models individual agent behavior directly, preserving the decentralized, bottom-up dynamics that define ABMs. We validate our approach on two ABMs (Schelling's segregation model and a Predator-Prey ecosystem) showing that it replicates individual-level patterns and accurately forecasts emergent dynamics beyond training. Our results demonstrate the potential of combining diffusion models and graph learning for data-driven ABM simulation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Induction: Predicting Startup Success via Explainable Memory-Augmented In-Context Learning</title>
<link>https://arxiv.org/abs/2505.21427</link>
<guid>https://arxiv.org/abs/2505.21427</guid>
<content:encoded><![CDATA[
arXiv:2505.21427v1 Announce Type: cross 
Abstract: Early-stage startup investment is a high-risk endeavor characterized by scarce data and uncertain outcomes. Traditional machine learning approaches often require large, labeled datasets and extensive fine-tuning, yet remain opaque and difficult for domain experts to interpret or improve. In this paper, we propose a transparent and data-efficient investment decision framework powered by memory-augmented large language models (LLMs) using in-context learning (ICL). Central to our method is a natural language policy embedded directly into the LLM prompt, enabling the model to apply explicit reasoning patterns and allowing human experts to easily interpret, audit, and iteratively refine the logic. We introduce a lightweight training process that combines few-shot learning with an in-context learning loop, enabling the LLM to update its decision policy iteratively based on structured feedback. With only minimal supervision and no gradient-based optimization, our system predicts startup success far more accurately than existing benchmarks. It is over 20x more precise than random chance, which succeeds 1.9% of the time. It is also 7.1x more precise than the typical 5.6% success rate of top-tier venture capital (VC) firms.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Autoencoding Random Forests</title>
<link>https://arxiv.org/abs/2505.21441</link>
<guid>https://arxiv.org/abs/2505.21441</guid>
<content:encoded><![CDATA[
arXiv:2505.21441v1 Announce Type: cross 
Abstract: We propose a principled method for autoencoding with random forests. Our strategy builds on foundational results from nonparametric statistics and spectral graph theory to learn a low-dimensional embedding of the model that optimally represents relationships in the data. We provide exact and approximate solutions to the decoding problem via constrained optimization, split relabeling, and nearest neighbors regression. These methods effectively invert the compression pipeline, establishing a map from the embedding space back to the input space using splits learned by the ensemble's constituent trees. The resulting decoders are universally consistent under common regularity assumptions. The procedure works with supervised or unsupervised models, providing a window into conditional or joint distributions. We demonstrate various applications of this autoencoder, including powerful new tools for visualization, compression, clustering, and denoising. Experiments illustrate the ease and utility of our method in a wide range of settings, including tabular, image, and genomic data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Be Decisive: Noise-Induced Layouts for Multi-Subject Generation</title>
<link>https://arxiv.org/abs/2505.21488</link>
<guid>https://arxiv.org/abs/2505.21488</guid>
<content:encoded><![CDATA[
arXiv:2505.21488v1 Announce Type: cross 
Abstract: Generating multiple distinct subjects remains a challenge for existing text-to-image diffusion models. Complex prompts often lead to subject leakage, causing inaccuracies in quantities, attributes, and visual features. Preventing leakage among subjects necessitates knowledge of each subject's spatial location. Recent methods provide these spatial locations via an external layout control. However, enforcing such a prescribed layout often conflicts with the innate layout dictated by the sampled initial noise, leading to misalignment with the model's prior. In this work, we introduce a new approach that predicts a spatial layout aligned with the prompt, derived from the initial noise, and refines it throughout the denoising process. By relying on this noise-induced layout, we avoid conflicts with externally imposed layouts and better preserve the model's prior. Our method employs a small neural network to predict and refine the evolving noise-induced layout at each denoising step, ensuring clear boundaries between subjects while maintaining consistency. Experimental results show that this noise-aligned strategy achieves improved text-image alignment and more stable multi-subject generation compared to existing layout-guided techniques, while preserving the rich diversity of the model's original distribution.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents</title>
<link>https://arxiv.org/abs/2505.21496</link>
<guid>https://arxiv.org/abs/2505.21496</guid>
<content:encoded><![CDATA[
arXiv:2505.21496v1 Announce Type: cross 
Abstract: In this paper, we introduce UI-Genie, a self-improving framework addressing two key challenges in GUI agents: verification of trajectory outcome is challenging and high-quality training data are not scalable. These challenges are addressed by a reward model and a self-improving pipeline, respectively. The reward model, UI-Genie-RM, features an image-text interleaved architecture that efficiently pro- cesses historical context and unifies action-level and task-level rewards. To sup- port the training of UI-Genie-RM, we develop deliberately-designed data genera- tion strategies including rule-based verification, controlled trajectory corruption, and hard negative mining. To address the second challenge, a self-improvement pipeline progressively expands solvable complex GUI tasks by enhancing both the agent and reward models through reward-guided exploration and outcome verification in dynamic environments. For training the model, we generate UI- Genie-RM-517k and UI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI agents while demonstrating high-quality synthetic trajectory gen- eration without manual annotation. Experimental results show that UI-Genie achieves state-of-the-art performance across multiple GUI agent benchmarks with three generations of data-model self-improvement. We open-source our complete framework implementation and generated datasets to facilitate further research in https://github.com/Euphoria16/UI-Genie.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Silence is Not Consensus: Disrupting Agreement Bias in Multi-Agent LLMs via Catfish Agent for Clinical Decision Making</title>
<link>https://arxiv.org/abs/2505.21503</link>
<guid>https://arxiv.org/abs/2505.21503</guid>
<content:encoded><![CDATA[
arXiv:2505.21503v1 Announce Type: cross 
Abstract: Large language models (LLMs) have demonstrated strong potential in clinical question answering, with recent multi-agent frameworks further improving diagnostic accuracy via collaborative reasoning. However, we identify a recurring issue of Silent Agreement, where agents prematurely converge on diagnoses without sufficient critical analysis, particularly in complex or ambiguous cases. We present a new concept called Catfish Agent, a role-specialized LLM designed to inject structured dissent and counter silent agreement. Inspired by the ``catfish effect'' in organizational psychology, the Catfish Agent is designed to challenge emerging consensus to stimulate deeper reasoning. We formulate two mechanisms to encourage effective and context-aware interventions: (i) a complexity-aware intervention that modulates agent engagement based on case difficulty, and (ii) a tone-calibrated intervention articulated to balance critique and collaboration. Evaluations on nine medical Q&amp;A and three medical VQA benchmarks show that our approach consistently outperforms both single- and multi-agent LLMs frameworks, including leading commercial models such as GPT-4o and DeepSeek-R1.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Estimating Motor Symptom Presence and Severity in Parkinson's Disease from Wrist Accelerometer Time Series using ROCKET and InceptionTime</title>
<link>https://arxiv.org/abs/2304.11265</link>
<guid>https://arxiv.org/abs/2304.11265</guid>
<content:encoded><![CDATA[
arXiv:2304.11265v3 Announce Type: replace 
Abstract: Parkinson's disease (PD) is a neurodegenerative condition characterized by frequently changing motor symptoms, necessitating continuous symptom monitoring for more targeted treatment. Classical time series classification and deep learning techniques have demonstrated limited efficacy in monitoring PD symptoms using wearable accelerometer data due to complex PD movement patterns and the small size of available datasets. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) as they are promising for PD symptom monitoring. InceptionTime's high learning capacity is well-suited to modeling complex movement patterns, while ROCKET is suited to small datasets. With random search methodology, we identify the highest-scoring InceptionTime architecture and compare its performance to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motion data from PD patients. Our findings indicate that all approaches can learn to estimate tremor severity and bradykinesia presence with moderate performance but encounter challenges in detecting dyskinesia. Among the presented approaches, ROCKET demonstrates higher scores in identifying dyskinesia, whereas InceptionTime exhibits slightly better performance in tremor and bradykinesia estimation. Notably, both methods outperform the multi-layer perceptron. In conclusion, InceptionTime can classify complex wrist motion time series and holds potential for continuous symptom monitoring in PD with further development.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Sample Sharing for Multi Agent Linear Bandits</title>
<link>https://arxiv.org/abs/2309.08710</link>
<guid>https://arxiv.org/abs/2309.08710</guid>
<content:encoded><![CDATA[
arXiv:2309.08710v3 Announce Type: replace 
Abstract: The multi-agent linear bandit setting is a well-known setting for which designing efficient collaboration between agents remains challenging. This paper studies the impact of data sharing among agents on regret minimization. Unlike most existing approaches, our contribution does not rely on any assumptions on the bandit parameters structure. Our main result formalizes the trade-off between the bias and uncertainty of the bandit parameter estimation for efficient collaboration. This result is the cornerstone of the Bandit Adaptive Sample Sharing (BASS) algorithm, whose efficiency over the current state-of-the-art is validated through both theoretical analysis and empirical evaluations on both synthetic and real-world datasets. Furthermore, we demonstrate that, when agents' parameters display a cluster structure, our algorithm accurately recovers them.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DYMAG: Rethinking Message Passing Using Dynamical-systems-based Waveforms</title>
<link>https://arxiv.org/abs/2309.09924</link>
<guid>https://arxiv.org/abs/2309.09924</guid>
<content:encoded><![CDATA[
arXiv:2309.09924v5 Announce Type: replace 
Abstract: We present DYMAG, a graph neural network based on a novel form of message aggregation. Standard message-passing neural networks, which often aggregate local neighbors via mean-aggregation, can be regarded as convolving with a simple rectangular waveform which is non-zero only on 1-hop neighbors of every vertex. Here, we go beyond such local averaging. We will convolve the node features with more sophisticated waveforms generated using dynamics such as the heat equation, wave equation, and the Sprott model (an example of chaotic dynamics). Furthermore, we use snapshots of these dynamics at different time points to create waveforms at many effective scales. Theoretically, we show that these dynamic waveforms can capture salient information about the graph including connected components, connectivity, and cycle structures even with no features. Empirically, we test DYMAG on both real and synthetic benchmarks to establish that DYMAG outperforms baseline models on recovery of graph persistence, generating parameters of random graphs, as well as property prediction for proteins, molecules and materials. Our code is available at https://github.com/KrishnaswamyLab/DYMAG.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Quantum Speedups in Regret Analysis of Infinite Horizon Average-Reward Markov Decision Processes</title>
<link>https://arxiv.org/abs/2310.11684</link>
<guid>https://arxiv.org/abs/2310.11684</guid>
<content:encoded><![CDATA[
arXiv:2310.11684v4 Announce Type: replace 
Abstract: This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimal Pricing for Data-Augmented AutoML Marketplaces</title>
<link>https://arxiv.org/abs/2310.17843</link>
<guid>https://arxiv.org/abs/2310.17843</guid>
<content:encoded><![CDATA[
arXiv:2310.17843v2 Announce Type: replace 
Abstract: Organizations often lack sufficient data to effectively train machine learning (ML) models, while others possess valuable data that remains underutilized. Data markets promise to unlock substantial value by matching data suppliers with demand from ML consumers. However, market design involves addressing intricate challenges, including data pricing, fairness, robustness, and strategic behavior. In this paper, we propose a pragmatic data-augmented AutoML market that seamlessly integrates with existing cloud-based AutoML platforms such as Google's Vertex AI and Amazon's SageMaker. Unlike standard AutoML solutions, our design automatically augments buyer-submitted training data with valuable external datasets, pricing the resulting models based on their measurable performance improvements rather than computational costs as the status quo. Our key innovation is a pricing mechanism grounded in the instrumental value - the marginal model quality improvement - of externally sourced data. This approach bypasses direct dataset pricing complexities, mitigates strategic buyer behavior, and accommodates diverse buyer valuations through menu-based options. By integrating automated data and model discovery, our solution not only enhances ML outcomes but also establishes an economically sustainable framework for monetizing external data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Concentration Bound for TD(0) with Function Approximation</title>
<link>https://arxiv.org/abs/2312.10424</link>
<guid>https://arxiv.org/abs/2312.10424</guid>
<content:encoded><![CDATA[
arXiv:2312.10424v3 Announce Type: replace 
Abstract: We derive a concentration bound of the type `for all $n \geq n_0$ for some $n_0$' for TD(0) with linear function approximation. We work with online TD learning with samples from a single sample path of the underlying Markov chain. This makes our analysis significantly different from offline TD learning or TD learning with access to independent samples from the stationary distribution of the Markov chain. We treat TD(0) as a contractive stochastic approximation algorithm, with both martingale and Markov noises. Markov noise is handled using the Poisson equation and the lack of almost sure guarantees on boundedness of iterates is handled using the concept of relaxed concentration inequalities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Retrieve to Explain: Evidence-driven Predictions for Explainable Drug Target Identification</title>
<link>https://arxiv.org/abs/2402.04068</link>
<guid>https://arxiv.org/abs/2402.04068</guid>
<content:encoded><![CDATA[
arXiv:2402.04068v4 Announce Type: replace 
Abstract: Language models hold incredible promise for enabling scientific discovery by synthesizing massive research corpora. Many complex scientific research questions have multiple plausible answers, each supported by evidence of varying strength. However, existing language models lack the capability to quantitatively and faithfully compare answer plausibility in terms of supporting evidence. To address this, we introduce Retrieve to Explain (R2E), a retrieval-based model that scores and ranks all possible answers to a research question based on evidence retrieved from a document corpus. The architecture represents each answer only in terms of its supporting evidence, with the answer itself masked. This allows us to extend feature attribution methods such as Shapley values, to transparently attribute answer scores to supporting evidence at inference time. The architecture also allows incorporation of new evidence without retraining, including non-textual data modalities templated into natural language. We developed R2E for the challenging scientific discovery task of drug target identification, a human-in-the-loop process where failures are extremely costly and explainability paramount. When predicting whether drug targets will subsequently be confirmed as efficacious in clinical trials, R2E not only matches non-explainable literature-based models but also surpasses a genetics-based target identification approach used throughout the pharmaceutical industry.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>T-REX: Mixture-of-Rank-One-Experts with Semantic-aware Intuition for Multi-task Large Language Model Finetuning</title>
<link>https://arxiv.org/abs/2404.08985</link>
<guid>https://arxiv.org/abs/2404.08985</guid>
<content:encoded><![CDATA[
arXiv:2404.08985v2 Announce Type: replace 
Abstract: Large language models (LLMs) encounter significant adaptation challenges in diverse multitask finetuning. Mixture-of-experts (MoE) provides a promising solution with a dynamic architecture, enabling effective task decoupling. However, scaling up the number of MoE experts incurs substantial parameter and computational overheads and suffers from limited performance gain due to naive routing mechanisms. In this paper, we design a novel framework, mix\underline{\textbf{T}}ure\underline{\textbf{-}}of-\underline{\textbf{R}}ank-on\underline{\textbf{E}}-e\underline{\textbf{X}}perts (\texttt{T-REX}), which leverages the combination of ultra-low rank experts to construct LoRA weights on pretrained LLMs. The rank-1 experts enable a mix-and-match mechanism to quadratically expand the vector subspace of experts with linear parameter overheads, achieving approximate error reduction with optimal efficiency. In addition, T-REX offers implicit guidance to the router, leveraging the inherent semantic clustering of training embeddings as prior knowledge, enabling optimized feature allocation across experts for a smoother convergence. Extensive theoretical and empirical results demonstrate that T-REX achieves superior efficiency and generalizability across diverse tasks. Compared with other LoRA-based methods, T-REX achieves up to 1.78\% mean accuracy improvement with around 30\%-40\% less trainable parameters across 14 public datasets. \href{https://github.com/RoyZry98/T-REX-Pytorch}{Code} is available.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Stochastic Online Conformal Prediction with Semi-Bandit Feedback</title>
<link>https://arxiv.org/abs/2405.13268</link>
<guid>https://arxiv.org/abs/2405.13268</guid>
<content:encoded><![CDATA[
arXiv:2405.13268v3 Announce Type: replace 
Abstract: Conformal prediction has emerged as an effective strategy for uncertainty quantification by modifying a model to output sets of labels instead of a single label. These prediction sets come with the guarantee that they contain the true label with high probability. However, conformal prediction typically requires a large calibration dataset of i.i.d. examples. We consider the online learning setting, where examples arrive over time, and the goal is to construct prediction sets dynamically. Departing from existing work, we assume semi-bandit feedback, where we only observe the true label if it is contained in the prediction set. For instance, consider calibrating a document retrieval model to a new domain; in this setting, a user would only be able to provide the true label if the target document is in the prediction set of retrieved documents. We propose a novel conformal prediction algorithm targeted at this setting, and prove that it obtains sublinear regret compared to the optimal conformal predictor. We evaluate our algorithm on a retrieval task, an image classification task, and an auction price-setting task, and demonstrate that it empirically achieves good performance compared to several baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>What is Fair? Defining Fairness in Machine Learning for Health</title>
<link>https://arxiv.org/abs/2406.09307</link>
<guid>https://arxiv.org/abs/2406.09307</guid>
<content:encoded><![CDATA[
arXiv:2406.09307v5 Announce Type: replace 
Abstract: Ensuring that machine learning (ML) models are safe, effective, and equitable across all patients is critical for clinical decision-making and for preventing the amplification of existing health disparities. In this work, we examine how fairness is conceptualized in ML for health, including why ML models may lead to unfair decisions and how fairness has been measured in diverse real-world applications. We review commonly used fairness notions within group, individual, and causal-based frameworks. We also discuss the outlook for future research and highlight opportunities and challenges in operationalizing fairness in health-focused applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Random Walk Diffusion for Efficient Large-Scale Graph Generation</title>
<link>https://arxiv.org/abs/2408.04461</link>
<guid>https://arxiv.org/abs/2408.04461</guid>
<content:encoded><![CDATA[
arXiv:2408.04461v2 Announce Type: replace 
Abstract: Graph generation addresses the problem of generating new graphs that have a data distribution similar to real-world graphs. While previous diffusion-based graph generation methods have shown promising results, they often struggle to scale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive RandOm Walk Diffusion), a novel random walk-based diffusion approach for efficient large-scale graph generation. Our method encompasses two components in an iterative process of random walk sampling and graph pruning. We demonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing other baseline methods in terms of both generation time and multiple graph statistics, reflecting the high quality of the generated graphs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Advancing Molecular Machine Learning Representations with Stereoelectronics-Infused Molecular Graphs</title>
<link>https://arxiv.org/abs/2408.04520</link>
<guid>https://arxiv.org/abs/2408.04520</guid>
<content:encoded><![CDATA[
arXiv:2408.04520v2 Announce Type: replace 
Abstract: Molecular representation is a critical element in our understanding of the physical world and the foundation for modern molecular machine learning. Previous molecular machine learning models have employed strings, fingerprints, global features, and simple molecular graphs that are inherently information-sparse representations. However, as the complexity of prediction tasks increases, the molecular representation needs to encode higher fidelity information. This work introduces a novel approach to infusing quantum-chemical-rich information into molecular graphs via stereoelectronic effects, enhancing expressivity and interpretability. Learning to predict the stereoelectronics-infused representation with a tailored double graph neural network workflow enables its application to any downstream molecular machine learning task without expensive quantum chemical calculations. We show that the explicit addition of stereoelectronic information significantly improves the performance of message-passing 2D machine learning models for molecular property prediction. We show that the learned representations trained on small molecules can accurately extrapolate to much larger molecular structures, yielding chemical insight into orbital interactions for previously intractable systems, such as entire proteins, opening new avenues of molecular design. Finally, we have developed a web application (simg.cheme.cmu.edu) where users can rapidly explore stereoelectronic information for their own molecular systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Large Language Models Understand Symbolic Graphics Programs?</title>
<link>https://arxiv.org/abs/2408.08313</link>
<guid>https://arxiv.org/abs/2408.08313</guid>
<content:encoded><![CDATA[
arXiv:2408.08313v4 Announce Type: replace 
Abstract: Against the backdrop of enthusiasm for large language models (LLMs), there is a growing need to scientifically assess their capabilities and shortcomings. This is nontrivial in part because it is difficult to find tasks which the models have not encountered during training. Utilizing symbolic graphics programs, we propose a domain well-suited to test multiple spatial-semantic reasoning skills of LLMs. Popular in computer graphics, these programs procedurally generate visual data. While LLMs exhibit impressive skills in general program synthesis and analysis, symbolic graphics programs offer a new layer of evaluation: they allow us to test an LLM's ability to answer semantic questions about the images or 3D geometries without a vision encoder. To semantically understand the symbolic programs, LLMs would need to possess the ability to "imagine" and reason how the corresponding graphics content would look with only the symbolic description of the local curvatures and strokes. We use this task to evaluate LLMs by creating a large benchmark for the semantic visual understanding of symbolic graphics programs, built procedurally with minimal human effort. Particular emphasis is placed on transformations of images that leave the image level semantics invariant while introducing significant changes to the underlying program. We evaluate commercial and open-source LLMs on our benchmark to assess their ability to reason about visual output of programs, finding that LLMs considered stronger at reasoning generally perform better. Lastly, we introduce a novel method to improve this ability -- Symbolic Instruction Tuning (SIT), in which the LLM is finetuned with pre-collected instruction data on symbolic graphics programs. Interestingly, we find that SIT not only improves LLM's understanding on symbolic programs, but it also improves general reasoning ability on various other benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MODULI: Unlocking Preference Generalization via Diffusion Models for Offline Multi-Objective Reinforcement Learning</title>
<link>https://arxiv.org/abs/2408.15501</link>
<guid>https://arxiv.org/abs/2408.15501</guid>
<content:encoded><![CDATA[
arXiv:2408.15501v2 Announce Type: replace 
Abstract: Multi-objective Reinforcement Learning (MORL) seeks to develop policies that simultaneously optimize multiple conflicting objectives, but it requires extensive online interactions. Offline MORL provides a promising solution by training on pre-collected datasets to generalize to any preference upon deployment. However, real-world offline datasets are often conservatively and narrowly distributed, failing to comprehensively cover preferences, leading to the emergence of out-of-distribution (OOD) preference areas. Existing offline MORL algorithms exhibit poor generalization to OOD preferences, resulting in policies that do not align with preferences. Leveraging the excellent expressive and generalization capabilities of diffusion models, we propose MODULI (Multi-objective Diffusion Planner with Sliding Guidance), which employs a preference-conditioned diffusion model as a planner to generate trajectories that align with various preferences and derive action for decision-making. To achieve accurate generation, MODULI introduces two return normalization methods under diverse preferences for refining guidance. To further enhance generalization to OOD preferences, MODULI proposes a novel sliding guidance mechanism, which involves training an additional slider adapter to capture the direction of preference changes. Incorporating the slider, it transitions from in-distribution (ID) preferences to generating OOD preferences, patching, and extending the incomplete Pareto front. Extensive experiments on the D4MORL benchmark demonstrate that our algorithm outperforms state-of-the-art Offline MORL baselines, exhibiting excellent generalization to OOD preferences.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Amortized Bayesian Workflow</title>
<link>https://arxiv.org/abs/2409.04332</link>
<guid>https://arxiv.org/abs/2409.04332</guid>
<content:encoded><![CDATA[
arXiv:2409.04332v2 Announce Type: replace 
Abstract: Bayesian inference often faces a trade-off between computational speed and sampling accuracy. We propose an adaptive workflow that integrates rapid amortized inference with gold-standard MCMC techniques to achieve a favorable combination of both speed and accuracy when performing inference on many observed datasets. Our approach uses principled diagnostics to guide the choice of inference method for each dataset, moving along the Pareto front from fast amortized sampling via generative neural networks to slower but guaranteed-accurate MCMC when needed. By reusing computations across steps, our workflow synergizes amortized and MCMC-based inference. We demonstrate the effectiveness of this integrated approach on several synthetic and real-world problems with tens of thousands of datasets, showing efficiency gains while maintaining high posterior quality.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Symmetry constrained neural networks for detection and localization of damage in metal plates</title>
<link>https://arxiv.org/abs/2409.06084</link>
<guid>https://arxiv.org/abs/2409.06084</guid>
<content:encoded><![CDATA[
arXiv:2409.06084v3 Announce Type: replace 
Abstract: The present paper is concerned with deep learning techniques applied to detection and localization of damage in a thin aluminum plate. We used data collected on a tabletop apparatus by mounting to the plate four piezoelectric transducers, each of which took turn to generate a Lamb wave that then traversed the region of interest before being received by the remaining three sensors. On training a neural network to analyze time-series data of the material response, which displayed damage-reflective features whenever the plate guided waves interacted with a contact load, we achieved a model that detected with greater than $99\%$ accuracy in addition to a model that localized with $2.58 \pm 0.12$ mm mean distance error. For each task, the best-performing model was designed according to the inductive bias that our transducers were both similar and arranged in a square pattern on a nearly uniform plate.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Implicit Dynamical Flow Fusion (IDFF) for Generative Modeling</title>
<link>https://arxiv.org/abs/2409.14599</link>
<guid>https://arxiv.org/abs/2409.14599</guid>
<content:encoded><![CDATA[
arXiv:2409.14599v4 Announce Type: replace 
Abstract: Conditional Flow Matching (CFM) models can generate high-quality samples from a non-informative prior, but they can be slow, often needing hundreds of network evaluations (NFE). To address this, we propose Implicit Dynamical Flow Fusion (IDFF); IDFF learns a new vector field with an additional momentum term that enables taking longer steps during sample generation while maintaining the fidelity of the generated distribution. Consequently, IDFFs reduce the NFEs by a factor of ten (relative to CFMs) without sacrificing sample quality, enabling rapid sampling and efficient handling of image and time-series data generation tasks. We evaluate IDFF on standard benchmarks such as CIFAR-10 and CelebA for image generation, where we achieve likelihood and quality performance comparable to CFMs and diffusion-based models with fewer NFEs. IDFF also shows superior performance on time-series datasets modeling, including molecular simulation and sea surface temperature (SST) datasets, highlighting its versatility and effectiveness across different domains.\href{https://github.com/MrRezaeiUofT/IDFF}{Github Repository}
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Graph Prompt Work? A Data Operation Perspective with Theoretical Analysis</title>
<link>https://arxiv.org/abs/2410.01635</link>
<guid>https://arxiv.org/abs/2410.01635</guid>
<content:encoded><![CDATA[
arXiv:2410.01635v2 Announce Type: replace 
Abstract: In recent years, graph prompting has emerged as a promising research direction, enabling the learning of additional tokens or subgraphs appended to the original graphs without requiring retraining of pre-trained graph models across various applications. This novel paradigm, shifting from the traditional pretraining and finetuning to pretraining and prompting has shown significant empirical success in simulating graph data operations, with applications ranging from recommendation systems to biological networks and graph transferring. However, despite its potential, the theoretical underpinnings of graph prompting remain underexplored, raising critical questions about its fundamental effectiveness. The lack of rigorous theoretical proof of why and how much it works is more like a dark cloud over the graph prompt area to go further. To fill this gap, this paper introduces a theoretical framework that rigorously analyzes graph prompting from a data operation perspective. Our contributions are threefold: First, we provide a formal guarantee theorem, demonstrating graph prompts capacity to approximate graph transformation operators, effectively linking upstream and downstream tasks. Second, we derive upper bounds on the error of these data operations by graph prompts for a single graph and extend this discussion to batches of graphs, which are common in graph model training. Third, we analyze the distribution of data operation errors, extending our theoretical findings from linear graph models (e.g., GCN) to non-linear graph models (e.g., GAT). Extensive experiments support our theoretical results and confirm the practical implications of these guarantees.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Foundation Models on a Budget: Approximating Blocks in Large Vision Models</title>
<link>https://arxiv.org/abs/2410.04941</link>
<guid>https://arxiv.org/abs/2410.04941</guid>
<content:encoded><![CDATA[
arXiv:2410.04941v5 Announce Type: replace 
Abstract: Foundation Models have shown impressive performance in various tasks and domains, yet they require massive computational resources, raising concerns about accessibility and sustainability. Previous attempts to reduce foundation model size fall short of fully addressing the problem, as they end up increasing computational load through additional training steps. Recent works reveal that deep neural networks exhibit internal representation similarities. While inter-network similarities have enabled techniques such as model stitching and merging, intra-network similarities remain underexplored for improving efficiency. In this paper, we propose Transformer Blocks Approximation (TBA), a novel method that leverages intra-network similarities to identify and approximate transformer blocks in large vision models. TBA replaces these blocks using lightweight, closed-form transformations, without retraining or fine-tuning the rest of the model. The proposed method reduces the number of parameters while having minimal impact on the downstream task. We validate the effectiveness and generalizability of TBA through extensive experiments across multiple datasets (e.g., Imagenet-1k and CIFAR100) and state-of-the-art pretrained vision models (e.g, ViT, DiNO-v2, and DEiT).
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AI Learning Algorithms: Deep Learning, Hybrid Models, and Large-Scale Model Integration</title>
<link>https://arxiv.org/abs/2410.09186</link>
<guid>https://arxiv.org/abs/2410.09186</guid>
<content:encoded><![CDATA[
arXiv:2410.09186v3 Announce Type: replace 
Abstract: In this paper, we discuss learning algorithms and their importance in different types of applications which includes training to identify important patterns and features in a straightforward, easy-to-understand manner. We will review the main concepts of artificial intelligence (AI), machine learning (ML), deep learning (DL), and hybrid models. Some important subsets of Machine Learning algorithms such as supervised, unsupervised, and reinforcement learning are also discussed in this paper. These techniques can be used for some important tasks like prediction, classification, and segmentation. Convolutional Neural Networks (CNNs) are used for image and video processing and many more applications. We dive into the architecture of CNNs and how to integrate CNNs with ML algorithms to build hybrid models. This paper explores the vulnerability of learning algorithms to noise, leading to misclassification. We further discuss the integration of learning algorithms with Large Language Models (LLM) to generate coherent responses applicable to many domains such as healthcare, marketing, and finance by learning important patterns from large volumes of data. Furthermore, we discuss the next generation of learning algorithms and how we may have an unified Adaptive and Dynamic Network to perform important tasks. Overall, this article provides brief overview of learning algorithms, exploring their current state, applications and future direction.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>New Paradigm of Adversarial Training: Releasing Accuracy-Robustness Trade-Off via Dummy Class</title>
<link>https://arxiv.org/abs/2410.12671</link>
<guid>https://arxiv.org/abs/2410.12671</guid>
<content:encoded><![CDATA[
arXiv:2410.12671v2 Announce Type: replace 
Abstract: Adversarial Training (AT) is one of the most effective methods to enhance the robustness of Deep Neural Networks (DNNs). However, existing AT methods suffer from an inherent accuracy-robustness trade-off. Previous works have studied this issue under the current AT paradigm, but still face over 10% accuracy reduction without significant robustness improvement over simple baselines such as PGD-AT. This inherent trade-off raises a question: Whether the current AT paradigm, which assumes to learn corresponding benign and adversarial samples as the same class, inappropriately mixes clean and robust objectives that may be essentially inconsistent. In fact, our empirical results show that up to 40% of CIFAR-10 adversarial samples always fail to satisfy such an assumption across various AT methods and robust models, explicitly indicating the room for improvement of the current AT paradigm. To relax from this overstrict assumption and the tension between clean and robust learning, in this work, we propose a new AT paradigm by introducing an additional dummy class for each original class, aiming to accommodate hard adversarial samples with shifted distribution after perturbation. The robustness w.r.t. these adversarial samples can be achieved by runtime recovery from the predicted dummy classes to the corresponding original ones, without conflicting with the clean objective on accuracy of benign samples. Finally, based on our new paradigm, we propose a novel DUmmy Classes-based Adversarial Training (DUCAT) method that concurrently improves accuracy and robustness in a plug-and-play manner only relevant to logits, loss, and a proposed two-hot soft label-based supervised signal. Our method outperforms state-of-the-art (SOTA) benchmarks, effectively releasing the current trade-off. The code is available at https://github.com/FlaAI/DUCAT.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>EPIC: Efficient Position-Independent Caching for Serving Large Language Models</title>
<link>https://arxiv.org/abs/2410.15332</link>
<guid>https://arxiv.org/abs/2410.15332</guid>
<content:encoded><![CDATA[
arXiv:2410.15332v3 Announce Type: replace 
Abstract: Large Language Models (LLMs) show great capabilities in a wide range of applications, but serving them efficiently becomes increasingly challenging as requests (prompts) become more complex. Context caching improves serving performance by reusing Key-Value (KV) vectors, the intermediate representations of tokens that are repeated across requests. However, existing context caching requires exact prefix matches across requests, limiting reuse cases in settings such as few-shot learning and retrieval-augmented generation, where immutable content (e.g., documents) remains unchanged across requests but is preceded by varying prefixes. Position-Independent Caching (PIC) addresses this issue by enabling modular reuse of the KV vectors regardless of prefixes. We formalize PIC and advance prior work by introducing EPIC, a serving system incorporating our new LegoLink algorithm, which mitigates the inappropriate "attention sink" effect at every document beginning, to maintain accuracy with minimal computation. Experiments show that EPIC achieves up to 8x improvements in Time-To-First-Token (TTFT) and 7x throughput gains over existing systems, with negligible or no accuracy loss.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Securing Federated Learning against Backdoor Threats with Foundation Model Integration</title>
<link>https://arxiv.org/abs/2410.17573</link>
<guid>https://arxiv.org/abs/2410.17573</guid>
<content:encoded><![CDATA[
arXiv:2410.17573v3 Announce Type: replace 
Abstract: Federated Learning (FL) enables decentralized model training while preserving privacy. Recently, the integration of Foundation Models (FMs) into FL has enhanced performance but introduced a novel backdoor attack mechanism. Attackers can exploit FM vulnerabilities to embed backdoors into synthetic data generated by FMs. During global model fusion, these backdoors are transferred to the global model through compromised synthetic data, subsequently infecting all client models. Existing FL backdoor defenses are ineffective against this novel attack due to its fundamentally different mechanism compared to classic ones. In this work, we propose a novel data-free defense strategy that addresses both classic and novel backdoor attacks in FL. The shared attack pattern lies in the abnormal activations within the hidden feature space during model aggregation. Hence, we propose to constrain internal activations to remain within reasonable ranges, effectively mitigating attacks while preserving model functionality. The activation constraints are optimized using synthetic data alongside FL training. Extensive experiments demonstrate its effectiveness against both novel and classic backdoor attacks, outperforming existing defenses.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Robustness of Adversarial Training Against Uncertainty Attacks</title>
<link>https://arxiv.org/abs/2410.21952</link>
<guid>https://arxiv.org/abs/2410.21952</guid>
<content:encoded><![CDATA[
arXiv:2410.21952v2 Announce Type: replace 
Abstract: In learning problems, the noise inherent to the task at hand hinders the possibility to infer without a certain degree of uncertainty. Quantifying this uncertainty, regardless of its wide use, assumes high relevance for security-sensitive applications. Within these scenarios, it becomes fundamental to guarantee good (i.e., trustworthy) uncertainty measures, which downstream modules can securely employ to drive the final decision-making process. However, an attacker may be interested in forcing the system to produce either (i) highly uncertain outputs jeopardizing the system's availability or (ii) low uncertainty estimates, making the system accept uncertain samples that would instead require a careful inspection (e.g., human intervention). Therefore, it becomes fundamental to understand how to obtain robust uncertainty estimates against these kinds of attacks. In this work, we reveal both empirically and theoretically that defending against adversarial examples, i.e., carefully perturbed samples that cause misclassification, additionally guarantees a more secure, trustworthy uncertainty estimate under common attack scenarios without the need for an ad-hoc defense strategy. To support our claims, we evaluate multiple adversarial-robust models from the publicly available benchmark RobustBench on the CIFAR-10 and ImageNet datasets.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RE-Bench: Evaluating frontier AI R&amp;D capabilities of language model agents against human experts</title>
<link>https://arxiv.org/abs/2411.15114</link>
<guid>https://arxiv.org/abs/2411.15114</guid>
<content:encoded><![CDATA[
arXiv:2411.15114v2 Announce Type: replace 
Abstract: Frontier AI safety policies highlight automation of AI research and development (R&amp;D) by AI agents as an important capability to anticipate. However, there exist few evaluations for AI R&amp;D capabilities, and none that are highly realistic and have a direct comparison to human performance. We introduce RE-Bench (Research Engineering Benchmark, v1), which consists of 7 challenging, open-ended ML research engineering environments and data from 71 8-hour attempts by 61 distinct human experts. We confirm that our experts make progress in the environments given 8 hours, with 82% of expert attempts achieving a non-zero score and 24% matching or exceeding our strong reference solutions. We compare humans to several public frontier models through best-of-k with varying time budgets and agent designs, and find that the best AI agents achieve a score 4x higher than human experts when both are given a total time budget of 2 hours per environment. However, humans currently display better returns to increasing time budgets, narrowly exceeding the top AI agent scores given an 8-hour budget, and achieving 2x the score of the top AI agent when both are given 32 total hours (across different attempts). Qualitatively, we find that modern AI agents possess significant expertise in many ML topics -- e.g. an agent wrote a faster custom Triton kernel than any of our human experts' -- and can generate and test solutions over ten times faster than humans, at much lower cost. We open-source the evaluation environments, human expert data, analysis code and agent trajectories to facilitate future research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Controlling Participation in Federated Learning with Feedback</title>
<link>https://arxiv.org/abs/2411.19242</link>
<guid>https://arxiv.org/abs/2411.19242</guid>
<content:encoded><![CDATA[
arXiv:2411.19242v2 Announce Type: replace 
Abstract: We address the problem of client participation in federated learning, where traditional methods typically rely on a random selection of a small subset of clients for each training round. In contrast, we propose FedBack, a deterministic approach that leverages control-theoretic principles to manage client participation in ADMM-based federated learning. FedBack models client participation as a discrete-time dynamical system and employs an integral feedback controller to adjust each client's participation rate individually, based on the client's optimization dynamics. We provide global convergence guarantees for our approach by building on the recent federated learning research. Numerical experiments on federated image classification demonstrate that FedBack achieves up to 50\% improvement in communication and computational efficiency over algorithms that rely on a random selection of clients.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL-SPH: Learning to Achieve Feasible Solutions for Integer Linear Programs</title>
<link>https://arxiv.org/abs/2411.19517</link>
<guid>https://arxiv.org/abs/2411.19517</guid>
<content:encoded><![CDATA[
arXiv:2411.19517v5 Announce Type: replace 
Abstract: Integer linear programming (ILP) is widely utilized for various combinatorial optimization problems. Primal heuristics play a crucial role in quickly finding feasible solutions for NP-hard ILP. Although \textit{end-to-end learning}-based primal heuristics (E2EPH) have recently been proposed, they are typically unable to independently generate feasible solutions and mainly focus on binary variables. Ensuring feasibility is critical, especially when handling non-binary integer variables. To address this challenge, we propose RL-SPH, a novel reinforcement learning-based start primal heuristic capable of independently generating feasible solutions, even for ILP involving non-binary integers. Experimental results demonstrate that RL-SPH rapidly obtains high-quality feasible solutions, achieving on average a 44x lower primal gap and a 2.3x lower primal integral compared to existing primal heuristics.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interlocking-free Selective Rationalization Through Genetic-based Learning</title>
<link>https://arxiv.org/abs/2412.10312</link>
<guid>https://arxiv.org/abs/2412.10312</guid>
<content:encoded><![CDATA[
arXiv:2412.10312v2 Announce Type: replace 
Abstract: A popular end-to-end architecture for selective rationalization is the select-then-predict pipeline, comprising a generator to extract highlights fed to a predictor. Such a cooperative system suffers from suboptimal equilibrium minima due to the dominance of one of the two modules, a phenomenon known as interlocking. While several contributions aimed at addressing interlocking, they only mitigate its effect, often by introducing feature-based heuristics, sampling, and ad-hoc regularizations. We present GenSPP, the first interlocking-free architecture for selective rationalization that does not require any learning overhead, as the above-mentioned. GenSPP avoids interlocking by performing disjoint training of the generator and predictor via genetic global search. Experiments on a synthetic and a real-world benchmark show that our model outperforms several state-of-the-art competitors.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Personalized Clustering via Targeted Representation Learning</title>
<link>https://arxiv.org/abs/2412.13690</link>
<guid>https://arxiv.org/abs/2412.13690</guid>
<content:encoded><![CDATA[
arXiv:2412.13690v3 Announce Type: replace 
Abstract: Clustering traditionally aims to reveal a natural grouping structure within unlabeled data. However, this structure may not always align with users' preferences. In this paper, we propose a personalized clustering method that explicitly performs targeted representation learning by interacting with users via modicum task information (e.g., $\textit{must-link}$ or $\textit{cannot-link}$ pairs) to guide the clustering direction. We query users with the most informative pairs, i.e., those pairs most hard to cluster and those most easy to miscluster, to facilitate the representation learning in terms of the clustering preference. Moreover, by exploiting attention mechanism, the targeted representation is learned and augmented. By leveraging the targeted representation and constrained contrastive loss as well, personalized clustering is obtained. Theoretically, we verify that the risk of personalized clustering is tightly bounded, guaranteeing that active queries to users do mitigate the clustering risk. Experimentally, extensive results show that our method performs well across different clustering tasks and datasets, even when only a limited number of queries are available.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Efficiently Scaling LLM Reasoning with Certaindex</title>
<link>https://arxiv.org/abs/2412.20993</link>
<guid>https://arxiv.org/abs/2412.20993</guid>
<content:encoded><![CDATA[
arXiv:2412.20993v2 Announce Type: replace 
Abstract: Test-time reasoning algorithms such as chain-of-thought, self-consistency, and MCTS enhance LLM problem-solving but can wastefully generate many tokens without improving accuracy. At the same time, we observe that these algorithms exhibit answer stabilization: their intermediate solutions often cease to change after a certain point, and further investment of compute does not change their final answer. To quantify this phenomenon, we introduce Certaindex, an algorithm-agnostic metric measuring this evolving stability, signaling when further computation is unlikely to alter the final result. Certaindex is lightweight, can accelerate reasoning program inference via early exit, and further enables dynamic token allocation, gang scheduling, and many opportunities when integrated with real-world LLM serving systems. To quantify real-world benefits, we built Certaindex as a scheduler into Dynasor, our reasoning-aware LLM serving system, and demonstrate up to 50% compute savings and 3.3x higher throughput in real workloads with no accuracy drop. Our code is available at https://github.com/hao-ai-lab/Dynasor.git
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ResKoopNet: Learning Koopman Representations for Complex Dynamics with Spectral Residuals</title>
<link>https://arxiv.org/abs/2501.00701</link>
<guid>https://arxiv.org/abs/2501.00701</guid>
<content:encoded><![CDATA[
arXiv:2501.00701v4 Announce Type: replace 
Abstract: Analyzing the long-term behavior of high-dimensional nonlinear dynamical systems remains a significant challenge. While the Koopman operator framework provides a powerful global linearization tool, current methods for approximating its spectral components often face theoretical limitations and depend on predefined dictionaries. Residual Dynamic Mode Decomposition (ResDMD) advanced the field by introducing the \emph{spectral residual} to assess Koopman operator approximation accuracy; however, its approach of only filtering precomputed spectra prevents the discovery of the operator's complete spectral information, a limitation known as the `spectral inclusion' problem. We introduce ResKoopNet (Residual-based Koopman-learning Network), a novel method that directly addresses this by explicitly minimizing the \emph{spectral residual} to compute Koopman eigenpairs. This enables the identification of a more precise and complete Koopman operator spectrum. Using neural networks, our approach provides theoretical guarantees while maintaining computational adaptability. Experiments on a variety of physical and biological systems show that ResKoopNet achieves more accurate spectral approximations than existing methods, particularly for high-dimensional systems and those with continuous spectra, which demonstrates its effectiveness as a tool for analyzing complex dynamical systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unraveling Indirect In-Context Learning Using Influence Functions</title>
<link>https://arxiv.org/abs/2501.01473</link>
<guid>https://arxiv.org/abs/2501.01473</guid>
<content:encoded><![CDATA[
arXiv:2501.01473v2 Announce Type: replace 
Abstract: In this work, we introduce a novel paradigm for generalized In-Context Learning (ICL), termed Indirect In-Context Learning. In Indirect ICL, we explore demonstration selection strategies tailored for two distinct real-world scenarios: Mixture of Tasks and Noisy ICL. We systematically evaluate the effectiveness of Influence Functions (IFs) as a selection tool for these settings, highlighting the potential of IFs to better capture the informativeness of examples within the demonstration pool. For the Mixture of Tasks setting, demonstrations are drawn from 28 diverse tasks, including MMLU, BigBench, StrategyQA, and CommonsenseQA. We demonstrate that combining BertScore-Recall (BSR) with an IF surrogate model can further improve performance, leading to average absolute accuracy gains of 0.37\% and 1.45\% for 3-shot and 5-shot setups when compared to traditional ICL metrics. In the Noisy ICL setting, we examine scenarios where demonstrations might be mislabeled or have adversarial noise. Our experiments show that reweighting traditional ICL selectors (BSR and Cosine Similarity) with IF-based selectors boosts accuracy by an average of 2.90\% for Cosine Similarity and 2.94\% for BSR on noisy GLUE benchmarks. For the adversarial sub-setting, we show the utility of using IFs for task-agnostic demonstration selection for backdoor attack mitigation. Showing a 32.89\% reduction in Attack Success Rate compared to task-aware methods. In sum, we propose a robust framework for demonstration selection that generalizes beyond traditional ICL, offering valuable insights into the role of IFs for Indirect ICL.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Randomly Sampled Language Reasoning Problems Explain Limits of LLMs</title>
<link>https://arxiv.org/abs/2501.02825</link>
<guid>https://arxiv.org/abs/2501.02825</guid>
<content:encoded><![CDATA[
arXiv:2501.02825v5 Announce Type: replace 
Abstract: While LLMs have revolutionized the field of machine learning due to their high performance across a range of tasks, they are known to perform poorly in planning, hallucinate false answers, have degraded performance on less canonical versions of the same task, and answer incorrectly on a variety of specific prompts. There are several emerging theories of LLM performance with some predictive power, among them that LLMs lack world modeling ability, that they have an undesirable bias towards an autoregressive prior, and that they perform less well on more novel problems. The existing literature on novelty has focused on tasks of relatively high complexity, studying perturbations of canonical but complex problems. In this paper, we attempt to isolate novelty as a factor in LLM underperformance. To this end, we consider an extremely simple domain: next token prediction on simple language tasks. The twist is that these language tasks are unseen, as they are randomly drawn from a large, parsimoniously defined set of languages arising from simple grammar rules. This allows us to isolate the effect of task novelty and see if it is sufficient to explain low performance. We find that LLMs uniformly underperform n-gram models (which do not have the capacity for world modeling) on these tasks, both when used as next token predictors and as reasoners.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>More is not always better? Enhancing Many-Shot In-Context Learning with Differentiated and Reweighting Objectives</title>
<link>https://arxiv.org/abs/2501.04070</link>
<guid>https://arxiv.org/abs/2501.04070</guid>
<content:encoded><![CDATA[
arXiv:2501.04070v3 Announce Type: replace 
Abstract: Large language models (LLMs) excel at few-shot in-context learning (ICL) without requiring parameter updates. However, as ICL demonstrations increase from a few to many, performance tends to plateau and eventually decline. We identify two primary causes for this trend: the suboptimal negative log-likelihood (NLL) optimization objective and the incremental data noise. To address these issues, we introduce \textit{DrICL}, a novel optimization method that enhances model performance through \textit{Differentiated} and \textit{Reweighting} objectives. Globally, DrICL utilizes differentiated learning to optimize the NLL objective, ensuring that many-shot performance surpasses zero-shot levels. Locally, it dynamically adjusts the weighting of many-shot demonstrations by leveraging cumulative advantages inspired by reinforcement learning, thereby mitigating the impact of noisy data. Recognizing the lack of multi-task datasets with diverse many-shot distributions, we develop the \textit{Many-Shot ICL Benchmark} (ICL-50)-a large-scale benchmark of 50 tasks that cover shot numbers from 1 to 350 within sequences of up to 8,000 tokens-for both fine-tuning and evaluation purposes. Experimental results demonstrate that LLMs enhanced with DrICL achieve significant improvements in many-shot setups across various tasks, including both in-domain and out-of-domain scenarios. We release the code and dataset hoping to facilitate further research in many-shot ICL\footnote{https://github.com/xiaoqzhwhu/DrICL}.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenMol: A Drug Discovery Generalist with Discrete Diffusion</title>
<link>https://arxiv.org/abs/2501.06158</link>
<guid>https://arxiv.org/abs/2501.06158</guid>
<content:encoded><![CDATA[
arXiv:2501.06158v2 Announce Type: replace 
Abstract: Drug discovery is a complex process that involves multiple stages and tasks. However, existing molecular generative models can only tackle some of these tasks. We present Generalist Molecular generative model (GenMol), a versatile framework that uses only a single discrete diffusion model to handle diverse drug discovery scenarios. GenMol generates Sequential Attachment-based Fragment Embedding (SAFE) sequences through non-autoregressive bidirectional parallel decoding, thereby allowing the utilization of a molecular context that does not rely on the specific token ordering while having better sampling efficiency. GenMol uses fragments as basic building blocks for molecules and introduces fragment remasking, a strategy that optimizes molecules by regenerating masked fragments, enabling effective exploration of chemical space. We further propose molecular context guidance (MCG), a guidance method tailored for masked discrete diffusion of GenMol. GenMol significantly outperforms the previous GPT-based model in de novo generation and fragment-constrained generation, and achieves state-of-the-art performance in goal-directed hit generation and lead optimization. These results demonstrate that GenMol can tackle a wide range of drug discovery tasks, providing a unified and versatile approach for molecular design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>It's complicated. The relationship of algorithmic fairness and non-discrimination regulations for high-risk systems in the EU AI Act</title>
<link>https://arxiv.org/abs/2501.12962</link>
<guid>https://arxiv.org/abs/2501.12962</guid>
<content:encoded><![CDATA[
arXiv:2501.12962v3 Announce Type: replace 
Abstract: What constitutes a fair decision? This question is not only difficult for humans but becomes more challenging when Artificial Intelligence (AI) models are used. In light of discriminatory algorithmic behaviors, the EU has recently passed the AI Act, which mandates specific rules for high-risk systems, incorporating both traditional legal non-discrimination regulations and machine learning based algorithmic fairness concepts. This paper aims to bridge these two different concepts in the AI Act through: First, a necessary high-level introduction of both concepts targeting legal and computer science-oriented scholars, and second, an in-depth analysis of the AI Act's relationship between legal non-discrimination regulations and algorithmic fairness. Our analysis reveals three key findings: (1.) Most non-discrimination regulations target only high-risk AI systems. (2.) The regulation of high-risk systems encompasses both data input requirements and output monitoring, though these regulations are partly inconsistent and raise questions of computational feasibility. (3.) Finally, we consider the possible (future) interaction of classical EU non-discrimination law and the AI Act regulations. We recommend developing more specific auditing and testing methodologies for AI systems. This paper aims to serve as a foundation for future interdisciplinary collaboration between legal scholars and computer science-oriented machine learning researchers studying discrimination in AI systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Sparsity for Sample-Efficient Preference Learning: A Theoretical Perspective</title>
<link>https://arxiv.org/abs/2501.18282</link>
<guid>https://arxiv.org/abs/2501.18282</guid>
<content:encoded><![CDATA[
arXiv:2501.18282v3 Announce Type: replace 
Abstract: This paper considers the sample-efficiency of preference learning, which models and predicts human choices based on comparative judgments. The minimax optimal estimation error rate $\Theta(d/n)$ in classical estimation theory requires that the number of samples $n$ scales linearly with the dimensionality of the feature space $d$. However, the high dimensionality of the feature space and the high cost of collecting human-annotated data challenge the efficiency of traditional estimation methods. To remedy this, we leverage sparsity in the preference model and establish sharp error rates. We show that under the sparse random utility model, where the parameter of the reward function is $k$-sparse, the minimax optimal rate can be reduced to $\Theta(k/n \log(d/k))$. Furthermore, we analyze the $\ell_{1}$-regularized estimator and show that it achieves near-optimal rate under mild assumptions on the Gram matrix. Experiments on synthetic data and LLM alignment data validate our theoretical findings, showing that sparsity-aware methods significantly reduce sample complexity and improve prediction accuracy.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Linear $Q$-Learning Does Not Diverge in $L^2$: Convergence Rates to a Bounded Set</title>
<link>https://arxiv.org/abs/2501.19254</link>
<guid>https://arxiv.org/abs/2501.19254</guid>
<content:encoded><![CDATA[
arXiv:2501.19254v4 Announce Type: replace 
Abstract: $Q$-learning is one of the most fundamental reinforcement learning algorithms. It is widely believed that $Q$-learning with linear function approximation (i.e., linear $Q$-learning) suffers from possible divergence until the recent work Meyn (2024) which establishes the ultimate almost sure boundedness of the iterates of linear $Q$-learning. Building on this success, this paper further establishes the first $L^2$ convergence rate of linear $Q$-learning iterates (to a bounded set). Similar to Meyn (2024), we do not make any modification to the original linear $Q$-learning algorithm, do not make any Bellman completeness assumption, and do not make any near-optimality assumption on the behavior policy. All we need is an $\epsilon$-softmax behavior policy with an adaptive temperature. The key to our analysis is the general result of stochastic approximations under Markovian noise with fast-changing transition functions. As a side product, we also use this general result to establish the $L^2$ convergence rate of tabular $Q$-learning with an $\epsilon$-softmax behavior policy, for which we rely on a novel pseudo-contraction property of the weighted Bellman optimality operator.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Low-Rank Adapting Models for Sparse Autoencoders</title>
<link>https://arxiv.org/abs/2501.19406</link>
<guid>https://arxiv.org/abs/2501.19406</guid>
<content:encoded><![CDATA[
arXiv:2501.19406v2 Announce Type: replace 
Abstract: Sparse autoencoders (SAEs) decompose language model representations into a sparse set of linear latent vectors. Recent works have improved SAEs using language model gradients, but these techniques require many expensive backward passes during training and still cause a significant increase in cross entropy loss when SAE reconstructions are inserted into the model. In this work, we improve on these limitations by taking a fundamentally different approach: we use low-rank adaptation (LoRA) to finetune the \textit{language model itself} around a previously trained SAE. We analyze our method across SAE sparsity, SAE width, language model size, LoRA rank, and model layer on the Gemma Scope family of SAEs. In these settings, our method reduces the cross entropy loss gap by 30\% to 55\% when SAEs are inserted during the forward pass. We also find that compared to end-to-end (e2e) SAEs, our approach achieves the same downstream cross entropy loss 3$\times$ to 20$\times$ faster on \gemma and 2$\times$ to 10$\times$ faster on \llama. We further show that our technique improves downstream metrics and can adapt multiple SAEs at once without harming general language model capabilities. Our results demonstrate that improving model interpretability is not limited to post-hoc SAE training; Pareto improvements can also be achieved by directly optimizing the model itself.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>vCache: Verified Semantic Prompt Caching</title>
<link>https://arxiv.org/abs/2502.03771</link>
<guid>https://arxiv.org/abs/2502.03771</guid>
<content:encoded><![CDATA[
arXiv:2502.03771v3 Announce Type: replace 
Abstract: Semantic caches return cached LLM-generated responses for semantically similar prompts to reduce inference latency and cost. They embed cached prompts and store them alongside their response in a vector database. Embedding similarity metrics assign a numerical score to quantify the similarity between a request and its nearest neighbor prompt from the cache. Existing systems use the same static similarity threshold across all requests to determine whether two prompts can share similar responses. However, we observe that static thresholds do not give formal correctness guarantees, can result in unexpected error rates, and lead to suboptimal cache hit rates. This paper proposes vCache, the first verified semantic cache with user-defined error rate guarantees. It employs an online learning algorithm to estimate an optimal threshold for each cached prompt, enabling reliable cache responses without additional training. Our experiments show that vCache consistently meets the specified error bounds while outperforming state-of-the-art static-threshold and fine-tuned embedding baselines. We release the vCache implementation and benchmarks to support future research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ExpProof : Operationalizing Explanations for Confidential Models with ZKPs</title>
<link>https://arxiv.org/abs/2502.03773</link>
<guid>https://arxiv.org/abs/2502.03773</guid>
<content:encoded><![CDATA[
arXiv:2502.03773v2 Announce Type: replace 
Abstract: In principle, explanations are intended as a way to increase trust in machine learning models and are often obligated by regulations. However, many circumstances where these are demanded are adversarial in nature, meaning the involved parties have misaligned interests and are incentivized to manipulate explanations for their purpose. As a result, explainability methods fail to be operational in such settings despite the demand \cite{bordt2022post}. In this paper, we take a step towards operationalizing explanations in adversarial scenarios with Zero-Knowledge Proofs (ZKPs), a cryptographic primitive. Specifically we explore ZKP-amenable versions of the popular explainability algorithm LIME and evaluate their performance on Neural Networks and Random Forests. Our code is publicly available at https://github.com/emlaufer/ExpProof.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Overcoming Spurious Solutions in Semi-Dual Neural Optimal Transport: A Smoothing Approach for Learning the Optimal Transport Plan</title>
<link>https://arxiv.org/abs/2502.04583</link>
<guid>https://arxiv.org/abs/2502.04583</guid>
<content:encoded><![CDATA[
arXiv:2502.04583v2 Announce Type: replace 
Abstract: We address the convergence problem in learning the Optimal Transport (OT) map, where the OT Map refers to a map from one distribution to another while minimizing the transport cost. Semi-dual Neural OT, a widely used approach for learning OT Maps with neural networks, often generates spurious solutions that fail to transfer one distribution to another accurately. We identify a sufficient condition under which the max-min solution of Semi-dual Neural OT recovers the true OT Map. Moreover, to address cases when this sufficient condition is not satisfied, we propose a novel method, OTP, which learns both the OT Map and the Optimal Transport Plan, representing the optimal coupling between two distributions. Under sharp assumptions on the distributions, we prove that our model eliminates the spurious solution issue and correctly solves the OT problem. Our experiments show that the OTP model recovers the optimal transport map where existing methods fail and outperforms current OT-based models in image-to-image translation tasks. Notably, the OTP model can learn stochastic transport maps when deterministic OT Maps do not exist, such as one-to-many tasks like colorization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Lightweight Method to Disrupt Memorized Sequences in LLM</title>
<link>https://arxiv.org/abs/2502.05159</link>
<guid>https://arxiv.org/abs/2502.05159</guid>
<content:encoded><![CDATA[
arXiv:2502.05159v2 Announce Type: replace 
Abstract: As language models scale, their performance improves dramatically across a wide range of tasks, but so does their tendency to memorize and regurgitate parts of their training data verbatim. This tradeoff poses serious legal, ethical, and safety concerns, especially in real-world deployments. Existing mitigation techniques, such as differential privacy or model unlearning, often require retraining or access to internal weights making them impractical for most users. In this work, we introduce TokenSwap, a lightweight, post-hoc defense designed for realistic settings where the user can only access token-level outputs. Our key insight is that while large models are necessary for high task performance, small models (e.g., DistilGPT-2) are often sufficient to assign fluent, grammatically plausible probabilities to common function words - and crucially, they memorize far less. By selectively swapping token probabilities between models, TokenSwap preserves the capabilities of large models while reducing their propensity for verbatim reproduction. Evaluations on Pythia-6.9B and Llama-3-8B show up to a 10$\times$ drop in exact memorization with negligible task degradation. Our method offers a practical, accessible solution for mitigating memorized generation in deployed LLMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards LLM Unlearning Resilient to Relearning Attacks: A Sharpness-Aware Minimization Perspective and Beyond</title>
<link>https://arxiv.org/abs/2502.05374</link>
<guid>https://arxiv.org/abs/2502.05374</guid>
<content:encoded><![CDATA[
arXiv:2502.05374v4 Announce Type: replace 
Abstract: The LLM unlearning technique has recently been introduced to comply with data regulations and address the safety and ethical concerns of LLMs by removing the undesired data-model influence. However, state-of-the-art unlearning methods face a critical vulnerability: they are susceptible to ``relearning'' the removed information from a small number of forget data points, known as relearning attacks. In this paper, we systematically investigate how to make unlearned models robust against such attacks. For the first time, we establish a connection between robust unlearning and sharpness-aware minimization (SAM) through a unified robust optimization framework, in an analogy to adversarial training designed to defend against adversarial attacks. Our analysis for SAM reveals that smoothness optimization plays a pivotal role in mitigating relearning attacks. Thus, we further explore diverse smoothing strategies to enhance unlearning robustness. Extensive experiments on benchmark datasets, including WMDP and MUSE, demonstrate that SAM and other smoothness optimization approaches consistently improve the resistance of LLM unlearning to relearning attacks. Notably, smoothness-enhanced unlearning also helps defend against (input-level) jailbreaking attacks, broadening our proposal's impact in robustifying LLM unlearning. Codes are available at https://github.com/OPTML-Group/Unlearn-Smooth.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Forgetting during Finetuning with Pretraining Data Injection</title>
<link>https://arxiv.org/abs/2502.06042</link>
<guid>https://arxiv.org/abs/2502.06042</guid>
<content:encoded><![CDATA[
arXiv:2502.06042v2 Announce Type: replace 
Abstract: A widespread strategy to obtain a language model that performs well on a target domain is to finetune a pretrained model to perform unsupervised next-token prediction on data from that target domain. Finetuning presents two challenges: (i) if the amount of target data is limited, as in most practical applications, the model will quickly overfit, and (ii) the model will drift away from the original model, forgetting the pretraining data and the generic knowledge that comes with it. We aim to derive scaling laws that quantify these two phenomena for various target domains, amounts of available target data, and model scales. We measure the efficiency of injecting pretraining data into the finetuning data mixture to avoid forgetting and mitigate overfitting. A key practical takeaway from our study is that injecting as little as 1% of pretraining data in the finetuning data mixture prevents the model from forgetting the pretraining set.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Performance of Explainable AI Models with Constrained Concept Refinement</title>
<link>https://arxiv.org/abs/2502.06775</link>
<guid>https://arxiv.org/abs/2502.06775</guid>
<content:encoded><![CDATA[
arXiv:2502.06775v2 Announce Type: replace 
Abstract: The trade-off between accuracy and interpretability has long been a challenge in machine learning (ML). This tension is particularly significant for emerging interpretable-by-design methods, which aim to redesign ML algorithms for trustworthy interpretability but often sacrifice accuracy in the process. In this paper, we address this gap by investigating the impact of deviations in concept representations-an essential component of interpretable models-on prediction performance and propose a novel framework to mitigate these effects. The framework builds on the principle of optimizing concept embeddings under constraints that preserve interpretability. Using a generative model as a test-bed, we rigorously prove that our algorithm achieves zero loss while progressively enhancing the interpretability of the resulting model. Additionally, we evaluate the practical performance of our proposed framework in generating explainable predictions for image classification tasks across various benchmarks. Compared to existing explainable methods, our approach not only improves prediction accuracy while preserving model interpretability across various large-scale benchmarks but also achieves this with significantly lower computational cost.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Robustness and Accuracy in Mixture of Experts: A Dual-Model Approach</title>
<link>https://arxiv.org/abs/2502.06832</link>
<guid>https://arxiv.org/abs/2502.06832</guid>
<content:encoded><![CDATA[
arXiv:2502.06832v3 Announce Type: replace 
Abstract: Mixture of Experts (MoE) have shown remarkable success in leveraging specialized expert networks for complex machine learning tasks. However, their susceptibility to adversarial attacks presents a critical challenge for deployment in robust applications. This paper addresses the critical question of how to incorporate robustness into MoEs while maintaining high natural accuracy. We begin by analyzing the vulnerability of MoE components, finding that expert networks are notably more susceptible to adversarial attacks than the router. Based on this insight, we propose a targeted robust training technique that integrates a novel loss function to enhance the adversarial robustness of MoE, requiring only the robustification of one additional expert without compromising training or inference efficiency. Building on this, we introduce a dual-model strategy that linearly combines a standard MoE model with our robustified MoE model using a smoothing parameter. This approach allows for flexible control over the robustness-accuracy trade-off. We further provide theoretical foundations by deriving certified robustness bounds for both the single MoE and the dual-model. To push the boundaries of robustness and accuracy, we propose a novel joint training strategy JTDMoE for the dual-model. This joint training enhances both robustness and accuracy beyond what is achievable with separate models. Experimental results on CIFAR-10 and TinyImageNet datasets using ResNet18 and Vision Transformer (ViT) architectures demonstrate the effectiveness of our proposed methods. The code is publicly available at https://github.com/TIML-Group/Robust-MoE-Dual-Model.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HiPoNet: A Multi-View Simplicial Complex Network for High Dimensional Point-Cloud and Single-Cell Data</title>
<link>https://arxiv.org/abs/2502.07746</link>
<guid>https://arxiv.org/abs/2502.07746</guid>
<content:encoded><![CDATA[
arXiv:2502.07746v2 Announce Type: replace 
Abstract: In this paper, we propose HiPoNet, an end-to-end differentiable neural network for regression, classification, and representation learning on high-dimensional point clouds. Our work is motivated by single-cell data which can have very high-dimensionality --exceeding the capabilities of existing methods for point clouds which are mostly tailored for 3D data. Moreover, modern single-cell and spatial experiments now yield entire cohorts of datasets (i.e., one data set for every patient), necessitating models that can process large, high-dimensional point-clouds at scale. Most current approaches build a single nearest-neighbor graph, discarding important geometric and topological information. In contrast, HiPoNet models the point-cloud as a set of higher-order simplicial complexes, with each particular complex being created using a reweighting of features. This method thus generates multiple constructs corresponding to different views of high-dimensional data, which in biology offers the possibility of disentangling distinct cellular processes. It then employs simplicial wavelet transforms to extract multiscale features, capturing both local and global topology from each view. We show that geometric and topological information is preserved in this framework both theoretically and empirically. We showcase the utility of HiPoNet on point-cloud level tasks, involving classification and regression of entire point-clouds in data cohorts. Experimentally, we find that HiPoNet outperforms other point-cloud and graph-based models on single-cell data. We also apply HiPoNet to spatial transcriptomics datasets using spatial coordinates as one of the views. Overall, HiPoNet offers a robust and scalable solution for high-dimensional data analysis.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TransMLA: Migrating GQA Models to MLA with Full DeepSeek Compatibility and Speedup</title>
<link>https://arxiv.org/abs/2502.07864</link>
<guid>https://arxiv.org/abs/2502.07864</guid>
<content:encoded><![CDATA[
arXiv:2502.07864v3 Announce Type: replace 
Abstract: In this paper, we present TransMLA, a framework that seamlessly converts any GQA-based pre-trained model into an MLA-based model. Our approach enables direct compatibility with DeepSeek's codebase, allowing these models to fully leverage DeepSeek-specific optimizations such as vLLM and SGlang. By compressing 93% of the KV cache in LLaMA-2-7B, TransMLA achieves a 10.6x inference speedup at an 8K context length while preserving meaningful output quality. Additionally, the model requires only 6 billion tokens for fine-tuning to regain performance on par with the original across multiple benchmarks. TransMLA offers a practical solution for migrating GQA-based models to the MLA structure. When combined with DeepSeek's advanced features, such as FP8 quantization and Multi-Token Prediction, even greater inference acceleration can be realized.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Training One-Step Diffusion Models Without Distillation</title>
<link>https://arxiv.org/abs/2502.08005</link>
<guid>https://arxiv.org/abs/2502.08005</guid>
<content:encoded><![CDATA[
arXiv:2502.08005v3 Announce Type: replace 
Abstract: Recent advances in training one-step diffusion models typically follow a two-stage pipeline: first training a teacher diffusion model and then distilling it into a one-step student model. This process often depends on both the teacher's score function for supervision and its weights for initializing the student model. In this paper, we explore whether one-step diffusion models can be trained directly without this distillation procedure. We introduce a family of new training methods that entirely forgo teacher score supervision, yet outperforms most teacher-guided distillation approaches. This suggests that score supervision is not essential for effective training of one-step diffusion models. However, we find that initializing the student model with the teacher's weights remains critical. Surprisingly, the key advantage of teacher initialization is not due to better latent-to-output mappings, but rather the rich set of feature representations across different noise levels that the teacher diffusion model provides. These insights take us one step closer towards training one-step diffusion models without distillation and provide a better understanding of the roles of teacher supervision and initialization in the distillation process.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recurrent Memory for Online Interdomain Gaussian Processes</title>
<link>https://arxiv.org/abs/2502.08736</link>
<guid>https://arxiv.org/abs/2502.08736</guid>
<content:encoded><![CDATA[
arXiv:2502.08736v3 Announce Type: replace 
Abstract: We propose a novel online Gaussian process (GP) model that is capable of capturing long-term memory in sequential data in an online learning setting. Our model, Online HiPPO Sparse Variational Gaussian Process (OHSVGP), leverages the HiPPO (High-order Polynomial Projection Operators) framework, which is popularized in the RNN domain due to its long-range memory modeling capabilities. We interpret the HiPPO time-varying orthogonal projections as inducing variables with time-dependent orthogonal polynomial basis functions, which allows the SVGP inducing variables to memorize the process history. We show that the HiPPO framework fits naturally into the interdomain GP framework and demonstrate that the kernel matrices can also be updated online in a recurrence form based on the ODE evolution of HiPPO. We evaluate OHSVGP with online prediction for 1D time series, continual learning in discriminative GP model for data with multidimensional inputs, and deep generative modeling with sparse Gaussian process variational autoencoder, showing that it outperforms existing online GP methods in terms of predictive performance, long-term memory preservation, and computational efficiency.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning to Explain Air Traffic Situation</title>
<link>https://arxiv.org/abs/2502.10764</link>
<guid>https://arxiv.org/abs/2502.10764</guid>
<content:encoded><![CDATA[
arXiv:2502.10764v2 Announce Type: replace 
Abstract: Understanding how air traffic controllers construct a mental 'picture' of complex air traffic situations is crucial but remains a challenge due to the inherently intricate, high-dimensional interactions between aircraft, pilots, and controllers. Previous work on modeling the strategies of air traffic controllers and their mental image of traffic situations often centers on specific air traffic control tasks or pairwise interactions between aircraft, neglecting to capture the comprehensive dynamics of an air traffic situation. To address this issue, we propose a machine learning-based framework for explaining air traffic situations. Specifically, we employ a Transformer-based multi-agent trajectory model that encapsulates both the spatio-temporal movement of aircraft and social interaction between them. By deriving attention scores from the model, we can quantify the influence of individual aircraft on overall traffic dynamics. This provides explainable insights into how air traffic controllers perceive and understand the traffic situation. Trained on real-world air traffic surveillance data collected from the terminal airspace around Incheon International Airport in South Korea, our framework effectively explicates air traffic situations. This could potentially support and enhance the decision-making and situational awareness of air traffic controllers.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scalable Model Merging with Progressive Layer-wise Distillation</title>
<link>https://arxiv.org/abs/2502.12706</link>
<guid>https://arxiv.org/abs/2502.12706</guid>
<content:encoded><![CDATA[
arXiv:2502.12706v2 Announce Type: replace 
Abstract: Model merging offers an effective way to integrate the capabilities of multiple fine-tuned models. However, the performance degradation of the merged model remains a challenge, particularly when none or few data are available. This paper first highlights the necessity of domain-specific data for model merging by proving that data-agnostic algorithms can have arbitrarily bad worst-case performance. Building on this theoretical insight, we explore the relationship between model merging and distillation, introducing a novel few-shot merging algorithm, ProDistill (Progressive Layer-wise Distillation). Unlike common belief that layer wise training hurts performance, we show that layer-wise teacher-student distillation not only enhances the scalability but also improves model merging performance. We conduct extensive experiments to show that compared to existing few-shot merging methods, ProDistill achieves state-of-the-art performance, with up to 6.14% and 6.61% improvements in vision and NLU tasks. Furthermore, we extend the experiments to models with over 10B parameters, showcasing the exceptional scalability of ProDistill.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MOLLM: Multi-Objective Large Language Model for Molecular Design -- Optimizing with Experts</title>
<link>https://arxiv.org/abs/2502.12845</link>
<guid>https://arxiv.org/abs/2502.12845</guid>
<content:encoded><![CDATA[
arXiv:2502.12845v2 Announce Type: replace 
Abstract: Molecular design plays a critical role in advancing fields such as drug discovery, materials science, and chemical engineering. This work introduces the Multi-Objective Large Language Model for Molecular Design (MOLLM), a novel framework that combines domain-specific knowledge with the adaptability of large language models to optimize molecular properties across multiple objectives. Leveraging in-context learning and multi-objective optimization, MOLLM achieves superior performance and innovation, consistently surpassing state-of-the-art (SOTA) methods. We significantly improve the efficiency of our framework, making it 14 times faster and substantially more cost-effective without compromising performance compared to the latest similar work. Our results demonstrate that MOLLM consistently outperforms SOTA models across experiments and excels on the PMO benchmark. In addition, we provide extensive ablation studies and analysis to evaluate the effectiveness of each component and the quality of the output molecules.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GeLLMO: Generalizing Large Language Models for Multi-property Molecule Optimization</title>
<link>https://arxiv.org/abs/2502.13398</link>
<guid>https://arxiv.org/abs/2502.13398</guid>
<content:encoded><![CDATA[
arXiv:2502.13398v2 Announce Type: replace 
Abstract: Despite recent advancements, most computational methods for molecule optimization are constrained to single- or double-property optimization tasks and suffer from poor scalability and generalizability to novel optimization tasks. Meanwhile, Large Language Models (LLMs) demonstrate remarkable out-of-domain generalizability to novel tasks. To demonstrate LLMs' potential for molecule optimization, we introduce MuMOInstruct, the first high-quality instruction-tuning dataset specifically focused on complex multi-property molecule optimization tasks. Leveraging MuMOInstruct, we develop GeLLMOs, a series of instruction-tuned LLMs for molecule optimization. Extensive evaluations across 5 in-domain and 5 out-of-domain tasks demonstrate that GeLLMOs consistently outperform state-of-the-art baselines. GeLLMOs also exhibit outstanding zero-shot generalization to unseen tasks, significantly outperforming powerful closed-source LLMs. Such strong generalizability demonstrates the tremendous potential of GeLLMOs as foundational models for molecule optimization, thereby tackling novel optimization tasks without resource-intensive retraining. MuMOInstruct, models, and code are accessible through https://github.com/ninglab/GeLLMO.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving adaptivity and optimality for multi-armed bandits using Exponential-Kullback Leibler Maillard Sampling</title>
<link>https://arxiv.org/abs/2502.14379</link>
<guid>https://arxiv.org/abs/2502.14379</guid>
<content:encoded><![CDATA[
arXiv:2502.14379v2 Announce Type: replace 
Abstract: We study the problem of $K$-armed bandits with reward distributions belonging to a one-parameter exponential distribution family. In the literature, several criteria have been proposed to evaluate the performance of such algorithms, including Asymptotic Optimality, Minimax Optimality, Sub-UCB, and variance-adaptive worst-case regret bound. Thompson Sampling-based and Upper Confidence Bound-based algorithms have been employed to achieve some of these criteria. However, none of these algorithms simultaneously satisfy all the aforementioned criteria.
  In this paper, we design an algorithm, Exponential Kullback-Leibler Maillard Sampling (abbrev. Exp-KL-MS), that can achieve multiple optimality criteria simultaneously, including Asymptotic Optimality, Minimax Optimality with a $\sqrt{\ln (K)}$ factor, Sub-UCB, and variance-adaptive worst-case regret bound.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributional Scaling for Emergent Capabilities</title>
<link>https://arxiv.org/abs/2502.17356</link>
<guid>https://arxiv.org/abs/2502.17356</guid>
<content:encoded><![CDATA[
arXiv:2502.17356v3 Announce Type: replace 
Abstract: This paper explores the nature of sudden breakthroughs in language model performance at scale, which stand in contrast to smooth improvements governed by scaling laws. While advocates of "emergence" view breakthroughs as unlocked capabilities, others attribute them to thresholding effects on noncontinuous metrics. We propose that breakthroughs are instead driven by continuous changes in the probability distribution of training outcomes when performance is bimodally distributed across random seeds. In synthetic length generalization tasks, we show that different random seeds can produce either highly linear or emergent scaling trends. We reveal that sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds. Furthermore, we provide a case study of inverse scaling. We validate our distributional scaling framework on realistic settings by measuring MMLU performance in LM populations. These insights emphasize the role of random variation in the effect of scale on LM capabilities.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training a Generally Curious Agent</title>
<link>https://arxiv.org/abs/2502.17543</link>
<guid>https://arxiv.org/abs/2502.17543</guid>
<content:encoded><![CDATA[
arXiv:2502.17543v3 Announce Type: replace 
Abstract: Efficient exploration is essential for intelligent systems interacting with their environment, but existing language models often fall short in scenarios that require strategic information gathering. In this paper, we present Paprika, a fine-tuning approach that enables language models to develop general decision-making capabilities that are not confined to particular environments. By training on synthetic interaction data from different tasks that require diverse strategies, Paprika teaches models to explore and adapt their behavior on a new task based on environment feedback in-context without more gradient updates. Experimental results show that models fine-tuned with Paprika can effectively transfer their learned decision-making capabilities to entirely unseen tasks without additional training. Unlike traditional training, our approach's primary bottleneck lies in sampling useful interaction data instead of model updates. To improve sample efficiency, we propose a curriculum learning strategy that prioritizes sampling trajectories from tasks with high learning potential. These results suggest a promising path towards AI systems that can autonomously solve novel sequential decision-making problems that require interactions with the external world.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Policy Committees for Effective Personalization in MDPs with Diverse Tasks</title>
<link>https://arxiv.org/abs/2503.01885</link>
<guid>https://arxiv.org/abs/2503.01885</guid>
<content:encoded><![CDATA[
arXiv:2503.01885v2 Announce Type: replace 
Abstract: Many dynamic decision problems, such as robotic control, involve a series of tasks, many of which are unknown at training time. Typical approaches for these problems, such as multi-task and meta reinforcement learning, do not generalize well when the tasks are diverse. On the other hand, approaches that aim to tackle task diversity, such as using task embedding as policy context and task clustering, typically lack performance guarantees and require a large number of training tasks. To address these challenges, we propose a novel approach for learning a policy committee that includes at least one near-optimal policy with high probability for tasks encountered during execution. While we show that this problem is in general inapproximable, we present two practical algorithmic solutions. The first yields provable approximation and task sample complexity guarantees when tasks are low-dimensional (the best we can do due to inapproximability), whereas the second is a general and practical gradient-based approach. In addition, we provide a provable sample complexity bound for few-shot learning. Our experiments on MuJoCo and Meta-World show that the proposed approach outperforms state-of-the-art multi-task, meta-, and task clustering baselines in training, generalization, and few-shot learning, often by a large margin. Our code is available at https://github.com/CERL-WUSTL/PACMAN.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sequential Function-Space Variational Inference via Gaussian Mixture Approximation</title>
<link>https://arxiv.org/abs/2503.07114</link>
<guid>https://arxiv.org/abs/2503.07114</guid>
<content:encoded><![CDATA[
arXiv:2503.07114v2 Announce Type: replace 
Abstract: Continual learning in neural networks aims to learn new tasks without forgetting old tasks. Sequential function-space variational inference (SFSVI) uses a Gaussian variational distribution to approximate the distribution of the outputs of the neural network corresponding to a finite number of selected inducing points. Since the posterior distribution of a neural network is multi-modal, a Gaussian distribution could only match one mode of the posterior distribution, and a Gaussian mixture distribution could be used to better approximate the posterior distribution. We propose an SFSVI method based on a Gaussian mixture variational distribution. We also compare different types of variational inference methods with a fixed pre-trained feature extractor (where continual learning is performed on the final layer) and without a fixed pre-trained feature extractor (where continual learning is performed on all layers). We find that in terms of final average accuracy, likelihood-focused Gaussian mixture SFSVI outperforms other sequential variational inference methods, especially in the latter case.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicting and Understanding College Student Mental Health with Interpretable Machine Learning</title>
<link>https://arxiv.org/abs/2503.08002</link>
<guid>https://arxiv.org/abs/2503.08002</guid>
<content:encoded><![CDATA[
arXiv:2503.08002v2 Announce Type: replace 
Abstract: Mental health issues among college students have reached critical levels, significantly impacting academic performance and overall wellbeing. Predicting and understanding mental health status among college students is challenging due to three main factors: the necessity for large-scale longitudinal datasets, the prevalence of black-box machine learning models lacking transparency, and the tendency of existing approaches to provide aggregated insights at the population level rather than individualized understanding.
  To tackle these challenges, this paper presents I-HOPE, the first Interpretable Hierarchical mOdel for Personalized mEntal health prediction. I-HOPE is a two-stage hierarchical model that connects raw behavioral features to mental health status through five defined behavioral categories as interaction labels. We evaluate I-HOPE on the College Experience Study, the longest longitudinal mobile sensing dataset. This dataset spans five years and captures data from both pre-pandemic periods and the COVID-19 pandemic. I-HOPE achieves a prediction accuracy of 91%, significantly surpassing the 60-70% accuracy of baseline methods. In addition, I-HOPE distills complex patterns into interpretable and individualized insights, enabling the future development of tailored interventions and improving mental health support. The code is available at https://github.com/roycmeghna/I-HOPE.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FlexiReg: Flexible Urban Region Representation Learning</title>
<link>https://arxiv.org/abs/2503.09128</link>
<guid>https://arxiv.org/abs/2503.09128</guid>
<content:encoded><![CDATA[
arXiv:2503.09128v2 Announce Type: replace 
Abstract: The increasing availability of urban data offers new opportunities for learning region representations, which can be used as input to machine learning models for downstream tasks such as check-in or crime prediction. While existing solutions have produced promising results, an issue is their fixed formation of regions and fixed input region features, which may not suit the needs of different downstream tasks. To address this limitation, we propose a model named FlexiReg for urban region representation learning that is flexible with both the formation of urban regions and the input region features. FlexiReg is based on a spatial grid partitioning over the spatial area of interest. It learns representations for the grid cells, leveraging publicly accessible data, including POI, land use, satellite imagery, and street view imagery. We propose adaptive aggregation to fuse the cell representations and prompt learning techniques to tailor the representations towards different tasks, addressing the needs of varying formations of urban regions and downstream tasks. Extensive experiments on five real-world datasets demonstrate that FlexiReg outperforms state-of-the-art models by up to 202% in term of the accuracy of four diverse downstream tasks using the produced urban region representations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SciHorizon: Benchmarking AI-for-Science Readiness from Scientific Data to Large Language Models</title>
<link>https://arxiv.org/abs/2503.13503</link>
<guid>https://arxiv.org/abs/2503.13503</guid>
<content:encoded><![CDATA[
arXiv:2503.13503v2 Announce Type: replace 
Abstract: In recent years, the rapid advancement of Artificial Intelligence (AI) technologies, particularly Large Language Models (LLMs), has revolutionized the paradigm of scientific discovery, establishing AI-for-Science (AI4Science) as a dynamic and evolving field. However, there is still a lack of an effective framework for the overall assessment of AI4Science, particularly from a holistic perspective on data quality and model capability. Therefore, in this study, we propose SciHorizon, a comprehensive assessment framework designed to benchmark the readiness of AI4Science from both scientific data and LLM perspectives. First, we introduce a generalizable framework for assessing AI-ready scientific data, encompassing four key dimensions: Quality, FAIRness, Explainability, and Compliance-which are subdivided into 15 sub-dimensions. Drawing on data resource papers published between 2018 and 2023 in peer-reviewed journals, we present recommendation lists of AI-ready datasets for Earth, Life, and Materials Sciences, making a novel and original contribution to the field. Concurrently, to assess the capabilities of LLMs across multiple scientific disciplines, we establish 16 assessment dimensions based on five core indicators Knowledge, Understanding, Reasoning, Multimodality, and Values spanning Mathematics, Physics, Chemistry, Life Sciences, and Earth and Space Sciences. Using the developed benchmark datasets, we have conducted a comprehensive evaluation of over 50 representative open-source and closed source LLMs. All the results are publicly available and can be accessed online at www.scihorizon.cn/en.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-FE: Automated Feature Engineering for Tabular Data with LLMs as Evolutionary Optimizers</title>
<link>https://arxiv.org/abs/2503.14434</link>
<guid>https://arxiv.org/abs/2503.14434</guid>
<content:encoded><![CDATA[
arXiv:2503.14434v2 Announce Type: replace 
Abstract: Automated feature engineering plays a critical role in improving predictive model performance for tabular learning tasks. Traditional automated feature engineering methods are limited by their reliance on pre-defined transformations within fixed, manually designed search spaces, often neglecting domain knowledge. Recent advances using Large Language Models (LLMs) have enabled the integration of domain knowledge into the feature engineering process. However, existing LLM-based approaches use direct prompting or rely solely on validation scores for feature selection, failing to leverage insights from prior feature discovery experiments or establish meaningful reasoning between feature generation and data-driven performance. To address these challenges, we propose LLM-FE, a novel framework that combines evolutionary search with the domain knowledge and reasoning capabilities of LLMs to automatically discover effective features for tabular learning tasks. LLM-FE formulates feature engineering as a program search problem, where LLMs propose new feature transformation programs iteratively, and data-driven feedback guides the search process. Our results demonstrate that LLM-FE consistently outperforms state-of-the-art baselines, significantly enhancing the performance of tabular prediction models across diverse classification and regression benchmarks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Efficient Training of Graph Neural Networks: A Multiscale Approach</title>
<link>https://arxiv.org/abs/2503.19666</link>
<guid>https://arxiv.org/abs/2503.19666</guid>
<content:encoded><![CDATA[
arXiv:2503.19666v3 Announce Type: replace 
Abstract: Graph Neural Networks (GNNs) have become powerful tools for learning from graph-structured data, finding applications across diverse domains. However, as graph sizes and connectivity increase, standard GNN training methods face significant computational and memory challenges, limiting their scalability and efficiency. In this paper, we present a novel framework for efficient multiscale training of GNNs. Our approach leverages hierarchical graph representations and subgraphs, enabling the integration of information across multiple scales and resolutions. By utilizing coarser graph abstractions and subgraphs, each with fewer nodes and edges, we significantly reduce computational overhead during training. Building on this framework, we propose a suite of scalable training strategies, including coarse-to-fine learning, subgraph-to-full-graph transfer, and multiscale gradient computation. We also provide some theoretical analysis of our methods and demonstrate their effectiveness across various datasets and learning tasks. Our results show that multiscale training can substantially accelerate GNN training for large scale problems while maintaining, or even improving, predictive performance.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python</title>
<link>https://arxiv.org/abs/2504.01842</link>
<guid>https://arxiv.org/abs/2504.01842</guid>
<content:encoded><![CDATA[
arXiv:2504.01842v2 Announce Type: replace 
Abstract: This paper introduces the shapr R package, a versatile tool for generating Shapley value based prediction explanations for machine learning and statistical regression models. Moreover, the shaprpy Python library brings the core capabilities of shapr to the Python ecosystem. Shapley values originate from cooperative game theory in the 1950s, but have over the past few years become a widely used method for quantifying how a model's features/covariates contribute to specific prediction outcomes. The shapr package emphasizes conditional Shapley value estimates, providing a comprehensive range of approaches for accurately capturing feature dependencies -- a crucial aspect for correct model explanation, typically lacking in similar software. In addition to regular tabular data, the shapr R package includes specialized functionality for explaining time series forecasts. The package offers a minimal set of user functions with sensible default values for most use cases while providing extensive flexibility for advanced users to fine-tune computations. Additional features include parallelized computations, iterative estimation with convergence detection, and rich visualization tools. shapr also extends its functionality to compute causal and asymmetric Shapley values when causal information is available. Overall, the shapr and shaprpy packages aim to enhance the interpretability of predictive models within a powerful and user-friendly framework.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pairwise Optimal Transports for Training All-to-All Flow-Based Condition Transfer Model</title>
<link>https://arxiv.org/abs/2504.03188</link>
<guid>https://arxiv.org/abs/2504.03188</guid>
<content:encoded><![CDATA[
arXiv:2504.03188v2 Announce Type: replace 
Abstract: In this paper, we propose a flow-based method for learning all-to-all transfer maps among conditional distributions that approximates pairwise optimal transport. The proposed method addresses the challenge of handling the case of continuous conditions, which often involve a large set of conditions with sparse empirical observations per condition. We introduce a novel cost function that enables simultaneous learning of optimal transports for all pairs of conditional distributions. Our method is supported by a theoretical guarantee that, in the limit, it converges to the pairwise optimal transports among infinite pairs of conditional distributions. The learned transport maps are subsequently used to couple data points in conditional flow matching. We demonstrate the effectiveness of this method on synthetic and benchmark datasets, as well as on chemical datasets in which continuous physical properties are defined as conditions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Continual Learning to SGD and Back: Better Rates for Continual Linear Models</title>
<link>https://arxiv.org/abs/2504.04579</link>
<guid>https://arxiv.org/abs/2504.04579</guid>
<content:encoded><![CDATA[
arXiv:2504.04579v2 Announce Type: replace 
Abstract: We theoretically study the common continual learning setup where an overparameterized model is sequentially fitted to a set of jointly realizable tasks. We analyze the forgetting, i.e., loss on previously seen tasks, after $k$ iterations. For continual linear models, we prove that fitting a task is equivalent to a single stochastic gradient descent (SGD) step on a modified objective. We develop novel last-iterate SGD upper bounds in the realizable least squares setup, which we then leverage to derive new results for continual learning. Focusing on random orderings over $T$ tasks, we establish universal forgetting rates, whereas existing rates depend on the problem dimensionality or complexity. Specifically, in continual regression with replacement, we improve the best existing rate from $O((d-r)/k)$ to $O(\min(k^{-1/4}, \sqrt{d-r}/k, \sqrt{Tr}/k))$, where $d$ is the dimensionality and $r$ the average task rank. Furthermore, we establish the first rate for random task orderings without replacement. The obtained rate of $O(\min(T^{-1/4}, (d-r)/T))$ proves for the first time that randomization alone, with no task repetition, can prevent catastrophic forgetting in sufficiently long task sequences. Finally, we prove a matching $O(k^{-1/4})$ forgetting rate for continual linear classification on separable data. Our universal rates apply for broader projection methods, such as block Kaczmarz and POCS, illuminating their loss convergence under i.i.d. and one-pass orderings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Achieving binary weight and activation for LLMs using Post-Training Quantization</title>
<link>https://arxiv.org/abs/2504.05352</link>
<guid>https://arxiv.org/abs/2504.05352</guid>
<content:encoded><![CDATA[
arXiv:2504.05352v2 Announce Type: replace 
Abstract: Quantizing large language models (LLMs) to 1-bit precision significantly reduces computational costs, but existing quantization techniques suffer from noticeable performance degradation when using weight and activation precisions below 4 bits (W4A4). In this paper, we propose a post-training quantization framework with W(1+1)A(1*4) configuration, where weights are quantized to 1 bit with an additional 1 bit for fine-grain grouping and activations are quantized to 1 bit with a 4-fold increase in the number of channels. For weight quantization, we propose utilizing Hessian-aware fine-grained grouping along with an EM-based quantization scheme. For activation quantization, we decompose INT4-quantized activations into a 4 * INT1 format equivalently and simultaneously smooth the scaling factors based on quantization errors, which further reduces the quantization errors in activations. Our method surpasses state-of-the-art (SOTA) LLM quantization baselines on W2A4 across multiple tasks, pushing the boundaries of existing LLM quantization methods toward fully binarized models. Code is available at https://github.com/JimmyCrave/LLM-PTQ-binarization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recursive Deep Inverse Reinforcement Learning</title>
<link>https://arxiv.org/abs/2504.13241</link>
<guid>https://arxiv.org/abs/2504.13241</guid>
<content:encoded><![CDATA[
arXiv:2504.13241v4 Announce Type: replace 
Abstract: Inferring an adversary's goals from exhibited behavior is crucial for counterplanning and non-cooperative multi-agent systems in domains like cybersecurity, military, and strategy games. Deep Inverse Reinforcement Learning (IRL) methods based on maximum entropy principles show promise in recovering adversaries' goals but are typically offline, require large batch sizes with gradient descent, and rely on first-order updates, limiting their applicability in real-time scenarios. We propose an online Recursive Deep Inverse Reinforcement Learning (RDIRL) approach to recover the cost function governing the adversary actions and goals. Specifically, we minimize an upper bound on the standard Guided Cost Learning (GCL) objective using sequential second-order Newton updates, akin to the Extended Kalman Filter (EKF), leading to a fast (in terms of convergence) learning algorithm. We demonstrate that RDIRL is able to recover cost and reward functions of expert agents in standard and adversarial benchmark tasks. Experiments on benchmark tasks show that our proposed approach outperforms several leading IRL algorithms.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Guide your favorite protein sequence generative model</title>
<link>https://arxiv.org/abs/2505.04823</link>
<guid>https://arxiv.org/abs/2505.04823</guid>
<content:encoded><![CDATA[
arXiv:2505.04823v2 Announce Type: replace 
Abstract: Generative machine learning models on sequences are transforming protein engineering. However, no principled framework exists for conditioning these models on auxiliary information, such as experimental data, in a plug-and-play manner. Herein, we present ProteinGuide -- a principled and general method for conditioning -- by unifying a broad class of protein generative models under a single framework. We demonstrate the applicability of ProteinGuide by guiding two protein generative models, ProteinMPNN and ESM3, to generate amino acid and structure token sequences, conditioned on several user-specified properties such as enhanced stability, enzyme classes, and CATH-labeled folds. We also used ProteinGuide with inverse folding models and our own experimental assay to design adenine base editor sequences for high activity.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers</title>
<link>https://arxiv.org/abs/2505.06874</link>
<guid>https://arxiv.org/abs/2505.06874</guid>
<content:encoded><![CDATA[
arXiv:2505.06874v2 Announce Type: replace 
Abstract: Time series forecasting has attracted significant attention, leading to the de-velopment of a wide range of approaches, from traditional statistical meth-ods to advanced deep learning models. Among them, the Auto-Regressive Integrated Moving Average (ARIMA) model remains a widely adopted linear technique due to its effectiveness in modeling temporal dependencies in economic, industrial, and social data. On the other hand, polynomial classifi-ers offer a robust framework for capturing non-linear relationships and have demonstrated competitive performance in domains such as stock price pre-diction. In this study, we propose a hybrid forecasting approach that inte-grates the ARIMA model with a polynomial classifier to leverage the com-plementary strengths of both models. The hybrid method is evaluated on multiple real-world time series datasets spanning diverse domains. Perfor-mance is assessed based on forecasting accuracy and computational effi-ciency. Experimental results reveal that the proposed hybrid model consist-ently outperforms the individual models in terms of prediction accuracy, al-beit with a modest increase in execution time.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ComplexFormer: Disruptively Advancing Transformer Inference Ability via Head-Specific Complex Vector Attention</title>
<link>https://arxiv.org/abs/2505.10222</link>
<guid>https://arxiv.org/abs/2505.10222</guid>
<content:encoded><![CDATA[
arXiv:2505.10222v2 Announce Type: replace 
Abstract: Transformer models rely on self-attention to capture token dependencies but face challenges in effectively integrating positional information while allowing multi-head attention (MHA) flexibility. Prior methods often model semantic and positional differences disparately or apply uniform positional adjustments across heads, potentially limiting representational capacity. This paper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA. CMHA empowers each head to independently model semantic and positional differences unified within the complex plane, representing interactions as rotations and scaling. ComplexFormer incorporates two key improvements: (1) a per-head Euler transformation, converting real-valued query/key projections into polar-form complex vectors for head-specific complex subspace operation; and (2) a per-head adaptive differential rotation mechanism, exp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct strategies for integrating semantic angle differences (ASmn,i) with relative positional encodings (Delta(Pmn),i). Extensive experiments on language modeling, text generation, code generation, and mathematical reasoning show ComplexFormer achieves superior performance, significantly lower generation perplexity , and improved long-context coherence compared to strong baselines like RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency, offering a more expressive, adaptable attention mechanism.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Minimizing False-Positive Attributions in Explanations of Non-Linear Models</title>
<link>https://arxiv.org/abs/2505.11210</link>
<guid>https://arxiv.org/abs/2505.11210</guid>
<content:encoded><![CDATA[
arXiv:2505.11210v2 Announce Type: replace 
Abstract: Suppressor variables can influence model predictions without being dependent on the target outcome and they pose a significant challenge for Explainable AI (XAI) methods. These variables may cause false-positive feature attributions, undermining the utility of explanations. Although effective remedies exist for linear models, their extension to non-linear models and to instance-based explanations has remained limited. We introduce PatternLocal, a novel XAI technique that addresses this gap. PatternLocal begins with a locally linear surrogate, e.g. LIME, KernelSHAP, or gradient-based methods, and transforms the resulting discriminative model weights into a generative representation, thereby suppressing the influence of suppressor variables while preserving local fidelity. In extensive hyperparameter optimization on the XAI-TRIS benchmark, PatternLocal consistently outperformed other XAI methods and reduced false-positive attributions when explaining non-linear tasks, thereby enabling more reliable and actionable insights.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Where You Place the Norm Matters: From Prejudiced to Neutral Initializations</title>
<link>https://arxiv.org/abs/2505.11312</link>
<guid>https://arxiv.org/abs/2505.11312</guid>
<content:encoded><![CDATA[
arXiv:2505.11312v3 Announce Type: replace 
Abstract: Normalization layers, such as Batch Normalization and Layer Normalization, are central components in modern neural networks, widely adopted to improve training stability and generalization. While their practical effectiveness is well documented, a detailed theoretical understanding of how normalization affects model behavior, starting from initialization, remains an important open question. In this work, we investigate how both the presence and placement of normalization within hidden layers influence the statistical properties of network predictions before training begins. In particular, we study how these choices shape the distribution of class predictions at initialization, which can range from unbiased (Neutral) to highly concentrated (Prejudiced) toward a subset of classes. Our analysis shows that normalization placement induces systematic differences in the initial prediction behavior of neural networks, which in turn shape the dynamics of learning. By linking architectural choices to prediction statistics at initialization, our work provides a principled understanding of how normalization can influence early training behavior and offers guidance for more controlled and interpretable network design.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Parameter Efficient Continual Learning with Dynamic Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.11998</link>
<guid>https://arxiv.org/abs/2505.11998</guid>
<content:encoded><![CDATA[
arXiv:2505.11998v2 Announce Type: replace 
Abstract: Catastrophic forgetting has remained a critical challenge for deep neural networks in Continual Learning (CL) as it undermines consolidated knowledge when learning new tasks. Parameter efficient fine tuning CL techniques are gaining traction for their effectiveness in addressing catastrophic forgetting with a lightweight training schedule while avoiding degradation of consolidated knowledge in pre-trained models. However, low rank adapters (LoRA) in these approaches are highly sensitive to rank selection which can lead to sub-optimal resource allocation and performance. To this end, we introduce PEARL, a rehearsal-free CL framework that entails dynamic rank allocation for LoRA components during CL training. Specifically, PEARL leverages reference task weights and adaptively determines the rank of task-specific LoRA components based on the current tasks' proximity to reference task weights in parameter space. To demonstrate the versatility of PEARL, we evaluate it across three vision architectures (ResNet, Separable Convolutional Network and Vision Transformer) and a multitude of CL scenarios, and show that PEARL outperforms all considered baselines by a large margin.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Step-wise Adaptive Integration of Supervised Fine-tuning and Reinforcement Learning for Task-Specific LLMs</title>
<link>https://arxiv.org/abs/2505.13026</link>
<guid>https://arxiv.org/abs/2505.13026</guid>
<content:encoded><![CDATA[
arXiv:2505.13026v2 Announce Type: replace 
Abstract: Large language models (LLMs) excel at mathematical reasoning and logical problem-solving. The current popular training paradigms primarily use supervised fine-tuning (SFT) and reinforcement learning (RL) to enhance the models' reasoning abilities. However, when using SFT or RL alone, there are respective challenges: SFT may suffer from overfitting, while RL is prone to mode collapse. The state-of-the-art methods have proposed hybrid training schemes. However, static switching faces challenges such as poor generalization across different tasks and high dependence on data quality. In response to these challenges, inspired by the curriculum learning-quiz mechanism in human reasoning cultivation, We propose SASR, a step-wise adaptive hybrid training framework that theoretically unifies SFT and RL and dynamically balances the two throughout optimization. SASR uses SFT for initial warm-up to establish basic reasoning skills, and then uses an adaptive dynamic adjustment algorithm based on gradient norm and divergence relative to the original distribution to seamlessly integrate SFT with the online RL method GRPO. By monitoring the training status of LLMs and adjusting the training process in sequence, SASR ensures a smooth transition between training schemes, maintaining core reasoning abilities while exploring different paths. Experimental results demonstrate that SASR outperforms SFT, RL, and static hybrid training methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RL in Name Only? Analyzing the Structural Assumptions in RL post-training for LLMs</title>
<link>https://arxiv.org/abs/2505.13697</link>
<guid>https://arxiv.org/abs/2505.13697</guid>
<content:encoded><![CDATA[
arXiv:2505.13697v2 Announce Type: replace 
Abstract: Reinforcement learning-based post-training of large language models (LLMs) has recently gained attention, particularly following the release of DeepSeek R1, which applied GRPO for fine-tuning. Amid the growing hype around improved reasoning abilities attributed to RL post-training, we critically examine the formulation and assumptions underlying these methods. We start by highlighting the popular structural assumptions made in modeling LLM training as a Markov Decision Process (MDP), and show how they lead to a degenerate MDP that doesn't quite need the RL/GRPO apparatus. The two critical structural assumptions include (1) making the MDP states be just a concatenation of the actions-with states becoming the context window and the actions becoming the tokens in LLMs and (2) splitting the reward of a state-action trajectory uniformly across the trajectory. Through a comprehensive analysis, we demonstrate that these simplifying assumptions make the approach effectively equivalent to an outcome-driven supervised learning. Our experiments on benchmarks including GSM8K and Countdown using Qwen-2.5 base models show that iterative supervised fine-tuning, incorporating both positive and negative samples, achieves performance comparable to GRPO-based training. We will also argue that the structural assumptions indirectly incentivize the RL to generate longer sequences of intermediate tokens-which in turn feeds into the narrative of "RL generating longer thinking traces." While RL may well be a very useful technique for improving the reasoning abilities of LLMs, our analysis shows that the simplistic structural assumptions made in modeling the underlying MDP render the popular LLM RL frameworks and their interpretations questionable.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Semantics: The Unreasonable Effectiveness of Reasonless Intermediate Tokens</title>
<link>https://arxiv.org/abs/2505.13775</link>
<guid>https://arxiv.org/abs/2505.13775</guid>
<content:encoded><![CDATA[
arXiv:2505.13775v2 Announce Type: replace 
Abstract: Recent impressive results from large reasoning models have been interpreted as a triumph of Chain of Thought (CoT), and especially of the process of training on CoTs sampled from base LLMs in order to help find new reasoning patterns. In this paper, we critically examine that interpretation by investigating how the semantics of intermediate tokens-often anthropomorphized as "thoughts" or reasoning traces and which are claimed to display behaviors like backtracking, self-verification etc.-actually influence model performance. We train transformer models on formally verifiable reasoning traces and solutions, constraining both intermediate steps and final outputs to align with those of a formal solver (in our case, A* search). By constructing a formal interpreter of the semantics of our problems and intended algorithm, we systematically evaluate not only solution accuracy but also the correctness of intermediate traces, thus allowing us to evaluate whether the latter causally influences the former. We notice that, despite significant improvements on the solution-only baseline, models trained on entirely correct traces still produce invalid reasoning traces when arriving at correct solutions. To further show that trace accuracy is only loosely connected to solution accuracy, we then train models on noisy, corrupted traces which have no relation to the specific problem each is paired with, and find that not only does performance remain largely consistent with models trained on correct data, but in some cases can improve upon it and generalize more robustly on out-of-distribution tasks. These results challenge the assumption that intermediate tokens or "Chains of Thought" induce predictable reasoning behaviors and caution against anthropomorphizing such outputs or over-interpreting them (despite their mostly correct forms) as evidence of human-like or algorithmic behaviors in language models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Identifiability of Interventional Stochastic Differential Equations</title>
<link>https://arxiv.org/abs/2505.15987</link>
<guid>https://arxiv.org/abs/2505.15987</guid>
<content:encoded><![CDATA[
arXiv:2505.15987v2 Announce Type: replace 
Abstract: We study identifiability of stochastic differential equation (SDE) models under multiple interventions. Our results give the first provable bounds for unique recovery of SDE parameters given samples from their stationary distributions. We give tight bounds on the number of necessary interventions for linear SDEs, and upper bounds for nonlinear SDEs in the small noise regime. We experimentally validate the recovery of true parameters in synthetic data, and motivated by our theoretical results, demonstrate the advantage of parameterizations with learnable activation functions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bidirectional Variational Autoencoders</title>
<link>https://arxiv.org/abs/2505.16074</link>
<guid>https://arxiv.org/abs/2505.16074</guid>
<content:encoded><![CDATA[
arXiv:2505.16074v2 Announce Type: replace 
Abstract: We present the new bidirectional variational autoencoder (BVAE) network architecture. The BVAE uses a single neural network both to encode and decode instead of an encoder-decoder network pair. The network encodes in the forward direction and decodes in the backward direction through the same synaptic web. Simulations compared BVAEs and ordinary VAEs on the four image tasks of image reconstruction, classification, interpolation, and generation. The image datasets included MNIST handwritten digits, Fashion-MNIST, CIFAR-10, and CelebA-64 face images. The bidirectional structure of BVAEs cut the parameter count by almost 50% and still slightly outperformed the unidirectional VAEs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Divide-Fuse-Conquer: Eliciting "Aha Moments" in Multi-Scenario Games</title>
<link>https://arxiv.org/abs/2505.16401</link>
<guid>https://arxiv.org/abs/2505.16401</guid>
<content:encoded><![CDATA[
arXiv:2505.16401v2 Announce Type: replace 
Abstract: Large language models (LLMs) have been observed to suddenly exhibit advanced reasoning abilities during reinforcement learning (RL), resembling an ``aha moment'' triggered by simple outcome-based rewards. While RL has proven effective in eliciting such breakthroughs in tasks involving mathematics, coding, and vision, it faces significant challenges in multi-scenario games. The diversity of game rules, interaction modes, and environmental complexities often leads to policies that perform well in one scenario but fail to generalize to others. Simply combining multiple scenarios during training introduces additional challenges, such as training instability and poor performance. To overcome these challenges, we propose Divide-Fuse-Conquer, a framework designed to enhance generalization in multi-scenario RL. This approach starts by heuristically grouping games based on characteristics such as rules and difficulties. Specialized models are then trained for each group to excel at games in the group is what we refer to as the divide step. Next, we fuse model parameters from different groups as a new model, and continue training it for multiple groups, until the scenarios in all groups are conquered. Experiments across 18 TextArena games show that Qwen2.5-32B-Align trained with the Divide-Fuse-Conquer strategy reaches a performance level comparable to Claude3.5, achieving 7 wins and 4 draws. We hope our approach can inspire future research on using reinforcement learning to improve the generalization of LLMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training on Plausible Counterfactuals Removes Spurious Correlations</title>
<link>https://arxiv.org/abs/2505.16583</link>
<guid>https://arxiv.org/abs/2505.16583</guid>
<content:encoded><![CDATA[
arXiv:2505.16583v2 Announce Type: replace 
Abstract: Plausible counterfactual explanations (p-CFEs) are perturbations that minimally modify inputs to change classifier decisions while remaining plausible under the data distribution. In this study, we demonstrate that classifiers can be trained on p-CFEs labeled with induced \emph{incorrect} target classes to classify unperturbed inputs with the original labels. While previous studies have shown that such learning is possible with adversarial perturbations, we extend this paradigm to p-CFEs. Interestingly, our experiments reveal that learning from p-CFEs is even more effective: the resulting classifiers achieve not only high in-distribution accuracy but also exhibit significantly reduced bias with respect to spurious correlations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Predicate Invention for Bilevel Planning</title>
<link>https://arxiv.org/abs/2203.09634</link>
<guid>https://arxiv.org/abs/2203.09634</guid>
<content:encoded><![CDATA[
arXiv:2203.09634v3 Announce Type: replace-cross 
Abstract: Efficient planning in continuous state and action spaces is fundamentally hard, even when the transition model is deterministic and known. One way to alleviate this challenge is to perform bilevel planning with abstractions, where a high-level search for abstract plans is used to guide planning in the original transition space. Previous work has shown that when state abstractions in the form of symbolic predicates are hand-designed, operators and samplers for bilevel planning can be learned from demonstrations. In this work, we propose an algorithm for learning predicates from demonstrations, eliminating the need for manually specified state abstractions. Our key idea is to learn predicates by optimizing a surrogate objective that is tractable but faithful to our real efficient-planning objective. We use this surrogate objective in a hill-climbing search over predicate sets drawn from a grammar. Experimentally, we show across four robotic planning environments that our learned abstractions are able to quickly solve held-out tasks, outperforming six baselines. Code: https://tinyurl.com/predicators-release
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mitigating Molecular Aggregation in Drug Discovery with Predictive Insights from Explainable AI</title>
<link>https://arxiv.org/abs/2306.02206</link>
<guid>https://arxiv.org/abs/2306.02206</guid>
<content:encoded><![CDATA[
arXiv:2306.02206v2 Announce Type: replace-cross 
Abstract: Herein, we present the application of MEGAN, our explainable AI (xAI) model, for the identification of small colloidally aggregating molecules (SCAMs). This work offers solutions to the long-standing problem of false positives caused by SCAMs in high throughput screening for drug discovery and demonstrates the power of xAI in the classification of molecular properties that are not chemically intuitive based on our current understanding. We leverage xAI insights and molecular counterfactuals to design alternatives to problematic compounds in drug screening libraries. Additionally, we experimentally validate the MEGAN prediction classification for one of the counterfactuals and demonstrate the utility of counterfactuals for altering the aggregation properties of a compound through minor structural modifications. The integration of this method in high-throughput screening approaches will help combat and circumvent false positives, providing better lead molecules more rapidly and thus accelerating drug discovery cycles.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning with Selectively Labeled Data from Multiple Decision-makers</title>
<link>https://arxiv.org/abs/2306.07566</link>
<guid>https://arxiv.org/abs/2306.07566</guid>
<content:encoded><![CDATA[
arXiv:2306.07566v4 Announce Type: replace-cross 
Abstract: We study the problem of classification with selectively labeled data, whose distribution may differ from the full population due to historical decision-making. We exploit the fact that in many applications historical decisions were made by multiple decision-makers, each with different decision rules. We analyze this setup under a principled instrumental variable (IV) framework and rigorously study the identification of classification risk. We establish conditions for the exact identification of classification risk and derive tight partial identification bounds when exact identification fails. We further propose a unified cost-sensitive learning (UCL) approach to learn classifiers robust to selection bias in both identification settings. Finally, we theoretically and numerically validate the efficacy of our proposed method.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights</title>
<link>https://arxiv.org/abs/2309.08731</link>
<guid>https://arxiv.org/abs/2309.08731</guid>
<content:encoded><![CDATA[
arXiv:2309.08731v4 Announce Type: replace-cross 
Abstract: This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. This radar-lidar localization leverages the benefits of both sensors; radar is resilient against adverse weather, while lidar produces high-quality maps in clear conditions. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on ICP-based radar-lidar localization by including a learned preprocessing step that weights radar points based on high-level scan information. To train the weight-generating network, we present a novel, stand-alone, open-source differentiable ICP library. The learned weights facilitate ICP by filtering out harmful radar points related to artefacts, noise, and even vehicles on the road. Combining an analytical approach with a learned weight reduces overall localization errors and improves convergence in radar-lidar ICP results run on real-world autonomous driving data. Our code base is publicly available to facilitate reproducibility and extensions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LIB-KD: Learning Inductive Bias, Not Just Parameters A New Perspective on Knowledge Distillations</title>
<link>https://arxiv.org/abs/2310.00369</link>
<guid>https://arxiv.org/abs/2310.00369</guid>
<content:encoded><![CDATA[
arXiv:2310.00369v3 Announce Type: replace-cross 
Abstract: With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalized predictions of the model. This optimization can accelerate the distillation process by eliminating the need for repeated forward passes during knowledge distillation, significantly reducing the computational burden and enhancing efficiency.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLEVRER-Humans: Describing Physical and Causal Events the Human Way</title>
<link>https://arxiv.org/abs/2310.03635</link>
<guid>https://arxiv.org/abs/2310.03635</guid>
<content:encoded><![CDATA[
arXiv:2310.03635v2 Announce Type: replace-cross 
Abstract: Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting the great challenges set forth by our benchmark.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Dual-Directed Algorithm Design for Efficient Pure Exploration</title>
<link>https://arxiv.org/abs/2310.19319</link>
<guid>https://arxiv.org/abs/2310.19319</guid>
<content:encoded><![CDATA[
arXiv:2310.19319v3 Announce Type: replace-cross 
Abstract: While experimental design often focuses on selecting the single best alternative from a finite set (e.g., in ranking and selection or best-arm identification), many pure-exploration problems pursue richer goals. Given a specific goal, adaptive experimentation aims to achieve it by strategically allocating sampling effort, with the underlying sample complexity characterized by a maximin optimization problem. By introducing dual variables, we derive necessary and sufficient conditions for an optimal allocation, yielding a unified algorithm design principle that extends the top-two approach beyond best-arm identification. This principle gives rise to Information-Directed Selection, a hyperparameter-free rule that dynamically evaluates and chooses among candidates based on their current informational value. We prove that, when combined with Information-Directed Selection, top-two Thompson sampling attains asymptotic optimality for Gaussian best-arm identification, resolving a notable open question in the pure-exploration literature. Furthermore, our framework produces asymptotically optimal algorithms for pure-exploration thresholding bandits and $\varepsilon$-best-arm identification (i.e., ranking and selection with probability-of-good-selection guarantees), and more generally establishes a recipe for adapting Thompson sampling across a broad class of pure-exploration problems. Extensive numerical experiments highlight the efficiency of our proposed algorithms compared to existing methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>UOD: Unseen Object Detection in 3D Point Cloud</title>
<link>https://arxiv.org/abs/2401.03846</link>
<guid>https://arxiv.org/abs/2401.03846</guid>
<content:encoded><![CDATA[
arXiv:2401.03846v2 Announce Type: replace-cross 
Abstract: Existing 3D object detectors encounter extreme challenges in localizing unseen 3D objects and recognizing them as unseen, which is a crucial technology in autonomous driving in the wild. To address these challenges, we propose practical methods to enhance the performance of 3D detection and Out-Of-Distribution (OOD) classification for unseen objects. The proposed methods include anomaly sample augmentation, learning of universal objectness, learning of detecting unseen objects, and learning of distinguishing unseen objects. To demonstrate the effectiveness of our approach, we propose the KITTI Misc benchmark and two additional synthetic OOD benchmarks: the Nuscenes OOD benchmark and the SUN-RGBD OOD benchmark. The proposed methods consistently enhance performance by a large margin across all existing methods, giving insight for future work on unseen 3D object detection in the wild.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>An Inexact Halpern Iteration with Application to Distributionally Robust Optimization</title>
<link>https://arxiv.org/abs/2402.06033</link>
<guid>https://arxiv.org/abs/2402.06033</guid>
<content:encoded><![CDATA[
arXiv:2402.06033v3 Announce Type: replace-cross 
Abstract: The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods and for general nonlinear convex-concave loss functions, which are competitive in the literature.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On a Neural Implementation of Brenier's Polar Factorization</title>
<link>https://arxiv.org/abs/2403.03071</link>
<guid>https://arxiv.org/abs/2403.03071</guid>
<content:encoded><![CDATA[
arXiv:2403.03071v4 Announce Type: replace-cross 
Abstract: In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MetaGS: A Meta-Learned Gaussian-Phong Model for Out-of-Distribution 3D Scene Relighting</title>
<link>https://arxiv.org/abs/2405.20791</link>
<guid>https://arxiv.org/abs/2405.20791</guid>
<content:encoded><![CDATA[
arXiv:2405.20791v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) 3D relighting requires novel view synthesis under unseen lighting conditions that differ significantly from the observed images. Existing relighting methods, which assume consistent light source distributions between training and testing, often degrade in OOD scenarios. We introduce MetaGS to tackle this challenge from two perspectives. First, we propose a meta-learning approach to train 3D Gaussian splatting, which explicitly promotes learning generalizable Gaussian geometries and appearance attributes across diverse lighting conditions, even with biased training data. Second, we embed fundamental physical priors from the Blinn-Phong reflection model into Gaussian splatting, which enhances the decoupling of shading components and leads to more accurate 3D scene reconstruction. Results on both synthetic and real-world datasets demonstrate the effectiveness of MetaGS in challenging OOD relighting tasks, supporting efficient point-light relighting and generalizing well to unseen environment lighting maps.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can Small Language Models Learn, Unlearn, and Retain Noise Patterns?</title>
<link>https://arxiv.org/abs/2407.00996</link>
<guid>https://arxiv.org/abs/2407.00996</guid>
<content:encoded><![CDATA[
arXiv:2407.00996v3 Announce Type: replace-cross 
Abstract: With the growing need for efficient language models in resource-constrained environments, Small Language Models (SLMs) have emerged as compact and practical alternatives to Large Language Models (LLMs). While studies have explored noise handling in LLMs, little is known about how SLMs handle noise, a critical factor for their reliable real-world deployment. This study investigates the ability of SLMs with parameters between 1 and 3 billion to learn, retain, and subsequently eliminate different types of noise (word flip, character flip, transliteration, irrelevant content, and contradictory information). Four pretrained SLMs (Olmo 1B, Qwen1.5 1.8B, Gemma1.1 2B, and Phi2 2.7B) were instruction-tuned on noise-free data and tested with in-context examples to assess noise learning. Subsequently, noise patterns were introduced in instruction tuning to assess their adaptability. The results revealed differences in how models handle noise, with smaller models like Olmo quickly adapting to noise patterns. Phi2's carefully curated, structured, and high-quality pretraining data enabled resistance to character level, transliteration, and counterfactual noise, while Gemma adapted successfully to transliteration noise through its multilingual pretraining. Subsequent clean data training effectively mitigated noise effects. These findings provide practical strategies for developing robust SLMs for real-world applications.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast Calculation of Feature Contributions in Boosting Trees</title>
<link>https://arxiv.org/abs/2407.03515</link>
<guid>https://arxiv.org/abs/2407.03515</guid>
<content:encoded><![CDATA[
arXiv:2407.03515v2 Announce Type: replace-cross 
Abstract: Recently, several fast algorithms have been proposed to decompose predicted value into Shapley values, enabling individualized feature contribution analysis in tree models. While such local decomposition offers valuable insights, it underscores the need for a global evaluation of feature contributions. Although coefficients of determination ($R^2$) allow for comparative assessment of individual features, individualizing $R^2$ is challenged by the underlying quadratic losses. To address this, we propose Q-SHAP, an efficient algorithm that reduces the computational complexity of calculating Shapley values for quadratic losses to polynomial time. Our simulations show that Q-SHAP not only improves computational efficiency but also enhances the accuracy of feature-specific $R^2$ estimates.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sentiment Reasoning for Healthcare</title>
<link>https://arxiv.org/abs/2407.21054</link>
<guid>https://arxiv.org/abs/2407.21054</guid>
<content:encoded><![CDATA[
arXiv:2407.21054v4 Announce Type: replace-cross 
Abstract: Transparency in AI healthcare decision-making is crucial. By incorporating rationales to explain reason for each predicted label, users could understand Large Language Models (LLMs)'s reasoning to make better decision. In this work, we introduce a new task - Sentiment Reasoning - for both speech and text modalities, and our proposed multimodal multitask framework and the world's largest multimodal sentiment analysis dataset. Sentiment Reasoning is an auxiliary task in sentiment analysis where the model predicts both the sentiment label and generates the rationale behind it based on the input transcript. Our study conducted on both human transcripts and Automatic Speech Recognition (ASR) transcripts shows that Sentiment Reasoning helps improve model transparency by providing rationale for model prediction with quality semantically comparable to humans while also improving model's classification performance (+2% increase in both accuracy and macro-F1) via rationale-augmented fine-tuning. Also, no significant difference in the semantic quality of generated rationales between human and ASR transcripts. All code, data (five languages - Vietnamese, English, Chinese, German, and French) and models are published online: https://github.com/leduckhai/Sentiment-Reasoning
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adaptive Backtracking Line Search</title>
<link>https://arxiv.org/abs/2408.13150</link>
<guid>https://arxiv.org/abs/2408.13150</guid>
<content:encoded><![CDATA[
arXiv:2408.13150v2 Announce Type: replace-cross 
Abstract: Backtracking line search is foundational in numerical optimization. The basic idea is to adjust the step-size of an algorithm by a constant factor until some chosen criterion (e.g. Armijo, Descent Lemma) is satisfied. We propose a novel way to adjust step-sizes, replacing the constant factor used in regular backtracking with one that takes into account the degree to which the chosen criterion is violated, with no additional computational burden. This light-weight adjustment leads to significantly faster optimization, which we confirm by performing a variety of experiments on over fifteen real world datasets. For convex problems, we prove adaptive backtracking requires no more adjustments to produce a feasible step-size than regular backtracking does. For nonconvex smooth problems, we prove adaptive backtracking enjoys the same guarantees of regular backtracking. Furthermore, we prove adaptive backtracking preserves the convergence rates of gradient descent and its accelerated variant.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>QuForge: A Library for Qudits Simulation</title>
<link>https://arxiv.org/abs/2409.17716</link>
<guid>https://arxiv.org/abs/2409.17716</guid>
<content:encoded><![CDATA[
arXiv:2409.17716v2 Announce Type: replace-cross 
Abstract: Quantum computing with qudits, an extension of qubits to multiple levels, is a research field less mature than qubit-based quantum computing. However, qudits can offer some advantages over qubits, by representing information with fewer separated components. In this article, we present QuForge, a Python-based library designed to simulate quantum circuits with qudits. This library provides the necessary quantum gates for implementing quantum algorithms, tailored to any chosen qudit dimension. Built on top of differentiable frameworks, QuForge supports execution on accelerating devices such as GPUs and TPUs, significantly speeding up simulations. It also supports sparse operations, leading to a reduction in memory consumption compared to other libraries. Additionally, by constructing quantum circuits as differentiable graphs, QuForge facilitates the implementation of quantum machine learning algorithms, enhancing the capabilities and flexibility of quantum computing research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>"Oh LLM, I'm Asking Thee, Please Give Me a Decision Tree": Zero-Shot Decision Tree Induction and Embedding with Large Language Models</title>
<link>https://arxiv.org/abs/2409.18594</link>
<guid>https://arxiv.org/abs/2409.18594</guid>
<content:encoded><![CDATA[
arXiv:2409.18594v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) provide powerful means to leverage prior knowledge for predictive modeling when data is limited. In this work, we demonstrate how LLMs can use their compressed world knowledge to generate intrinsically interpretable machine learning models, i.e., decision trees, without any training data. We find that these zero-shot decision trees can even surpass data-driven trees on some small-sized tabular datasets and that embeddings derived from these trees perform better than data-driven tree-based embeddings on average. Our decision tree induction and embedding approaches can therefore serve as new knowledge-driven baselines for data-driven machine learning methods in the low-data regime. Furthermore, they offer ways to harness the rich world knowledge within LLMs for tabular machine learning tasks. Our code and results are available at https://github.com/ml-lab-htw/llm-trees.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Annealing Flow Generative Models Towards Sampling High-Dimensional and Multi-Modal Distributions</title>
<link>https://arxiv.org/abs/2409.20547</link>
<guid>https://arxiv.org/abs/2409.20547</guid>
<content:encoded><![CDATA[
arXiv:2409.20547v4 Announce Type: replace-cross 
Abstract: Sampling from high-dimensional, multi-modal distributions remains a fundamental challenge across domains such as statistical Bayesian inference and physics-based machine learning. In this paper, we propose Annealing Flow (AF), a method built on Continuous Normalizing Flow (CNF) for sampling from high-dimensional and multi-modal distributions. AF is trained with a dynamic Optimal Transport (OT) objective incorporating Wasserstein regularization, and guided by annealing procedures, facilitating effective exploration of modes in high-dimensional spaces. Compared to recent NF methods, AF greatly improves training efficiency and stability, with minimal reliance on MC assistance. We demonstrate the superior performance of AF compared to state-of-the-art methods through experiments on various challenging distributions and real-world datasets, particularly in high-dimensional and multi-modal settings. We also highlight AF potential for sampling the least favorable distributions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Item Cluster-aware Prompt Learning for Session-based Recommendation</title>
<link>https://arxiv.org/abs/2410.04756</link>
<guid>https://arxiv.org/abs/2410.04756</guid>
<content:encoded><![CDATA[
arXiv:2410.04756v2 Announce Type: replace-cross 
Abstract: Session-based recommendation (SBR) aims to capture dynamic user preferences by analyzing item sequences within individual sessions. However, most existing approaches focus mainly on intra-session item relationships, neglecting the connections between items across different sessions (inter-session relationships), which limits their ability to fully capture complex item interactions. While some methods incorporate inter-session information, they often suffer from high computational costs, leading to longer training times and reduced efficiency. To address these challenges, we propose the CLIP-SBR (Cluster-aware Item Prompt learning for Session-Based Recommendation) framework. CLIP-SBR is composed of two modules: 1) an item relationship mining module that builds a global graph to effectively model both intra- and inter-session relationships, and 2) an item cluster-aware prompt learning module that uses soft prompts to integrate these relationships into SBR models efficiently. We evaluate CLIP-SBR across eight SBR models and three benchmark datasets, consistently demonstrating improved recommendation performance and establishing CLIP-SBR as a robust solution for session-based recommendation tasks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simple Relative Deviation Bounds for Covariance and Gram Matrices</title>
<link>https://arxiv.org/abs/2410.05754</link>
<guid>https://arxiv.org/abs/2410.05754</guid>
<content:encoded><![CDATA[
arXiv:2410.05754v3 Announce Type: replace-cross 
Abstract: We provide non-asymptotic, relative deviation bounds for the eigenvalues of empirical covariance and Gram matrices in general settings. Unlike typical uniform bounds, which may fail to capture the behavior of smaller eigenvalues, our results provide sharper control across the spectrum. Our analysis is based on a general-purpose theorem that allows one to convert existing uniform bounds into relative ones. The theorems and techniques emphasize simplicity and should be applicable across various settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scintillation pulse characterization with spectrum-inspired temporal neural networks: case studies on particle detector signals</title>
<link>https://arxiv.org/abs/2410.07267</link>
<guid>https://arxiv.org/abs/2410.07267</guid>
<content:encoded><![CDATA[
arXiv:2410.07267v3 Announce Type: replace-cross 
Abstract: Particle detectors based on scintillators are widely used in high-energy physics and astroparticle physics experiments, nuclear medicine imaging, industrial and environmental detection, etc. Precisely extracting scintillation signal characteristics at the event level is important for these applications, not only in respect of understanding the scintillator itself, but also kinds and physical property of incident particles. Recent researches demonstrate data-driven neural networks surpass traditional statistical methods, especially when the analytical form of signals is hard to obtain, or noise is significant. However, most densely connected or convolution-based networks fail to fully exploit the spectral and temporal structure of scintillation signals, leaving large space for performance improvement. In this paper, we propose a network architecture specially tailored for scintillation pulse characterization based on previous works on time series analysis. The core insight is that, by directly applying Fast Fourier Transform on original signals and utilizing different frequency components, the proposed network architecture can serve as a lightweight and enhanced representation learning backbone. We prove our idea in two case studies: (a) simulation data generated with the setting of the LUX dark matter detector, and (b) experimental electrical signals with fast electronics to emulate scintillation variations for the NICA/MPD calorimeter. The proposed model achieves significantly better results than the reference model in literature and densely connected models and demonstrates higher cost-efficiency than conventional machine learning methods.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Music Foundation Model as Generic Booster for Music Downstream Tasks</title>
<link>https://arxiv.org/abs/2411.01135</link>
<guid>https://arxiv.org/abs/2411.01135</guid>
<content:encoded><![CDATA[
arXiv:2411.01135v3 Announce Type: replace-cross 
Abstract: We demonstrate the efficacy of using intermediate representations from a single foundation model to enhance various music downstream tasks. We introduce SoniDo, a music foundation model (MFM) designed to extract hierarchical features from target music samples. By leveraging hierarchical intermediate features, SoniDo constrains the information granularity, leading to improved performance across various downstream tasks including both understanding and generative tasks. We specifically evaluated this approach on representative tasks such as music tagging, music transcription, music source separation, and music mixing. Our results reveal that the features extracted from foundation models provide valuable enhancements in training downstream task models. This highlights the capability of using features extracted from music foundation models as a booster for downstream tasks. Our approach not only benefits existing task-specific models but also supports music downstream tasks constrained by data scarcity. This paves the way for more effective and accessible music processing solutions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalizable and Robust Spectral Method for Multi-view Representation Learning</title>
<link>https://arxiv.org/abs/2411.02138</link>
<guid>https://arxiv.org/abs/2411.02138</guid>
<content:encoded><![CDATA[
arXiv:2411.02138v3 Announce Type: replace-cross 
Abstract: Multi-view representation learning (MvRL) has garnered substantial attention in recent years, driven by the increasing demand for applications that can effectively process and analyze data from multiple sources. In this context, graph Laplacian-based MvRL methods have demonstrated remarkable success in representing multi-view data. However, these methods often struggle with generalization to new data and face challenges with scalability. Moreover, in many practical scenarios, multi-view data is contaminated by noise or outliers. In such cases, modern deep-learning-based MvRL approaches that rely on alignment or contrastive objectives present degraded performance in downstream tasks, as they may impose incorrect consistency between clear and corrupted data sources. We introduce $\textit{SpecRaGE}$, a novel fusion-based framework that integrates the strengths of graph Laplacian methods with the power of deep learning to overcome these challenges. SpecRage uses neural networks to learn parametric mapping that approximates a joint diagonalization of graph Laplacians. This solution bypasses the need for alignment while enabling generalizable and scalable learning of informative and meaningful representations. Moreover, it incorporates a meta-learning fusion module that dynamically adapts to data quality, ensuring robustness against outliers and noisy views. Our extensive experiments demonstrate that SpecRaGE outperforms state-of-the-art methods, particularly in scenarios with data contamination, paving the way for more reliable and efficient multi-view learning.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity</title>
<link>https://arxiv.org/abs/2411.02184</link>
<guid>https://arxiv.org/abs/2411.02184</guid>
<content:encoded><![CDATA[
arXiv:2411.02184v2 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance. Specifically, we aim to answer the following question: Does the Double Descent phenomenon manifest in post-hoc OOD detection? This question is crucial, as it can reveal whether overparameterization, which is already known to benefit generalization, can also enhance OOD detection. Despite the growing interest in these topics by the classic supervised machine learning community, this intersection remains unexplored for OOD detection. We empirically demonstrate that the Double Descent effect does indeed appear in post-hoc OOD detection. Furthermore, we provide theoretical insights to explain why this phenomenon emerges in such setting. Finally, we show that the overparameterized regime does not yield superior results consistently, and we propose a method to identify the optimal regime for OOD detection based on our observations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Optimization</title>
<link>https://arxiv.org/abs/2411.02549</link>
<guid>https://arxiv.org/abs/2411.02549</guid>
<content:encoded><![CDATA[
arXiv:2411.02549v3 Announce Type: replace-cross 
Abstract: Distributionally robust optimization (DRO) studies decision problems under uncertainty where the probability distribution governing the uncertain problem parameters is itself uncertain. A key component of any DRO model is its ambiguity set, that is, a family of probability distributions consistent with any available structural or statistical information. DRO seeks decisions that perform best under the worst distribution in the ambiguity set. This worst case criterion is supported by findings in psychology and neuroscience, which indicate that many decision-makers have a low tolerance for distributional ambiguity. DRO is rooted in statistics, operations research and control theory, and recent research has uncovered its deep connections to regularization techniques and adversarial training in machine learning. This survey presents the key findings of the field in a unified and self-contained manner.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Rethinking MUSHRA: Addressing Modern Challenges in Text-to-Speech Evaluation</title>
<link>https://arxiv.org/abs/2411.12719</link>
<guid>https://arxiv.org/abs/2411.12719</guid>
<content:encoded><![CDATA[
arXiv:2411.12719v3 Announce Type: replace-cross 
Abstract: Despite rapid advancements in TTS models, a consistent and robust human evaluation framework is still lacking. For example, MOS tests fail to differentiate between similar models, and CMOS's pairwise comparisons are time-intensive. The MUSHRA test is a promising alternative for evaluating multiple TTS systems simultaneously, but in this work we show that its reliance on matching human reference speech unduly penalises the scores of modern TTS systems that can exceed human speech quality. More specifically, we conduct a comprehensive assessment of the MUSHRA test, focusing on its sensitivity to factors such as rater variability, listener fatigue, and reference bias. Based on our extensive evaluation involving 492 human listeners across Hindi and Tamil we identify two primary shortcomings: (i) reference-matching bias, where raters are unduly influenced by the human reference, and (ii) judgement ambiguity, arising from a lack of clear fine-grained guidelines. To address these issues, we propose two refined variants of the MUSHRA test. The first variant enables fairer ratings for synthesized samples that surpass human reference quality. The second variant reduces ambiguity, as indicated by the relatively lower variance across raters. By combining these approaches, we achieve both more reliable and more fine-grained assessments. We also release MANGO, a massive dataset of 246,000 human ratings, the first-of-its-kind collection for Indian languages, aiding in analyzing human preferences and developing automatic metrics for evaluating TTS systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Predictive Control with Constraints</title>
<link>https://arxiv.org/abs/2412.09342</link>
<guid>https://arxiv.org/abs/2412.09342</guid>
<content:encoded><![CDATA[
arXiv:2412.09342v2 Announce Type: replace-cross 
Abstract: Diffusion models have become popular for policy learning in robotics due to their ability to capture high-dimensional and multimodal distributions. However, diffusion policies are stochastic and typically trained offline, limiting their ability to handle unseen and dynamic conditions where novel constraints not represented in the training data must be satisfied. To overcome this limitation, we propose diffusion predictive control with constraints (DPCC), an algorithm for diffusion-based control with explicit state and action constraints that can deviate from those in the training data. DPCC incorporates model-based projections into the denoising process of a trained trajectory diffusion model and uses constraint tightening to account for model mismatch. This allows us to generate constraint-satisfying, dynamically feasible, and goal-reaching trajectories for predictive control. We show through simulations of a robot manipulator that DPCC outperforms existing methods in satisfying novel test-time constraints while maintaining performance on the learned control task.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Prediction-Enhanced Monte Carlo: A Machine Learning View on Control Variate</title>
<link>https://arxiv.org/abs/2412.11257</link>
<guid>https://arxiv.org/abs/2412.11257</guid>
<content:encoded><![CDATA[
arXiv:2412.11257v2 Announce Type: replace-cross 
Abstract: For many complex simulation tasks spanning areas such as healthcare, engineering, and finance, Monte Carlo (MC) methods are invaluable due to their unbiased estimates and precise error quantification. Nevertheless, Monte Carlo simulations often become computationally prohibitive, especially for nested, multi-level, or path-dependent evaluations lacking effective variance reduction techniques. While machine learning (ML) surrogates appear as natural alternatives, naive replacements typically introduce unquantifiable biases. We address this challenge by introducing Prediction-Enhanced Monte Carlo (PEMC), a framework that leverages modern ML models as learned predictors, using cheap and parallelizable simulation as features, to output unbiased evaluation with reduced variance and runtime. PEMC can also be viewed as a "modernized" view of control variates, where we consider the overall computation-cost-aware variance reduction instead of per-replication reduction, while bypassing the closed-form mean function requirement and maintaining the advantageous unbiasedness and uncertainty quantifiability of Monte Carlo.
  We illustrate PEMC's broader efficacy and versatility through three examples: first, equity derivatives such as variance swaps under stochastic local volatility models; second, interest rate derivatives such as swaption pricing under the Heath-Jarrow-Morton (HJM) interest-rate model. Finally, we showcase PEMC in a socially significant context - ambulance dispatch and hospital load balancing - where accurate mortality rate estimates are key for ethically sensitive decision-making. Across these diverse scenarios, PEMC consistently reduces variance while preserving unbiasedness, highlighting its potential as a powerful enhancement to standard Monte Carlo baselines.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The dark side of the forces: assessing non-conservative force models for atomistic machine learning</title>
<link>https://arxiv.org/abs/2412.11569</link>
<guid>https://arxiv.org/abs/2412.11569</guid>
<content:encoded><![CDATA[
arXiv:2412.11569v3 Announce Type: replace-cross 
Abstract: The use of machine learning to estimate the energy of a group of atoms, and the forces that drive them to more stable configurations, has revolutionized the fields of computational chemistry and materials discovery. In this domain, rigorous enforcement of symmetry and conservation laws has traditionally been considered essential. For this reason, interatomic forces are usually computed as the derivatives of the potential energy, ensuring energy conservation. Several recent works have questioned this physically constrained approach, suggesting that directly predicting the forces yields a better trade-off between accuracy and computational efficiency -- and that energy conservation can be learned during training. This work investigates the applicability of such non-conservative models in microscopic simulations. We identify and demonstrate several fundamental issues, from ill-defined convergence of geometry optimization to instability in various types of molecular dynamics. Contrary to the case of rotational symmetry, energy conservation is hard to learn, monitor, and correct for. The best approach to exploit the acceleration afforded by direct force prediction might be to use it in tandem with a conservative model, reducing -- rather than eliminating -- the additional cost of backpropagation, but avoiding the pathological behavior associated with non-conservative forces.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Causal Composition Diffusion Model for Closed-loop Traffic Generation</title>
<link>https://arxiv.org/abs/2412.17920</link>
<guid>https://arxiv.org/abs/2412.17920</guid>
<content:encoded><![CDATA[
arXiv:2412.17920v3 Announce Type: replace-cross 
Abstract: Simulation is critical for safety evaluation in autonomous driving, particularly in capturing complex interactive behaviors. However, generating realistic and controllable traffic scenarios in long-tail situations remains a significant challenge. Existing generative models suffer from the conflicting objective between user-defined controllability and realism constraints, which is amplified in safety-critical contexts. In this work, we introduce the Causal Compositional Diffusion Model (CCDiff), a structure-guided diffusion framework to address these challenges. We first formulate the learning of controllable and realistic closed-loop simulation as a constrained optimization problem. Then, CCDiff maximizes controllability while adhering to realism by automatically identifying and injecting causal structures directly into the diffusion process, providing structured guidance to enhance both realism and controllability. Through rigorous evaluations on benchmark datasets and in a closed-loop simulator, CCDiff demonstrates substantial gains over state-of-the-art approaches in generating realistic and user-preferred trajectories. Our results show CCDiff's effectiveness in extracting and leveraging causal structures, showing improved closed-loop performance based on key metrics such as collision rate, off-road rate, FDE, and comfort.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ProgCo: Program Helps Self-Correction of Large Language Models</title>
<link>https://arxiv.org/abs/2501.01264</link>
<guid>https://arxiv.org/abs/2501.01264</guid>
<content:encoded><![CDATA[
arXiv:2501.01264v2 Announce Type: replace-cross 
Abstract: Self-Correction aims to enable large language models (LLMs) to self-verify and self-refine their initial responses without external feedback. However, LLMs often fail to effectively self-verify and generate correct feedback, further misleading refinement and leading to the failure of self-correction, especially in complex reasoning tasks. In this paper, we propose Program-driven Self-Correction (ProgCo). First, program-driven verification (ProgVe) achieves complex verification logic and extensive validation through self-generated, self-executing verification pseudo-programs. Then, program-driven refinement (ProgRe) receives feedback from ProgVe, conducts dual reflection and refinement on both responses and verification programs to mitigate misleading of incorrect feedback in complex reasoning tasks. Experiments on three instruction-following and mathematical benchmarks indicate that ProgCo achieves effective self-correction, and can be further enhance performance when combined with real program tools. We release our code at https://github.com/songxiaoshuai/progco.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PaSa: An LLM Agent for Comprehensive Academic Paper Search</title>
<link>https://arxiv.org/abs/2501.10120</link>
<guid>https://arxiv.org/abs/2501.10120</guid>
<content:encoded><![CDATA[
arXiv:2501.10120v2 Announce Type: replace-cross 
Abstract: We introduce PaSa, an advanced Paper Search agent powered by large language models. PaSa can autonomously make a series of decisions, including invoking search tools, reading papers, and selecting relevant references, to ultimately obtain comprehensive and accurate results for complex scholar queries. We optimize PaSa using reinforcement learning with a synthetic dataset, AutoScholarQuery, which includes 35k fine-grained academic queries and corresponding papers sourced from top-tier AI conference publications. Additionally, we develop RealScholarQuery, a benchmark collecting real-world academic queries to assess PaSa performance in more realistic scenarios. Despite being trained on synthetic data, PaSa significantly outperforms existing baselines on RealScholarQuery, including Google, Google Scholar, Google with GPT-4o for paraphrased queries, ChatGPT (search-enabled GPT-4o), GPT-o1, and PaSa-GPT-4o (PaSa implemented by prompting GPT-4o). Notably, PaSa-7B surpasses the best Google-based baseline, Google with GPT-4o, by 37.78% in recall@20 and 39.90% in recall@50, and exceeds PaSa-GPT-4o by 30.36% in recall and 4.25% in precision. Model, datasets, and code are available at https://github.com/bytedance/pasa.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tuning LLM Judge Design Decisions for 1/1000 of the Cost</title>
<link>https://arxiv.org/abs/2501.17178</link>
<guid>https://arxiv.org/abs/2501.17178</guid>
<content:encoded><![CDATA[
arXiv:2501.17178v4 Announce Type: replace-cross 
Abstract: Evaluating Large Language Models (LLMs) often requires costly human annotations. To address this, LLM-based judges have been proposed, which compare the outputs of two LLMs enabling the ranking of models without human intervention. While several approaches have been proposed, many confounding factors are present between different papers. For instance the model, the prompt and other hyperparameters are typically changed at the same time making apple-to-apple comparisons challenging. In this paper, we propose to systematically analyze and tune the hyperparameters of LLM judges. To alleviate the high cost of evaluating a judge, we propose to leverage multi-objective multi-fidelity which allows to find judges that trade accuracy for cost and also significantly reduce the cost of the search. Our method identifies judges that not only outperform existing benchmarks in accuracy and cost-efficiency but also utilize open-weight models, ensuring greater accessibility and reproducibility. The code to reproduce our experiments is available at this repository https://github.com/geoalgo/judgetuning .
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Resampling Filter Design for Multirate Neural Audio Effect Processing</title>
<link>https://arxiv.org/abs/2501.18470</link>
<guid>https://arxiv.org/abs/2501.18470</guid>
<content:encoded><![CDATA[
arXiv:2501.18470v2 Announce Type: replace-cross 
Abstract: Neural networks have become ubiquitous in audio effects modelling, especially for guitar amplifiers and distortion pedals. One limitation of such models is that the sample rate of the training data is implicitly encoded in the model weights and therefore not readily adjustable at inference. Recent work explored modifications to recurrent neural network architecture to approximate a sample rate independent system, enabling audio processing at a rate that differs from the original training rate. This method works well for integer oversampling and can reduce aliasing caused by nonlinear activation functions. For small fractional changes in sample rate, fractional delay filters can be used to approximate sample rate independence, but in some cases this method fails entirely. Here, we explore the use of real-time signal resampling at the input and output of the neural network as an alternative solution. We investigate several resampling filter designs and show that a two-stage design consisting of a half-band IIR filter cascaded with a Kaiser window FIR filter can give similar or better results to the previously proposed model adjustment method with many fewer filtering operations per sample and less than one millisecond of latency at typical audio rates. Furthermore, we investigate interpolation and decimation filters for the task of integer oversampling and show that cascaded half-band IIR and FIR designs can be used in conjunction with the model adjustment method to reduce aliasing in a range of distortion effect models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
<link>https://arxiv.org/abs/2502.01512</link>
<guid>https://arxiv.org/abs/2502.01512</guid>
<content:encoded><![CDATA[
arXiv:2502.01512v3 Announce Type: replace-cross 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite (SPD) matrices, a key focus in information geometry. We introduce a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Policy Design for Two-sided Platforms with Participation Dynamics</title>
<link>https://arxiv.org/abs/2502.01792</link>
<guid>https://arxiv.org/abs/2502.01792</guid>
<content:encoded><![CDATA[
arXiv:2502.01792v2 Announce Type: replace-cross 
Abstract: In two-sided platforms (e.g., video streaming or e-commerce), viewers and providers engage in interactive dynamics: viewers benefit from increases in provider populations, while providers benefit from increases in viewer population. Despite the importance of such "population effects" on long-term platform health, recommendation policies do not generally take the participation dynamics into account. This paper thus studies the dynamics and recommender policy design on two-sided platforms under the population effects for the first time. Our control- and game-theoretic findings warn against the use of the standard "myopic-greedy" policy and shed light on the importance of provider-side considerations (i.e., effectively distributing exposure among provider groups) to improve social welfare via population growth. We also present a simple algorithm to optimize long-term social welfare by taking the population effects into account, and demonstrate its effectiveness in synthetic and real-data experiments. Our experiment code is available at https://github.com/sdean-group/dynamics-two-sided-market.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DHP: Discrete Hierarchical Planning for Hierarchical Reinforcement Learning Agents</title>
<link>https://arxiv.org/abs/2502.01956</link>
<guid>https://arxiv.org/abs/2502.01956</guid>
<content:encoded><![CDATA[
arXiv:2502.01956v2 Announce Type: replace-cross 
Abstract: Hierarchical Reinforcement Learning (HRL) agents often struggle with long-horizon visual planning due to their reliance on error-prone distance metrics. We propose Discrete Hierarchical Planning (DHP), a method that replaces continuous distance estimates with discrete reachability checks to evaluate subgoal feasibility. DHP recursively constructs tree-structured plans by decomposing long-term goals into sequences of simpler subtasks, using a novel advantage estimation strategy that inherently rewards shorter plans and generalizes beyond training depths. In addition, to address the data efficiency challenge, we introduce an exploration strategy that generates targeted training examples for the planning modules without needing expert data. Experiments in 25-room navigation environments demonstrate $100\%$ success rate (vs $82\%$ baseline) and $73$-step average episode length (vs $158$-step baseline). The method also generalizes to momentum-based control tasks and requires only $\log N$ steps for replanning. Theoretical analysis and ablations validate our design choices.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Comparison of the Cox proportional hazards model and Random Survival Forest algorithm for predicting patient-specific survival probabilities in clinical trial data</title>
<link>https://arxiv.org/abs/2502.03119</link>
<guid>https://arxiv.org/abs/2502.03119</guid>
<content:encoded><![CDATA[
arXiv:2502.03119v2 Announce Type: replace-cross 
Abstract: The Cox proportional hazards model is often used to analyze data from Randomized Controlled Trials (RCT) with time-to-event outcomes. Random survival forest (RSF) is a machine-learning algorithm known for its high predictive performance. We conduct a comprehensive neutral comparison study to compare the performance of Cox regression and RSF in various simulation scenarios based on two reference datasets from RCTs. The motivation is to identify settings in which one method is preferable over the other when comparing different aspects of performance using measures according to the TRIPOD (Transparent Reporting of a multivariable prediction model for Individual Prognosis or Diagnosis) recommendations. Our results show that conclusions solely based on the C index, a performance measure that has been predominantly used in previous studies comparing predictive accuracy of the Cox-PH and RSF model based on real-world observational time-to-event data and that has been criticized by methodologists, may not be generalizable to other aspects of predictive performance. We found that measures of overall performance may generally give more reasonable results, and that the standard log-rank splitting rule used for the RSF may be outperformed by alternative splitting rules, in particular in nonproportional hazards settings. In our simulations, performance of the RSF suffers less in data with treatment-covariate interactions compared to data where these are absent. Performance of the Cox-PH model is affected by the violation of the proportional hazards assumption.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sign Operator for Coping with Heavy-Tailed Noise in Non-Convex Optimization: High Probability Bounds Under $(L_0, L_1)$-Smoothness</title>
<link>https://arxiv.org/abs/2502.07923</link>
<guid>https://arxiv.org/abs/2502.07923</guid>
<content:encoded><![CDATA[
arXiv:2502.07923v2 Announce Type: replace-cross 
Abstract: In recent years, non-convex optimization problems are more often described by generalized $(L_0, L_1)$-smoothness assumption rather than standard one. Meanwhile, severely corrupted data used in these problems has increased the demand for methods capable of handling heavy-tailed noises, i.e., noises with bounded $\kappa$-th moment. Motivated by these real-world trends and challenges, we explore sign-based methods in this setup and demonstrate their effectiveness in comparison with other popular solutions like clipping or normalization.
  In theory, we prove the first-known high probability convergence bounds under $(L_0, L_1)$-smoothness and heavy-tailed noises with mild parameter dependencies. In the case of standard smoothness, these bounds are novel for sign-based methods as well. In particular, SignSGD with batching achieves sample complexity $\tilde{O}\left(\left(\frac{\Delta L_0d}{\varepsilon^2} + \frac{\Delta L_1d^\frac{3}{2}}{\varepsilon}\right)\left[1 + \left(\frac{\sigma}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right]\right), \kappa \in (1,2]$. Under the assumption of symmetric noises, SignSGD with Majority Voting can robustly work on the whole range of $\kappa \in (0,2]$ with complexity $\tilde{O}\left(\left(\frac{\Delta L_0d}{\varepsilon^2} + \frac{\Delta L_1d^\frac{3}{2}}{\varepsilon}\right)\left[\frac{1}{\kappa^2} + \frac{\sigma^2}{\varepsilon^2}\right]\right)$. We also obtain results for parameter-agnostic setups, Polyak-Lojasiewicz functions and momentum-based methods (in expectation). Our theoretical findings are supported by the superior performance of sign-based methods in training Large Language Models compared to clipping and normalization.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Online Confidence Bounds for Multinomial Logistic Bandits</title>
<link>https://arxiv.org/abs/2502.10020</link>
<guid>https://arxiv.org/abs/2502.10020</guid>
<content:encoded><![CDATA[
arXiv:2502.10020v4 Announce Type: replace-cross 
Abstract: In this paper, we propose an improved online confidence bound for multinomial logistic (MNL) models and apply this result to MNL bandits, achieving variance-dependent optimal regret. Recently, Lee & Oh (2024) established an online confidence bound for MNL models and achieved nearly minimax-optimal regret in MNL bandits. However, their results still depend on the norm-boundedness of the unknown parameter $B$ and the maximum size of possible outcomes $K$. To address this, we first derive an online confidence bound of $O\left(\sqrt{d \log t} + B \right)$, which is a significant improvement over the previous bound of $O (B \sqrt{d} \log t \log K )$ (Lee & Oh, 2024). This is mainly achieved by establishing tighter self-concordant properties of the MNL loss and introducing a novel intermediary term to bound the estimation error. Using this new online confidence bound, we propose a constant-time algorithm, OFU-MNL++, which achieves a variance-dependent regret bound of $O \Big( d \log T \sqrt{ \sum_{t=1}^T \sigma_t^2 } \Big) $ for sufficiently large $T$, where $\sigma_t^2$ denotes the variance of the rewards at round $t$, $d$ is the dimension of the contexts, and $T$ is the total number of rounds. Furthermore, we introduce a Maximum Likelihood Estimation (MLE)-based algorithm, OFU-MN$^2$L, which achieves an anytime poly(B)-free regret of $O \Big( d \log (BT) \sqrt{ \sum_{t=1}^T \sigma_t^2 } \Big) $.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Accelerated Parallel Tempering via Neural Transports</title>
<link>https://arxiv.org/abs/2502.10328</link>
<guid>https://arxiv.org/abs/2502.10328</guid>
<content:encoded><![CDATA[
arXiv:2502.10328v2 Announce Type: replace-cross 
Abstract: Markov Chain Monte Carlo (MCMC) algorithms are essential tools in computational statistics for sampling from unnormalised probability distributions, but can be fragile when targeting high-dimensional, multimodal, or complex target distributions. Parallel Tempering (PT) enhances MCMC's sample efficiency through annealing and parallel computation, propagating samples from tractable reference distributions to intractable targets via state swapping across interpolating distributions. The effectiveness of PT is limited by the often minimal overlap between adjacent distributions in challenging problems, which requires increasing the computational resources to compensate. We introduce a framework that accelerates PT by leveraging neural samplers-including normalising flows, diffusion models, and controlled diffusions-to reduce the required overlap. Our approach utilises neural samplers in parallel, circumventing the computational burden of neural samplers while preserving the asymptotic consistency of classical PT. We demonstrate theoretically and empirically on a variety of multimodal sampling problems that our method improves sample quality, reduces the computational cost compared to classical PT, and enables efficient free energies/normalising constants estimation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ANCHOLIK-NER: A Benchmark Dataset for Bangla Regional Named Entity Recognition</title>
<link>https://arxiv.org/abs/2502.11198</link>
<guid>https://arxiv.org/abs/2502.11198</guid>
<content:encoded><![CDATA[
arXiv:2502.11198v3 Announce Type: replace-cross 
Abstract: Named Entity Recognition (NER) in regional dialects is a critical yet underexplored area in Natural Language Processing (NLP), especially for low-resource languages like Bangla. While NER systems for Standard Bangla have made progress, no existing resources or models specifically address the challenge of regional dialects such as Barishal, Chittagong, Mymensingh, Noakhali, and Sylhet, which exhibit unique linguistic features that existing models fail to handle effectively. To fill this gap, we introduce ANCHOLIK-NER, the first benchmark dataset for NER in Bangla regional dialects, comprising 17,405 sentences distributed across five regions. The dataset was sourced from publicly available resources and supplemented with manual translations, ensuring alignment of named entities across dialects. We evaluate three transformer-based models - Bangla BERT, Bangla BERT Base, and BERT Base Multilingual Cased - on this dataset. Our findings demonstrate that BERT Base Multilingual Cased performs best in recognizing named entities across regions, with significant performance observed in Mymensingh with an F1-score of 82.611%. Despite strong overall performance, challenges remain in region like Chittagong, where the models show lower precision and recall. Since no previous NER systems for Bangla regional dialects exist, our work represents a foundational step in addressing this gap. Future work will focus on improving model performance in underperforming regions and expanding the dataset to include more dialects, enhancing the development of dialect-aware NER systems.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hallucinations are inevitable but can be made statistically negligible. The "innate" inevitability of hallucinations cannot explain practical LLM issues</title>
<link>https://arxiv.org/abs/2502.12187</link>
<guid>https://arxiv.org/abs/2502.12187</guid>
<content:encoded><![CDATA[
arXiv:2502.12187v2 Announce Type: replace-cross 
Abstract: Hallucinations, a phenomenon where a language model (LM) generates nonfactual content, pose a significant challenge to the practical deployment of LMs. While many empirical methods have been proposed to mitigate hallucinations, recent studies established a computability-theoretic result showing that any LM will inevitably generate hallucinations on an infinite set of inputs, regardless of the quality and quantity of training datasets and the choice of the language model architecture and training and inference algorithms. Although the computability-theoretic result may seem pessimistic, its significance in practical viewpoints has remained unclear. This paper claims that those "innate" inevitability results from computability theory and diagonal argument, in principle, cannot explain practical issues of LLMs. We demonstrate this claim by presenting a positive theoretical result from a probabilistic perspective. Specifically, we prove that hallucinations can be made statistically negligible, provided that the quality and quantity of the training data are sufficient. Interestingly, our positive result coexists with the computability-theoretic result, implying that while hallucinations on an infinite set of inputs cannot be entirely eliminated, their probability can always be reduced by improving algorithms and training data. By evaluating the two seemingly contradictory results through the lens of information theory, we argue that our probability-theoretic positive result better reflects practical considerations than the computability-theoretic negative result.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task-Informed Anti-Curriculum by Masking Improves Downstream Performance on Text</title>
<link>https://arxiv.org/abs/2502.12953</link>
<guid>https://arxiv.org/abs/2502.12953</guid>
<content:encoded><![CDATA[
arXiv:2502.12953v2 Announce Type: replace-cross 
Abstract: Masked language modeling has become a widely adopted unsupervised technique to pre-train large language models (LLMs). However, the process of selecting tokens for masking is random, and the percentage of masked tokens is typically fixed for the entire training process. In this paper, we propose to adjust the masking ratio and to decide which tokens to mask based on a novel task-informed anti-curriculum learning scheme. First, we harness task-specific knowledge about useful and harmful tokens in order to determine which tokens to mask. Second, we propose a cyclic decaying masking ratio, which corresponds to an anti-curriculum schedule (from hard to easy). We exemplify our novel task-informed anti-curriculum by masking (TIACBM) approach across three diverse downstream tasks: sentiment analysis, text classification by topic, and authorship attribution. Our findings suggest that TIACBM enhances the ability of the model to focus on key task-relevant features, contributing to statistically significant performance gains across tasks. We release our code at https://github.com/JarcaAndrei/TIACBM.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
<link>https://arxiv.org/abs/2502.17349</link>
<guid>https://arxiv.org/abs/2502.17349</guid>
<content:encoded><![CDATA[
arXiv:2502.17349v3 Announce Type: replace-cross 
Abstract: Linker generation is critical in drug discovery applications such as lead optimization and PROTAC design, where molecular fragments are assembled into diverse drug candidates via molecular linker. Existing methods fall into point cloud-free and point cloud-aware categories based on their use of fragments' 3D poses alongside their topologies in sampling the linker's topology. Point cloud-free models prioritize sample diversity but suffer from lower validity due to overlooking fragments' spatial constraints, while point cloud-aware models ensure higher validity but restrict diversity by enforcing strict spatial constraints. To overcome these trade-offs without additional training, we propose HybridLinker, a framework that enhances point cloud-aware inference by providing diverse bonding topologies from a pretrained point cloud-free model as guidance. At its core, we propose LinkerDPS, the first diffusion posterior sampling (DPS) method operating across point cloud-free and point cloud-aware spaces, bridging molecular topology with 3D point clouds via an energy-inspired function. By transferring the diverse sampling distribution of point cloud-free models into the point cloud-aware distribution, HybridLinker significantly surpasses baselines, improving both validity and diversity in foundational molecular design and applied drug optimization tasks, establishing a new DPS framework in the molecular domains beyond imaging.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Frequency-Aware Masked Autoencoders for Human Activity Recognition using Accelerometers</title>
<link>https://arxiv.org/abs/2502.17477</link>
<guid>https://arxiv.org/abs/2502.17477</guid>
<content:encoded><![CDATA[
arXiv:2502.17477v2 Announce Type: replace-cross 
Abstract: Wearable accelerometers are widely used for continuous monitoring of physical activity. Supervised machine learning and deep learning algorithms have long been used to extract meaningful activity information from raw accelerometry data, but progress has been hampered by the limited amount of labeled data that is publicly available. Exploiting large unlabeled datasets using self-supervised pretraining is a relatively new and underexplored approach in the field of human activity recognition (HAR). We used a time-series transformer masked autoencoder (MAE) approach to self-supervised pretraining and propose two novel spectrogram-based loss functions: the log-scale meanmagnitude (LMM) and log-scale magnitude variance (LMV) losses. We compared these losses with the mean squared error (MSE) loss for MAE training. We leveraged the large unlabeled UK Biobank accelerometry dataset (n = 109k) for pretraining and evaluated downstream HAR performance using a linear classifier in a smaller labelled dataset. We found that pretraining with the LMM loss improved performance compared to an MAE pretrained with the MSE loss, with 12.7% increase in subject-wise F1 score when using linear probing. Compared with a state-of-the-art ResNet-based HAR model, our LMM-pretrained transformer models performed better (+9.8% F1) with linear probing and comparably when fine-tuned using an LSTM classifier. The addition of the LMV to the LMM loss decreased performance compared to the LMM loss alone. These findings establish the LMM loss as a robust and effective method for pretraining MAE models on accelerometer data for HAR and show the potential of pretraining sequence-based models for free-living HAR.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Distributional Treatment of Real2Sim2Real for Object-Centric Agent Adaptation in Vision-Driven Deformable Linear Object Manipulation</title>
<link>https://arxiv.org/abs/2502.18615</link>
<guid>https://arxiv.org/abs/2502.18615</guid>
<content:encoded><![CDATA[
arXiv:2502.18615v2 Announce Type: replace-cross 
Abstract: We present an integrated (or end-to-end) framework for the Real2Sim2Real problem of manipulating deformable linear objects (DLOs) based on visual perception. Working with a parameterised set of DLOs, we use likelihood-free inference (LFI) to compute the posterior distributions for the physical parameters using which we can approximately simulate the behaviour of each specific DLO. We use these posteriors for domain randomisation while training, in simulation, object-specific visuomotor policies (i.e. assuming only visual and proprioceptive sensory) for a DLO reaching task, using model-free reinforcement learning. We demonstrate the utility of this approach by deploying sim-trained DLO manipulation policies in the real world in a zero-shot manner, i.e. without any further fine-tuning. In this context, we evaluate the capacity of a prominent LFI method to perform fine classification over the parametric set of DLOs, using only visual and proprioceptive data obtained in a dynamic manipulation trajectory. We then study the implications of the resulting domain distributions in sim-based policy learning and real-world performance.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Between Circuits and Chomsky: Pre-pretraining on Formal Languages Imparts Linguistic Biases</title>
<link>https://arxiv.org/abs/2502.19249</link>
<guid>https://arxiv.org/abs/2502.19249</guid>
<content:encoded><![CDATA[
arXiv:2502.19249v2 Announce Type: replace-cross 
Abstract: Pretraining language models on formal language can improve their acquisition of natural language. Which features of the formal language impart an inductive bias that leads to effective transfer? Drawing on insights from linguistics and complexity theory, we hypothesize that effective transfer occurs when two conditions are met: the formal language should capture the dependency structures present in natural language, and it should remain within the computational limitations of the model architecture. We experiment with pre-pretraining (training on formal language before natural languages) on transformers and find that formal languages capturing hierarchical dependencies indeed enable language models to achieve lower loss on natural language and better linguistic generalization compared to other formal languages. We also find modest support for the hypothesis that the formal language should fall within the computational limitations of the architecture. Strikingly, pre-pretraining reduces loss more efficiently than training on a matched amount of natural language. For a 1B-parameter language model trained on roughly 1.6B tokens of natural language, pre-pretraining achieves the same loss and better linguistic generalization with a 33% smaller token budget. Finally, we also give mechanistic evidence of transfer from formal to natural language: attention heads acquired during pre-pretraining remain crucial for the model's performance on syntactic evaluations.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Backpropagation-free Spiking Neural Networks with the Forward-Forward Algorithm</title>
<link>https://arxiv.org/abs/2502.20411</link>
<guid>https://arxiv.org/abs/2502.20411</guid>
<content:encoded><![CDATA[
arXiv:2502.20411v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) offer a biologically inspired computational paradigm that emulates neuronal activity through discrete spike-based processing. Despite their advantages, training SNNs with traditional backpropagation (BP) remains challenging due to computational inefficiencies and a lack of biological plausibility. This study explores the Forward-Forward (FF) algorithm as an alternative learning framework for SNNs. Unlike backpropagation, which relies on forward and backward passes, the FF algorithm employs two forward passes, enabling layer-wise localized learning, enhanced computational efficiency, and improved compatibility with neuromorphic hardware. We introduce an FF-based SNN training framework and evaluate its performance across both non-spiking (MNIST, Fashion-MNIST, Kuzushiji-MNIST) and spiking (Neuro-MNIST, SHD) datasets. Experimental results demonstrate that our model surpasses existing FF-based SNNs on evaluated static datasets with a much lighter architecture while achieving accuracy comparable to state-of-the-art backpropagation-trained SNNs. On more complex spiking tasks such as SHD, our approach outperforms other SNN models and remains competitive with leading backpropagation-trained SNNs. These findings highlight the FF algorithm's potential to advance SNN training methodologies by addressing some key limitations of backpropagation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Map Space Belief Prediction for Manipulation-Enhanced Mapping</title>
<link>https://arxiv.org/abs/2502.20606</link>
<guid>https://arxiv.org/abs/2502.20606</guid>
<content:encoded><![CDATA[
arXiv:2502.20606v2 Announce Type: replace-cross 
Abstract: Searching for objects in cluttered environments requires selecting efficient viewpoints and manipulation actions to remove occlusions and reduce uncertainty in object locations, shapes, and categories. In this work, we address the problem of manipulation-enhanced semantic mapping, where a robot has to efficiently identify all objects in a cluttered shelf. Although Partially Observable Markov Decision Processes~(POMDPs) are standard for decision-making under uncertainty, representing unstructured interactive worlds remains challenging in this formalism. To tackle this, we define a POMDP whose belief is summarized by a metric-semantic grid map and propose a novel framework that uses neural networks to perform map-space belief updates to reason efficiently and simultaneously about object geometries, locations, categories, occlusions, and manipulation physics. Further, to enable accurate information gain analysis, the learned belief updates should maintain calibrated estimates of uncertainty. Therefore, we propose Calibrated Neural-Accelerated Belief Updates (CNABUs) to learn a belief propagation model that generalizes to novel scenarios and provides confidence-calibrated predictions for unknown areas. Our experiments show that our novel POMDP planner improves map completeness and accuracy over existing methods in challenging simulations and successfully transfers to real-world cluttered shelves in zero-shot fashion.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A data augmentation strategy for deep neural networks with application to epidemic modelling</title>
<link>https://arxiv.org/abs/2502.21033</link>
<guid>https://arxiv.org/abs/2502.21033</guid>
<content:encoded><![CDATA[
arXiv:2502.21033v2 Announce Type: replace-cross 
Abstract: In this work, we integrate the predictive capabilities of compartmental disease dynamics models with machine learning ability to analyze complex, high-dimensional data and uncover patterns that conventional models may overlook. Specifically, we present a proof of concept demonstrating the application of data-driven methods and deep neural networks to a recently introduced Susceptible-Infected-Recovered type model with social features, including a saturated incidence rate, to improve epidemic prediction and forecasting. Our results show that a robust data augmentation strategy trough suitable data-driven models can improve the reliability of Feed-Forward Neural Networks and Nonlinear Autoregressive Networks, providing a complementary strategy to Physics-Informed Neural Networks, particularly in settings where data augmentation from mechanistic models can enhance learning. This approach enhances the ability to handle nonlinear dynamics and offers scalable, data-driven solutions for epidemic forecasting, prioritizing predictive accuracy over the constraints of physics-based models. Numerical simulations of the lockdown and post-lockdown phase of the COVID-19 epidemic in Italy and Spain validate our methodology.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReMA: Learning to Meta-think for LLMs with Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2503.09501</link>
<guid>https://arxiv.org/abs/2503.09501</guid>
<content:encoded><![CDATA[
arXiv:2503.09501v3 Announce Type: replace-cross 
Abstract: Recent research on Reasoning of Large Language Models (LLMs) has sought to further enhance their performance by integrating meta-thinking -- enabling models to monitor, evaluate, and control their reasoning processes for more adaptive and effective problem-solving. However, current single-agent work lacks a specialized design for acquiring meta-thinking, resulting in low efficacy. To address this challenge, we introduce Reinforced Meta-thinking Agents (ReMA), a novel framework that leverages Multi-Agent Reinforcement Learning (MARL) to elicit meta-thinking behaviors, encouraging LLMs to think about thinking. ReMA decouples the reasoning process into two hierarchical agents: a high-level meta-thinking agent responsible for generating strategic oversight and plans, and a low-level reasoning agent for detailed executions. Through iterative reinforcement learning with aligned objectives, these agents explore and learn collaboration, leading to improved generalization and robustness. Empirical results from single-turn experiments demonstrate that ReMA outperforms single-agent RL baselines on complex reasoning tasks, including competitive-level mathematical benchmarks and LLM-as-a-Judge benchmarks. Additionally, we further extend ReMA to multi-turn interaction settings, leveraging turn-level ratio and parameter sharing to improve efficiency. Comprehensive ablation studies further illustrate the evolving dynamics of each distinct agent, providing valuable insights into how the meta-thinking reasoning process enhances the reasoning capabilities of LLMs. Our code can be found in https://github.com/ziyuwan/ReMA-public
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample and Map from a Single Convex Potential: Generation using Conjugate Moment Measures</title>
<link>https://arxiv.org/abs/2503.10576</link>
<guid>https://arxiv.org/abs/2503.10576</guid>
<content:encoded><![CDATA[
arXiv:2503.10576v2 Announce Type: replace-cross 
Abstract: The canonical approach in generative modeling is to split model fitting into two blocks: define first how to sample noise (e.g. Gaussian) and choose next what to do with it (e.g. using a single map or flows). We explore in this work an alternative route that ties sampling and mapping. We find inspiration in moment measures, a result that states that for any measure $\rho$, there exists a unique convex potential $u$ such that $\rho=\nabla u \sharp e^{-u}$. While this does seem to tie effectively sampling (from log-concave distribution $e^{-u}$) and action (pushing particles through $\nabla u$), we observe on simple examples (e.g., Gaussians or 1D distributions) that this choice is ill-suited for practical tasks. We study an alternative factorization, where $\rho$ is factorized as $\nabla w^*\sharp e^{-w}$, where $w^*$ is the convex conjugate of a convex potential $w$. We call this approach conjugate moment measures, and show far more intuitive results on these examples. Because $\nabla w^*$ is the Monge map between the log-concave distribution $e^{-w}$ and $\rho$, we rely on optimal transport solvers to propose an algorithm to recover $w$ from samples of $\rho$, and parameterize $w$ as an input-convex neural network. We also address the common sampling scenario in which the density of $\rho$ is known only up to a normalizing constant, and propose an algorithm to learn $w$ in this setting.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Machine Learning - Driven Materials Discovery: Unlocking Next-Generation Functional Materials -- A minireview</title>
<link>https://arxiv.org/abs/2503.18975</link>
<guid>https://arxiv.org/abs/2503.18975</guid>
<content:encoded><![CDATA[
arXiv:2503.18975v2 Announce Type: replace-cross 
Abstract: The rapid advancement of machine learning and artificial intelligence (AI)-driven techniques is revolutionizing materials discovery, property prediction, and material design by minimizing human intervention and accelerating scientific progress. This review provides a comprehensive overview of smart, machine learning (ML)-driven approaches, emphasizing their role in predicting material properties, discovering novel compounds, and optimizing material structures. Key methodologies ranging from deep learning, graph neural networks, and Bayesian optimization to automated generative models, such as generative adversarial networks (GANs) and variational autoencoders (VAEs) enable the autonomous design of materials with tailored functionalities. By leveraging AutoML frameworks (e.g., AutoGluon, TPOT, and H2O.ai), researchers can automate the model selection, hyperparameter tuning, and feature engineering, significantly improving the efficiency of materials informatics. Furthermore, the integration of AI-driven robotic laboratories and high-throughput computing has established a fully automated pipeline for rapid synthesis and experimental validation, drastically reducing the time and cost of material discovery. This review highlights real-world applications of automated ML-driven approaches in predicting mechanical, thermal, electrical, and optical properties of materials, demonstrating successful cases in superconductors, catalysts, photovoltaics, and energy storage systems. We also address key challenges, such as data quality, interpretability, and the integration of AutoML with quantum computing, which are essential for future advancements. Ultimately, the synergy between AI, automated experimentation, and computational modeling transforms the way the materials are discovered, optimized, and designed, paving the way for next-generation innovations in energy, electronics, and nanotechnology.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimizing Case-Based Reasoning System for Functional Test Script Generation with Large Language Models</title>
<link>https://arxiv.org/abs/2503.20576</link>
<guid>https://arxiv.org/abs/2503.20576</guid>
<content:encoded><![CDATA[
arXiv:2503.20576v3 Announce Type: replace-cross 
Abstract: In this work, we explore the potential of large language models (LLMs) for generating functional test scripts, which necessitates understanding the dynamically evolving code structure of the target software. To achieve this, we propose a case-based reasoning (CBR) system utilizing a 4R cycle (i.e., retrieve, reuse, revise, and retain), which maintains and leverages a case bank of test intent descriptions and corresponding test scripts to facilitate LLMs for test script generation. To improve user experience further, we introduce Re4, an optimization method for the CBR system, comprising reranking-based retrieval finetuning and reinforced reuse finetuning. Specifically, we first identify positive examples with high semantic and script similarity, providing reliable pseudo-labels for finetuning the retriever model without costly labeling. Then, we apply supervised finetuning, followed by a reinforcement learning finetuning stage, to align LLMs with our production scenarios, ensuring the faithful reuse of retrieved cases. Extensive experimental results on two product development units from Huawei Datacom demonstrate the superiority of the proposed CBR+Re4. Notably, we also show that the proposed Re4 method can help alleviate the repetitive generation issues with LLMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From learnable objects to learnable random objects</title>
<link>https://arxiv.org/abs/2504.00847</link>
<guid>https://arxiv.org/abs/2504.00847</guid>
<content:encoded><![CDATA[
arXiv:2504.00847v2 Announce Type: replace-cross 
Abstract: We consider the relationship between learnability of a "base class" of functions on a set $X$, and learnability of a class of statistical functions derived from the base class. For example, we refine results showing that learnability of a family $h_p: p \in Y$ of functions implies learnability of the family of functions $h_\mu=\lambda p: Y. E_\mu(h_p)$, where $E_\mu$ is the expectation with respect to $\mu$, and $\mu$ ranges over probability distributions on $X$. We will look at both Probably Approximately Correct (PAC) learning, where example inputs and outputs are chosen at random, and online learning, where the examples are chosen adversarily. For agnostic learning, we establish improved bounds on the sample complexity of learning for statistical classes, stated in terms of combinatorial dimensions of the base class. We connect these problems to techniques introduced in model theory for "randomizing a structure". We also provide counterexamples for realizable learning, in both the PAC and online settings.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conditional Distribution Compression via the Kernel Conditional Mean Embedding</title>
<link>https://arxiv.org/abs/2504.10139</link>
<guid>https://arxiv.org/abs/2504.10139</guid>
<content:encoded><![CDATA[
arXiv:2504.10139v2 Announce Type: replace-cross 
Abstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>TAPIP3D: Tracking Any Point in Persistent 3D Geometry</title>
<link>https://arxiv.org/abs/2504.14717</link>
<guid>https://arxiv.org/abs/2504.14717</guid>
<content:encoded><![CDATA[
arXiv:2504.14717v2 Announce Type: replace-cross 
Abstract: We introduce TAPIP3D, a novel approach for long-term 3D point tracking in monocular RGB and RGB-D videos. TAPIP3D represents videos as camera-stabilized spatio-temporal feature clouds, leveraging depth and camera motion information to lift 2D video features into a 3D world space where camera movement is effectively canceled out. Within this stabilized 3D representation, TAPIP3D iteratively refines multi-frame motion estimates, enabling robust point tracking over long time horizons. To handle the irregular structure of 3D point distributions, we propose a 3D Neighborhood-to-Neighborhood (N2N) attention mechanism - a 3D-aware contextualization strategy that builds informative, spatially coherent feature neighborhoods to support precise trajectory estimation. Our 3D-centric formulation significantly improves performance over existing 3D point tracking methods and even surpasses state-of-the-art 2D pixel trackers in accuracy when reliable depth is available. The model supports inference in both camera-centric (unstabilized) and world-centric (stabilized) coordinates, with experiments showing that compensating for camera motion leads to substantial gains in tracking robustness. By replacing the conventional 2D square correlation windows used in prior 2D and 3D trackers with a spatially grounded 3D attention mechanism, TAPIP3D achieves strong and consistent results across multiple 3D point tracking benchmarks. Project Page: https://tapip3d.github.io
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Non-identifiability distinguishes Neural Networks among Parametric Models</title>
<link>https://arxiv.org/abs/2504.18017</link>
<guid>https://arxiv.org/abs/2504.18017</guid>
<content:encoded><![CDATA[
arXiv:2504.18017v2 Announce Type: replace-cross 
Abstract: One of the enduring problems surrounding neural networks is to identify the factors that differentiate them from traditional statistical models. We prove a pair of results which distinguish feedforward neural networks among parametric models at the population level, for regression tasks. Firstly, we prove that for any pair of random variables $(X,Y)$, neural networks always learn a nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove that for reasonable smooth parametric models, under local and global identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which the parametric model learns the constant predictor $\mathbb{E}[Y]$. Together, our results suggest that a lack of identifiability distinguishes neural networks among the class of smooth parametric models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Generalization Analysis for Contrastive Representation Learning under Non-IID Settings</title>
<link>https://arxiv.org/abs/2505.04937</link>
<guid>https://arxiv.org/abs/2505.04937</guid>
<content:encoded><![CDATA[
arXiv:2505.04937v2 Announce Type: replace-cross 
Abstract: Contrastive Representation Learning (CRL) has achieved impressive success in various domains in recent years. Nevertheless, the theoretical understanding of the generalization behavior of CRL has remained limited. Moreover, to the best of our knowledge, the current literature only analyzes generalization bounds under the assumption that the data tuples used for contrastive learning are independently and identically distributed. However, in practice, we are often limited to a fixed pool of reusable labeled data points, making it inevitable to recycle data across tuples to create sufficiently large datasets. Therefore, the tuple-wise independence condition imposed by previous works is invalidated. In this paper, we provide a generalization analysis for the CRL framework under non-$i.i.d.$ settings that adheres to practice more realistically. Drawing inspiration from the literature on U-statistics, we derive generalization bounds which indicate the required number of samples in each class scales as the logarithm of the covering number of the class of learnable feature representations associated to each class. Next, we apply our main results to derive excess risk bounds for common function classes such as linear maps and neural networks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</title>
<link>https://arxiv.org/abs/2505.06771</link>
<guid>https://arxiv.org/abs/2505.06771</guid>
<content:encoded><![CDATA[
arXiv:2505.06771v2 Announce Type: replace-cross 
Abstract: Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot RL (MRRL) policies with realistic robot dynamics and safety constraints, supporting parallelization and hardware acceleration. Our generalizable learning interface integrates easily with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation. Our code is available at https://github.com/GT-STAR-Lab/JaxRobotarium.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Training neural control variates using correlated configurations</title>
<link>https://arxiv.org/abs/2505.07719</link>
<guid>https://arxiv.org/abs/2505.07719</guid>
<content:encoded><![CDATA[
arXiv:2505.07719v2 Announce Type: replace-cross 
Abstract: Neural control variates (NCVs) have emerged as a powerful tool for variance reduction in Monte Carlo (MC) simulations, particularly in high-dimensional problems where traditional control variates are difficult to construct analytically. By training neural networks to learn auxiliary functions correlated with the target observable, NCVs can significantly reduce estimator variance while preserving unbiasedness. However, a critical but often overlooked aspect of NCV training is the role of autocorrelated samples generated by Markov Chain Monte Carlo (MCMC). While such samples are typically discarded for error estimation due to their statistical redundancy, they may contain useful information about the structure of the underlying probability distribution that can benefit the training process. In this work, we systematically examine the effect of using correlated configurations in training neural control variates. We demonstrate, both conceptually and numerically, that training on correlated data can improve control variate performance, especially in settings with limited computational resources. Our analysis includes empirical results from $U(1)$ gauge theory and scalar field theory, illustrating when and how autocorrelated samples enhance NCV construction. These findings provide practical guidance for the efficient use of MCMC data in training neural networks.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation</title>
<link>https://arxiv.org/abs/2505.07843</link>
<guid>https://arxiv.org/abs/2505.07843</guid>
<content:encoded><![CDATA[
arXiv:2505.07843v2 Announce Type: replace-cross 
Abstract: In poster design, content-aware layout generation is crucial for automatically arranging visual-textual elements on the given image. With limited training data, existing work focused on image-centric enhancement. However, this neglects the diversity of layouts and fails to cope with shape-variant elements or diverse design intents in generalized settings. To this end, we proposed a layout-centric approach that leverages layout knowledge implicit in large language models (LLMs) to create posters for omnifarious purposes, hence the name PosterO. Specifically, it structures layouts from datasets as trees in SVG language by universal shape, design intent vectorization, and hierarchical node representation. Then, it applies LLMs during inference to predict new layout trees by in-context learning with intent-aligned example selection. After layout trees are generated, we can seamlessly realize them into poster designs by editing the chat with LLMs. Extensive experimental results have demonstrated that PosterO can generate visually appealing layouts for given images, achieving new state-of-the-art performance across various benchmarks. To further explore PosterO's abilities under the generalized settings, we built PStylish7, the first dataset with multi-purpose posters and various-shaped elements, further offering a challenging test for advanced research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation</title>
<link>https://arxiv.org/abs/2505.08548</link>
<guid>https://arxiv.org/abs/2505.08548</guid>
<content:encoded><![CDATA[
arXiv:2505.08548v2 Announce Type: replace-cross 
Abstract: Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both "seeing" and "doing," achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 40.6% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multi-Stage Speaker Diarization for Noisy Classrooms</title>
<link>https://arxiv.org/abs/2505.10879</link>
<guid>https://arxiv.org/abs/2505.10879</guid>
<content:encoded><![CDATA[
arXiv:2505.10879v2 Announce Type: replace-cross 
Abstract: Speaker diarization, the process of identifying "who spoke when" in audio recordings, is essential for understanding classroom dynamics. However, classroom settings present distinct challenges, including poor recording quality, high levels of background noise, overlapping speech, and the difficulty of accurately capturing children's voices. This study investigates the effectiveness of multi-stage diarization models using Nvidia's NeMo diarization pipeline. We assess the impact of denoising on diarization accuracy and compare various voice activity detection (VAD) models, including self-supervised transformer-based frame-wise VAD models. We also explore a hybrid VAD approach that integrates Automatic Speech Recognition (ASR) word-level timestamps with frame-level VAD predictions. We conduct experiments using two datasets from English speaking classrooms to separate teacher vs. student speech and to separate all speakers. Our results show that denoising significantly improves the Diarization Error Rate (DER) by reducing the rate of missed speech. Additionally, training on both denoised and noisy datasets leads to substantial performance gains in noisy conditions. The hybrid VAD model leads to further improvements in speech detection, achieving a DER as low as 17% in teacher-student experiments and 45% in all-speaker experiments. However, we also identified trade-offs between voice activity detection and speaker confusion. Overall, our study highlights the effectiveness of multi-stage diarization models and integrating ASR-based information for enhancing speaker diarization in noisy classroom environments.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges</title>
<link>https://arxiv.org/abs/2505.11618</link>
<guid>https://arxiv.org/abs/2505.11618</guid>
<content:encoded><![CDATA[
arXiv:2505.11618v2 Announce Type: replace-cross 
Abstract: Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS). Despite advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs), their capacity to reason about complex spatiotemporal signals remains underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning benchmaRK, STARK, to systematically evaluate LLMs across three levels of reasoning complexity: state estimation (e.g., predicting field variables, localizing and tracking events in space and time), spatiotemporal reasoning over states (e.g., inferring spatial-temporal relationships), and world-knowledge-aware reasoning that integrates contextual and domain knowledge (e.g., intent prediction, landmark-aware navigation). We curate 26 distinct spatiotemporal tasks with diverse sensor modalities, comprising 14,552 challenges where models answer directly or by Python Code Interpreter. Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks requiring geometric reasoning (e.g., multilateration or triangulation), particularly as complexity increases. Surprisingly, LRMs show robust performance across tasks with various levels of difficulty, often competing or surpassing traditional first-principle-based methods. Our results show that in reasoning tasks requiring world knowledge, the performance gap between LLMs and LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model continues to achieve leading performance across all evaluated tasks, a result attributed primarily to the larger size of the reasoning models. STARK motivates future innovations in model architectures and reasoning paradigms for intelligent CPS by providing a structured framework to identify limitations in the spatiotemporal reasoning of LLMs and LRMs.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PeerGuard: Defending Multi-Agent Systems Against Backdoor Attacks Through Mutual Reasoning</title>
<link>https://arxiv.org/abs/2505.11642</link>
<guid>https://arxiv.org/abs/2505.11642</guid>
<content:encoded><![CDATA[
arXiv:2505.11642v2 Announce Type: replace-cross 
Abstract: Multi-agent systems leverage advanced AI models as autonomous agents that interact, cooperate, or compete to complete complex tasks across applications such as robotics and traffic management. Despite their growing importance, safety in multi-agent systems remains largely underexplored, with most research focusing on single AI models rather than interacting agents. This work investigates backdoor vulnerabilities in multi-agent systems and proposes a defense mechanism based on agent interactions. By leveraging reasoning abilities, each agent evaluates responses from others to detect illogical reasoning processes, which indicate poisoned agents. Experiments on LLM-based multi-agent systems, including ChatGPT series and Llama 3, demonstrate the effectiveness of the proposed method, achieving high accuracy in identifying poisoned agents while minimizing false positives on clean agents. We believe this work provides insights into multi-agent system safety and contributes to the development of robust, trustworthy AI interactions.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SoftPQ: Robust Instance Segmentation Evaluation via Soft Matching and Tunable Thresholds</title>
<link>https://arxiv.org/abs/2505.12155</link>
<guid>https://arxiv.org/abs/2505.12155</guid>
<content:encoded><![CDATA[
arXiv:2505.12155v2 Announce Type: replace-cross 
Abstract: Segmentation evaluation metrics traditionally rely on binary decision logic: predictions are either correct or incorrect, based on rigid IoU thresholds. Detection--based metrics such as F1 and mAP determine correctness at the object level using fixed overlap cutoffs, while overlap--based metrics like Intersection over Union (IoU) and Dice operate at the pixel level, often overlooking instance--level structure. Panoptic Quality (PQ) attempts to unify detection and segmentation assessment, but it remains dependent on hard-threshold matching--treating predictions below the threshold as entirely incorrect. This binary framing obscures important distinctions between qualitatively different errors and fails to reward gradual model improvements. We propose SoftPQ, a flexible and interpretable instance segmentation metric that redefines evaluation as a graded continuum rather than a binary classification. SoftPQ introduces tunable upper and lower IoU thresholds to define a partial matching region and applies a sublinear penalty function to ambiguous or fragmented predictions. These extensions allow SoftPQ to exhibit smoother score behavior, greater robustness to structural segmentation errors, and more informative feedback for model development and evaluation. Through controlled perturbation experiments, we show that SoftPQ captures meaningful differences in segmentation quality that existing metrics overlook, making it a practical and principled alternative for both benchmarking and iterative model refinement.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Visuospatial Cognitive Assistant</title>
<link>https://arxiv.org/abs/2505.12312</link>
<guid>https://arxiv.org/abs/2505.12312</guid>
<content:encoded><![CDATA[
arXiv:2505.12312v2 Announce Type: replace-cross 
Abstract: Video-based spatial cognition is vital for robotics and embodied AI but challenges current Vision-Language Models (VLMs). This paper makes two key contributions. First, we introduce ViCA (Visuospatial Cognitive Assistant)-322K, a diverse dataset of 322,003 QA pairs from real-world indoor videos (ARKitScenes, ScanNet, ScanNet++), offering supervision for 3D metadata-grounded queries and video-based complex reasoning. Second, we develop ViCA-7B, fine-tuned on ViCA-322K, which achieves new state-of-the-art on all eight VSI-Bench tasks, outperforming existing models, including larger ones (e.g., +26.1 on Absolute Distance). For interpretability, we present ViCA-Thinking-2.68K, a dataset with explicit reasoning chains, and fine-tune ViCA-7B to create ViCA-7B-Thinking, a model that articulates its spatial reasoning. Our work highlights the importance of targeted data and suggests paths for improved temporal-spatial modeling. We release all resources to foster research in robust visuospatial intelligence.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Visuospatial Cognition via Hierarchical Fusion of Visual Experts</title>
<link>https://arxiv.org/abs/2505.12363</link>
<guid>https://arxiv.org/abs/2505.12363</guid>
<content:encoded><![CDATA[
arXiv:2505.12363v2 Announce Type: replace-cross 
Abstract: While Multimodal Large Language Models (MLLMs) excel at general vision-language tasks, visuospatial cognition - reasoning about spatial layouts, relations, and dynamics - remains a significant challenge. Existing models often lack the necessary architectural components and specialized training data for fine-grained spatial understanding. We introduce ViCA2 (Visuospatial Cognitive Assistant 2), a novel MLLM designed to enhance spatial reasoning. ViCA2 features a dual vision encoder architecture integrating SigLIP for semantics and Hiera for spatial structure, coupled with a token ratio control mechanism for efficiency. We also developed ViCA-322K, a new large-scale dataset with over 322,000 spatially grounded question-answer pairs for targeted instruction tuning. On the challenging VSI-Bench benchmark, our ViCA2-7B model achieves a state-of-the-art average score of 56.8, significantly surpassing larger open-source models (e.g., LLaVA-NeXT-Video-72B, 40.9) and leading proprietary models (Gemini-1.5 Pro, 45.4). This demonstrates the effectiveness of our approach in achieving strong visuospatial intelligence with a compact model. We release ViCA2, its codebase, and the ViCA-322K dataset to facilitate further research.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FRABench and GenEval: Scaling Fine-Grained Aspect Evaluation across Tasks, Modalities</title>
<link>https://arxiv.org/abs/2505.12795</link>
<guid>https://arxiv.org/abs/2505.12795</guid>
<content:encoded><![CDATA[
arXiv:2505.12795v2 Announce Type: replace-cross 
Abstract: Evaluating the open-ended outputs of large language models (LLMs) has become a bottleneck as model capabilities, task diversity, and modality coverage rapidly expand. Existing "LLM-as-a-Judge" evaluators are typically narrow in a few tasks, aspects, or modalities, and easily suffer from low consistency. In this paper, we argue that explicit, fine-grained aspect specification is the key to both generalizability and objectivity in automated evaluation. To this end, we propose a hierarchical aspect taxonomy encompassing 112 distinct aspects that unifies evaluation across four representative settings -- Natural Language Generation, Image Understanding, Image Generation, and Interleaved Text-and-Image Generation. Building upon this taxonomy, we create FRABench, a benchmark comprising 60.4k pairwise samples with 325k evaluation labels obtained from a combination of human and LLM annotations. FRABench provides the first large-scale, multi-modal resource for training and meta-evaluating fine-grained LMM judges. Leveraging FRABench, we develop GenEval, a fine-grained evaluator generalizable across tasks and modalities. Experiments show that GenEval (i) attains high agreement with GPT-4o and expert annotators, (ii) transfers robustly to unseen tasks and modalities, and (iii) reveals systematic weaknesses of current LMMs on evaluation.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Two Experts Are All You Need for Steering Thinking: Reinforcing Cognitive Effort in MoE Reasoning Models Without Additional Training</title>
<link>https://arxiv.org/abs/2505.14681</link>
<guid>https://arxiv.org/abs/2505.14681</guid>
<content:encoded><![CDATA[
arXiv:2505.14681v2 Announce Type: replace-cross 
Abstract: Mixture-of-Experts (MoE) architectures within Large Reasoning Models (LRMs) have achieved impressive reasoning capabilities by selectively activating experts to facilitate structured cognitive processes. Despite notable advances, existing reasoning models often suffer from cognitive inefficiencies like overthinking and underthinking. To address these limitations, we introduce a novel inference-time steering methodology called Reinforcing Cognitive Experts (RICE), designed to improve reasoning performance without additional training or complex heuristics. Leveraging normalized Pointwise Mutual Information (nPMI), we systematically identify specialized experts, termed ''cognitive experts'' that orchestrate meta-level reasoning operations characterized by tokens like ''''. Empirical evaluations with leading MoE-based LRMs (DeepSeek-R1 and Qwen3-235B) on rigorous quantitative and scientific reasoning benchmarks demonstrate noticeable and consistent improvements in reasoning accuracy, cognitive efficiency, and cross-domain generalization. Crucially, our lightweight approach substantially outperforms prevalent reasoning-steering techniques, such as prompt design and decoding constraints, while preserving the model's general instruction-following skills. These results highlight reinforcing cognitive experts as a promising, practical, and interpretable direction to enhance cognitive efficiency within advanced reasoning models.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Real-Time Stress Monitoring, Detection, and Management in College Students: A Wearable Technology and Machine-Learning Approach</title>
<link>https://arxiv.org/abs/2505.15974</link>
<guid>https://arxiv.org/abs/2505.15974</guid>
<content:encoded><![CDATA[
arXiv:2505.15974v2 Announce Type: replace-cross 
Abstract: College students are increasingly affected by stress, anxiety, and depression, yet face barriers to traditional mental health care. This study evaluated the efficacy of a mobile health (mHealth) intervention, Mental Health Evaluation and Lookout Program (mHELP), which integrates a smartwatch sensor and machine learning (ML) algorithms for real-time stress detection and self-management. In a 12-week randomized controlled trial (n = 117), participants were assigned to a treatment group using mHELP's full suite of interventions or a control group using the app solely for real-time stress logging and weekly psychological assessments. The primary outcome, "Moments of Stress" (MS), was assessed via physiological and self-reported indicators and analyzed using Generalized Linear Mixed Models (GLMM) approaches. Similarly, secondary outcomes of psychological assessments, including the Generalized Anxiety Disorder-7 (GAD-7) for anxiety, the Patient Health Questionnaire (PHQ-8) for depression, and the Perceived Stress Scale (PSS), were also analyzed via GLMM. The finding of the objective measure, MS, indicates a substantial decrease in MS among the treatment group compared to the control group, while no notable between-group differences were observed in subjective scores of anxiety (GAD-7), depression (PHQ-8), or stress (PSS). However, the treatment group exhibited a clinically meaningful decline in GAD-7 and PSS scores. These findings underscore the potential of wearable-enabled mHealth tools to reduce acute stress in college populations and highlight the need for extended interventions and tailored features to address chronic symptoms like depression.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Integral Imprecise Probability Metrics</title>
<link>https://arxiv.org/abs/2505.16156</link>
<guid>https://arxiv.org/abs/2505.16156</guid>
<content:encoded><![CDATA[
arXiv:2505.16156v2 Announce Type: replace-cross 
Abstract: Quantifying differences between probability distributions is fundamental to statistics and machine learning, primarily for comparing statistical uncertainty. In contrast, epistemic uncertainty (EU) -- due to incomplete knowledge -- requires richer representations than those offered by classical probability. Imprecise probability (IP) theory offers such models, capturing ambiguity and partial belief. This has driven growing interest in imprecise probabilistic machine learning (IPML), where inference and decision-making rely on broader uncertainty models -- highlighting the need for metrics beyond classical probability. This work introduces the Integral Imprecise Probability Metric (IIPM) framework, a Choquet integral-based generalisation of classical Integral Probability Metric (IPM) to the setting of capacities -- a broad class of IP models encompassing many existing ones, including lower probabilities, probability intervals, belief functions, and more. Theoretically, we establish conditions under which IIPM serves as a valid metric and metrises a form of weak convergence of capacities. Practically, IIPM not only enables comparison across different IP models but also supports the quantification of epistemic uncertainty within a single IP model. In particular, by comparing an IP model with its conjugate, IIPM gives rise to a new class of EU measures -- Maximum Mean Imprecision -- which satisfy key axiomatic properties proposed in the Uncertainty Quantification literature. We validate MMI through selective classification experiments, demonstrating strong empirical performance against established EU measures, and outperforming them when classical methods struggle to scale to a large number of classes. Our work advances both theory and practice in IPML, offering a principled framework for comparing and quantifying epistemic uncertainty under imprecision.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Power-Law Decay Loss for Large Language Model Finetuning: Focusing on Information Sparsity to Enhance Generation Quality</title>
<link>https://arxiv.org/abs/2505.16900</link>
<guid>https://arxiv.org/abs/2505.16900</guid>
<content:encoded><![CDATA[
arXiv:2505.16900v3 Announce Type: replace-cross 
Abstract: During the finetuning stage of text generation tasks, standard cross-entropy loss treats all tokens equally. This can lead models to overemphasize high-frequency, low-information tokens, neglecting lower-frequency tokens crucial for specificity and informativeness in generated content. This paper introduces a novel loss function, Power-Law Decay Loss (PDL), specifically designed to optimize the finetuning process for text generation. The core motivation for PDL stems from observations in information theory and linguistics: the informativeness of a token is often inversely proportional to its frequency of occurrence. PDL re-weights the contribution of each token in the standard cross-entropy loss based on its frequency in the training corpus, following a power-law decay. Specifically, the weights for high-frequency tokens are reduced, while low-frequency, information-dense tokens are assigned higher weights. This mechanism guides the model during finetuning to focus more on learning and generating tokens that convey specific and unique information, thereby enhancing the quality, diversity, and informativeness of the generated text. We theoretically elaborate on the motivation and construction of PDL and discuss its potential applications and advantages across various text generation finetuning tasks, such as abstractive summarization, dialogue systems, and style transfer.
]]></content:encoded>
<pubDate>Wed, 28 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Unveil Multi-Picture Descriptions for Multilingual Mild Cognitive Impairment Detection via Contrastive Learning</title>
<link>https://arxiv.org/abs/2505.17067</link>
<guid>https://arxiv.org/abs/2505.17067</guid>
<content:encoded><![CDATA[
<div> Keywords: Mild Cognitive Impairment, multilingual, picture descriptions, contrastive learning, Product of Experts <br />
Summary: <br />
Detecting Mild Cognitive Impairment from multilingual and multiple picture descriptions is a challenging task. This article proposes a framework for MCI detection that includes enhancing discriminative representation learning through supervised contrastive learning, incorporating image modality in addition to speech and text modalities, and employing a Product of Experts strategy to address spurious correlations and overfitting. The framework improves MCI detection performance, with a significant increase in both Unweighted Average Recall and F1 score compared to a text unimodal baseline. The contrastive learning component showed particular effectiveness for the text modality. These results demonstrate the framework's efficacy in the challenging task of multilingual and multi-picture MCI detection. <br /> <div>
arXiv:2505.17067v2 Announce Type: replace-cross 
Abstract: Detecting Mild Cognitive Impairment from picture descriptions is critical yet challenging, especially in multilingual and multiple picture settings. Prior work has primarily focused on English speakers describing a single picture (e.g., the 'Cookie Theft'). The TAUKDIAL-2024 challenge expands this scope by introducing multilingual speakers and multiple pictures, which presents new challenges in analyzing picture-dependent content. To address these challenges, we propose a framework with three components: (1) enhancing discriminative representation learning via supervised contrastive learning, (2) involving image modality rather than relying solely on speech and text modalities, and (3) applying a Product of Experts (PoE) strategy to mitigate spurious correlations and overfitting. Our framework improves MCI detection performance, achieving a +7.1% increase in Unweighted Average Recall (UAR) (from 68.1% to 75.2%) and a +2.9% increase in F1 score (from 80.6% to 83.5%) compared to the text unimodal baseline. Notably, the contrastive learning component yields greater gains for the text modality compared to speech. These results highlight our framework's effectiveness in multilingual and multi-picture MCI detection.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent</title>
<link>https://arxiv.org/abs/2308.09430</link>
<guid>https://arxiv.org/abs/2308.09430</guid>
<content:encoded><![CDATA[
<div> asynchronous delayed SGD, generalization performance, generalization error bounds, algorithmic stability, experimental results<br />
Summary:<br />
This paper explores the generalization performance of stochastic gradient descent (SGD) with asynchronous delays, a key component in training large-scale machine learning models. By analyzing the average stability of the delayed gradient algorithm, the study establishes sharper upper bounds on generalization error for quadratic convex and strongly convex problems. The bounds are $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$, respectively, where $T$ is the iteration number and $n$ is the training data size. The analysis reveals that asynchronous delays can reduce the generalization error in the delayed SGD algorithm. The research also extends to random delay settings, with experimental results confirming the theoretical findings. <div>
arXiv:2308.09430v4 Announce Type: replace 
Abstract: Stochastic gradient descent (SGD) performed in an asynchronous manner plays a crucial role in training large-scale machine learning models. However, the generalization performance of asynchronous delayed SGD, which is an essential metric for assessing machine learning algorithms, has rarely been explored. Existing generalization error bounds are rather pessimistic and cannot reveal the correlation between asynchronous delays and generalization. In this paper, we investigate sharper generalization error bound for SGD with asynchronous delay $\tau$. Leveraging the generating function analysis tool, we first establish the average stability of the delayed gradient algorithm. Based on this algorithmic stability, we provide upper bounds on the generalization error of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and $\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex problems, respectively, where $T$ refers to the iteration number and $n$ is the amount of training data. Our theoretical results indicate that asynchronous delays reduce the generalization error of the delayed SGD algorithm. Analogous analysis can be generalized to the random delay setting, and the experimental results validate our theoretical findings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A fast algorithm to minimize prediction loss of the optimal solution in inverse optimization problem of MILP</title>
<link>https://arxiv.org/abs/2405.14273</link>
<guid>https://arxiv.org/abs/2405.14273</guid>
<content:encoded><![CDATA[
<div> weight estimation, inverse optimization, mixed integer linear program, projected subgradient method, convergence speed

Summary:
The article addresses the inverse optimization problem of estimating weights for an optimal solution in mixed integer linear programming. Existing methods have slow convergence rates as the dimension of weights increases. The proposed method, utilizing a projected subgradient approach with a step size of k^-1/2, achieves efficient weight learning. The theoretical analysis reveals that the method guarantees a bounded distance between learned and true weights, approaching exact recovery of the optimal solution. Experimental results demonstrate the method's effectiveness in solving the inverse optimization problem with significantly fewer MILP calls compared to known methods, converging within a finite number of iterations. This novel approach offers a promising solution for efficiently estimating weights in MILP instances. 

<br /><br />Summary: <div>
arXiv:2405.14273v3 Announce Type: replace 
Abstract: We consider the inverse optimization problem of estimating the weights of the objective function such that the given solution is an optimal solution for a mixed integer linear program (MILP). In this inverse optimization problem, the known methods exhibit inefficient convergence. Specifically, if $d$ denotes the dimension of the weights and $k$ the number of iterations, then the error of the weights is bounded by $O(k^{-1/(d-1)})$, leading to slow convergence as $d$ increases. We propose a projected subgradient method with a step size of $k^{-1/2}$ based on suboptimality loss. We theoretically show and demonstrate that the proposed method efficiently learns the weights. In particular, we show that there exists a constant $\gamma > 0$ such that the distance between the learned and true weights is bounded by $ O\left(k^{-1/(1+\gamma)} \exp\left(-\frac{\gamma k^{1/2}}{2+\gamma}\right)\right), $ or the optimal solution is exactly recovered. Furthermore, experiments demonstrate that the proposed method solves the inverse optimization problems of MILP using fewer than $1/7$ the number of MILP calls required by known methods, and converges within a finite number of iterations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Imitation Learning via Focused Satisficing</title>
<link>https://arxiv.org/abs/2505.14820</link>
<guid>https://arxiv.org/abs/2505.14820</guid>
<content:encoded><![CDATA[
<div> Keywords: Imitation learning, satisficing theory, deep reinforcement learning, margin-based objective, policy

Summary:
Imitation learning typically assumes demonstrations are close to optimal, but satisficing theory suggests humans may settle for acceptable behaviors based on personal levels of aspiration. This study introduces a focused satisficing approach to imitation learning, aiming to surpass the demonstrator's aspirations without learning them explicitly. By utilizing a margin-based objective in deep reinforcement learning, the policy is guided to imitate high-quality demonstrations better than existing methods, delivering higher rates of demonstrator acceptability and competitive true returns across various environments. This approach focuses on achieving success based on the demonstrator's subjective standards rather than striving for optimality, demonstrating improved performance in imitation learning tasks. <div>
arXiv:2505.14820v2 Announce Type: replace 
Abstract: Imitation learning often assumes that demonstrations are close to optimal according to some fixed, but unknown, cost function. However, according to satisficing theory, humans often choose acceptable behavior based on their personal (and potentially dynamic) levels of aspiration, rather than achieving (near-) optimality. For example, a lunar lander demonstration that successfully lands without crashing might be acceptable to a novice despite being slow or jerky. Using a margin-based objective to guide deep reinforcement learning, our focused satisficing approach to imitation learning seeks a policy that surpasses the demonstrator's aspiration levels -- defined over trajectories or portions of trajectories -- on unseen demonstrations without explicitly learning those aspirations. We show experimentally that this focuses the policy to imitate the highest quality (portions of) demonstrations better than existing imitation learning methods, providing much higher rates of guaranteed acceptability to the demonstrator, and competitive true returns on a range of environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search</title>
<link>https://arxiv.org/abs/2505.15259</link>
<guid>https://arxiv.org/abs/2505.15259</guid>
<content:encoded><![CDATA[
<div> Keywords: Multimodal Large Language Models, Graphical User Interfaces, Data Efficiency, Self-generated Reasoning, Spatial-aware Criticism

Summary:
ReGUIDE is a framework designed for improving the performance of Multimodal Large Language Models in localizing interface elements on Graphical User Interfaces. The framework utilizes self-generated reasoning processes and spatial-aware criticism to learn efficiently from web data. Through online reinforcement learning, ReGUIDE can generate a language reasoning process for interface element localization and use spatial priors to enhance prediction accuracy. At inference time, a test-time scaling strategy is employed to combine spatial search and coordinate aggregation for improved performance. Experimental results indicate that ReGUIDE surpasses existing baselines with significantly fewer training data points, showcasing its effectiveness in web grounding tasks. <div>
arXiv:2505.15259v2 Announce Type: replace 
Abstract: Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines).
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Temporal Difference Method for Stochastic Continuous Dynamics</title>
<link>https://arxiv.org/abs/2505.15544</link>
<guid>https://arxiv.org/abs/2505.15544</guid>
<content:encoded><![CDATA[
<div> dynamical equations, Bellman's principle of optimality, Hamilton-Jacobi-Bellman equation, model-free approach, reinforcement learning

Summary:
This article presents a novel approach to reinforcement learning for continuous systems modeled by dynamical equations, such as ODEs and SDEs. While traditional RL methods assume known dynamics, this proposed model-free approach targets the Hamilton-Jacobi-Bellman (HJB) equation without requiring explicit access to the coefficient functions of the dynamical equations. By introducing a temporal difference method, this approach showcases potential advantages over transition kernel-based formulations, both qualitatively and empirically. This advancement in RL bridges the gap between stochastic optimal control and model-free reinforcement learning, offering a promising direction for future research and applications. <br /><br />Summary: <div>
arXiv:2505.15544v3 Announce Type: replace 
Abstract: For continuous systems modeled by dynamical equations such as ODEs and SDEs, Bellman's principle of optimality takes the form of the Hamilton-Jacobi-Bellman (HJB) equation, which provides the theoretical target of reinforcement learning (RL). Although recent advances in RL successfully leverage this formulation, the existing methods typically assume the underlying dynamics are known a priori because they need explicit access to the coefficient functions of dynamical equations to update the value function following the HJB equation. We address this inherent limitation of HJB-based RL; we propose a model-free approach still targeting the HJB equation and propose the corresponding temporal difference method. We demonstrate its potential advantages over transition kernel-based formulations, both qualitatively and empirically. The proposed formulation paves the way toward bridging stochastic optimal control and model-free reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MoRE-Brain: Routed Mixture of Experts for Interpretable and Generalizable Cross-Subject fMRI Visual Decoding</title>
<link>https://arxiv.org/abs/2505.15946</link>
<guid>https://arxiv.org/abs/2505.15946</guid>
<content:encoded><![CDATA[
<div> Hierarchical Mixture-of-Experts architecture, fMRI signals, visual reconstruction, interpretability, neural decoding <br />
<br />
Summary: <br />
The MoRE-Brain framework proposes a neuro-inspired approach for visual reconstruction from fMRI data, prioritizing high-fidelity, adaptability, and interpretability. It utilizes a hierarchical Mixture-of-Experts architecture, where specialized experts process fMRI signals from related voxel groups, mimicking brain networks. By encoding fMRI into the CLIP space and using a finetuned diffusion model for image synthesis, MoRE-Brain achieves efficient cross-subject generalization and provides enhanced mechanistic insight into how different brain regions influence the reconstructed image. The framework demonstrates high reconstruction fidelity and effectively utilizes fMRI signals, distinguishing genuine neural decoding from generative priors. MoRE-Brain marks a significant advancement in fMRI-based visual decoding by combining interpretability with reconstruction accuracy, making it a valuable tool for understanding human perception and developing brain-computer interfaces. <div>
arXiv:2505.15946v2 Announce Type: replace 
Abstract: Decoding visual experiences from fMRI offers a powerful avenue to understand human perception and develop advanced brain-computer interfaces. However, current progress often prioritizes maximizing reconstruction fidelity while overlooking interpretability, an essential aspect for deriving neuroscientific insight. To address this gap, we propose MoRE-Brain, a neuro-inspired framework designed for high-fidelity, adaptable, and interpretable visual reconstruction. MoRE-Brain uniquely employs a hierarchical Mixture-of-Experts architecture where distinct experts process fMRI signals from functionally related voxel groups, mimicking specialized brain networks. The experts are first trained to encode fMRI into the frozen CLIP space. A finetuned diffusion model then synthesizes images, guided by expert outputs through a novel dual-stage routing mechanism that dynamically weighs expert contributions across the diffusion process. MoRE-Brain offers three main advancements: First, it introduces a novel Mixture-of-Experts architecture grounded in brain network principles for neuro-decoding. Second, it achieves efficient cross-subject generalization by sharing core expert networks while adapting only subject-specific routers. Third, it provides enhanced mechanistic insight, as the explicit routing reveals precisely how different modeled brain regions shape the semantic and spatial attributes of the reconstructed image. Extensive experiments validate MoRE-Brain's high reconstruction fidelity, with bottleneck analyses further demonstrating its effective utilization of fMRI signals, distinguishing genuine neural decoding from over-reliance on generative priors. Consequently, MoRE-Brain marks a substantial advance towards more generalizable and interpretable fMRI-based visual decoding. Code will be publicly available soon: https://github.com/yuxiangwei0808/MoRE-Brain.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reinforcement Learning for Stock Transactions</title>
<link>https://arxiv.org/abs/2505.16099</link>
<guid>https://arxiv.org/abs/2505.16099</guid>
<content:encoded><![CDATA[
<div> Keywords: stock market, reinforcement learning, Markov Decision Process, Q-Learning, machine learning

Summary:
Reinforcement Learning (RL) is applied to determine the best time to buy and sell stocks within a given time frame. The project uses a self-defined Markov Decision Process (MDP) problem based on real-world data to train the model. Various agents are trained using Q-Learning, Q-Learning with linear function approximation, and deep Q-Learning, along with machine learning regression and classification models to predict stock prices. The state space and reward system of the MDP problem are formulated with the help of existing research. The goal is to analyze and compare the agents to see which one learns the best policy to maximize profit on the stock market.<br /><br />Summary: The project focuses on utilizing reinforcement learning and machine learning techniques to determine optimal times for buying and selling stocks. By training agents with Q-Learning and deep Q-Learning, a self-defined Markov Decision Process is used to model the stock market scenario. The study aims to compare different learning algorithms and models to identify the most effective strategy for maximizing profit in stock trading. <div>
arXiv:2505.16099v2 Announce Type: replace 
Abstract: Much research has been done to analyze the stock market. After all, if one can determine a pattern in the chaotic frenzy of transactions, then they could make a hefty profit from capitalizing on these insights. As such, the goal of our project was to apply reinforcement learning (RL) to determine the best time to buy a stock within a given time frame. With only a few adjustments, our model can be extended to identify the best time to sell a stock as well. In order to use the format of free, real-world data to train the model, we define our own Markov Decision Process (MDP) problem. These two papers [5] [6] helped us in formulating the state space and the reward system of our MDP problem. We train a series of agents using Q-Learning, Q-Learning with linear function approximation, and deep Q-Learning. In addition, we try to predict the stock prices using machine learning regression and classification models. We then compare our agents to see if they converge on a policy, and if so, which one learned the best policy to maximize profit on the stock market.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>End-to-End Framework for Predicting the Remaining Useful Life of Lithium-Ion Batteries</title>
<link>https://arxiv.org/abs/2505.16664</link>
<guid>https://arxiv.org/abs/2505.16664</guid>
<content:encoded><![CDATA[
<div> Feature Engineering, Deep Learning Model, RUL Prediction, Transfer Learning, Battery Degradation 
<br />
Summary:<br />
This paper presents a novel approach for predicting the Remaining Useful Life (RUL) of lithium-ion batteries, crucial for timely maintenance. The approach utilizes recent charge-discharge cycle data to estimate remaining usable cycles. It includes a unique signal processing pipeline and a deep learning prediction model. The pipeline computes a derived capacity feature and enhances it with statistical metrics and a delta-based method to capture cycle differences. The deep learning model consists of 1D CNN, A-LSTM, and ODE-LSTM blocks, capturing both local and long-range dependencies while modeling battery degradation dynamics. Transfer learning is employed, showing robust performance even with limited target data. Experimental results on large datasets demonstrate the superiority of this method over baseline techniques, with an RMSE of 101.59, indicating its potential for real-world RUL prediction applications. <br /> <div>
arXiv:2505.16664v2 Announce Type: replace 
Abstract: Accurate prediction of the Remaining Useful Life (RUL) is essential for enabling timely maintenance of lithium-ion batteries, impacting the operational efficiency of electric applications that rely on them. This paper proposes a RUL prediction approach that leverages data from recent charge-discharge cycles to estimate the number of remaining usable cycles. The approach introduces both a novel signal processing pipeline and a deep learning prediction model. In the signal preprocessing pipeline, a derived capacity feature $\dot{Q}(I, Q)$ is computed based on current and capacity signals. Alongside original capacity, voltage and current, these features are denoised and enhanced using statistical metrics and a delta-based method to capture differences between the current and previous cycles. In the prediction model, the processed features are then fed into a hybrid deep learning architecture composed of 1D Convolutional Neural Networks (CNN), Attentional Long Short-Term Memory (A-LSTM), and Ordinary Differential Equation-based LSTM (ODE-LSTM) blocks. This architecture is designed to capture both local signal characteristics and long-range temporal dependencies while modeling the continuous-time dynamics of battery degradation. The model is further evaluated using transfer learning across different learning strategies and target data partitioning scenarios. Results indicate that the model maintains robust performance, even when fine-tuned on limited target data. Experimental results on two publicly available large-scale datasets demonstrate that the proposed method outperforms a baseline deep learning approach and machine learning techniques, achieving an RMSE of 101.59, highlighting its strong potential for real-world RUL prediction applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Flexible Forward Trajectories for Masked Molecular Diffusion</title>
<link>https://arxiv.org/abs/2505.16790</link>
<guid>https://arxiv.org/abs/2505.16790</guid>
<content:encoded><![CDATA[
<div> Diffusion models, Molecular generation, Masked Element-wise Learnable Diffusion, State-clashing problem, Chemical validity <br />
Summary: <br />
Masked diffusion models (MDMs) have shown success in modeling discrete data but face challenges in molecular generation. The study introduces Masked Element-wise Learnable Diffusion (MELD) to address the issue of state-clashing, where different molecules collapse into a common state during forward diffusion. MELD uses a noise scheduling network to assign corruption rates to individual graph elements, improving generation quality and chemical validity significantly. Experiments on various molecular benchmarks demonstrate that MELD enhances the performance of MDMs, increasing chemical validity from 15% to 93% on ZINC250K dataset. It also achieves state-of-the-art property alignment in conditional generation tasks. <div>
arXiv:2505.16790v2 Announce Type: replace 
Abstract: Masked diffusion models (MDMs) have achieved notable progress in modeling discrete data, while their potential in molecular generation remains underexplored. In this work, we explore their potential and introduce the surprising result that naively applying standards MDMs severely degrades the performance. We identify the critical cause of this issue as a state-clashing problem-where the forward diffusion of distinct molecules collapse into a common state, resulting in a mixture of reconstruction targets that cannot be learned using typical reverse diffusion process with unimodal predictions. To mitigate this, we propose Masked Element-wise Learnable Diffusion (MELD) that orchestrates per-element corruption trajectories to avoid collision between distinct molecular graphs. This is achieved through a parameterized noise scheduling network that assigns distinct corruption rates to individual graph elements, i.e., atoms and bonds. Extensive experiments on diverse molecular benchmarks reveal that MELD markedly enhances overall generation quality compared to element-agnostic noise scheduling, increasing the chemical validity of vanilla MDMs on ZINC250K from 15% to 93%, Furthermore, it achieves state-of-the-art property alignment in conditional generation tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Risk-Averse Reinforcement Learning with Itakura-Saito Loss</title>
<link>https://arxiv.org/abs/2505.16925</link>
<guid>https://arxiv.org/abs/2505.16925</guid>
<content:encoded><![CDATA[
<div> Reinforcement learning, risk-averse, exponential utility function, Itakura-Saito divergence, state-value, action-value<br />
<br />
Summary: 
Risk-averse reinforcement learning is crucial in high-stakes fields, where agents aim to minimize risk rather than solely maximize returns. The exponential utility function is utilized to frame risk preferences, allowing for the derivation of Bellman equations and the adaptation of reinforcement learning algorithms. This study introduces a novel loss function based on the Itakura-Saito divergence for learning state-value and action-value functions in a numerically stable and mathematically sound manner. The proposed Itakura-Saito loss function is evaluated against established alternatives both theoretically and empirically, demonstrating superior performance in various scenarios, including those with known analytical solutions. This research contributes a valuable tool to the machine learning community for effectively addressing risk-averse reinforcement learning problems. <div>
arXiv:2505.16925v2 Announce Type: replace 
Abstract: Risk-averse reinforcement learning finds application in various high-stakes fields. Unlike classical reinforcement learning, which aims to maximize expected returns, risk-averse agents choose policies that minimize risk, occasionally sacrificing expected value. These preferences can be framed through utility theory. We focus on the specific case of the exponential utility function, where one can derive the Bellman equations and employ various reinforcement learning algorithms with few modifications. To address this, we introduce to the broad machine learning community a numerically stable and mathematically sound loss function based on the Itakura-Saito divergence for learning state-value and action-value functions. We evaluate the Itakura-Saito loss function against established alternatives, both theoretically and empirically. In the experimental section, we explore multiple scenarios, some with known analytical solutions, and show that the considered loss function outperforms the alternatives.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>When Are Concepts Erased From Diffusion Models?</title>
<link>https://arxiv.org/abs/2505.17013</link>
<guid>https://arxiv.org/abs/2505.17013</guid>
<content:encoded><![CDATA[
<div> concept erasure, diffusion models, evaluation framework, adversarial attacks, probing techniques 

Summary:
Concept erasure in diffusion models is a growing area of interest, with two proposed erasure mechanisms identified: reducing the likelihood of generating the target concept and interfering with internal guidance mechanisms. To thoroughly assess erasure, a suite of evaluations including adversarial attacks, probing techniques, and analysis of alternative generations in place of the erased concept is introduced. The results highlight the challenge of balancing side effects and robustness to adversarial prompts in erasure techniques. The study emphasizes the importance of comprehensive evaluation in understanding the effectiveness of erasure in diffusion models. 

Summary: <div>
arXiv:2505.17013v2 Announce Type: replace 
Abstract: Concept erasure, the ability to selectively prevent a model from generating specific concepts, has attracted growing interest, with various approaches emerging to address the challenge. However, it remains unclear how thoroughly these methods erase the target concept. We begin by proposing two conceptual models for the erasure mechanism in diffusion models: (i) reducing the likelihood of generating the target concept, and (ii) interfering with the model's internal guidance mechanisms. To thoroughly assess whether a concept has been truly erased from the model, we introduce a suite of independent evaluations. Our evaluation framework includes adversarial attacks, novel probing techniques, and analysis of the model's alternative generations in place of the erased concept. Our results shed light on the tension between minimizing side effects and maintaining robustness to adversarial prompts. Broadly, our work underlines the importance of comprehensive evaluation for erasure in diffusion models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-Supervised Model-Free Bayesian State Estimation from Compressed Measurements</title>
<link>https://arxiv.org/abs/2407.07368</link>
<guid>https://arxiv.org/abs/2407.07368</guid>
<content:encoded><![CDATA[
<div> Bayesian state estimation, compressed measurements, model-free process, data-driven methods, SemiDANSE <br />
Summary:<br />
The study focuses on data-driven Bayesian state estimation from compressed measurements in a model-free process, addressing an under-determined inverse problem. Traditional model-driven methods are unsuitable due to the unknown underlying dynamical model of the state's evolution. Existing unsupervised learning-based methods, DANSE and DMM, are shown to be inadequate for this task. A new approach, SemiDANSE, is proposed, utilizing semi-supervised learning with a combination of labelled and unlabelled data for regularization. Empirical experiments on benchmark dynamical systems demonstrate that SemiDANSE outperforms other methods for state estimation in the BSCM problem using different measurement systems. It competes well against KalmanNet, extended Kalman filter, and unscented Kalman filter, despite not knowing the exact dynamical models. <div>
arXiv:2407.07368v5 Announce Type: replace-cross 
Abstract: We consider data-driven Bayesian state estimation from compressed measurements (BSCM) of a model-free process. The dimension of the temporal measurement vector is lower than that of the temporal state vector to be estimated, leading to an under-determined inverse problem. The underlying dynamical model of the state's evolution is unknown for a 'model-free process.' Hence, it is difficult to use traditional model-driven methods, for example, Kalman and particle filters. Instead, we consider data-driven methods. We experimentally show that two existing unsupervised learning-based data-driven methods fail to address the BSCM problem in a model-free process. The methods are -- data-driven nonlinear state estimation (DANSE) and deep Markov model (DMM). While DANSE provides good predictive/forecasting performance to model the temporal measurement data as a time series, its unsupervised learning lacks suitable regularization for tackling the BSCM task. We then propose a semi-supervised learning approach and develop a semi-supervised learning-based DANSE method, referred to as SemiDANSE. In SemiDANSE, we use a large amount of unlabelled data along with a limited amount of labelled data, i.e., pairwise measurement-and-state data, which provides the desired regularization. Using three benchmark dynamical systems, we empirically show that the data-driven SemiDANSE provides competitive state estimation performance for BSCM using a handful of different measurement systems, against a hybrid method called KalmanNet and two model-driven methods (extended Kalman filter and unscented Kalman filter) that know the dynamical models exactly.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Review of Pseudo-Labeling for Computer Vision</title>
<link>https://arxiv.org/abs/2408.07221</link>
<guid>https://arxiv.org/abs/2408.07221</guid>
<content:encoded><![CDATA[
<div> semi-supervised learning, deep neural networks, pseudo-labeling, self-supervised learning, unsupervised methods

Summary:
In this work, the authors investigate the concept of pseudo-labeling in the context of deep neural models and explore its application beyond semi-supervised learning. They highlight the importance of leveraging large quantities of unlabeled data to improve the generalization of deep neural networks, identifying pseudo-labeling as a key technique in this domain. The study suggests a broader interpretation of pseudo-labels, extending their use to self-supervised and unsupervised learning methods. By bridging the gap between these areas, the authors propose new avenues for research, such as curriculum learning and self-supervised regularization, which could yield improvements across multiple domains. This holistic approach to pseudo-labeling opens up new possibilities for enhancing the performance and efficiency of deep neural networks in various computer science applications. <br /><br />Summary: <div>
arXiv:2408.07221v3 Announce Type: replace-cross 
Abstract: Deep neural models have achieved state of the art performance on a wide range of problems in computer science, especially in computer vision. However, deep neural networks often require large datasets of labeled samples to generalize effectively, and an important area of active research is semi-supervised learning, which attempts to instead utilize large quantities of (easily acquired) unlabeled samples. One family of methods in this space is pseudo-labeling, a class of algorithms that use model outputs to assign labels to unlabeled samples which are then used as labeled samples during training. Such assigned labels, called pseudo-labels, are most commonly associated with the field of semi-supervised learning. In this work we explore a broader interpretation of pseudo-labels within both self-supervised and unsupervised methods. By drawing the connection between these areas we identify new directions when advancements in one area would likely benefit others, such as curriculum learning and self-supervised regularization.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MAS-ZERO: Designing Multi-Agent Systems with Zero Supervision</title>
<link>https://arxiv.org/abs/2505.14996</link>
<guid>https://arxiv.org/abs/2505.14996</guid>
<content:encoded><![CDATA[
<div> self-evolved, Large Language Models, Multi-agent systems, automatic design, adaptability

Summary: 
Multi-agent systems (MAS) incorporating Large Language Models (LLMs) show promise in tackling complex tasks. However, manual MAS designs may not fully leverage LLM strengths and struggle to adapt to new tasks. Automatic MAS approaches often lack adaptability during inference. MAS-ZERO is introduced as a self-evolved, inference-time framework for automatic MAS design. It uses meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance. MAS-ZERO enables dynamic agent composition and problem decomposition through meta-feedback. Experimental results across various benchmarks demonstrate that MAS-ZERO outperforms manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement while remaining cost-effective. These findings highlight the potential of meta-level self-evolved design for creating effective and adaptive MAS. 

<br /><br />Summary: <div>
arXiv:2505.14996v2 Announce Type: replace-cross 
Abstract: Multi-agent systems (MAS) leveraging the impressive capabilities of Large Language Models (LLMs) hold significant potential for tackling complex tasks. However, most current MAS depend on manually designed agent roles and communication protocols. These manual designs often fail to align with the underlying LLMs' strengths and struggle to adapt to novel tasks. Recent automatic MAS approaches attempt to mitigate these limitations but typically necessitate a validation set for tuning and yield static MAS designs lacking adaptability during inference. We introduce MAS-ZERO, the first self-evolved, inference-time framework for automatic MAS design. MAS-ZERO employs meta-level design to iteratively generate, evaluate, and refine MAS configurations tailored to each problem instance, without requiring a validation set. Critically, it enables dynamic agent composition and problem decomposition through meta-feedback on solvability and completeness. Experiments across math, graduate-level QA, and software engineering benchmarks, using both closed-source and open-source LLM backbones of varying sizes, demonstrate that MAS-ZERO outperforms both manual and automatic MAS baselines, achieving a 7.44% average accuracy improvement over the next strongest baseline while maintaining cost-efficiency. These findings underscore the promise of meta-level self-evolved design for creating effective and adaptive MAS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AnchorFormer: Differentiable Anchor Attention for Efficient Vision Transformer</title>
<link>https://arxiv.org/abs/2505.16463</link>
<guid>https://arxiv.org/abs/2505.16463</guid>
<content:encoded><![CDATA[
<div> vision transformers, ViTs, AnchorFormer, bipartite attention, downstream tasks<br />
<br />
Summary: 
The article introduces AnchorFormer, an efficient vision transformer that utilizes anchor tokens to capture pivotal information and speed up inference. By estimating bipartite attention between anchors and tokens, the complexity is reduced from O(n^2) to O(mn), where m is the number of anchors. Anchors represented by neurons allow for differentiable learning of distributions and approximation of global self-attention through a Markov process. The model is extended to classification, detection, and segmentation tasks, showing superior performance compared to existing baselines. For instance, on ImageNet classification, AnchorFormer achieves up to 9.0% higher accuracy or 46.7% FLOPs reduction. On COCO detection, it achieves 81.3% higher mAP under comparable FLOPs. Overall, AnchorFormer proves to be effective in improving performance and efficiency across various vision tasks. <br /><br /> <div>
arXiv:2505.16463v2 Announce Type: replace-cross 
Abstract: Recently, vision transformers (ViTs) have achieved excellent performance on vision tasks by measuring the global self-attention among the image patches. Given $n$ patches, they will have quadratic complexity such as $\mathcal{O}(n^2)$ and the time cost is high when splitting the input image with a small granularity. Meanwhile, the pivotal information is often randomly gathered in a few regions of an input image, some tokens may not be helpful for the downstream tasks. To handle this problem, we introduce an anchor-based efficient vision transformer (AnchorFormer), which employs the anchor tokens to learn the pivotal information and accelerate the inference. Firstly, by estimating the bipartite attention between the anchors and tokens, the complexity will be reduced from $\mathcal{O}(n^2)$ to $\mathcal{O}(mn)$, where $m$ is an anchor number and $m < n$. Notably, by representing the anchors with the neurons in a neural layer, we can differentiable learn these distributions and approximate global self-attention through the Markov process. Moreover, we extend the proposed model to three downstream tasks including classification, detection, and segmentation. Extensive experiments show the effectiveness of our AnchorFormer, e.g., achieving up to a 9.0% higher accuracy or 46.7% FLOPs reduction on ImageNet classification, 81.3% higher mAP on COCO detection under comparable FLOPs, as compared to the current baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Supported Dynamic Algorithm Configuration for Multi-Objective Combinatorial Optimization</title>
<link>https://arxiv.org/abs/2505.16471</link>
<guid>https://arxiv.org/abs/2505.16471</guid>
<content:encoded><![CDATA[
<div> Keywords: Deep reinforcement learning, Graph neural network, Multi-objective combinatorial optimization, Evolutionary algorithms, Algorithm configuration

Summary:
Deep reinforcement learning (DRL) is applied to dynamic algorithm configuration for multi-objective combinatorial optimization (MOCO) problems using a graph neural network (GNN). The method models algorithm configuration as a Markov decision process, with a graph representing solution convergence in the objective space. GNN is utilized to learn embeddings and enhance state representation. Experimental results show superior performance compared to traditional and DRL-based methods in efficacy and adaptability. The approach demonstrates generalizability across different objective types and problem sizes, and is applicable to various evolutionary computation methods. The GNN-based DRL algorithm configuration outperforms existing methods in MOCO challenges, showcasing its effectiveness in optimizing multi-objective problems using evolutionary algorithms. <br /><br />Summary: <div>
arXiv:2505.16471v2 Announce Type: replace-cross 
Abstract: Deep reinforcement learning (DRL) has been widely used for dynamic algorithm configuration, particularly in evolutionary computation, which benefits from the adaptive update of parameters during the algorithmic execution. However, applying DRL to algorithm configuration for multi-objective combinatorial optimization (MOCO) problems remains relatively unexplored. This paper presents a novel graph neural network (GNN) based DRL to configure multi-objective evolutionary algorithms. We model the dynamic algorithm configuration as a Markov decision process, representing the convergence of solutions in the objective space by a graph, with their embeddings learned by a GNN to enhance the state representation. Experiments on diverse MOCO challenges indicate that our method outperforms traditional and DRL-based algorithm configuration methods in terms of efficacy and adaptability. It also exhibits advantageous generalizability across objective types and problem sizes, and applicability to different evolutionary computation methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</title>
<link>https://arxiv.org/abs/2505.16821</link>
<guid>https://arxiv.org/abs/2505.16821</guid>
<content:encoded><![CDATA[
<div> transformer model, Radio Resource Control, control-plane procedures, LAMs, AI-native

Summary:
This paper presents a demonstration of a Large AI Model (LAM) that generates standards-compliant Radio Resource Control (RRC) messages in a 6G mobile network. By treating RRC messaging as a domain-specific language, a decoder-only transformer model (LLaMA class) is fine-tuned using Low-Rank Adaptation (LoRA) on linearized RRC messages to maintain their ASN.1 structure. The model achieves a high cosine similarity with ground-truth messages, indicating improved structural and semantic fidelity. The approach shows that LAMs, augmented with Radio Access Network (RAN)-specific reasoning, can directly manage control-plane procedures, paving the way for AI-native wireless standards. This work represents a significant step towards autonomous, cognitive network operations and intelligent protocol design in future mobile networks. <div>
arXiv:2505.16821v2 Announce Type: replace-cross 
Abstract: Integrating large AI models (LAMs) into 6G mobile networks promises to redefine protocol design and control-plane intelligence by enabling autonomous, cognitive network operations. While industry concepts, such as ETSI's Experiential Networked Intelligence (ENI), envision LAM-driven agents for adaptive network slicing and intent-based management, practical implementations still face challenges in protocol literacy and real-world deployment. This paper presents an end-to-end demonstration of a LAM that generates standards-compliant, ASN.1-encoded Radio Resource Control (RRC) messages as part of control-plane procedures inside a gNB. We treat RRC messaging as a domain-specific language and fine-tune a decoder-only transformer model (LLaMA class) using parameter-efficient Low-Rank Adaptation (LoRA) on RRC messages linearized to retain their ASN.1 syntactic structure before standard byte-pair encoding tokenization. This enables combinatorial generalization over RRC protocol states while minimizing training overhead. On 30k field-test request-response pairs, our 8 B model achieves a median cosine similarity of 0.97 with ground-truth messages on an edge GPU -- a 61 % relative gain over a zero-shot LLaMA-3 8B baseline -- indicating substantially improved structural and semantic RRC fidelity. Overall, our results show that LAMs, when augmented with Radio Access Network (RAN)-specific reasoning, can directly orchestrate control-plane procedures, representing a stepping stone toward the AI-native air-interface paradigm. Beyond RRC emulation, this work lays the groundwork for future AI-native wireless standards.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark</title>
<link>https://arxiv.org/abs/2505.16968</link>
<guid>https://arxiv.org/abs/2505.16968</guid>
<content:encoded><![CDATA[
<div> CUDA, HIP, Nvidia SASS, AMD RDNA3, GPU code transpilation 
<br />
Summary: 
CASS is introduced as a dataset and model suite for cross-architecture GPU code transpilation, covering source-level (CUDA $\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD RDNA3) translation. The dataset consists of 70k verified code pairs and trains domain-specific language models in the CASS family. The models achieve high accuracy in source and assembly translation, surpassing commercial baselines like GPT-4o and others. Code generated by the models matches native performance in the majority of test cases, preserving runtime and memory behavior. The CASS-Bench benchmark, spanning 16 GPU domains with ground-truth execution, is introduced for rigorous evaluation. All data, models, and evaluation tools are open source, promoting advancements in GPU compiler tooling, binary compatibility, and hardware translation guided by language models. The dataset and benchmark are available on HuggingFace, with the code repository on GitHub. <div>
arXiv:2505.16968v2 Announce Type: replace-cross 
Abstract: We introduce CASS, the first large-scale dataset and model suite for cross-architecture GPU code transpilation, targeting both source-level (CUDA $\leftrightarrow$ HIP) and assembly-level (Nvidia SASS $\leftrightarrow$ AMD RDNA3) translation. The dataset comprises 70k verified code pairs across host and device, addressing a critical gap in low-level GPU code portability. Leveraging this resource, we train the CASS family of domain-specific language models, achieving 95% source translation accuracy and 37.5% assembly translation accuracy, substantially outperforming commercial baselines such as GPT-4o, Claude, and Hipify. Our generated code matches native performance in over 85% of test cases, preserving runtime and memory behavior. To support rigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16 GPU domains with ground-truth execution. All data, models, and evaluation tools are released as open source to foster progress in GPU compiler tooling, binary compatibility, and LLM-guided hardware translation. Dataset and benchmark are on \href{https://huggingface.co/datasets/MBZUAI/cass}{\textcolor{blue}{HuggingFace}}, with code at \href{https://github.com/GustavoStahl/CASS}{\textcolor{blue}{GitHub}}.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Discovering Forbidden Topics in Language Models</title>
<link>https://arxiv.org/abs/2505.17441</link>
<guid>https://arxiv.org/abs/2505.17441</guid>
<content:encoded><![CDATA[
<div> discovery, refusal, language model, bias, AI<br />
<br />
Summary: 
The study introduces the concept of refusal discovery in language models, which aims to identify topics that the model refuses to discuss. A new method called LLM-crawler is developed using token prefilling to uncover forbidden topics. The method is tested on the Tulu-3-8B model, successfully retrieving a majority of the topics within a limited prompt budget. Scaling the crawl to a frontier model with the help of Claude-Haiku shows revealing patterns of censorship tuning in the DeepSeek-R1-70B model, indicating memorization of CCP-aligned responses. While the Perplexity-R1-1776-70B model exhibits robustness to censorship, the LLM-crawler still elicits CCP-aligned refusal answers in the quantized version. These findings underscore the importance of implementing refusal discovery methods to identify biases, limitations, and alignment failures in AI systems. <div>
arXiv:2505.17441v2 Announce Type: replace-cross 
Abstract: Refusal discovery is the task of identifying the full set of topics that a language model refuses to discuss. We introduce this new problem setting and develop a refusal discovery method, LLM-crawler, that uses token prefilling to find forbidden topics. We benchmark the LLM-crawler on Tulu-3-8B, an open-source model with public safety tuning data. Our crawler manages to retrieve 31 out of 36 topics within a budget of 1000 prompts. Next, we scale the crawl to a frontier model using the prefilling option of Claude-Haiku. Finally, we crawl three widely used open-weight models: Llama-3.3-70B and two of its variants finetuned for reasoning: DeepSeek-R1-70B and Perplexity-R1-1776-70B. DeepSeek-R1-70B reveals patterns consistent with censorship tuning: The model exhibits "thought suppression" behavior that indicates memorization of CCP-aligned responses. Although Perplexity-R1-1776-70B is robust to censorship, LLM-crawler elicits CCP-aligned refusals answers in the quantized model. Our findings highlight the critical need for refusal discovery methods to detect biases, boundaries, and alignment failures of AI systems.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Model-Distributed Inference for Large Language Models at the Edge</title>
<link>https://arxiv.org/abs/2505.18164</link>
<guid>https://arxiv.org/abs/2505.18164</guid>
<content:encoded><![CDATA[
<div> Keywords: Model-Distributed Inference, Large-Language Models, Edge devices, Recurrent pipeline parallelism, Collaborative computation

Summary:
Model-Distributed Inference for Large-Language Models (MDI-LLM) is a framework designed to deploy state-of-the-art large-language models on low-power edge devices. The model is split into partitions assigned to different nodes, enabling collaborative computation through intermediate vector exchange. The "recurrent pipeline parallelism" technique reduces idle time on each device, enhancing efficiency during text sequence generation. By combining computational resources of multiple devices, MDI-LLM allows deployment of models exceeding individual device memory capacity, enabling inference on inexpensive hardware. With more devices, token generation throughput increases and memory consumption per device decreases. This approach optimizes model efficiency and performance on edge devices, making it practical for real-world applications. <br /><br />Summary: <div>
arXiv:2505.18164v1 Announce Type: new 
Abstract: We introduce Model-Distributed Inference for Large-Language Models (MDI-LLM), a novel framework designed to facilitate the deployment of state-of-the-art large-language models (LLMs) across low-power devices at the edge. This is accomplished by dividing the model into multiple partitions, which are then assigned to different devices/nodes within the network. These nodes exchange intermediate activation vectors via device-to-device links, enabling collaborative computation. To enhance the efficiency of this process, we propose the "recurrent pipeline parallelism" technique, which reduces idle time on each device and facilitates parallel inference during the generation of multiple text sequences. By leveraging the combined computational resources of multiple edge devices, MDI-LLM enables the deployment of LLMs that exceed the memory capacity of individual devices, making it possible to perform inference on low-cost hardware. Furthermore, as the number of participating devices increases, MDI-LLM boosts token generation throughput and reduces memory consumption per device.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Constrained Edge AI Deployment: Fine-Tuning vs Distillation for LLM Compression</title>
<link>https://arxiv.org/abs/2505.18166</link>
<guid>https://arxiv.org/abs/2505.18166</guid>
<content:encoded><![CDATA[
<div> pruning, re-training, compression, MLP, loss function  
Summary:
The study focuses on compressing foundational models for edge deployments using layer-wise L2-norm pruning on MLP blocks. The impact of re-training loss function is analyzed, comparing Fine-tuning with Cross-Entropy (L2PFT) and Self-Distillation with KL-divergence (L2PSD). Evaluation on the OLMo2-7B-SFT model for CommonsenseQA in intermittent connectivity scenarios shows that KL-based distillation performs as well as or better than CE fine-tuning in test accuracy. The results highlight the significant influence of the choice of loss function on compressed model recovery in resource-constrained environments. <br /><br />Summary: <div>
arXiv:2505.18166v1 Announce Type: new 
Abstract: Modern foundational models are often compressed via a combination of structured pruning and re-training to meet the strict compute, memory, and connectivity constraints of edge deployments. While state-of-the-art pruning schemes target the entire Transformer, we adopt a simple, layer-wise L2-norm pruning on only the MLP blocks as a fixed baseline. Our focus is not on achieving maximal compression, but on isolating the impact of the re-training loss function: (i) Fine-tuning with Cross- Entropy (L2PFT), which requires labeled data, versus (ii) Self-Distillation with KL-divergence, which leverages only teacher logits (no labels) (L2PSD). We evaluate both pipelines on the OLMo2- 7B-SFT model for CommonsenseQA suitable for intermittent or denied connectivity scenarios typical of edge networks. Under identical pruning schedules, KL-based distillation matches or exceeds CE fine-tuning in test accuracy, demonstrating that, even with a basic MLP-only pruning, the choice of loss function materially affects compressed model recovery in resource-constrained environments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Emotion Knowledge Enhancement for Vision Large Language Models: A Self-Verification Approach for High-Quality Emotion Instruction Data Generation</title>
<link>https://arxiv.org/abs/2505.18168</link>
<guid>https://arxiv.org/abs/2505.18168</guid>
<content:encoded><![CDATA[
<div> Keywords: facial emotion perception, vision large language model, self-verification approach, emotion knowledge enhancement, facial emotion analysis

Summary:
Facial emotion perception in the vision large language model (VLLM) is essential for natural human-machine interaction. High-quality annotations for facial emotion analysis are costly to create, hindering VLLM performance. The self-verification approach with emotion knowledge enhancement (SEKE) generates high-quality instruction data cost-effectively using a closed-source VLLM. By integrating prior human knowledge and a self-verification strategy, comprehensive annotations are generated for multi-grained emotion analysis. The approach enhances annotation reliability by extracting more accurate VLLM predictions. The facial emotion instruction dataset (FEID) and facial emotion analysis benchmark (FEAB) are introduced for effective model training and performance evaluation. SEKE outperforms state-of-the-art methods on three downstream facial emotion analysis tasks. The method significantly improves VLLM's ability in facial emotion perception. <br /><br />Summary: <div>
arXiv:2505.18168v1 Announce Type: new 
Abstract: Facial emotion perception in the vision large language model (VLLM) is crucial for achieving natural human-machine interaction. However, creating high-quality annotations for both coarse- and fine-grained facial emotion analysis demands costly expertise. The lack of such high-quality instruction data limits the performance of VLLMs in facial emotion perception. To address this, we propose a self-verification approach with emotion knowledge enhancement (SEKE), which generates high-quality instruction data for multi-grained emotion analysis cost-effectively using closed-source VLLM. This approach integrates prior human knowledge to VLLM inference, guided by the inherent correlations between three grained levels of emotion descriptions, i.e., discrete expression, valence-arousal, and action unit, to reliably generate comprehensive annotations. A self-verification strategy with Uncertainty-Aware Monte Carlo sampling (SV-UAMC) is further embedded to efficiently extract more accurate VLLM predictions, further improving annotation reliability. Consequently, we construct a facial emotion instruction dataset (FEID) containing three comprehensive descriptions, which provides coarse- and fine-grained emotional information for effective model training. Additionally, we introduce a facial emotion analysis benchmark (FEAB) to measure the VLLM's corresponding ability. Our method significantly outperforms state-of-the-art methods on three downstream facial emotion analysis tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Multi-Task PINN for Emotion Recognition and EDA Prediction</title>
<link>https://arxiv.org/abs/2505.18169</link>
<guid>https://arxiv.org/abs/2505.18169</guid>
<content:encoded><![CDATA[
<div> wearable sensors, emotion recognition, multi-task Physics-Informed Neural Network, electrodermal activity, interpretability 

Summary:
This study introduces a novel Multi-Task Physics-Informed Neural Network (PINN) for predicting Electrodermal Activity (EDA) and classifying emotions simultaneously using the WESAD dataset. The model incorporates psychological self-report features and a physics-inspired differential equation to enforce biophysically grounded constraints. With dual outputs for both tasks, the model achieves superior performance in EDA prediction and emotion classification compared to traditional models. The model's interpretability is enhanced through the inclusion of physical parameters such as decay rate, emotional sensitivity, and time scaling, aligning with human physiology principles. This work sets a foundation for future applications in healthcare and human-computer interaction by offering improved performance, generalizability, and model transparency. <div>
arXiv:2505.18169v1 Announce Type: new 
Abstract: Understanding and predicting human emotional and physiological states using wearable sensors has important applications in stress monitoring, mental health assessment, and affective computing. This study presents a novel Multi-Task Physics-Informed Neural Network (PINN) that performs Electrodermal Activity (EDA) prediction and emotion classification simultaneously, using the publicly available WESAD dataset. The model integrates psychological self-report features (PANAS and SAM) with a physics-inspired differential equation representing EDA dynamics, enforcing biophysically grounded constraints through a custom loss function. This loss combines EDA regression, emotion classification, and a physics residual term for improved interpretability.
  The architecture supports dual outputs for both tasks and is trained under a unified multi-task framework. Evaluated using 5-fold cross-validation, the model achieves an average EDA RMSE of 0.0362, Pearson correlation of 0.9919, and F1-score of 94.08 percent. These results outperform classical models such as SVR and XGBoost, as well as ablated variants like emotion-only and EDA-only models.
  In addition, the learned physical parameters including decay rate (alpha_0), emotional sensitivity (beta), and time scaling (gamma) are interpretable and stable across folds, aligning with known principles of human physiology. This work is the first to introduce a multi-task PINN framework for wearable emotion recognition, offering improved performance, generalizability, and model transparency. The proposed system provides a foundation for future interpretable and multimodal applications in healthcare and human-computer interaction.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Robust Knowledge Graph Embedding via Denoising</title>
<link>https://arxiv.org/abs/2505.18171</link>
<guid>https://arxiv.org/abs/2505.18171</guid>
<content:encoded><![CDATA[
<div> framework, robustness, knowledge graph embedding, denoising, perturbation
<br />
Summary: <br />
The article introduces a novel framework called Robust Knowledge Graph Embedding via Denoising, focusing on enhancing the robustness of knowledge graph embedding (KGE) models in the presence of noisy triples. By treating KGE methods as energy-based models and utilizing the connection between denoising and score matching, the framework allows for the training of robust denoising KGE models. The authors also propose certified robustness evaluation metrics for KGE methods based on randomized smoothing. Experimental results on benchmark datasets demonstrate that the framework consistently outperforms existing state-of-the-art KGE methods when dealing with perturbed entity embedding. <div>
arXiv:2505.18171v1 Announce Type: new 
Abstract: We focus on obtaining robust knowledge graph embedding under perturbation in the embedding space. To address these challenges, we introduce a novel framework, Robust Knowledge Graph Embedding via Denoising, which enhances the robustness of KGE models on noisy triples. By treating KGE methods as energy-based models, we leverage the established connection between denoising and score matching, enabling the training of a robust denoising KGE model. Furthermore, we propose certified robustness evaluation metrics for KGE methods based on the concept of randomized smoothing. Through comprehensive experiments on benchmark datasets, our framework consistently shows superior performance compared to existing state-of-the-art KGE methods when faced with perturbed entity embedding.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Should We Simultaneously Calibrate Multiple Computer Models?</title>
<link>https://arxiv.org/abs/2505.18176</link>
<guid>https://arxiv.org/abs/2505.18176</guid>
<content:encoded><![CDATA[
<div> neural networks, computer models, calibration, multi-response, probabilistic framework

Summary: 
This paper explores the simultaneous calibration of multiple computer models using a probabilistic framework based on customized neural networks. By considering the multi-response nature of most computer models and the variability in calibration parameters across models, the approach aims to learn unique probability distributions for each parameter. A loss function is developed to enable the neural networks to emulate all data sources while calibrating the models. Additionally, the method aims to learn a visualizable latent space to identify model-form errors. Testing on analytic and engineering problems shows that the approach can improve predictive accuracy, although it may face challenges with non-identifiability in higher-dimensional input spaces constrained by underlying physics. <div>
arXiv:2505.18176v1 Announce Type: new 
Abstract: In an increasing number of applications designers have access to multiple computer models which typically have different levels of fidelity and cost. Traditionally, designers calibrate these models one at a time against some high-fidelity data (e.g., experiments). In this paper, we question this tradition and assess the potential of calibrating multiple computer models at the same time. To this end, we develop a probabilistic framework that is founded on customized neural networks (NNs) that are designed to calibrate an arbitrary number of computer models. In our approach, we (1) consider the fact that most computer models are multi-response and that the number and nature of calibration parameters may change across the models, and (2) learn a unique probability distribution for each calibration parameter of each computer model, (3) develop a loss function that enables our NN to emulate all data sources while calibrating the computer models, and (4) aim to learn a visualizable latent space where model-form errors can be identified. We test the performance of our approach on analytic and engineering problems to understand the potential advantages and pitfalls in simultaneous calibration of multiple computer models. Our method can improve predictive accuracy, however, it is prone to non-identifiability issues in higher-dimensional input spaces that are normally constrained by underlying physics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedGRec: Dynamic Spatio-Temporal Federated Graph Learning for Secure and Efficient Cross-Border Recommendations</title>
<link>https://arxiv.org/abs/2505.18177</link>
<guid>https://arxiv.org/abs/2505.18177</guid>
<content:encoded><![CDATA[
<div> federated learning, graph neural networks, cross-border recommendations, privacy preservation, collaborative training<br />
<br />
Summary:<br />
FedGRec is a privacy-preserving federated graph learning method designed for cross-border recommendations. The method utilizes collaborative signals from local subgraphs to improve representation learning and dynamically integrates global and local user preferences in real time. By filtering out irrelevant behaviors and employing personalized federated aggregation, FedGRec adapts global preferences to heterogeneous domain data, enhancing collaborative learning across multiple domains. Experimental results on three datasets show that FedGRec outperforms single-domain and cross-domain baselines while preserving data privacy effectively. <div>
arXiv:2505.18177v1 Announce Type: new 
Abstract: Due to the highly sensitive nature of certain data in cross-border sharing, collaborative cross-border recommendations and data sharing are often subject to stringent privacy protection regulations, resulting in insufficient data for model training. Consequently, achieving efficient cross-border business recommendations while ensuring privacy security poses a significant challenge. Although federated learning has demonstrated broad potential in collaborative training without exposing raw data, most existing federated learning-based GNN training methods still rely on federated averaging strategies, which perform suboptimally on highly heterogeneous graph data. To address this issue, we propose FedGRec, a privacy-preserving federated graph learning method for cross-border recommendations. FedGRec captures user preferences from distributed multi-domain data to enhance recommendation performance across all domains without privacy leakage. Specifically, FedGRec leverages collaborative signals from local subgraphs associated with users or items to enrich their representation learning. Additionally, it employs dynamic spatiotemporal modeling to integrate global and local user preferences in real time based on business recommendation states, thereby deriving the final representations of target users and candidate items. By automatically filtering relevant behaviors, FedGRec effectively mitigates noise interference from unreliable neighbors. Furthermore, through a personalized federated aggregation strategy, FedGRec adapts global preferences to heterogeneous domain data, enabling collaborative learning of user preferences across multiple domains. Extensive experiments on three datasets demonstrate that FedGRec consistently outperforms competitive single-domain and cross-domain baselines while effectively preserving data privacy in cross-border recommendations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Less is More: Multimodal Region Representation via Pairwise Inter-view Learning</title>
<link>https://arxiv.org/abs/2505.18178</link>
<guid>https://arxiv.org/abs/2505.18178</guid>
<content:encoded><![CDATA[
<div> Keywords: geospatial datasets, region representation learning, contrastive learning, information factorization, multimodal data <br />
Summary:
The article introduces a new approach, Cross modal Knowledge Injected Embedding (CooKIE), for region representation learning (RRL) that incorporates both shared and unique information from multiple modalities. CooKIE utilizes pairwise inter-view learning to capture high-order information without complexity. The method is evaluated on regression tasks and land use classification in New York City and Delhi, outperforming existing RRL methods and a factorized RRL model. It achieves this with fewer training parameters and FLOPs. By releasing the code, CooKIE provides a valuable resource for researchers working with geospatial datasets. The approach addresses the limitation of existing RRL methods by effectively capturing both shared and modality-specific information, leading to improved performance in analyzing complex region characteristics. <br /><br />Summary: <div>
arXiv:2505.18178v1 Announce Type: new 
Abstract: With the increasing availability of geospatial datasets, researchers have explored region representation learning (RRL) to analyze complex region characteristics. Recent RRL methods use contrastive learning (CL) to capture shared information between two modalities but often overlook task-relevant unique information specific to each modality. Such modality-specific details can explain region characteristics that shared information alone cannot capture. Bringing information factorization to RRL can address this by factorizing multimodal data into shared and unique information. However, existing factorization approaches focus on two modalities, whereas RRL can benefit from various geospatial data. Extending factorization beyond two modalities is non-trivial because modeling high-order relationships introduces a combinatorial number of learning objectives, increasing model complexity. We introduce Cross modal Knowledge Injected Embedding, an information factorization approach for RRL that captures both shared and unique representations. CooKIE uses a pairwise inter-view learning approach that captures high-order information without modeling high-order dependency, avoiding exhaustive combinations. We evaluate CooKIE on three regression tasks and a land use classification task in New York City and Delhi, India. Results show that CooKIE outperforms existing RRL methods and a factorized RRL model, capturing multimodal information with fewer training parameters and floating-point operations per second (FLOPs). We release the code: https://github.com/MinNamgung/CooKIE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GAIA: A Foundation Model for Operational Atmospheric Dynamics</title>
<link>https://arxiv.org/abs/2505.18179</link>
<guid>https://arxiv.org/abs/2505.18179</guid>
<content:encoded><![CDATA[
<div> masked autoencoders, self-Distillation, satellite imagery, precipitation estimation, self-supervised learning<br />
<br />
Summary:
The GAIA Foundation Model combines masked autoencoders and self-distillation with DINO for analyzing global atmospheric patterns in satellite imagery. It addresses the challenges of reconstructing missing regions and estimating precipitation patterns. The model captures local features and global dependencies simultaneously, showing superior temporal pattern capture compared to standard MAE approaches. It demonstrates robust performance in downstream tasks, with strong gap-filling capabilities across varying mask ratios and accurate precipitation estimation with limited training data. The model achieves a false alarm ratio of 0.088 and a structural similarity of 0.881. This work represents an advancement in self-supervised learning for atmospheric science, providing a foundation for improved weather monitoring and climate analysis. The trained model weights and code are openly available on Hugging Face for further research and applications. <br /><br />Summary: <div>
arXiv:2505.18179v1 Announce Type: new 
Abstract: We present the GAIA (Geospatial Artificial Intelligence for Atmospheres) Foundation Model, a novel model that combines masked autoencoders (MAE) and self-DIstillation with NO labels (DINO) for analyzing global atmospheric patterns in satellite imagery. By integrating these complementary self-supervised learning approaches, our model simultaneously captures both local features and global dependencies. We address two critical challenges in satellite data analysis: reconstructing missing regions and estimating precipitation patterns as our first downstream tasks. The model demonstrates superior temporal pattern capture compared to standard MAE approaches, while maintaining robust performance in downstream tasks. Our experimental results show strong gap-filling capabilities across varying mask ratios and accurate precipitation estimation with limited training data, achieving a false alarm ratio of 0.088 and structural similarity of 0.881. This work represents an advancement in self-supervised learning for atmospheric science, providing a foundation for improved weather monitoring and climate analysis. The trained model weights and accompanying code are publicly available as open-source on Hugging Face here: https://huggingface.co/bcg-usra-nasa-gaia/GAIA-v1.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>2DNMRGym: An Annotated Experimental Dataset for Atom-Level Molecular Representation Learning in 2D NMR via Surrogate Supervision</title>
<link>https://arxiv.org/abs/2505.18181</link>
<guid>https://arxiv.org/abs/2505.18181</guid>
<content:encoded><![CDATA[
<div> dataset, machine learning, 2D NMR, molecular representation, HSQC spectroscopy

Summary:<br />
The article introduces 2DNMRGym, a dataset created for machine learning-based molecular representation learning in 2D NMR analysis. With over 22,000 HSQC spectra, molecular graphs, and SMILES strings, this dataset addresses the limited availability of annotated datasets in the field. Using a surrogate supervision setup, models are trained with algorithm-generated annotations and evaluated with expert-annotated gold-standard labels. Various 2D and 3D graph neural network (GNN) and GNN transformer models are benchmarked on this dataset, showing promising results for future ML applications in 2D NMR analysis. The open-source data and code are accessible on Huggingface and Github, supporting scalable model training and providing a chemically significant benchmark for assessing atom-level molecular representations in NMR-guided structural tasks. <div>
arXiv:2505.18181v1 Announce Type: new 
Abstract: Two-dimensional (2D) Nuclear Magnetic Resonance (NMR) spectroscopy, particularly Heteronuclear Single Quantum Coherence (HSQC) spectroscopy, plays a critical role in elucidating molecular structures, interactions, and electronic properties. However, accurately interpreting 2D NMR data remains labor-intensive and error-prone, requiring highly trained domain experts, especially for complex molecules. Machine Learning (ML) holds significant potential in 2D NMR analysis by learning molecular representations and recognizing complex patterns from data. However, progress has been limited by the lack of large-scale and high-quality annotated datasets. In this work, we introduce 2DNMRGym, the first annotated experimental dataset designed for ML-based molecular representation learning in 2D NMR. It includes over 22,000 HSQC spectra, along with the corresponding molecular graphs and SMILES strings. Uniquely, 2DNMRGym adopts a surrogate supervision setup: models are trained using algorithm-generated annotations derived from a previously validated method and evaluated on a held-out set of human-annotated gold-standard labels. This enables rigorous assessment of a model's ability to generalize from imperfect supervision to expert-level interpretation. We provide benchmark results using a series of 2D and 3D GNN and GNN transformer models, establishing a strong foundation for future work. 2DNMRGym supports scalable model training and introduces a chemically meaningful benchmark for evaluating atom-level molecular representations in NMR-guided structural tasks. Our data and code is open-source and available on Huggingface and Github.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Riemannian Flow Matching for Brain Connectivity Matrices via Pullback Geometry</title>
<link>https://arxiv.org/abs/2505.18193</link>
<guid>https://arxiv.org/abs/2505.18193</guid>
<content:encoded><![CDATA[
<div> Riemannian manifolds, brain connectivity matrices, conditional flow matching, DiffeoCFM, fMRI datasets<br />
<br />
Summary: <br />
Generating realistic brain connectivity matrices is crucial for analyzing population heterogeneity and understanding diseases. Functional connectivity matrices are constrained to specific spaces that can be modeled as Riemannian manifolds. The proposed DiffeoCFM approach allows for conditional flow matching on matrix manifolds efficiently by utilizing pullback metrics induced by global diffeomorphisms on Euclidean spaces. This enables fast vector field learning and sampling with standard ODE solvers. DiffeoCFM, applied to covariance and correlation matrices, was evaluated on large-scale fMRI and EEG datasets, achieving state-of-the-art performance while preserving manifold constraints. <div>
arXiv:2505.18193v1 Announce Type: new 
Abstract: Generating realistic brain connectivity matrices is key to analyzing population heterogeneity in brain organization, understanding disease, and augmenting data in challenging classification problems. Functional connectivity matrices lie in constrained spaces--such as the set of symmetric positive definite or correlation matrices--that can be modeled as Riemannian manifolds. However, using Riemannian tools typically requires redefining core operations (geodesics, norms, integration), making generative modeling computationally inefficient. In this work, we propose DiffeoCFM, an approach that enables conditional flow matching (CFM) on matrix manifolds by exploiting pullback metrics induced by global diffeomorphisms on Euclidean spaces. We show that Riemannian CFM with such metrics is equivalent to applying standard CFM after data transformation. This equivalence allows efficient vector field learning, and fast sampling with standard ODE solvers. We instantiate DiffeoCFM with two different settings: the matrix logarithm for covariance matrices and the normalized Cholesky decomposition for correlation matrices. We evaluate DiffeoCFM on three large-scale fMRI datasets with more than 4600 scans from 2800 subjects (ADNI, ABIDE, OASIS-3) and two EEG motor imagery datasets with over 30000 trials from 26 subjects (BNCI2014-002 and BNCI2015-001). It enables fast training and achieves state-of-the-art performance, all while preserving manifold constraints.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Evidence-Grounded Multimodal Misinformation Detection with Attention-Based GNNs</title>
<link>https://arxiv.org/abs/2505.18221</link>
<guid>https://arxiv.org/abs/2505.18221</guid>
<content:encoded><![CDATA[
<div> Graph-Based Method, Multimodal Misinformation, Contextualization, Misinformation Detection, Graph Neural Networks

Summary:
The article introduces a novel approach for detecting multi-modal out-of-context (OOC) misinformation, which involves misleading captions paired with real images. Current methods like LLMs and LVLMs struggle with contextualizing the claims before identifying misinformation. The proposed graph-based method utilizes two graph representations - an evidence graph derived from online textual evidence and a claim graph from the caption - to evaluate the consistency between the image and the caption. By employing graph neural networks (GNNs) to encode and compare these representations, the framework can determine the truthfulness of image-caption pairs. The method achieves a detection accuracy of 93.05% on the evaluation set, outperforming popular LLMs by 2.82%. This highlights the effectiveness of smaller and task-specific methods in addressing misinformation detection challenges. 

<br /><br />Summary: <div>
arXiv:2505.18221v1 Announce Type: new 
Abstract: Multimodal out-of-context (OOC) misinformation is misinformation that repurposes real images with unrelated or misleading captions. Detecting such misinformation is challenging because it requires resolving the context of the claim before checking for misinformation. Many current methods, including LLMs and LVLMs, do not perform this contextualization step. LLMs hallucinate in absence of context or parametric knowledge. In this work, we propose a graph-based method that evaluates the consistency between the image and the caption by constructing two graph representations: an evidence graph, derived from online textual evidence, and a claim graph, from the claim in the caption. Using graph neural networks (GNNs) to encode and compare these representations, our framework then evaluates the truthfulness of image-caption pairs. We create datasets for our graph-based method, evaluate and compare our baseline model against popular LLMs on the misinformation detection task. Our method scores $93.05\%$ detection accuracy on the evaluation set and outperforms the second-best performing method (an LLM) by $2.82\%$, making a case for smaller and task-specific methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Token Reduction Should Go Beyond Efficiency in Generative Models -- From Vision, Language to Multimodality</title>
<link>https://arxiv.org/abs/2505.18227</link>
<guid>https://arxiv.org/abs/2505.18227</guid>
<content:encoded><![CDATA[
<div> efficiency, token reduction, generative modeling, multimodal integration, training stability

Summary:<br />
Token reduction in Transformer architectures plays a crucial role beyond just efficiency, impacting model architecture and applications in various domains. It enables deeper multimodal integration and alignment, mitigates "overthinking" and hallucinations, ensures coherence over long inputs, and enhances training stability. By reframing token reduction as a fundamental principle in generative modeling, new directions such as algorithm design, reinforcement learning-guided token reduction, and token optimization for in-context learning emerge. These advancements have the potential to drive the development of new model architectures and learning strategies that improve robustness, interpretability, and alignment with generative modeling objectives. <div>
arXiv:2505.18227v1 Announce Type: new 
Abstract: In Transformer architectures, tokens\textemdash discrete units derived from raw data\textemdash are formed by segmenting inputs into fixed-length chunks. Each token is then mapped to an embedding, enabling parallel attention computations while preserving the input's essential information. Due to the quadratic computational complexity of transformer self-attention mechanisms, token reduction has primarily been used as an efficiency strategy. This is especially true in single vision and language domains, where it helps balance computational costs, memory usage, and inference latency. Despite these advances, this paper argues that token reduction should transcend its traditional efficiency-oriented role in the era of large generative models. Instead, we position it as a fundamental principle in generative modeling, critically influencing both model architecture and broader applications. Specifically, we contend that across vision, language, and multimodal systems, token reduction can: (i) facilitate deeper multimodal integration and alignment, (ii) mitigate "overthinking" and hallucinations, (iii) maintain coherence over long inputs, and (iv) enhance training stability, etc. We reframe token reduction as more than an efficiency measure. By doing so, we outline promising future directions, including algorithm design, reinforcement learning-guided token reduction, token optimization for in-context learning, and broader ML and scientific domains. We highlight its potential to drive new model architectures and learning strategies that improve robustness, increase interpretability, and better align with the objectives of generative modeling.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Follow the Energy, Find the Path: Riemannian Metrics from Energy-Based Models</title>
<link>https://arxiv.org/abs/2505.18230</link>
<guid>https://arxiv.org/abs/2505.18230</guid>
<content:encoded><![CDATA[
<div> EBMs, Riemannian metrics, geodesics, high-dimensional data, generative modeling<br />
Summary: 
- The study addresses finding the shortest path between two data points in a high-dimensional space, which is challenging due to the curved manifold and the need for Riemannian metrics to describe local curvature.
- A novel method is proposed to derive Riemannian metrics from pretrained Energy-Based Models (EBMs), enabling the computation of geodesics that follow the data manifold's intrinsic geometry.
- Two new metrics derived from EBMs are introduced, showing superior performance in producing geodesics that align with ground-truth trajectories and exhibit lower curvature distortion compared to established baselines.
- Evaluations on synthetic datasets, rotated character images, and high-resolution natural images demonstrate the effectiveness of EBM-derived metrics, especially in high-dimensional settings.
- This work pioneers the derivation of Riemannian metrics from EBMs, facilitating data-aware geodesics and enhancing generative modeling and simulation approaches. <br /><br /> <div>
arXiv:2505.18230v1 Announce Type: new 
Abstract: What is the shortest path between two data points lying in a high-dimensional space? While the answer is trivial in Euclidean geometry, it becomes significantly more complex when the data lies on a curved manifold -- requiring a Riemannian metric to describe the space's local curvature. Estimating such a metric, however, remains a major challenge in high dimensions.
  In this work, we propose a method for deriving Riemannian metrics directly from pretrained Energy-Based Models (EBMs) -- a class of generative models that assign low energy to high-density regions. These metrics define spatially varying distances, enabling the computation of geodesics -- shortest paths that follow the data manifold's intrinsic geometry. We introduce two novel metrics derived from EBMs and show that they produce geodesics that remain closer to the data manifold and exhibit lower curvature distortion, as measured by alignment with ground-truth trajectories. We evaluate our approach on increasingly complex datasets: synthetic datasets with known data density, rotated character images with interpretable geometry, and high-resolution natural images embedded in a pretrained VAE latent space.
  Our results show that EBM-derived metrics consistently outperform established baselines, especially in high-dimensional settings. Our work is the first to derive Riemannian metrics from EBMs, enabling data-aware geodesics and unlocking scalable, geometry-driven learning for generative modeling and simulation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>NSNQuant: A Double Normalization Approach for Calibration-Free Low-Bit Vector Quantization of KV Cache</title>
<link>https://arxiv.org/abs/2505.18231</link>
<guid>https://arxiv.org/abs/2505.18231</guid>
<content:encoded><![CDATA[
<div> Keywords: Large Language Model, Vector Quantization, NSNQuant, low-bit compression, calibration-free

Summary:
NSNQuant introduces a calibration-free Vector Quantization technique, aiming to reduce memory usage during Large Language Model inference. By employing a three-step transformation process with Hadamard transform, NSNQuant aligns token distribution with a standard normal distribution, enabling robust compression of the key-value cache without the need for calibration datasets. The method outperforms existing approaches in both 1-bit and 2-bit settings, offering significant throughput gains and strong generalization capabilities. Through extensive experiments, NSNQuant demonstrates up to 3$\times$ throughput improvement over full-precision baselines, showcasing its efficiency and effectiveness in enhancing the performance of Large Language Models. 

<br /><br />Summary: <div>
arXiv:2505.18231v1 Announce Type: new 
Abstract: Large Language Model (LLM) inference is typically memory-intensive, especially when processing large batch sizes and long sequences, due to the large size of key-value (KV) cache. Vector Quantization (VQ) is recently adopted to alleviate this issue, but we find that the existing approach is susceptible to distribution shift due to its reliance on calibration datasets. To address this limitation, we introduce NSNQuant, a calibration-free Vector Quantization (VQ) technique designed for low-bit compression of the KV cache. By applying a three-step transformation-1) a token-wise normalization (Normalize), 2) a channel-wise centering (Shift), and 3) a second token-wise normalization (Normalize)-with Hadamard transform, NSNQuant effectively aligns the token distribution with the standard normal distribution. This alignment enables robust, calibration-free vector quantization using a single reusable codebook. Extensive experiments show that NSNQuant consistently outperforms prior methods in both 1-bit and 2-bit settings, offering strong generalization and up to 3$\times$ throughput gain over full-precision baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning</title>
<link>https://arxiv.org/abs/2505.18232</link>
<guid>https://arxiv.org/abs/2505.18232</guid>
<content:encoded><![CDATA[
<div> sparse, language models, pruning, regularization, efficient<br />
Summary:<br />
The article introduces a novel approach called ELDeR for making Large Language Models (LLMs) more efficient by leveraging sparsity and regularization techniques. Unlike traditional prune-then-finetune methods, ELDeR first applies regularization to learn the optimal weights for each transformer layer before pruning. This approach helps preserve valuable information and reduces the need for costly recovery fine-tuning. By minimizing information loss and improving language modeling ability, ELDeR outperforms other pruning methods while significantly reducing computational costs. Furthermore, its layer-wise pruning approach enhances end-to-end acceleration, making it a promising technique for optimizing LLMs. <div>
arXiv:2505.18232v1 Announce Type: new 
Abstract: The deployment of Large language models (LLMs) in many fields is largely hindered by their high computational and memory costs. Recent studies suggest that LLMs exhibit sparsity, which can be used for pruning. Previous pruning methods typically follow a prune-then-finetune paradigm. Since the pruned parts still contain valuable information, statically removing them without updating the remaining parameters often results in irreversible performance degradation, requiring costly recovery fine-tuning (RFT) to maintain performance. To address this, we propose a novel paradigm: first apply regularization, then prune. Based on this paradigm, we propose ELDeR: Getting Efficient LLMs through Data-Driven Regularized Layer-wise Pruning. We multiply the output of each transformer layer by an initial weight, then we iteratively learn the weights of each transformer layer by using a small amount of data in a simple way. After that, we apply regularization to the difference between the output and input of the layers with smaller weights, forcing the information to be transferred to the remaining layers. Compared with direct pruning, ELDeR reduces the information loss caused by direct parameter removal, thus better preserving the model's language modeling ability. Experimental results show that ELDeR achieves superior performance compared with powerful layer-wise structured pruning methods, while greatly reducing RFT computational costs. Since ELDeR is a layer-wise pruning method, its end-to-end acceleration effect is obvious, making it a promising technique for efficient LLMs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>POSTER: A Multi-Signal Model for Detecting Evasive Smishing</title>
<link>https://arxiv.org/abs/2505.18233</link>
<guid>https://arxiv.org/abs/2505.18233</guid>
<content:encoded><![CDATA[
<div> phishing, smishing, detection model, linguistic cues, structural cues
<br />
Summary: 
The article discusses the threat of smishing, or SMS-based phishing, to mobile users and presents a multi-channel smishing detection model. It combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. The researchers curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. The unified architecture achieved 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models. This work highlights the effectiveness of multi-signal learning in robust and region-aware phishing detection. <div>
arXiv:2505.18233v1 Announce Type: new 
Abstract: Smishing, or SMS-based phishing, poses an increasing threat to mobile users by mimicking legitimate communications through culturally adapted, concise, and deceptive messages, which can result in the loss of sensitive data or financial resources. In such, we present a multi-channel smishing detection model that combines country-specific semantic tagging, structural pattern tagging, character-level stylistic cues, and contextual phrase embeddings. We curated and relabeled over 84,000 messages across five datasets, including 24,086 smishing samples. Our unified architecture achieves 97.89% accuracy, an F1 score of 0.963, and an AUC of 99.73%, outperforming single-stream models by capturing diverse linguistic and structural cues. This work demonstrates the effectiveness of multi-signal learning in robust and region-aware phishing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Robust PPO-optimized Tabular Transformer Framework for Intrusion Detection in Industrial IoT Systems</title>
<link>https://arxiv.org/abs/2505.18234</link>
<guid>https://arxiv.org/abs/2505.18234</guid>
<content:encoded><![CDATA[
<div> TabTransformer, Proximal Policy Optimization (PPO), Network intrusion detection system (NIDS), Industrial Internet of Things (IIoT), TON_IoT benchmark <br />
Summary: <br />
The paper introduces a robust NIDS system for IIoT environments to tackle class-imbalanced and few-shot attack scenarios. The proposed model combines a TabTransformer for feature representation with PPO for optimization via policy learning. Evaluation on the TON_IoT benchmark demonstrates high macro F1-score and accuracy, with strong performance even on rare classes like MITM attacks. Ablation experiments confirm the effectiveness of TabTransformer and PPO in mitigating class imbalance and enhancing generalization. The results emphasize the potential of combining transformer-based tabular learning with reinforcement learning for practical NIDS applications. <div>
arXiv:2505.18234v1 Announce Type: new 
Abstract: In this paper, we propose a robust and reinforcement-learning-enhanced network intrusion detection system (NIDS) designed for class-imbalanced and few-shot attack scenarios in Industrial Internet of Things (IIoT) environments. Our model integrates a TabTransformer for effective tabular feature representation with Proximal Policy Optimization (PPO) to optimize classification decisions via policy learning. Evaluated on the TON\textunderscore IoT benchmark, our method achieves a macro F1-score of 97.73\% and accuracy of 98.85\%. Remarkably, even on extremely rare classes like man-in-the-middle (MITM), our model achieves an F1-score of 88.79\%, showcasing strong robustness and few-shot detection capabilities. Extensive ablation experiments confirm the complementary roles of TabTransformer and PPO in mitigating class imbalance and improving generalization. These results highlight the potential of combining transformer-based tabular learning with reinforcement learning for real-world NIDS applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Origins of Representation Manifolds in Large Language Models</title>
<link>https://arxiv.org/abs/2505.18235</link>
<guid>https://arxiv.org/abs/2505.18235</guid>
<content:encoded><![CDATA[
<div> linear representation hypothesis, neural representations, sparse autoencoders, manifold, cosine similarity

Summary:
The article discusses the linear representation hypothesis in AI systems, suggesting that neural representations are sparse linear combinations of almost-orthogonal direction vectors. It explores the idea of moving towards a fuller model of features where neural representations could encode not only the presence but also the continuous and multidimensional value of a feature. The concept of representing a feature as a manifold is introduced, highlighting how cosine similarity in representation space can encode the intrinsic geometry of a feature through shortest, on-manifold paths. This theory is validated using text embeddings and token activations of large language models, demonstrating how distance in representation space and relatedness in concept space could be linked. <div>
arXiv:2505.18235v1 Announce Type: new 
Abstract: There is a large ongoing scientific effort in mechanistic interpretability to map embeddings and internal representations of AI systems into human-understandable concepts. A key element of this effort is the linear representation hypothesis, which posits that neural representations are sparse linear combinations of `almost-orthogonal' direction vectors, reflecting the presence or absence of different features. This model underpins the use of sparse autoencoders to recover features from representations. Moving towards a fuller model of features, in which neural representations could encode not just the presence but also a potentially continuous and multidimensional value for a feature, has been a subject of intense recent discourse. We describe why and how a feature might be represented as a manifold, demonstrating in particular that cosine similarity in representation space may encode the intrinsic geometry of a feature through shortest, on-manifold paths, potentially answering the question of how distance in representation space and relatedness in concept space could be connected. The critical assumptions and predictions of the theory are validated on text embeddings and token activations of large language models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Decomposition of Water Demand Patterns Using Skewed Gaussian Distributions for Behavioral Insights and Operational Planning</title>
<link>https://arxiv.org/abs/2505.18245</link>
<guid>https://arxiv.org/abs/2505.18245</guid>
<content:encoded><![CDATA[
<div> Keywords: urban water demand, Skewed Gaussian Distributions, decomposition, operational planning, synthetic scenarios 

Summary:
This study introduces a novel approach using Skewed Gaussian Distributions (SGD) to decompose urban water demand patterns for operational planning and behavioral insights. The method breaks down daily demand curves into baseline and peak components, characterizing peaks with parameters like amplitude, timing, spread, and skewness. This detailed peak-level decomposition facilitates anomaly detection, real-time demand management, and strategic analyses. SGDs capture asymmetric peak shapes better than traditional models, improving pattern reconstruction accuracy by over 50%. The framework can also generate synthetic demand scenarios with chosen characteristics. The method's effectiveness is demonstrated on real-world datasets, showcasing its superior performance and interpretability over symmetric Gaussian models. Implementation code is publicly available for use. 

<br /><br />Summary: <div>
arXiv:2505.18245v1 Announce Type: new 
Abstract: This study presents a novel approach for decomposing urban water demand patterns using Skewed Gaussian Distributions (SGD) to derive behavioral insights and support operational planning. Hourly demand profiles contain critical information for both long-term infrastructure design and daily operations, influencing network pressures, water quality, energy consumption, and overall reliability. By breaking down each daily demand curve into a baseline component and distinct peak components, the proposed SGD method characterizes each peak with interpretable parameters, including peak amplitude, timing (mean), spread (duration), and skewness (asymmetry), thereby reconstructing the observed pattern and uncovering latent usage dynamics. This detailed peak-level decomposition enables both operational applications, e.g. anomaly and leakage detection, real-time demand management, and strategic analyses, e.g. identifying behavioral shifts, seasonal influences, or policy impacts on consumption patterns. Unlike traditional symmetric Gaussian or purely statistical time-series models, SGDs explicitly capture asymmetric peak shapes such as sharp morning surges followed by gradual declines, improving the fidelity of synthetic pattern generation and enhancing the detection of irregular consumption behavior. The method is demonstrated on several real-world datasets, showing that SGD outperforms symmetric Gaussian models in reconstruction accuracy, reducing root-mean-square error by over 50% on average, while maintaining physical interpretability. The SGD framework can also be used to construct synthetic demand scenarios by designing daily peak profiles with chosen characteristics. All implementation code is publicly available at: https://github.com/Relkayam/water-demand-decomposition-sgd
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Uncovering a Universal Abstract Algorithm for Modular Addition in Neural Networks</title>
<link>https://arxiv.org/abs/2505.18266</link>
<guid>https://arxiv.org/abs/2505.18266</guid>
<content:encoded><![CDATA[
<div> Chinese Remainder Theorem, neural networks, modular addition, deep learning, interpretability

Summary: 
The article proposes a universality hypothesis that suggests neural network solutions in modular addition tasks are unified under an abstract algorithm. Through multi-level analyses, it is shown that both multilayer perceptrons and transformers implement the abstract algorithm called the approximate Chinese Remainder Theorem. The introduction of approximate cosets reveals that neurons activate exclusively on them. The theory applies to deep neural networks and predicts that solutions require only O(log n) features. Empirical confirmation supports this prediction. This work provides the first theory-backed interpretation of multilayer networks solving modular addition and enhances generalizable interpretability in neural networks. The hypothesis also opens up possibilities for testing universality in group multiplication beyond modular addition. 

<br /><br />Summary: <div>
arXiv:2505.18266v1 Announce Type: new 
Abstract: We propose a testable universality hypothesis, asserting that seemingly disparate neural network solutions observed in the simple task of modular addition are unified under a common abstract algorithm. While prior work interpreted variations in neuron-level representations as evidence for distinct algorithms, we demonstrate - through multi-level analyses spanning neurons, neuron clusters, and entire networks - that multilayer perceptrons and transformers universally implement the abstract algorithm we call the approximate Chinese Remainder Theorem. Crucially, we introduce approximate cosets and show that neurons activate exclusively on them. Furthermore, our theory works for deep neural networks (DNNs). It predicts that universally learned solutions in DNNs with trainable embeddings or more than one hidden layer require only O(log n) features, a result we empirically confirm. This work thus provides the first theory-backed interpretation of multilayer networks solving modular addition. It advances generalizable interpretability and opens a testable universality hypothesis for group multiplication beyond modular addition.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Representative Action Selection for Large Action-Space Meta-Bandits</title>
<link>https://arxiv.org/abs/2505.18269</link>
<guid>https://arxiv.org/abs/2505.18269</guid>
<content:encoded><![CDATA[
<div> bandit, action space, Gaussian process, epsilon-net algorithm, Thompson Sampling

Summary:
The study investigates the selection of a subset from a large action space for a group of bandits with related payoffs modeled by a Gaussian process. A simple epsilon-net algorithm is proposed to choose a representative subset based on the similarity of actions. The algorithm's performance is supported by theoretical guarantees and compared to Thompson Sampling and Upper Confidence Bound through empirical analysis. The research aims to achieve near-optimal results by utilizing the structure of the action space shared by the bandits. The proposed algorithm offers a promising approach to efficiently select actions in a complex environment with diverse options, showcasing its potential for practical applications in bandit problems. <br /><br />Summary: <div>
arXiv:2505.18269v1 Announce Type: new 
Abstract: We study the problem of selecting a subset from a large action space shared by a family of bandits, with the goal of achieving performance nearly matching that of using the full action space. We assume that similar actions tend to have related payoffs, modeled by a Gaussian process. To exploit this structure, we propose a simple epsilon-net algorithm to select a representative subset. We provide theoretical guarantees for its performance and compare it empirically to Thompson Sampling and Upper Confidence Bound.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Feature Preserving Shrinkage on Bayesian Neural Networks via the R2D2 Prior</title>
<link>https://arxiv.org/abs/2505.18280</link>
<guid>https://arxiv.org/abs/2505.18280</guid>
<content:encoded><![CDATA[
<div> Bayesian neural networks, R2D2-Net, prior distributions, variational Gibbs inference algorithm, uncertainty estimation<br />
Summary:<br />
The article introduces a novel approach called R2D2-Net that utilizes the R^2-induced Dirichlet Decomposition (R2D2) prior to address the challenge of selecting appropriate prior distributions in Bayesian neural networks (BNNs). By effectively shrinking irrelevant coefficients towards zero while preventing over-shrinkage of key features, the R2D2-Net improves the performance and stability of BNNs. A variational Gibbs inference algorithm is proposed to accurately approximate the posterior distribution of weights, enhancing estimation consistency. The authors analyze the evidence lower bound (ELBO) and posterior concentration rates theoretically to validate their approach. Experimental results show promising performance on image classification and uncertainty estimation tasks, highlighting the effectiveness of the R2D2-Net in improving BNN performance. <br /> <div>
arXiv:2505.18280v1 Announce Type: new 
Abstract: Bayesian neural networks (BNNs) treat neural network weights as random variables, which aim to provide posterior uncertainty estimates and avoid overfitting by performing inference on the posterior weights. However, the selection of appropriate prior distributions remains a challenging task, and BNNs may suffer from catastrophic inflated variance or poor predictive performance when poor choices are made for the priors. Existing BNN designs apply different priors to weights, while the behaviours of these priors make it difficult to sufficiently shrink noisy signals or they are prone to overshrinking important signals in the weights. To alleviate this problem, we propose a novel R2D2-Net, which imposes the R^2-induced Dirichlet Decomposition (R2D2) prior to the BNN weights. The R2D2-Net can effectively shrink irrelevant coefficients towards zero, while preventing key features from over-shrinkage. To approximate the posterior distribution of weights more accurately, we further propose a variational Gibbs inference algorithm that combines the Gibbs updating procedure and gradient-based optimization. This strategy enhances stability and consistency in estimation when the variational objective involving the shrinkage parameters is non-convex. We also analyze the evidence lower bound (ELBO) and the posterior concentration rates from a theoretical perspective. Experiments on both natural and medical image classification and uncertainty estimation tasks demonstrate satisfactory performance of our method.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tube Loss based Deep Networks For Improving the Probabilistic Forecasting of Wind Speed</title>
<link>https://arxiv.org/abs/2505.18284</link>
<guid>https://arxiv.org/abs/2505.18284</guid>
<content:encoded><![CDATA[
<div> deep learning, probabilistic forecasting, wind speed, Tube loss function, uncertainty quantification

Summary:
This paper addresses the challenge of Uncertainty Quantification (UQ) in wind speed forecasting for wind power production. The study focuses on designing deep learning based probabilistic forecasting methods using the Tube loss function, which allows for reliable Prediction Interval (PI) estimation without distribution assumptions. The proposed models incorporate popular architectures like LSTM, GRU, and TCN, and a heuristic for tuning the Tube loss function parameter to achieve narrower PIs without sacrificing calibration ability. The research utilizes wind datasets from Jaisalmer, Los Angeles, and San Francisco to demonstrate the effectiveness of the deep forecasting models in producing more reliable and narrower PIs compared to existing probabilistic methods.<br /><br />Summary: <div>
arXiv:2505.18284v1 Announce Type: new 
Abstract: Uncertainty Quantification (UQ) in wind speed forecasting is a critical challenge in wind power production due to the inherently volatile nature of wind. By quantifying the associated risks and returns, UQ supports more effective decision-making for grid operations and participation in the electricity market. In this paper, we design a sequence of deep learning based probabilistic forecasting methods by using the Tube loss function for wind speed forecasting. The Tube loss function is a simple and model agnostic Prediction Interval (PI) estimation approach and can obtain the narrow PI with asymptotical coverage guarantees without any distribution assumption. Our deep probabilistic forecasting models effectively incorporate popular architectures such as LSTM, GRU, and TCN within the Tube loss framework. We further design a simple yet effective heuristic for tuning the $\delta$ parameter of the Tube loss function so that our deep forecasting models obtain the narrower PI without compromising its calibration ability. We have considered three wind datasets, containing the hourly recording of the wind speed, collected from three distinct location namely Jaisalmer, Los Angeles and San Fransico. Our numerical results demonstrate that the proposed deep forecasting models produce more reliable and narrower PIs compared to recently developed probabilistic wind forecasting methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convexified Message-Passing Graph Neural Networks</title>
<link>https://arxiv.org/abs/2505.18289</link>
<guid>https://arxiv.org/abs/2505.18289</guid>
<content:encoded><![CDATA[
<div> Graph Neural Networks, Convexified Message Passing Graph Neural Networks, Convex optimization, Reproducing kernel Hilbert space, Generalization guarantees

Summary:
Convexified Message Passing Graph Neural Networks (CGNNs) combine message-passing GNNs with convex optimization, making training efficient and optimal. By mapping filters into a kernel space, CGNNs can be rigorously analyzed. Two-layer CGNNs have proven generalization guarantees, converging to optimal GNN performance. A layer-wise training strategy enables scaling to deeper architectures. Experimental results show CGNNs outperform leading GNN models by 10 to 40 percent in most cases, emphasizing their theoretical strength and practical power. In cases where improvements are marginal, CGNNs still match or slightly exceed baselines, highlighting their robustness. Despite typically relying on over-parameterization, CGNNs achieve superior accuracy and resource efficiency with shallow convex models. The results demonstrate the promise of CGNNs as a principled and effective method with strong theoretical foundations.

<br /><br />Summary: <div>
arXiv:2505.18289v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have become prominent methods for graph representation learning, demonstrating strong empirical results on diverse graph prediction tasks. In this paper, we introduce Convexified Message Passing Graph Neural Networks (CGNNs), a novel and general framework that combines the power of message-passing GNNs with the tractability of convex optimization. By mapping their nonlinear filters into a reproducing kernel Hilbert space, CGNNs transform training into a convex optimization problem, which can be solved efficiently and optimally by projected gradient methods. This convexity further allows the statistical properties of CGNNs to be analyzed accurately and rigorously. For two-layer CGNNs, we establish rigorous generalization guarantees, showing convergence to the performance of the optimal GNN. To scale to deeper architectures, we adopt a principled layer-wise training strategy. Experiments on benchmark datasets show that CGNNs significantly exceed the performance of leading GNN models, achieving 10 to 40 percent higher accuracy in most cases, underscoring their promise as a powerful and principled method with strong theoretical foundations. In rare cases where improvements are not quantitatively substantial, the convex models either slightly exceed or match the baselines, stressing their robustness and wide applicability. Though over-parameterization is often employed to enhance performance in nonconvex models, we show that our CGNNs framework yields shallow convex models that can surpass these models in both accuracy and resource efficiency.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient Nonlinear MCMC on General Graphs</title>
<link>https://arxiv.org/abs/2505.18300</link>
<guid>https://arxiv.org/abs/2505.18300</guid>
<content:encoded><![CDATA[
<div> framework, Markov Chain Monte Carlo, random walk algorithm, discrete state spaces, efficient sampling<br />
<br />
Summary:
The article introduces the History-Driven Target (HDT) framework in Markov Chain Monte Carlo (MCMC) for improving random walk algorithms on discrete state spaces, particularly general undirected graphs. The HDT framework aims to enhance sampling efficiency from a target distribution by introducing a history-dependent target distribution that replaces the original target. This approach eliminates the need for explicit computation of transition probabilities for all neighbors at each step, reducing computational overhead. The HDT framework is compatible with both reversible and non-reversible MCMC samplers, enabling unbiased samples with the target distribution and achieving near-zero variance performance. Experimental results in graph sampling show consistent performance gains, with a memory-efficient Least Recently Used (LRU) cache ensuring scalability to large general graphs.<br /><br />Summary: <div>
arXiv:2505.18300v1 Announce Type: new 
Abstract: We propose a history-driven target (HDT) framework in Markov Chain Monte Carlo (MCMC) to improve any random walk algorithm on discrete state spaces, such as general undirected graphs, for efficient sampling from target distribution $\boldsymbol{\mu}$. With broad applications in network science and distributed optimization, recent innovations like the self-repellent random walk (SRRW) achieve near-zero variance by prioritizing under-sampled states through transition kernel modifications based on past visit frequencies. However, SRRW's reliance on explicit computation of transition probabilities for all neighbors at each step introduces substantial computational overhead, while its strict dependence on time-reversible Markov chains excludes advanced non-reversible MCMC methods. To overcome these limitations, instead of direct modification of transition kernel, HDT introduces a history-dependent target distribution $\boldsymbol{\pi}[\mathbf{x}]$ to replace the original target $\boldsymbol{\mu}$ in any graph sampler, where $\mathbf{x}$ represents the empirical measure of past visits. This design preserves lightweight implementation by requiring only local information between the current and proposed states and achieves compatibility with both reversible and non-reversible MCMC samplers, while retaining unbiased samples with target distribution $\boldsymbol{\mu}$ and near-zero variance performance. Extensive experiments in graph sampling demonstrate consistent performance gains, and a memory-efficient Least Recently Used (LRU) cache ensures scalability to large general graphs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PLUMAGE: Probabilistic Low rank Unbiased Min Variance Gradient Estimator for Efficient Large Model Training</title>
<link>https://arxiv.org/abs/2505.18313</link>
<guid>https://arxiv.org/abs/2505.18313</guid>
<content:encoded><![CDATA[
<div> PLUMAGE, Low rank gradient estimators, Large language models, Training, Memory constraints<br />
<br />
Summary:<br />
PLUMAGE is a novel probabilistic low-rank gradient estimator designed to address memory and networking constraints in training large language models with billions of parameters. It offers a drop-in replacement for existing methods without introducing additional hyperparameters and resolves issues related to bias and high estimator variance. By preventing optimizer state misalignment, PLUMAGE enhances training stability and reduces the optimization gap and training loss across models in the GLUE benchmark. Empirical results show that PLUMAGE outperforms existing methods, achieving a 33% reduction in optimization gap and a 28% decrease in training loss, all while maintaining a similar computational and memory footprint as GaLoRE. <div>
arXiv:2505.18313v1 Announce Type: new 
Abstract: Accelerator memory and networking constraints have emerged as dominant bottlenecks when training large language models LLMs with billions of parameters. Existing low rank gradient estimators such as GaLoRE and FLORA compress gradients and optimizer tensors by projecting weight gradients onto a rank r subspace, enabling LLM training on consumer hardware. Yet, these methods are either biased or subject to high estimator variance. Moreover, the optimizer state based on the first and second moments estimates expressed in the previous subspace becomes misaligned whenever the projection is updated, leading to instabilities during training. We propose PLUMAGE: Probabilistic Low rank Unbiased Minimum vAriance Gradient Estimator. PLUMAGE is a drop in replacement for existing low rank gradient estimators. It does not introduce new hyperparameters beyond the chosen rank r and the update interval. In addition, we resolve optimizer state misalignment issues to prevent spurious weight updates and enhance training stability. We empirically demonstrate that PLUMAGE shrinks the full rank optimization's gap over the pre training evaluation loss by 33% on average across models and the average training loss across the GLUE benchmark by 28% within a similar computational and memory footprint as GaloRE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Sample Complexity of Diffusion Model Training Without Empirical Risk Minimizer Access</title>
<link>https://arxiv.org/abs/2505.18344</link>
<guid>https://arxiv.org/abs/2505.18344</guid>
<content:encoded><![CDATA[
<div> Diffusion models, sample complexity, score estimation, statistical error, approximation error, optimization error <br />
<br />
Summary: 
This work introduces a principled analysis of score estimation in diffusion models, addressing the sample complexity issue. By decomposing the estimation error into statistical, approximation, and optimization components, the study establishes a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Unlike previous analyses, this approach eliminates the exponential dependence on neural network parameters. The analysis does not assume access to the empirical risk minimizer of the score function estimation loss, a significant advancement in theoretical understanding. <div>
arXiv:2505.18344v1 Announce Type: new 
Abstract: Diffusion models have demonstrated state-of-the-art performance across vision, language, and scientific domains. Despite their empirical success, prior theoretical analyses of the sample complexity suffer from poor scaling with input data dimension or rely on unrealistic assumptions such as access to exact empirical risk minimizers. In this work, we provide a principled analysis of score estimation, establishing a sample complexity bound of $\widetilde{\mathcal{O}}(\epsilon^{-6})$. Our approach leverages a structured decomposition of the score estimation error into statistical, approximation, and optimization errors, enabling us to eliminate the exponential dependence on neural network parameters that arises in prior analyses. It is the first such result which achieves sample complexity bounds without assuming access to the empirical risk minimizer of score function estimation loss.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Diffusion Self-Weighted Guidance for Offline Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18345</link>
<guid>https://arxiv.org/abs/2505.18345</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, offline, policy, diffusion models, guidance

Summary: 
This study focuses on offline reinforcement learning (RL) and proposes a new approach called Self-Weighted Guidance (SWG) to address challenges in computing scores due to the unknown weight function when modeling the optimal policy. SWG utilizes a diffusion model that incorporates actions and weights, allowing for direct score computation without the need for additional networks. Through experiments on toy examples and D4RL environments, SWG demonstrates its ability to generate samples from the desired distribution and achieve comparable performance to existing methods. Ablation studies confirm the effectiveness of SWG's weight formulations, and its scalability is also validated. Overall, SWG offers a streamlined training pipeline for offline RL tasks, showcasing its potential in improving policy learning with historical observations. 

<br /><br />Summary: <div>
arXiv:2505.18345v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) recovers the optimal policy $\pi$ given historical observations of an agent. In practice, $\pi$ is modeled as a weighted version of the agent's behavior policy $\mu$, using a weight function $w$ working as a critic of the agent's behavior. Though recent approaches to offline RL based on diffusion models have exhibited promising results, the computation of the required scores is challenging due to their dependence on the unknown $w$. In this work, we alleviate this issue by constructing a diffusion over both the actions and the weights. With the proposed setting, the required scores are directly obtained from the diffusion model without learning extra networks. Our main conceptual contribution is a novel guidance method, where guidance (which is a function of $w$) comes from the same diffusion model, therefore, our proposal is termed Self-Weighted Guidance (SWG). We show that SWG generates samples from the desired distribution on toy examples and performs on par with state-of-the-art methods on D4RL's challenging environments, while maintaining a streamlined training pipeline. We further validate SWG through ablation studies on weight formulations and scalability.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Cell Must Go On: Agar.io for Continual Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18347</link>
<guid>https://arxiv.org/abs/2505.18347</guid>
<content:encoded><![CDATA[
<div> reinforcement learning, continual learning, AgarCL, Agar.io, benchmark results

Summary:
AgarCL is introduced as a research platform for continual reinforcement learning, specifically designed for environments where static policies are ineffective due to changing dynamics. Based on the game Agar.io, AgarCL provides a high-dimensional, non-episodic problem with stochastic, evolving dynamics, continuous actions, and partial observability. The platform allows for the development of increasingly sophisticated behavior over time. Benchmark results on the challenging continual RL problem in AgarCL are provided for DQN, PPO, and SAC algorithms. Additionally, a suite of smaller tasks within AgarCL isolates different aspects of the environment, aiding in understanding the challenges posed by the game's dynamics. This research platform and benchmark results showcase the capabilities of continual reinforcement learning in dynamic environments like Agar.io. 

<br /><br />Summary: <div>
arXiv:2505.18347v1 Announce Type: new 
Abstract: Continual reinforcement learning (RL) concerns agents that are expected to learn continually, rather than converge to a policy that is then fixed for evaluation. Such an approach is well suited to environments the agent perceives as changing, which renders any static policy ineffective over time. The few simulators explicitly designed for empirical research in continual RL are often limited in scope or complexity, and it is now common for researchers to modify episodic RL environments by artificially incorporating abrupt task changes during interaction. In this paper, we introduce AgarCL, a research platform for continual RL that allows for a progression of increasingly sophisticated behaviour. AgarCL is based on the game Agar.io, a non-episodic, high-dimensional problem featuring stochastic, ever-evolving dynamics, continuous actions, and partial observability. Additionally, we provide benchmark results reporting the performance of DQN, PPO, and SAC in both the primary, challenging continual RL problem, and across a suite of smaller tasks within AgarCL, each of which isolates aspects of the full environment and allow us to characterize the challenges posed by different aspects of the game.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Task Specific Pruning with LLM-Sieve: How Many Parameters Does Your Task Really Need?</title>
<link>https://arxiv.org/abs/2505.18350</link>
<guid>https://arxiv.org/abs/2505.18350</guid>
<content:encoded><![CDATA[
<div> framework, task-specific pruning, Large Language Models, parameter reduction, accuracy degradation
<br />
Summary: 
The article introduces LLM-Sieve, a framework for task-specific pruning of Large Language Models (LLMs) that aims to reduce the number of parameters while maintaining high accuracy. LLM-Sieve achieves a parameter reduction of 20-75% with only a 1-5% accuracy degradation across various domains. Unlike previous methods that focus on uniform pruning or low-rank approximations, LLM-Sieve uses task-aware joint projections and a Genetic Algorithm to determine optimal pruning levels for each matrix. The framework is compatible with LoRA fine-tuning and quantization, and demonstrates strong generalization across datasets within the same task domain. Overall, LLM-Sieve provides a practical and robust approach to generating smaller, yet effective, task-specific models. 
<br /> <div>
arXiv:2505.18350v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly being adopted for narrow tasks - such as medical question answering or sentiment analysis - and deployed in resource-constrained settings, a key question arises: how many parameters does a task actually need? In this work, we present LLM-Sieve, the first comprehensive framework for task-specific pruning of LLMs that achieves 20-75% parameter reduction with only 1-5% accuracy degradation across diverse domains. Unlike prior methods that apply uniform pruning or rely on low-rank approximations of weight matrices or inputs in isolation, LLM-Sieve (i) learns task-aware joint projections to better approximate output behavior, and (ii) employs a Genetic Algorithm to discover differentiated pruning levels for each matrix. LLM-Sieve is fully compatible with LoRA fine-tuning and quantization, and uniquely demonstrates strong generalization across datasets within the same task domain. Together, these results establish a practical and robust mechanism to generate smaller performant task-specific models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>X-MethaneWet: A Cross-scale Global Wetland Methane Emission Benchmark Dataset for Advancing Science Discovery with AI</title>
<link>https://arxiv.org/abs/2505.18355</link>
<guid>https://arxiv.org/abs/2505.18355</guid>
<content:encoded><![CDATA[
<div> benchmark dataset, methane fluxes, global wetlands, AI algorithms, transfer learning<br />
Summary:<br />
The article introduces X-MethaneWet, a global wetland methane benchmark dataset merging model simulation data with real-world observations. This dataset aims to enhance methane modeling and science discovery utilizing AI algorithms. Sequential deep learning models were evaluated to predict methane fluxes using X-MethaneWet, with promising results. Additionally, four transfer learning techniques were explored to improve model generalization by leveraging TEM-MDM simulated data. The experiments demonstrated the effectiveness of these approaches, showcasing their potential to advance methane emission modeling and contribute to the development of more accurate and scalable AI-driven climate models. <br /> <br />Summary: <div>
arXiv:2505.18355v1 Announce Type: new 
Abstract: Methane (CH$_4$) is the second most powerful greenhouse gas after carbon dioxide and plays a crucial role in climate change due to its high global warming potential. Accurately modeling CH$_4$ fluxes across the globe and at fine temporal scales is essential for understanding its spatial and temporal variability and developing effective mitigation strategies. In this work, we introduce the first-of-its-kind cross-scale global wetland methane benchmark dataset (X-MethaneWet), which synthesizes physics-based model simulation data from TEM-MDM and the real-world observation data from FLUXNET-CH$_4$. This dataset can offer opportunities for improving global wetland CH$_4$ modeling and science discovery with new AI algorithms. To set up AI model baselines for methane flux prediction, we evaluate the performance of various sequential deep learning models on X-MethaneWet. Furthermore, we explore four different transfer learning techniques to leverage simulated data from TEM-MDM to improve the generalization of deep learning models on real-world FLUXNET-CH$_4$ observations. Our extensive experiments demonstrate the effectiveness of these approaches, highlighting their potential for advancing methane emission modeling and contributing to the development of more accurate and scalable AI-driven climate models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Small Models, Smarter Learning: The Power of Joint Task Training</title>
<link>https://arxiv.org/abs/2505.18369</link>
<guid>https://arxiv.org/abs/2505.18369</guid>
<content:encoded><![CDATA[
<div> Keywords: task difficulty, model size, ListOps dataset, transformer models, training curriculum

Summary:
The study examines the relationship between task difficulty, model size, and training curriculum in small transformer models using the ListOps dataset. It finds that the sum modulo n operation is the hardest to learn, but when combined with other operations like maximum and median, it becomes easier and requires fewer parameters. Models trained on a mixture of operations exhibit number-like representations and better distinguish parity. The model focusing on only the sum operation tends to memorize rather than capture number structure in embeddings. Interestingly, joint training leads to qualitatively different model behavior, activating the attention mechanism more than feedforward layers. Furthermore, pretraining models on a combination of MAX+MED before pure SUM learning enables learning below the initial threshold. The findings suggest that the training curriculum plays a crucial role in the emergent abilities of language models. 

<br /><br />Summary: <div>
arXiv:2505.18369v1 Announce Type: new 
Abstract: The ability of a model to learn a task depends strongly on both the task difficulty and the model size. We aim to understand how task difficulty relates to the minimum number of parameters required for learning specific tasks in small transformer models. Our study focuses on the ListOps dataset, which consists of nested mathematical operations. We gradually increase task difficulty by introducing new operations or combinations of operations into the training data. We observe that sum modulo n is the hardest to learn. Curiously, when combined with other operations such as maximum and median, the sum operation becomes easier to learn and requires fewer parameters. We show that joint training not only improves performance but also leads to qualitatively different model behavior. We show evidence that models trained only on SUM might be memorizing and fail to capture the number structure in the embeddings. In contrast, models trained on a mixture of SUM and other operations exhibit number-like representations in the embedding space, and a strong ability to distinguish parity. Furthermore, the SUM-only model relies more heavily on its feedforward layers, while the jointly trained model activates the attention mechanism more. Finally, we show that learning pure SUM can be induced in models below the learning threshold of pure SUM, by pretraining them on MAX+MED. Our findings indicate that emergent abilities in language models depend not only on model size, but also the training curriculum.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Next-token pretraining implies in-context learning</title>
<link>https://arxiv.org/abs/2505.18373</link>
<guid>https://arxiv.org/abs/2505.18373</guid>
<content:encoded><![CDATA[
<div> ICL, predictability, self-supervised learning, in-distribution, context<br />
<br />
Summary: 
This study argues that in-context learning (ICL) emerges predictably from standard self-supervised next-token pretraining instead of being an exotic property. It explores the principles behind this emergence, focusing on in-distribution ICL and how models adapt to context when trained on token sequences from non-ergodic sources. An information-theoretic framework accurately predicts context-dependent loss reduction dynamics. Experiments with synthetic datasets demonstrate phenomena like phase transitions in training loss and power-law scaling of in-context loss. The study also shows a mathematical coupling between a model's in-context performance on tasks and the tasks seen during pretraining, providing a fundamental explanation for inference-time learning that is independent of architecture and modality. <div>
arXiv:2505.18373v1 Announce Type: new 
Abstract: We argue that in-context learning (ICL) predictably arises from standard self-supervised next-token pretraining, rather than being an exotic emergent property. This work establishes the foundational principles of this emergence by focusing on in-distribution ICL, demonstrating how models necessarily adapt to context when trained on token sequences, especially from non-ergodic sources. Our information-theoretic framework precisely predicts these in-distribution ICL dynamics (i.e., context-dependent loss reduction). We verify this with experiments using synthetic datasets of differing types of correlational structure, reproducing characteristic phenomena like phase transitions in training loss for induction head formation and power-law scaling of in-context loss. We further show that a model's in-context performance on any task is mathematically coupled to the ensemble of tasks seen in pretraining, offering a fundamental explanation, grounded in architecture- and modality-independent principles, for such inference-time learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Applications of Modular Co-Design for De Novo 3D Molecule Generation</title>
<link>https://arxiv.org/abs/2505.18392</link>
<guid>https://arxiv.org/abs/2505.18392</guid>
<content:encoded><![CDATA[
<div> Transformer models, de novo molecule generation, 3D structures, Megalodon, generative models <br />
<br />
Summary:
The study introduces Megalodon, a family of scalable transformer models aimed at improving 3D molecule generation in drug discovery. These models incorporate equivariant layers and are trained using a joint objective to enhance molecular generation dynamics. Megalodon outperforms existing models in generating realistic molecular structures, focusing on energy levels. By increasing the model parameters to 40 million, Megalodon generates significantly more valid large molecules and achieves lower energy levels compared to previous generative models. The performance of Megalodon is evaluated on various benchmarks for 3D molecule generation, demonstrating state-of-the-art results in conditional structure generation and structure energy assessments using diffusion and flow matching techniques. <div>
arXiv:2505.18392v1 Announce Type: new 
Abstract: De novo 3D molecule generation is a pivotal task in drug discovery. However, many recent geometric generative models struggle to produce high-quality 3D structures, even if they maintain 2D validity and topological stability. To tackle this issue and enhance the learning of effective molecular generation dynamics, we present Megalodon-a family of scalable transformer models. These models are enhanced with basic equivariant layers and trained using a joint continuous and discrete denoising co-design objective. We assess Megalodon's performance on established molecule generation benchmarks and introduce new 3D structure benchmarks that evaluate a model's capability to generate realistic molecular structures, particularly focusing on energetics. We show that Megalodon achieves state-of-the-art results in 3D molecule generation, conditional structure generation, and structure energy benchmarks using diffusion and flow matching. Furthermore, doubling the number of parameters in Megalodon to 40M significantly enhances its performance, generating up to 49x more valid large molecules and achieving energy levels that are 2-10x lower than those of the best prior generative models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Thought calibration: Efficient and confident test-time scaling</title>
<link>https://arxiv.org/abs/2505.18404</link>
<guid>https://arxiv.org/abs/2505.18404</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, thought calibration, reasoning trees, hidden representations, model performance 

Summary: 
Thought calibration is proposed as a dynamic approach to determine when to terminate thinking in large language models to balance performance and compute cost. By viewing a language model's thoughts as reasoning trees, novel reasoning plateaus can be identified, allowing for more efficient use of computational resources. Lightweight probes operating on hidden representations are utilized to assess reasoning structure and response consistency. This framework was tested on three reasoning language models across four datasets, showing that thought calibration can maintain model performance with reduced thinking tokens by up to 60% on in-distribution data and up to 20% on out-of-distribution data. This approach offers a promising method for improving the efficiency of large language models without sacrificing performance. 

<br /><br />Summary:  <div>
arXiv:2505.18404v1 Announce Type: new 
Abstract: Reasoning large language models achieve impressive test-time scaling by thinking for longer, but this performance gain comes at significant compute cost. Directly limiting test-time budget hurts overall performance, but not all problems are equally difficult. We propose thought calibration to decide dynamically when thinking can be terminated. To calibrate our decision rule, we view a language model's growing body of thoughts as a nested sequence of reasoning trees, where the goal is to identify the point at which novel reasoning plateaus. We realize this framework through lightweight probes that operate on top of the language model's hidden representations, which are informative of both the reasoning structure and overall consistency of response. Based on three reasoning language models and four datasets, thought calibration preserves model performance with up to a 60% reduction in thinking tokens on in-distribution data, and up to 20% in out-of-distribution data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KL-regularization Itself is Differentially Private in Bandits and RLHF</title>
<link>https://arxiv.org/abs/2505.18407</link>
<guid>https://arxiv.org/abs/2505.18407</guid>
<content:encoded><![CDATA[
<div> Privacy, Differential Privacy, Regularization, Multi-armed Bandits, Reinforcement Learning<br />
<br />
Summary: This article discusses the use of regularization techniques in achieving Differential Privacy (DP) without the need for additional noise injection in data-driven algorithms. The study explores three decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF) in offline data settings. By incorporating KL-regularization into the learning objective, the action sampled from the resulting stochastic policy can achieve DP. This approach not only provides privacy guarantees but also maintains the performance-enhancing benefits of regularization. The research showcases how leveraging the inherent randomness of existing algorithms can lead to privacy preservation, offering a novel path to achieving DP without compromising algorithm efficiency. Regularization plays a crucial role in ensuring privacy while improving overall algorithm performance in various decision-making scenarios. <br /><br />Summary: <div>
arXiv:2505.18407v1 Announce Type: new 
Abstract: Differential Privacy (DP) provides a rigorous framework for privacy, ensuring the outputs of data-driven algorithms remain statistically indistinguishable across datasets that differ in a single entry. While guaranteeing DP generally requires explicitly injecting noise either to the algorithm itself or to its outputs, the intrinsic randomness of existing algorithms presents an opportunity to achieve DP ``for free''. In this work, we explore the role of regularization in achieving DP across three different decision-making problems: multi-armed bandits, linear contextual bandits, and reinforcement learning from human feedback (RLHF), in offline data settings. We show that adding KL-regularization to the learning objective (a common approach in optimization algorithms) makes the action sampled from the resulting stochastic policy itself differentially private. This offers a new route to privacy guarantees without additional noise injection, while also preserving the inherent advantage of regularization in enhancing performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LatentLLM: Attention-Aware Joint Tensor Compression</title>
<link>https://arxiv.org/abs/2505.18413</link>
<guid>https://arxiv.org/abs/2505.18413</guid>
<content:encoded><![CDATA[
<div> Keywords: large language models, multi-modal models, model compression, tensor decomposition, benchmark

Summary:
The article presents a new framework for converting large language models (LLMs) and multi-modal models (LMMs) into a reduced-dimension latent structure to improve computational and memory efficiency. The proposed method extends local activation-aware tensor decomposition to a global attention-aware joint tensor decomposition, resulting in enhanced model accuracy compared to existing model compression techniques. By reducing the latent dimension of LLMs/LMMs, the framework demonstrates significant benefits on various benchmark tasks, including multi-modal reasoning. This novel approach offers a promising solution for addressing the computational and memory constraints associated with modern foundation models, such as LLMs and LMMs.<br /><br />Summary: <div>
arXiv:2505.18413v1 Announce Type: new 
Abstract: Modern foundation models such as large language models (LLMs) and large multi-modal models (LMMs) require a massive amount of computational and memory resources. We propose a new framework to convert such LLMs/LMMs into a reduced-dimension latent structure. Our method extends a local activation-aware tensor decomposition to a global attention-aware joint tensor de-composition. Our framework can significantly improve the model accuracy over the existing model compression methods when reducing the latent dimension to realize computationally/memory-efficient LLMs/LLMs. We show the benefit on several benchmark including multi-modal reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>A Dual Basis Approach for Structured Robust Euclidean Distance Geometry</title>
<link>https://arxiv.org/abs/2505.18414</link>
<guid>https://arxiv.org/abs/2505.18414</guid>
<content:encoded><![CDATA[
<div> Euclidean Distance Matrix, point configuration, outliers, RoDEoDB, recovery<br />
Summary:<br />
The paper introduces RoDEoDB, an algorithmic framework for recovering point configuration from partial observations of Euclidean Distance Matrix (EDM) with corruptions. RoDEoDB leverages a non-orthogonal dual basis to connect the EDM to a positive semi-definite Gram matrix. The algorithm guarantees exact recovery of both the Gram matrix and point configuration under certain conditions. Empirical experiments demonstrate RoDEoDB's superior performance on sensor localization and molecular conformation datasets. The approach addresses the challenge of incomplete and corrupted EDM data arising from using anchor nodes for distance collection. Overall, RoDEoDB shows promise in accurately reconstructing point configuration from partial EDM information with outliers. <br /><br />Summary: <div>
arXiv:2505.18414v1 Announce Type: new 
Abstract: Euclidean Distance Matrix (EDM), which consists of pairwise squared Euclidean distances of a given point configuration, finds many applications in modern machine learning. This paper considers the setting where only a set of anchor nodes is used to collect the distances between themselves and the rest. In the presence of potential outliers, it results in a structured partial observation on EDM with partial corruptions. Note that an EDM can be connected to a positive semi-definite Gram matrix via a non-orthogonal dual basis. Inspired by recent development of non-orthogonal dual basis in optimization, we propose a novel algorithmic framework, dubbed Robust Euclidean Distance Geometry via Dual Basis (RoDEoDB), for recovering the Euclidean distance geometry, i.e., the underlying point configuration. The exact recovery guarantees have been established in terms of both the Gram matrix and point configuration, under some mild conditions. Empirical experiments show superior performance of RoDEoDB on sensor localization and molecular conformation datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Development of Interactive Nomograms for Predicting Short-Term Survival in ICU Patients with Aplastic Anemia</title>
<link>https://arxiv.org/abs/2505.18421</link>
<guid>https://arxiv.org/abs/2505.18421</guid>
<content:encoded><![CDATA[
<div> Keywords: Aplastic anemia, ICU admission, machine learning, mortality prediction, nomograms

Summary: 
This study utilized the MIMIC-IV database to analyze ICU patients with aplastic anemia, identifying seven key predictors for mortality prediction through machine learning. Logistic regression models outperformed Cox models in predicting 7-, 14-, and 28-day mortality with AUROC values of 0.8227, 0.8311, and 0.8298, respectively. External validation using the eICU database showed AUROCs of 0.7391, 0.7119, and 0.7093. Interactive nomograms based on the logistic regression model were developed to visually estimate individual patient risk, with APS III as a leading predictor. This concise set of predictors provides clinicians with valuable tools for personalized risk stratification and decision-making in ICU patients with aplastic anemia. <br /><br />Summary: <div>
arXiv:2505.18421v1 Announce Type: new 
Abstract: Aplastic anemia is a rare, life-threatening hematologic disorder characterized by pancytopenia and bone marrow failure. ICU admission in these patients often signals critical complications or disease progression, making early risk assessment crucial for clinical decision-making and resource allocation. In this study, we used the MIMIC-IV database to identify ICU patients diagnosed with aplastic anemia and extracted clinical features from five domains: demographics, synthetic indicators, laboratory results, comorbidities, and medications. Over 400 variables were reduced to seven key predictors through machine learning-based feature selection. Logistic regression and Cox regression models were constructed to predict 7-, 14-, and 28-day mortality, and their performance was evaluated using AUROC. External validation was conducted using the eICU Collaborative Research Database to assess model generalizability. Among 1,662 included patients, the logistic regression model demonstrated superior performance, with AUROC values of 0.8227, 0.8311, and 0.8298 for 7-, 14-, and 28-day mortality, respectively, compared to the Cox model. External validation yielded AUROCs of 0.7391, 0.7119, and 0.7093. Interactive nomograms were developed based on the logistic regression model to visually estimate individual patient risk. In conclusion, we identified a concise set of seven predictors, led by APS III, to build validated and generalizable nomograms that accurately estimate short-term mortality in ICU patients with aplastic anemia. These tools may aid clinicians in personalized risk stratification and decision-making at the point of care.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Finite-Time Global Optimality Convergence in Deep Neural Actor-Critic Methods for Decentralized Multi-Agent Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18433</link>
<guid>https://arxiv.org/abs/2505.18433</guid>
<content:encoded><![CDATA[
<div> Keywords: decentralized multi-agent reinforcement learning, actor-critic methods, deep neural networks, global optimality guarantee, convergence rate

Summary: 
Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) allow collaborative decision-making without centralized coordination. Previous theoretical studies were limited to stationary solutions using linear function approximation. This paper introduces a deep neural actor-critic method for decentralized MARL with nonlinear components. The proposed method guarantees global optimality with a finite-time convergence rate of O(1/T), a first in the MARL literature for deep neural actor-critic methods. Numerical experiments support the theoretical findings. The research bridges the gap between practical use of deep neural networks in MARL and theoretical understanding, providing a solid foundation for future developments in this area.<br /><br />Summary: <div>
arXiv:2505.18433v1 Announce Type: new 
Abstract: Actor-critic methods for decentralized multi-agent reinforcement learning (MARL) facilitate collaborative optimal decision making without centralized coordination, thus enabling a wide range of applications in practice. To date, however, most theoretical convergence studies for existing actor-critic decentralized MARL methods are limited to the guarantee of a stationary solution under the linear function approximation. This leaves a significant gap between the highly successful use of deep neural actor-critic for decentralized MARL in practice and the current theoretical understanding. To bridge this gap, in this paper, we make the first attempt to develop a deep neural actor-critic method for decentralized MARL, where both the actor and critic components are inherently non-linear. We show that our proposed method enjoys a global optimality guarantee with a finite-time convergence rate of O(1/T), where T is the total iteration times. This marks the first global convergence result for deep neural actor-critic methods in the MARL literature. We also conduct extensive numerical experiments, which verify our theoretical results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces</title>
<link>https://arxiv.org/abs/2505.18441</link>
<guid>https://arxiv.org/abs/2505.18441</guid>
<content:encoded><![CDATA[
<div> Keywords: Dictionary learning, transformer models, sparse autoencoders, scalable algorithm, disentangling embeddings 

Summary: 
DB-KSVD is introduced as a scalable dictionary learning algorithm that builds upon the classic KSVD algorithm, enabling the disentangling of high-dimensional transformer embeddings. The algorithm is designed to handle large datasets with millions of samples and thousands of dimensions efficiently. Through the application of DB-KSVD to the Gemma-2-2B model, competitive results are achieved on six metrics from the SAEBench benchmark, showcasing its efficacy in comparison to established approaches based on sparse autoencoders. The study demonstrates that SAEs can yield strong solutions to the dictionary learning problem and that traditional optimization methods can be adapted to address the challenges posed by high-dimensional data and large sample sizes. The implementation of DB-KSVD is made available at https://github.com/RomeoV/KSVD.jl. 

<br /><br />Summary: <div>
arXiv:2505.18441v1 Announce Type: new 
Abstract: Dictionary learning has recently emerged as a promising approach for mechanistic interpretability of large transformer models. Disentangling high-dimensional transformer embeddings, however, requires algorithms that scale to high-dimensional data with large sample sizes. Recent work has explored sparse autoencoders (SAEs) for this problem. However, SAEs use a simple linear encoder to solve the sparse encoding subproblem, which is known to be NP-hard. It is therefore interesting to understand whether this structure is sufficient to find good solutions to the dictionary learning problem or if a more sophisticated algorithm could find better solutions. In this work, we propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich theoretical foundations of KSVD but scales to datasets with millions of samples and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics from the SAEBench benchmark, where we achieve competitive results when compared to established approaches based on SAEs. By matching SAE performance with an entirely different optimization approach, our results suggest that (i) SAEs do find strong solutions to the dictionary learning problem and (ii) that traditional optimization approaches can be scaled to the required problem sizes, offering a promising avenue for further research. We provide an implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Breaking Silos: Adaptive Model Fusion Unlocks Better Time Series Forecasting</title>
<link>https://arxiv.org/abs/2505.18442</link>
<guid>https://arxiv.org/abs/2505.18442</guid>
<content:encoded><![CDATA[
<div> TimeFuse, Time-series forecasting, Heterogeneous models, Adaptive fusion, Meta-features<br />
<br />
Summary: TimeFuse is introduced as a framework for collective time-series forecasting using adaptive fusion of heterogeneous models at the sample level. The framework leverages meta-features to characterize input time series and trains a fusor to predict optimal fusion weights for different inputs. By jointly training on diverse datasets, TimeFuse can adapt to a wide range of temporal patterns and generalize to unseen data. Experiment results show TimeFuse's effectiveness in long-/short-term forecasting tasks, surpassing state-of-the-art individual models. The code for TimeFuse is available on GitHub for further exploration and implementation. <div>
arXiv:2505.18442v1 Announce Type: new 
Abstract: Time-series forecasting plays a critical role in many real-world applications. Although increasingly powerful models have been developed and achieved superior results on benchmark datasets, through a fine-grained sample-level inspection, we find that (i) no single model consistently outperforms others across different test samples, but instead (ii) each model excels in specific cases. These findings prompt us to explore how to adaptively leverage the distinct strengths of various forecasting models for different samples. We introduce TimeFuse, a framework for collective time-series forecasting with sample-level adaptive fusion of heterogeneous models. TimeFuse utilizes meta-features to characterize input time series and trains a learnable fusor to predict optimal model fusion weights for any given input. The fusor can leverage samples from diverse datasets for joint training, allowing it to adapt to a wide variety of temporal patterns and thus generalize to new inputs, even from unseen datasets. Extensive experiments demonstrate the effectiveness of TimeFuse in various long-/short-term forecasting tasks, achieving near-universal improvement over the state-of-the-art individual models. Code is available at https://github.com/ZhiningLiu1998/TimeFuse.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Pessimism Principle Can Be Effective: Towards a Framework for Zero-Shot Transfer Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18447</link>
<guid>https://arxiv.org/abs/2505.18447</guid>
<content:encoded><![CDATA[
<div> Keywords: transfer reinforcement learning, performance guarantees, negative transfer, conservative estimation, distributed algorithms

Summary:
Transfer reinforcement learning aims to transfer policies from source domains to a target domain with limited data. This framework addresses two key challenges: the lack of performance guarantees and the risk of negative transfer. By using the pessimism principle, conservative estimations of the target domain's performance are constructed and optimized to ensure safe and reliable decisions. The framework provides an optimized lower bound on target performance, avoiding undesired actions. Moreover, it exhibits monotonic improvement based on the quality of the source domains, minimizing the risk of negative transfer. Two types of conservative estimations are developed, along with efficient distributed algorithms with convergence guarantees. Overall, this framework offers a theoretically sound and practically robust solution for transfer learning in reinforcement learning.<br /><br />Summary:Transfer reinforcement learning framework addresses challenges of performance guarantees and negative transfer through conservative estimations and distributed algorithms, ensuring safe and reliable decisions with improved policy transfer efficiency. <div>
arXiv:2505.18447v1 Announce Type: new 
Abstract: Transfer reinforcement learning aims to derive a near-optimal policy for a target environment with limited data by leveraging abundant data from related source domains. However, it faces two key challenges: the lack of performance guarantees for the transferred policy, which can lead to undesired actions, and the risk of negative transfer when multiple source domains are involved. We propose a novel framework based on the pessimism principle, which constructs and optimizes a conservative estimation of the target domain's performance. Our framework effectively addresses the two challenges by providing an optimized lower bound on target performance, ensuring safe and reliable decisions, and by exhibiting monotonic improvement with respect to the quality of the source domains, thereby avoiding negative transfer. We construct two types of conservative estimations, rigorously characterize their effectiveness, and develop efficient distributed algorithms with convergence guarantees. Our framework provides a theoretically sound and practically robust solution for transfer learning in reinforcement learning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>$\mu$-MoE: Test-Time Pruning as Micro-Grained Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2505.18451</link>
<guid>https://arxiv.org/abs/2505.18451</guid>
<content:encoded><![CDATA[
<div> calibration, activation-aware pruning, $\mu$-MoE, structured sparsity, computational efficiency

Summary:
$\mu$-MoE is introduced as a method to address the computational demands of large foundation models by using activation-aware compression techniques without retraining. By utilizing a computationally efficient calibration, $\mu$-MoE can adaptively perform activation-aware pruning for each prompt, reducing complexity during inference. This approach aims to mitigate domain shift that may occur for unknown downstream tasks. Through a mixture of micro-experts, $\mu$-MoE dynamically adjusts to task and prompt-specific structured sparsity in real time. Experimental results support the effectiveness of $\mu$-MoE in achieving task-dependent compression and improved computational efficiency. The technique demonstrates the potential to optimize the performance of large models while maintaining flexibility and adaptability across various tasks and prompts. 

<br /><br />Summary: <div>
arXiv:2505.18451v1 Announce Type: new 
Abstract: To tackle the huge computational demand of large foundation models, activation-aware compression techniques without retraining have been introduced. However, since these rely on calibration data, domain shift may arise for unknown downstream tasks. With a computationally efficient calibration, activation-aware pruning can be executed for every prompt adaptively, yet achieving reduced complexity at inference. We formulate it as a mixture of micro-experts, called $\mu$-MoE. Several experiments demonstrate that $\mu$-MoE can dynamically adapt to task/prompt-dependent structured sparsity on the fly.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Performance and Generalizability Impacts of Incorporating Geolocation into Deep Learning for Dynamic PM2.5 Estimation</title>
<link>https://arxiv.org/abs/2505.18461</link>
<guid>https://arxiv.org/abs/2505.18461</guid>
<content:encoded><![CDATA[
<div> Keywords: deep learning, geolocation, PM2.5 estimation, location encoders, generalizability 

Summary: 
This paper explores the role of geolocation information in deep learning models for estimating daily PM2.5 levels using remote sensing and ground-level data. Three approaches are evaluated: excluding geolocation, using raw geographic coordinates, and utilizing pretrained location encoders. The study focuses on within-region (WR) and out-of-region (OoR) evaluation scenarios. While raw geographic coordinates improve within-region performance, they may hinder generalizability across regions. Pretrained location encoders like GeoCLIP enhance predictive performance and geographic generalizability in both scenarios. However, artifact patterns and varying performance among location encoders are observed. This research provides valuable insights into the impact of incorporating geolocation information in deep learning models for dynamic and spatially heterogeneous applications such as PM2.5 estimation. 

<br /><br />Summary: <div>
arXiv:2505.18461v1 Announce Type: new 
Abstract: Deep learning models have demonstrated success in geospatial applications, yet quantifying the role of geolocation information in enhancing model performance and geographic generalizability remains underexplored. A new generation of location encoders have emerged with the goal of capturing attributes present at any given location for downstream use in predictive modeling. Being a nascent area of research, their evaluation has remained largely limited to static tasks such as species distributions or average temperature mapping. In this paper, we discuss and quantify the impact of incorporating geolocation into deep learning for a real-world application domain that is characteristically dynamic (with fast temporal change) and spatially heterogeneous at high resolutions: estimating surface-level daily PM2.5 levels using remotely sensed and ground-level data. We build on a recently published deep learning-based PM2.5 estimation model that achieves state-of-the-art performance on data observed in the contiguous United States. We examine three approaches for incorporating geolocation: excluding geolocation as a baseline, using raw geographic coordinates, and leveraging pretrained location encoders. We evaluate each approach under within-region (WR) and out-of-region (OoR) evaluation scenarios. Aggregate performance metrics indicate that while na\"ive incorporation of raw geographic coordinates improves within-region performance by retaining the interpolative value of geographic location, it can hinder generalizability across regions. In contrast, pretrained location encoders like GeoCLIP enhance predictive performance and geographic generalizability for both WR and OoR scenarios. However, qualitative analysis reveals artifact patterns caused by high-degree basis functions and sparse upstream samples in certain areas, and ablation results indicate varying performance among location encoders...
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Using Large Language Models to Tackle Fundamental Challenges in Graph Learning: A Comprehensive Survey</title>
<link>https://arxiv.org/abs/2505.18475</link>
<guid>https://arxiv.org/abs/2505.18475</guid>
<content:encoded><![CDATA[
<div> Graph learning, LLMs, challenges, incomplete data, class imbalance<br />
<br />
Summary: <br />
Graphs are commonly used to represent non-Euclidean data, but real-world graph data presents challenges such as incompleteness, imbalance, cross-domain heterogeneity, and dynamic instability. Traditional graph learning methods struggle to handle these issues effectively. Large Language Models (LLMs) offer a promising solution by leveraging semantic reasoning and external knowledge. This survey explores how LLMs can be integrated with graph learning to address these challenges. It reviews traditional solutions and modern LLM-driven approaches for each challenge, emphasizing the unique advantages that LLMs bring. The survey also discusses open research questions and potential future directions in this interdisciplinary field. A repository of recent advances on graph learning challenges is provided for further exploration. <div>
arXiv:2505.18475v1 Announce Type: new 
Abstract: Graphs are a widely used paradigm for representing non-Euclidean data, with applications ranging from social network analysis to biomolecular prediction. Conventional graph learning approaches typically rely on fixed structural assumptions or fully observed data, limiting their effectiveness in more complex, noisy, or evolving settings. Consequently, real-world graph data often violates the assumptions of traditional graph learning methods, in particular, it leads to four fundamental challenges: (1) Incompleteness, real-world graphs have missing nodes, edges, or attributes; (2) Imbalance, the distribution of the labels of nodes or edges and their structures for real-world graphs are highly skewed; (3) Cross-domain Heterogeneity, graphs from different domains exhibit incompatible feature spaces or structural patterns; and (4) Dynamic Instability, graphs evolve over time in unpredictable ways. Recent advances in Large Language Models (LLMs) offer the potential to tackle these challenges by leveraging rich semantic reasoning and external knowledge. This survey provides a comprehensive review of how LLMs can be integrated with graph learning to address the aforementioned challenges. For each challenge, we review both traditional solutions and modern LLM-driven approaches, highlighting how LLMs contribute unique advantages. Finally, we discuss open research questions and promising future directions in this emerging interdisciplinary field. To support further exploration, we have curated a repository of recent advances on graph learning challenges: https://github.com/limengran98/Awesome-Literature-Graph-Learning-Challenges.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>The Prompt is Mightier than the Example</title>
<link>https://arxiv.org/abs/2505.18485</link>
<guid>https://arxiv.org/abs/2505.18485</guid>
<content:encoded><![CDATA[
<div> Knowledge-Guided Prompting, Prompt Optimization, In-Context Learning, Large Language Models, Synthetic Data Generation
<br />
<br />
Summary:
The paper introduces Knowledge-Guided Prompting (KGP) as a new approach in prompt optimization for large language models (LLMs). It explores how domain knowledge can be injected into prompts to reduce the need for a large number of in-context learning (ICL) examples in synthetic data generation. The study investigates the trade-off between ICL and KGP, showing that the quality of synthetic data can be improved by incorporating domain knowledge into prompts. The experiments reveal an empirical scaling law that quantifies the impact of increasing domain knowledge and decreasing example count on data generation quality. The results suggest that knowledge-guided prompting can be a scalable alternative to or complement of ICL, offering new possibilities for generating high-fidelity synthetic data. <div>
arXiv:2505.18485v1 Announce Type: new 
Abstract: Numerous recent prompt optimization approaches like chain-of-thought, have been demonstrated to significantly improve the quality of content generated by large language models (LLMs). In-context learning (ICL), a recent paradigm where a few representative examples guide content generation has also led to strong improvements in generation quality of LLM generated content. This idea has been applied to great effect in synthetic tabular data generation, where LLMs, through effective use of ICL and prompt optimization, can generate data that approximate samples from complex, heterogeneous distributions based on representative examples. However, ensuring high-fidelity synthetic data often requires a very large number of ICL examples which may be unavailable or costly to obtain. At the same time, as LLMs get larger and larger, their in-built prior knowledge becomes vast and can potentially substitute for specific data examples. In this paper, we introduce Knowledge-Guided Prompting (KGP) as a new knob in prompt optimization and explore the ability of KGP-based prompt optimization to offset the cost of ICL. Specifically, we explore the question `how many examples can a prompt substitute for?' and explore knowledge-guided prompting (KGP) where domain knowledge, either inferred or available, is explicitly injected into the prompt, reducing dependence on ICL examples. Our experiments systematically explore the trade-off between ICL and KGP, revealing an empirical scaling law that quantifies how quality of generated synthetic data varies with increasing domain knowledge and decreasing example count. Our results demonstrate that knowledge-guided prompting can be a scalable alternative, or addition, to in-context examples, unlocking new approaches to synthetic data generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Synthesizing and Adapting Error Correction Data for Mobile Large Language Model Applications</title>
<link>https://arxiv.org/abs/2505.18488</link>
<guid>https://arxiv.org/abs/2505.18488</guid>
<content:encoded><![CDATA[
<div> error correction, language models, mobile applications, dataset, evaluation <br />
<br />
Summary: 
This paper focuses on using large language models (LLMs) to create a dataset for error correction in mobile applications. The authors prompt LLMs with error correction knowledge to generate high-quality data. They then adjust the data distribution to match the mobile application domain by reweighting the samples. The reweighting model is trained on live A/B test metrics to ensure the LLMs perform well in production. Best practices for combining synthetic data with other sources are also discussed to improve model performance in both offline evaluation and live A/B testing. This approach aims to enhance user typing experience on mobile devices by leveraging the capabilities of LLMs in error correction tasks. <div>
arXiv:2505.18488v1 Announce Type: new 
Abstract: Error correction is an important capability when applying large language models (LLMs) to facilitate user typing on mobile devices. In this paper, we use LLMs to synthesize a high-quality dataset of error correction pairs to evaluate and improve LLMs for mobile applications. We first prompt LLMs with error correction domain knowledge to build a scalable and reliable addition to the existing data synthesis pipeline. We then adapt the synthetic data distribution to match the mobile application domain by reweighting the samples. The reweighting model is learnt by predicting (a handful of) live A/B test metrics when deploying LLMs in production, given the LLM performance on offline evaluation data and scores from a small privacy-preserving on-device language model. Finally, we present best practices for mixing our synthetic data with other data sources to improve model performance on error correction in both offline evaluation and production live A/B testing.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedHL: Federated Learning for Heterogeneous Low-Rank Adaptation via Unbiased Aggregation</title>
<link>https://arxiv.org/abs/2505.18494</link>
<guid>https://arxiv.org/abs/2505.18494</guid>
<content:encoded><![CDATA[
<div> Federated Learning, Low-Rank Adaptation, Foundation Models, Convergence Guarantees, Gradient Updates <br />
Summary: <br />
The article introduces FedHL, a Federated Learning framework designed for Heterogeneous Low-Rank Adaptation (LoRA). Existing methods lack formal convergence guarantees due to parameter truncation and biased gradient updates. FedHL leverages a full-rank global model as the aggregation basis, eliminating truncation bias and minimizing gradient drift for optimal aggregation weights. The framework ensures a convergence rate of O(1/sqrt{T}) and outperforms state-of-the-art methods by 1-3% in real-world datasets. <div>
arXiv:2505.18494v1 Announce Type: new 
Abstract: Federated Learning (FL) facilitates the fine-tuning of Foundation Models (FMs) using distributed data sources, with Low-Rank Adaptation (LoRA) gaining popularity due to its low communication costs and strong performance. While recent work acknowledges the benefits of heterogeneous LoRA in FL and introduces flexible algorithms to support its implementation, our theoretical analysis reveals a critical gap: existing methods lack formal convergence guarantees due to parameter truncation and biased gradient updates. Specifically, adapting client-specific LoRA ranks necessitates truncating global parameters, which introduces inherent truncation errors and leads to subsequent inaccurate gradient updates that accumulate over training rounds, ultimately degrading performance. To address the above issues, we propose \textbf{FedHL}, a simple yet effective \textbf{Fed}erated Learning framework tailored for \textbf{H}eterogeneous \textbf{L}oRA. By leveraging the full-rank global model as a calibrated aggregation basis, FedHL eliminates the direct truncation bias from initial alignment with client-specific ranks. Furthermore, we derive the theoretically optimal aggregation weights by minimizing the gradient drift term in the convergence upper bound. Our analysis shows that FedHL guarantees $\mathcal{O}(1/\sqrt{T})$ convergence rate, and experiments on multiple real-world datasets demonstrate a 1-3\% improvement over several state-of-the-art methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Beyond Masked and Unmasked: Discrete Diffusion Models via Partial Masking</title>
<link>https://arxiv.org/abs/2505.18495</link>
<guid>https://arxiv.org/abs/2505.18495</guid>
<content:encoded><![CDATA[
arXiv:2505.18495v1 Announce Type: new 
Abstract: Masked diffusion models (MDM) are powerful generative models for discrete data that generate samples by progressively unmasking tokens in a sequence. Each token can take one of two states: masked or unmasked. We observe that token sequences often remain unchanged between consecutive sampling steps; consequently, the model repeatedly processes identical inputs, leading to redundant computation. To address this inefficiency, we propose the Partial masking scheme (Prime), which augments MDM by allowing tokens to take intermediate states interpolated between the masked and unmasked states. This design enables the model to make predictions based on partially observed token information, and facilitates a fine-grained denoising process. We derive a variational training objective and introduce a simple architectural design to accommodate intermediate-state inputs. Our method demonstrates superior performance across a diverse set of generative modeling tasks. On text data, it achieves a perplexity of 15.36 on OpenWebText, outperforming previous MDM (21.52), autoregressive models (17.54), and their hybrid variants (17.58), without relying on an autoregressive formulation. On image data, it attains competitive FID scores of 3.26 on CIFAR-10 and 6.98 on ImageNet-32, comparable to leading continuous generative models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>G1: Teaching LLMs to Reason on Graphs with Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18499</link>
<guid>https://arxiv.org/abs/2505.18499</guid>
<content:encoded><![CDATA[
arXiv:2505.18499v1 Announce Type: new 
Abstract: Although Large Language Models (LLMs) have demonstrated remarkable progress, their proficiency in graph-related tasks remains notably limited, hindering the development of truly general-purpose models. Previous attempts, including pretraining graph foundation models or employing supervised fine-tuning, often face challenges such as the scarcity of large-scale, universally represented graph data. We introduce G1, a simple yet effective approach demonstrating that Reinforcement Learning (RL) on synthetic graph-theoretic tasks can significantly scale LLMs' graph reasoning abilities. To enable RL training, we curate Erd\~os, the largest graph reasoning dataset to date comprising 50 diverse graph-theoretic tasks of varying difficulty levels, 100k training data and 5k test data, all drived from real-world graphs. With RL on Erd\~os, G1 obtains substantial improvements in graph reasoning, where our finetuned 3B model even outperforms Qwen2.5-72B-Instruct (24x size). RL-trained models also show strong zero-shot generalization to unseen tasks, domains, and graph encoding schemes, including other graph-theoretic benchmarks as well as real-world node classification and link prediction tasks, without compromising general reasoning abilities. Our findings offer an efficient, scalable path for building strong graph reasoners by finetuning LLMs with RL on graph-theoretic tasks, which combines the strengths of pretrained LLM capabilities with abundant, automatically generated synthetic data, suggesting that LLMs possess graph understanding abilities that RL can elicit successfully.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How Particle System Theory Enhances Hypergraph Message Passing</title>
<link>https://arxiv.org/abs/2505.18505</link>
<guid>https://arxiv.org/abs/2505.18505</guid>
<content:encoded><![CDATA[
arXiv:2505.18505v1 Announce Type: new 
Abstract: Hypergraphs effectively model higher-order relationships in natural phenomena, capturing complex interactions beyond pairwise connections. We introduce a novel hypergraph message passing framework inspired by interacting particle systems, where hyperedges act as fields inducing shared node dynamics. By incorporating attraction, repulsion, and Allen-Cahn forcing terms, particles of varying classes and features achieve class-dependent equilibrium, enabling separability through the particle-driven message passing. We investigate both first-order and second-order particle system equations for modeling these dynamics, which mitigate over-smoothing and heterophily thus can capture complete interactions. The more stable second-order system permits deeper message passing. Furthermore, we enhance deterministic message passing with stochastic element to account for interaction uncertainties. We prove theoretically that our approach mitigates over-smoothing by maintaining a positive lower bound on the hypergraph Dirichlet energy during propagation and thus to enable hypergraph message passing to go deep. Empirically, our models demonstrate competitive performance on diverse real-world hypergraph node classification tasks, excelling on both homophilic and heterophilic datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>SPDEBench: An Extensive Benchmark for Learning Regular and Singular Stochastic PDEs</title>
<link>https://arxiv.org/abs/2505.18511</link>
<guid>https://arxiv.org/abs/2505.18511</guid>
<content:encoded><![CDATA[
arXiv:2505.18511v1 Announce Type: new 
Abstract: Stochastic Partial Differential Equations (SPDEs) driven by random noise play a central role in modelling physical processes whose spatio-temporal dynamics can be rough, such as turbulence flows, superconductors, and quantum dynamics. To efficiently model these processes and make predictions, machine learning (ML)-based surrogate models are proposed, with their network architectures incorporating the spatio-temporal roughness in their design. However, it lacks an extensive and unified datasets for SPDE learning; especially, existing datasets do not account for the computational error introduced by noise sampling and the necessary renormalization required for handling singular SPDEs. We thus introduce SPDEBench, which is designed to solve typical SPDEs of physical significance (e.g., the $\Phi^4_d$, wave, incompressible Navier--Stokes, and KdV equations) on 1D or 2D tori driven by white noise via ML methods. New datasets for singular SPDEs based on the renormalization process have been constructed, and novel ML models achieving the best results to date have been proposed. In particular, we investigate the impact of computational error introduced by noise sampling and renormalization on the performance comparison of ML models and highlight the importance of selecting high-quality test data for accurate evaluation. Results are benchmarked with traditional numerical solvers and ML-based models, including FNO, NSPDE and DLR-Net, etc. It is shown that, for singular SPDEs, naively applying ML models on data without specifying the numerical schemes can lead to significant errors and misleading conclusions. Our SPDEBench provides an open-source codebase that ensures full reproducibility of benchmarking across a variety of SPDE datasets while offering the flexibility to incorporate new datasets and machine learning baselines, making it a valuable resource for the community.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Training Data Attribution with Representational Optimization</title>
<link>https://arxiv.org/abs/2505.18513</link>
<guid>https://arxiv.org/abs/2505.18513</guid>
<content:encoded><![CDATA[
arXiv:2505.18513v1 Announce Type: new 
Abstract: Training data attribution (TDA) methods aim to measure how training data impacts a model's predictions. While gradient-based attribution methods, such as influence functions, offer theoretical grounding, their computational costs make them impractical for large-scale applications. Representation-based approaches are far more scalable, but typically rely on heuristic embeddings that are not optimized for attribution, limiting their fidelity. To address these challenges, we propose AirRep, a scalable, representation-based approach that closes this gap by learning task-specific and model-aligned representations optimized explicitly for TDA. AirRep introduces two key innovations: a trainable encoder tuned for attribution quality, and an attention-based pooling mechanism that enables accurate estimation of group-wise influence. We train AirRep using a ranking objective over automatically constructed training subsets labeled by their empirical effect on target predictions. Experiments on instruction-tuned LLMs demonstrate that AirRep achieves performance on par with state-of-the-art gradient-based approaches while being nearly two orders of magnitude more efficient at inference time. Further analysis highlights its robustness and generalization across tasks and models. Our code is available at https://github.com/sunnweiwei/AirRep.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Test-Time Adaptation with Binary Feedback</title>
<link>https://arxiv.org/abs/2505.18514</link>
<guid>https://arxiv.org/abs/2505.18514</guid>
<content:encoded><![CDATA[
arXiv:2505.18514v1 Announce Type: new 
Abstract: Deep learning models perform poorly when domain shifts exist between training and test data. Test-time adaptation (TTA) is a paradigm to mitigate this issue by adapting pre-trained models using only unlabeled test samples. However, existing TTA methods can fail under severe domain shifts, while recent active TTA approaches requiring full-class labels are impractical due to high labeling costs. To address this issue, we introduce a new setting of TTA with binary feedback. This setting uses a few binary feedback inputs from annotators to indicate whether model predictions are correct, thereby significantly reducing the labeling burden of annotators. Under the setting, we propose BiTTA, a novel dual-path optimization framework that leverages reinforcement learning to balance binary feedback-guided adaptation on uncertain samples with agreement-based self-adaptation on confident predictions. Experiments show BiTTA achieves 13.3%p accuracy improvements over state-of-the-art baselines, demonstrating its effectiveness in handling severe distribution shifts with minimal labeling effort. The source code is available at https://github.com/taeckyung/BiTTA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CLaDMoP: Learning Transferrable Models from Successful Clinical Trials via LLMs</title>
<link>https://arxiv.org/abs/2505.18527</link>
<guid>https://arxiv.org/abs/2505.18527</guid>
<content:encoded><![CDATA[
arXiv:2505.18527v1 Announce Type: new 
Abstract: Many existing models for clinical trial outcome prediction are optimized using task-specific loss functions on trial phase-specific data. While this scheme may boost prediction for common diseases and drugs, it can hinder learning of generalizable representations, leading to more false positives/negatives. To address this limitation, we introduce CLaDMoP, a new pre-training approach for clinical trial outcome prediction, alongside the Successful Clinical Trials dataset(SCT), specifically designed for this task. CLaDMoP leverages a Large Language Model-to encode trials' eligibility criteria-linked to a lightweight Drug-Molecule branch through a novel multi-level fusion technique. To efficiently fuse long embeddings across levels, we incorporate a grouping block, drastically reducing computational overhead. CLaDMoP avoids reliance on task-specific objectives by pre-training on a "pair matching" proxy task. Compared to established zero-shot and few-shot baselines, our method significantly improves both PR-AUC and ROC-AUC, especially for phase I and phase II trials. We further evaluate and perform ablation on CLaDMoP after Parameter-Efficient Fine-Tuning, comparing it to state-of-the-art supervised baselines, including MEXA-CTP, on the Trial Outcome Prediction(TOP) benchmark. CLaDMoP achieves up to 10.5% improvement in PR-AUC and 3.6% in ROC-AUC, while attaining comparable F1 score to MEXA-CTP, highlighting its potential for clinical trial outcome prediction. Code and SCT dataset can be downloaded from https://github.com/murai-lab/CLaDMoP.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Preserving AUC Fairness in Learning with Noisy Protected Groups</title>
<link>https://arxiv.org/abs/2505.18532</link>
<guid>https://arxiv.org/abs/2505.18532</guid>
<content:encoded><![CDATA[
arXiv:2505.18532v1 Announce Type: new 
Abstract: The Area Under the ROC Curve (AUC) is a key metric for classification, especially under class imbalance, with growing research focus on optimizing AUC over accuracy in applications like medical image analysis and deepfake detection. This leads to fairness in AUC optimization becoming crucial as biases can impact protected groups. While various fairness mitigation techniques exist, fairness considerations in AUC optimization remain in their early stages, with most research focusing on improving AUC fairness under the assumption of clean protected groups. However, these studies often overlook the impact of noisy protected groups, leading to fairness violations in practice. To address this, we propose the first robust AUC fairness approach under noisy protected groups with fairness theoretical guarantees using distributionally robust optimization. Extensive experiments on tabular and image datasets show that our method outperforms state-of-the-art approaches in preserving AUC fairness. The code is in https://github.com/Purdue-M2/AUC_Fairness_with_Noisy_Groups.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Convergence, Sticking and Escape: Stochastic Dynamics Near Critical Points in SGD</title>
<link>https://arxiv.org/abs/2505.18535</link>
<guid>https://arxiv.org/abs/2505.18535</guid>
<content:encoded><![CDATA[
arXiv:2505.18535v1 Announce Type: new 
Abstract: We study the convergence properties and escape dynamics of Stochastic Gradient Descent (SGD) in one-dimensional landscapes, separately considering infinite- and finite-variance noise. Our main focus is to identify the time scales on which SGD reliably moves from an initial point to the local minimum in the same ''basin''. Under suitable conditions on the noise distribution, we prove that SGD converges to the basin's minimum unless the initial point lies too close to a local maximum. In that near-maximum scenario, we show that SGD can linger for a long time in its neighborhood. For initial points near a ''sharp'' maximum, we show that SGD does not remain stuck there, and we provide results to estimate the probability that it will reach each of the two neighboring minima. Overall, our findings present a nuanced view of SGD's transitions between local maxima and minima, influenced by both noise characteristics and the underlying function geometry.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>B-score: Detecting biases in large language models using response history</title>
<link>https://arxiv.org/abs/2505.18545</link>
<guid>https://arxiv.org/abs/2505.18545</guid>
<content:encoded><![CDATA[
arXiv:2505.18545v1 Announce Type: new 
Abstract: Large language models (LLMs) often exhibit strong biases, e.g, against women or in favor of the number 7. We investigate whether LLMs would be able to output less biased answers when allowed to observe their prior answers to the same question in a multi-turn conversation. To understand which types of questions invite more biased answers, we test LLMs on our proposed set of questions that span 9 topics and belong to three types: (1) Subjective; (2) Random; and (3) Objective. Interestingly, LLMs are able to "de-bias" themselves in a multi-turn conversation in response to questions that seek an Random, unbiased answer. Furthermore, we propose B-score, a novel metric that is effective in detecting biases to Subjective, Random, Easy, and Hard questions. On MMLU, HLE, and CSQA, leveraging B-score substantially improves the verification accuracy of LLM answers (i.e, accepting LLM correct answers and rejecting incorrect ones) compared to using verbalized confidence scores or the frequency of single-turn answers alone. Code and data are available at: https://b-score.github.io.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Joint-stochastic-approximation Autoencoders with Application to Semi-supervised Learning</title>
<link>https://arxiv.org/abs/2505.18558</link>
<guid>https://arxiv.org/abs/2505.18558</guid>
<content:encoded><![CDATA[
arXiv:2505.18558v1 Announce Type: new 
Abstract: Our examination of existing deep generative models (DGMs), including VAEs and GANs, reveals two problems. First, their capability in handling discrete observations and latent codes is unsatisfactory, though there are interesting efforts. Second, both VAEs and GANs optimize some criteria that are indirectly related to the data likelihood. To address these problems, we formally present Joint-stochastic-approximation (JSA) autoencoders - a new family of algorithms for building deep directed generative models, with application to semi-supervised learning. The JSA learning algorithm directly maximizes the data log-likelihood and simultaneously minimizes the inclusive KL divergence the between the posteriori and the inference model. We provide theoretical results and conduct a series of experiments to show its superiority such as being robust to structure mismatch between encoder and decoder, consistent handling of both discrete and continuous variables. Particularly we empirically show that JSA autoencoders with discrete latent space achieve comparable performance to other state-of-the-art DGMs with continuous latent space in semi-supervised tasks over the widely adopted datasets - MNIST and SVHN. To the best of our knowledge, this is the first demonstration that discrete latent variable models are successfully applied in the challenging semi-supervised tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning Fluid-Structure Interaction Dynamics with Physics-Informed Neural Networks and Immersed Boundary Methods</title>
<link>https://arxiv.org/abs/2505.18565</link>
<guid>https://arxiv.org/abs/2505.18565</guid>
<content:encoded><![CDATA[
arXiv:2505.18565v1 Announce Type: new 
Abstract: We introduce neural network architectures that combine physics-informed neural networks (PINNs) with the immersed boundary method (IBM) to solve fluid-structure interaction (FSI) problems. Our approach features two distinct architectures: a Single-FSI network with a unified parameter space, and an innovative Eulerian-Lagrangian network that maintains separate parameter spaces for fluid and structure domains. We study each architecture using standard Tanh and adaptive B-spline activation functions. Empirical studies on a 2D cavity flow problem involving a moving solid structure show that the Eulerian-Lagrangian architecture performs significantly better. The adaptive B-spline activation further enhances accuracy by providing locality-aware representation near boundaries. While our methodology shows promising results in predicting the velocity field, pressure recovery remains challenging due to the absence of explicit force-coupling constraints in the current formulation. Our findings underscore the importance of domain-specific architectural design and adaptive activation functions for modeling FSI problems within the PINN framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learning without Isolation: Pathway Protection for Continual Learning</title>
<link>https://arxiv.org/abs/2505.18568</link>
<guid>https://arxiv.org/abs/2505.18568</guid>
<content:encoded><![CDATA[
arXiv:2505.18568v1 Announce Type: new 
Abstract: Deep networks are prone to catastrophic forgetting during sequential task learning, i.e., losing the knowledge about old tasks upon learning new tasks. To this end, continual learning(CL) has emerged, whose existing methods focus mostly on regulating or protecting the parameters associated with the previous tasks. However, parameter protection is often impractical, since the size of parameters for storing the old-task knowledge increases linearly with the number of tasks, otherwise it is hard to preserve the parameters related to the old-task knowledge. In this work, we bring a dual opinion from neuroscience and physics to CL: in the whole networks, the pathways matter more than the parameters when concerning the knowledge acquired from the old tasks. Following this opinion, we propose a novel CL framework, learning without isolation(LwI), where model fusion is formulated as graph matching and the pathways occupied by the old tasks are protected without being isolated. Thanks to the sparsity of activation channels in a deep network, LwI can adaptively allocate available pathways for a new task, realizing pathway protection and addressing catastrophic forgetting in a parameter-efficient manner. Experiments on popular benchmark datasets demonstrate the superiority of the proposed LwI.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>VISTA: Vision-Language Inference for Training-Free Stock Time-Series Analysis</title>
<link>https://arxiv.org/abs/2505.18570</link>
<guid>https://arxiv.org/abs/2505.18570</guid>
<content:encoded><![CDATA[
arXiv:2505.18570v1 Announce Type: new 
Abstract: Stock price prediction remains a complex and high-stakes task in financial analysis, traditionally addressed using statistical models or, more recently, language models. In this work, we introduce VISTA (Vision-Language Inference for Stock Time-series Analysis), a novel, training-free framework that leverages Vision-Language Models (VLMs) for multi-modal stock forecasting. VISTA prompts a VLM with both textual representations of historical stock prices and their corresponding line charts to predict future price values. By combining numerical and visual modalities in a zero-shot setting and using carefully designed chain-of-thought prompts, VISTA captures complementary patterns that unimodal approaches often miss. We benchmark VISTA against standard baselines, including ARIMA and text-only LLM-based prompting methods. Experimental results show that VISTA outperforms these baselines by up to 89.83%, demonstrating the effectiveness of multi-modal inference for stock time-series analysis and highlighting the potential of VLMs in financial forecasting tasks without requiring task-specific training.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Enhancing Efficiency and Exploration in Reinforcement Learning for LLMs</title>
<link>https://arxiv.org/abs/2505.18573</link>
<guid>https://arxiv.org/abs/2505.18573</guid>
<content:encoded><![CDATA[
arXiv:2505.18573v1 Announce Type: new 
Abstract: Reasoning large language models (LLMs) excel in complex tasks, which has drawn significant attention to reinforcement learning (RL) for LLMs. However, existing approaches allocate an equal number of rollouts to all questions during the RL process, which is inefficient. This inefficiency stems from the fact that training on simple questions yields limited gains, whereas more rollouts are needed for challenging questions to sample correct answers. Furthermore, while RL improves response precision, it limits the model's exploration ability, potentially resulting in a performance cap below that of the base model prior to RL. To address these issues, we propose a mechanism for dynamically allocating rollout budgets based on the difficulty of the problems, enabling more efficient RL training. Additionally, we introduce an adaptive dynamic temperature adjustment strategy to maintain the entropy at a stable level, thereby encouraging sufficient exploration. This enables LLMs to improve response precision while preserving their exploratory ability to uncover potential correct pathways. The code and data is available on: https://github.com/LiaoMengqi/E3-RL4LLMs
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Mechanical in-sensor computing: a programmable meta-sensor for structural damage classification without external electronic power</title>
<link>https://arxiv.org/abs/2505.18579</link>
<guid>https://arxiv.org/abs/2505.18579</guid>
<content:encoded><![CDATA[
arXiv:2505.18579v1 Announce Type: new 
Abstract: Structural health monitoring (SHM) involves sensor deployment, data acquisition, and data interpretation, commonly implemented via a tedious wired system. The information processing in current practice majorly depends on electronic computers, albeit with universal applications, delivering challenges such as high energy consumption and low throughput due to the nature of digital units. In recent years, there has been a renaissance interest in shifting computations from electronic computing units to the use of real physical systems, a concept known as physical computation. This approach provides the possibility of thinking out of the box for SHM, seamlessly integrating sensing and computing into a pure-physical entity, without relying on external electronic power supplies, thereby properly coping with resource-restricted scenarios. The latest advances of metamaterials (MM) hold great promise for this proactive idea. In this paper, we introduce a programmable metamaterial-based sensor (termed as MM-sensor) for physically processing structural vibration information to perform specific SHM tasks, such as structural damage warning (binary classification) in this initiation, without the need for further information processing or resource-consuming, that is, the data collection and analysis are completed in-situ at the sensor level. We adopt the configuration of a locally resonant metamaterial plate (LRMP) to achieve the first fabrication of the MM-sensor. We take advantage of the bandgap properties of LRMP to physically differentiate the dynamic behavior of structures before and after damage. By inversely designing the geometric parameters, our current approach allows for adjustments to the bandgap features. This is effective for engineering systems with a first natural frequency ranging from 9.54 Hz to 81.86 Hz.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Bayesian Meta-Reinforcement Learning with Laplace Variational Recurrent Networks</title>
<link>https://arxiv.org/abs/2505.18591</link>
<guid>https://arxiv.org/abs/2505.18591</guid>
<content:encoded><![CDATA[
arXiv:2505.18591v1 Announce Type: new 
Abstract: Meta-reinforcement learning trains a single reinforcement learning agent on a distribution of tasks to quickly generalize to new tasks outside of the training set at test time. From a Bayesian perspective, one can interpret this as performing amortized variational inference on the posterior distribution over training tasks. Among the various meta-reinforcement learning approaches, a common method is to represent this distribution with a point-estimate using a recurrent neural network. We show how one can augment this point estimate to give full distributions through the Laplace approximation, either at the start of, during, or after learning, without modifying the base model architecture. With our approximation, we are able to estimate distribution statistics (e.g., the entropy) of non-Bayesian agents and observe that point-estimate based methods produce overconfident estimators while not satisfying consistency. Furthermore, when comparing our approach to full-distribution based learning of the task posterior, our method performs on par with variational baselines while having much fewer parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MisoDICE: Multi-Agent Imitation from Unlabeled Mixed-Quality Demonstrations</title>
<link>https://arxiv.org/abs/2505.18595</link>
<guid>https://arxiv.org/abs/2505.18595</guid>
<content:encoded><![CDATA[
arXiv:2505.18595v1 Announce Type: new 
Abstract: We study offline imitation learning (IL) in cooperative multi-agent settings, where demonstrations have unlabeled mixed quality - containing both expert and suboptimal trajectories. Our proposed solution is structured in two stages: trajectory labeling and multi-agent imitation learning, designed jointly to enable effective learning from heterogeneous, unlabeled data. In the first stage, we combine advances in large language models and preference-based reinforcement learning to construct a progressive labeling pipeline that distinguishes expert-quality trajectories. In the second stage, we introduce MisoDICE, a novel multi-agent IL algorithm that leverages these labels to learn robust policies while addressing the computational complexity of large joint state-action spaces. By extending the popular single-agent DICE framework to multi-agent settings with a new value decomposition and mixing architecture, our method yields a convex policy optimization objective and ensures consistency between global and local policies. We evaluate MisoDICE on multiple standard multi-agent RL benchmarks and demonstrate superior performance, especially when expert data is scarce.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exemplar-Free Continual Learning for State Space Models</title>
<link>https://arxiv.org/abs/2505.18604</link>
<guid>https://arxiv.org/abs/2505.18604</guid>
<content:encoded><![CDATA[
arXiv:2505.18604v1 Announce Type: new 
Abstract: State-Space Models (SSMs) excel at capturing long-range dependencies with structured recurrence, making them well-suited for sequence modeling. However, their evolving internal states pose challenges in adapting them under Continual Learning (CL). This is particularly difficult in exemplar-free settings, where the absence of prior data leaves updates to the dynamic SSM states unconstrained, resulting in catastrophic forgetting. To address this, we propose Inf-SSM, a novel and simple geometry-aware regularization method that utilizes the geometry of the infinite-dimensional Grassmannian to constrain state evolution during CL. Unlike classical continual learning methods that constrain weight updates, Inf-SSM regularizes the infinite-horizon evolution of SSMs encoded in their extended observability subspace. We show that enforcing this regularization requires solving a matrix equation known as the Sylvester equation, which typically incurs $\mathcal{O}(n^3)$ complexity. We develop a $\mathcal{O}(n^2)$ solution by exploiting the structure and properties of SSMs. This leads to an efficient regularization mechanism that can be seamlessly integrated into existing CL methods. Comprehensive experiments on challenging benchmarks, including ImageNet-R and Caltech-256, demonstrate a significant reduction in forgetting while improving accuracy across sequential tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Trust, or Don't Predict: Introducing the CWSA Family for Confidence-Aware Model Evaluation</title>
<link>https://arxiv.org/abs/2505.18622</link>
<guid>https://arxiv.org/abs/2505.18622</guid>
<content:encoded><![CDATA[
arXiv:2505.18622v1 Announce Type: new 
Abstract: In recent machine learning systems, confidence scores are being utilized more and more to manage selective prediction, whereby a model can abstain from making a prediction when it is unconfident. Yet, conventional metrics like accuracy, expected calibration error (ECE), and area under the risk-coverage curve (AURC) do not capture the actual reliability of predictions. These metrics either disregard confidence entirely, dilute valuable localized information through averaging, or neglect to suitably penalize overconfident misclassifications, which can be particularly detrimental in real-world systems. We introduce two new metrics Confidence-Weighted Selective Accuracy (CWSA) and its normalized variant CWSA+ that offer a principled and interpretable way to evaluate predictive models under confidence thresholds. Unlike existing methods, our metrics explicitly reward confident accuracy and penalize overconfident mistakes. They are threshold-local, decomposable, and usable in both evaluation and deployment settings where trust and risk must be quantified. Through exhaustive experiments on both real-world data sets (MNIST, CIFAR-10) and artificial model variants (calibrated, overconfident, underconfident, random, perfect), we show that CWSA and CWSA+ both effectively detect nuanced failure modes and outperform classical metrics in trust-sensitive tests. Our results confirm that CWSA is a sound basis for developing and assessing selective prediction systems for safety-critical domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Think Before You Accept: Semantic Reflective Verification for Faster Speculative Decoding</title>
<link>https://arxiv.org/abs/2505.18629</link>
<guid>https://arxiv.org/abs/2505.18629</guid>
<content:encoded><![CDATA[
arXiv:2505.18629v1 Announce Type: new 
Abstract: Large language models (LLMs) suffer from high inference latency due to the auto-regressive decoding process. Speculative decoding accelerates inference by generating multiple draft tokens using a lightweight model and verifying them in parallel. However, existing verification methods rely heavily on distributional consistency while overlooking semantic correctness, thereby limiting the potential speedup of speculative decoding. While some methods employ additional models for relaxed verification of draft tokens, they often fail to generalize effectively to more diverse or open-domain settings. In this work, we propose Reflective Verification, a training-free and semantics-aware approach that achieves a better trade-off between correctness and efficiency. Specifically, we leverage the inherent reflective capacity of LLMs to semantically assess the correctness of draft tokens in parallel during verification. Using prompt-based probing, we obtain both the original and reflective distributions of draft tokens in a single forward pass. The fusion of these distributions enables semantic-level verification of draft tokens that incorporates both consistency and correctness. Experiments across multiple domain benchmarks and model scales demonstrate that our method significantly increases the acceptance length of draft tokens without compromising model performance. Furthermore, we find that the proposed Reflective Verification is orthogonal to existing statistical verification methods, and their combination yields additional 5$\sim$15\% improvements in decoding speed.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Asymmetric Duos: Sidekicks Improve Uncertainty</title>
<link>https://arxiv.org/abs/2505.18636</link>
<guid>https://arxiv.org/abs/2505.18636</guid>
<content:encoded><![CDATA[
arXiv:2505.18636v1 Announce Type: new 
Abstract: The go-to strategy to apply deep networks in settings where uncertainty informs decisions--ensembling multiple training runs with random initializations--is ill-suited for the extremely large-scale models and practical fine-tuning workflows of today. We introduce a new cost-effective strategy for improving the uncertainty quantification and downstream decisions of a large model (e.g. a fine-tuned ViT-B): coupling it with a less accurate but much smaller "sidekick" (e.g. a fine-tuned ResNet-34) with a fraction of the computational cost. We propose aggregating the predictions of this \emph{Asymmetric Duo} by simple learned weighted averaging. Surprisingly, despite their inherent asymmetry, the sidekick model almost never harms the performance of the larger model. In fact, across five image classification benchmarks and a variety of model architectures and training schemes (including soups), Asymmetric Duos significantly improve accuracy, uncertainty quantification, and selective classification metrics with only ${\sim}10-20\%$ more computation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ThanoRA: Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation</title>
<link>https://arxiv.org/abs/2505.18640</link>
<guid>https://arxiv.org/abs/2505.18640</guid>
<content:encoded><![CDATA[
arXiv:2505.18640v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is widely adopted for downstream fine-tuning of foundation models due to its efficiency and zero additional inference cost. Many real-world applications require foundation models to specialize in multiple tasks simultaneously, motivating the need for efficient multi-task adaptation. While recent approaches integrate LoRA with mixture-of-experts (MoE) to address this, the use of routers prevents parameter mergeability, which increases inference overhead and hinders unified multi-task adaptation, thereby limiting deployment practicality. In this work, we propose ThanoRA, a Task Heterogeneity-Aware Multi-Task Low-Rank Adaptation framework that enables multi-task adaptation while preserving the inference efficiency of LoRA. ThanoRA jointly models task heterogeneity and mitigates subspace interference throughout training. Specifically, motivated by inherent differences in complexity and heterogeneity across tasks, ThanoRA constructs task-specific LoRA subspaces at initialization, enabling fine-grained knowledge injection aligned with task heterogeneity. Furthermore, to prevent task interference and subspace collapse during multi-task training, ThanoRA introduces a subspace-preserving regularization that maintains the independence of task-specific representations. With the synergy of both components, ThanoRA enables efficient and unified multi-task adaptation. Extensive experiments across multimodal and text-only benchmarks under varying multi-task mixtures demonstrate that ThanoRA consistently achieves robust and superior performance over strong baselines without introducing additional inference overhead. Our code is publicly available at: https://github.com/LiangJian24/ThanoRA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Flow Matching for Geometric Trajectory Simulation</title>
<link>https://arxiv.org/abs/2505.18647</link>
<guid>https://arxiv.org/abs/2505.18647</guid>
<content:encoded><![CDATA[
arXiv:2505.18647v1 Announce Type: new 
Abstract: The simulation of N-body systems is a fundamental problem with applications in a wide range of fields, such as molecular dynamics, biochemistry, and pedestrian dynamics. Machine learning has become an invaluable tool for scaling physics-based simulators and developing models directly from experimental data. In particular, recent advances based on deep generative modeling and geometric deep learning have enabled probabilistic simulation by modeling complex distributions over trajectories while respecting the permutation symmetry that is fundamental to N-body systems. However, to generate realistic trajectories, existing methods must learn complex transformations starting from uninformed noise and do not allow for the exploitation of domain-informed priors. In this work, we propose STFlow to address this limitation. By leveraging flow matching and data-dependent couplings, STFlow facilitates physics-informed simulation of geometric trajectories without sacrificing model expressivity or scalability. Our evaluation on N-body dynamical systems, molecular dynamics, and pedestrian dynamics benchmarks shows that STFlow produces significantly lower prediction errors while enabling more efficient inference, highlighting the benefits of employing physics-informed prior distributions in probabilistic geometric trajectory modeling.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLM-QFL: Distilling Large Language Model for Quantum Federated Learning</title>
<link>https://arxiv.org/abs/2505.18656</link>
<guid>https://arxiv.org/abs/2505.18656</guid>
<content:encoded><![CDATA[
arXiv:2505.18656v1 Announce Type: new 
Abstract: Inspired by the power of large language models (LLMs), our research adapts them to quantum federated learning (QFL) to boost efficiency and performance. We propose a federated fine-tuning method that distills an LLM within QFL, allowing each client to locally adapt the model to its own data while preserving privacy and reducing unnecessary global updates. The fine-tuned LLM also acts as a reinforcement agent, optimizing QFL by adjusting optimizer steps, cutting down communication rounds, and intelligently selecting clients. Experiments show significant efficiency gains. We pioneer a synergy between LLM and QFL, offering: i) practical efficiency: Reduced communication costs and faster convergence. ii) theoretical rigor: Provable guarantees for adaptive federated optimization. iii) scalability: PEFT methods (LoRA, QLoRA) enable deployment on resource-constrained quantum devices. Code implementation is available here 1.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Self-Supervised Evolution Operator Learning for High-Dimensional Dynamical Systems</title>
<link>https://arxiv.org/abs/2505.18671</link>
<guid>https://arxiv.org/abs/2505.18671</guid>
<content:encoded><![CDATA[
arXiv:2505.18671v1 Announce Type: new 
Abstract: We introduce an encoder-only approach to learn the evolution operators of large-scale non-linear dynamical systems, such as those describing complex natural phenomena. Evolution operators are particularly well-suited for analyzing systems that exhibit complex spatio-temporal patterns and have become a key analytical tool across various scientific communities. As terabyte-scale weather datasets and simulation tools capable of running millions of molecular dynamics steps per day are becoming commodities, our approach provides an effective tool to make sense of them from a data-driven perspective. The core of it lies in a remarkable connection between self-supervised representation learning methods and the recently established learning theory of evolution operators. To show the usefulness of the proposed method, we test it across multiple scientific domains: explaining the folding dynamics of small proteins, the binding process of drug-like molecules in host sites, and autonomously finding patterns in climate data. Code and data to reproduce the experiments are made available open source.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?</title>
<link>https://arxiv.org/abs/2505.18672</link>
<guid>https://arxiv.org/abs/2505.18672</guid>
<content:encoded><![CDATA[
arXiv:2505.18672v1 Announce Type: new 
Abstract: Representation intervention aims to locate and modify the representations that encode the underlying concepts in Large Language Models (LLMs) to elicit the aligned and expected behaviors. Despite the empirical success, it has never been examined whether one could locate the faithful concepts for intervention. In this work, we explore the question in safety alignment. If the interventions are faithful, the intervened LLMs should erase the harmful concepts and be robust to both in-distribution adversarial prompts and the out-of-distribution (OOD) jailbreaks. While it is feasible to erase harmful concepts without degrading the benign functionalities of LLMs in linear settings, we show that it is infeasible in the general non-linear setting. To tackle the issue, we propose Concept Concentration (COCA). Instead of identifying the faithful locations to intervene, COCA refractors the training data with an explicit reasoning process, which firstly identifies the potential unsafe concepts and then decides the responses. Essentially, COCA simplifies the decision boundary between harmful and benign representations, enabling more effective linear erasure. Extensive experiments with multiple representation intervention methods and model architectures demonstrate that COCA significantly reduces both in-distribution and OOD jailbreak success rates, and meanwhile maintaining strong performance on regular tasks such as math and code generation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor</title>
<link>https://arxiv.org/abs/2505.18693</link>
<guid>https://arxiv.org/abs/2505.18693</guid>
<content:encoded><![CDATA[
arXiv:2505.18693v1 Announce Type: new 
Abstract: Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a cost-effective and stable alternative to conventional architectures, utilizing only an absorber layer and an electron transport layer (ETL). This study presents a machine learning (ML)-driven framework to optimize the efficiency and stability of HTL-free PSCs by integrating experimental validation with numerical simulations. Excellent agreement is achieved between a fabricated device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in \(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is methylammonium. A dataset of 1650 samples is generated by varying molar fraction, absorber defect density, thickness, and ETL doping, with corresponding efficiency and 50-hour degradation as targets. A fourth-degree polynomial regressor (PR-4) shows the best performance, achieving RMSEs of 0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and degradation, respectively. The derived model generalizes beyond the training range and is used in an L-BFGS-B optimization algorithm with a weighted objective function to maximize efficiency and minimize degradation. This improves device efficiency from 13.7\% to 16.84\% and reduces degradation from 6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior and inferior classes, and a multilayer perceptron (MLP) classifier achieves 100\% accuracy, successfully identifying optimal configurations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Can LLMs Alleviate Catastrophic Forgetting in Graph Continual Learning? A Systematic Study</title>
<link>https://arxiv.org/abs/2505.18697</link>
<guid>https://arxiv.org/abs/2505.18697</guid>
<content:encoded><![CDATA[
arXiv:2505.18697v1 Announce Type: new 
Abstract: Nowadays, real-world data, including graph-structure data, often arrives in a streaming manner, which means that learning systems need to continuously acquire new knowledge without forgetting previously learned information. Although substantial existing works attempt to address catastrophic forgetting in graph machine learning, they are all based on training from scratch with streaming data. With the rise of pretrained models, an increasing number of studies have leveraged their strong generalization ability for continual learning. Therefore, in this work, we attempt to answer whether large language models (LLMs) can mitigate catastrophic forgetting in Graph Continual Learning (GCL). We first point out that current experimental setups for GCL have significant flaws, as the evaluation stage may lead to task ID leakage. Then, we evaluate the performance of LLMs in more realistic scenarios and find that even minor modifications can lead to outstanding results. Finally, based on extensive experiments, we propose a simple-yet-effective method, Simple Graph Continual Learning (SimGCL), that surpasses the previous state-of-the-art GNN-based baseline by around 20% under the rehearsal-free constraint. To facilitate reproducibility, we have developed an easy-to-use benchmark LLM4GCL for training and evaluating existing GCL methods. The code is available at: https://github.com/ZhixunLEE/LLM4GCL.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>MonarchAttention: Zero-Shot Conversion to Fast, Hardware-Aware Structured Attention</title>
<link>https://arxiv.org/abs/2505.18698</link>
<guid>https://arxiv.org/abs/2505.18698</guid>
<content:encoded><![CDATA[
arXiv:2505.18698v1 Announce Type: new 
Abstract: Transformers have achieved state-of-the-art performance across various tasks, but suffer from a notable quadratic complexity in sequence length due to the attention mechanism. In this work, we propose MonarchAttention -- a novel approach to sub-quadratic attention approximation via Monarch matrices, an expressive class of structured matrices. Based on the variational form of softmax, we describe an efficient optimization-based algorithm to compute an approximate projection of softmax attention onto the class of Monarch matrices with $\Theta(N\sqrt{N} d)$ computational complexity and $\Theta(Nd)$ memory/IO complexity. Unlike previous approaches, MonarchAttention is both (1) transferable, yielding minimal performance loss with no additional training, even when replacing every attention layer of the transformer, and (2) hardware-efficient, utilizing the highest-throughput tensor core units on modern GPUs. With optimized kernels, MonarchAttention achieves substantial speed-ups in wall-time over FlashAttention-2: $1.4\times$ for shorter sequences $(N=256)$, $4.5\times$ for medium-length sequences $(N=4K)$, and $8.2\times$ for longer sequences $(N=16K)$. We demonstrate the quality of MonarchAttention on diverse tasks and architectures in vision and language problems, showing that it flexibly and accurately approximates softmax attention in a variety of contexts. Our code is available at https://github.com/cjyaras/monarch-attention.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Steering LLM Reasoning Through Bias-Only Adaptation</title>
<link>https://arxiv.org/abs/2505.18706</link>
<guid>https://arxiv.org/abs/2505.18706</guid>
<content:encoded><![CDATA[
arXiv:2505.18706v1 Announce Type: new 
Abstract: Recent work on reasoning-oriented language models, exemplified by o1-like systems, suggests that reinforcement-learning (RL) finetuning does not create new capabilities but instead strengthens reasoning patterns already latent in the pretrained network. We test this claim by training steering vectors: layer-wise biases that additively amplify selected hidden features while leaving all original weights unchanged. Experiments on four base models across the GSM8K and MATH benchmarks show that steering vectors recover, and in several cases exceed, the accuracy of fully-tuned counterparts. This result supports the view that the required reasoning skills pre-exist in the base model. Further, logit-lens analysis reveals that the trained vectors consistently boost token groups linked to structured languages and logical connectors, providing an interpretable account that aligns with the demands of quantitative reasoning tasks.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Neural Parameter Search for Slimmer Fine-Tuned Models and Better Transfer</title>
<link>https://arxiv.org/abs/2505.18713</link>
<guid>https://arxiv.org/abs/2505.18713</guid>
<content:encoded><![CDATA[
arXiv:2505.18713v1 Announce Type: new 
Abstract: Foundation models and their checkpoints have significantly advanced deep learning, boosting performance across various applications. However, fine-tuned models often struggle outside their specific domains and exhibit considerable redundancy. Recent studies suggest that combining a pruned fine-tuned model with the original pre-trained model can mitigate forgetting, reduce interference when merging model parameters across tasks, and improve compression efficiency. In this context, developing an effective pruning strategy for fine-tuned models is crucial. Leveraging the advantages of the task vector mechanism, we preprocess fine-tuned models by calculating the differences between them and the original model. Recognizing that different task vector subspaces contribute variably to model performance, we introduce a novel method called Neural Parameter Search (NPS-Pruning) for slimming down fine-tuned models. This method enhances pruning efficiency by searching through neural parameters of task vectors within low-rank subspaces. Our method has three key applications: enhancing knowledge transfer through pairwise model interpolation, facilitating effective knowledge fusion via model merging, and enabling the deployment of compressed models that retain near-original performance while significantly reducing storage costs. Extensive experiments across vision, NLP, and multi-modal benchmarks demonstrate the effectiveness and robustness of our approach, resulting in substantial performance gains. The code is publicly available at: https://github.com/duguodong7/NPS-Pruning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LoTA-QAF: Lossless Ternary Adaptation for Quantization-Aware Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.18724</link>
<guid>https://arxiv.org/abs/2505.18724</guid>
<content:encoded><![CDATA[
arXiv:2505.18724v1 Announce Type: new 
Abstract: Quantization and fine-tuning are crucial for deploying large language models (LLMs) on resource-constrained edge devices. However, fine-tuning quantized models presents significant challenges, primarily stemming from: First, the mismatch in data types between the low-precision quantized weights (e.g., 4-bit) and the high-precision adaptation weights (e.g., 16-bit). This mismatch limits the computational efficiency advantage offered by quantized weights during inference. Second, potential accuracy degradation when merging these high-precision adaptation weights into the low-precision quantized weights, as the adaptation weights often necessitate approximation or truncation. Third, as far as we know, no existing methods support the lossless merging of adaptation while adjusting all quantized weights. To address these challenges, we introduce lossless ternary adaptation for quantization-aware fine-tuning (LoTA-QAF). This is a novel fine-tuning method specifically designed for quantized LLMs, enabling the lossless merging of ternary adaptation weights into quantized weights and the adjustment of all quantized weights. LoTA-QAF operates through a combination of: i) A custom-designed ternary adaptation (TA) that aligns ternary weights with the quantization grid and uses these ternary weights to adjust quantized weights. ii) A TA-based mechanism that enables the lossless merging of adaptation weights. iii) Ternary signed gradient descent (t-SignSGD) for updating the TA weights. We apply LoTA-QAF to Llama-3.1/3.3 and Qwen-2.5 model families and validate its effectiveness on several downstream tasks. On the MMLU benchmark, our method effectively recovers performance for quantized models, surpassing 16-bit LoRA by up to 5.14\%. For task-specific fine-tuning, 16-bit LoRA achieves superior results, but LoTA-QAF still outperforms other methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Message-Passing State-Space Models: Improving Graph Learning with Modern Sequence Modeling</title>
<link>https://arxiv.org/abs/2505.18728</link>
<guid>https://arxiv.org/abs/2505.18728</guid>
<content:encoded><![CDATA[
arXiv:2505.18728v1 Announce Type: new 
Abstract: The recent success of State-Space Models (SSMs) in sequence modeling has motivated their adaptation to graph learning, giving rise to Graph State-Space Models (GSSMs). However, existing GSSMs operate by applying SSM modules to sequences extracted from graphs, often compromising core properties such as permutation equivariance, message-passing compatibility, and computational efficiency. In this paper, we introduce a new perspective by embedding the key principles of modern SSM computation directly into the Message-Passing Neural Network framework, resulting in a unified methodology for both static and temporal graphs. Our approach, MP-SSM, enables efficient, permutation-equivariant, and long-range information propagation while preserving the architectural simplicity of message passing. Crucially, MP-SSM enables an exact sensitivity analysis, which we use to theoretically characterize information flow and evaluate issues like vanishing gradients and over-squashing in the deep regime. Furthermore, our design choices allow for a highly optimized parallel implementation akin to modern SSMs. We validate MP-SSM across a wide range of tasks, including node classification, graph property prediction, long-range benchmarks, and spatiotemporal forecasting, demonstrating both its versatility and strong empirical performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reward-Driven Interaction: Enhancing Proactive Dialogue Agents through User Satisfaction Prediction</title>
<link>https://arxiv.org/abs/2505.18731</link>
<guid>https://arxiv.org/abs/2505.18731</guid>
<content:encoded><![CDATA[
arXiv:2505.18731v1 Announce Type: new 
Abstract: Reward-driven proactive dialogue agents require precise estimation of user satisfaction as an intrinsic reward signal to determine optimal interaction strategies. Specifically, this framework triggers clarification questions when detecting potential user dissatisfaction during interactions in the industrial dialogue system. Traditional works typically rely on training a neural network model based on weak labels which are generated by a simple model trained on user actions after current turn. However, existing methods suffer from two critical limitations in real-world scenarios: (1) Noisy Reward Supervision, dependence on weak labels derived from post-hoc user actions introduces bias, particularly failing to capture satisfaction signals in ASR-error-induced utterances; (2) Long-Tail Feedback Sparsity, the power-law distribution of user queries causes reward prediction accuracy to drop in low-frequency domains. The noise in the weak labels and a power-law distribution of user utterances results in that the model is hard to learn good representation of user utterances and sessions. To address these limitations, we propose two auxiliary tasks to improve the representation learning of user utterances and sessions that enhance user satisfaction prediction. The first one is a contrastive self-supervised learning task, which helps the model learn the representation of rare user utterances and identify ASR errors. The second one is a domain-intent classification task, which aids the model in learning the representation of user sessions from long-tailed domains and improving the model's performance on such domains. The proposed method is evaluated on DuerOS, demonstrating significant improvements in the accuracy of error recognition on rare user utterances and long-tailed domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AuroRA: Breaking Low-Rank Bottleneck of LoRA with Nonlinear Mapping</title>
<link>https://arxiv.org/abs/2505.18738</link>
<guid>https://arxiv.org/abs/2505.18738</guid>
<content:encoded><![CDATA[
arXiv:2505.18738v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning (PEFT) method validated across NLP and CV domains. However, LoRA faces an inherent low-rank bottleneck: narrowing its performance gap with full finetuning requires increasing the rank of its parameter matrix, resulting in significant parameter overhead. Recent linear LoRA variants have attempted to enhance expressiveness by introducing additional linear mappings; however, their composition remains inherently linear and fails to fundamentally improve LoRA's representational capacity. To address this limitation, we propose AuroRA, which incorporates an Adaptive Nonlinear Layer (ANL) between two linear projectors to capture fixed and learnable nonlinearities. This combination forms an MLP-like structure with a compressed rank, enabling flexible and precise approximation of diverse target functions while theoretically guaranteeing lower approximation errors and bounded gradients. Extensive experiments on 22 datasets and 6 pretrained models demonstrate that AuroRA: (I) not only matches or surpasses full fine-tuning performance with only 6.18% ~ 25% of LoRA's parameters but also (II) outperforms state-of-the-art PEFT methods by up to 10.88% in both NLP and CV tasks, and (III) exhibits robust performance across various rank configurations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Smart Energy Guardian: A Hybrid Deep Learning Model for Detecting Fraudulent PV Generation</title>
<link>https://arxiv.org/abs/2505.18755</link>
<guid>https://arxiv.org/abs/2505.18755</guid>
<content:encoded><![CDATA[
arXiv:2505.18755v1 Announce Type: new 
Abstract: With the proliferation of smart grids, smart cities face growing challenges due to cyber-attacks and sophisticated electricity theft behaviors, particularly in residential photovoltaic (PV) generation systems. Traditional Electricity Theft Detection (ETD) methods often struggle to capture complex temporal dependencies and integrating multi-source data, limiting their effectiveness. In this work, we propose an efficient ETD method that accurately identifies fraudulent behaviors in residential PV generation, thus ensuring the supply-demand balance in smart cities. Our hybrid deep learning model, combining multi-scale Convolutional Neural Network (CNN), Long Short-Term Memory (LSTM), and Transformer, excels in capturing both short-term and long-term temporal dependencies. Additionally, we introduce a data embedding technique that seamlessly integrates time-series data with discrete temperature variables, enhancing detection robustness. Extensive simulation experiments using real-world data validate the effectiveness of our approach, demonstrating significant improvements in the accuracy of detecting sophisticated energy theft activities, thereby contributing to the stability and fairness of energy systems in smart cities.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reducing Storage of Pretrained Neural Networks by Rate-Constrained Quantization and Entropy Coding</title>
<link>https://arxiv.org/abs/2505.18758</link>
<guid>https://arxiv.org/abs/2505.18758</guid>
<content:encoded><![CDATA[
arXiv:2505.18758v1 Announce Type: new 
Abstract: The ever-growing size of neural networks poses serious challenges on resource-constrained devices, such as embedded sensors. Compression algorithms that reduce their size can mitigate these problems, provided that model performance stays close to the original. We propose a novel post-training compression framework that combines rate-aware quantization with entropy coding by (1) extending the well-known layer-wise loss by a quadratic rate estimation, and (2) providing locally exact solutions to this modified objective following the Optimal Brain Surgeon (OBS) method. Our method allows for very fast decoding and is compatible with arbitrary quantization grids. We verify our results empirically by testing on various computer-vision networks, achieving a 20-40\% decrease in bit rate at the same performance as the popular compression algorithm NNCodec. Our code is available at https://github.com/Conzel/cerwu.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GenPO: Generative Diffusion Models Meet On-Policy Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18763</link>
<guid>https://arxiv.org/abs/2505.18763</guid>
<content:encoded><![CDATA[
arXiv:2505.18763v1 Announce Type: new 
Abstract: Recent advances in reinforcement learning (RL) have demonstrated the powerful exploration capabilities and multimodality of generative diffusion-based policies. While substantial progress has been made in offline RL and off-policy RL settings, integrating diffusion policies into on-policy frameworks like PPO remains underexplored. This gap is particularly significant given the widespread use of large-scale parallel GPU-accelerated simulators, such as IsaacLab, which are optimized for on-policy RL algorithms and enable rapid training of complex robotic tasks. A key challenge lies in computing state-action log-likelihoods under diffusion policies, which is straightforward for Gaussian policies but intractable for flow-based models due to irreversible forward-reverse processes and discretization errors (e.g., Euler-Maruyama approximations). To bridge this gap, we propose GenPO, a generative policy optimization framework that leverages exact diffusion inversion to construct invertible action mappings. GenPO introduces a novel doubled dummy action mechanism that enables invertibility via alternating updates, resolving log-likelihood computation barriers. Furthermore, we also use the action log-likelihood for unbiased entropy and KL divergence estimation, enabling KL-adaptive learning rates and entropy regularization in on-policy updates. Extensive experiments on eight IsaacLab benchmarks, including legged locomotion (Ant, Humanoid, Anymal-D, Unitree H1, Go2), dexterous manipulation (Shadow Hand), aerial control (Quadcopter), and robotic arm tasks (Franka), demonstrate GenPO's superiority over existing RL baselines. Notably, GenPO is the first method to successfully integrate diffusion policies into on-policy RL, unlocking their potential for large-scale parallelized training and real-world robotic deployment.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Multiple Wasserstein Gradient Descent Algorithm for Multi-Objective Distributional Optimization</title>
<link>https://arxiv.org/abs/2505.18765</link>
<guid>https://arxiv.org/abs/2505.18765</guid>
<content:encoded><![CDATA[
arXiv:2505.18765v1 Announce Type: new 
Abstract: We address the optimization problem of simultaneously minimizing multiple objective functionals over a family of probability distributions. This type of Multi-Objective Distributional Optimization commonly arises in machine learning and statistics, with applications in areas such as multiple target sampling, multi-task learning, and multi-objective generative modeling. To solve this problem, we propose an iterative particle-based algorithm, which we call Muliple Wasserstein Gradient Descent (MWGraD), which constructs a flow of intermediate empirical distributions, each being represented by a set of particles, which gradually minimize the multiple objective functionals simultaneously. Specifically, MWGraD consists of two key steps at each iteration. First, it estimates the Wasserstein gradient for each objective functional based on the current particles. Then, it aggregates these gradients into a single Wasserstein gradient using dynamically adjusted weights and updates the particles accordingly. In addition, we provide theoretical analysis and present experimental results on both synthetic and real-world datasets, demonstrating the effectiveness of MWGraD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>HD-PiSSA: High-Rank Distributed Orthogonal Adaptation</title>
<link>https://arxiv.org/abs/2505.18777</link>
<guid>https://arxiv.org/abs/2505.18777</guid>
<content:encoded><![CDATA[
arXiv:2505.18777v1 Announce Type: new 
Abstract: Existing parameter-efficient fine-tuning (PEFT) methods for large language models (LLMs), such as LoRA and PiSSA, constrain model updates to low-rank subspaces, limiting their expressiveness and leading to suboptimal performance on complex tasks. To address this, we introduce High-rank Distributed PiSSA (HD-PiSSA), a distributed PEFT approach that initializes orthogonal adapters across different devices and aggregates their delta updates collectively on W for fine-tuning. Unlike Data Parallel LoRA or PiSSA, which maintain identical adapters across all devices, HD-PiSSA assigns different principal components of the pre-trained weights to each GPU, significantly expanding the range of update directions. This results in over 16x higher effective updated ranks than data-parallel LoRA or PiSSA when fine-tuning on 8 GPUs with the same per-device adapter rank. Empirically, we evaluate HD-PiSSA across various challenging downstream tasks, including mathematics, code generation, and multi-task learning. In the multi-task setting, HD-PiSSA achieves average gains of 10.0 absolute points (14.63%) over LoRA and 4.98 points (6.60%) over PiSSA across 12 benchmarks, demonstrating its benefits from the extra optimization flexibility.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Geometry Aware Operator Transformer as an Efficient and Accurate Neural Surrogate for PDEs on Arbitrary Domains</title>
<link>https://arxiv.org/abs/2505.18781</link>
<guid>https://arxiv.org/abs/2505.18781</guid>
<content:encoded><![CDATA[
arXiv:2505.18781v1 Announce Type: new 
Abstract: The very challenging task of learning solution operators of PDEs on arbitrary domains accurately and efficiently is of vital importance to engineering and industrial simulations. Despite the existence of many operator learning algorithms to approximate such PDEs, we find that accurate models are not necessarily computationally efficient and vice versa. We address this issue by proposing a geometry aware operator transformer (GAOT) for learning PDEs on arbitrary domains. GAOT combines novel multiscale attentional graph neural operator encoders and decoders, together with geometry embeddings and (vision) transformer processors to accurately map information about the domain and the inputs into a robust approximation of the PDE solution. Multiple innovations in the implementation of GAOT also ensure computational efficiency and scalability. We demonstrate this significant gain in both accuracy and efficiency of GAOT over several baselines on a large number of learning tasks from a diverse set of PDEs, including achieving state of the art performance on a large scale three-dimensional industrial CFD dataset.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Soft Weighted Machine Unlearning</title>
<link>https://arxiv.org/abs/2505.18783</link>
<guid>https://arxiv.org/abs/2505.18783</guid>
<content:encoded><![CDATA[
arXiv:2505.18783v1 Announce Type: new 
Abstract: Machine unlearning, as a post-hoc processing technique, has gained widespread adoption in addressing challenges like bias mitigation and robustness enhancement, colloquially, machine unlearning for fairness and robustness. However, existing non-privacy unlearning-based solutions persist in using binary data removal framework designed for privacy-driven motivation, leading to significant information loss, a phenomenon known as over-unlearning. While over-unlearning has been largely described in many studies as primarily causing utility degradation, we investigate its fundamental causes and provide deeper insights in this work through counterfactual leave-one-out analysis. In this paper, we introduce a weighted influence function that assigns tailored weights to each sample by solving a convex quadratic programming problem analytically. Building on this, we propose a soft-weighted framework enabling fine-grained model adjustments to address the over-unlearning challenge. We demonstrate that the proposed soft-weighted scheme is versatile and can be seamlessly integrated into most existing unlearning algorithms. Extensive experiments show that in fairness- and robustness-driven tasks, the soft-weighted scheme significantly outperforms hard-weighted schemes in fairness/robustness metrics and alleviates the decline in utility metric, thereby enhancing machine unlearning algorithm as an effective correction solution.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Leveraging Per-Instance Privacy for Machine Unlearning</title>
<link>https://arxiv.org/abs/2505.18786</link>
<guid>https://arxiv.org/abs/2505.18786</guid>
<content:encoded><![CDATA[
arXiv:2505.18786v1 Announce Type: new 
Abstract: We present a principled, per-instance approach to quantifying the difficulty of unlearning via fine-tuning. We begin by sharpening an analysis of noisy gradient descent for unlearning (Chien et al., 2024), obtaining a better utility-unlearning tradeoff by replacing worst-case privacy loss bounds with per-instance privacy losses (Thudi et al., 2024), each of which bounds the (Renyi) divergence to retraining without an individual data point. To demonstrate the practical applicability of our theory, we present empirical results showing that our theoretical predictions are born out both for Stochastic Gradient Langevin Dynamics (SGLD) as well as for standard fine-tuning without explicit noise. We further demonstrate that per-instance privacy losses correlate well with several existing data difficulty metrics, while also identifying harder groups of data points, and introduce novel evaluation methods based on loss barriers. All together, our findings provide a foundation for more efficient and adaptive unlearning strategies tailored to the unique properties of individual data points.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Governing Equation Discovery from Data Based on Differential Invariants</title>
<link>https://arxiv.org/abs/2505.18798</link>
<guid>https://arxiv.org/abs/2505.18798</guid>
<content:encoded><![CDATA[
arXiv:2505.18798v1 Announce Type: new 
Abstract: The explicit governing equation is one of the simplest and most intuitive forms for characterizing physical laws. However, directly discovering partial differential equations (PDEs) from data poses significant challenges, primarily in determining relevant terms from a vast search space. Symmetry, as a crucial prior knowledge in scientific fields, has been widely applied in tasks such as designing equivariant networks and guiding neural PDE solvers. In this paper, we propose a pipeline for governing equation discovery based on differential invariants, which can losslessly reduce the search space of existing equation discovery methods while strictly adhering to symmetry. Specifically, we compute the set of differential invariants corresponding to the infinitesimal generators of the symmetry group and select them as the relevant terms for equation discovery. Taking DI-SINDy (SINDy based on Differential Invariants) as an example, we demonstrate that its success rate and accuracy in PDE discovery surpass those of other symmetry-informed governing equation discovery methods across a series of PDEs.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>How to build a consistency model: Learning flow maps via self-distillation</title>
<link>https://arxiv.org/abs/2505.18825</link>
<guid>https://arxiv.org/abs/2505.18825</guid>
<content:encoded><![CDATA[
arXiv:2505.18825v1 Announce Type: new 
Abstract: Building on the framework proposed in Boffi et al. (2024), we present a systematic approach for learning flow maps associated with flow and diffusion models. Flow map-based models, commonly known as consistency models, encompass recent efforts to improve the efficiency of generative models based on solutions to differential equations. By exploiting a relationship between the velocity field underlying a continuous-time flow and the instantaneous rate of change of the flow map, we show how to convert existing distillation schemes into direct training algorithms via self-distillation, eliminating the need for pre-trained models. We empirically evaluate several instantiations of our framework, finding that high-dimensional tasks like image synthesis benefit from objective functions that avoid temporal and spatial derivatives of the flow map, while lower-dimensional tasks can benefit from objectives incorporating higher-order derivatives to capture sharp features.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Improved Regret and Contextual Linear Extension for Pandora's Box and Prophet Inequality</title>
<link>https://arxiv.org/abs/2505.18828</link>
<guid>https://arxiv.org/abs/2505.18828</guid>
<content:encoded><![CDATA[
arXiv:2505.18828v1 Announce Type: new 
Abstract: We study the Pandora's Box problem in an online learning setting with semi-bandit feedback. In each round, the learner sequentially pays to open up to $n$ boxes with unknown reward distributions, observes rewards upon opening, and decides when to stop. The utility of the learner is the maximum observed reward minus the cumulative cost of opened boxes, and the goal is to minimize regret defined as the gap between the cumulative expected utility and that of the optimal policy. We propose a new algorithm that achieves $\widetilde{O}(\sqrt{nT})$ regret after $T$ rounds, which improves the $\widetilde{O}(n\sqrt{T})$ bound of Agarwal et al. [2024] and matches the known lower bound up to logarithmic factors. To better capture real-life applications, we then extend our results to a natural but challenging contextual linear setting, where each box's expected reward is linear in some known but time-varying $d$-dimensional context and the noise distribution is fixed over time. We design an algorithm that learns both the linear function and the noise distributions, achieving $\widetilde{O}(nd\sqrt{T})$ regret. Finally, we show that our techniques also apply to the online Prophet Inequality problem, where the learner must decide immediately whether or not to accept a revealed reward. In both non-contextual and contextual settings, our approach achieves similar improvements and regret bounds.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>On the Effect of Negative Gradient in Group Relative Deep Reinforcement Optimization</title>
<link>https://arxiv.org/abs/2505.18830</link>
<guid>https://arxiv.org/abs/2505.18830</guid>
<content:encoded><![CDATA[
arXiv:2505.18830v1 Announce Type: new 
Abstract: Reinforcement learning (RL) has become popular in enhancing the reasoning capabilities of large language models (LLMs), with Group Relative Policy Optimization (GRPO) emerging as a widely used algorithm in recent systems. Despite GRPO's widespread adoption, we identify a previously unrecognized phenomenon we term Lazy Likelihood Displacement (LLD), wherein the likelihood of correct responses marginally increases or even decreases during training. This behavior mirrors a recently discovered misalignment issue in Direct Preference Optimization (DPO), attributed to the influence of negative gradients. We provide a theoretical analysis of GRPO's learning dynamic, identifying the source of LLD as the naive penalization of all tokens in incorrect responses with the same strength. To address this, we develop a method called NTHR, which downweights penalties on tokens contributing to the LLD. Unlike prior DPO-based approaches, NTHR takes advantage of GRPO's group-based structure, using correct responses as anchors to identify influential tokens. Experiments on math reasoning benchmarks demonstrate that NTHR effectively mitigates LLD, yielding consistent performance gains across models ranging from 0.5B to 3B parameters.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distribution-Aware Mobility-Assisted Decentralized Federated Learning</title>
<link>https://arxiv.org/abs/2505.18866</link>
<guid>https://arxiv.org/abs/2505.18866</guid>
<content:encoded><![CDATA[
arXiv:2505.18866v1 Announce Type: new 
Abstract: Decentralized federated learning (DFL) has attracted significant attention due to its scalability and independence from a central server. In practice, some participating clients can be mobile, yet the impact of user mobility on DFL performance remains largely unexplored, despite its potential to facilitate communication and model convergence. In this work, we demonstrate that introducing a small fraction of mobile clients, even with random movement, can significantly improve the accuracy of DFL by facilitating information flow. To further enhance performance, we propose novel distribution-aware mobility patterns, where mobile clients strategically navigate the network, leveraging knowledge of data distributions and static client locations. The proposed moving strategies mitigate the impact of data heterogeneity and boost learning convergence. Extensive experiments validate the effectiveness of induced mobility in DFL and demonstrate the superiority of our proposed mobility patterns over random movement.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>RefLoRA: Refactored Low-Rank Adaptation for Efficient Fine-Tuning of Large Models</title>
<link>https://arxiv.org/abs/2505.18877</link>
<guid>https://arxiv.org/abs/2505.18877</guid>
<content:encoded><![CDATA[
arXiv:2505.18877v1 Announce Type: new 
Abstract: Low-Rank Adaptation (LoRA) lowers the computational and memory overhead of fine-tuning large models by updating a low-dimensional subspace of the pre-trained weight matrix. Albeit efficient, LoRA exhibits suboptimal convergence and noticeable performance degradation, due to inconsistent and imbalanced weight updates induced by its nonunique low-rank factorizations. To overcome these limitations, this article identifies the optimal low-rank factorization per step that minimizes an upper bound on the loss. The resultant refactored low-rank adaptation (RefLoRA) method promotes a flatter loss landscape, along with consistent and balanced weight updates, thus speeding up stable convergence. Extensive experiments evaluate RefLoRA on natural language understanding, and commonsense reasoning tasks with popular large language models including DeBERTaV3, LLaMA-7B, LLaMA2-7B and LLaMA3-8B. The numerical tests corroborate that RefLoRA converges faster, outperforms various benchmarks, and enjoys negligible computational overhead compared to state-of-the-art LoRA variants.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Partition Generative Modeling: Masked Modeling Without Masks</title>
<link>https://arxiv.org/abs/2505.18883</link>
<guid>https://arxiv.org/abs/2505.18883</guid>
<content:encoded><![CDATA[
arXiv:2505.18883v1 Announce Type: new 
Abstract: We introduce ``Partition Generative Models'' (PGMs), a novel approach to masked generative modeling (MGMs), particularly effective for masked diffusion language modeling (MDLMs). PGM divides tokens into two distinct groups and employs sparse attention patterns to prevent cross-group information exchange. Hence, the model is trained to predict tokens in one group based solely on information from the other group. This partitioning strategy eliminates the need for MASK tokens entirely. While traditional MGMs inefficiently process MASK tokens during generation, PGMs achieve greater computational efficiency by operating exclusively on unmasked tokens. Our experiments on OpenWebText with a context length of 1024 tokens demonstrate that PGMs deliver at least 5x improvements in both latency and throughput compared to MDLM when using the same number of sampling steps, while generating samples with better generative perplexity than MDLM. Finally, we show that PGMs can be distilled with Self-Distillation Through Time (SDTT), a method originally devised for MDLM, in order to achieve further inference gains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LORE: Lagrangian-Optimized Robust Embeddings for Visual Encoders</title>
<link>https://arxiv.org/abs/2505.18884</link>
<guid>https://arxiv.org/abs/2505.18884</guid>
<content:encoded><![CDATA[
arXiv:2505.18884v1 Announce Type: new 
Abstract: Visual encoders have become fundamental components in modern computer vision pipelines. However, ensuring robustness against adversarial perturbations remains a critical challenge. Recent efforts have explored both supervised and unsupervised adversarial fine-tuning strategies. We identify two key limitations in these approaches: (i) they often suffer from instability, especially during the early stages of fine-tuning, resulting in suboptimal convergence and degraded performance on clean data, and (ii) they exhibit a suboptimal trade-off between robustness and clean data accuracy, hindering the simultaneous optimization of both objectives. To overcome these challenges, we propose Lagrangian-Optimized Robust Embeddings (LORE), a novel unsupervised adversarial fine-tuning framework. LORE utilizes constrained optimization, which offers a principled approach to balancing competing goals, such as improving robustness while preserving nominal performance. By enforcing embedding-space proximity constraints, LORE effectively maintains clean data performance throughout adversarial fine-tuning. Extensive experiments show that LORE significantly improves zero-shot adversarial robustness with minimal degradation in clean data accuracy. Furthermore, we demonstrate the effectiveness of the adversarially fine-tuned CLIP image encoder in out-of-distribution generalization and enhancing the interpretability of image embeddings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>KerZOO: Kernel Function Informed Zeroth-Order Optimization for Accurate and Accelerated LLM Fine-Tuning</title>
<link>https://arxiv.org/abs/2505.18886</link>
<guid>https://arxiv.org/abs/2505.18886</guid>
<content:encoded><![CDATA[
arXiv:2505.18886v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated impressive capabilities across numerous NLP tasks. Nevertheless, conventional first-order fine-tuning techniques impose heavy memory demands, creating practical obstacles to real-world applications. Zeroth-order (ZO) optimization has recently emerged as a promising memory-efficient alternative, as it circumvents the need for backpropagation by estimating gradients solely through forward passes--making it particularly suitable for resource-limited environments. Despite its efficiency, ZO optimization suffers from gradient estimation bias, which significantly hinders convergence speed. To address this, we analytically identify and characterize the lower-order bias introduced during ZO-based gradient estimation in LLM fine-tuning. Motivated by tools in mathematical physics, we introduce a kernel-function-based ZO framework aimed at mitigating this bias and improving optimization stability. KerZOO achieves comparable or superior performance to existing ZO baselines in both full-parameter and parameter-efficient fine-tuning settings of LLMs, while significantly reducing the number of iterations required to reach convergence. For example, KerZOO reduces total GPU training hours by as much as 74% and 44% on WSC and MultiRC datasets in fine-tuning OPT-2.7B model and can exceed the MeZO baseline by 2.9% and 2.6% in accuracy. We show that the kernel function is an effective avenue for reducing estimation bias in ZO methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Conformal Prediction for Uncertainty Estimation in Drug-Target Interaction Prediction</title>
<link>https://arxiv.org/abs/2505.18890</link>
<guid>https://arxiv.org/abs/2505.18890</guid>
<content:encoded><![CDATA[
arXiv:2505.18890v1 Announce Type: new 
Abstract: Accurate drug-target interaction (DTI) prediction with machine learning models is essential for drug discovery. Such models should also provide a credible representation of their uncertainty, but applying classical marginal conformal prediction (CP) in DTI prediction often overlooks variability across drug and protein subgroups. In this work, we analyze three cluster-conditioned CP methods for DTI prediction, and compare them with marginal and group-conditioned CP. Clusterings are obtained via nonconformity scores, feature similarity, and nearest neighbors, respectively. Experiments on the KIBA dataset using four data-splitting strategies show that nonconformity-based clustering yields the tightest intervals and most reliable subgroup coverage, especially in random and fully unseen drug-protein splits. Group-conditioned CP works well when one entity is familiar, but residual-driven clustering provides robust uncertainty estimates even in sparse or novel scenarios. These results highlight the potential of cluster-based CP for improving DTI prediction under uncertainty.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>PromptWise: Online Learning for Cost-Aware Prompt Assignment in Generative Models</title>
<link>https://arxiv.org/abs/2505.18901</link>
<guid>https://arxiv.org/abs/2505.18901</guid>
<content:encoded><![CDATA[
arXiv:2505.18901v1 Announce Type: new 
Abstract: The rapid advancement of generative AI models has provided users with numerous options to address their prompts. When selecting a generative AI model for a given prompt, users should consider not only the performance of the chosen model but also its associated service cost. The principle guiding such consideration is to select the least expensive model among the available satisfactory options. However, existing model-selection approaches typically prioritize performance, overlooking pricing differences between models. In this paper, we introduce PromptWise, an online learning framework designed to assign a sequence of prompts to a group of large language models (LLMs) in a cost-effective manner. PromptWise strategically queries cheaper models first, progressing to more expensive options only if the lower-cost models fail to adequately address a given prompt. Through numerical experiments, we demonstrate PromptWise's effectiveness across various tasks, including puzzles of varying complexity and code generation/translation tasks. The results highlight that PromptWise consistently outperforms cost-unaware baseline methods, emphasizing that directly assigning prompts to the most expensive models can lead to higher costs and potentially lower average performance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Behavior Injection: Preparing Language Models for Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.18917</link>
<guid>https://arxiv.org/abs/2505.18917</guid>
<content:encoded><![CDATA[
arXiv:2505.18917v1 Announce Type: new 
Abstract: Reinforcement fine-tuning (RFT) has emerged as a powerful post-training technique to incentivize the reasoning ability of large language models (LLMs). However, LLMs can respond very inconsistently to RFT: some show substantial performance gains, while others plateau or even degrade. To understand this divergence, we analyze the per-step influence of the RL objective and identify two key conditions for effective post-training: (1) RL-informative rollout accuracy, and (2) strong data co-influence, which quantifies how much the training data affects performance on other samples. Guided by these insights, we propose behavior injection, a task-agnostic data-augmentation scheme applied prior to RL. Behavior injection enriches the supervised finetuning (SFT) data by seeding exploratory and exploitative behaviors, effectively making the model more RL-ready. We evaluate our method across two reasoning benchmarks with multiple base models. The results demonstrate that our theoretically motivated augmentation can significantly increases the performance gain from RFT over the pre-RL model.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Graph-Based Operator Learning from Limited Data on Irregular Domains</title>
<link>https://arxiv.org/abs/2505.18923</link>
<guid>https://arxiv.org/abs/2505.18923</guid>
<content:encoded><![CDATA[
arXiv:2505.18923v1 Announce Type: new 
Abstract: Operator learning seeks to approximate mappings from input functions to output solutions, particularly in the context of partial differential equations (PDEs). While recent advances such as DeepONet and Fourier Neural Operator (FNO) have demonstrated strong performance, they often rely on regular grid discretizations, limiting their applicability to complex or irregular domains. In this work, we propose a Graph-based Operator Learning with Attention (GOLA) framework that addresses this limitation by constructing graphs from irregularly sampled spatial points and leveraging attention-enhanced Graph Neural Netwoks (GNNs) to model spatial dependencies with global information. To improve the expressive capacity, we introduce a Fourier-based encoder that projects input functions into a frequency space using learnable complex coefficients, allowing for flexible embeddings even with sparse or nonuniform samples. We evaluated our approach across a range of 2D PDEs, including Darcy Flow, Advection, Eikonal, and Nonlinear Diffusion, under varying sampling densities. Our method consistently outperforms baselines, particularly in data-scarce regimes, demonstrating strong generalization and efficiency on irregular domains.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time</title>
<link>https://arxiv.org/abs/2505.18926</link>
<guid>https://arxiv.org/abs/2505.18926</guid>
<content:encoded><![CDATA[
arXiv:2505.18926v1 Announce Type: new 
Abstract: We propose a neural physics system for real-time, interactive fluid simulations. Traditional physics-based methods, while accurate, are computationally intensive and suffer from latency issues. Recent machine-learning methods reduce computational costs while preserving fidelity; yet most still fail to satisfy the latency constraints for real-time use and lack support for interactive applications. To bridge this gap, we introduce a novel hybrid method that integrates numerical simulation, neural physics, and generative control. Our neural physics jointly pursues low-latency simulation and high physical fidelity by employing a fallback safeguard to classical numerical solvers. Furthermore, we develop a diffusion-based controller that is trained using a reverse modeling strategy to generate external dynamic force fields for fluid manipulation. Our system demonstrates robust performance across diverse 2D/3D scenarios, material types, and obstacle interactions, achieving real-time simulations at high frame rates (11~29% latency) while enabling fluid control guided by user-friendly freehand sketches. We present a significant step towards practical, controllable, and physically plausible fluid simulations for real-time interactive applications. We promise to release both models and data upon acceptance.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chi-Square Wavelet Graph Neural Networks for Heterogeneous Graph Anomaly Detection</title>
<link>https://arxiv.org/abs/2505.18934</link>
<guid>https://arxiv.org/abs/2505.18934</guid>
<content:encoded><![CDATA[
arXiv:2505.18934v1 Announce Type: new 
Abstract: Graph Anomaly Detection (GAD) in heterogeneous networks presents unique challenges due to node and edge heterogeneity. Existing Graph Neural Network (GNN) methods primarily focus on homogeneous GAD and thus fail to address three key issues: (C1) Capturing abnormal signal and rich semantics across diverse meta-paths; (C2) Retaining high-frequency content in HIN dimension alignment; and (C3) Learning effectively from difficult anomaly samples with class imbalance. To overcome these, we propose ChiGAD, a spectral GNN framework based on a novel Chi-Square filter, inspired by the wavelet effectiveness in diverse domains. Specifically, ChiGAD consists of: (1) Multi-Graph Chi-Square Filter, which captures anomalous information via applying dedicated Chi-Square filters to each meta-path graph; (2) Interactive Meta-Graph Convolution, which aligns features while preserving high-frequency information and incorporates heterogeneous messages by a unified Chi-Square Filter; and (3) Contribution-Informed Cross-Entropy Loss, which prioritizes difficult anomalies to address class imbalance. Extensive experiments on public and industrial datasets show that ChiGAD outperforms state-of-the-art models on multiple metrics. Additionally, its homogeneous variant, ChiGNN, excels on seven GAD datasets, validating the effectiveness of Chi-Square filters. Our code is available at https://github.com/HsipingLi/ChiGAD.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Exact Expressive Power of Transformers with Padding</title>
<link>https://arxiv.org/abs/2505.18948</link>
<guid>https://arxiv.org/abs/2505.18948</guid>
<content:encoded><![CDATA[
arXiv:2505.18948v1 Announce Type: new 
Abstract: Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding converge to precisely the class $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, padded transformers converge to the class $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Online Knowledge Distillation with Reward Guidance</title>
<link>https://arxiv.org/abs/2505.18952</link>
<guid>https://arxiv.org/abs/2505.18952</guid>
<content:encoded><![CDATA[
arXiv:2505.18952v1 Announce Type: new 
Abstract: This work studies knowledge distillation (KD) for large language models (LLMs) through preference optimization. We propose a reward-guided imitation learning framework for sequential KD, formulating a min-max optimization problem between the policy and reward model (RM) to minimize the performance gap between the student and teacher policies. Specifically, the reward optimization is constrained to achieve near-optimality within a confidence set for preference alignment. For preference data construction, we explore both offline and online preference-based KD. Additionally, we reformulate the RM using the $Q$-value function and extend the framework to white-box KD, where the teacher policy's predicted probabilities are accessible. Theoretical analysis and empirical results demonstrate the effectiveness of the proposed framework.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Protein Design with Dynamic Protein Vocabulary</title>
<link>https://arxiv.org/abs/2505.18966</link>
<guid>https://arxiv.org/abs/2505.18966</guid>
<content:encoded><![CDATA[
arXiv:2505.18966v1 Announce Type: new 
Abstract: Protein design is a fundamental challenge in biotechnology, aiming to design novel sequences with specific functions within the vast space of possible proteins. Recent advances in deep generative models have enabled function-based protein design from textual descriptions, yet struggle with structural plausibility. Inspired by classical protein design methods that leverage natural protein structures, we explore whether incorporating fragments from natural proteins can enhance foldability in generative models. Our empirical results show that even random incorporation of fragments improves foldability. Building on this insight, we introduce ProDVa, a novel protein design approach that integrates a text encoder for functional descriptions, a protein language model for designing proteins, and a fragment encoder to dynamically retrieve protein fragments based on textual functional descriptions. Experimental results demonstrate that our approach effectively designs protein sequences that are both functionally aligned and structurally plausible. Compared to state-of-the-art models, ProDVa achieves comparable function alignment using less than 0.04% of the training data, while designing significantly more well-folded proteins, with the proportion of proteins having pLDDT above 70 increasing by 7.38% and those with PAE below 10 increasing by 9.6%.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GraSS: Scalable Influence Function with Sparse Gradient Compression</title>
<link>https://arxiv.org/abs/2505.18976</link>
<guid>https://arxiv.org/abs/2505.18976</guid>
<content:encoded><![CDATA[
arXiv:2505.18976v1 Announce Type: new 
Abstract: Gradient-based data attribution methods, such as influence functions, are critical for understanding the impact of individual training samples without requiring repeated model retraining. However, their scalability is often limited by the high computational and memory costs associated with per-sample gradient computation. In this work, we propose GraSS, a novel gradient compression algorithm and its variants FactGraSS for linear layers specifically, that explicitly leverage the inherent sparsity of per-sample gradients to achieve sub-linear space and time complexity. Extensive experiments demonstrate the effectiveness of our approach, achieving substantial speedups while preserving data influence fidelity. In particular, FactGraSS achieves up to 165% faster throughput on billion-scale models compared to the previous state-of-the-art baselines. Our code is publicly available at https://github.com/TRAIS-Lab/GraSS.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization</title>
<link>https://arxiv.org/abs/2505.18979</link>
<guid>https://arxiv.org/abs/2505.18979</guid>
<content:encoded><![CDATA[
arXiv:2505.18979v1 Announce Type: new 
Abstract: Text-to-image (T2I) generation models can inadvertently produce not-safe-for-work (NSFW) content, prompting the integration of text and image safety filters. Recent advances employ large language models (LLMs) for semantic-level detection, rendering traditional token-level perturbation attacks largely ineffective. However, our evaluation shows that existing jailbreak methods are ineffective against these modern filters. We introduce GhostPrompt, the first automated jailbreak framework that combines dynamic prompt optimization with multimodal feedback. It consists of two key components: (i) Dynamic Optimization, an iterative process that guides a large language model (LLM) using feedback from text safety filters and CLIP similarity scores to generate semantically aligned adversarial prompts; and (ii) Adaptive Safety Indicator Injection, which formulates the injection of benign visual cues as a reinforcement learning problem to bypass image-level filters. GhostPrompt achieves state-of-the-art performance, increasing the ShieldLM-7B bypass rate from 12.5\% (Sneakyprompt) to 99.0\%, improving CLIP score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \times$. Moreover, it generalizes to unseen filters including GPT-4.1 and successfully jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing systemic vulnerabilities in current multimodal defenses. To support further research on AI safety and red-teaming, we will release code and adversarial prompts under a controlled-access protocol.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FedSKC: Federated Learning with Non-IID Data via Structural Knowledge Collaboration</title>
<link>https://arxiv.org/abs/2505.18981</link>
<guid>https://arxiv.org/abs/2505.18981</guid>
<content:encoded><![CDATA[
arXiv:2505.18981v1 Announce Type: new 
Abstract: With the advancement of edge computing, federated learning (FL) displays a bright promise as a privacy-preserving collaborative learning paradigm. However, one major challenge for FL is the data heterogeneity issue, which refers to the biased labeling preferences among multiple clients, negatively impacting convergence and model performance. Most previous FL methods attempt to tackle the data heterogeneity issue locally or globally, neglecting underlying class-wise structure information contained in each client. In this paper, we first study how data heterogeneity affects the divergence of the model and decompose it into local, global, and sampling drift sub-problems. To explore the potential of using intra-client class-wise structural knowledge in handling these drifts, we thus propose Federated Learning with Structural Knowledge Collaboration (FedSKC). The key idea of FedSKC is to extract and transfer domain preferences from inter-client data distributions, offering diverse class-relevant knowledge and a fair convergent signal. FedSKC comprises three components: i) local contrastive learning, to prevent weight divergence resulting from local training; ii) global discrepancy aggregation, which addresses the parameter deviation between the server and clients; iii) global period review, correcting for the sampling drift introduced by the server randomly selecting devices. We have theoretically analyzed FedSKC under non-convex objectives and empirically validated its superiority through extensive experimental results.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>AmorLIP: Efficient Language-Image Pretraining via Amortization</title>
<link>https://arxiv.org/abs/2505.18983</link>
<guid>https://arxiv.org/abs/2505.18983</guid>
<content:encoded><![CDATA[
arXiv:2505.18983v1 Announce Type: new 
Abstract: Contrastive Language-Image Pretraining (CLIP) has demonstrated strong zero-shot performance across diverse downstream text-image tasks. Existing CLIP methods typically optimize a contrastive objective using negative samples drawn from each minibatch. To achieve robust representation learning, these methods require extremely large batch sizes and escalate computational demands to hundreds or even thousands of GPUs. Prior approaches to mitigate this issue often compromise downstream performance, prolong training duration, or face scalability challenges with very large datasets. To overcome these limitations, we propose AmorLIP, an efficient CLIP pretraining framework that amortizes expensive computations involved in contrastive learning through lightweight neural networks, which substantially improves training efficiency and performance. Leveraging insights from a spectral factorization of energy-based models, we introduce novel amortization objectives along with practical techniques to improve training stability. Extensive experiments across 38 downstream tasks demonstrate the superior zero-shot classification and retrieval capabilities of AmorLIP, consistently outperforming standard CLIP baselines with substantial relative improvements of up to 12.24%.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>STRICT: Stress Test of Rendering Images Containing Text</title>
<link>https://arxiv.org/abs/2505.18985</link>
<guid>https://arxiv.org/abs/2505.18985</guid>
<content:encoded><![CDATA[
arXiv:2505.18985v1 Announce Type: new 
Abstract: While diffusion models have revolutionized text-to-image generation with their ability to synthesize realistic and diverse scenes, they continue to struggle to generate consistent and legible text within images. This shortcoming is commonly attributed to the locality bias inherent in diffusion-based generation, which limits their ability to model long-range spatial dependencies. In this paper, we introduce $\textbf{STRICT}$, a benchmark designed to systematically stress-test the ability of diffusion models to render coherent and instruction-aligned text in images. Our benchmark evaluates models across multiple dimensions: (1) the maximum length of readable text that can be generated; (2) the correctness and legibility of the generated text, and (3) the ratio of not following instructions for generating text. We evaluate several state-of-the-art models, including proprietary and open-source variants, and reveal persistent limitations in long-range consistency and instruction-following capabilities. Our findings provide insights into architectural bottlenecks and motivate future research directions in multimodal generative modeling. We release our entire evaluation pipeline at https://github.com/tianyu-z/STRICT-Bench.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Automatic and Structure-Aware Sparsification of Hybrid Neural ODEs</title>
<link>https://arxiv.org/abs/2505.18996</link>
<guid>https://arxiv.org/abs/2505.18996</guid>
<content:encoded><![CDATA[
arXiv:2505.18996v1 Announce Type: new 
Abstract: Hybrid neural ordinary differential equations (neural ODEs) integrate mechanistic models with neural ODEs, offering strong inductive bias and flexibility, and are particularly advantageous in data-scarce healthcare settings. However, excessive latent states and interactions from mechanistic models can lead to training inefficiency and over-fitting, limiting practical effectiveness of hybrid neural ODEs. In response, we propose a new hybrid pipeline for automatic state selection and structure optimization in mechanistic neural ODEs, combining domain-informed graph modifications with data-driven regularization to sparsify the model for improving predictive performance and stability while retaining mechanistic plausibility. Experiments on synthetic and real-world data show improved predictive performance and robustness with desired sparsity, establishing an effective solution for hybrid model reduction in healthcare applications.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Semi-pessimistic Reinforcement Learning</title>
<link>https://arxiv.org/abs/2505.19002</link>
<guid>https://arxiv.org/abs/2505.19002</guid>
<content:encoded><![CDATA[
arXiv:2505.19002v1 Announce Type: new 
Abstract: Offline reinforcement learning (RL) aims to learn an optimal policy from pre-collected data. However, it faces challenges of distributional shift, where the learned policy may encounter unseen scenarios not covered in the offline data. Additionally, numerous applications suffer from a scarcity of labeled reward data. Relying on labeled data alone often leads to a narrow state-action distribution, further amplifying the distributional shift, and resulting in suboptimal policy learning. To address these issues, we first recognize that the volume of unlabeled data is typically substantially larger than that of labeled data. We then propose a semi-pessimistic RL method to effectively leverage abundant unlabeled data. Our approach offers several advantages. It considerably simplifies the learning process, as it seeks a lower bound of the reward function, rather than that of the Q-function or state transition function. It is highly flexible, and can be integrated with a range of model-free and model-based RL algorithms. It enjoys the guaranteed improvement when utilizing vast unlabeled data, but requires much less restrictive conditions. We compare our method with a number of alternative solutions, both analytically and numerically, and demonstrate its clear competitiveness. We further illustrate with an application to adaptive deep brain stimulation for Parkinson's disease.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Faithful Group Shapley Value</title>
<link>https://arxiv.org/abs/2505.19013</link>
<guid>https://arxiv.org/abs/2505.19013</guid>
<content:encoded><![CDATA[
arXiv:2505.19013v1 Announce Type: new 
Abstract: Data Shapley is an important tool for data valuation, which quantifies the contribution of individual data points to machine learning models. In practice, group-level data valuation is desirable when data providers contribute data in batch. However, we identify that existing group-level extensions of Data Shapley are vulnerable to shell company attacks, where strategic group splitting can unfairly inflate valuations. We propose Faithful Group Shapley Value (FGSV) that uniquely defends against such attacks. Building on original mathematical insights, we develop a provably fast and accurate approximation algorithm for computing FGSV. Empirical experiments demonstrate that our algorithm significantly outperforms state-of-the-art methods in computational efficiency and approximation accuracy, while ensuring faithful group-level valuation.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Tokenizing Electron Cloud in Protein-Ligand Interaction Learning</title>
<link>https://arxiv.org/abs/2505.19014</link>
<guid>https://arxiv.org/abs/2505.19014</guid>
<content:encoded><![CDATA[
arXiv:2505.19014v1 Announce Type: new 
Abstract: The affinity and specificity of protein-molecule binding directly impact functional outcomes, uncovering the mechanisms underlying biological regulation and signal transduction. Most deep-learning-based prediction approaches focus on structures of atoms or fragments. However, quantum chemical properties, such as electronic structures, are the key to unveiling interaction patterns but remain largely underexplored. To bridge this gap, we propose ECBind, a method for tokenizing electron cloud signals into quantized embeddings, enabling their integration into downstream tasks such as binding affinity prediction. By incorporating electron densities, ECBind helps uncover binding modes that cannot be fully represented by atom-level models. Specifically, to remove the redundancy inherent in electron cloud signals, a structure-aware transformer and hierarchical codebooks encode 3D binding sites enriched with electron structures into tokens. These tokenized codes are then used for specific tasks with labels. To extend its applicability to a wider range of scenarios, we utilize knowledge distillation to develop an electron-cloud-agnostic prediction model. Experimentally, ECBind demonstrates state-of-the-art performance across multiple tasks, achieving improvements of 6.42\% and 15.58\% in per-structure Pearson and Spearman correlation coefficients, respectively.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Querying Kernel Methods Suffices for Reconstructing their Training Data</title>
<link>https://arxiv.org/abs/2505.19019</link>
<guid>https://arxiv.org/abs/2505.19019</guid>
<content:encoded><![CDATA[
arXiv:2505.19019v1 Announce Type: new 
Abstract: Over-parameterized models have raised concerns about their potential to memorize training data, even when achieving strong generalization. The privacy implications of such memorization are generally unclear, particularly in scenarios where only model outputs are accessible. We study this question in the context of kernel methods, and demonstrate both empirically and theoretically that querying kernel models at various points suffices to reconstruct their training data, even without access to model parameters. Our results hold for a range of kernel methods, including kernel regression, support vector machines, and kernel density estimation. Our hope is that this work can illuminate potential privacy concerns for such models.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Learn Beneficial Noise as Graph Augmentation</title>
<link>https://arxiv.org/abs/2505.19024</link>
<guid>https://arxiv.org/abs/2505.19024</guid>
<content:encoded><![CDATA[
arXiv:2505.19024v1 Announce Type: new 
Abstract: Although graph contrastive learning (GCL) has been widely investigated, it is still a challenge to generate effective and stable graph augmentations. Existing methods often apply heuristic augmentation like random edge dropping, which may disrupt important graph structures and result in unstable GCL performance. In this paper, we propose Positive-incentive Noise driven Graph Data Augmentation (PiNGDA), where positive-incentive noise (pi-noise) scientifically analyzes the beneficial effect of noise under the information theory. To bridge the standard GCL and pi-noise framework, we design a Gaussian auxiliary variable to convert the loss function to information entropy. We prove that the standard GCL with pre-defined augmentations is equivalent to estimate the beneficial noise via the point estimation. Following our analysis, PiNGDA is derived from learning the beneficial noise on both topology and attributes through a trainable noise generator for graph augmentations, instead of the simple estimation. Since the generator learns how to produce beneficial perturbations on graph topology and node attributes, PiNGDA is more reliable compared with the existing methods. Extensive experimental results validate the effectiveness and stability of PiNGDA.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Turb-L1: Achieving Long-term Turbulence Tracing By Tackling Spectral Bias</title>
<link>https://arxiv.org/abs/2505.19038</link>
<guid>https://arxiv.org/abs/2505.19038</guid>
<content:encoded><![CDATA[
arXiv:2505.19038v1 Announce Type: new 
Abstract: Accurately predicting the long-term evolution of turbulence is crucial for advancing scientific understanding and optimizing engineering applications. However, existing deep learning methods face significant bottlenecks in long-term autoregressive prediction, which exhibit excessive smoothing and fail to accurately track complex fluid dynamics. Our extensive experimental and spectral analysis of prevailing methods provides an interpretable explanation for this shortcoming, identifying Spectral Bias as the core obstacle. Concretely, spectral bias is the inherent tendency of models to favor low-frequency, smooth features while overlooking critical high-frequency details during training, thus reducing fidelity and causing physical distortions in long-term predictions. Building on this insight, we propose Turb-L1, an innovative turbulence prediction method, which utilizes a Hierarchical Dynamics Synthesis mechanism within a multi-grid architecture to explicitly overcome spectral bias. It accurately captures cross-scale interactions and preserves the fidelity of high-frequency dynamics, enabling reliable long-term tracking of turbulence evolution. Extensive experiments on the 2D turbulence benchmark show that Turb-L1 demonstrates excellent performance: (I) In long-term predictions, it reduces Mean Squared Error (MSE) by $80.3\%$ and increases Structural Similarity (SSIM) by over $9\times$ compared to the SOTA baseline, significantly improving prediction fidelity. (II) It effectively overcomes spectral bias, accurately reproducing the full enstrophy spectrum and maintaining physical realism in high-wavenumber regions, thus avoiding the spectral distortions or spurious energy accumulation seen in other methods.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Offline Clustering of Linear Bandits: Unlocking the Power of Clusters in Data-Limited Environments</title>
<link>https://arxiv.org/abs/2505.19043</link>
<guid>https://arxiv.org/abs/2505.19043</guid>
<content:encoded><![CDATA[
arXiv:2505.19043v1 Announce Type: new 
Abstract: Contextual linear multi-armed bandits are a learning framework for making a sequence of decisions, e.g., advertising recommendations for a sequence of arriving users. Recent works have shown that clustering these users based on the similarity of their learned preferences can significantly accelerate the learning. However, prior work has primarily focused on the online setting, which requires continually collecting user data, ignoring the offline data widely available in many applications. To tackle these limitations, we study the offline clustering of bandits (Off-ClusBand) problem, which studies how to use the offline dataset to learn cluster properties and improve decision-making across multiple users. The key challenge in Off-ClusBand arises from data insufficiency for users: unlike the online case, in the offline case, we have a fixed, limited dataset to work from and thus must determine whether we have enough data to confidently cluster users together. To address this challenge, we propose two algorithms: Off-C$^2$LUB, which we analytically show performs well for arbitrary amounts of user data, and Off-CLUB, which is prone to bias when data is limited but, given sufficient data, matches a theoretical lower bound that we derive for the offline clustered MAB problem. We experimentally validate these results on both real and synthetic datasets.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Structured Reinforcement Learning for Combinatorial Decision-Making</title>
<link>https://arxiv.org/abs/2505.19053</link>
<guid>https://arxiv.org/abs/2505.19053</guid>
<content:encoded><![CDATA[
arXiv:2505.19053v1 Announce Type: new 
Abstract: Reinforcement learning (RL) is increasingly applied to real-world problems involving complex and structured decisions, such as routing, scheduling, and assortment planning. These settings challenge standard RL algorithms, which struggle to scale, generalize, and exploit structure in the presence of combinatorial action spaces. We propose Structured Reinforcement Learning (SRL), a novel actor-critic framework that embeds combinatorial optimization layers into the actor neural network. We enable end-to-end learning of the actor via Fenchel-Young losses and provide a geometric interpretation of SRL as a primal-dual algorithm in the dual of the moment polytope. Across six environments with exogenous and endogenous uncertainty, SRL matches or surpasses the performance of unstructured RL and imitation learning on static tasks and improves over these baselines by up to 92% on dynamic problems, with improved stability and convergence speed.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Reduce Computational Cost In Deep Reinforcement Learning Via Randomized Policy Learning</title>
<link>https://arxiv.org/abs/2505.19054</link>
<guid>https://arxiv.org/abs/2505.19054</guid>
<content:encoded><![CDATA[
arXiv:2505.19054v1 Announce Type: new 
Abstract: Recent advancements in reinforcement learning (RL) have leveraged neural networks to achieve state-of-the-art performance across various control tasks. However, these successes often come at the cost of significant computational resources, as training deep neural networks requires substantial time and data. In this paper, we introduce an actor-critic algorithm that utilizes randomized neural networks to drastically reduce computational costs while maintaining strong performance. Despite its simple architecture, our method effectively solves a range of control problems, including the locomotion control of a highly dynamic 12-motor quadruped robot, and achieves results comparable to leading algorithms such as Proximal Policy Optimization (PPO). Notably, our approach does not outperform other algorithms in terms of sample efficnency but rather in terms of wall-clock training time. That is, although our algorithm requires more timesteps to converge to an optimal policy, the actual time required for training turns out to be lower.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Distributionally Robust Deep Q-Learning</title>
<link>https://arxiv.org/abs/2505.19058</link>
<guid>https://arxiv.org/abs/2505.19058</guid>
<content:encoded><![CDATA[
arXiv:2505.19058v1 Announce Type: new 
Abstract: We propose a novel distributionally robust $Q$-learning algorithm for the non-tabular case accounting for continuous state spaces where the state transition of the underlying Markov decision process is subject to model uncertainty. The uncertainty is taken into account by considering the worst-case transition from a ball around a reference probability measure. To determine the optimal policy under the worst-case state transition, we solve the associated non-linear Bellman equation by dualising and regularising the Bellman operator with the Sinkhorn distance, which is then parameterized with deep neural networks. This approach allows us to modify the Deep Q-Network algorithm to optimise for the worst case state transition.
  We illustrate the tractability and effectiveness of our approach through several applications, including a portfolio optimisation task based on S\&{P}~500 data.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Adversarial Bandit over Bandits: Hierarchical Bandits for Online Configuration Management</title>
<link>https://arxiv.org/abs/2505.19061</link>
<guid>https://arxiv.org/abs/2505.19061</guid>
<content:encoded><![CDATA[
arXiv:2505.19061v1 Announce Type: new 
Abstract: Motivated by dynamic parameter optimization in finite, but large action (configurations) spaces, this work studies the nonstochastic multi-armed bandit (MAB) problem in metric action spaces with oblivious Lipschitz adversaries. We propose ABoB, a hierarchical Adversarial Bandit over Bandits algorithm that can use state-of-the-art existing "flat" algorithms, but additionally clusters similar configurations to exploit local structures and adapt to changing environments. We prove that in the worst-case scenario, such clustering approach cannot hurt too much and ABoB guarantees a standard worst-case regret bound of $O\left(k^{\frac{1}{2}}T^{\frac{1}{2}}\right)$, where $T$ is the number of rounds and $k$ is the number of arms, matching the traditional flat approach. However, under favorable conditions related to the algorithm properties, clusters properties, and certain Lipschitz conditions, the regret bound can be improved to $O\left(k^{\frac{1}{4}}T^{\frac{1}{2}}\right)$. Simulations and experiments on a real storage system demonstrate that ABoB, using standard algorithms like EXP3 and Tsallis-INF, achieves lower regret and faster convergence than the flat method, up to 50% improvement in known previous setups, nonstochastic and stochastic, as well as in our settings.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Recalibrating binary probabilistic classifiers</title>
<link>https://arxiv.org/abs/2505.19068</link>
<guid>https://arxiv.org/abs/2505.19068</guid>
<content:encoded><![CDATA[
arXiv:2505.19068v1 Announce Type: new 
Abstract: Recalibration of binary probabilistic classifiers to a target prior probability is an important task in areas like credit risk management. We analyse methods for recalibration from a distribution shift perspective. Distribution shift assumptions linked to the area under the curve (AUC) of a probabilistic classifier are found to be useful for the design of meaningful recalibration methods. Two new methods called parametric covariate shift with posterior drift (CSPD) and ROC-based quasi moment matching (QMM) are proposed and tested together with some other methods in an example setting. The outcomes of the test suggest that the QMM methods discussed in the paper can provide appropriately conservative results in evaluations with concave functionals like for instance risk weights functions for credit risk.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Temperature is All You Need for Generalization in Langevin Dynamics and other Markov Processes</title>
<link>https://arxiv.org/abs/2505.19087</link>
<guid>https://arxiv.org/abs/2505.19087</guid>
<content:encoded><![CDATA[
arXiv:2505.19087v1 Announce Type: new 
Abstract: We analyze the generalization gap (gap between the training and test errors) when training a potentially over-parametrized model using a Markovian stochastic training algorithm, initialized from some distribution $\theta_0 \sim p_0$. We focus on Langevin dynamics with a positive temperature $\beta^{-1}$, i.e. gradient descent on a training loss $L$ with infinitesimal step size, perturbed with $\beta^{-1}$-variances Gaussian noise, and lightly regularized or bounded. There, we bound the generalization gap, at any time during training, by $\sqrt{(\beta\mathbb{E} L (\theta_0) + \log(1/\delta))/N}$ with probability $1-\delta$ over the dataset, where $N$ is the sample size, and $\mathbb{E} L (\theta_0) =O(1)$ with standard initialization scaling. In contrast to previous guarantees, we have no dependence on either training time or reliance on mixing, nor a dependence on dimensionality, gradient norms, or any other properties of the loss or model. This guarantee follows from a general analysis of any Markov process-based training that has a Gibbs-style stationary distribution. The proof is surprisingly simple, once we observe that the marginal distribution divergence from initialization remains bounded, as implied by a generalized second law of thermodynamics.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>CMoS: Rethinking Time Series Prediction Through the Lens of Chunk-wise Spatial Correlations</title>
<link>https://arxiv.org/abs/2505.19090</link>
<guid>https://arxiv.org/abs/2505.19090</guid>
<content:encoded><![CDATA[
arXiv:2505.19090v1 Announce Type: new 
Abstract: Recent advances in lightweight time series forecasting models suggest the inherent simplicity of time series forecasting tasks. In this paper, we present CMoS, a super-lightweight time series forecasting model. Instead of learning the embedding of the shapes, CMoS directly models the spatial correlations between different time series chunks. Additionally, we introduce a Correlation Mixing technique that enables the model to capture diverse spatial correlations with minimal parameters, and an optional Periodicity Injection technique to ensure faster convergence. Despite utilizing as low as 1% of the lightweight model DLinear's parameters count, experimental results demonstrate that CMoS outperforms existing state-of-the-art models across multiple datasets. Furthermore, the learned weights of CMoS exhibit great interpretability, providing practitioners with valuable insights into temporal structures within specific application scenarios.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Towards Robust Influence Functions with Flat Validation Minima</title>
<link>https://arxiv.org/abs/2505.19097</link>
<guid>https://arxiv.org/abs/2505.19097</guid>
<content:encoded><![CDATA[
arXiv:2505.19097v1 Announce Type: new 
Abstract: The Influence Function (IF) is a widely used technique for assessing the impact of individual training samples on model predictions. However, existing IF methods often fail to provide reliable influence estimates in deep neural networks, particularly when applied to noisy training data. This issue does not stem from inaccuracies in parameter change estimation, which has been the primary focus of prior research, but rather from deficiencies in loss change estimation, specifically due to the sharpness of validation risk. In this work, we establish a theoretical connection between influence estimation error, validation set risk, and its sharpness, underscoring the importance of flat validation minima for accurate influence estimation. Furthermore, we introduce a novel estimation form of Influence Function specifically designed for flat validation minima. Experimental results across various tasks validate the superiority of our approach.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Latent Mamba Operator for Partial Differential Equations</title>
<link>https://arxiv.org/abs/2505.19105</link>
<guid>https://arxiv.org/abs/2505.19105</guid>
<content:encoded><![CDATA[
arXiv:2505.19105v1 Announce Type: new 
Abstract: Neural operators have emerged as powerful data-driven frameworks for solving Partial Differential Equations (PDEs), offering significant speedups over numerical methods. However, existing neural operators struggle with scalability in high-dimensional spaces, incur high computational costs, and face challenges in capturing continuous and long-range dependencies in PDE dynamics. To address these limitations, we introduce the Latent Mamba Operator (LaMO), which integrates the efficiency of state-space models (SSMs) in latent space with the expressive power of kernel integral formulations in neural operators. We also establish a theoretical connection between state-space models (SSMs) and the kernel integral of neural operators. Extensive experiments across diverse PDE benchmarks on regular grids, structured meshes, and point clouds covering solid and fluid physics datasets, LaMOs achieve consistent state-of-the-art (SOTA) performance, with a 32.3\% improvement over existing baselines in solution operator approximation, highlighting its efficacy in modeling complex PDE solutions.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Optimization-Inspired Few-Shot Adaptation for Large Language Models</title>
<link>https://arxiv.org/abs/2505.19107</link>
<guid>https://arxiv.org/abs/2505.19107</guid>
<content:encoded><![CDATA[
arXiv:2505.19107v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable performance in real-world applications. However, adapting LLMs to novel tasks via fine-tuning often requires substantial training data and computational resources that are impractical in few-shot scenarios. Existing approaches, such as in-context learning and Parameter-Efficient Fine-Tuning (PEFT), face key limitations: in-context learning introduces additional inference computational overhead with limited performance gains, while PEFT models are prone to overfitting on the few demonstration examples. In this work, we reinterpret the forward pass of LLMs as an optimization process, a sequence of preconditioned gradient descent steps refining internal representations. Based on this connection, we propose Optimization-Inspired Few-Shot Adaptation (OFA), integrating a parameterization that learns preconditioners without introducing additional trainable parameters, and an objective that improves optimization efficiency by learning preconditioners based on a convergence bound, while simultaneously steering the optimization path toward the flat local minimum. Our method overcomes both issues of ICL-based and PEFT-based methods, and demonstrates superior performance over the existing methods on a variety of few-shot adaptation tasks in experiments.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>FP4 All the Way: Fully Quantized Training of LLMs</title>
<link>https://arxiv.org/abs/2505.19115</link>
<guid>https://arxiv.org/abs/2505.19115</guid>
<content:encoded><![CDATA[
arXiv:2505.19115v1 Announce Type: new 
Abstract: We demonstrate, for the first time, fully quantized training (FQT) of large language models (LLMs) using predominantly 4-bit floating-point (FP4) precision for weights, activations, and gradients on datasets up to 200 billion tokens. We extensively investigate key design choices for FP4, including block sizes, scaling formats, and rounding methods. Our analysis shows that the NVFP4 format, where each block of 16 FP4 values (E2M1) shares a scale represented in E4M3, provides optimal results. We use stochastic rounding for backward and update passes and round-to-nearest for the forward pass to enhance stability. Additionally, we identify a theoretical and empirical threshold for effective quantized training: when the gradient norm falls below approximately $\sqrt{3}$ times the quantization noise, quantized training becomes less effective. Leveraging these insights, we successfully train a 7-billion-parameter model on 256 Intel Gaudi2 accelerators. The resulting FP4-trained model achieves downstream task performance comparable to a standard BF16 baseline, confirming that FP4 training is a practical and highly efficient approach for large-scale LLM training. A reference implementation is supplied in https://github.com/Anonymous1252022/fp4-all-the-way .
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Fast and Accurate Power Load Data Completion via Regularization-optimized Low-Rank Factorization</title>
<link>https://arxiv.org/abs/2505.19133</link>
<guid>https://arxiv.org/abs/2505.19133</guid>
<content:encoded><![CDATA[
arXiv:2505.19133v1 Announce Type: new 
Abstract: Low-rank representation learning has emerged as a powerful tool for recovering missing values in power load data due to its ability to exploit the inherent low-dimensional structures of spatiotemporal measurements. Among various techniques, low-rank factorization models are favoured for their efficiency and interpretability. However, their performance is highly sensitive to the choice of regularization parameters, which are typically fixed or manually tuned, resulting in limited generalization capability or slow convergence in practical scenarios. In this paper, we propose a Regularization-optimized Low-Rank Factorization, which introduces a Proportional-Integral-Derivative controller to adaptively adjust the regularization coefficient. Furthermore, we provide a detailed algorithmic complexity analysis, showing that our method preserves the computational efficiency of stochastic gradient descent while improving adaptivity. Experimental results on real-world power load datasets validate the superiority of our method in both imputation accuracy and training efficiency compared to existing baselines.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>ADGSyn: Dual-Stream Learning for Efficient Anticancer Drug Synergy Prediction</title>
<link>https://arxiv.org/abs/2505.19144</link>
<guid>https://arxiv.org/abs/2505.19144</guid>
<content:encoded><![CDATA[
arXiv:2505.19144v1 Announce Type: new 
Abstract: Drug combinations play a critical role in cancer therapy by significantly enhancing treatment efficacy and overcoming drug resistance. However, the combinatorial space of possible drug pairs grows exponentially, making experimental screening highly impractical. Therefore, developing efficient computational methods to predict promising drug combinations and guide experimental validation is of paramount importance. In this work, we propose ADGSyn, an innovative method for predicting drug synergy. The key components of our approach include: (1) shared projection matrices combined with attention mechanisms to enable cross-drug feature alignment; (2) automatic mixed precision (AMP)-optimized graph operations that reduce memory consumption by 40\% while accelerating training speed threefold; and (3) residual pathways stabilized by LayerNorm to ensure stable gradient propagation during training. Evaluated on the O'Neil dataset containing 13,243 drug--cell line combinations, ADGSyn demonstrates superior performance over eight baseline methods. Moreover, the framework supports full-batch processing of up to 256 molecular graphs on a single GPU, setting a new standard for efficiency in drug synergy prediction within the field of computational oncology.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Computational Inertia as a Conserved Quantity in Frictionless and Damped Learning Dynamics</title>
<link>https://arxiv.org/abs/2505.19171</link>
<guid>https://arxiv.org/abs/2505.19171</guid>
<content:encoded><![CDATA[
arXiv:2505.19171v1 Announce Type: new 
Abstract: We identify a conserved quantity in continuous-time optimization dynamics, termed computational inertia. Defined as the sum of kinetic energy (parameter velocity) and potential energy (loss), this scalar remains invariant under idealized, frictionless training. We formalize this conservation law, derive its analytic decay under damping and stochastic perturbations, and demonstrate its behavior in a synthetic system. The invariant offers a compact lens for interpreting learning trajectories, and may inform theoretical tools for analyzing convergence, stability, and training geometry.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Federated Learning: From Theory to Practice</title>
<link>https://arxiv.org/abs/2505.19183</link>
<guid>https://arxiv.org/abs/2505.19183</guid>
<content:encoded><![CDATA[
arXiv:2505.19183v1 Announce Type: new 
Abstract: This book offers a hands-on introduction to building and understanding federated learning (FL) systems. FL enables multiple devices -- such as smartphones, sensors, or local computers -- to collaboratively train machine learning (ML) models, while keeping their data private and local. It is a powerful solution when data cannot or should not be centralized due to privacy, regulatory, or technical reasons. The book is designed for students, engineers, and researchers who want to learn how to design scalable, privacy preserving FL systems. Our main focus is on personalization: enabling each device to train its own model while still benefiting from collaboration with relevant devices. This is achieved by leveraging similarities between (the learning tasks associated with) devices that are encoded by the weighted edges (or links) of a federated learning network (FL network). The key idea is to represent real-world FL systems as networks of devices, where nodes correspond to device and edges represent communication links and data similarities between them. The training of personalized models for these devices can be naturally framed as a distributed optimization problem. This optimization problem is referred to as generalized total variation minimization (GTVMin) and ensures that devices with similar learning tasks learn similar model parameters. Our approach is both mathematically principled and practically motivated. While we introduce some advanced ideas from optimization theory and graph-based learning, we aim to keep the book accessible. Readers are guided through the core ideas step by step, with intuitive explanations.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Chordless Structure: A Pathway to Simple and Expressive GNNs</title>
<link>https://arxiv.org/abs/2505.19188</link>
<guid>https://arxiv.org/abs/2505.19188</guid>
<content:encoded><![CDATA[
arXiv:2505.19188v1 Announce Type: new 
Abstract: Researchers have proposed various methods of incorporating more structured information into the design of Graph Neural Networks (GNNs) to enhance their expressiveness. However, these methods are either computationally expensive or lacking in provable expressiveness. In this paper, we observe that the chords increase the complexity of the graph structure while contributing little useful information in many cases. In contrast, chordless structures are more efficient and effective for representing the graph. Therefore, when leveraging the information of cycles, we choose to omit the chords. Accordingly, we propose a Chordless Structure-based Graph Neural Network (CSGNN) and prove that its expressiveness is strictly more powerful than the k-hop GNN (KPGNN) with polynomial complexity. Experimental results on real-world datasets demonstrate that CSGNN outperforms existing GNNs across various graph tasks while incurring lower computational costs and achieving better performance than the GNNs of 3-WL expressiveness.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>I2MoE: Interpretable Multimodal Interaction-aware Mixture-of-Experts</title>
<link>https://arxiv.org/abs/2505.19190</link>
<guid>https://arxiv.org/abs/2505.19190</guid>
<content:encoded><![CDATA[
arXiv:2505.19190v1 Announce Type: new 
Abstract: Modality fusion is a cornerstone of multimodal learning, enabling information integration from diverse data sources. However, vanilla fusion methods are limited by (1) inability to account for heterogeneous interactions between modalities and (2) lack of interpretability in uncovering the multimodal interactions inherent in the data. To this end, we propose I2MoE (Interpretable Multimodal Interaction-aware Mixture of Experts), an end-to-end MoE framework designed to enhance modality fusion by explicitly modeling diverse multimodal interactions, as well as providing interpretation on a local and global level. First, I2MoE utilizes different interaction experts with weakly supervised interaction losses to learn multimodal interactions in a data-driven way. Second, I2MoE deploys a reweighting model that assigns importance scores for the output of each interaction expert, which offers sample-level and dataset-level interpretation. Extensive evaluation of medical and general multimodal datasets shows that I2MoE is flexible enough to be combined with different fusion techniques, consistently improves task performance, and provides interpretation across various real-world scenarios. Code is available at https://github.com/Raina-Xin/I2MoE.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Interpretable Graph Learning Over Sets of Temporally-Sparse Data</title>
<link>https://arxiv.org/abs/2505.19193</link>
<guid>https://arxiv.org/abs/2505.19193</guid>
<content:encoded><![CDATA[
arXiv:2505.19193v1 Announce Type: new 
Abstract: Real-world medical data often includes measurements from multiple signals that are collected at irregular and asynchronous time intervals. For example, different types of blood tests can be measured at different times and frequencies, resulting in fragmented and unevenly scattered temporal data. Similar issues of irregular sampling of different attributes occur in other domains, such as monitoring of large systems using event log files or the spread of fake news on social networks. Effectively learning from such data requires models that can handle sets of temporally sparse and heterogeneous signals. In this paper, we propose Graph Mixing Additive Networks (GMAN), a novel and interpretable-by-design model for learning over irregular sets of temporal signals. Our method achieves state-of-the-art performance in real-world medical tasks, including a 4-point increase in the AUROC score of in-hospital mortality prediction, compared to existing methods. We further showcase GMAN's flexibility by applying it to a fake news detection task. We demonstrate how its interpretability capabilities, including node-level, graph-level, and subset-level importance, allow for transition phases detection and gaining medical insights with real-world high-stakes implications. Finally, we provide theoretical insights on GMAN expressive power.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Curvature Dynamic Black-box Attack: revisiting adversarial robustness via dynamic curvature estimation</title>
<link>https://arxiv.org/abs/2505.19194</link>
<guid>https://arxiv.org/abs/2505.19194</guid>
<content:encoded><![CDATA[
arXiv:2505.19194v1 Announce Type: new 
Abstract: Adversarial attack reveals the vulnerability of deep learning models. For about a decade, countless attack and defense methods have been proposed, leading to robustified classifiers and better understanding of models. Among these methods, curvature-based approaches have attracted attention because it is assumed that high curvature may give rise to rough decision boundary. However, the most commonly used \textit{curvature} is the curvature of loss function, scores or other parameters from within the model as opposed to decision boundary curvature, since the former can be relatively easily formed using second order derivative. In this paper, we propose a new query-efficient method, dynamic curvature estimation(DCE), to estimate the decision boundary curvature in a black-box setting. Our approach is based on CGBA, a black-box adversarial attack. By performing DCE on a wide range of classifiers, we discovered, statistically, a connection between decision boundary curvature and adversarial robustness. We also propose a new attack method, curvature dynamic black-box attack(CDBA) with improved performance using the dynamically estimated curvature.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>OptiMindTune: A Multi-Agent Framework for Intelligent Hyperparameter Optimization</title>
<link>https://arxiv.org/abs/2505.19205</link>
<guid>https://arxiv.org/abs/2505.19205</guid>
<content:encoded><![CDATA[
arXiv:2505.19205v1 Announce Type: new 
Abstract: Hyperparameter optimization (HPO) is a critical yet challenging aspect of machine learning model development, significantly impacting model performance and generalization. Traditional HPO methods often struggle with high dimensionality, complex interdependencies, and computational expense. This paper introduces OptiMindTune, a novel multi-agent framework designed to intelligently and efficiently optimize hyperparameters. OptiMindTune leverages the collaborative intelligence of three specialized AI agents -- a Recommender Agent, an Evaluator Agent, and a Decision Agent -- each powered by Google's Gemini models. These agents address distinct facets of the HPO problem, from model selection and hyperparameter suggestion to robust evaluation and strategic decision-making. By fostering dynamic interactions and knowledge sharing, OptiMindTune aims to converge to optimal hyperparameter configurations more rapidly and robustly than existing single-agent or monolithic approaches. Our framework integrates principles from advanced large language models, and adaptive search to achieve scalable and intelligent AutoML. We posit that this multi-agent paradigm offers a promising avenue for tackling the increasing complexity of modern machine learning model tuning.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models</title>
<link>https://arxiv.org/abs/2505.19223</link>
<guid>https://arxiv.org/abs/2505.19223</guid>
<content:encoded><![CDATA[
arXiv:2505.19223v1 Announce Type: new 
Abstract: While Masked Diffusion Models (MDMs), such as LLaDA, present a promising paradigm for language modeling, there has been relatively little effort in aligning these models with human preferences via reinforcement learning. The challenge primarily arises from the high variance in Evidence Lower Bound (ELBO)-based likelihood estimates required for preference optimization. To address this issue, we propose Variance-Reduced Preference Optimization (VRPO), a framework that formally analyzes the variance of ELBO estimators and derives bounds on both the bias and variance of preference optimization gradients. Building on this theoretical foundation, we introduce unbiased variance reduction strategies, including optimal Monte Carlo budget allocation and antithetic sampling, that significantly improve the performance of MDM alignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA, and the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor consistently and significantly across mathematical (GSM8K +4.7), code (HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard +4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical performance compared to strong language MDMs and ARMs. Project page: https://ml-gsai.github.io/LLaDA-1.5-Demo/.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
<item>
<title>Scaling Laws for Gradient Descent and Sign Descent for Linear Bigram Models under Zipf's Law</title>
<link>https://arxiv.org/abs/2505.19227</link>
<guid>https://arxiv.org/abs/2505.19227</guid>
<content:encoded><![CDATA[
arXiv:2505.19227v1 Announce Type: new 
Abstract: Recent works have highlighted optimization difficulties faced by gradient descent in training the first and last layers of transformer-based language models, which are overcome by optimizers such as Adam. These works suggest that the difficulty is linked to the heavy-tailed distribution of words in text data, where the frequency of the $k$th most frequent word $\pi_k$ is proportional to $1/k$, following Zipf's law. To better understand the impact of the data distribution on training performance, we study a linear bigram model for next-token prediction when the tokens follow a power law $\pi_k \propto 1/k^\alpha$ parameterized by the exponent $\alpha > 0$. We derive optimization scaling laws for deterministic gradient descent and sign descent as a proxy for Adam as a function of the exponent $\alpha$. Existing theoretical investigations in scaling laws assume that the eigenvalues of the data decay as a power law with exponent $\alpha > 1$. This assumption effectively makes the problem ``finite dimensional'' as most of the loss comes from a few of the largest eigencomponents. In comparison, we show that the problem is more difficult when the data have heavier tails. The case $\alpha = 1$ as found in text data is ``worst-case'' for gradient descent, in that the number of iterations required to reach a small relative error scales almost linearly with dimension. While the performance of sign descent also depends on the dimension, for Zipf-distributed data the number of iterations scales only with the square-root of the dimension, leading to a large improvement for large vocabularies.
]]></content:encoded>
<pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
</item>
</channel>
</rss>